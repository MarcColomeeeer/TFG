[
    {
        "title": "A note on digitized angles",
        "authors": [
            "Donald E. Knuth"
        ],
        "category": "cs.GR",
        "published_year": "1990",
        "summary": "  We study the configurations of pixels that occur when two digitized straight\nlines meet each other.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/9301112v1"
    },
    {
        "title": "Computer-Generated Photorealistic Hair",
        "authors": [
            "Alice J. Lin"
        ],
        "category": "cs.GR",
        "published_year": "2002",
        "summary": "  This paper presents an efficient method for generating and rendering\nphotorealistic hair in two dimensional pictures. The method consists of three\nmajor steps. Simulating an artist drawing is used to design the rough hair\nshape. A convolution based filter is then used to generate photorealistic hair\npatches. A refine procedure is finally used to blend the boundaries of the\npatches with surrounding areas. This method can be used to create all types of\nphotorealistic human hair (head hair, facial hair and body hair). It is also\nsuitable for fur and grass generation. Applications of this method include:\nhairstyle designing/editing, damaged hair image restoration, human hair\nanimation, virtual makeover of a human, and landscape creation.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0206029v1"
    },
    {
        "title": "Embedded Reflection Mapping",
        "authors": [
            "Paul Anderson",
            "Goncalo Carvalho"
        ],
        "category": "cs.GR",
        "published_year": "2003",
        "summary": "  Environment maps are used to simulate reflections off curved objects. We\npresent a technique to reflect a user, or a group of users, in a real\nenvironment, onto a virtual object, in a virtual reality application, using the\nlive video feeds from a set of cameras, in real-time. Our setup can be used in\na variety of environments ranging from outdoor or indoor scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0304011v1"
    },
    {
        "title": "The Persint visualization program for the ATLAS experiment",
        "authors": [
            "D. Pomarede",
            "M. Virchaux"
        ],
        "category": "cs.GR",
        "published_year": "2003",
        "summary": "  The Persint program is designed for the three-dimensional representation of\nobjects and for the interfacing and access to a variety of independent\napplications, in a fully interactive way. Facilities are provided for the\nspatial navigation and the definition of the visualization properties, in order\nto interactively set the viewing and viewed points, and to obtain the desired\nperspective. In parallel, applications may be launched through the use of\ndedicated interfaces, such as the interactive reconstruction and display of\nphysics events. Recent developments have focalized on the interfacing to the\nXML ATLAS General Detector Description AGDD, making it a widely used tool for\nXML developers. The graphics capabilities of this program were exploited in the\ncontext of the ATLAS 2002 Muon Testbeam where it was used as an online event\ndisplay, integrated in the online software framework and participating in the\ncommissioning and debug of the detector system.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0305057v1"
    },
    {
        "title": "GraXML - Modular Geometric Modeler",
        "authors": [
            "Julius Hrivnac"
        ],
        "category": "cs.GR",
        "published_year": "2003",
        "summary": "  Many entities managed by HEP Software Frameworks represent spatial\n(3-dimensional) real objects. Effective definition, manipulation and\nvisualization of such objects is an indispensable functionality.\n  GraXML is a modular Geometric Modeling toolkit capable of processing\ngeometric data of various kinds (detector geometry, event geometry) from\ndifferent sources and delivering them in ways suitable for further use.\nGeometric data are first modeled in one of the Generic Models. Those Models are\nthen used to populate powerful Geometric Model based on the Java3D technology.\nWhile Java3D has been originally created just to provide visualization of 3D\nobjects, its light weight and high functionality allow an effective reuse as a\ngeneral geometric component. This is possible also thanks to a large overlap\nbetween graphical and general geometric functionality and modular design of\nJava3D itself. Its graphical functionalities also allow a natural visualization\nof all manipulated elements.\n  All these techniques have been developed primarily (or only) for the Java\nenvironment. It is, however, possible to interface them transparently to\nFrameworks built in other languages, like for example C++.\n  The GraXML toolkit has been tested with data from several sources, as for\nexample ATLAS and ALICE detector description and ATLAS event data. Prototypes\nfor other sources, like Geometry Description Markup Language (GDML) exist too\nand interface to any other source is easy to add.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0306012v1"
    },
    {
        "title": "The FRED Event Display: an Extensible HepRep Client for GLAST",
        "authors": [
            "Marco Frailis",
            "Riccardo Giannitrapani"
        ],
        "category": "cs.GR",
        "published_year": "2003",
        "summary": "  A new graphics client prototype for the HepRep protocol is presented. Based\non modern toolkits and high level languages (C++ and Ruby), Fred is an\nexperiment to test applicability of scripting facilities to the high energy\nphysics event display domain. Its flexible structure, extensibility and the use\nof the HepRep protocol are key features for its use in the astroparticle\nexperiment GLAST.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0306031v1"
    },
    {
        "title": "The Use of HepRep in GLAST",
        "authors": [
            "J. Perl",
            "R. Giannitrapani",
            "M. Frailis"
        ],
        "category": "cs.GR",
        "published_year": "2003",
        "summary": "  HepRep is a generic, hierarchical format for description of graphics\nrepresentables that can be augmented by physics information and relational\nproperties. It was developed for high energy physics event display applications\nand is especially suited to client/server or component frameworks. The GLAST\nexperiment, an international effort led by NASA for a gamma-ray telescope to\nlaunch in 2006, chose HepRep to provide a flexible, extensible and maintainable\nframework for their event display without tying their users to any one graphics\napplication. To support HepRep in their GUADI infrastructure, GLAST developed a\nHepRep filler and builder architecture. The architecture hides the details of\nXML and CORBA in a set of base and helper classes allowing physics experts to\nfocus on what data they want to represent. GLAST has two GAUDI services:\nHepRepSvc, which registers HepRep fillers in a global registry and allows the\nHepRep to be exported to XML, and CorbaSvc, which allows the HepRep to be\npublished through a CORBA interface and which allows the client application to\nfeed commands back to GAUDI (such as start next event, or run some GAUDI\nalgorithm). GLAST's HepRep solution gives users a choice of client\napplications, WIRED (written in Java) or FRED (written in C++ and Ruby), and\nleaves them free to move to any future HepRep-compliant event display.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0306059v1"
    },
    {
        "title": "Application of interactive parallel visualization for commodity-based\n  clusters using visualization APIs",
        "authors": [
            "Stanimire Tomov",
            "Robert Bennett",
            "Michael McGuigan",
            "Arnold Peskin",
            "Gordon Smith",
            "John Spiletic"
        ],
        "category": "cs.GR",
        "published_year": "2003",
        "summary": "  We present an efficient and inexpensive to develop application for\ninteractive high-performance parallel visualization. We extend popular APIs\nsuch as Open Inventor and VTK to support commodity-based cluster visualization.\nOur implementation follows a standard master/slave concept: the general idea is\nto have a ``Master'' node, which will intercept a sequential graphical user\ninterface (GUI) and broadcast it to the ``Slave'' nodes. The interactions\nbetween the nodes are implemented using MPI. The parallel remote rendering uses\nChromium. This paper is mainly the report of our implementation experiences. We\npresent in detail the proposed model and key aspects of its implementation.\nAlso, we present performance measurements, we benchmark and quantitatively\ndemonstrate the dependence of the visualization speed on the data size and the\nnetwork bandwidth, and we identify the singularities and draw conclusions on\nChromium's sort-first rendering architecture. The most original part of this\nwork is the combined use of Open Inventor and Chromium.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0307065v1"
    },
    {
        "title": "Visualization of variations in human brain morphology using\n  differentiating reflection functions",
        "authors": [
            "Gibby Koldenhof"
        ],
        "category": "cs.GR",
        "published_year": "2003",
        "summary": "  Conventional visualization media such as MRI prints and computer screens are\ninherently two dimensional, making them incapable of displaying true 3D volume\ndata sets. By applying only transparency or intensity projection, and ignoring\nlight-matter interaction, results will likely fail to give optimal results.\nLittle research has been done on using reflectance functions to visually\nseparate the various segments of a MRI volume. We will explore if applying\nspecific reflectance functions to individual anatomical structures can help in\nbuilding an intuitive 2D image from a 3D dataset. We will test our hypothesis\nby visualizing a statistical analysis of the genetic influences on variations\nin human brain morphology because it inherently contains complex and many\ndifferent types of data making it a good candidate for our approach\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0311034v1"
    },
    {
        "title": "An Algorithm for Transforming Color Images into Tactile Graphics",
        "authors": [
            "Artur Rataj"
        ],
        "category": "cs.GR",
        "published_year": "2004",
        "summary": "  This paper presents an algorithm that transforms color visual images, like\nphotographs or paintings, into tactile graphics. In the algorithm, the edges of\nobjects are detected and colors of the objects are estimated. Then, the edges\nand the colors are encoded into lines and textures in the output tactile image.\nDesign of the method is substantiated by various qualities of haptic\nrecognizing of images. Also, means of presentation of the tactile images in\nprintouts are discussed. Example translated images are shown.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0404022v1"
    },
    {
        "title": "Interactive visualization of higher dimensional data in a multiview\n  environment",
        "authors": [
            "Stanimire Tomov",
            "Michael McGuigan"
        ],
        "category": "cs.GR",
        "published_year": "2004",
        "summary": "  We develop multiple view visualization of higher dimensional data. Our work\nwas chiefly motivated by the need to extract insight from four dimensional\nQuantum Chromodynamic (QCD) data. We develop visualization where multiple\nviews, generally views of 3D projections or slices of a higher dimensional\ndata, are tightly coupled not only by their specific order but also by a view\nsynchronizing interaction style, and an internally defined interaction\nlanguage. The tight coupling of the different views allows a fast and\nwell-coordinated exploration of the data. In particular, the visualization\nallowed us to easily make consistency checks of the 4D QCD data and to infer\nthe correctness of particle properties calculations. The software developed was\nalso successfully applied in material studies, in particular studies of\nmeteorite properties. Our implementation uses the VTK API. To handle a large\nnumber of views (slices/projections) and to still maintain good resolution, we\nuse IBM T221 display (3840 X 2400 pixels).\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0405048v1"
    },
    {
        "title": "Analytic Definition of Curves and Surfaces by Parabolic Blending",
        "authors": [
            "A. W. Overhauser"
        ],
        "category": "cs.GR",
        "published_year": "2005",
        "summary": "  A procedure for interpolating between specified points of a curve or surface\nis described. The method guarantees slope continuity at all junctions. A\nsurface panel divided into p x q contiguous patches is completely specified by\nthe coordinates of (p+1) x (q+1) points. Each individual patch, however,\ndepends parametrically on the coordinates of 16 points, allowing shape\nflexibility and global conformity.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0503054v1"
    },
    {
        "title": "Lattice Gas Cellular Automata for Computational Fluid Animation",
        "authors": [
            "Gilson A. Giraldi",
            "Adilson V. Xavier",
            "Antonio L. Apolinario Jr",
            "Paulo S. Rodrigues"
        ],
        "category": "cs.GR",
        "published_year": "2005",
        "summary": "  The past two decades showed a rapid growing of physically-based modeling of\nfluids for computer graphics applications. In this area, a common top down\napproach is to model the fluid dynamics by Navier-Stokes equations and apply a\nnumerical techniques such as Finite Differences or Finite Elements for the\nsimulation. In this paper we focus on fluid modeling through Lattice Gas\nCellular Automata (LGCA) for computer graphics applications. LGCA are discrete\nmodels based on point particles that move on a lattice, according to suitable\nand simple rules in order to mimic a fully molecular dynamics. By\nChapman-Enskog expansion, a known multiscale technique in this area, it can be\ndemonstrated that the Navier-Stokes model can be reproduced by the LGCA\ntechnique. Thus, with LGCA we get a fluid model that does not require solution\nof complicated equations. Therefore, we combine the advantage of the low\ncomputational cost of LGCA and its ability to mimic the realistic fluid\ndynamics to develop a new animating framework for computer graphics\napplications. In this work, we discuss the theoretical elements of our proposal\nand show experimental results.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0507012v1"
    },
    {
        "title": "Methods for Analytical Understanding of Agent-Based Modeling of Complex\n  Systems",
        "authors": [
            "Gilson A. Giraldi",
            "Luis C. da Costa",
            "Adilson V. Xavier",
            "Paulo S. Rodrigues"
        ],
        "category": "cs.GR",
        "published_year": "2005",
        "summary": "  Von Neuman's work on universal machines and the hardware development have\nallowed the simulation of dynamical systems through a large set of interacting\nagents. This is a bottom-up approach which tries to derive global properties of\na complex system through local interaction rules and agent behaviour.\nTraditionally, such systems are modeled and simulated through top-down methods\nbased on differential equations. Agent-Based Modeling has the advantage of\nsimplicity and low computational cost. However, unlike differential equations,\nthere is no standard way to express agent behaviour. Besides, it is not clear\nhow to analytically predict the results obtained by the simulation. In this\npaper we survey some of these methods. For expressing agent behaviour formal\nmethods, like Stochastic Process Algebras have been used. Such approach is\nuseful if the global properties of interest can be expressed as a function of\nstochastic time series. However, if space variables must be considered, we\nshall change the focus. In this case, multiscale techniques, based on\nChapman-Enskog expansion, was used to establish the connection between the\nmicroscopic dynamics and the macroscopic observables. Also, we use data mining\ntechniques,like Principal Component Analysis (PCA), to study agent systems like\nCellular Automata. With the help of these tools we will discuss a simple\nsociety model, a Lattice Gas Automaton for fluid modeling, and knowledge\ndiscovery in CA databases. Besides, we show the capabilities of the NetLogo, a\nsoftware for agent simulation of complex system and show our experience about.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0508002v1"
    },
    {
        "title": "MathPSfrag: Creating Publication-Quality Labels in Mathematica Plots",
        "authors": [
            "J. Grosse"
        ],
        "category": "cs.GR",
        "published_year": "2005",
        "summary": "  This article introduces a Mathematica package providing a graphics export\nfunction that automatically replaces Mathematica expressions in a graphic by\nthe corresponding LaTeX constructs and positions them correctly. It thus\nfacilitates the creation of publication-quality Enscapulated PostScript (EPS)\ngraphics.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0510087v1"
    },
    {
        "title": "Graphics Turing Test",
        "authors": [
            "Michael McGuigan"
        ],
        "category": "cs.GR",
        "published_year": "2006",
        "summary": "  We define a Graphics Turing Test to measure graphics performance in a similar\nmanner to the definition of the traditional Turing Test. To pass the test one\nneeds to reach a computational scale, the Graphics Turing Scale, for which\nComputer Generated Imagery becomes comparatively indistinguishable from real\nimages while also being interactive. We derive an estimate for this\ncomputational scale which, although large, is within reach of todays\nsupercomputers. We consider advantages and disadvantages of various computer\nsystems designed to pass the Graphics Turing Test. Finally we discuss\ncommercial applications from the creation of such a system, in particular\nInteractive Cinema.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0603132v1"
    },
    {
        "title": "Interactive Hatching and Stippling by Example",
        "authors": [
            "Pascal Barla",
            "Simon Breslav",
            "Lee Markosian",
            "Joëlle Thollot"
        ],
        "category": "cs.GR",
        "published_year": "2006",
        "summary": "  We describe a system that lets a designer interactively draw patterns of\nstrokes in the picture plane, then guide the synthesis of similar patterns over\nnew picture regions. Synthesis is based on an initial user-assisted analysis\nphase in which the system recognizes distinct types of strokes (hatching and\nstippling) and organizes them according to perceptual grouping criteria. The\nsynthesized strokes are produced by combining properties (eg. length,\norientation, parallelism, proximity) of the stroke groups extracted from the\ninput examples. We illustrate our technique with a drawing application that\nallows the control of attributes and scale-dependent reproduction of the\nsynthesized patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0607050v2"
    },
    {
        "title": "Non-photorealistic image rendering with a labyrinthine tiling",
        "authors": [
            "A. Sparavigna",
            "B. Montrucchio"
        ],
        "category": "cs.GR",
        "published_year": "2006",
        "summary": "  The paper describes a new image processing for a non-photorealistic\nrendering. The algorithm is based on a random generation of gray tones and\ncompeting statistical requirements. The gray tone value of each pixel in the\nstarting image is replaced selecting among randomly generated tone values,\naccording to the statistics of nearest-neighbor and next-nearest-neighbor\npixels. Two competing conditions for replacing the tone values - one position\non the local mean value the other on the local variance - produce a peculiar\npattern on the image. This pattern has a labyrinthine tiling aspect. For\ncertain subjects, the pattern enhances the look of the image.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0609084v1"
    },
    {
        "title": "Vector field visualization with streamlines",
        "authors": [
            "A. Sparavigna",
            "B. Montrucchio"
        ],
        "category": "cs.GR",
        "published_year": "2006",
        "summary": "  We have recently developed an algorithm for vector field visualization with\noriented streamlines, able to depict the flow directions everywhere in a dense\nvector field and the sense of the local orientations. The algorithm has useful\napplications in the visualization of the director field in nematic liquid\ncrystals. Here we propose an improvement of the algorithm able to enhance the\nvisualization of the local magnitude of the field. This new approach of the\nalgorithm is compared with the same procedure applied to the Line Integral\nConvolution (LIC) visualization.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0610088v1"
    },
    {
        "title": "Shape preservation behavior of spline curves",
        "authors": [
            "Ravi Shankar Gautam"
        ],
        "category": "cs.GR",
        "published_year": "2007",
        "summary": "  Shape preservation behavior of a spline consists of criterial conditions for\npreserving convexity, inflection, collinearity, torsion and coplanarity shapes\nof data polgonal arc. We present our results which acts as an improvement in\nthe definitions of and provide geometrical insight into each of the above shape\npreservation criteria. We also investigate the effect of various results from\nthe literature on various shape preservation criteria. These results have not\nbeen earlier refered in the context of shape preservation behaviour of splines.\nWe point out that each curve segment need to satisfy more than one shape\npreservation criteria. We investigate the conflict between different shape\npreservation criteria 1)on each curve segment and 2)of adjacent curve segments.\nWe derive simplified formula for shape preservation criteria for cubic curve\nsegments. We study the shape preservation behavior of cubic Catmull-Rom splines\nand see that, though being very simple spline curve, it indeed satisfy all the\nshape preservation criteria.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0702026v1"
    },
    {
        "title": "Virtual Environments for Training: From Individual Learning to\n  Collaboration with Humanoids",
        "authors": [
            "Stéphanie Gerbaud",
            "Nicolas Mollet",
            "Bruno Arnaldi"
        ],
        "category": "cs.GR",
        "published_year": "2007",
        "summary": "  The next generation of virtual environments for training is oriented towards\ncollaborative aspects. Therefore, we have decided to enhance our platform for\nvirtual training environments, adding collaboration opportunities and\nintegrating humanoids. In this paper we put forward a model of humanoid that\nsuits both virtual humans and representations of real users, according to\ncollaborative training activities. We suggest adaptations to the scenario model\nof our platform making it possible to write collaborative procedures. We\nintroduce a mechanism of action selection made up of a global repartition and\nan individual choice. These models are currently being integrated and validated\nin GVT, a virtual training tool for maintenance of military equipments,\ndeveloped in collaboration with the French company NEXTER-Group.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.0712v1"
    },
    {
        "title": "Efficient Binary and Run Length Morphology and its Application to\n  Document Image Processing",
        "authors": [
            "Thomas M. Breuel"
        ],
        "category": "cs.GR",
        "published_year": "2007",
        "summary": "  This paper describes the implementation and evaluation of an open source\nlibrary for mathematical morphology based on packed binary and run-length\ncompressed images for document imaging applications. Abstractions and patterns\nuseful in the implementation of the interval operations are described. A number\nof benchmarks and comparisons to bit-blit based implementations on standard\ndocument images are provided.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.0121v1"
    },
    {
        "title": "Realistic Haptic Rendering of Interacting Deformable Objects in Virtual\n  Environments",
        "authors": [
            "Christian Duriez",
            "Frédéric Dubois",
            "Abderrahmane Kheddar",
            "Claude Andriot"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  A new computer haptics algorithm to be used in general interactive\nmanipulations of deformable virtual objects is presented. In multimodal\ninteractive simulations, haptic feedback computation often comes from contact\nforces. Subsequently, the fidelity of haptic rendering depends significantly on\ncontact space modeling. Contact and friction laws between deformable models are\noften simplified in up to date methods. They do not allow a \"realistic\"\nrendering of the subtleties of contact space physical phenomena (such as slip\nand stick effects due to friction or mechanical coupling between contacts). In\nthis paper, we use Signorini's contact law and Coulomb's friction law as a\ncomputer haptics basis. Real-time performance is made possible thanks to a\nlinearization of the behavior in the contact space, formulated as the so-called\nDelassus operator, and iteratively solved by a Gauss-Seidel type algorithm.\nDynamic deformation uses corotational global formulation to obtain the Delassus\noperator in which the mass and stiffness ratio are dissociated from the\nsimulation time step. This last point is crucial to keep stable haptic\nfeedback. This global approach has been packaged, implemented, and tested.\nStable and realistic 6D haptic feedback is demonstrated through a clipping task\nexperiment.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.0561v1"
    },
    {
        "title": "Quasi-Mandelbrot sets for perturbed complex analytic maps: visual\n  patterns",
        "authors": [
            "A. V. Toporensky"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  We consider perturbations of the complex quadratic map $ z \\to z^2 +c$ and\ncorresponding changes in their quasi-Mandelbrot sets. Depending on particular\nperturbation, visual forms of quasi-Mandelbrot set changes either sharply (when\nthe perturbation reaches some critical value) or continuously. In the latter\ncase we have a smooth transition from the classical form of the set to some\nforms, constructed from mostly linear structures, as it is typical for\ntwo-dimensional real number dynamics. Two examples of continuous evolution of\nthe quasi-Mandelbrot set are described.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.1667v1"
    },
    {
        "title": "Polyomino-Based Digital Halftoning",
        "authors": [
            "David Vanderhaeghe",
            "Victor Ostromoukhov"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  In this work, we present a new method for generating a threshold structure.\nThis kind of structure can be advantageously used in various halftoning\nalgorithms such as clustered-dot or dispersed-dot dithering, error diffusion\nwith threshold modulation, etc. The proposed method is based on rectifiable\npolyominoes -- a non-periodic hierarchical structure, which tiles the Euclidean\nplane with no gaps. Each polyomino contains a fixed number of discrete\nthreshold values. Thanks to its inherent non-periodic nature combined with\noff-line optimization of threshold values, our polyomino-based threshold\nstructure shows blue-noise spectral properties. The halftone images produced\nwith this threshold structure have high visual quality. Although the proposed\nmethod is general, and can be applied on any polyomino tiling, we consider one\nparticular case: tiling with G-hexominoes. We compare our polyomino-based\nthreshold structure with the best known state-of-the-art methods for generation\nthreshold matrices, and conclude considerable improvement achieved with our\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.1647v1"
    },
    {
        "title": "Dynamic Deformation of Uniform Elastic Two-Layer Objects",
        "authors": [
            "Miao Song"
        ],
        "category": "cs.GR",
        "published_year": "2009",
        "summary": "  This thesis presents a two-layer uniform facet elastic object for real-time\nsimulation based on physics modeling method. It describes the elastic object\nprocedural modeling algorithm with particle system from the simplest\none-dimensional object, to more complex two-dimensional and three-dimensional\nobjects.\n  The double-layered elastic object consists of inner and outer elastic mass\nspring surfaces and compressible internal pressure. The density of the inner\nlayer can be set different from the density of the outer layer; the motion of\nthe inner layer can be opposite to the motion of the outer layer. These special\nfeatures, which cannot be achieved by a single layered object, result in\nimproved imitation of a soft body, such as tissue's liquidity non-uniform\ndeformation. The construction of the double-layered elastic object is closer to\nthe real tissue's physical structure.\n  The inertial behavior of the elastic object is well illustrated in\nenvironments with gravity and collisions with walls, ceiling, and floor. The\ncollision detection is defined by elastic collision penalty method and the\nmotion of the object is guided by the Ordinary Differential Equation\ncomputation.\n  Users can interact with the modeled objects, deform them, and observe the\nresponse to their action in real time.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.4364v2"
    },
    {
        "title": "Yet Another Pacman 3D Adventures",
        "authors": [
            "Serguei A. Mokhov",
            "Yingying She"
        ],
        "category": "cs.GR",
        "published_year": "2009",
        "summary": "  This game is meant to be extension of the overly-beaten pacman-style game\n(code-named \"Yet Another Pacman 3D Adventures\", or YAP3DAD) from the proposed\nideas and other projects with advance visual and computer graphics features,\nincluding a-game-in-a-game approach. The project is an open-source project\npublished on SourceForge.net for possible future development and extension.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.4854v1"
    },
    {
        "title": "Digital Image Watermarking for Arbitrarily Shaped Objects Based On\n  SA-DWT",
        "authors": [
            "A. Essaouabi",
            "E. Ibnelhaj",
            "F. Fegragui"
        ],
        "category": "cs.GR",
        "published_year": "2009",
        "summary": "  Many image watermarking schemes have been proposed in recent years, but they\nusually involve embedding a watermark to the entire image without considering\nonly a particular object in the image, which the image owner may be interested\nin. This paper proposes a watermarking scheme that can embed a watermark to an\narbitrarily shaped object in an image. Before embedding, the image owner\nspecifies an object of arbitrary shape that is of a concern to him. Then the\nobject is transformed into the wavelet domain using in place lifting shape\nadaptive DWT(SADWT) and a watermark is embedded by modifying the wavelet\ncoefficients. In order to make the watermark robust and transparent, the\nwatermark is embedded in the average of wavelet blocks using the visual model\nbased on the human visual system. Wavelet coefficients n least significant bits\n(LSBs) are adjusted in concert with the average. Simulation results shows that\nthe proposed watermarking scheme is perceptually invisible and robust against\nmany attacks such as lossy compression (e.g.JPEG, JPEG2000), scaling, adding\nnoise, filtering, etc.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.0902v1"
    },
    {
        "title": "Secure Watermarking Scheme for Color Image Using Intensity of Pixel and\n  LSB Substitution",
        "authors": [
            "Nagaraj V. Dharwadkar",
            "B. B. Amberker"
        ],
        "category": "cs.GR",
        "published_year": "2009",
        "summary": "  In this paper a novel spatial domain LSB based watermarking scheme for color\nImages is proposed. The proposed scheme is of type blind and invisible\nwatermarking. Our scheme introduces the concept of storing variable number of\nbits in each pixel based on the actual color value of pixel. Equal or higher\nthe color value of channels with respect to intensity of pixel stores higher\nnumber of watermark bits. The Red, Green and Blue channel of the color image\nhas been used for watermark embedding. The watermark is embedded into selected\nchannels of pixel. The proposed method supports high watermark embedding\ncapacity, which is equivalent to the size of cover image. The security of\nwatermark is preserved by permuting the watermark bits using secret key. The\nproposed scheme is found robust to various image processing operations such as\nimage compression, blurring, salt and pepper noise, filtering and cropping.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.3923v1"
    },
    {
        "title": "Resolution scalability improvement for JPEG2000 standard color image",
        "authors": [
            "U. Vijayasankar",
            "S. Prasadh.",
            "A. Arul Lawrence Selvakumar"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  Removed by arXiv administration. This article was plagiarised from\nhttp://www.dmi.unict.it/~battiato/download/NSIP_2003_VQ.pdf and other\nlocations.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.3481v2"
    },
    {
        "title": "Spatial Domain Watermarking Scheme for Colored Images Based on\n  Log-average Luminance",
        "authors": [
            "Jamal A. Hussein"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  In this paper a new watermarking scheme is presented based on log-average\nluminance. A colored-image is divided into blocks after converting the RGB\ncolored image to YCbCr color space. A monochrome image of 1024 bytes is used as\nthe watermark. To embed the watermark, 16 blocks of size 8X8 are selected and\nused to embed the watermark image into the original image. The selected blocks\nare chosen spirally (beginning form the center of the image) among the blocks\nthat have log-average luminance higher than or equal the log-average luminance\nof the entire image. Each byte of the monochrome watermark is added by updating\na luminance value of a pixel of the image. If the byte of the watermark image\nrepresented white color (255) a value <alpha> is added to the image pixel\nluminance value, if it is black (0) the <alpha> is subtracted from the\nluminance value. To extract the watermark, the selected blocks are chosen as\nthe above, if the difference between the luminance value of the watermarked\nimage pixel and the original image pixel is greater than 0, the watermark pixel\nis supposed to be white, otherwise it supposed to be black. Experimental\nresults show that the proposed scheme is efficient against changing the\nwatermarked image to grayscale, image cropping, and JPEG compression.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.3496v1"
    },
    {
        "title": "Text/Graphics Separation and Skew Correction of Text Regions of Business\n  Card Images for Mobile Devices",
        "authors": [
            "Ayatullah Faruk Mollah",
            "Subhadip Basu",
            "Mita Nasipuri"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  Separation of the text regions from background texture and graphics is an\nimportant step of any optical character recognition system for the images\ncontaining both texts and graphics. In this paper, we have presented a novel\ntext/graphics separation technique and a method for skew correction of text\nregions extracted from business card images captured with a cell-phone camera.\nAt first, the background is eliminated at a coarse level based on intensity\nvariance. This makes the foreground components distinct from each other. Then\nthe non-text components are removed using various characteristic features of\ntext and graphics. Finally, the text regions are skew corrected for further\nprocessing. Experimenting with business card images of various resolutions, we\nhave found an optimum performance of 98.25% (recall) with 0.75 MP images, that\ntakes 0.17 seconds processing time and 1.1 MB peak memory on a moderately\npowerful computer (DualCore 1.73 GHz Processor, 1 GB RAM, 1 MB L2 Cache). The\ndeveloped technique is computationally efficient and consumes low memory so as\nto be applicable on mobile devices.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.4006v1"
    },
    {
        "title": "Macro and micro view on steady states in state space",
        "authors": [
            "Branislav Sobota",
            "Milan Guzan"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  This paper describes visualization of chaotic attractor and elements of the\nsingularities in 3D space. 3D view of these effects enables to create a\ndemonstrative projection about relations of chaos generated by physical\ncircuit, the Chua's circuit. Via macro views on chaotic attractor is obtained\nnot only visual space illustration of representative point motion in state\nspace, but also its relation to planes of singularity elements. Our created\nprogram enables view on chaotic attractor both in 2D and 3D space together with\nplane objects visualization -- elements of singularities.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.1401v1"
    },
    {
        "title": "A Very Simple Approach for 3-D to 2-D Mapping",
        "authors": [
            "Sandipan Dey",
            "Ajith Abraham",
            "Sugata Sanyal"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  Many times we need to plot 3-D functions e.g., in many scientificc\nexperiments. To plot this 3-D functions on 2-D screen it requires some kind of\nmapping. Though OpenGL, DirectX etc 3-D rendering libraries have made this job\nvery simple, still these libraries come with many complex pre- operations that\nare simply not intended, also to integrate these libraries with any kind of\nsystem is often a tough trial. This article presents a very simple method of\nmapping from 3D to 2D, that is free from any complex pre-operation, also it\nwill work with any graphics system where we have some primitive 2-D graphics\nfunction. Also we discuss the inverse transform and how to do basic computer\ngraphics transformations using our coordinate mapping system.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.4036v1"
    },
    {
        "title": "Text/Graphics Separation for Business Card Images for Mobile Devices",
        "authors": [
            "Ayatullah Faruk Mollah",
            "Subhadip Basu",
            "Mita Nasipuri",
            "Dipak Kumar Basu"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  Separation of the text regions from background texture and graphics is an\nimportant step of any optical character recognition sytem for the images\ncontaing both texts and graphics. In this paper, we have presented a novel\ntext/graphics separation technique for business card images captured with a\ncell-phone camera. At first, the background is eliminated at a coarse level\nbased on intensity variance. This makes the foreground components distinct from\neach other. Then the non-text components are removed using various\ncharacteristic features of text and graphics. Finally, the text regions are\nskew corrected and binarized for further processing. Experimenting with\nbusiness card images of various resolutions, we have found an optimum\nperformance of 98.54% with 0.75 MP images, that takes 0.17 seconds processing\ntime and 1.1 MB peak memory on a moderately powerful computer (DualCore 1.73\nGHz Processor, 1 GB RAM, 1 MB L2 Cache). The developed technique is\ncomputationally efficient and consumes low memory so as to be applicable on\nmobile devices.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.0766v1"
    },
    {
        "title": "Finding and Classifying Critical Points of 2D Vector Fields: A\n  Cell-Oriented Approach Using Group Theory",
        "authors": [
            "Felix Effenberger",
            "Daniel Weiskopf"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  We present a novel approach to finding critical points in cell-wise\nbarycentrically or bilinearly interpolated vector fields on surfaces. The\nPoincar\\e index of the critical points is determined by investigating the\nqualitative behavior of 0-level sets of the interpolants of the vector field\ncomponents in parameter space using precomputed combinatorial results, thus\navoiding the computation of the Jacobian of the vector field at the critical\npoints in order to determine its index. The locations of the critical points\nwithin a cell are determined analytically to achieve accurate results. This\napproach leads to a correct treatment of cases with two first-order critical\npoints or one second-order critical point of bilinearly interpolated vector\nfields within one cell, which would be missed by examining the linearized field\nonly. We show that for the considered interpolation schemes determining the\nindex of a critical point can be seen as a coloring problem of cell edges. A\ncomplete classification of all possible colorings in terms of the types and\nnumber of critical points yielded by each coloring is given using computational\ngroup theory. We present an efficient algorithm that makes use of these\nprecomputed classifications in order to find and classify critical points in a\ncell-by-cell fashion. Issues of numerical stability, construction of the\ntopological skeleton, topological simplification, and the statistics of the\ndifferent types of critical points are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4485v1"
    },
    {
        "title": "Virtual Texturing",
        "authors": [
            "Andreas Neu"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  In this thesis a rendering system and an accompanying tool chain for Virtual\nTexturing is presented. Our tools allow to automatically retexture existing\ngeometry in order to apply unique texturing on each face. Furthermore we\ninvestigate several techniques that try to minimize visual artifacts in the\ncase that only a small amount of pages can be streamed per frame. We analyze\nthe influence of different heuristics that are responsible for the page\nselection. Alongside these results we present a measurement method to allow the\ncomparison of our heuristics.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.3163v1"
    },
    {
        "title": "What's wrong with Phong - Designers' appraisal of shading in CAD-systems",
        "authors": [
            "Jörg M. Hahn"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  The Phong illumination model is still widely used in realtime 3D\nvisualization systems. The aim of this article is to document problems with the\nPhong illumination model that are encountered by an important professional user\ngroup, namely digital designers. This leads to a visual evaluation of Phong\nillumination, which at least in this condensed form seems still to be missing\nin the literature. It is hoped that by explicating these flaws, awareness about\nthe limitations and interdependencies of the model will increase, both among\nfellow users, and among researchers and developers.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.2204v1"
    },
    {
        "title": "Geoglyphs of Titicaca as an ancient example of graphic design",
        "authors": [
            "Amelia Carolina Sparavigna"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  The paper proposes an ancient landscape design as an example of graphic\ndesign for an age and place where no written documents existed. It is created\nby a network of earthworks, which constitute the remains of an extensive\nancient agricultural system. It can be seen by means of the Google satellite\nimagery on the Peruvian region near the Titicaca Lake, as a texture\nsuperimposed to the background landform. In this texture, many drawings\n(geoglyphs) can be observed.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.4602v1"
    },
    {
        "title": "Across Browsers SVG Implementation",
        "authors": [
            "Liang Wang",
            "Nies Huijsmans",
            "Michael S. Lew",
            "Dan Tsymbala"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  In this work SVG will be translated into VML or HTML by using Javascript\nbased on Backbase Client Framework. The target of this project is to implement\nSVG to be viewed in Internet Explorer without any plug-in and work together\nwith other Backbase Client Framework languages. The result of this project will\nbe added as an extension to the current Backbase Client Framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.0243v1"
    },
    {
        "title": "High Speed and Area Efficient 2D DWT Processor based Image Compression\"\n  Signal & Image Processing",
        "authors": [
            "Sugreev Kaur",
            "Rajesh Mehra"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  This paper presents a high speed and area efficient DWT processor based\ndesign for Image Compression applications. In this proposed design, pipelined\npartially serial architecture has been used to enhance the speed along with\noptimal utilization and resources available on target FPGA. The proposed model\nhas been designed and simulated using Simulink and System Generator blocks,\nsynthesized with Xilinx Synthesis tool (XST) and implemented on Spartan 2 and 3\nbased XC2S100-5tq144 and XC3S500E-4fg320 target device. The results show that\nproposed design can operate at maximum frequency 231 MHz in case of Spartan 3\nby consuming power of 117mW at 28 degree/c junction temperature. The result\ncomparison has shown an improvement of 15% in speed.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.0262v1"
    },
    {
        "title": "Improving the Performance of K-Means for Color Quantization",
        "authors": [
            "M. Emre Celebi"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  Color quantization is an important operation with many applications in\ngraphics and image processing. Most quantization methods are essentially based\non data clustering algorithms. However, despite its popularity as a general\npurpose clustering algorithm, k-means has not received much respect in the\ncolor quantization literature because of its high computational requirements\nand sensitivity to initialization. In this paper, we investigate the\nperformance of k-means as a color quantizer. We implement fast and exact\nvariants of k-means with several initialization schemes and then compare the\nresulting quantizers to some of the most popular quantizers in the literature.\nExperiments on a diverse set of images demonstrate that an efficient\nimplementation of k-means with an appropriate initialization strategy can in\nfact serve as a very effective color quantizer.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.0395v1"
    },
    {
        "title": "Ray-Based Reflectance Model for Diffraction",
        "authors": [
            "Tom Cuypers",
            "Se Baek Oh",
            "Tom Haber",
            "Philippe Bekaert",
            "Ramesh Raskar"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  We present a novel method of simulating wave effects in graphics using\nray--based renderers with a new function: the Wave BSDF (Bidirectional\nScattering Distribution Function). Reflections from neighboring surface patches\nrepresented by local BSDFs are mutually independent. However, in many surfaces\nwith wavelength-scale microstructures, interference and diffraction requires a\njoint analysis of reflected wavefronts from neighboring patches. We demonstrate\na simple method to compute the BSDF for the entire microstructure, which can be\nused independently for each patch. This allows us to use traditional ray--based\nrendering pipelines to synthesize wave effects of light and sound. We exploit\nthe Wigner Distribution Function (WDF) to create transmissive, reflective, and\nemissive BSDFs for various diffraction phenomena in a physically accurate way.\nIn contrast to previous methods for computing interference, we circumvent the\nneed to explicitly keep track of the phase of the wave by using BSDFs that\ninclude positive as well as negative coefficients. We describe and compare the\ntheory in relation to well understood concepts in rendering and demonstrate a\nstraightforward implementation. In conjunction with standard raytracers, such\nas PBRT, we demonstrate wave effects for a range of scenarios such as\nmulti--bounce diffraction materials, holograms and reflection of high frequency\nsurfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.5490v1"
    },
    {
        "title": "Glioblastoma Multiforme Segmentation in MRI Data with a Balloon\n  Inflation Approach",
        "authors": [
            "Dženan Zukić",
            "Jan Egger",
            "Miriam H. A. Bauer",
            "Daniela Kuhnt",
            "Barbara Carl",
            "Bernd Freisleben",
            "Andreas Kolb",
            "Christopher Nimsky"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  Gliomas are the most common primary brain tumors, evolving from the cerebral\nsupportive cells. For clinical follow-up, the evaluation of the preoperative\ntumor volume is essential. Volumetric assessment of tumor volume with manual\nsegmentation of its outlines is a time-consuming process that can be overcome\nwith the help of computer-assisted segmentation methods. In this paper, a\nsemi-automatic approach for World Health Organization (WHO) grade IV glioma\nsegmentation is introduced that uses balloon inflation forces, and relies on\nthe detection of high-intensity tumor boundaries that are coupled by using\ncontrast agent gadolinium. The presented method is evaluated on 27 magnetic\nresonance imaging (MRI) data sets and the ground truth data of the tumor\nboundaries - for evaluation of the results - are manually extracted by\nneurosurgeons.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.0634v1"
    },
    {
        "title": "Mathematics of Human Motion: from Animation towards Simulation (A View\n  form the Outside)",
        "authors": [
            "A. I. Zhmakin"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  Simulation of human motion is the subject of study in a number of\ndisciplines: Biomechanics, Robotics, Computer Animation, Control Theory,\nNeurophysiology, Medicine, Ergonomics. Since the author has never visited any\nof these fields, this review is indeed a passer-by's impression. On the other\nhand, he happens to be a human (who occasionally is moving) and, as everybody\nelse, rates himself an expert in Applied Common Sense. Thus the author hopes\nthat this view from the {\\em outside} will be of some interest not only for the\nstrangers like himself, but for those who are {\\em inside} as well.\n  Two flaws of the text that follows are inevitable. First, some essential\nissues that are too familar to the specialists to discuss them may be missing.\nSecond, the author probably failed to provide the uniform \"level-of-detail\" for\nthis wide range of topics.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.4992v1"
    },
    {
        "title": "Linear-Time Poisson-Disk Patterns",
        "authors": [
            "Thouis R. Jones",
            "David R. Karger"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  We present an algorithm for generating Poisson-disc patterns taking O(N) time\nto generate $N$ points. The method is based on a grid of regions which can\ncontain no more than one point in the final pattern, and uses an explicit model\nof point arrival times under a uniform Poisson process.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.3013v1"
    },
    {
        "title": "A self-rendering digital image encoding",
        "authors": [
            "Daniel L. Ruderman"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  Without careful long-term preservation digital data may be lost to a number\nof factors, including physical media decay, lack of suitable decoding\nequipment, and the absence of software. When raw data can be read but lack\nsuitable annotations as to provenance, the ability to interpret them is more\nstraightforward if they can be assessed through simple visual techniques. In\nthis regard digital images are a special case since their data have a natural\nrepresentation on two-dimensional media surfaces. This paper presents a novel\nbinary image pixel encoding that produces an approximate analog rendering of\nencoded images when the image bits are arranged spatially in an appropriate\nmanner. This simultaneous digital and analog representation acts to inseparably\nannotate bits as image data, which may contribute to the longevity of\nso-encoded images.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.6032v1"
    },
    {
        "title": "Efficient and Effective Volume Visualization with Enhanced Isosurface\n  Rendering",
        "authors": [
            "Fei Yang",
            "Yong Cao",
            "Jie Tian"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Compared with full volume rendering, isosurface rendering has several well\nrecognized advantages in efficiency and accuracy. However, standard isosurface\nrendering has some limitations in effectiveness. First, it uses a monotone\ncolored approach and can only visualize the geometry features of an isosurface.\nThe lack of the capability to illustrate the material property and the internal\nstructures behind an isosurface has been a big limitation of this method in\napplications. Another limitation of isosurface rendering is the difficulty to\nreveal physically meaningful structures, which are hidden in one or multiple\nisosurfaces. As such, the application requirements of extract and recombine\nstructures of interest can not be implemented effectively with isosurface\nrendering. In this work, we develop an enhanced isosurface rendering technique\nto improve the effectiveness while maintaining the performance efficiency of\nthe standard isosurface rendering. First, an isosurface color enhancement\nmethod is proposed to illustrate the neighborhood density and to reveal some of\nthe internal structures. Second, we extend the structure extraction capability\nof isosurface rendering by enabling explicit scene exploration within a\n3D-view, using surface peeling, voxel-selecting, isosurface segmentation, and\nmulti-surface-structure visualization. Our experiments show that the color\nenhancement not only improves the visual fidelity of the rendering, but also\nreveals the internal structures without significant increase of the\ncomputational cost. Explicit scene exploration is also demonstrated as a\npowerful tool in some application scenarios, such as displaying multiple\nabdominal organs.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.5360v1"
    },
    {
        "title": "Efficient computational noise in GLSL",
        "authors": [
            "Ian McEwan",
            "David Sheets",
            "Stefan Gustavson",
            "Mark Richardson"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  We present GLSL implementations of Perlin noise and Perlin simplex noise that\nrun fast enough for practical consideration on current generation GPU hardware.\nThe key benefits are that the functions are purely computational, i.e. they use\nneither textures nor lookup tables, and that they are implemented in GLSL\nversion 1.20, which means they are compatible with all current GLSL-capable\nplatforms, including OpenGL ES 2.0 and WebGL 1.0. Their performance is on par\nwith previously presented GPU implementations of noise, they are very\nconvenient to use, and they scale well with increasing parallelism in present\nand upcoming GPU architectures.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.1461v1"
    },
    {
        "title": "Numerical Analysis of Diagonal-Preserving, Ripple-Minimizing and\n  Low-Pass Image Resampling Methods",
        "authors": [
            "Chantal Racette"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Image resampling is a necessary component of any operation that changes the\nsize of an image or its geometry.\n  Methods tuned for natural image upsampling (roughly speaking, image\nenlargement) are analyzed and developed with a focus on their ability to\npreserve diagonal features and suppress overshoots. Monotone, locally bounded\nand almost monotone \"direct\" interpolation and filtering methods, as well as\nface split and vertex split surface subdivision methods, alone or in\ncombination, are studied. Key properties are established by way of proofs and\ncounterexamples as well as numerical experiments involving 1D curve and 2D\ndiagonal data resampling.\n  In addition, the Remez minimax method for the computation of low-cost\npolynomial approximations of low-pass filter kernels tuned for natural image\ndownsampling (roughly speaking, image reduction) is refactored for relative\nerror minimization in the presence of roots in the interior of the interval of\napproximation and so that even and odd functions are approximated with like\npolynomials. The accuracy and frequency response of the approximations are\ntabulated and plotted against the original, establishing their rapid\nconvergence.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.4734v1"
    },
    {
        "title": "Geodesics in Heat",
        "authors": [
            "Keenan Crane",
            "Clarisse Weischedel",
            "Max Wardetzky"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  We introduce the heat method for computing the shortest geodesic distance to\na specified subset (e.g., point or curve) of a given domain. The heat method is\nrobust, efficient, and simple to implement since it is based on solving a pair\nof standard linear elliptic problems. The method represents a significant\nbreakthrough in the practical computation of distance on a wide variety of\ngeometric domains, since the resulting linear systems can be prefactored once\nand subsequently solved in near-linear time. In practice, distance can be\nupdated via the heat method an order of magnitude faster than with\nstate-of-the-art methods while maintaining a comparable level of accuracy. We\nprovide numerical evidence that the method converges to the exact geodesic\ndistance in the limit of refinement; we also explore smoothed approximations of\ndistance suitable for applications where more regularity is required.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.6216v2"
    },
    {
        "title": "Visualizing 2D Flows with Animated Arrow Plots",
        "authors": [
            "Bruno Jobard",
            "Nicolas Ray",
            "Dmitry Sokolov"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Flow fields are often represented by a set of static arrows to illustrate\nscientific vulgarization, documentary film, meteorology, etc. This simple\nschematic representation lets an observer intuitively interpret the main\nproperties of a flow: its orientation and velocity magnitude. We propose to\ngenerate dynamic versions of such representations for 2D unsteady flow fields.\nOur algorithm smoothly animates arrows along the flow while controlling their\ndensity in the domain over time. Several strategies have been combined to lower\nthe unavoidable popping artifacts arising when arrows appear and disappear and\nto achieve visually pleasing animations. Disturbing arrow rotations in low\nvelocity regions are also handled by continuously morphing arrow glyphs to\nsemi-transparent discs. To substantiate our method, we provide results for\nsynthetic and real velocity field datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.5204v1"
    },
    {
        "title": "Fast View Frustum Culling of Spatial Object by Analytical Bounding Bin",
        "authors": [
            "Munsu Ju",
            "Yunchol Jong"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  It is a common sense to apply the VFC (view frustum culling) of spatial\nobject to bounding cube of the object in 3D graphics. The accuracy of VFC can\nnot be guaranteed even in cube rotated three-dimensionally. In this paper is\nproposed a method which is able to carry out more precise and fast VFC of any\nspatial object in the image domain of cube by an analytic mapping, and is\ndemonstrated the effect of the method for terrain block on global surface.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.3899v1"
    },
    {
        "title": "An algorithm for improving the quality of compacted JPEG image by\n  minimizes the blocking artifacts",
        "authors": [
            "Sukhpal Singh"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  The Block Transform Coded, JPEG- a lossy image compression format has been\nused to keep storage and bandwidth requirements of digital image at practical\nlevels. However, JPEG compression schemes may exhibit unwanted image artifacts\nto appear - such as the 'blocky' artifact found in smooth/monotone areas of an\nimage, caused by the coarse quantization of DCT coefficients. A number of image\nfiltering approaches have been analyzed in literature incorporating\nvalue-averaging filters in order to smooth out the discontinuities that appear\nacross DCT block boundaries. Although some of these approaches are able to\ndecrease the severity of these unwanted artifacts to some extent, other\napproaches have certain limitations that cause excessive blurring to\nhigh-contrast edges in the image. The image deblocking algorithm presented in\nthis paper aims to filter the blocked boundaries. This is accomplished by\nemploying smoothening, detection of blocked edges and then filtering the\ndifference between the pixels containing the blocked edge. The deblocking\nalgorithm presented has been successful in reducing blocky artifacts in an\nimage and therefore increases the subjective as well as objective quality of\nthe reconstructed image.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.1983v1"
    },
    {
        "title": "Nearest Neighbor Value Interpolation",
        "authors": [
            "Olivier Rukundo",
            "Hanqiang Cao"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  This paper presents the nearest neighbor value (NNV) algorithm for high\nresolution (H.R.) image interpolation. The difference between the proposed\nalgorithm and conventional nearest neighbor algorithm is that the concept\napplied, to estimate the missing pixel value, is guided by the nearest value\nrather than the distance. In other words, the proposed concept selects one\npixel, among four directly surrounding the empty location, whose value is\nalmost equal to the value generated by the conventional bilinear interpolation\nalgorithm. The proposed method demonstrated higher performances in terms of\nH.R. when compared to the conventional interpolation algorithms mentioned.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.1768v2"
    },
    {
        "title": "Gap Processing for Adaptive Maximal Poisson-Disk Sampling",
        "authors": [
            "Dong-Ming Yan",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  In this paper, we study the generation of maximal Poisson-disk sets with\nvarying radii. First, we present a geometric analysis of gaps in such disk\nsets. This analysis is the basis for maximal and adaptive sampling in Euclidean\nspace and on manifolds. Second, we propose efficient algorithms and data\nstructures to detect gaps and update gaps when disks are inserted, deleted,\nmoved, or have their radius changed. We build on the concepts of the regular\ntriangulation and the power diagram. Third, we will show how our analysis can\nmake a contribution to the state-of-the-art in surface remeshing.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.3297v2"
    },
    {
        "title": "Color scales that are effective in both color and grayscale",
        "authors": [
            "Silas Alben"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  We consider the problem of finding a color scale which performs well when\nconverted to a grayscale. We assume that each color is converted to a shade of\ngray with the same intensity as the color. We also assume that the color scales\nhave a linear variation of intensity and hue, and find scales which maximize\nthe average chroma (or \"colorfulness\") of the colors. We find two classes of\nsolutions, which traverse the color wheel in opposite directions. The two\nclasses of scales start with hues near cyan and red. The average chroma of the\nscales are 65-77% those of the pure colors.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.3659v1"
    },
    {
        "title": "Analysis-suitable T-splines: characterization, refineability, and\n  approximation",
        "authors": [
            "Xin Li",
            "M. A. Scott"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  We establish several fundamental properties of analysis-suitable T-splines\nwhich are important for design and analysis. First, we characterize T-spline\nspaces and prove that the space of smooth bicubic polynomials, defined over the\nextended T-mesh of an analysis-suitable T-spline, is contained in the\ncorresponding analysis-suitable T-spline space. This is accomplished through\nthe theory of perturbed analysis-suitable T-spline spaces and a simple\ntopological dimension formula. Second, we establish the theory of\nanalysis-suitable local refinement and describe the conditions under which two\nanalysis-suitable T-spline spaces are nested. Last, we demonstrate that these\nresults can be used to establish basic approximation results which are critical\nfor analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.5669v1"
    },
    {
        "title": "A Novel Algorithm for Real-time Procedural Generation of Building Floor\n  Plans",
        "authors": [
            "Maysam Mirahmadi",
            "Abdallah Shami"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Real-time generation of natural-looking floor plans is vital in games with\ndynamic environments. This paper presents an algorithm to generate suburban\nhouse floor plans in real-time. The algorithm is based on the work presented in\n[1]. However, the corridor placement is redesigned to produce floor plans\nsimilar to real houses. Moreover, an optimization stage is added to find a\ncorridor placement with the minimum used space, an approach that is designed to\nmimic the real-life practices to minimize the wasted spaces in the design. The\nresults show very similar floor plans to the ones designed by an architect.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.5842v1"
    },
    {
        "title": "Skeletal Representations and Applications",
        "authors": [
            "Andrea Tagliasacchi"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  When representing a solid object there are alternatives to the use of\ntraditional explicit (surface meshes) or implicit (zero crossing of implicit\nfunctions) methods. Skeletal representations encode shape information in a\nmixed fashion: they are composed of a set of explicit primitives, yet they are\nable to efficiently encode the shape's volume as well as its topology. I will\ndiscuss, in two dimensions, how symmetry can be used to reduce the\ndimensionality of the data (from a 2D solid to a 1D curve), and how this\nrelates to the classical definition of skeletons by Medial Axis Transform.\nWhile the medial axis of a 2D shape is composed of a set of curves, in 3D it\nresults in a set of sheets connected in a complex fashion. Because of this\ncomplexity, medial skeletons are difficult to use in practical applications.\nCurve skeletons address this problem by strictly requiring their geometry to be\none dimensional, resulting in an intuitive yet powerful shape representation.\nIn this report I will define both medial and curve skeletons and discuss their\nmutual relationship. I will also present several algorithms for their\ncomputation and a variety of scenarios where skeletons are employed, with a\nspecial focus on geometry processing and shape analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.6809v2"
    },
    {
        "title": "k-d Darts: Sampling by k-Dimensional Flat Searches",
        "authors": [
            "Mohamed S. Ebeida",
            "Anjul Patney",
            "Scott A. Mitchell",
            "Keith R. Dalbey",
            "Andrew A. Davidson",
            "John D. Owens"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We formalize the notion of sampling a function using k-d darts. A k-d dart is\na set of independent, mutually orthogonal, k-dimensional subspaces called k-d\nflats. Each dart has d choose k flats, aligned with the coordinate axes for\nefficiency. We show that k-d darts are useful for exploring a function's\nproperties, such as estimating its integral, or finding an exemplar above a\nthreshold. We describe a recipe for converting an algorithm from point sampling\nto k-d dart sampling, assuming the function can be evaluated along a k-d flat.\n  We demonstrate that k-d darts are more efficient than point-wise samples in\nhigh dimensions, depending on the characteristics of the sampling domain: e.g.\nthe subregion of interest has small volume and evaluating the function along a\nflat is not too expensive. We present three concrete applications using line\ndarts (1-d darts): relaxed maximal Poisson-disk sampling, high-quality\nrasterization of depth-of-field blur, and estimation of the probability of\nfailure from a response surface for uncertainty quantification. In these\napplications, line darts achieve the same fidelity output as point darts in\nless time. We also demonstrate the accuracy of higher dimensional darts for a\nvolume estimation problem. For Poisson-disk sampling, we use significantly less\nmemory, enabling the generation of larger point clouds in higher dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.3917v1"
    },
    {
        "title": "On Linear Spaces of Polyhedral Meshes",
        "authors": [
            "Roi Poranne",
            "Renjie Chen",
            "Craig Gotsman"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Polyhedral meshes (PM) - meshes having planar faces - have enjoyed a rise in\npopularity in recent years due to their importance in architectural and\nindustrial design. However, they are also notoriously difficult to generate and\nmanipulate. Previous methods start with a smooth surface and then apply\nelaborate meshing schemes to create polyhedral meshes approximating the\nsurface. In this paper, we describe a reverse approach: given the topology of a\nmesh, we explore the space of possible planar meshes with that topology.\n  Our approach is based on a complete characterization of the maximal linear\nspaces of polyhedral meshes contained in the curved manifold of polyhedral\nmeshes with a given topology. We show that these linear spaces can be described\nas nullspaces of differential operators, much like harmonic functions are\nnullspaces of the Laplacian operator. An analysis of this operator provides\ntools for global and local design of a polyhedral mesh, which fully expose the\ngeometric possibilities and limitations of the given topology.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.4110v1"
    },
    {
        "title": "Software for creating pictures in the LaTeX environment",
        "authors": [
            "Bezhentcev Roman Vadimovich"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  To create a text with graphic instructions for output pictures into LATEX\ndocument, we offer software that allows us to build a picture in WIZIWIG mode\nand for setting the text with these graphical instructions.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.0600v1"
    },
    {
        "title": "Glyph Sorting: Interactive Visualization for Multi-dimensional Data",
        "authors": [
            "David H. S. Chung",
            "Philip A. Legg",
            "Matthew L. Parry",
            "Rhodri Bown",
            "Iwan W. Griffiths",
            "Robert S. Laramee",
            "Min Chen"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Glyph-based visualization is an effective tool for depicting multivariate\ninformation. Since sorting is one of the most common analytical tasks performed\non individual attributes of a multi-dimensional data set, this motivates the\nhypothesis that introducing glyph sorting would significantly enhance the\nusability of glyph-based visualization. In this paper, we present a glyph-based\nconceptual framework as part of a visualization process for interactive sorting\nof multivariate data. We examine several technical aspects of glyph sorting and\nprovide design principles for developing effective, visually sortable glyphs.\nGlyphs that are visually sortable provide two key benefits: 1) performing\ncomparative analysis of multiple attributes between glyphs and 2) to support\nmulti-dimensional visual search. We describe a system that incorporates focus\nand context glyphs to control sorting in a visually intuitive manner and for\nviewing sorted results in an Interactive, Multi-dimensional Glyph (IMG) plot\nthat enables users to perform high-dimensional sorting, analyse and examine\ndata trends in detail. To demonstrate the usability of glyph sorting, we\npresent a case study in rugby event analysis for comparing and analysing trends\nwithin matches. This work is undertaken in conjunction with a national rugby\nteam. From using glyph sorting, analysts have reported the discovery of new\ninsight beyond traditional match analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.2889v1"
    },
    {
        "title": "The Logarithmic Curvature Graphs of Generalised Cornu Spirals",
        "authors": [
            "R. U. Gobithaasan",
            "J. M. Ali",
            "Kenjiro T. Miura"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  The Generalized Cornu Spiral (GCS) was first proposed by Ali et al. in 1995\n[9]. Due to the monotonocity of its curvature function, the surface generated\nwith GCS segments has been considered as a high quality surface and it has\npotential applications in surface design [2]. In this paper, the analysis of\nGCS segment is carried out by determining its aesthetic value using the log\ncurvature Graph (LCG) as proposed by Kanaya et al.[10]. The analysis of LCG\nsupports the claim that GCS is indeed a generalized aesthetic curve.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7842v1"
    },
    {
        "title": "G2 Transition curve using Quartic Bezier Curve",
        "authors": [
            "Azhar Ahmad",
            "R. Gobithasan",
            "Jamaluddin Md. Ali"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  A method to construct transition curves using a family of the quartic Bezier\nspiral is described. The transition curves discussed are S-shape and C-shape of\ncontact, between two separated circles. A spiral is a curve of monotone\nincreasing or monotone decreasing curvature of one sign. Thus, a spiral cannot\nhave an inflection point or curvature extreme. The family of quartic Bezier\nspiral form which is introduced has more degrees of freedom and will give a\nbetter approximation. It is proved that the methods of constructing transition\ncurves can be simplified by the transformation process and the ratio of two\nradii has no restriction, which extends the application area, and it gives a\nfamily of transition curves that allow more flexible curve designs.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7845v1"
    },
    {
        "title": "Characterization of Planar Cubic Alternative curve",
        "authors": [
            "Azhar Ahmad",
            "R. Gobithasan",
            "Jamaluddin Md. Ali"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  In this paper, we analyze the planar cubic Alternative curve to determine the\nconditions for convex, loops, cusps and inflection points. Thus cubic curve is\nrepresented by linear combination of three control points and basis function\nthat consist of two shape parameters. By using algebraic manipulation, we can\ndetermine the constraint of shape parameters and sufficient conditions are\nderived which ensure that the curve is a strictly convex, loops, cusps and\ninflection point. We conclude the result in a shape diagram of parameters. The\nsimplicity of this form makes characterization more intuitive and efficient to\ncompute.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7848v1"
    },
    {
        "title": "Variational Formulation of the Log-Aesthetic Surface and Development of\n  Discrete Surface Filters",
        "authors": [
            "K. T. Miura",
            "R. Shirahata",
            "S. Agari",
            "S. Usuki",
            "R. U. Gobithaasan"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  The log-aesthetic curves include the logarithmic (equiangular) spiral,\nclothoid, and involute curves. Although most of them are expressed only by an\nintegral form of the tangent vector, it is possible to interactively generate\nand deform them and they are expected to be utilized for practical use of\nindustrial and graphical design. The discrete log-aesthetic filter based on the\nformulation of the log-aesthetic curve has successfully been introduced not to\nimpose strong constraints on the designer's activity, to let him/her design\nfreely and to embed the properties of the log-aesthetic curves for complicated\nones with both increasing and decreasing curvature. In this paper, in order to\ndefine the log-aesthetic surface and develop surface filters based on its\nformulation, at first we reformulate the log-aesthetic curve with variational\nprinciple. Then we propose several new functionals to be minimized for\nfree-form surfaces and define the log-aesthetic surface. Furthermore we propose\nnew discrete surface filters based on the log-aesthetic surface formulation\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7852v1"
    },
    {
        "title": "Normal type-2 Fuzzy Rational B-Spline Curve",
        "authors": [
            "Rozaimi Zakaria",
            "Abd. Fatah Wahab",
            "R. U. Gobithaasan"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  In this paper, we proposed a new form of type-2 fuzzy data points(T2FDPs)\nthat is normal type-2 data points(NT2FDPs). These brand-new forms of data were\ndefined by using the definition of normal type-2 triangular fuzzy\nnumber(NT2TFN). Then, we applied fuzzification(alpha-cut) and type-reduction\nprocesses towards NT2FDPs after they had been redefined based on the situation\nof NT2FDPs. Furthermore, we redefine the defuzzification definition along with\nthe new definitions of fuzzification process and type-reduction method to\nobtain crisp type-2 fuzzy solution data points. For all these processes from\nthe defining the NT2FDPs to defuzzification of NT2FDPs, we demonstrate through\ncurve representation by using the rational B-spline curve function as the\nexample form modeling these NT2FDPs.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7868v1"
    },
    {
        "title": "Various Types of Aesthetic Curves",
        "authors": [
            "R. U. Gobithaasan"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  The research on developing planar curves to produce visually pleasing\nproducts (ranges from electric appliances to car body design) and\nindentifying/modifying planar curves for special purposes namely for railway\ndesign, highway design and robot trajectories have been progressing since\n1970s. The pattern of research in this field of study has branched to five\nmajor groups namely curve synthesis, fairing process, improvement in control of\nnatural spiral, construction of new type of planar curves and, natural spiral\nfitting & approximation techniques. The purpose of is this paper is to briefly\nreview recent progresses in Computer Aided Geometric Design (CAGD) focusing on\nthe topics states above.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7881v1"
    },
    {
        "title": "An Improvised Algorithm to Identify The Beauty of A Planar Curve",
        "authors": [
            "R. U. Gobithaasan",
            "Jamaludin Md. Ali",
            "Kenjiro T. Miura"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  An improvised algorithm is proposed based on the work of Yoshimoto and\nHarada. The improvised algorithm results a graph which is called LDGC or\nLogarithmic Distribution Graph of Curvature. This graph has the capability to\nidentify the beauty of monotonic planar curves with less effort as compared to\nLDDC by Yoshimoto and Harada.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7883v1"
    },
    {
        "title": "Pattern Recognition and Revealing using Parallel Coordinates Plot",
        "authors": [
            "Xin Zhao",
            "Bo Li"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Parallel coordinates plot (PCP) is an excellent tool for multivariate\nvisualization and analysis, but it may fail to reveal inherent structures for\ndatasets with a large number of items. In this paper, we propose a suite of\nnovel clustering, dimension ordering and visualization techniques based on PCP,\nto reveal and highlight hidden structures. First, we propose a continuous\nspline based polycurves design to extract and classify different cluster\naspects of the data. Then, we provide an efficient and optimal correlation\nbased sorting technique to reorder coordinates, as a helpful visualization tool\nfor data analysis. Various results generated by our framework visually\nrepresent much structure, trend and correlation information to guide the user,\nand improve the efficacy of analysis, especially for complex and noisy\ndatasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.1959v2"
    },
    {
        "title": "Multimaterial Front Tracking",
        "authors": [
            "Fang Da",
            "Christopher Batty",
            "Eitan Grinspun"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We present the first triangle mesh-based technique for tracking the evolution\nof general three-dimensional multimaterial interfaces undergoing complex\ntopology changes induced by deformations and collisions. Our core\nrepresentation is a non-manifold triangle surface mesh with material labels\nassigned to each half-face to distinguish volumetric regions. We advect the\nvertices of the mesh in a Lagrangian manner, and employ a complete set of\ncollision-safe mesh improvement and topological operations that track and\nupdate material labels. In particular, we develop a unified, collision-safe\nstrategy for handling complex topological operations acting on evolving triple-\nand higher-valence junctions, and a flexible method to merge colliding\nmultimaterial meshes. We demonstrate our approach with a number of challenging\ngeometric flows, including passive advection, normal flow, and mean curvature\nflow.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3113v2"
    },
    {
        "title": "A New 3D Geometric Approach to Focus and Context Lens Effect Simulation",
        "authors": [
            "Bo Li",
            "Xin Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We present a novel methodology based on geometric approach to simulate\nmagnification lens effects. Our aim is to promote new applications of powerful\ngeometric modeling techniques in visual computing. Conventional image\nprocessing/visualization methods are computed in two dimensional space (2D). We\nexamine this conventional 2D manipulation from a completely innovative\nperspective of 3D geometric processing. Compared with conventional optical lens\ndesign, 3D geometric method are much more capable of preserving shape features\nand minimizing distortion. We magnify an area of interest to better visualize\nthe interior details, while keeping the rest of area without perceivable\ndistortion. We flatten the mesh back into 2D space for viewing, and further\napplications in the screen space. In both steps, we devise an iterative\ndeformation scheme to minimize distortion around both focus and context region,\nwhile avoiding the noncontinuous transition region between the focus and\ncontext areas. Particularly, our method allows the user to flexibly modify the\nROI shapes to accommodate complex feature. The user can also easily specify a\nspectrum of metrics for different visual effects. Various experimental results\ndemonstrate the effectiveness, robustness, and efficiency of our framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.0375v1"
    },
    {
        "title": "Inverse Procedural Modeling of Facade Layouts",
        "authors": [
            "Fuzhang Wu",
            "Dong-Ming Yan",
            "Weiming Dong",
            "Xiaopeng Zhang",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  In this paper, we address the following research problem: How can we generate\na meaningful split grammar that explains a given facade layout? To evaluate if\na grammar is meaningful, we propose a cost function based on the description\nlength and minimize this cost using an approximate dynamic programming\nframework. Our evaluation indicates that our framework extracts meaningful\nsplit grammars that are competitive with those of expert users, while some\nusers and all competing automatic solutions are less successful.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.0419v2"
    },
    {
        "title": "A Survey of Spline-based Volumetric Data Modeling Framework and Its\n  Applications",
        "authors": [
            "Bo Li"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  The rapid advances in 3D scanning and acquisition techniques have given rise\nto the explosive increase of volumetric digital models in recent years. This\ndissertation systematically trailblazes a novel volumetric modeling framework\nto represent 3D solids. The need to explore more efficient and robust 3D\nmodeling framework has gained the prominence. Although the traditional surface\nrepresentation (e.g., triangle mesh) has many attractive properties, it is\nincapable of expressing the interior space and materials. Such a serious\ndrawback overshadows many potential modeling and analysis applications.\nConsequently volumetric modeling techniques become the well-known solution to\nthis problem. Nevertheless, many unsolved research issues remain when\ndeveloping an efficient modeling paradigm for existing 3D models: complex\ngeometry (fine details and extreme concaveness), arbitrary topology,\nheterogenous materials, large-scale data storage and processing, etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.0867v1"
    },
    {
        "title": "A Spline-based Volumetric Data Modeling Framework and Its Applications",
        "authors": [
            "Bo Li"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  In this dissertation, we concentrate on the challenging research issue of\ndeveloping a spline-based modeling framework, which converts the conventional\ndata (e.g., surface meshes) to tensor-product trivariate splines. This\nmethodology can represent both boundary/volumetric geometry and real volumetric\nphysical attributes in a compact and continuous fashion. The regular\ntensor-product structure enables our new developed methods to be embedded into\nthe industry standard seamlessly. These properties make our techniques highly\npreferable in many physically-based applications including mechanical analysis,\nshape deformation and editing, virtual surgery training, etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.0869v1"
    },
    {
        "title": "Barycentric Coordinates as Interpolants",
        "authors": [
            "Russell A. Brown"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Barycentric coordinates are frequently used as interpolants to shade computer\ngraphics images. A simple equation transforms barycentric coordinates from\nscreen space into eye space in order to undo the perspective transformation and\npermit accurate interpolative shading of texture maps. This technique is\namenable to computation using a block-normalized integer representation.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.1279v9"
    },
    {
        "title": "Medial Meshes for Volume Approximation",
        "authors": [
            "Feng Sun",
            "Yi-King Choi",
            "Yizhou Yu",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Volume approximation is an important problem found in many applications of\ncomputer graphics, vision, and image processing. The problem is about computing\nan accurate and compact approximate representation of 3D volumes using some\nsimple primitives. In this study, we propose a new volume representation,\ncalled medial meshes, and present an efficient method for its computation.\nSpecifically, we use the union of a novel type of simple volume primitives,\nwhich are spheres and the convex hulls of two or three spheres, to approximate\na given 3D shape. We compute such a volume approximation based on a new method\nfor medial axis simplification guided by Hausdorff errors. We further\ndemonstrate the superior efficiency and accuracy of our method over existing\nmethods for medial axis simplification.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.3917v1"
    },
    {
        "title": "Affordable Virtual Reality System Architecture for Representation of\n  Implicit Object Properties",
        "authors": [
            "Stoyan Maleshkov",
            "Dimo Chotrov"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  A flexible, scalable and affordable virtual reality software system\narchitecture is proposed. This solution can be easily implemented on different\nhardware configurations: on a single computer or on a computer cluster. The\narchitecture is aimed to be integrated in the workflow for solving engineering\ntasks and oriented towards presenting implicit object properties through\nmultiple sensorial channels (visual, audio and haptic). Implicit properties\nrepresent hidden object features (i.e. magnetization, radiation, humidity,\ntoxicity, etc.) which cannot be perceived by the observer through his or her\nsenses but require specialized equipment in order to expand the sensory ability\nof the observer. Our approach extends the underlying general scene graph\nstructure incorporating additional effects nodes for implicit properties\nrepresentation.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.5843v1"
    },
    {
        "title": "Post-processing of Engineering Analysis Results for Visualization in VR\n  Systems",
        "authors": [
            "Stoyan Maleshkov",
            "Dimo Chotrov"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  The applicability of Virtual Reality for evaluating engineering analysis\nresults is beginning to receive increased appreciation in the last years. The\nproblem many engineers are still facing is how to import their model together\nwith the analysis results in a virtual reality environment for exploration and\nresults validation. In this paper we propose an algorithm for transforming\nmodel data and results from finite element analysis (FEA) solving application\nto a format easily interpretable by a virtual reality application. The\nalgorithm includes also steps for reducing the face-count of the resulting mesh\nby eliminating faces from the inner part of the model in the cases when only\nthe surfaces of the model is analyzed. We also describe a possibility for\nsimultaneously assessing multiple analysis results relying on multimodal\nresults presentation by stimulating different senses of the operator.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.5847v1"
    },
    {
        "title": "Zahir: a Object-Oriented Framework for Computer Graphics",
        "authors": [
            "Eduardo Graells-Garrido",
            "María Cecilia Rivara"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  In this article we present Zahir, a framework for experimentation in Computer\nGraphics that provides a group of object-oriented base components that take\ncare of common tasks in rendering techniques and algorithms, specially those of\nNon Photo-realistic Rendering (NPR). These components allow developers to\nimplement rendering techniques and algorithms over static and animated meshes.\nCurrently, Zahir is being used in a Master's Thesis and as support material in\nthe undergraduate Computer Graphics course in University of Chile.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.1917v1"
    },
    {
        "title": "Detection and Characterization of Intrinsic Symmetry",
        "authors": [
            "Anirban Mukhopadhyay",
            "Suchendra M. Bhandarkar",
            "Fatih Porikli"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  A comprehensive framework for detection and characterization of overlapping\nintrinsic symmetry over 3D shapes is proposed. To identify prominent symmetric\nregions which overlap in space and vary in form, the proposed framework is\ndecoupled into a Correspondence Space Voting procedure followed by a\nTransformation Space Mapping procedure. In the correspondence space voting\nprocedure, significant symmetries are first detected by identifying surface\npoint pairs on the input shape that exhibit local similarity in terms of their\nintrinsic geometry while simultaneously maintaining an intrinsic distance\nstructure at a global level. Since different point pairs can share a common\npoint, the detected symmetric shape regions can potentially overlap. To this\nend, a global intrinsic distance-based voting technique is employed to ensure\nthe inclusion of only those point pairs that exhibit significant symmetry. In\nthe transformation space mapping procedure, the Functional Map framework is\nemployed to generate the final map of symmetries between point pairs. The\ntransformation space mapping procedure ensures the retrieval of the underlying\ndense correspondence map throughout the 3D shape that follows a particular\nsymmetry. Additionally, the formulation of a novel cost matrix enables the\ninner product to succesfully indicate the complexity of the underlying symmetry\ntransformation. The proposed transformation space mapping procedure is shown to\nresult in the formulation of a semi-metric symmetry space where each point in\nthe space represents a specific symmetry transformation and the distance\nbetween points represents the complexity between the corresponding\ntransformations. Experimental results show that the proposed framework can\nsuccessfully process complex 3D shapes that possess rich symmetries.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.7472v1"
    },
    {
        "title": "Gradient-Domain Processing for Large EM Image Stacks",
        "authors": [
            "Michael Kazhdan",
            "Randal Burns",
            "Bobby Kasthuri",
            "Jeff Lichtman",
            "Jacob Vogelstein",
            "Joshua Vogelstein"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We propose a new gradient-domain technique for processing registered EM image\nstacks to remove the inter-image discontinuities while preserving intra-image\ndetail. To this end, we process the image stack by first performing anisotropic\ndiffusion to smooth the data along the slice axis and then solving a\nscreened-Poisson equation within each slice to re-introduce the detail. The\nfinal image stack is both continuous across the slice axis (facilitating the\ntracking of information between slices) and maintains sharp details within each\nslice (supporting automatic feature detection). To support this editing, we\ndescribe the implementation of the first multigrid solver designed for\nefficient gradient domain processing of large, out-of-core, voxel grids.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.0041v1"
    },
    {
        "title": "Compression of animated 3D models using HO-SVD",
        "authors": [
            "Michał Romaszewski",
            "Piotr Gawron",
            "Sebastian Opozda"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  This work presents an analysis of Higher Order Singular Value Decomposition\n(HO-SVD) applied to lossy compression of 3D mesh animations. We describe\nstrategies for choosing a number of preserved spatial and temporal components\nafter tensor decomposition. Compression error is measured using three metrics\n(MSE, Hausdorff, MSDM). Results are compared with a method based on Principal\nComponent Analysis (PCA) and presented on a set of animations with typical mesh\ndeformations.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.1240v1"
    },
    {
        "title": "Matching LBO eigenspace of non-rigid shapes via high order statistics",
        "authors": [
            "Alon Shtern",
            "Ron Kimmel"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  A fundamental tool in shape analysis is the virtual embedding of the\nRiemannian manifold describing the geometry of a shape into Euclidean space.\nSeveral methods have been proposed to embed isometric shapes in flat domains\nwhile preserving distances measured on the manifold. Recently, attention has\nbeen given to embedding shapes into the eigenspace of the Lapalce-Beltrami\noperator. The Laplace-Beltrami eigenspace preserves the diffusion distance, and\nis invariant under isometric transformations. However, Laplace-Beltrami\neigenfunctions computed independently for different shapes are often\nincompatible with each other. Applications involving multiple shapes, such as\npointwise correspondence, would greatly benefit if their respective\neigenfunctions were somehow matched. Here, we introduce a statistical approach\nfor matching eigenfunctions. We consider the values of the eigenfunctions over\nthe manifold as sampling of random variables, and try to match their\nmultivariate distributions. Comparing distributions is done indirectly, using\nhigh order statistics. We show that the permutation and sign ambiguities of low\norder eigenfunctions, can be inferred by minimizing the difference of their\nthird order moments. The sign ambiguities of antisymmetric eigenfunctions can\nbe resolved by exploiting isometric invariant relations between the gradients\nof the eigenfunctions and the surface normal. We present experiments\ndemonstrating the success of the proposed method applied to feature point\ncorrespondence.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.4459v1"
    },
    {
        "title": "A Dual-Beam Method-of-Images 3D Searchlight BSSRDF",
        "authors": [
            "Eugene d'Eon"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We present a novel BSSRDF for rendering translucent materials. Angular\neffects lacking in previous BSSRDF models are incorporated by using a dual-beam\nformulation. We employ a Placzek's Lemma interpretation of the method of images\nand discard diffusion theory. Instead, we derive a plane-parallel\ntransformation of the BSSRDF to form the associated BRDF and optimize the image\nconfiurations such that the BRDF is close to the known analytic solutions for\nthe associated albedo problem. This ensures reciprocity, accurate colors, and\nprovides an automatic level-of-detail transition for translucent objects that\nappear at various distances in an image. Despite optimizing the subsurface\nfluence in a plane-parallel setting, we find that this also leads to fairly\naccurate fluence distributions throughout the volume in the original 3D\nsearchlight problem. Our method-of-images modifications can also improve the\naccuracy of previous BSSRDFs.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.0955v2"
    },
    {
        "title": "On the impact of explicit or semi-implicit integration methods over the\n  stability of real-time numerical simulations",
        "authors": [
            "Teodor Cioaca",
            "Horea Caramizaru"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Physics-based animation of soft or rigid bodies for real-time applications\noften suffers from numerical instabilities. We analyse one of the most common\nsources of unwanted behaviour: the numerical integration strategy. To assess\nthe impact of popular integration methods, we consider a scenario where soft\nand hard constraints are added to a custom designed deformable linear object.\nSince the goal for this class of simulation methods is to attain interactive\nframe-rates, we present the drawbacks of using explicit integration methods\nover inherently stable, implicit integrators. To help numerical solver\ndesigners better understand the impact of an integrator on a certain simulated\nworld, we have conceived a method of benchmarking the efficiency of an\nintegrator with respect to its speed, stability and symplecticity.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.5018v1"
    },
    {
        "title": "Digitize Your Body and Action in 3-D at Over 10 FPS: Real Time Dense\n  Voxel Reconstruction and Marker-less Motion Tracking via GPU Acceleration",
        "authors": [
            "Jian Song",
            "Yatao Bian",
            "Junchi Yan",
            "Xu Zhao",
            "Yuncai Liu"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  In this paper, we present an approach to reconstruct 3-D human motion from\nmulti-cameras and track human skeleton using the reconstructed human 3-D point\n(voxel) cloud. We use an improved and more robust algorithm, probabilistic\nshape from silhouette to reconstruct human voxel. In addition, the annealed\nparticle filter is applied for tracking, where the measurement is computed\nusing the reprojection of reconstructed voxel. We use two different ways to\naccelerate the approach. For the CPU only acceleration, we leverage Intel TBB\nto speed up the hot spot of the computational overhead and reached an\naccelerating ratio of 3.5 on a 4-core CPU. Moreover, we implement an\nintensively paralleled version via GPU acceleration without TBB. Taking account\nall data transfer and computing time, the GPU version is about 400 times faster\nthan the original CPU implementation, leading the approach to run at a\nreal-time speed.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.6811v1"
    },
    {
        "title": "A local Gaussian filter and adaptive morphology as tools for completing\n  partially discontinuous curves",
        "authors": [
            "P. Spurek",
            "A. Chaikouskaya",
            "J. Tabor",
            "E. Zając"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  This paper presents a method for extraction and analysis of curve--type\nstructures which consist of disconnected components. Such structures are found\nin electron--microscopy (EM) images of metal nanograins, which are widely used\nin the field of nanosensor technology.\n  The topography of metal nanograins in compound nanomaterials is crucial to\nnanosensor characteristics. The method of completing such templates consists of\nthree steps. In the first step, a local Gaussian filter is used with different\nweights for each neighborhood. In the second step, an adaptive morphology\noperation is applied to detect the endpoints of curve segments and connect\nthem. In the last step, pruning is employed to extract a curve which optimally\nfits the template.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.7430v1"
    },
    {
        "title": "Continuous Collision Detection for Composite Quadric Models",
        "authors": [
            "Yi-King Choi",
            "Wenping Wang",
            "Bernard Mourrain",
            "Changhe Tu",
            "Xiaohong Jia",
            "Feng Sun"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  A composite quadric model (CQM) is an object modeled by piecewise linear or\nquadric patches. We study the continuous detection problem of a special type of\nCQM objects which are commonly used in CAD/CAM, that is, the boundary surfaces\nof such a CQM intersect only in straight line segments or conic curve segments.\nWe present a framework for continuous collision detection (CCD) of this special\ntype of CQM (which we also call CQM for brevity) in motion. We derive algebraic\nformulations and compute numerically the first contact time instants and the\ncontact points of two moving CQMs in $\\mathbb R^3$. Since it is difficult to\nprocess CCD of two CQMs in a direct manner because they are composed of\nsemi-algebraic varieties, we break down the problem into subproblems of solving\nCCD of pairs of boundary elements of the CQMs. We present procedures to solve\nCCD of different types of boundary element pairs in different dimensions. Some\nCCD problems are reduced to their equivalents in a lower dimensional setting,\nwhere they can be solved more efficiently.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.7462v1"
    },
    {
        "title": "Compact Part-Based Shape Spaces for Dense Correspondences",
        "authors": [
            "Oliver Burghard",
            "Alexander Berner",
            "Michael Wand",
            "Niloy Mitra",
            "Hans-Peter Seidel",
            "Reinhard Klein"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We consider the problem of establishing dense correspondences within a set of\nrelated shapes of strongly varying geometry. For such input, traditional shape\nmatching approaches often produce unsatisfactory results. We propose an\nensemble optimization method that improves given coarse correspondences to\nobtain dense correspondences. Following ideas from minimum description length\napproaches, it maximizes the compactness of the induced shape space to obtain\nhigh-quality correspondences. We make a number of improvements that are\nimportant for computer graphics applications: Our approach handles meshes of\ngeneral topology and handles partial matching between input of varying\ntopology. To this end we introduce a novel part-based generative statistical\nshape model. We develop a novel analysis algorithm that learns such models from\ntraining shapes of varying topology. We also provide a novel synthesis method\nthat can generate new instances with varying part layouts and subject to\ngeneric variational constraints. In practical experiments, we obtain a\nsubstantial improvement in correspondence quality over state-of-the-art\nmethods. As example application, we demonstrate a system that learns shape\nfamilies as assemblies of deformable parts and permits real-time editing with\ncontinuous and discrete variability.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.7535v2"
    },
    {
        "title": "Ergonomic-driven Geometric Exploration and Reshaping",
        "authors": [
            "Youyi Zheng",
            "Julie Dorsey",
            "Niloy Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  The paper addresses the following problem: given a set of man-made shapes,\ne.g., chairs, can we quickly rank and explore the set of shapes with respect to\na given avatar pose? Answering this question requires identifying which shapes\nare more suitable for the defined avatar and pose; and moreover, to provide\nfast preview of how to alter the input geometry to better fit the deformed\nshapes to the given avatar pose? The problem naturally links physical\nproportions of human body and its interaction with object shapes in an attempt\nto connect ergonomics with shape geometry. We designed an interaction system\nthat allows users to explore shape collections using the deformation of human\ncharacters while at the same time providing interactive previews of how to\nalter the shapes to better fit the user-specified character. We achieve this by\nfirst mapping ergonomics guidelines into a set of simultaneous multi-part\nconstraints based on target contacts; and then, proposing a novel contact-based\ndeformation model to realize multi-contact constraints. We evaluate our\nframework on various chair models and validate the results via a small user\nstudy.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.5440v1"
    },
    {
        "title": "Flux-Limited Diffusion for Multiple Scattering in Participating Media",
        "authors": [
            "David Koerner",
            "Jamie Portsmouth",
            "Filip Sadlo",
            "Thomas Ertl",
            "Bernd Eberhardt"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  For the rendering of multiple scattering effects in participating media,\nmethods based on the diffusion approximation are an extremely efficient\nalternative to Monte Carlo path tracing. However, in sufficiently transparent\nregions, classical diffusion approximation suffers from non-physical radiative\nfluxes which leads to a poor match to correct light transport. In particular,\nthis prevents the application of classical diffusion approximation to\nheterogeneous media, where opaque material is embedded within transparent\nregions. To address this limitation, we introduce flux-limited diffusion, a\ntechnique from the astrophysics domain. This method provides a better\napproximation to light transport than classical diffusion approximation,\nparticularly when applied to heterogeneous media, and hence broadens the\napplicability of diffusion-based techniques. We provide an algorithm for\nflux-limited diffusion, which is validated using the transport theory for a\npoint light source in an infinite homogeneous medium. We further demonstrate\nthat our implementation of flux-limited diffusion produces more accurate\nrenderings of multiple scattering in various heterogeneous datasets than\nclassical diffusion approximation, by comparing both methods to ground truth\nrenderings obtained via volumetric path tracing.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.8105v1"
    },
    {
        "title": "Consistently Orienting Facets in Polygon Meshes by Minimizing the\n  Dirichlet Energy of Generalized Winding Numbers",
        "authors": [
            "Kenshi Takayama",
            "Alec Jacobson",
            "Ladislav Kavan",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Jacobson et al. [JKSH13] hypothesized that the local coherency of the\ngeneralized winding number function could be used to correctly determine\nconsistent facet orientations in polygon meshes. We report on an approach to\nconsistently orienting facets in polygon meshes by minimizing the Dirichlet\nenergy of generalized winding numbers. While the energy can be concisely\nformulated and efficiently computed, we found that this approach is\nfundamentally flawed and is unfortunately not applicable for most handmade\nmeshes shared on popular mesh repositories such as Google 3D Warehouse.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.5431v2"
    },
    {
        "title": "3D Texture Coordinates on Polygon Mesh Sequences",
        "authors": [
            "Eric Mootz"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  A method for creating 3D texture coordinates for a sequence of polygon meshes\nwith changing topology and vertex motion vectors.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.6786v2"
    },
    {
        "title": "DASS: Detail Aware Sketch-Based Surface Modeling",
        "authors": [
            "Emilio Vital Brazil"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  We present a sketch-based modeling system suitable for detail editing, based\non a multilevel representation for surfaces. The main advantage of this\nrepresentation allowing for the control of local (details) and global changes\nof the model. We used an adaptive mesh (4-8 mesh) and developed a label theory\nto construct a manifold structure, which is responsible for controlling local\nediting of the model. The overall shape and global modifications are defined by\na variational implicit surface (Hermite RBF). Our system assembles the manifold\nstructures to allow the user to add details without changing the overall shape,\nas well as edit the overall shape while repositioning details coherently.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.7025v1"
    },
    {
        "title": "Order-Independent Texture Synthesis",
        "authors": [
            "Li-Yi Wei",
            "Marc Levoy"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Search-based texture synthesis algorithms are sensitive to the order in which\ntexture samples are generated; different synthesis orders yield different\ntextures. Unfortunately, most polygon rasterizers and ray tracers do not\nguarantee the order with which surfaces are sampled. To circumvent this\nproblem, textures are synthesized beforehand at some maximum resolution and\nrendered using texture mapping.\n  We describe a search-based texture synthesis algorithm in which samples can\nbe generated in arbitrary order, yet the resulting texture remains identical.\nThe key to our algorithm is a pyramidal representation in which each texture\nsample depends only on a fixed number of neighboring samples at each level of\nthe pyramid. The bottom (coarsest) level of the pyramid consists of a noise\nimage, which is small and predetermined. When a sample is requested by the\nrenderer, all samples on which it depends are generated at once. Using this\napproach, samples can be generated in any order. To make the algorithm\nefficient, we propose storing texture samples and their dependents in a\npyramidal cache. Although the first few samples are expensive to generate,\nthere is substantial reuse, so subsequent samples cost less. Fortunately, most\nrendering algorithms exhibit good coherence, so cache reuse is high.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.7338v1"
    },
    {
        "title": "A Moving Least Squares Based Approach for Contour Visualization of\n  Multi-Dimensional Data",
        "authors": [
            "Chris W. Muelder",
            "Nick Leaf",
            "Carmen Sigovan",
            "Kwan-Liu Ma"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Analysis of high dimensional data is a common task. Often, small multiples\nare used to visualize 1 or 2 dimensions at a time, such as in a scatterplot\nmatrix. Associating data points between different views can be difficult\nthough, as the points are not fixed. Other times, dimensional reduction\ntechniques are employed to summarize the whole dataset in one image, but\nindividual dimensions are lost in this view. In this paper, we present a means\nof augmenting a dimensional reduction plot with isocontours to reintroduce the\noriginal dimensions. By applying this to each dimension in the original data,\nwe create multiple views where the points are consistent, which facilitates\ntheir comparison. Our approach employs a combination of a novel, graph-based\nprojection technique with a GPU accelerated implementation of moving least\nsquares to interpolate space between the points. We also present evaluations of\nthis approach both with a case study and with a user study.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.0677v1"
    },
    {
        "title": "Spoke-Darts for High-Dimensional Blue-Noise Sampling",
        "authors": [
            "Scott A. Mitchell",
            "Mohamed S. Ebeida",
            "Muhammad A. Awad",
            "Chonhyon Park",
            "Anjul Patney",
            "Ahmad A. Rushdi",
            "Laura P. Swiler",
            "Dinesh Manocha",
            "Li-Yi Wei"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Blue noise sampling has proved useful for many graphics applications, but\nremains underexplored in high-dimensional spaces due to the difficulty of\ngenerating distributions and proving properties about them. We present a blue\nnoise sampling method with good quality and performance across different\ndimensions. The method, spoke-dart sampling, shoots rays from prior samples and\nselects samples from these rays. It combines the advantages of two major\nhigh-dimensional sampling methods: the locality of advancing front with the\ndimensionality-reduction of hyperplanes, specifically line sampling. We prove\nthat the output sampling is saturated with high probability, with bounds on\ndistances between pairs of samples and between any domain point and its nearest\nsample. We demonstrate spoke-dart applications for approximate Delaunay graph\nconstruction, global optimization, and robotic motion planning. Both the\nblue-noise quality of the output distribution and the adaptability of the\nintermediate processes of our method are useful in these applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.1118v3"
    },
    {
        "title": "Regularized Harmonic Surface Deformation",
        "authors": [
            "Yeara Kozlov",
            "Janick Martinez Esturo",
            "Hans-Peter Seidel",
            "Tino Weinkauf"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Harmonic surface deformation is a well-known geometric modeling method that\ncreates plausible deformations in an interactive manner. However, this method\nis susceptible to artifacts, in particular close to the deformation handles.\nThese artifacts often correlate with strong gradients of the deformation\nenergy.In this work, we propose a novel formulation of harmonic surface\ndeformation, which incorporates a regularization of the deformation energy. To\ndo so, we build on and extend a recently introduced generic linear\nregularization approach. It can be expressed as a change of norm for the linear\noptimization problem, i.e., the regularization is baked into the optimization.\nThis minimizes the implementation complexity and has only a small impact on\nruntime. Our results show that a moderate use of regularization suppresses many\ndeformation artifacts common to the well-known harmonic surface deformation\nmethod, without introducing new artifacts.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.3326v1"
    },
    {
        "title": "Footprint-Driven Locomotion Composition",
        "authors": [
            "Christos Mousas",
            "Paul Newbury",
            "Christos-Nikolaos Anagnostopoulos"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  One of the most efficient ways of generating goal-directed walking motions is\nsynthesising the final motion based on footprints. Nevertheless, current\nimplementations have not examined the generation of continuous motion based on\nfootprints, where different behaviours can be generated automatically.\nTherefore, in this paper a flexible approach for footprint-driven locomotion\ncomposition is presented. The presented solution is based on the ability to\ngenerate footprint-driven locomotion, with flexible features like jumping,\nrunning, and stair stepping. In addition, the presented system examines the\nability of generating the desired motion of the character based on predefined\nfootprint patterns that determine which behaviour should be performed. Finally,\nit is examined the generation of transition patterns based on the velocity of\nthe root and the number of footsteps required to achieve the target behaviour\nsmoothly and naturally.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1906v1"
    },
    {
        "title": "Mesh2Fab: Reforming Shapes for Material-specific Fabrication",
        "authors": [
            "Yong-Liang Yang",
            "Jun Wang",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  As humans, we regularly associate shape of an object with its built material.\nIn the context of geometric modeling, however, this interrelation between form\nand material is rarely explored. In this work, we propose a novel data-driven\nreforming (i.e., reshaping) algorithm that adapts an input multi-component\nmodel for a target fabrication material. The algorithm adapts both the part\ngeometry and the inter-part topology of the input shape to better align with\nmaterial specific fabrication requirements. As output, we produce the reshaped\nmodel along with respective part dimensions and inter-part junction\nspecifications. We evaluate our algorithm on a range of man-made models and\ndemonstrate non-trivial model reshaping examples focusing only on metal and\nwooden materials. We also appraise the output of our algorithm using a user\nstudy.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.3632v1"
    },
    {
        "title": "Reverse Engineering Point Clouds to Fit Tensor Product B-Spline Surfaces\n  by Blending Local Fits",
        "authors": [
            "Lavanya Sita Tekumalla",
            "Elaine Cohen"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Being able to reverse engineer from point cloud data to obtain 3D models is\nimportant in modeling. As our main contribution, we present a new method to\nobtain a tensor product B-spline representation from point cloud data by\nfitting surfaces to appropriately segmented data. By blending multiple local\nfits our method is more efficient than existing techniques, with the ability to\ndeal with more detail by efficiently introducing a high number of knots.\nFurther point cloud data obtained by digitizing 3D data, typically presents\nmany associated complications like noise and missing data. As our second\ncontribution, we propose an end-to-end framework for smoothing, hole filling,\nparameterization, knot selection and B-spline fitting that addresses these\nissues, works robustly with large irregularly shaped data containing holes and\nis straightforward to implement.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.5993v1"
    },
    {
        "title": "Ceramics Fragments Digitization by Photogrammetry, Reconstructions and\n  Applications",
        "authors": [
            "Jean-Baptiste Barreau",
            "Théophane Nicolas",
            "G Bruniaux",
            "E Petit",
            "Q Petit",
            "Y Bernard",
            "Ronan Gaugne",
            "Valérie Gouranton"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  This paper presents an application of photogrammetry on ceramic fragments\nfrom two excavation sites located north-west of France. The restitution by\nphotogrammetry of these different fragments allowed reconstructions of the\npotteries in their original state or at least to get to as close as possible.\nWe used the 3D reconstructions to compute some metrics and to generate a\npresentation support by using a 3D printer. This work is based on affordable\ntools and illustrates how 3D technologies can be quite easily integrated in\narchaeology process with limited financial resources. 1. INTRODUCTION Today,\nphotogrammetry and 3D modelling are an integral part of the methods used in\narcheology and heritage management. They provide answers to scientific needs in\nthe fields of conservation, preservation, restoration and mediation of\narchitectural, archaeological and cultural heritage [2] [6] [7] [9].\nPhotogrammetry on ceramic fragments was one of the first applications\ncontemporary of the development of this technique applied in the archaeological\ncommunity [3]. More recently and due to its democratization, it was applied\nmore generally to artifacts [5]. Finally joined today by the rise of 3D\nprinting [8] [10], it can restore fragmented artifacts [1] [12]. These examples\ntarget one or several particular objects and use different types of equipment\nthat can be expensive. These aspects can put off uninitiated archaeologists. So\nit would be appropriate to see if these techniques could be generalized to a\nwhole class of geometrically simple and common artifacts, such as ceramics.\nFrom these observations, associated to ceramics specialists with fragments of\nbroken ceramics, we aimed at arranging different tools and methods, including\nphotogrammetry, to explore opportunities for a cheap and attainable\nreconstruction methodology and its possible applications. Our first objective\nwas to establish a protocol for scanning fragments with photogrammetry, and for\nreconstruction of original ceramics. We used the digital reconstitutions of the\nceramics we got following our process to calculate some metrics and to design\nand 3D print a display for the remaining fragments of one pottery.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.1330v1"
    },
    {
        "title": "Merging of Bézier curves with box constraints",
        "authors": [
            "Przemysław Gospodarczyk",
            "Paweł Woźny"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  In this paper, we present a novel approach to the problem of merging of\nB\\'ezier curves with respect to the $L_2$-norm. We give illustrative examples\nto show that the solution of the conventional merging problem may not be\nsuitable for further modification and applications. As in the case of the\ndegree reduction problem, we apply the so-called restricted area approach --\nproposed recently in (P. Gospodarczyk, Computer-Aided Design 62 (2015),\n143--151) -- to avoid certain defects and make the resulting curve more useful.\nA method of solving the new problem is based on box-constrained quadratic\nprogramming approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.3841v6"
    },
    {
        "title": "Interactive Visual Exploration of Halos in Large Scale Cosmology\n  Simulation",
        "authors": [
            "Guihua Shan",
            "Maojin Xie",
            "FengAn Li",
            "Yang Gao",
            "Xuebin Chi"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Halo is one of the most important basic elements in cosmology simulation,\nwhich merges from small clumps to ever larger objects. The processes of the\nbirth and merging of the halos play a fundamental role in studying the\nevolution of large scale cosmological structures. In this paper, a visual\nanalysis system is developed to interactively identify and explore the\nevolution histories of thousands of halos. In this system, an intelligent\nstructure-aware selection method in What You See Is What You Get manner is\ndesigned to efficiently define the interesting region in 3D space with 2D\nhand-drawn lasso input. Then the exact information of halos within this 3D\nregion is identified by data mining in the merger tree files. To avoid visual\nclutter, all the halos are projected in 2D space with a MDS method. Through the\nlinked view of 3D View and 2D graph, Users can interactively explore these\nhalos, including the tracing path and evolution history tree.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.7780v1"
    },
    {
        "title": "Marching Surfaces: Isosurface Approximation using G$^1$ Multi-Sided\n  Surfaces",
        "authors": [
            "Gustavo Chávez",
            "Alyn Rockwood"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Marching surfaces is a method for isosurface extraction and approximation\nbased on a $G^1$ multi-sided patch interpolation scheme. Given a 3D grid of\nscalar values, an underlying curve network is formed using second order\ninformation and cubic Hermite splines. Circular arc fitting defines the tangent\nvectors for the Hermite curves at specified isovalues. Once the boundary curve\nnetwork is formed, a loop of curves is determined for each grid cell and then\ninterpolated with multi-sided surface patches, which are $G^1$ continuous at\nthe joins. The data economy of the method and its continuity preserving\nproperties provide an effective compression scheme, ideal for indirect volume\nrendering on mobile devices, or collaborating on the Internet, while enhancing\nvisual fidelity. The use of multi-sided patches enables a more natural way to\napproximate the isosurfaces than using a fixed number of sides or polygons as\nis proposed in the literature. This assertion is supported with comparisons to\nthe traditional Marching Cubes algorithm and other $G^1$ methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.02139v1"
    },
    {
        "title": "Avatar-independent scripting for real-time gesture animation",
        "authors": [
            "Richard Kennaway"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  When animation of a humanoid figure is to be generated at run-time, instead\nof by replaying pre-composed motion clips, some method is required of\nspecifying the avatar's movements in a form from which the required motion data\ncan be automatically generated. This form must be of a more abstract nature\nthan raw motion data: ideally, it should be independent of the particular\navatar's proportions, and both writable by hand and suitable for automatic\ngeneration from higher-level descriptions of the required actions.\n  We describe here the development and implementation of such a scripting\nlanguage for the particular area of sign languages of the deaf, called SiGML\n(Signing Gesture Markup Language), based on the existing HamNoSys notation for\nsign languages.\n  We conclude by suggesting how this work may be extended to more general\nanimation for interactive virtual reality applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.02961v1"
    },
    {
        "title": "Sketch-based Shape Retrieval using Pyramid-of-Parts",
        "authors": [
            "Changqing Zou",
            "Zhe Huang",
            "Rynson W. H. Lau",
            "Jianzhuang Liu",
            "Hongbo Fu"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We present a multi-scale approach to sketch-based shape retrieval. It is\nbased on a novel multi-scale shape descriptor called Pyramidof- Parts, which\nencodes the features and spatial relationship of the semantic parts of query\nsketches. The same descriptor can also be used to represent 2D projected views\nof 3D shapes, allowing effective matching of query sketches with 3D shapes\nacross multiple scales. Experimental results show that the proposed method\noutperforms the state-of-the-art method, whether the sketch segmentation\ninformation is obtained manually or automatically by considering each stroke as\na semantic part.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.04232v1"
    },
    {
        "title": "Relative Squared Distances to a Conic Berserkless 8-Connected Midpoint\n  Algorithm",
        "authors": [
            "Valere Huypens"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  The midpoint method or technique is a measurement and as each measurement it\nhas a tolerance, but worst of all it can be invalid, called Out-of-Control or\nOoC. The core of all midpoint methods is the accurate measurement of the\ndifference of the squared distances of two points to the polar of their\nmidpoint with respect to the conic. When this measurement is valid, it also\nmeasures the difference of the squared distances of these points to the conic,\nalthough it may be inaccurate, called Out-of-Accuracy or OoA. The primary\ncondition is the necessary and sufficient condition that a measurement is\nvalid. It is comletely new and it can be checked ultra fast and before the\nactual measurement starts. Modeling an incremental algorithm, shows that the\ncurve must be subdivided into piecewise monotonic sections, the start point\nmust be optimal, and it explains that the 2D-incremental method can find,\nlocally, the global Least Square Distance. Locally means that there are at most\nthree candidate points for a given monotonic direction; therefore the\n2D-midpoint method has, locally, at most three measurements. When all the\npossible measurements are invalid, the midpoint method cannot be applied, and\nin that case the ultra fast OoC-rule selects the candidate point. This\nguarantees, for the first time, a 100% stable, ultra-fast, berserkless midpoint\nalgorithm, which can be easily transformed to hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.04268v1"
    },
    {
        "title": "Analysis of Design Principles and Requirements for Procedural Rigging of\n  Bipeds and Quadrupeds Characters with Custom Manipulators for Animation",
        "authors": [
            "Zeeshan Bhatti",
            "Asadullah Shah",
            "Ahmad Waqas",
            "Nadeem Mahmood"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Character rigging is a process of endowing a character with a set of custom\nmanipulators and controls making it easy to animate by the animators. These\ncontrols consist of simple joints, handles, or even separate character\nselection windows.This research paper present an automated rigging system for\nquadruped characters with custom controls and manipulators for animation.The\nfull character rigging mechanism is procedurally driven based on various\nprinciples and requirements used by the riggers and animators. The automation\nis achieved initially by creating widgets according to the character type.\nThese widgets then can be customized by the rigger according to the character\nshape, height and proportion. Then joint locations for each body parts are\ncalculated and widgets are replaced programmatically.Finally a complete and\nfully operational procedurally generated character control rig is created and\nattached with the underlying skeletal joints. The functionality and feasibility\nof the rig was analyzed from various source of actual character motion and a\nrequirements criterion was met. The final rigged character provides an\nefficient and easy to manipulate control rig with no lagging and at high frame\nrate.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.06419v1"
    },
    {
        "title": "Data-Driven Shape Analysis and Processing",
        "authors": [
            "Kai Xu",
            "Vladimir G. Kim",
            "Qixing Huang",
            "Evangelos Kalogerakis"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Data-driven methods play an increasingly important role in discovering\ngeometric, structural, and semantic relationships between 3D shapes in\ncollections, and applying this analysis to support intelligent modeling,\nediting, and visualization of geometric data. In contrast to traditional\napproaches, a key feature of data-driven approaches is that they aggregate\ninformation from a collection of shapes to improve the analysis and processing\nof individual shapes. In addition, they are able to learn models that reason\nabout properties and relationships of shapes without relying on hard-coded\nrules or explicitly programmed instructions. We provide an overview of the main\nconcepts and components of these techniques, and discuss their application to\nshape classification, segmentation, matching, reconstruction, modeling and\nexploration, as well as scene analysis and synthesis, through reviewing the\nliterature and relating the existing works with both qualitative and numerical\ncomparisons. We conclude our report with ideas that can inspire future research\nin data-driven shape analysis and processing.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.06686v1"
    },
    {
        "title": "On Integrating Information Visualization Techniques into Data Mining: A\n  Review",
        "authors": [
            "Keqian Li"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  The exploding growth of digital data in the information era and its\nimmeasurable potential value has called for different types of data-driven\ntechniques to exploit its value for further applications. Information\nvisualization and data mining are two research field with such goal. While the\ntwo communities advocates different approaches of problem solving, the vision\nof combining the sophisticated algorithmic techniques from data mining as well\nas the intuitivity and interactivity of information visualization is tempting.\nIn this paper, we attempt to survey recent researches and real world systems\nintegrating the wisdom in two fields towards more effective and efficient data\nanalytics. More specifically, we study the intersection from a data mining\npoint of view, explore how information visualization can be used to complement\nand improve different stages of data mining through established theories for\noptimized visual presentation as well as practical toolsets for rapid\ndevelopment. We organize the survey by identifying three main stages of typical\nprocess of data mining, the preliminary analysis of data, the model\nconstruction, as well as the model evaluation, and study how each stage can\nbenefit from information visualization.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00202v1"
    },
    {
        "title": "Interactive Illustrative Line Styles and Line Style Transfer Functions\n  for Flow Visualization",
        "authors": [
            "Maarten H. Everts",
            "Henk Bekker",
            "Jos B. T. M. Roerdink",
            "Tobias Isenberg"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We present a flexible illustrative line style model for the visualization of\nstreamline data. Our model partitions view-oriented line strips into parallel\nbands whose basic visual properties can be controlled independently. We thus\nextend previous line stylization techniques specifically for visualization\npurposes by allowing the parametrization of these bands based on the local line\ndata attributes. Moreover, our approach supports emphasis and abstraction by\nintroducing line style transfer functions that map local line attribute values\nto complete line styles. With a flexible GPU implementation of this line style\nmodel we enable the interactive exploration of visual representations of\nstreamlines. We demonstrate the effectiveness of our model by applying it to 3D\nflow field datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.05787v2"
    },
    {
        "title": "3D Density Histograms for Criteria-driven Edge Bundling",
        "authors": [
            "Daniel C. Moura"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  This paper presents a graph bundling algorithm that agglomerates edges taking\ninto account both spatial proximity as well as user-defined criteria in order\nto reveal patterns that were not perceivable with previous bundling techniques.\nEach edge belongs to a group that may either be an input of the problem or\nfound by clustering one or more edge properties such as origin, destination,\norientation, length or domain-specific properties. Bundling is driven by a\nstack of density maps, with each map capturing both the edge density of a given\ngroup as well as interactions with edges from other groups. Density maps are\nefficiently calculated by smoothing 2D histograms of edge occurrence using\nrepeated averaging filters based on integral images.\n  A CPU implementation of the algorithm is tested on several graphs, and\ndifferent grouping criteria are used to illustrate how the proposed technique\ncan render different visualizations of the same data. Bundling performance is\nmuch higher than on previous approaches, being particularly noticeable on large\ngraphs, with millions of edges being bundled in seconds.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.02687v1"
    },
    {
        "title": "Real-time Tool for Affine Transformations of Two Dimensional IFS\n  Fractals",
        "authors": [
            "Elena Hadzieva",
            "Marija Shuminoska"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  This work introduces a novel tool for interactive, real-time transformations\nof two dimensional IFS fractals. We assign barycentric coordinates (relative to\nan arbitrary affine basis of $\\mathbb{R}^2$) to the points that constitute the\nimage of a fractal. The tool uses some of the nice properties of the\nbarycentric coordinates, enabling any affine transformation of the basis, done\nby click-and-drag, to be immediately followed by the same affine transformation\nof the IFS fractal attractor. In order to have a better control over the\nfractal, as affine basis we use a kind of minimal simplex that contains the\nattractor. We give theoretical grounds of the tool and then the software\napplication.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.02744v1"
    },
    {
        "title": "Massively Parallel Ray Tracing Algorithm Using GPU",
        "authors": [
            "Yutong Qin",
            "Jianbiao Lin",
            "Xiang Huang"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Ray tracing is a technique for generating an image by tracing the path of\nlight through pixels in an image plane and simulating the effects of\nhigh-quality global illumination at a heavy computational cost. Because of the\nhigh computation complexity, it can't reach the requirement of real-time\nrendering. The emergence of many-core architectures, makes it possible to\nreduce significantly the running time of ray tracing algorithm by employing the\npowerful ability of floating point computation. In this paper, a new GPU\nimplementation and optimization of the ray tracing to accelerate the rendering\nprocess is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.03151v1"
    },
    {
        "title": "Real-time correction of panoramic images using hyperbolic Möbius\n  transformations",
        "authors": [
            "Luis Peñaranda",
            "Luiz Velho",
            "Leonardo Sacht"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Wide-angle images gained a huge popularity in the last years due to the\ndevelopment of computational photography and imaging technological advances.\nThey present the information of a scene in a way which is more natural for the\nhuman eye but, on the other hand, they introduce artifacts such as bent lines.\nThese artifacts become more and more unnatural as the field of view increases.\n  In this work, we present a technique aimed to improve the perceptual quality\nof panorama visualization. The main ingredients of our approach are, on one\nhand, considering the viewing sphere as a Riemann sphere, what makes natural\nthe application of M\\\"obius (complex) transformations to the input image, and,\non the other hand, a projection scheme which changes in function of the field\nof view used.\n  We also introduce an implementation of our method, compare it against images\nproduced with other methods and show that the transformations can be done in\nreal-time, which makes our technique very appealing for new settings, as well\nas for existing interactive panorama applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.04565v1"
    },
    {
        "title": "Bijective Deformations in $\\mathbb{R}^n$ via Integral Curve Coordinates",
        "authors": [
            "Lisa Huynh",
            "Yotam Gingold"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We introduce Integral Curve Coordinates, which identify each point in a\nbounded domain with a parameter along an integral curve of the gradient of a\nfunction $f$ on that domain; suitable functions have exactly one critical\npoint, a maximum, in the domain, and the gradient of the function on the\nboundary points inward. Because every integral curve intersects the boundary\nexactly once, Integral Curve Coordinates provide a natural bijective mapping\nfrom one domain to another given a bijection of the boundary. Our approach can\nbe applied to shapes in any dimension, provided that the boundary of the shape\n(or cage) is topologically equivalent to an $n$-sphere. We present a simple\nalgorithm for generating a suitable function space for $f$ in any dimension. We\ndemonstrate our approach in 2D and describe a practical (simple and robust)\nalgorithm for tracing integral curves on a (piecewise-linear) triangulated\nregular grid.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.00073v1"
    },
    {
        "title": "B$\\acute{e}$zier curves based on Lupaş $(p,q)$-analogue of Bernstein\n  polynomials in CAGD",
        "authors": [
            "Khalid Khan",
            "D. K. Lobiyal"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  In this paper, we use the blending functions of Lupa\\c{s} type (rational)\n$(p,q)$-Bernstein operators based on $(p,q)$-integers for construction of\nLupa\\c{s} $(p,q)$-B$\\acute{e}$zier curves (rational curves) and surfaces\n(rational surfaces) with shape parameters. We study the nature of degree\nelevation and degree reduction for Lupa\\c{s} $(p,q)$-B$\\acute{e}$zier Bernstein\nfunctions. Parametric curves are represented using Lupa\\c{s} $(p,q)$-Bernstein\nbasis. We introduce affine de Casteljau algorithm for Lupa\\c{s} type\n$(p,q)$-Bernstein B$\\acute{e}$zier curves. The new curves have some properties\nsimilar to $q$-B$\\acute{e}$zier curves. Moreover, we construct the\ncorresponding tensor product surfaces over the rectangular domain $(u, v) \\in\n[0, 1] \\times [0, 1] $ depending on four parameters. We also study the de\nCasteljau algorithm and degree evaluation properties of the surfaces for these\ngeneralization over the rectangular domain. We get $q$-B$\\acute{e}$zier\nsurfaces for $(u, v) \\in [0, 1] \\times [0, 1] $ when we set the parameter\n$p_1=p_2=1.$ In comparison to $q$-B$\\acute{e}$zier curves and surfaces based on\nLupa\\c{s} $q$-Bernstein polynomials, our generalization gives us more\nflexibility in controlling the shapes of curves and surfaces.\n  We also show that the $(p,q)$-analogue of Lupa\\c{s} Bernstein operator\nsequence $L^{n}_{p_n,q_n}(f,x)$ converges uniformly to $f(x)\\in C[0,1]$ if and\nonly if $0<q_n<p_n\\leq1$ such that $\\lim\\limits_{n\\to\\infty} q_n=1, $\n$\\lim\\limits_{n\\to\\infty} p_n=1$ and $\\lim\\limits_{n\\to\\infty}p_n^n=a,$\n$\\lim\\limits_{n\\to\\infty}q_n^n=b$ with $0<a,b\\leq1.$ On the other hand, for any\n$p>0$ fixed and $p \\neq 1,$ the sequence $L^{n}_{p,q}(f,x)$ converges uniformly\nto $f(x)~ \\in C[0,1]$ if and only if $f(x)=ax+b$ for some $a, b \\in\n\\mathbb{R}.$\n",
        "pdf_link": "http://arxiv.org/pdf/1505.01810v5"
    },
    {
        "title": "A Connectivity-Aware Multi-level Finite-Element System for Solving\n  Laplace-Beltrami Equations",
        "authors": [
            "Ming Chuang",
            "Michael Kazhdan"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Recent work on octree-based finite-element systems has developed a multigrid\nsolver for Poisson equations on meshes. While the idea of defining a regularly\nindexed function space has been successfully used in a number of applications,\nit has also been noted that the richness of the function space is limited\nbecause the function values can be coupled across locally disconnected regions.\nIn this work, we show how to enrich the function space by introducing functions\nthat resolve the coupling while still preserving the nesting hierarchy that\nsupports multigrid. A spectral analysis reveals the superior quality of the\nresulting Laplace-Beltrami operator and applications to surface flow\ndemonstrate that our new solver more efficiently converges to the correct\nsolution.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.03615v1"
    },
    {
        "title": "A wide diversity of 3D surfaces Generator using a new implicit function",
        "authors": [
            "Jelloul Elmesbahi",
            "Ahmed Errami",
            "Mohammed Khaldoun",
            "Omar Bouattane"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We present in this paper a new family of implicit function for synthesizing a\nwide variety of 3D surfaces. The basis of this family consists of the usual\nfunctions that are: the function rectangular pulses, the function saw-tooth\npulses, the function of triangular pulses, the staircase function and the power\nfunction. By combining these common functions, named constituent functions, in\none implicit function and by varying some parameters of this function we can\nsynthesize a wide variety of 3D surfaces with the possibility to set their\ndeformations.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.03977v2"
    },
    {
        "title": "Implementing a Photorealistic Rendering System using GLSL",
        "authors": [
            "Toshiya Hachisuka"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Ray tracing on GPUs is becoming quite common these days. There are many\npublicly available documents on how to implement basic ray tracing on GPUs for\nspheres and implicit surfaces. We even have some general frameworks for ray\ntracing on GPUs. We however hardly find details on how to implement more\ncomplex ray tracing algorithms themselves that are commonly used for\nphotorealistic rendering. This paper explains an implementation of a\nstand-alone rendering system on GPUs which supports the bounding volume\nhierarchy and stochastic progressive photon mapping. The key characteristic of\nthe system is that it uses only GLSL shaders without relying on any platform\ndependent feature. The system can thus run on many platforms that support\nOpenGL, making photorealistic rendering on GPUs widely accessible. This paper\nalso sketches practical ideas for stackless traversal and pseudorandom number\ngeneration which both fit well with the limited system configuration.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.06022v1"
    },
    {
        "title": "A survey on Information Visualization in light of Vision and Cognitive\n  sciences",
        "authors": [
            "Jose Rodrigues-Jr",
            "Luciana Zaina",
            "Maria Oliveira",
            "Bruno Brandoli",
            "Agma Traina"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Information Visualization techniques are built on a context with many factors\nrelated to both vision and cognition, making it difficult to draw a clear\npicture of how data visually turns into comprehension. In the intent of\npromoting a better picture, here, we survey concepts on vision, cognition, and\nInformation Visualization organized in a theorization named Visual Expression\nProcess. Our theorization organizes the basis of visualization techniques with\na reduced level of complexity; still, it is complete enough to foster\ndiscussions related to design and analytical tasks. Our work introduces the\nfollowing contributions: (1) a Theoretical compilation of vision, cognition,\nand Information Visualization; (2) Discussions supported by vast literature;\nand (3) Reflections on visual-cognitive aspects concerning use and design. We\nexpect our contributions will provide further clarification about how users and\ndesigners think about InfoVis, leveraging the potential of systems and\ntechniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.07079v2"
    },
    {
        "title": "The Spatial-Perceptual Design Space: a new comprehension for Data\n  Visualization",
        "authors": [
            "Jose F. Rodrigues Jr",
            "Agma J. M. Traina",
            "Maria C. F. Oliveira",
            "Caetano Traina Jr"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We revisit the design space of visualizations aiming at identifying and\nrelating its components. In this sense, we establish a model to examine the\nprocess through which visualizations become expressive for users. This model\nhas leaded us to a taxonomy oriented to the human visual perception, a\nconceptualization that provides natural criteria in order to delineate a novel\nunderstanding for the visualization design space. The new organization of\nconcepts that we introduce is our main contribution: a grammar for the\nvisualization design based on the review of former works and of classical and\nstate-of-the-art techniques. Like so, the paper is presented as a survey whose\nstructure introduces a new conceptualization for the space of techniques\nconcerning visual analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.07804v1"
    },
    {
        "title": "A Hybrid Graph-drawing Algorithm for Large, Naturally-clustered,\n  Disconnected Graphs",
        "authors": [
            "Toni-Jan Keith P. Monserrat",
            "Jaderick P. Pabico",
            "Eliezer A. Albacea"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  In this paper, we present a hybrid graph-drawing algorithm (GDA) for\nlayouting large, naturally-clustered, disconnected graphs. We called it a\nhybrid algorithm because it is an implementation of a series of already known\ngraph-drawing and graph-theoretic procedures. We remedy in this hybrid the\nproblematic nature of the current force-based GDA which has the inability to\nscale to large, naturally-clustered, and disconnected graphs. These kinds of\ngraph usually model the complex inter-relationships among entities in social,\nbiological, natural, and artificial networks. Obviously, the hybrid runs longer\nthan the current GDAs. By using two extreme cases of graphs as inputs, we\npresent in this paper the derivation of the time complexity of the hybrid which\nwe found to be $O(|\\V|^3)$.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.02766v1"
    },
    {
        "title": "Meshfree C^2-Weighting for Shape Deformation",
        "authors": [
            "Chuhua Xian",
            "Shuo Jin",
            "Charlie C. L. Wang"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Handle-driven deformation based on linear blending is widely used in many\napplications because of its merits in intuitiveness, efficiency and easiness of\nimplementation. We provide a meshfree method to compute the smooth weights of\nlinear blending for shape deformation. The C2-continuity of weighting is\nguaranteed by the carefully formulated basis functions, with which the\ncomputation of weights is in a closed-form. Criteria to ensure the quality of\ndeformation are preserved by the basis functions after decomposing the shape\ndomain according to the Voronoi diagram of handles. The cost of inserting a new\nhandle is only the time to evaluate the distances from the new handle to all\nsample points in the space of deformation. Moreover, a virtual handle insertion\nalgorithm has been developed to allow users freely placing handles while\npreserving the criteria on weights. Experimental examples for real-time 2D/3D\ndeformations are shown to demonstrate the effectiveness of this method.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.02800v1"
    },
    {
        "title": "A Closed-Form Formulation of HRBF-Based Surface Reconstruction",
        "authors": [
            "Shengjun Liu",
            "Charlie C. L. Wang",
            "Guido Brunnett",
            "Jun Wang"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  The Hermite radial basis functions (HRBFs) implicits have been used to\nreconstruct surfaces from scattered Hermite data points. In this work, we\npropose a closed-form formulation to construct HRBF-based implicits by a\nquasi-solution approximating the exact solution. A scheme is developed to\nautomatically adjust the support sizes of basis functions to hold the error\nbound of a quasi-solution. Our method can generate an implicit function from\npositions and normals of scattered points without taking any global operation.\nWorking together with an adaptive sampling algorithm, the HRBF-based implicits\ncan also reconstruct surfaces from point clouds with non-uniformity and noises.\nRobust and efficient reconstruction has been observed in our experimental tests\non real data captured from a variety of scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.02860v1"
    },
    {
        "title": "On Smooth 3D Frame Field Design",
        "authors": [
            "Nicolas Ray",
            "Dmitry Sokolov"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We analyze actual methods that generate smooth frame fields both in 2D and in\n3D. We formalize the 2D problem by representing frames as functions (as it was\ndone in 3D), and show that the derived optimization problem is the one that\nprevious work obtain via \"representation vectors.\" We show (in 2D) why this non\nlinear optimization problem is easier to solve than directly minimizing the\nrotation angle of the field, and observe that the 2D algorithm is able to find\ngood fields.\n  Now, the 2D and the 3D optimization problems are derived from the same\nformulation (based on representing frames by functions). Their energies share\nsome similarities from an optimization point of view (smoothness, local minima,\nbounds of partial derivatives, etc.), so we applied the 2D resolution mechanism\nto the 3D problem. Our evaluation of all existing 3D methods suggests to\ninitialize the field by this new algorithm, but possibly use another method for\nfurther smoothing.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.03351v1"
    },
    {
        "title": "A de Casteljau Algorithm for Bernstein type Polynomials based on\n  (p,q)-integers",
        "authors": [
            "Khalid Khan",
            "D. K. Lobiyal",
            "Adem Kilicman"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  In this paper, a de Casteljau algorithm to compute (p,q)-Bernstein Bezier\ncurves based on (p,q)-integers is introduced. We study the nature of degree\nelevation and degree reduction for (p,q)-Bezier Bernstein functions. The new\ncurves have some properties similar to q-Bezier curves. Moreover, we construct\nthe corresponding tensor product surfaces over the rectangular domain (u, v)\n\\in [0, 1] \\times [0, 1] depending on four parameters. We also study the de\nCasteljau algorithm and degree evaluation properties of the surfaces for these\ngeneralization over the rectangular domain. Furthermore, some fundamental\nproperties for (p,q)-Bernstein Bezier curves are discussed. We get q-Bezier\ncurves and surfaces for (u, v) \\in [0, 1] \\times [0, 1] when we set the\nparameter p1 = p2 = 1.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04110v4"
    },
    {
        "title": "A concise parametrisation of affine transformation",
        "authors": [
            "Shizuo Kaji",
            "Hiroyuki Ochiai"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Good parametrisations of affine transformations are essential to\ninterpolation, deformation, and analysis of shape, motion, and animation. It\nhas been one of the central research topics in computer graphics. However,\nthere is no single perfect method and each one has both advantages and\ndisadvantages. In this paper, we propose a novel parametrisation of affine\ntransformations, which is a generalisation to or an improvement of existing\nmethods. Our method adds yet another choice to the existing toolbox and shows\nbetter performance in some applications. A C++ implementation is available to\nmake our framework ready to use in various applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.05290v2"
    },
    {
        "title": "Inappropriate use of L-BFGS, Illustrated on frame field design",
        "authors": [
            "Nicolas Ray",
            "Dmitry Sokolov"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  L-BFGS is a hill climbing method that is guarantied to converge only for\nconvex problems. In computer graphics, it is often used as a black box solver\nfor a more general class of non linear problems, including problems having many\nlocal minima. Some works obtain very nice results by solving such difficult\nproblems with L-BFGS. Surprisingly, the method is able to escape local minima:\nour interpretation is that the approximation of the Hessian is smoother than\nthe real Hessian, making it possible to evade the local minima. We analyse the\nbehavior of L-BFGS on the design of 2D frame fields. It involves an energy\nfunction that is infinitly continuous, strongly non linear and having many\nlocal minima. Moreover, the local minima have a clear visual interpretation:\nthey corresponds to differents frame field topologies. We observe that the\nperformances of LBFGS are almost unpredictables: they are very competitive when\nthe field is sampled on the primal graph, but really poor when they are sampled\non the dual graph.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.02826v1"
    },
    {
        "title": "The Prose Storyboard Language: A Tool for Annotating and Directing\n  Movies",
        "authors": [
            "Remi Ronfard",
            "Vineet Gandhi",
            "Laurent Boiron",
            "Vaishnavi Ameya Murukutla"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  The prose storyboard language is a formal language for describing movies shot\nby shot, where each shot is described with a unique sentence. The language uses\na simple syntax and limited vocabulary borrowed from working practices in\ntraditional movie-making, and is intended to be readable both by machines and\nhumans. The language is designed to serve as a high-level user interface for\nintelligent cinematography and editing systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.07593v5"
    },
    {
        "title": "Decomposing Digital Paintings into Layers via RGB-space Geometry",
        "authors": [
            "Jianchao Tan",
            "Jyh-Ming Lien",
            "Yotam Gingold"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  In digital painting software, layers organize paintings. However, layers are\nnot explicitly represented, transmitted, or published with the final digital\npainting. We propose a technique to decompose a digital painting into layers.\nIn our decomposition, each layer represents a coat of paint of a single paint\ncolor applied with varying opacity throughout the image. Our decomposition is\nbased on the painting's RGB-space geometry. In RGB-space, a geometric structure\nis revealed due to the linear nature of the standard Porter-Duff \"over\" pixel\ncompositing operation. The vertices of the convex hull of pixels in RGB-space\nsuggest paint colors. Users choose the degree of simplification to perform on\nthe convex hull, as well as a layer order for the colors. We solve a\nconstrained optimization problem to find maximally translucent, spatially\ncoherent opacity for each layer, such that the composition of the layers\nreproduces the original image. We demonstrate the utility of the resulting\ndecompositions for re-editing.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.03335v1"
    },
    {
        "title": "Good Colour Maps: How to Design Them",
        "authors": [
            "Peter Kovesi"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Many colour maps provided by vendors have highly uneven perceptual contrast\nover their range. It is not uncommon for colour maps to have perceptual flat\nspots that can hide a feature as large as one tenth of the total data range.\nColour maps may also have perceptual discontinuities that induce the appearance\nof false features. Previous work in the design of perceptually uniform colour\nmaps has mostly failed to recognise that CIELAB space is only designed to be\nperceptually uniform at very low spatial frequencies. The most important factor\nin designing a colour map is to ensure that the magnitude of the incremental\nchange in perceptual lightness of the colours is uniform. The specific\nrequirements for linear, diverging, rainbow and cyclic colour maps are\ndeveloped in detail. To support this work two test images for evaluating colour\nmaps are presented. The use of colour maps in combination with relief shading\nis considered and the conditions under which colour can enhance or disrupt\nrelief shading are identified. Finally, a set of new basis colours for the\nconstruction of ternary images are presented. Unlike the RGB primaries these\nbasis colours produce images whereby the salience of structures are consistent\nirrespective of the assignment of basis colours to data channels.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.03700v1"
    },
    {
        "title": "Elements of Validation of Artificial Lighting through the Software\n  CODYRUN: Application to a Test Case of the International Commission on\n  Illumination (CIE)",
        "authors": [
            "Ali Hamada Fakra",
            "Frédéric Miranville",
            "Dimitri Bigot",
            "Harry Boyer"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  CODYRUN is a software for computational aeraulic and thermal simulation in\nbuildings developed by the Laboratory of Building Physics and Systems\n(L.P.B.S). Numerical simulation codes of artificial lighting have been\nintroduced to extend the tool capacity. These calculation codes are able to\npredict the amount of light received by any point of a given working plane and\nfrom one or more sources installed on the ceiling of the room. The model used\nfor these calculations is original and semi-detailed (simplified). The test\ncase references of the task-3 TC-33 International Commission on Illumination\n(CIE) were applied to the software to ensure reliability to properly handle\nthis photometric aspect. This allowed having a precise idea about the\nreliability of the results of numerical simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.05330v1"
    },
    {
        "title": "Printed Perforated Lampshades for Continuous Projective Images",
        "authors": [
            "Haisen Zhao",
            "Lin Lu",
            "Yuan Wei",
            "Dani Lischinski",
            "Andrei Sharf",
            "Daniel Cohen-Or",
            "Baoquan Chen"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We present a technique for designing 3D-printed perforated lampshades, which\nproject continuous grayscale images onto the surrounding walls. Given the\ngeometry of the lampshade and a target grayscale image, our method computes a\ndistribution of tiny holes over the shell, such that the combined footprints of\nthe light emanating through the holes form the target image on a nearby diffuse\nsurface. Our objective is to approximate the continuous tones and the spatial\ndetail of the target image, to the extent possible within the constraints of\nthe fabrication process.\n  To ensure structural integrity, there are lower bounds on the thickness of\nthe shell, the radii of the holes, and the minimal distances between adjacent\nholes. Thus, the holes are realized as thin tubes distributed over the\nlampshade surface. The amount of light passing through a single tube may be\ncontrolled by the tube's radius and by its direction (tilt angle). The core of\nour technique thus consists of determining a suitable configuration of the\ntubes: their distribution across the relevant portion of the lampshade, as well\nas the parameters (radius, tilt angle) of each tube. This is achieved by\ncomputing a capacity-constrained Voronoi tessellation over a suitably defined\ndensity function, and embedding a tube inside the maximal inscribed circle of\neach tessellation cell. The density function for a particular target image is\nderived from a series of simulated images, each corresponding to a different\nuniform density tube pattern on the lampshade.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.03023v1"
    },
    {
        "title": "Surface Approximation via Asymptotic Optimal Geometric Partition",
        "authors": [
            "Yiqi Cai",
            "Xiaohu Guo",
            "Yang Liu",
            "Wenping Wang",
            "Weihua Mao",
            "Zichun Zhong"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  In this paper, we present a surface remeshing method with high approximation\nquality based on Principal Component Analysis. Given a triangular mesh and a\nuser assigned polygon/vertex budget, traditional methods usually require the\nextra curvature metric field for the desired anisotropy to best approximate the\nsurface, even though the estimated curvature metric is known to be imperfect\nand already self-contained in the surface. In our approach, this anisotropic\ncontrol is achieved through the optimal geometry partition without this\nexplicit metric field. The minimization of our proposed partition energy has\nthe following properties: Firstly, on a C2 surface, it is theoretically\nguaranteed to have the optimal aspect ratio and cluster size as specified in\napproximation theory for L1 piecewise linear approximation. Secondly, it\ncaptures sharp features on practical models without any pre-tagging. We develop\nan effective merging-swapping framework to seek the optimal partition and\nconstruct polygonal/triangular mesh afterwards. The effectiveness and\nefficiency of our method are demonstrated through the comparison with other\nstate-of-the-art remeshing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.03935v2"
    },
    {
        "title": "Smooth surface interpolation using patches with rational offsets",
        "authors": [
            "Miroslav Lávička",
            "Zbyněk Šír",
            "Jan Vršek"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We present a new method for the interpolation of given data points and\nassociated normals with surface parametric patches with rational normal fields.\nWe give some arguments why a dual approach is the most convenient for these\nsurfaces, which are traditionally called Pythagorean normal vector (PN)\nsurfaces. Our construction is based on the isotropic model of the dual space to\nwhich the original data are pushed. Then the bicubic Coons patches are\nconstructed in the isotropic space and then pulled back to the standard three\ndimensional space. As a result we obtain the patch construction which is\ncompletely local and produces surfaces with the global G1~continuity.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.01224v2"
    },
    {
        "title": "Effective Clipart Image Vectorization Through Direct Optimization of\n  Bezigons",
        "authors": [
            "Ming Yang",
            "Hongyang Chao",
            "Chi Zhang",
            "Jun Guo",
            "Lu Yuan",
            "Jian Sun"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Bezigons, i.e., closed paths composed of B\\'ezier curves, have been widely\nemployed to describe shapes in image vectorization results. However, most\nexisting vectorization techniques infer the bezigons by simply approximating an\nintermediate vector representation (such as polygons). Consequently, the\nresultant bezigons are sometimes imperfect due to accumulated errors, fitting\nambiguities, and a lack of curve priors, especially for low-resolution images.\nIn this paper, we describe a novel method for vectorizing clipart images. In\ncontrast to previous methods, we directly optimize the bezigons rather than\nusing other intermediate representations; therefore, the resultant bezigons are\nnot only of higher fidelity compared with the original raster image but also\nmore reasonable because they were traced by a proficient expert. To enable such\noptimization, we have overcome several challenges and have devised a\ndifferentiable data energy as well as several curve-based prior terms. To\nimprove the efficiency of the optimization, we also take advantage of the local\ncontrol property of bezigons and adopt an overlapped piecewise optimization\nstrategy. The experimental results show that our method outperforms both the\ncurrent state-of-the-art method and commonly used commercial software in terms\nof bezigon quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.01913v1"
    },
    {
        "title": "Towards Zero-Waste Furniture Design",
        "authors": [
            "Bongjin Koo",
            "Jean Hergel",
            "Sylvain Lefebvre",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  In traditional design, shapes are first conceived, and then fabricated. While\nthis decoupling simplifies the design process, it can result in inefficient\nmaterial usage, especially where off-cut pieces are hard to reuse. The\ndesigner, in absence of explicit feedback on material usage remains helpless to\neffectively adapt the design -- even though design variabilities exist. In this\npaper, we investigate {\\em waste minimizing furniture design} wherein based on\nthe current design, the user is presented with design variations that result in\nmore effective usage of materials. Technically, we dynamically analyze material\nspace layout to determine {\\em which} parts to change and {\\em how}, while\nmaintaining original design intent specified in the form of design constraints.\nWe evaluate the approach on simple and complex furniture design scenarios, and\ndemonstrate effective material usage that is difficult, if not impossible, to\nachieve without computational support.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.00047v1"
    },
    {
        "title": "On the Hessian of Shape Matching Energy",
        "authors": [
            "Yun Fei"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  In this technical report we derive the analytic form of the Hessian matrix\nfor shape matching energy. Shape matching is a useful technique for meshless\ndeformation, which can be easily combined with multiple techniques in real-time\ndynamics. Nevertheless, it has been rarely applied in scenarios where implicit\nintegrators are required, and hence strong viscous damping effect, though\npopular in simulation systems nowadays, is forbidden for shape matching. The\nreason lies in the difficulty to derive the Hessian matrix of the shape\nmatching energy. Computing the Hessian matrix correctly, and stably, is the key\nto more broadly application of shape matching in implicitly-integrated systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.02483v3"
    },
    {
        "title": "Algorithms and Identities for B$\\acute{e}$zier curves via Post Quantum\n  Blossom",
        "authors": [
            "Alaa Mohammed Obad",
            "Khalid Khan",
            "D. K. Lobiyal",
            "Asif Khan"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  In this paper, a new analogue of blossom based on post quantum calculus is\nintroduced. The post quantum blossom has been adapted for developing identities\nand algorithms for Bernstein bases and B$\\acute{e}$zier curves. By applying the\npost quantum blossom, various new identities and formulae expressing the\nmonomials in terms of the post quantun Bernstein basis functions and a post\nquantun variant of Marsden's identity are investigated. For each post quantum\nB$\\acute{e}$zier curves of degree $m,$ a collection of $m!$ new, affine\ninvariant, recursive evaluation algorithms are derived.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.03220v2"
    },
    {
        "title": "Towards Real-time Simulation of Hyperelastic Materials",
        "authors": [
            "Tiantian Liu",
            "Sofien Bouaziz",
            "Ladislav Kavan"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We present a new method for real-time physics-based simulation supporting\nmany different types of hyperelastic materials. Previous methods such as\nPosition Based or Projective Dynamics are fast, but support only limited\nselection of materials; even classical materials such as the Neo-Hookean\nelasticity are not supported. Recently, Xu et al. [2015] introduced new\n\"spline-based materials\" which can be easily controlled by artists to achieve\ndesired animation effects. Simulation of these types of materials currently\nrelies on Newton's method, which is slow, even with only one iteration per\ntimestep. In this paper, we show that Projective Dynamics can be interpreted as\na quasi-Newton method. This insight enables very efficient simulation of a\nlarge class of hyperelastic materials, including the Neo-Hookean, spline-based\nmaterials, and others. The quasi-Newton interpretation also allows us to\nleverage ideas from numerical optimization. In particular, we show that our\nsolver can be further accelerated using L-BFGS updates (Limited-memory\nBroyden-Fletcher-Goldfarb-Shanno algorithm). Our final method is typically more\nthan 10 times faster than one iteration of Newton's method without compromising\nquality. In fact, our result is often more accurate than the result obtained\nwith one iteration of Newton's method. Our method is also easier to implement,\nimplying reduced software development costs.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.07378v1"
    },
    {
        "title": "Augmented Reality Oculus Rift",
        "authors": [
            "Markus Höll",
            "Nikolaus Heran",
            "Vincent Lepetit"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  This paper covers the whole process of developing an Augmented Reality\nStereoscopig Render Engine for the Oculus Rift. To capture the real world in\nform of a camera stream, two cameras with fish-eye lenses had to be installed\non the Oculus Rift DK1 hardware. The idea was inspired by Steptoe\n\\cite{steptoe2014presence}. After the introduction, a theoretical part covers\nall the most neccessary elements to achieve an AR System for the Oculus Rift,\nfollowing the implementation part where the code from the AR Stereo Engine is\nexplained in more detail. A short conclusion section shows some results,\nreflects some experiences and in the final chapter some future works will be\ndiscussed. The project can be accessed via the git repository\nhttps://github.com/MaXvanHeLL/ARift.git.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.08848v1"
    },
    {
        "title": "SurfCuit: Surface Mounted Circuits on 3D Prints",
        "authors": [
            "Nobuyuki Umetani",
            "Ryan Schmidt"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We present, SurfCuit, a novel approach to design and construction of electric\ncircuits on the surface of 3D prints. Our surface mounting technique allows\ndurable construction of circuits on the surface of 3D prints. SurfCuit does not\nrequire tedious circuit casing design or expensive set-ups, thus we can\nexpedite the process of circuit construction for 3D models. Our technique\nallows the user to construct complex circuits for consumer-level desktop fused\ndecomposition modeling (FDM) 3D printers. The key idea behind our technique is\nthat FDM plastic forms a strong bond with metal when it is melted. This\nobservation enables construction of a robust circuit traces using copper tape\nand soldering. We also present an interactive tool to design such circuits on\narbitrary 3D geometry. We demonstrate the effectiveness of our approach through\nvarious actual construction examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.09540v1"
    },
    {
        "title": "A Visualization Method of Four Dimensional Polytopes by Oval Display of\n  Parallel Hyperplane Slices",
        "authors": [
            "Akira Kageyama"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  A method to visualize polytopes in a four dimensional euclidian space\n$(x,y,z,w)$ is proposed. A polytope is sliced by multiple hyperplanes that are\nparallel each other and separated by uniform intervals. Since the hyperplanes\nare perpendicular to the $w$ axis, the resulting multiple slices appear in the\nthree-dimensional $(x,y,z)$ space and they are shown by the standard computer\ngraphics. The polytope is rotated extrinsically in the four dimensional space\nby means of a simple input method based on keyboard typings. The multiple\nslices are placed on a parabola curve in the three-dimensional world\ncoordinates. The slices in a view window form an oval appearance. Both the\nsimple and the double rotations in the four dimensional space are applied to\nthe polytope. All slices synchronously change their shapes when a rotation is\napplied to the polytope. The compact display in the oval of many slices with\nthe help of quick rotations facilitate a grasp of the four dimensional\nconfiguration of the polytope.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.01102v1"
    },
    {
        "title": "HoloMed: A Low-Cost Gesture-Based Holographic",
        "authors": [
            "Juan Perozo",
            "Mimia Lo Leung",
            "Esmitt Ramírez"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  During medicine studies, visualization of certain elements is common and\nindispensable in order to get more information about the way they work.\nCurrently, we resort to the use of photographs -which are insufficient due to\nbeing static- or tests in patients, which can be invasive or even risky.\nTherefore, a low-cost approach is proposed by using a 3D visualization. This\npaper presents a holographic system built with low-cost materials for teaching\nobstetrics, where student interaction is performed by using voice and gestures.\nOur solution, which we called HoloMed, is focused on the projection of a\neuthocic normal delivery under a web-based infrastructure which also employs a\nKinect. HoloMed is divided in three (3) essential modules: a gesture analyzer,\na data server, and a holographic projection architecture, which can be executed\nin several interconnected computers using different network protocols. Tests\nused for determining the user's position, illumination factors, and response\ntimes, demonstrate HoloMed's effectiveness as a low-cost system for teaching,\nusing a natural user interface and 3D images.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.05812v1"
    },
    {
        "title": "Efficient Optimal Control of Smoke using Spacetime Multigrid",
        "authors": [
            "Zherong Pan",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We present a novel algorithm to control the physically-based animation of\nsmoke. Given a set of keyframe smoke shapes, we compute a dense sequence of\ncontrol force fields that can drive the smoke shape to match several keyframes\nat certain time instances. Our approach formulates this control problem as a\nPDE constrained spacetime optimization and computes locally optimal control\nforces as the stationary point of the Karush-Kuhn-Tucker conditions. In order\nto reduce the high complexity of multiple passes of fluid resimulation, we\nutilize the coherence between consecutive fluid simulation passes and update\nour solution using a novel spacetime full approximation scheme (STFAS). We\ndemonstrate the benefits of our approach by computing accurate solutions on 2D\nand 3D benchmarks. In practice, we observe more than an order of magnitude\nimprovement over prior methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.01102v1"
    },
    {
        "title": "Geoplotlib: a Python Toolbox for Visualizing Geographical Data",
        "authors": [
            "Andrea Cuttone",
            "Sune Lehmann",
            "Jakob Eg Larsen"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We introduce geoplotlib, an open-source python toolbox for visualizing\ngeographical data. geoplotlib supports the development of hardware-accelerated\ninteractive visualizations in pure python, and provides implementations of dot\nmaps, kernel density estimation, spatial graphs, Voronoi tesselation,\nshapefiles and many more common spatial visualizations. We describe geoplotlib\ndesign, functionalities and use cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.01933v1"
    },
    {
        "title": "A weighted binary average of point-normal pairs with application to\n  subdivision schemes",
        "authors": [
            "Evgeny Lipovetsky",
            "Nira Dyn"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Subdivision is a well-known and established method for generating smooth\ncurves and surfaces from discrete data by repeated refinements. The typical\ninput for such a process is a mesh of vertices. In this work we propose to\nrefine 2D data consisting of vertices of a polygon and a normal at each vertex.\nOur core refinement procedure is based on a circle average, which is a new\nnon-linear weighted average of two points and their corresponding normals. The\nability to locally approximate curves by the circle average is demonstrated.\nWith this ability, the circle average is a candidate for modifying linear\nsubdivision schemes refining points, to schemes refining point-normal pairs.\nThis is done by replacing the weighted binary arithmetic means in a linear\nsubdivision scheme, expressed in terms of repeated binary averages, by circle\naverages with the same weights. Here we investigate the modified\nLane-Riesenfeld algorithm and the 4-point scheme. For the case that the initial\ndata consists of a control polygon only, a naive method for choosing initial\nnormals is proposed. An example demonstrates the superiority of the above two\nmodified schemes, with the naive choice of initial normals over the\ncorresponding linear schemes, when applied to a control polygon with edges of\nsignificantly different lengths.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04082v1"
    },
    {
        "title": "Infill Optimization for Additive Manufacturing -- Approaching Bone-like\n  Porous Structures",
        "authors": [
            "Jun Wu",
            "Niels Aage",
            "Ruediger Westermann",
            "Ole Sigmund"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Porous structures such as trabecular bone are widely seen in nature. These\nstructures exhibit superior mechanical properties whilst being lightweight. In\nthis paper, we present a method to generate bone-like porous structures as\nlightweight infill for additive manufacturing. Our method builds upon and\nextends voxel-wise topology optimization. In particular, for the purpose of\ngenerating sparse yet stable structures distributed in the interior of a given\nshape, we propose upper bounds on the localized material volume in the\nproximity of each voxel in the design domain. We then aggregate the local\nper-voxel constraints by their p-norm into an equivalent global constraint, in\norder to facilitate an efficient optimization process. Implemented on a\nhigh-resolution topology optimization framework, our results demonstrate\nmechanically optimized, detailed porous structures which mimic those found in\nnature. We further show variants of the optimized structures subject to\ndifferent design specifications, and analyze the optimality and robustness of\nthe obtained structures.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04366v2"
    },
    {
        "title": "Adaptive Position-Based Fluids: Improving Performance of Fluid\n  Simulations for Real-Time Applications",
        "authors": [
            "Marcel Köster",
            "Antonio Krüger"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  The Position Based Fluids (PBF) method is a state-of-the-art approach for\nfluid simulations in the context of real-time applications like games. It uses\nan iterative solver concept that tries to maintain a constant fluid density\n(incompressibility) to realize incompressible fluids like water. However,\nlarger fluid volumes that consist of several hundred thousand particles (e.g.\nfor the simulation of oceans) require many iterations and a lot of simulation\npower. We present a lightweight and easy-to-integrate extension to PBF that\nadaptively adjusts the number of solver iterations on a fine-grained basis.\nUsing a novel adaptive-simulation approach, we are able to achieve significant\nimprovements in performance on our evaluation scenarios while maintaining\nhigh-quality results in terms of visualization quality, which makes it a\nperfect choice for game developers. Furthermore, our method does not weaken the\nadvantages of prior work and seamlessly integrates into other position-based\nmethods for physically-based simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04721v1"
    },
    {
        "title": "Design and Implementation of a Procedural Content Generation Web\n  Application for Vertex Shaders",
        "authors": [
            "Juan C. Quiroz",
            "Sergiu M. Dascalu"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We present a web application for the procedural generation of transformations\nof 3D models. We generate the transformations by algorithmically generating the\nvertex shaders of the 3D models. The vertex shaders are created with an\ninteractive genetic algorithm, which displays to the user the visual effect\ncaused by each vertex shader, allows the user to select the visual effect the\nuser likes best, and produces a new generation of vertex shaders using the user\nfeedback as the fitness measure of the genetic algorithm. We use genetic\nprogramming to represent each vertex shader as a computer program. This paper\npresents details of requirements specification, software architecture, high and\nlow-level design, and prototype user interface. We discuss the project's\ncurrent status and development challenges.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05231v2"
    },
    {
        "title": "A Data-Driven Approach for Mapping Multivariate Data to Color",
        "authors": [
            "Shenghui Cheng",
            "Wei Xu",
            "Wen Zhong",
            "Klaus Mueller"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  A wide variety of color schemes have been devised for mapping scalar data to\ncolor. Some use the data value to index a color scale. Others assign colors to\ndifferent, usually blended disjoint materials, to handle areas where materials\noverlap. A number of methods can map low-dimensional data to color, however,\nthese methods do not scale to higher dimensional data. Likewise, schemes that\ntake a more artistic approach through color mixing and the like also face\nlimits when it comes to the number of variables they can encode. We address the\nchallenge of mapping multivariate data to color and avoid these limitations at\nthe same time. It is a data driven method, which first gauges the similarity of\nthe attributes and then arranges them according to the periphery of a convex 2D\ncolor space, such as HSL. The color of a multivariate data sample is then\nobtained via generalized barycentric coordinate (GBC) interpolation.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05772v1"
    },
    {
        "title": "Extending Scatterplots to Scalar Fields",
        "authors": [
            "Shenghui Cheng",
            "Pengcheng Cui",
            "Klaus Mueller"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Embedding high-dimensional data into a 2D canvas is a popular strategy for\ntheir visualization.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05773v1"
    },
    {
        "title": "Interpolations of Smoke and Liquid Simulations",
        "authors": [
            "Nils Thuerey"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We present a novel method to interpolate smoke and liquid simulations in\norder to perform data-driven fluid simulations. Our approach calculates a dense\nspace-time deformation using grid-based signed-distance functions of the\ninputs. A key advantage of this implicit Eulerian representation is that it\nallows us to use powerful techniques from the optical flow area. We employ a\nfive-dimensional optical flow solve. In combination with a projection\nalgorithm, and residual iterations, we achieve a robust matching of the inputs.\nOnce the match is computed, arbitrary in between variants can be created very\nefficiently. To concatenate multiple long-range deformations, we propose a\nnovel alignment technique. Our approach has numerous advantages, including\nautomatic matches without user input, volumetric deformations that can be\napplied to details around the surface, and the inherent handling of topology\nchanges. As a result, we can interpolate swirling smoke clouds, and splashing\nliquid simulations. We can even match and interpolate phenomena with\nfundamentally different physics: a drop of liquid, and a blob of heavy smoke.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.08570v1"
    },
    {
        "title": "Volume Raycasting mit OpenCL",
        "authors": [
            "Nils Kopal"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  This German paper was written entirely at the University of Duisburg-Essen in\n2011 for a 3D modeling masters course in applied computer science. We publish\nthis paper, thus, interested people can acquire a first impression of the topic\n\"volume raycasting\". In addition to writing this paper, we developed a\nfunctioning open-source OpenCL raycaster. A video of this raycaster is\navailable: http://www.youtube.com/watch?v=VMMsQnf4zEY. Additionally, we\narchived and published the complete source code of the raycaster in the Google\nCode Archive: http://code.google.com/p/gputracer/. If this is no longer the\ncase, those who are interested can also write an email to the author, hence, we\ncan provide the source code.\n  This paper provides an introduction and overview of the topic \"volume ray\ncasting with OpenCL\". We show how volume data can be loaded, manipulated, and\nvisualized by modern GPUs in real time. In addition, we present basic\nalgorithms and data structures that are necessary for building such a\nraycaster. Then, we describe how we built a rudimentary raycaster using OpenCL\nand .NET C#. Furthermore, we analyze different gradient operators\n(CentralDifference, Sobel3D and Zucker-Hummel) for surface detection and show\nan evaluation of these with respect to their performance. Finally, we present\noptimization techniques (hitpoint refinement, adaptive sampling, octrees, and\nempty-space-skipping) for improving a raycaster.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.01317v1"
    },
    {
        "title": "Sampling BSSRDFs with non-perpendicular incidence",
        "authors": [
            "Etienne Ferrier"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Sub-surface scattering is key to our perception of translucent materials.\nModels based on diffusion theory are used to render such materials in a\nrealistic manner by evaluating an approximation of the material BSSRDF at any\ntwo points of the surface. Under the assumption of perpendicular incidence,\nthis BSSRDF approximation can be tabulated over 2 dimensions to provide fast\nevaluation and importance sampling. However, accounting for non-perpendicular\nincidence with the same approach would require to tabulate over 4 dimensions,\nmaking the model too large for practical applications. In this report, we\npresent a method to efficiently evaluate and importance sample the\nmulti-scattering component of diffusion based BSSRDFs for non-perpendicular\nincidence. Our approach is based on tabulating a compressed angular model of\nPhoton Beam Diffusion. We explain how to generate, evaluate and sample our\nmodel. We show that 1 MiB is enough to store a model of the multi-scattering\nBSSRDF that is within $0.5\\%$ relative error of Photon Beam Diffusion. Finally,\nwe present a method to use our model in a Monte Carlo particle tracer and show\nresults of our implementation in PBRT.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.02072v1"
    },
    {
        "title": "Anti-aliasing for fused filament deposition",
        "authors": [
            "Hai-Chuan Song",
            "Nicolas Ray",
            "Dmitry Sokolov",
            "Sylvain Lefebvre"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Layered manufacturing inherently suffers from staircase defects along\nsurfaces that are gently slopped with respect to the build direction. Reducing\nthe slice thickness improves the situation but never resolves it completely as\nflat layers remain a poor approximation of the true surface in these regions.\nIn addition, reducing the slice thickness largely increases the print time. In\nthis work we focus on a simple yet effective technique to improve the print\naccuracy for layered manufacturing by filament deposition. Our method works\nwith standard three-axis 3D filament printers (e.g. the typical, widely\navailable 3D printers), using standard extrusion nozzles. It better reproduces\nthe geometry of sloped surfaces without increasing the print time. Our key idea\nis to perform a local anti-aliasing, working at a sub-layer accuracy to produce\nslightly curved deposition paths and reduce approximation errors. This is\ninspired by Computer Graphics anti-aliasing techniques which consider sub-pixel\nprecision to treat aliasing effects. We show that the necessary deviation in\nheight compared to standard slicing is bounded by half the layer thickness.\nTherefore, the height changes remain small and plastic deposition remains\nreliable. We further split and order paths to minimize defects due to the\nextruder nozzle shape, avoiding any change to the existing hardware. We apply\nand analyze our approach on 3D printed examples, showing that our technique\ngreatly improves surface accuracy and silhouette quality while keeping the\nprint time nearly identical.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.03032v2"
    },
    {
        "title": "Optimisations for Real-Time Volumetric Cloudscapes",
        "authors": [
            "Alastair Toft",
            "Huw Bowles",
            "Daniel Zimmermann"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Volumetric cloudscapes are prohibitively expensive to render in real time\nwithout extensive optimisations. Previous approaches render the clouds to an\noffscreen buffer at one quarter resolution and update a fraction of the pixels\nper frame, drawing the remaining pixels by temporal reprojection. We present an\nalternative approach, reducing the number of raymarching steps and adding a\nrandomly jittered offset to the raymarch. We use an analytical integration\ntechnique to make results consistent with a lower number of raymarching steps.\nTo remove noise from the resulting image we apply a temporal anti-aliasing\nimplementation. The result is a technique producing visually similar results\nwith 1/16 the number of steps.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.05344v1"
    },
    {
        "title": "Unsupervised Co-segmentation of 3D Shapes via Functional Maps",
        "authors": [
            "Jun Yang",
            "Zhenhua Tian"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We present an unsupervised method for co-segmentation of a set of 3D shapes\nfrom the same class with the aim of segmenting the input shapes into consistent\nsemantic parts and establishing their correspondence across the set. Starting\nfrom meaningful pre-segmentation of all given shapes individually, we construct\nthe correspondence between same candidate parts and obtain the labels via\nfunctional maps. And then, we use these labels to mark the input shapes and\nobtain results of co-segmentation. The core of our algorithm is to seek for an\noptimal correspondence between semantically similar parts through functional\nmaps and mark such shape parts. Experimental results on the benchmark datasets\nshow the efficiency of this method and comparable accuracy to the\nstate-of-the-art algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.08313v1"
    },
    {
        "title": "Dynamic Polygon Clouds: Representation and Compression for VR/AR",
        "authors": [
            "Philip A. Chou",
            "Eduardo Pavez",
            "Ricardo L. de Queiroz",
            "Antonio Ortega"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We introduce the {\\em polygon cloud}, also known as a polygon set or {\\em\nsoup}, as a compressible representation of 3D geometry (including its\nattributes, such as color texture) intermediate between polygonal meshes and\npoint clouds. Dynamic or time-varying polygon clouds, like dynamic polygonal\nmeshes and dynamic point clouds, can take advantage of temporal redundancy for\ncompression, if certain challenges are addressed. In this paper, we propose\nmethods for compressing both static and dynamic polygon clouds, specifically\ntriangle clouds. We compare triangle clouds to both triangle meshes and point\nclouds in terms of compression, for live captured dynamic colored geometry. We\nfind that triangle clouds can be compressed nearly as well as triangle meshes,\nwhile being far more robust to noise and other structures typically found in\nlive captures, which violate the assumption of a smooth surface manifold, such\nas lines, points, and ragged boundaries. We also find that triangle clouds can\nbe used to compress point clouds with significantly better performance than\npreviously demonstrated point cloud compression methods. In particular, for\nintra-frame coding of geometry, our method improves upon octree-based\nintra-frame coding by a factor of 5-10 in bit rate. Inter-frame coding improves\nthis by another factor of 2-5. Overall, our dynamic triangle cloud compression\nimproves over the previous state-of-the-art in dynamic point cloud compression\nby 33\\% or more.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.00402v2"
    },
    {
        "title": "Inverse Diffusion Curves using Shape Optimization",
        "authors": [
            "Shuang Zhao",
            "Fredo Durand",
            "Changxi Zheng"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  The inverse diffusion curve problem focuses on automatic creation of\ndiffusion curve images that resemble user provided color fields. This problem\nis challenging since the 1D curves have a nonlinear and global impact on\nresulting color fields via a partial differential equation (PDE). We introduce\na new approach complementary to previous methods by optimizing curve geometry.\nIn particular, we propose a novel iterative algorithm based on the theory of\nshape derivatives. The resulting diffusion curves are clean and well-shaped,\nand the final image closely approximates the input. Our method provides a\nuser-controlled parameter to regularize curve complexity, and generalizes to\nhandle input color fields represented in a variety of formats.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.02769v1"
    },
    {
        "title": "Polynomial methods for Procedural Terrain Generation",
        "authors": [
            "Yann Thorimbert",
            "Bastien Chopard"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  A new method is presented, allowing for the generation of 3D terrain and\ntexture from coherent noise. The method is significantly faster than prevailing\nfractal brownian motion approaches, while producing results of equivalent\nquality. The algorithm is derived through a systematic approach that\ngeneralizes to an arbitrary number of spatial dimensions and gradient\nsmoothness. The results are compared, in terms of performance and quality, to\nfundamental and efficient gradient noise methods widely used in the domain of\nfast terrain generation: Perlin noise and OpenSimplex noise. Finally, to\nobjectively quantify the degree of realism of the results, a fractal analysis\nof the generated landscapes is performed and compared to real terrain data.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.03525v4"
    },
    {
        "title": "Simplification of Multi-Scale Geometry using Adaptive Curvature Fields",
        "authors": [
            "Patrick Seemann",
            "Simon Fuhrmann",
            "Stefan Guthe",
            "Fabian Langguth",
            "Michael Goesele"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We present a novel algorithm to compute multi-scale curvature fields on\ntriangle meshes. Our algorithm is based on finding robust mean curvatures using\nthe ball neighborhood, where the radius of a ball corresponds to the scale of\nthe features. The essential problem is to find a good radius for each ball to\nobtain a reliable curvature estimation. We propose an algorithm that finds\nsuitable radii in an automatic way. In particular, our algorithm is applicable\nto meshes produced by image-based reconstruction systems. These meshes often\ncontain geometric features at various scales, for example if certain regions\nhave been captured in greater detail. We also show how such a multi-scale\ncurvature field can be converted to a density field and used to guide\napplications like mesh simplification.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.07368v2"
    },
    {
        "title": "Selecting the Best Quadrilateral Mesh for Given Planar Shape",
        "authors": [
            "Petra Surynkova"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  The problem of mesh matching is addressed in this work. For a given n-sided\nplanar region bounded by one loop of n polylines we are selecting optimal\nquadrilateral mesh from existing catalogue of meshes. The formulation of\nmatching between planar shape and quadrilateral mesh from the catalogue is\nbased on the problem of finding longest common subsequence (LCS). Theoretical\nfoundation of mesh matching method is provided. Suggested method represents a\nviable technique for selecting best mesh for planar region and stepping stone\nfor further parametrization of the region.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.09988v1"
    },
    {
        "title": "Two New Contributions to the Visualization of AMR Grids: I. Interactive\n  Rendering of Extreme-Scale 2-Dimensional Grids II. Novel Selection Filters in\n  Arbitrary Dimension",
        "authors": [
            "Guénolé Harel",
            "Jacques-Bernard Lekien",
            "Philippe P. Pébaÿ"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present here the result of continuation work, performed to further fulfill\nthe vision we outlined in [Harel,Lekien,P\\'eba\\\"y-2017] for the visualization\nand analysis of tree-based adaptive mesh refinement (AMR) simulations, using\nthe hypertree grid paradigm which we proposed.\n  The first filter presented hereafter implements an adaptive approach in order\nto accelerate the rendering of 2-dimensional AMR grids, hereby solving the\nproblem posed by the loss of interactivity that occurs when dealing with large\nand/or deeply refined meshes. Specifically, view parameters are taken into\naccount, in order to: on one hand, avoid creating surface elements that are\noutside of the view area; on the other hand, utilize level-of-detail properties\nto cull those cells that are deemed too small to be visible with respect to the\ngiven view parameters. This adaptive approach often results in a massive\nincrease in rendering performance.\n  In addition, two new selection filters provide data analysis capabilities, by\nmeans of allowing for the extraction of those cells within a hypertree grid\nthat are deemed relevant in some sense, either geometrically or topologically.\nAfter a description of these new algorithms, we illustrate their use within the\nVisualization Toolkit (VTK) in which we implemented them. This note ends with\nsome suggestions for subsequent work.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00212v1"
    },
    {
        "title": "Volumetric parametrization from a level set boundary representation with\n  PHT Splines",
        "authors": [
            "Chiu Ling Chan",
            "Cosmin Anitescu",
            "Timon Rabczuk"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  A challenge in isogeometric analysis is constructing analysis-suitable\nvolumetric meshes which can accurately represent the geometry of a given\nphysical domain. In this paper, we propose a method to derive a spline-based\nrepresentation of a domain of interest from voxel-based data. We show an\nefficient way to obtain a boundary representation of the domain by a level-set\nfunction. Then, we use the geometric information from the boundary (the normal\nvectors and curvature) to construct a matching C1 representation with\nhierarchical cubic splines. The approximation is done by a single template and\nlinear transformations (scaling, translations and rotations) without the need\nfor solving an optimization problem. We illustrate our method with several\nexamples in two and three dimensions, and show good performance on some\nstandard benchmark test problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.07167v1"
    },
    {
        "title": "CanvoX: High-resolution VR Painting in Large Volumetric Canvas",
        "authors": [
            "Yeojin Kim",
            "Byungmoon Kim",
            "Jiyang Kim",
            "Young J. Kim"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  With virtual reality, digital painting on 2D canvases is now being extended\nto 3D spaces. Tilt Brush and Oculus Quill are widely accepted among artists as\ntools that pave the way to a new form of art - 3D emmersive painting. Current\n3D painting systems are only a start, emitting textured triangular geometries.\nIn this paper, we advance this new art of 3D painting to 3D volumetric painting\nthat enables an artist to draw a huge scene with full control of spatial color\nfields. Inspired by the fact that 2D paintings often use vast space to paint\nbackground and small but detailed space for foreground, we claim that\nsupporting a large canvas in varying detail is essential for 3D painting. In\norder to help artists focus and audiences to navigate the large canvas space,\nwe provide small artist-defined areas, called rooms, that serve as beacons for\nartist-suggested scales, spaces, locations for intended appreciation view of\nthe painting. Artists and audiences can easily transport themselves between\ndifferent rooms. Technically, our canvas is represented as an array of deep\noctrees of depth 24 or higher, built on CPU for volume painting and on GPU for\nvolume rendering using accurate ray casting. In CPU side, we design an\nefficient iterative algorithm to refine or coarsen octree, as a result of\nvolumetric painting strokes, at highly interactive rates, and update the\ncorresponding GPU textures. Then we use GPU-based ray casting algorithms to\nrender the volumetric painting result. We explore precision issues stemming\nfrom ray-casting the octree of high depth, and provide a new analysis and\nverification. From our experimental results as well as the positive feedback\nfrom the participating artists, we strongly believe that our new 3D volume\npainting system can open up a new possibility for VR-driven digital art medium\nto professional artists as well as to novice users.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.02724v1"
    },
    {
        "title": "Projection Mapping Technologies for AR",
        "authors": [
            "Daisuke Iwai"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This invited talk will present recent projection mapping technologies for\naugmented reality. First, fundamental technologies are briefly explained, which\nhave been proposed to overcome the technical limitations of ordinary\nprojectors. Second, augmented reality (AR) applications using projection\nmapping technologies are introduced.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.02897v1"
    },
    {
        "title": "Denoising a Point Cloud for Surface Reconstruction",
        "authors": [
            "Siu-Wing Cheng",
            "Man-Kit Lau"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Surface reconstruction from an unorganized point cloud is an important\nproblem due to its widespread applications. White noise, possibly clustered\noutliers, and noisy perturbation may be generated when a point cloud is sampled\nfrom a surface. Most existing methods handle limited amount of noise. We\ndevelop a method to denoise a point cloud so that the users can run their\nsurface reconstruction codes or perform other analyses afterwards. Our\nexperiments demonstrate that our method is computationally efficient and it has\nsignificantly better noise handling ability than several existing surface\nreconstruction codes.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.04038v2"
    },
    {
        "title": "Reversible Jump Metropolis Light Transport using Inverse Mappings",
        "authors": [
            "Benedikt Bitterli",
            "Wenzel Jakob",
            "Jan Novák",
            "Wojciech Jarosz"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We study Markov Chain Monte Carlo (MCMC) methods operating in primary sample\nspace and their interactions with multiple sampling techniques. We observe that\nincorporating the sampling technique into the state of the Markov Chain, as\ndone in Multiplexed Metropolis Light Transport (MMLT), impedes the ability of\nthe chain to properly explore the path space, as transitions between sampling\ntechniques lead to disruptive alterations of path samples. To address this\nissue, we reformulate Multiplexed MLT in the Reversible Jump MCMC framework\n(RJMCMC) and introduce inverse sampling techniques that turn light paths into\nthe random numbers that would produce them. This allows us to formulate a novel\nperturbation that can locally transition between sampling techniques without\nchanging the geometry of the path, and we derive the correct acceptance\nprobability using RJMCMC. We investigate how to generalize this concept to\nnon-invertible sampling techniques commonly found in practice, and introduce\nprobabilistic inverses that extend our perturbation to cover most sampling\nmethods found in light transport simulations. Our theory reconciles the\ninverses with RJMCMC yielding an unbiased algorithm, which we call Reversible\nJump MLT (RJMLT). We verify the correctness of our implementation in canonical\nand practical scenarios and demonstrate improved temporal coherence, decrease\nin structured artifacts, and faster convergence on a wide variety of scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.06835v1"
    },
    {
        "title": "Boundary First Flattening",
        "authors": [
            "Rohan Sawhney",
            "Keenan Crane"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  A conformal flattening maps a curved surface to the plane without distorting\nangles---such maps have become a fundamental building block for problems in\ngeometry processing, numerical simulation, and computational design. Yet\nexisting methods provide little direct control over the shape of the flattened\ndomain, or else demand expensive nonlinear optimization. Boundary first\nflattening (BFF) is a linear method for conformal parameterization which is\nfaster than traditional linear methods, yet provides control and quality\ncomparable to sophisticated nonlinear schemes. The key insight is that the\nboundary data for many conformal mapping problems can be efficiently\nconstructed via the Cherrier formula together with a pair of Poincare-Steklov\noperators; once the boundary is known, the map can be easily extended over the\nrest of the domain. Since computation demands only a single factorization of\nthe real Laplace matrix, the amortized cost is about 50x less than any\npreviously published technique for boundary-controlled conformal flattening. As\na result, BFF opens the door to real-time editing or fast optimization of\nhigh-resolution maps, with direct control over boundary length or angle. We\nshow how this method can be used to construct maps with sharp corners, cone\nsingularities, minimal area distortion, and uniformization over the unit disk;\nwe also demonstrate for the first time how a surface can be conformally\nflattened directly onto any given target shape.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.06873v2"
    },
    {
        "title": "Automatic Content-aware Projection for 360° Videos",
        "authors": [
            "Yeong Won Kim",
            "Dae-Yong Jo",
            "Chang-Ryeol Lee",
            "Hyeok-Jae Choi",
            "Yong Hoon Kwon",
            "Kuk-Jin Yoon"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  To watch 360{\\deg} videos on normal 2D displays, we need to project the\nselected part of the 360{\\deg} image onto the 2D display plane. In this paper,\nwe propose a fully-automated framework for generating content-aware 2D\nnormal-view perspective videos from 360{\\deg} videos. Especially, we focus on\nthe projection step preserving important image contents and reducing image\ndistortion. Basically, our projection method is based on Pannini projection\nmodel. At first, the salient contents such as linear structures and salient\nregions in the image are preserved by optimizing the single Panini projection\nmodel. Then, the multiple Panini projection models at salient regions are\ninterpolated to suppress image distortion globally. Finally, the temporal\nconsistency for image projection is enforced for producing temporally stable\nnormal-view videos. Our proposed projection method does not require any\nuser-interaction and is much faster than previous content-preserving methods.\nIt can be applied to not only images but also videos taking the temporal\nconsistency of projection into account. Experiments on various 360{\\deg} videos\nshow the superiority of the proposed projection method quantitatively and\nqualitatively.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.07528v2"
    },
    {
        "title": "The Iray Light Transport Simulation and Rendering System",
        "authors": [
            "Alexander Keller",
            "Carsten Wächter",
            "Matthias Raab",
            "Daniel Seibert",
            "Dietger van Antwerpen",
            "Johann Korndörfer",
            "Lutz Kettner"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  While ray tracing has become increasingly common and path tracing is well\nunderstood by now, a major challenge lies in crafting an easy-to-use and\nefficient system implementing these technologies. Following a purely\nphysically-based paradigm while still allowing for artistic workflows, the Iray\nlight transport simulation and rendering system allows for rendering complex\nscenes by the push of a button and thus makes accurate light transport\nsimulation widely available. In this document we discuss the challenges and\nimplementation choices that follow from our primary design decisions,\ndemonstrating that such a rendering system can be made a practical, scalable,\nand efficient real-world application that has been adopted by various companies\nacross many fields and is in use by many industry professionals today.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01263v1"
    },
    {
        "title": "Learning Hierarchical Shape Segmentation and Labeling from Online\n  Repositories",
        "authors": [
            "Li Yi",
            "Leonidas Guibas",
            "Aaron Hertzmann",
            "Vladimir G. Kim",
            "Hao Su",
            "Ersin Yumer"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We propose a method for converting geometric shapes into hierarchically\nsegmented parts with part labels. Our key idea is to train category-specific\nmodels from the scene graphs and part names that accompany 3D shapes in public\nrepositories. These freely-available annotations represent an enormous,\nuntapped source of information on geometry. However, because the models and\ncorresponding scene graphs are created by a wide range of modelers with\ndifferent levels of expertise, modeling tools, and objectives, these models\nhave very inconsistent segmentations and hierarchies with sparse and noisy\ntextual tags. Our method involves two analysis steps. First, we perform a joint\noptimization to simultaneously cluster and label parts in the database while\nalso inferring a canonical tag dictionary and part hierarchy. We then use this\nlabeled data to train a method for hierarchical segmentation and labeling of\nnew 3D shapes. We demonstrate that our method can mine complex information,\ndetecting hierarchies in man-made objects and their constituent parts,\nobtaining finer scale details than existing alternatives. We also show that, by\nperforming domain transfer using a few supervised examples, our technique\noutperforms fully-supervised techniques that require hundreds of\nmanually-labeled models.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01661v1"
    },
    {
        "title": "Semi-Global Weighted Least Squares in Image Filtering",
        "authors": [
            "Wei Liu",
            "Xiaogang Chen",
            "Chuanhua Shen",
            "Zhi Liu",
            "Jie Yang"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Solving the global method of Weighted Least Squares (WLS) model in image\nfiltering is both time- and memory-consuming. In this paper, we present an\nalternative approximation in a time- and memory- efficient manner which is\ndenoted as Semi-Global Weighed Least Squares (SG-WLS). Instead of solving a\nlarge linear system, we propose to iteratively solve a sequence of subsystems\nwhich are one-dimensional WLS models. Although each subsystem is\none-dimensional, it can take two-dimensional neighborhood information into\naccount due to the proposed special neighborhood construction. We show such a\ndesirable property makes our SG-WLS achieve close performance to the original\ntwo-dimensional WLS model but with much less time and memory cost. While\nprevious related methods mainly focus on the 4-connected/8-connected\nneighborhood system, our SG-WLS can handle a more general and larger\nneighborhood system thanks to the proposed fast solution. We show such a\ngeneralization can achieve better performance than the 4-connected/8-connected\nneighborhood system in some applications. Our SG-WLS is $\\sim20$ times faster\nthan the WLS model. For an image of $M\\times N$, the memory cost of SG-WLS is\nat most at the magnitude of $max\\{\\frac{1}{M}, \\frac{1}{N}\\}$ of that of the\nWLS model. We show the effectiveness and efficiency of our SG-WLS in a range of\napplications. The code is publicly available at:\nhttps://github.com/wliusjtu/Semi-Global-Weighted-Least-Squares-in-Image-Filtering.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01674v4"
    },
    {
        "title": "On Discrete Conformal Seamless Similarity Maps",
        "authors": [
            "Marcel Campen",
            "Denis Zorin"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  An algorithm for the computation of global discrete conformal\nparametrizations with prescribed global holonomy signatures for triangle meshes\nwas recently described in [Campen and Zorin 2017]. In this paper we provide a\ndetailed analysis of convergence and correctness of this algorithm. We\ngeneralize and extend ideas of [Springborn et al. 2008] to show a connection of\nthe algorithm to Newton's algorithm applied to solving the system of\nconstraints on angles in the parametric domain, and demonstrate that this\nsystem can be obtained as a gradient of a convex energy.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.02422v1"
    },
    {
        "title": "From 3D Models to 3D Prints: an Overview of the Processing Pipeline",
        "authors": [
            "Marco Livesu",
            "Stefano Ellero",
            "Jonás Martìnez",
            "Sylvain Lefebvre",
            "Marco Attene"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Due to the wide diffusion of 3D printing technologies, geometric algorithms\nfor Additive Manufacturing are being invented at an impressive speed. Each\nsingle step, in particular along the Process Planning pipeline, can now count\non dozens of methods that prepare the 3D model for fabrication, while analysing\nand optimizing geometry and machine instructions for various objectives. This\nreport provides a classification of this huge state of the art, and elicits the\nrelation between each single algorithm and a list of desirable objectives\nduring Process Planning. The objectives themselves are listed and discussed,\nalong with possible needs for tradeoffs. Additive Manufacturing technologies\nare broadly categorized to explicitly relate classes of devices and supported\nfeatures. Finally, this report offers an analysis of the state of the art while\ndiscussing open and challenging problems from both an academic and an\nindustrial perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.03811v2"
    },
    {
        "title": "Visualization of Feature Separation in Advected Scalar Fields",
        "authors": [
            "Grzegorz K. Karch",
            "Filip Sadlo",
            "Sebastian Boblest",
            "Moritz Ertl",
            "Bernhard Weigand",
            "Kelly Gaither",
            "Thomas Ertl"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Scalar features in time-dependent fluid flow are traditionally visualized\nusing 3D representation, and their topology changes over time are often\nconveyed with abstract graphs. Using such techniques, however, the structural\ndetails of feature separation and the temporal evolution of features undergoing\ntopological changes are difficult to analyze. In this paper, we propose a novel\napproach for the spatio-temporal visualization of feature separation that\nsegments feature volumes into regions with respect to their contribution to\ndistinct features after separation. To this end, we employ particle-based\nfeature tracking to find volumetric correspondences between features at two\ndifferent instants of time. We visualize this segmentation by constructing mesh\nboundaries around each volume segment of a feature at the initial time that\ncorrespond to the separated features at the later time. To convey temporal\nevolution of the partitioning within the investigated time interval, we\ncomplement our approach with spatio-temporal separation surfaces. For the\napplication of our approach to multiphase flow, we additionally present a\nfeature-based corrector method to ensure phase-consistent particle\ntrajectories. The utility of our technique is demonstrated by application to\ntwo-phase (liquid-gas) and multi-component (liquid-liquid) flows where the\nscalar field represents the fraction of one of the phases.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.05138v2"
    },
    {
        "title": "Computed Axial Lithography (CAL): Toward Single Step 3D Printing of\n  Arbitrary Geometries",
        "authors": [
            "Brett Kelly",
            "Indrasen Bhattacharya",
            "Maxim Shusteff",
            "Robert M. Panas",
            "Hayden K. Taylor",
            "Christopher M. Spadaccini"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Most additive manufacturing processes today operate by printing voxels (3D\npixels) serially point-by-point to build up a 3D part. In some more\nrecently-developed techniques, for example optical printing methods such as\nprojection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],\nparts are printed layer-by-layer by curing full 2d (very thin in one dimension)\nlayers of the 3d part in each print step. There does not yet exist a technique\nwhich is able to print arbitrarily-defined 3D geometries in a single print\nstep. If such a technique existed, it could be used to expand the range of\nprintable geometries in additive manufacturing and relax constraints on factors\nsuch as overhangs in topology optimization. It could also vastly increase print\nspeed for 3D parts. In this work, we develop the principles for an approach for\nsingle exposure 3D printing of arbitrarily defined geometries. The approach,\ntermed Computed Axial Lithgography (CAL), is based on tomographic\nreconstruction, with mathematical optimization to generate a set of projections\nto optically define an arbitrary dose distribution within a target volume. We\ndemonstrate the potential ability of the technique to print 3D parts using a\nprototype CAL system based on sequential illumination from many angles. We also\npropose new hardware designs which will help us to realize true single-shot\narbitrary-geometry 3D CAL.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.05893v1"
    },
    {
        "title": "Scratch iridescence: Wave-optical rendering of diffractive surface\n  structure",
        "authors": [
            "Sebastian Werner",
            "Zdravko Velinov",
            "Wenzel Jakob",
            "Matthias B. Hullin"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  The surface of metal, glass and plastic objects is often characterized by\nmicroscopic scratches caused by manufacturing and/or wear. A closer look onto\nsuch scratches reveals iridescent colors with a complex dependency on viewing\nand lighting conditions. The physics behind this phenomenon is well understood;\nit is caused by diffraction of the incident light by surface features on the\norder of the optical wavelength. Existing analytic models are able to reproduce\nspatially unresolved microstructure such as the iridescent appearance of\ncompact disks and similar materials. Spatially resolved scratches, on the other\nhand, have proven elusive due to the highly complex wave-optical light\ntransport simulations needed to account for their appearance. In this paper, we\npropose a wave-optical shading model based on non-paraxial scalar diffraction\ntheory to render this class of effects. Our model expresses surface roughness\nas a collection of line segments. To shade a point on the surface, the\nindividual diffraction patterns for contributing scratch segments are computed\nanalytically and superimposed coherently. This provides natural transitions\nfrom localized glint-like iridescence to smooth BRDFs representing the\nsuperposition of many reflections at large viewing distances. We demonstrate\nthat our model is capable of recreating the overall appearance as well as\ncharacteristic detail effects observed on real-world examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.06086v1"
    },
    {
        "title": "DS++: A flexible, scalable and provably tight relaxation for matching\n  problems",
        "authors": [
            "Nadav Dym",
            "Haggai Maron",
            "Yaron Lipman"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Correspondence problems are often modelled as quadratic optimization problems\nover permutations. Common scalable methods for approximating solutions of these\nNP-hard problems are the spectral relaxation for non-convex energies and the\ndoubly stochastic (DS) relaxation for convex energies. Lately, it has been\ndemonstrated that semidefinite programming relaxations can have considerably\nimproved accuracy at the price of a much higher computational cost. We present\na convex quadratic programming relaxation which is provably stronger than both\nDS and spectral relaxations, with the same scalability as the DS relaxation.\nThe derivation of the relaxation also naturally suggests a projection method\nfor achieving meaningful integer solutions which improves upon the standard\nclosest-permutation projection. Our method can be easily extended to\noptimization over doubly stochastic matrices, partial or injective matching,\nand problems with additional linear constraints. We employ recent advances in\noptimization of linear-assignment type problems to achieve an efficient\nalgorithm for solving the convex relaxation.\n  We present experiments indicating that our method is more accurate than local\nminimization or competing relaxations for non-convex problems. We successfully\napply our algorithm to shape matching and to the problem of ordering images in\na grid, obtaining results which compare favorably with state of the art\nmethods. We believe our results indicate that our method should be considered\nthe method of choice for quadratic optimization over permutations.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.06148v1"
    },
    {
        "title": "Shape Classification using Spectral Graph Wavelets",
        "authors": [
            "Majid Masoumi",
            "A. Ben Hamza"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Spectral shape descriptors have been used extensively in a broad spectrum of\ngeometry processing applications ranging from shape retrieval and segmentation\nto classification. In this pa- per, we propose a spectral graph wavelet\napproach for 3D shape classification using the bag-of-features paradigm. In an\neffort to capture both the local and global geometry of a 3D shape, we present\na three-step feature description framework. First, local descriptors are\nextracted via the spectral graph wavelet transform having the Mexican hat\nwavelet as a generating ker- nel. Second, mid-level features are obtained by\nembedding lo- cal descriptors into the visual vocabulary space using the soft-\nassignment coding step of the bag-of-features model. Third, a global descriptor\nis constructed by aggregating mid-level fea- tures weighted by a geodesic\nexponential kernel, resulting in a matrix representation that describes the\nfrequency of appearance of nearby codewords in the vocabulary. Experimental\nresults on two standard 3D shape benchmarks demonstrate the effective- ness of\nthe proposed classification approach in comparison with state-of-the-art\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.06250v1"
    },
    {
        "title": "3D Mesh Segmentation via Multi-branch 1D Convolutional Neural Networks",
        "authors": [
            "David George",
            "Xianghua Xie",
            "Gary KL Tam"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  There is an increasing interest in applying deep learning to 3D mesh\nsegmentation. We observe that 1) existing feature-based techniques are often\nslow or sensitive to feature resizing, 2) there are minimal comparative studies\nand 3) techniques often suffer from reproducibility issue. This study\ncontributes in two ways. First, we propose a novel convolutional neural network\n(CNN) for mesh segmentation. It uses 1D data, filters and a multi-branch\narchitecture for separate training of multi-scale features. Together with a\nnovel way of computing conformal factor (CF), our technique clearly\nout-performs existing work. Secondly, we publicly provide implementations of\nseveral deep learning techniques, namely, neural networks (NNs), autoencoders\n(AEs) and CNNs, whose architectures are at least two layers deep. The\nsignificance of this study is that it proposes a robust form of CF, offers a\nnovel and accurate CNN technique, and a comprehensive study of several deep\nlearning techniques for baseline comparison.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.11050v2"
    },
    {
        "title": "ComplementMe: Weakly-Supervised Component Suggestions for 3D Modeling",
        "authors": [
            "Minhyuk Sung",
            "Hao Su",
            "Vladimir G. Kim",
            "Siddhartha Chaudhuri",
            "Leonidas Guibas"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Assembly-based tools provide a powerful modeling paradigm for non-expert\nshape designers. However, choosing a component from a large shape repository\nand aligning it to a partial assembly can become a daunting task. In this paper\nwe describe novel neural network architectures for suggesting complementary\ncomponents and their placement for an incomplete 3D part assembly. Unlike most\nexisting techniques, our networks are trained on unlabeled data obtained from\npublic online repositories, and do not rely on consistent part segmentations or\nlabels. Absence of labels poses a challenge in indexing the database of parts\nfor the retrieval. We address it by jointly training embedding and retrieval\nnetworks, where the first indexes parts by mapping them to a low-dimensional\nfeature space, and the second maps partial assemblies to appropriate\ncomplements. The combinatorial nature of part arrangements poses another\nchallenge, since the retrieval network is not a function: several complements\ncan be appropriate for the same input. Thus, instead of predicting a single\noutput, we train our network to predict a probability distribution over the\nspace of part embeddings. This allows our method to deal with ambiguities and\nnaturally enables a UI that seamlessly integrates user preferences into the\ndesign process. We demonstrate that our method can be used to design complex\nshapes with minimal or no user input. To evaluate our approach we develop a\nnovel benchmark for component suggestion systems demonstrating significant\nimprovement over state-of-the-art techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.01841v2"
    },
    {
        "title": "Temporal Upsampling of Depth Maps Using a Hybrid Camera",
        "authors": [
            "Ming-Ze Yuan",
            "Lin Gao",
            "Hongbo Fu",
            "Shihong Xia"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In recent years, consumer-level depth cameras have been adopted for various\napplications. However, they often produce depth maps at only a moderately high\nframe rate (approximately 30 frames per second), preventing them from being\nused for applications such as digitizing human performance involving fast\nmotion. On the other hand, low-cost, high-frame-rate video cameras are\navailable. This motivates us to develop a hybrid camera that consists of a\nhigh-frame-rate video camera and a low-frame-rate depth camera and to allow\ntemporal interpolation of depth maps with the help of auxiliary color images.\nTo achieve this, we develop a novel algorithm that reconstructs intermediate\ndepth maps and estimates scene flow simultaneously. We test our algorithm on\nvarious examples involving fast, non-rigid motions of single or multiple\nobjects. Our experiments show that our scene flow estimation method is more\nprecise than a tracking-based method and the state-of-the-art techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.03760v2"
    },
    {
        "title": "Eccentricity Effects on Blur and Depth Perception",
        "authors": [
            "Qi Sun",
            "Fu-Chung Huang",
            "Li-Yi Wei",
            "David Luebke",
            "Arie Kaufman",
            "Joohwan Kim"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Foveation and focus cue are the two most discussed topics on vision in\ndesigning near-eye displays. Foveation reduces rendering load by omitting\nspatial details in the content that the peripheral vision cannot appreciate;\nProviding richer focal cue can resolve vergence-accommodation conflict thereby\nlessening visual discomfort in using near-eye displays. We performed two\npsychophysical experiments to investigate the relationship between foveation\nand focus cue. The first study measured blur discrimination sensitivity as a\nfunction of visual eccentricity, where we found discrimination thresholds\nsignificantly lower than previously reported. The second study measured depth\ndiscrimination threshold where we found a clear dependency on visual\neccentricity. We discuss the results from the two studies and suggest further\ninvestigation.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.06034v2"
    },
    {
        "title": "Fractions, Projective Representation, Duality, Linear Algebra and\n  Geometry",
        "authors": [
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This contribution describes relationship between fractions, projective\nrepresentation, duality, linear algebra and geometry. Many problems lead to a\nsystem of linear equations. This paper presents equivalence of the\nCross-product operation and solution of a system of linear equations Ax=0 or\nAx=b using projective space representation and homogeneous coordinates. It\nleads to conclusion that division operation is not required for a solution of a\nsystem of linear equations, if the projective representation and homogeneous\ncoordinates are used. An efficient solution on CPU and GPU based architectures\nis presented with an application to barycentric coordinates computation as\nwell.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.06684v1"
    },
    {
        "title": "A two-level approach to implicit surface modeling with compactly\n  supported radial basis functions",
        "authors": [
            "Rongjiang Pan",
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We describe a two-level method for computing a function whose zero-level set\nis the surface reconstructed from given points scattered over the surface and\nassociated with surface normal vectors. The function is defined as a linear\ncombination of compactly supported radial basis functions (CSRBFs). The method\npreserves the simplicity and efficiency of implicit surface interpolation with\nCSRBFs and the reconstructed implicit surface owns the attributes, which are\npreviously only associated with globally supported or globally regularized\nradial basis functions, such as exhibiting less extra zero-level sets, suitable\nfor inside and outside tests. First, in the coarse scale approximation, we\nchoose basis function centers on a grid that covers the enlarged bounding box\nof the given point set and compute their signed distances to the underlying\nsurface using local quadratic approximations of the nearest surface points.\nThen a fitting to the residual errors on the surface points and additional\noff-surface points is performed with fine scale basis functions. The final\nfunction is the sum of the two intermediate functions and is a good\napproximation of the signed distance field to the surface in the bounding box.\nExamples of surface reconstruction and set operations between shapes are\nprovided.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.06695v1"
    },
    {
        "title": "Efficient barycentric point sampling on meshes",
        "authors": [
            "Jamie Portsmouth"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present an easy-to-implement and efficient analytical inversion algorithm\nfor the unbiased random sampling of a set of points on a triangle mesh whose\nsurface density is specified by barycentric interpolation of non-negative\nper-vertex weights. The correctness of the inversion algorithm is verified via\nstatistical tests, and we show that it is faster on average than rejection\nsampling.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07559v1"
    },
    {
        "title": "Active Animations of Reduced Deformable Models with Environment\n  Interactions",
        "authors": [
            "Zherong Pan",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present an efficient spacetime optimization method to automatically\ngenerate animations for a general volumetric, elastically deformable body. Our\napproach can model the interactions between the body and the environment and\nautomatically generate active animations. We model the frictional contact\nforces using contact invariant optimization and the fluid drag forces using a\nsimplified model. To handle complex objects, we use a reduced deformable model\nand present a novel hybrid optimizer to search for the local minima\nefficiently. This allows us to use long-horizon motion planning to\nautomatically generate animations such as walking, jumping, swimming, and\nrolling. We evaluate the approach on different shapes and animations, including\ndeformable body navigation and combining with an open-loop controller for\nrealtime forward simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.08188v2"
    },
    {
        "title": "MLSEB: Edge Bundling using Moving Least Squares Approximation",
        "authors": [
            "Jieting Wu",
            "Jianping Zeng",
            "Feiyu Zhu",
            "Hongfeng Yu"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Edge bundling methods can effectively alleviate visual clutter and reveal\nhigh-level graph structures in large graph visualization. Researchers have\ndevoted significant efforts to improve edge bundling according to different\nmetrics. As the edge bundling family evolve rapidly, the quality of edge\nbundles receives increasing attention in the literature accordingly. In this\npaper, we present MLSEB, a novel method to generate edge bundles based on\nmoving least squares (MLS) approximation. In comparison with previous edge\nbundling methods, we argue that our MLSEB approach can generate better results\nbased on a quantitative metric of quality, and also ensure scalability and the\nefficiency for visualizing large graphs.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.01221v2"
    },
    {
        "title": "Sparse Data Driven Mesh Deformation",
        "authors": [
            "Lin Gao",
            "Yu-Kun Lai",
            "Jie Yang",
            "Ling-Xiao Zhang",
            "Leif Kobbelt",
            "Shihong Xia"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Example-based mesh deformation methods are powerful tools for realistic shape\nediting. However, existing techniques typically combine all the example\ndeformation modes, which can lead to overfitting, i.e. using a overly\ncomplicated model to explain the user-specified deformation. This leads to\nimplausible or unstable deformation results, including unexpected global\nchanges outside the region of interest. To address this fundamental limitation,\nwe propose a sparse blending method that automatically selects a smaller number\nof deformation modes to compactly describe the desired deformation. This along\nwith a suitably chosen deformation basis including spatially localized\ndeformation modes leads to significant advantages, including more meaningful,\nreliable, and efficient deformations because fewer and localized deformation\nmodes are applied. To cope with large rotations, we develop a simple but\neffective representation based on polar decomposition of deformation gradients,\nwhich resolves the ambiguity of large global rotations using an\nas-consistent-as-possible global optimization. This simple representation has a\nclosed form solution for derivatives, making it efficient for sparse localized\nrepresentation and thus ensuring interactive performance. Experimental results\nshow that our method outperforms state-of-the-art data-driven mesh deformation\nmethods, for both quality of results and efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.01250v1"
    },
    {
        "title": "360 Panorama Cloning on Sphere",
        "authors": [
            "Qiang Zhao",
            "Liang Wan",
            "Wei Feng",
            "Jiawan Zhang",
            "Tien-Tsin Wong"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In this paper, we address a novel problem of cloning a patch of the source\nspherical panoramic image to the target spherical panoramic image, which we\ncall 360 panorama cloning. Considering the sphere geometry constraint embedded\nin spherical panoramic images, we develop a coordinate-based method that\ndirectly clones in the spherical domain. Our method neither differentiates the\npolar regions and equatorial regions, nor identifies the boundaries in the\nunrolled planar-formatted panorama. We discuss in depth two unique issues in\npanorama cloning, i.e. preserving the patch's orientation, and handling the\nlarge-patch cloning (covering over 180 field of view) which may suffer from\ndiscoloration artifacts. As experimental results demonstrate, our method is\nable to get visually pleasing cloning results and achieve real time cloning\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.01638v1"
    },
    {
        "title": "Global spectral graph wavelet signature for surface analysis of carpal\n  bones",
        "authors": [
            "Majid Masoumi",
            "A. Ben Hamza"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In this paper, we present a spectral graph wavelet approach for shape\nanalysis of carpal bones of human wrist. We apply a metric called global\nspectral graph wavelet signature for representation of cortical surface of the\ncarpal bone based on eigensystem of Laplace-Beltrami operator. Furthermore, we\npropose a heuristic and efficient way of aggregating local descriptors of a\ncarpal bone surface to global descriptor. The resultant global descriptor is\nnot only isometric invariant, but also much more efficient and requires less\nmemory storage. We perform experiments on shape of the carpal bones of ten\nwomen and ten men from a publicly-available database. Experimental results show\nthe excellency of the proposed GSGW compared to recent proposed GPS embedding\napproach for comparing shapes of the carpal bones across populations.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.02782v1"
    },
    {
        "title": "Mesh-based Autoencoders for Localized Deformation Component Analysis",
        "authors": [
            "Qingyang Tan",
            "Lin Gao",
            "Yu-Kun Lai",
            "Jie Yang",
            "Shihong Xia"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Spatially localized deformation components are very useful for shape analysis\nand synthesis in 3D geometry processing. Several methods have recently been\ndeveloped, with an aim to extract intuitive and interpretable deformation\ncomponents. However, these techniques suffer from fundamental limitations\nespecially for meshes with noise or large-scale deformations, and may not\nalways be able to identify important deformation components. In this paper we\npropose a novel mesh-based autoencoder architecture that is able to cope with\nmeshes with irregular topology. We introduce sparse regularization in this\nframework, which along with convolutional operations, helps localize\ndeformations. Our framework is capable of extracting localized deformation\ncomponents from mesh data sets with large-scale deformations and is robust to\nnoise. It also provides a nonlinear approach to reconstruction of meshes using\nthe extracted basis, which is more effective than the current linear\ncombination approach. Extensive experiments show that our method outperforms\nstate-of-the-art methods in both qualitative and quantitative evaluations.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.04304v2"
    },
    {
        "title": "Variational Autoencoders for Deforming 3D Mesh Models",
        "authors": [
            "Qingyang Tan",
            "Lin Gao",
            "Yu-Kun Lai",
            "Shihong Xia"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  3D geometric contents are becoming increasingly popular. In this paper, we\nstudy the problem of analyzing deforming 3D meshes using deep neural networks.\nDeforming 3D meshes are flexible to represent 3D animation sequences as well as\ncollections of objects of the same category, allowing diverse shapes with\nlarge-scale non-linear deformations. We propose a novel framework which we call\nmesh variational autoencoders (mesh VAE), to explore the probabilistic latent\nspace of 3D surfaces. The framework is easy to train, and requires very few\ntraining examples. We also propose an extended model which allows flexibly\nadjusting the significance of different latent variables by altering the prior\ndistribution. Extensive experiments demonstrate that our general framework is\nable to learn a reasonable representation for a collection of deformable\nshapes, and produce competitive results for a variety of applications,\nincluding shape generation, shape interpolation, shape space embedding and\nshape exploration, outperforming state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.04307v3"
    },
    {
        "title": "The wave method of building color palette and its application in\n  computer graphics",
        "authors": [
            "I. I. Sabo",
            "H. R. Lagoda"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This article describes a method of getting a harmonious combination of\ncolors, developed by us on the basis of the relationship of color and acoustic\nwaves. Presents a parallel between harmoniously matched colors and the concept\nof harmony in music theory (consonance). Describes the physical assumption of\nthe essence of the phenomenon of harmony (consonance). The article also\nprovides algorithm of implementation wave method for the sRGB color model.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.04752v1"
    },
    {
        "title": "Functional Characterization of Deformation Fields",
        "authors": [
            "Etienne Corman",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In this paper we present a novel representation for deformation fields of 3D\nshapes, by considering the induced changes in the underlying metric. In\nparticular, our approach allows to represent a deformation field in a\ncoordinate-free way as a linear operator acting on real-valued functions\ndefined on the shape. Such a representation both provides a way to relate\ndeformation fields to other classical functional operators and enables analysis\nand processing of deformation fields using standard linear-algebraic tools.\nThis opens the door to a wide variety of applications such as explicitly adding\nextrinsic information into the computation of functional maps, intrinsic shape\nsymmetrization, joint deformation design through precise control of metric\ndistortion, and coordinate-free deformation transfer without requiring\npointwise correspondences. Our method is applicable to both surface and\nvolumetric shape representations and we guarantee the equivalence between the\noperator-based and standard deformation field representation under mild\ngenericity conditions in the discrete setting. We demonstrate the utility of\nour approach by comparing it with existing techniques and show how our\nrepresentation provides a powerful toolbox for a wide variety of challenging\nproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.09701v1"
    },
    {
        "title": "Photometric Stabilization for Fast-forward Videos",
        "authors": [
            "Xuaner Cecilia Zhang",
            "Joon-Young Lee",
            "Kalyan Sunkavalli",
            "Zhaowen Wang"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Videos captured by consumer cameras often exhibit temporal variations in\ncolor and tone that are caused by camera auto-adjustments like white-balance\nand exposure. When such videos are sub-sampled to play fast-forward, as in the\nincreasingly popular forms of timelapse and hyperlapse videos, these temporal\nvariations are exacerbated and appear as visually disturbing high frequency\nflickering. Previous techniques to photometrically stabilize videos typically\nrely on computing dense correspondences between video frames, and use these\ncorrespondences to remove all color changes in the video sequences. However,\nthis approach is limited in fast-forward videos that often have large content\nchanges and also might exhibit changes in scene illumination that should be\npreserved. In this work, we propose a novel photometric stabilization algorithm\nfor fast-forward videos that is robust to large content-variation across\nframes. We compute pairwise color and tone transformations between neighboring\nframes and smooth these pair-wise transformations while taking in account the\npossibility of scene/content variations. This allows us to eliminate\nhigh-frequency fluctuations, while still adapting to real variations in scene\ncharacteristics. We evaluate our technique on a new dataset consisting of\ncontrolled synthetic and real videos, and demonstrate that our techniques\noutperforms the state-of-the-art.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10214v1"
    },
    {
        "title": "Full-Body Locomotion Reconstruction of Virtual Characters Using a Single\n  IMU",
        "authors": [
            "Christos Mousas"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This paper presents a method of reconstructing full-body locomotion sequences\nfor virtual characters in real-time, using data from a single inertial\nmeasurement unit (IMU). This process can be characterized by its difficulty\nbecause of the need to reconstruct a high number of degrees of freedom (DOFs)\nfrom a very low number of DOFs. To solve such a complex problem, the presented\nmethod is divided into several steps. The user's full-body locomotion and the\nIMU's data are recorded simultaneously. Then, the data is preprocessed in such\na way that would be handled more efficiently. By developing a hierarchical\nmultivariate hidden Markov model with reactive interpolation functionality the\nsystem learns the structure of the motion sequences. Specifically, the phases\nof the locomotion sequence are assigned in the higher hierarchical level, and\nthe frame structure of the motion sequences are assigned at the lower\nhierarchical level. During the runtime of the method, the forward algorithm is\nused for reconstructing the full-body motion of a virtual character. Firstly,\nthe method predicts the phase where the input motion belongs (higher\nhierarchical level). Secondly, the method predicts the closest trajectories and\ntheir progression and interpolates the most probable of them to reconstruct the\nvirtual character's full-body motion (lower hierarchical level). Evaluating the\nproposed method shows that it works on reasonable framerates and minimizes the\nreconstruction errors compared with previous approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.04194v1"
    },
    {
        "title": "Overlaying Quantitative Measurement on Networks: An Evaluation of Three\n  Positioning and Nine Visual Marker Techniques",
        "authors": [
            "Guohao Zhang",
            "Alexander P. Auchus",
            "Peter Kochunov",
            "Niklas Elmqvist",
            "Jian Chen"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We report results from an experiment on ranking visual markers and node\npositioning techniques for network visualizations. Inspired by prior ranking\nstudies, we rethink the ranking when the dataset size increases and when the\nmarkers are distributed in space. Centrality indices are visualized as node\nattributes. Our experiment studies nine visual markers and three positioning\nmethods. Our results suggest that direct encoding of quantities improves\naccuracy by about 20% compared to previous results. Of the three positioning\ntechniques, circular was always in the top group, and matrix and projection\nswitch orders depending on two factors: whether or not the tasks demand\nsymmetry, or the nodes are within closely proximity. Among the most interesting\nresults of ranking the visual markers for comparison tasks are that hue and\narea fall into the top groups for nearly all multi-scale comparison tasks;\nShape (ordered by curvature) is perhaps not as scalable as we have thought and\ncan support more accurate answers only when two quantities are compared;\nLightness and slope are least accurate for quantitative comparisons regardless\nof scale of the comparison tasks. Our experiment is among the first to acquire\na complete picture of ranking visual markers in different scales for comparison\ntasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.04419v1"
    },
    {
        "title": "Robust and High Fidelity Mesh Denoising",
        "authors": [
            "Sunil Kumar Yadav",
            "Ulrich Reitebuch",
            "Konrad Polthier"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This paper presents a simple and effective two-stage mesh denoising\nalgorithm, where in the first stage, the face normal filtering is done by using\nthe bilateral normal filtering in the robust statistics framework. Tukey's\nbi-weight function is used as similarity function in the bilateral weighting,\nwhich is a robust estimator and stops the diffusion at sharp edges to retain\nfeatures and removes noise from flat regions effectively. In the second stage,\nan edge weighted Laplace operator is introduced to compute a differential\ncoordinate. This differential coordinate helps the algorithm to produce a\nhigh-quality mesh without any face normal flips and makes the method robust\nagainst high-intensity noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.05341v2"
    },
    {
        "title": "Optimized Visibility Functions for Revectorization-Based Shadow Mapping",
        "authors": [
            "M. C. F. Macedo",
            "A. L. Apolinário",
            "K. A. Agüero"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  High-quality shadow anti-aliasing is a challenging problem in shadow mapping.\nRevectorization-based shadow mapping (RBSM) minimizes shadow aliasing by\nrevectorizing the jagged shadow edges generated with shadow mapping, keeping\nlow memory footprint and real-time performance for the shadow computation.\nHowever, the current implementation of RBSM is not so well optimized because\nits visibility functions are composed of a set of 43 cases, each one of them\nhandling a specific revectorization scenario and being implemented as a\nspecific branch in the shader. Here, we take advantage of the shadow shape\npatterns to reformulate the RBSM visibility functions, simplifying the\nimplementation of the technique and further providing an optimized version of\nthe RBSM. Our results indicate that our implementation runs faster than the\noriginal implementation of RBSM, while keeping its same visual quality and\nmemory consumption. Furthermore, we show GLSL source codes to ease the\nimplementation of our technique, provide a comparison between the optimized\nRBSM and related work, and discuss the limitations of the shadow\nrevectorization.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.07793v1"
    },
    {
        "title": "Cascaded 3D Full-body Pose Regression from Single Depth Image at 100 FPS",
        "authors": [
            "Shihong Xia",
            "Zihao Zhang",
            "Le Su"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  There are increasing real-time live applications in virtual reality, where it\nplays an important role in capturing and retargetting 3D human pose. But it is\nstill challenging to estimate accurate 3D pose from consumer imaging devices\nsuch as depth camera. This paper presents a novel cascaded 3D full-body pose\nregression method to estimate accurate pose from a single depth image at 100\nfps. The key idea is to train cascaded regressors based on Gradient Boosting\nalgorithm from pre-recorded human motion capture database. By incorporating\nhierarchical kinematics model of human pose into the learning procedure, we can\ndirectly estimate accurate 3D joint angles instead of joint positions. The\nbiggest advantage of this model is that the bone length can be preserved during\nthe whole 3D pose estimation procedure, which leads to more effective features\nand higher pose estimation accuracy. Our method can be used as an\ninitialization procedure when combining with tracking methods. We demonstrate\nthe power of our method on a wide range of synthesized human motion data from\nCMU mocap database, Human3.6M dataset and real human movements data captured in\nreal time. In our comparison against previous 3D pose estimation methods and\ncommercial system such as Kinect 2017, we achieve the state-of-the-art\naccuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08126v2"
    },
    {
        "title": "Visual Analytics of Group Differences in Tensor Fields: Application to\n  Clinical DTI",
        "authors": [
            "Amin Abbasloo",
            "Vitalis Wiens",
            "Tobias Schmidt-Wilcke",
            "Pia Sundgren",
            "Reinhard Klein",
            "Thomas Schultz"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a visual analytics system for exploring group differences in\ntensor fields with respect to all six degrees of freedom that are inherent in\nsymmetric second-order tensors. Our framework closely integrates quantitative\nanalysis, based on multivariate hypothesis testing and spatial cluster\nenhancement, with suitable visualization tools that facilitate interpretation\nof results, and forming of new hypotheses. Carefully chosen and linked spatial\nand abstract views show clusters of strong differences, and allow the analyst\nto relate them to the affected structures, to reveal the exact nature of the\ndifferences, and to investigate potential correlations. A mechanism for\nvisually comparing the results of different tests or levels of smoothing is\nalso provided.\n  We carefully justify the need for such a visual analytics tool from a\npractical and theoretical point of view. In close collaboration with our\nclinical co-authors, we apply it to the results of a diffusion tensor imaging\nstudy of systemic lupus erythematosus, in which it revealed previously unknown\ngroup differences.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08279v1"
    },
    {
        "title": "Visual Subpopulation Discovery and Validation in Cohort Study Data",
        "authors": [
            "Shiva Alemzadeh",
            "Tommy Hielscher",
            "Uli Niemann",
            "Lena Cibulski",
            "Till Ittermann",
            "Henry Völzke",
            "Myra Spiliopoulou",
            "Bernhard Preim"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Epidemiology aims at identifying subpopulations of cohort participants that\nshare common characteristics (e.g. alcohol consumption) to explain risk factors\nof diseases in cohort study data. These data contain information about the\nparticipants' health status gathered from questionnaires, medical examinations,\nand image acquisition. Due to the growing volume and heterogeneity of\nepidemiological data, the discovery of meaningful subpopulations is\nchallenging. Subspace clustering can be leveraged to find subpopulations in\nlarge and heterogeneous cohort study datasets. In our collaboration with\nepidemiologists, we realized their need for a tool to validate discovered\nsubpopulations. For this purpose, identified subpopulations should be searched\nfor independent cohorts to check whether the findings apply there as well. In\nthis paper we describe our interactive Visual Analytics framework S-ADVIsED for\nSubpopulAtion Discovery and Validation In Epidemiological Data. S-ADVIsED\nenables epidemiologists to explore and validate findings derived from subspace\nclustering. We provide a coordinated multiple view system, which includes a\nsummary view of all subpopulations, detail views, and statistical information.\nUsers can assess the quality of subspace clusters by considering different\ncriteria via visualization. Furthermore, intervals for variables involved in a\nsubspace cluster can be adjusted. This extension was suggested by\nepidemiologists. We investigated the replication of a selected subpopulation\nwith multiple variables in another population by considering different\nmeasurements. As a specific result, we observed that study participants\nexhibiting high liver fat accumulation deviate strongly from other\nsubpopulations and from the total study population with respect to age, body\nmass index, thyroid volume and thyroid-stimulating hormone.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.09377v1"
    },
    {
        "title": "Constraint Bubbles: Adding Efficient Zero-Density Bubbles to\n  Incompressible Free Surface Flow",
        "authors": [
            "Ryan Goldade",
            "Christopher Batty"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Liquid simulations for computer animation often avoid simulating the air\nphase to reduce computational costs and ensure good conditioning of the linear\nsystems required to enforce incompressibility. However, this free surface\nassumption leads to an inability to realistically treat bubbles: submerged gaps\nin the liquid are interpreted as empty voids that immediately collapse. To\naddress this shortcoming, we present an efficient, practical, and conceptually\nsimple approach to augment free surface flows with negligible density bubbles.\nOur method adds a new constraint to each disconnected air region that\nguarantees zero net flux across its entire surface, and requires neither\nsimulating both phases nor reformulating into stream function variables.\nImplementation of the method requires only minor modifications to the pressure\nsolve of a standard grid-based fluid solver, and yields linear systems that\nremain sparse and symmetric positive definite. In our evaluations, solving the\nmodified pressure projection system took no more than 10% longer than the\ncorresponding free surface solve. We demonstrate the method's effectiveness and\nflexibility by incorporating it into commercial fluid animation software and\nusing it to generate a variety of dynamic bubble scenarios showcasing glugging\neffects, viscous and inviscid bubbles, interactions with irregularly-shaped and\nmoving solid boundaries, and surface tension effects.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.11470v1"
    },
    {
        "title": "Bivariate Separable-Dimension Glyphs can Improve Visual Analysis of\n  Holistic Features",
        "authors": [
            "Henan Zhao",
            "Jian Chen"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We introduce the cause of the inefficiency of bivariate glyphs by defining\nthe corresponding error. To recommend efficient and perceptually accurate\nbivariate-glyph design, we present an empirical study of five bivariate glyphs\nbased on three psychophysics principles: integral-separable dimensions, visual\nhierarchy, and pre-attentive pop out, to choose one integral pair\n($length_y-length_x$), three separable pairs ($length-color$, $length-texture$,\n$length_y-length_y$), and one redundant pair ($length_y-color/length_x$).\nTwenty participants performed four tasks requiring: reading numerical values,\nestimating ratio, comparing two points, and looking for extreme values among a\nsubset of points belonging to the same sub-group. The most surprising result\nwas that $length-texture$ was among the most effective methods, suggesting that\nlocal spatial frequency features can lead to global pattern detection that\nfacilitate visual search in complex 3D structure. Our results also reveal the\nfollowing: $length-color$ bivariate glyphs led to the most accurate answers and\nthe least task execution time, while $length_y-length_x$ (integral) dimensions\nwere among the worst and is not recommended; it achieved high performance only\nwhen pop-up color was added.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.02333v2"
    },
    {
        "title": "Static/Dynamic Filtering for Mesh Geometry",
        "authors": [
            "Juyong Zhang",
            "Bailin Deng",
            "Yang Hong",
            "Yue Peng",
            "Wenjie Qin",
            "Ligang Liu"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  The joint bilateral filter, which enables feature-preserving signal smoothing\naccording to the structural information from a guidance, has been applied for\nvarious tasks in geometry processing. Existing methods either rely on a static\nguidance that may be inconsistent with the input and lead to unsatisfactory\nresults, or a dynamic guidance that is automatically updated but sensitive to\nnoises and outliers. Inspired by recent advances in image filtering, we propose\na new geometry filtering technique called static/dynamic filter, which utilizes\nboth static and dynamic guidances to achieve state-of-the-art results. The\nproposed filter is based on a nonlinear optimization that enforces smoothness\nof the signal while preserving variations that correspond to features of\ncertain scales. We develop an efficient iterative solver for the problem, which\nunifies existing filters that are based on static or dynamic guidances. The\nfilter can be applied to mesh face normals followed by vertex position update,\nto achieve scale-aware and feature-preserving filtering of mesh geometry. It\nalso works well for other types of signals defined on mesh surfaces, such as\ntexture colors. Extensive experimental results demonstrate the effectiveness of\nthe proposed filter for various geometry processing applications such as mesh\ndenoising, geometry feature enhancement, and texture color filtering.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.03574v2"
    },
    {
        "title": "Graphic Narrative with Interactive Stylization Design",
        "authors": [
            "Ignacio Garcia-Dorado",
            "Pascal Getreuer",
            "Madison Le",
            "Robin Debreuil",
            "Alex Kauffmann",
            "Peyman Milanfar"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a system to convert any set of images (e.g., a video clip or a\nphoto album) into a storyboard. We aim to create multiple pleasing graphic\nrepresentations of the content at interactive rates, so the user can explore\nand find the storyboard (images, layout, and stylization) that best suits their\nneeds and taste. The main challenges of this work are: selecting the content\nimages, placing them into panels, and applying a stylization. For the latter,\nwe propose an interactive design tool to create new stylizations using a wide\nrange of filter blocks. This approach unleashes the creativity by allowing the\nuser to tune, modify, and intuitively design new sequences of filters. In\nparallel to this manual design, we propose a novel procedural approach that\nautomatically assembles sequences of filters for innovative results. We aim to\nkeep the algorithm complexity as low as possible such that it can run\ninteractively on a mobile device. Our results include examples of styles\ndesigned using both our interactive and procedural tools, as well as their\nfinal composition into interesting and appealing storyboards.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.06654v1"
    },
    {
        "title": "On the one method of a third-degree bezier type spline curve\n  construction",
        "authors": [
            "O. Stelia",
            "L. Potapenko",
            "I. Sirenko"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  A method is proposed for constructing a spline curve of the Bezier type,\nwhich is continuous along with its first derivative by a piecewise polynomial\nfunction. Conditions for its existence and uniqueness are given. The\nconstructed curve lies inside the convex hull of the control points, and the\nsegments of the broken line connecting the control points are tangent to the\ncurve. To construct the curve, we use the approach proposed earlier for\nconstructing a parabolic spline. The idea is to use additional points with\nunknown values of some function. Additional points are used as spline nodes,\nand the function values are determined from the condition of the first\nderivative continuity of a piecewise polynomial curve. In multiple\ninterpolation nodes, the function takes the given values and the values of the\nfirst derivative, which are determined by the control points. Examples of\nconstructing a spline curve are given.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.07485v1"
    },
    {
        "title": "A Comparative Study of LOWESS and RBF Approximations for Visualization",
        "authors": [
            "Michal Smolik",
            "Vaclav Skala",
            "Ondrej Nedved"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Approximation methods are widely used in many fields and many techniques have\nbeen published already. This comparative study presents a comparison of LOWESS\n(Locally weighted scatterplot smoothing) and RBF (Radial Basis Functions)\napproximation methods on noisy data as they use different approaches. The RBF\napproach is generally convenient for high dimensional scattered data sets. The\nLOWESS method needs finding a subset of nearest points if data are scattered.\nThe experiments proved that LOWESS approximation gives slightly better results\nthan RBF in the case of lower dimension, while in the higher dimensional case\n",
        "pdf_link": "http://arxiv.org/pdf/1801.00432v1"
    },
    {
        "title": "A Fast Algorithm for Line Clipping by Convex Polyhedron in E3",
        "authors": [
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  A new algorithm for line clipping against convex polyhedron is given. The\nsuggested algorithm is faster for higher number of facets of the given\npolyhedron than the traditional Cyrus-Beck's and others algorithms with\ncomplexity O(N) . The suggested algorithm has O(N) complexity in the worst N\ncase and expected O(sqrt(N))) complexity. The speed up is achieved because of\n'known order' of triangles. Some principal results of comparisons of selected\nalgorithms are presented and give some imagination how the proposed algorithm\ncould be used effectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.00441v1"
    },
    {
        "title": "O(lgN) Line Clipping Algorithm in E2",
        "authors": [
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  A new O(lg N) line clipping algorithm in E2 against a convex window is\npresented. The main advantage of the presented algorithm is the principal\nacceleration of the line clipping problem solution. A comparison of the\nproposed algorithm with others shows a significant improvement in run-time.\nExperimental results for selected known algorithms are also shown.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.00442v1"
    },
    {
        "title": "A Voxel-based Rendering Pipeline for Large 3D Line Sets",
        "authors": [
            "Mathias Kanzler",
            "Marc Rautenhaus",
            "Rüdiger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a voxel-based rendering pipeline for large 3D line sets that\nemploys GPU ray-casting to achieve scalable rendering including transparency\nand global illumination effects that cannot be achieved with GPU rasterization.\nEven for opaque lines we demonstrate superior rendering performance compared to\nGPU rasterization of lines, and when transparency is used we can interactively\nrender large amounts of lines that are infeasible to be rendered via\nrasterization. To achieve this, we propose a direction-preserving encoding of\nlines into a regular voxel grid, along with the quantization of directions\nusing face-to-face connectivity in this grid. On the regular grid structure,\nparallel GPU ray-casting is used to determine visible fragments in correct\nvisibility order. To enable interactive rendering of global illumination\neffects like low-frequency shadows and ambient occlusions, illumination\nsimulation is performed during ray-casting on a level-of-detail (LoD) line\nrepresentation that considers the number of lines and their lengths per voxel.\nIn this way we can render effects which are very difficult to render via GPU\nrasterization. A detailed performance and quality evaluation compares our\napproach to rasterization-based rendering of lines.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.01155v1"
    },
    {
        "title": "Vectorization of Line Drawings via PolyVector Fields",
        "authors": [
            "Mikhail Bessmeltsev",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Image tracing is a foundational component of the workflow in graphic design,\nengineering, and computer animation, linking hand-drawn concept images to\ncollections of smooth curves needed for geometry processing and editing. Even\nfor clean line drawings, modern algorithms often fail to faithfully vectorize\njunctions, or points at which curves meet; this produces vector drawings with\nincorrect connectivity. This subtle issue undermines the practical application\nof vectorization tools and accounts for hesitance among artists and engineers\nto use automatic vectorization software. To address this issue, we propose a\nnovel image vectorization method based on state-of-the-art mathematical\nalgorithms for frame field processing. Our algorithm is tailored specifically\nto disambiguate junctions without sacrificing quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.01922v2"
    },
    {
        "title": "Reversible Harmonic Maps between Discrete Surfaces",
        "authors": [
            "Danielle Ezuz",
            "Justin Solomon",
            "Mirela Ben-Chen"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Information transfer between triangle meshes is of great importance in\ncomputer graphics and geometry processing. To facilitate this process, a smooth\nand accurate map is typically required between the two meshes. While such maps\ncan sometimes be computed between nearly-isometric meshes, the more general\ncase of meshes with diverse geometries remains challenging. We propose a novel\napproach for direct map computation between triangle meshes without mapping to\nan intermediate domain, which optimizes for the harmonicity and reversibility\nof the forward and backward maps. Our method is general both in the information\nit can receive as input, e.g. point landmarks, a dense map or a functional map,\nand in the diversity of the geometries to which it can be applied. We\ndemonstrate that our maps exhibit lower conformal distortion than the\nstate-of-the-art, while succeeding in correctly mapping key features of the\ninput shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02453v1"
    },
    {
        "title": "Innovative Non-parametric Texture Synthesis via Patch Permutations",
        "authors": [
            "Ryan Webster"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In this work, we present a non-parametric texture synthesis algorithm capable\nof producing plausible images without copying large tiles of the exemplar. We\nfocus on a simple synthesis algorithm, where we explore two patch match\nheuristics; the well known Bidirectional Similarity (BS) measure and a\nheuristic that finds near permutations using the solution of an entropy\nregularized optimal transport (OT) problem. Innovative synthesis is achieved\nwith a small patch size, where global plausibility relies on the qualities of\nthe match. For OT, less entropic regularization also meant near permutations\nand more plausible images. We examine the tile maps of the synthesized images,\nshowing that they are indeed novel superpositions of the input and contain few\nor no verbatim copies. Synthesis results are compared to a statistical method,\nnamely a random convolutional network. We conclude by remarking simple\nalgorithms using only the input image can synthesize textures decently well and\ncall for more modest approaches in future algorithm design.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04619v1"
    },
    {
        "title": "Edge-Preserving Piecewise Linear Image Smoothing Using Piecewise\n  Constant Filters",
        "authors": [
            "Wei Liu",
            "Wei Xu",
            "Xiaogang Chen",
            "Xiaolin Huang",
            "Chunhua Shen",
            "Jie Yang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Most image smoothing filters in the literature assume a piecewise constant\nmodel of smoothed output images. However, the piecewise constant model\nassumption can cause artifacts such as gradient reversals in applications such\nas image detail enhancement, HDR tone mapping, etc. In these applications, a\npiecewise linear model assumption is more preferred. In this paper, we propose\na simple yet very effective framework to smooth images of piecewise linear\nmodel assumption using classical filters with the piecewise constant model\nassumption. Our method is capable of handling with gradient reversal artifacts\ncaused by the piecewise constant model assumption. In addition, our method can\nfurther help accelerated methods, which need to quantize image intensity values\ninto different bins, to achieve similar results that need a large number of\nbins using a much smaller number of bins. This can greatly reduce the\ncomputational cost. We apply our method to various classical filters with the\npiecewise constant model assumption. Experimental results of several\napplications show the effectiveness of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06928v1"
    },
    {
        "title": "Smooth, Efficient, and Interruptible Zooming and Panning",
        "authors": [
            "Andrew Reach",
            "Chris North"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This paper introduces a novel technique for smooth and efficient zooming and\npanning based on dynamical systems in hyperbolic space. Unlike the technique of\nvan Wijk and Nuij, the animations produced by our technique are smooth at the\nendpoints and when interrupted by a change of target. To analyze the results of\nour technique, we introduce world/screen diagrams, a novel technique for\nvisualizing zooming and panning animations.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.09358v1"
    },
    {
        "title": "Animation-by-Demonstration Computer Puppetry Authoring Framework",
        "authors": [
            "Yaoyuan Cui",
            "Christos Mousas"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This paper presents Master of Puppets (MOP), an animation-by-demonstration\nframework that allows users to control the motion of virtual characters\n(puppets) in real time. In the first step, the user is asked to perform the\nnecessary actions that correspond to the character's motions. The user's\nactions are recorded, and a hidden Markov model (HMM) is used to learn the\ntemporal profile of the actions. During the runtime of the framework, the user\ncontrols the motions of the virtual character based on the specified\nactivities. The advantage of the MOP framework is that it recognizes and\nfollows the progress of the user's actions in real time. Based on the forward\nalgorithm, the method predicts the evolution of the user's actions, which\ncorresponds to the evolution of the character's motion. This method treats\ncharacters as puppets that can perform only one motion at a time. This means\nthat combinations of motion segments (motion synthesis), as well as the\ninterpolation of individual motion sequences, are not provided as\nfunctionalities. By implementing the framework and presenting several computer\npuppetry scenarios, its efficiency and flexibility in animating virtual\ncharacters is demonstrated.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.10040v1"
    },
    {
        "title": "Position-Based Multi-Agent Dynamics for Real-Time Crowd Simulation (MiG\n  paper)",
        "authors": [
            "Tomer Weiss",
            "Alan Litteneker",
            "Chenfanfu Jiang",
            "Demetri Terzopoulos"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Exploiting the efficiency and stability of Position-Based Dynamics (PBD), we\nintroduce a novel crowd simulation method that runs at interactive rates for\nhundreds of thousands of agents. Our method enables the detailed modeling of\nper-agent behavior in a Lagrangian formulation. We model short-range and\nlong-range collision avoidance to simulate both sparse and dense crowds. On the\nparticles representing agents, we formulate a set of positional constraints\nthat can be readily integrated into a standard PBD solver. We augment the\ntentative particle motions with planning velocities to determine the preferred\nvelocities of agents, and project the positions onto the constraint manifold to\neliminate colliding configurations. The local short-range interaction is\nrepresented with collision and frictional contact between agents, as in the\ndiscrete simulation of granular materials. We incorporate a cohesion model for\nmodeling collective behaviors and propose a new constraint for dealing with\npotential future collisions. Our new method is suitable for use in interactive\ngames.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.02673v2"
    },
    {
        "title": "Hierarchical Cloth Simulation using Deep Neural Networks",
        "authors": [
            "Young Jin Oh",
            "Tae Min Lee",
            "In-Kwon Lee"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Fast and reliable physically-based simulation techniques are essential for\nproviding flexible visual effects for computer graphics content. In this paper,\nwe propose a fast and reliable hierarchical cloth simulation method, which\ncombines conventional physically-based simulation with deep neural networks\n(DNN). Simulations of the coarsest level of the hierarchical model are\ncalculated using conventional physically-based simulations, and more detailed\nlevels are generated by inference using DNN models. We demonstrate that our\nmethod generates reliable and fast cloth simulation results through experiments\nunder various conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.03168v1"
    },
    {
        "title": "Automatic thread painting generation",
        "authors": [
            "Xiao-Nan Fang",
            "Bin Liu",
            "Ariel Shamir"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  ThreadTone is an NPR representation of an input image by half-toning using\nthreads on a circle. Current approaches to create ThreadTone paintings greedily\ndraw the chords on the circle. We introduce the concept of chord space, and\ndesign a new algorithm to improve the quality of the thread painting. We use an\noptimization process that estimates the fitness of every chord in the chord\nspace, and an error-diffusion based sampling process that selects a moderate\nnumber of chords to produce the output painting. We used an image similarity\nmeasure to evaluate the quality of our thread painting and also conducted a\nuser study. Our approach can produce high quality results on portraits,\nsketches as well as cartoon pictures.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.04706v1"
    },
    {
        "title": "$C^1$ analysis of 2D subdivision schemes refining point-normal pairs\n  with the circle average",
        "authors": [
            "Evgeny Lipovetsky",
            "Nira Dyn"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This article continues the investigation started in [9] on subdivision\nschemes refining 2D point-normal pairs, obtained by modifying linear\nsubdivision schemes using the circle average. While in [9] the convergence of\nthe Modified Lane-Riesenfeld algorithm and the Modified 4-Point schemes is\nproved, here we show that the curves generated by these two schemes are $C^1$.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.07555v2"
    },
    {
        "title": "\"How to squash a mathematical tomato\", Rubic's cube-like surfaces and\n  their connection to reversible computation",
        "authors": [
            "Ioannis Tamvakis"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Here we show how reversible computation processes, like Margolus diffusion,\ncan be envisioned as physical turning operations on a 2-dimensional rigid\nsurface that is cut by a regular pattern of intersecting circles. We then\nbriefly explore the design-space of these patterns, and report on the discovery\nof an interesting fractal subdivision of space by iterative circle packings. We\ndevise two different ways for creating this fractal, both showing interesting\nproperties, some resembling properties of the dragon curve. The patterns\npresented here can have interesting applications to the engineering of modular,\nkinetic, active surfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.07592v1"
    },
    {
        "title": "Medical Volume Reconstruction Techniques",
        "authors": [
            "Wenhui Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Medical visualization is the use of computers to create 3D images from\nmedical imaging data sets, almost all surgery and cancer treatment in the\ndeveloped world relies on it.Volume visualization techniques includes\niso-surface visualization, mesh visualization and point cloud visualization\ntechniques, these techniques have revolutionized medicine. Much of modern\nmedicine relies on the 3D imaging that is possible with magnetic resonance\nimaging (MRI) scanners, functional magnetic resonance imaging (fMRI)scanners,\npositron emission tomography (PET) scanners, ultrasound imaging (US) scanners,\nX-Ray scanners, bio-marker microscopy imaging scanners and computed tomography\n(CT) scanners, which make 3D images out of 2D slices. The primary goal of this\nreport is the application-oriented optimization of existing volume rendering\nmethods providing interactive frame rates. Techniques are presented for\ntraditional alpha-blending rendering, surface-shaded display, maximum intensity\nprojection (MIP), and fast previewing with fully interactive parameter control.\nDifferent preprocessing strategies are proposed for interactive iso-surface\nrendering and fast previewing, such as the well-known marching cube algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.07710v1"
    },
    {
        "title": "Equalizer 2.0 - Convergence of a Parallel Rendering Framework",
        "authors": [
            "Stefan Eilemann",
            "David Steiner",
            "Renato Pajarola"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Developing complex, real world graphics applications which leverage multiple\nGPUs and computers for interactive 3D rendering tasks is a complex task. It\nrequires expertise in distributed systems and parallel rendering in addition to\nthe application domain itself. We present a mature parallel rendering framework\nwhich provides a large set of features, algorithms and system integration for a\nwide range of real-world research and industry applications. Using the\nEqualizer parallel rendering framework, we show how a wide set of generic\nalgorithms can be integrated in the framework to help application scalability\nand development in many different domains, highlighting how concrete\napplications benefit from the diverse aspects and use cases of Equalizer. We\npresent novel parallel rendering algorithms, powerful abstractions for large\nvisualization setups and virtual reality, as well as new experimental results\nfor parallel rendering and data distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.08022v1"
    },
    {
        "title": "Deep Online Video Stabilization",
        "authors": [
            "Miao Wang",
            "Guo-Ye Yang",
            "Jin-Kun Lin",
            "Ariel Shamir",
            "Song-Hai Zhang",
            "Shao-Ping Lu",
            "Shi-Min Hu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Video stabilization technique is essential for most hand-held captured videos\ndue to high-frequency shakes. Several 2D-, 2.5D- and 3D-based stabilization\ntechniques are well studied, but to our knowledge, no solutions based on deep\nneural networks had been proposed. The reason for this is mostly the shortage\nof training data, as well as the challenge of modeling the problem using neural\nnetworks. In this paper, we solve the video stabilization problem using a\nconvolutional neural network (ConvNet). Instead of dealing with offline\nholistic camera path smoothing based on feature matching, we focus on\nlow-latency real-time camera path smoothing without explicitly representing the\ncamera path. Our network, called StabNet, learns a transformation for each\ninput unsteady frame progressively along the time-line, while creating a more\nstable latent camera path. To train the network, we create a dataset of\nsynchronized steady/unsteady video pairs via a well designed hand-held\nhardware. Experimental results shows that the proposed online method (without\nusing future frames) performs comparatively to traditional offline video\nstabilization methods, while running about 30 times faster. Further, the\nproposed StabNet is able to handle night-time and blurry videos, where existing\nmethods fail in robust feature matching.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.08091v1"
    },
    {
        "title": "Palette-based image decomposition, harmonization, and color transfer",
        "authors": [
            "Jianchao Tan",
            "Jose Echevarria",
            "Yotam Gingold"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a palette-based framework for color composition for visual\napplications. Color composition is a critical aspect of visual applications in\nart, design, and visualization. The color wheel is often used to explain\npleasing color combinations in geometric terms, and, in digital design, to\nprovide a user interface to visualize and manipulate colors. We abstract\nrelationships between palette colors as a compact set of axes describing\nharmonic templates over perceptually uniform color wheels. Our framework\nprovides a basis for a variety of color-aware image operations, such as color\nharmonization and color transfer, and can be applied to videos. To enable our\napproach, we introduce an extremely scalable and efficient yet simple\npalette-based image decomposition algorithm. Our approach is based on the\ngeometry of images in RGBXY-space. This new geometric approach is orders of\nmagnitude more efficient than previous work and requires no numerical\noptimization. We demonstrate a real-time layer decomposition tool. After\npreprocessing, our algorithm can decompose 6 MP images into layers in 20\nmilliseconds. We also conducted three large-scale, wide-ranging perceptual\nstudies on the perception of harmonic colors and harmonization algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01225v3"
    },
    {
        "title": "How could we ignore the lens and pupils of eyeballs: Metamaterial optics\n  for retinal projection",
        "authors": [
            "Yoichi Ochiai"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Retinal projection is required for xR applications that can deliver immersive\nvisual experience throughout the day. If general-purpose retinal projection\nmethods can be realized at a low cost, not only could the image be displayed on\nthe retina using less energy, but there is also the possibility of cutting off\nthe weight of projection unit itself from the AR goggles. Several retinal\nprojection methods have been previously proposed; however, as the lenses and\niris of the eyeball are in front of the retina, which is a limitation of the\neyeball, the proposal of retinal projection is generally fraught with narrow\nviewing angles and small eyebox problems. In this short technical report, we\nintroduce ideas and samples of an optical system for solving the common\nproblems of retinal projection by using the metamaterial mirror (plane\nsymmetric transfer optical system). Using this projection method, the designing\nof retinal projection can becomes easy, and if appropriate optics are\navailable, it would be possible to construct an optical system that allows the\nquick follow-up of retinal projection hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01253v2"
    },
    {
        "title": "Deep Painterly Harmonization",
        "authors": [
            "Fujun Luan",
            "Sylvain Paris",
            "Eli Shechtman",
            "Kavita Bala"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Copying an element from a photo and pasting it into a painting is a\nchallenging task. Applying photo compositing techniques in this context yields\nsubpar results that look like a collage --- and existing painterly stylization\nalgorithms, which are global, perform poorly when applied locally. We address\nthese issues with a dedicated algorithm that carefully determines the local\nstatistics to be transferred. We ensure both spatial and inter-scale\nstatistical consistency and demonstrate that both aspects are key to generating\nquality results. To cope with the diversity of abstraction levels and types of\npaintings, we introduce a technique to adjust the parameters of the transfer\ndepending on the painting. We show that our algorithm produces significantly\nbetter results than photo compositing or global stylization techniques and that\nit enables creative painterly edits that would be otherwise difficult to\nachieve.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03189v4"
    },
    {
        "title": "Deformation Capture via Soft and Stretchable Sensor Arrays",
        "authors": [
            "Oliver Glauser",
            "Daniele Panozzo",
            "Otmar Hilliges",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We propose a hardware and software pipeline to fabricate flexible wearable\nsensors and use them to capture deformations without line of sight. Our first\ncontribution is a low-cost fabrication pipeline to embed multiple aligned\nconductive layers with complex geometries into silicone compounds. Overlapping\nconductive areas from separate layers form local capacitors that measure dense\narea changes. Contrary to existing fabrication methods, the proposed technique\nonly requires hardware that is readily available in modern fablabs. While area\nmeasurements alone are not enough to reconstruct the full 3D deformation of a\nsurface, they become sufficient when paired with a data-driven prior. A novel\nsemi-automatic tracking algorithm, based on an elastic surface geometry\ndeformation, allows to capture ground-truth data with an optical mocap system,\neven under heavy occlusions or partially unobservable markers. The resulting\ndataset is used to train a regressor based on deep neural networks, directly\nmapping the area readings to global positions of surface vertices. We\ndemonstrate the flexibility and accuracy of the proposed hardware and software\nin a series of controlled experiments, and design a prototype of wearable\nwrist, elbow and biceps sensors, which do not require line-of-sight and can be\nworn below regular clothing.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.04013v3"
    },
    {
        "title": "TomoReal: Tomographic Displays",
        "authors": [
            "Seungjae Lee",
            "Youngjin Jo",
            "Dongheon Yoo",
            "Jaebum Cho",
            "Dukho Lee",
            "Byoungho Lee"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Since the history of display technologies began, people have dreamed an\nultimate 3D display system. In order to get close to the dream, 3D displays\nshould provide both of psychological and physiological cues for recognition of\ndepth information. However, it is challenging to satisfy the essential features\nwithout sacrifice in conventional technical values including resolution, frame\nrate, and eye-box. Here, we present a new type of 3D displays: tomographic\ndisplays. We claim that tomographic displays may support extremely wide depth\nof field, quasi-continuous accommodation, omni-directional motion parallax,\npreserved resolution, full frame, and moderate field of view within enough\neye-box. Tomographic displays consist of focus-tunable optics, 2D display\npanel, and fast spatially adjustable backlight. The synchronization of the\nfocus-tunable optics and the backlight enables the 2D display panel to express\nthe depth information. Tomographic displays have various applications including\ntabletop 3D displays, head-up displays, and near-eye stereoscopes. In this\nstudy, we implement a near-eye display named TomoReal, which is one of the most\npromising application of tomographic displays. We conclude with the detailed\nanalysis and thorough discussion for tomographic displays, which would open a\nnew research field.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.04619v1"
    },
    {
        "title": "Normal Image Manipulation for Bas-relief Generation with Hybrid Styles",
        "authors": [
            "Zhongping Ji",
            "Xianfang Sun",
            "Weiyin Ma"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We introduce a normal-based bas-relief generation and stylization method\nwhich is motivated by the recent advancement in this topic. Creating bas-relief\nfrom normal images has successfully facilitated bas-relief modeling in image\nspace. However, the use of normal images in previous work is often restricted\nto certain type of operations only. This paper is intended to extend\nnormal-based methods and construct bas-reliefs from normal images in a\nversatile way. Our method can not only generate a new normal image by combining\nvarious frequencies of existing normal images and details transferring, but\nalso build bas-reliefs from a single RGB image and its edge-based sketch image.\nIn addition, we introduce an auxiliary function to represent a smooth base\nsurface and generate a layered global shape. To integrate above considerations\ninto our framework, we formulate the bas- relief generation as a variational\nproblem which can be solved by a screened Poisson equation. Some advantages of\nour method are that it expands the bas-relief shape space and generates\ndiversified styles of results, and that it is capable of transferring details\nfrom one region to other regions. Our method is easy to implement, and produces\ngood-quality bas-relief models. We experiment our method on a range of normal\nimages and it compares favorably to other popular classic and state-of-the-art\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06092v1"
    },
    {
        "title": "A New Radial Basis Function Approximation with Reproduction",
        "authors": [
            "Zuzana Majdisova",
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Approximation of scattered geometric data is often a task in many engineering\nproblems. The Radial Basis Function (RBF) approximation is appropriate for\nlarge scattered (unordered) datasets in d-dimensional space. This method is\nuseful for a higher dimension d>=2, because the other methods require a\nconversion of a scattered dataset to a semi-regular mesh using some\ntessellation techniques, which is computationally expensive. The RBF\napproximation is non-separable, as it is based on a distance of two points. It\nleads to a solution of overdetermined Linear System of Equations (LSE). In this\npaper a new RBF approximation method is derived and presented. The presented\napproach is applicable for d dimensional cases in general.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06662v1"
    },
    {
        "title": "Metamorphs: Bistable Planar Structures",
        "authors": [
            "Gaurav Bharaj",
            "Danny Kaufman",
            "Etienne Vouga",
            "Hanspeter Pfister"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Extreme deformation can drastically morph a structure from one structural\nform into another. Programming such deformation properties into the structure\nis often challenging and in many cases an impossible task. The morphed forms do\nnot hold and usually relapse to the original form, where the structure is in\nits lowest energy state. For example, a stick, when bent, resists its bent form\nand tends to go back to its initial straight form, where it holds the least\namount of potential energy.\n  In this project, we present a computational design method which can create\nfabricable planar structure that can morph into two different bistable forms.\nOnce the user provides the initial desired forms, the method automatically\ncreates support structures (internal springs), such that, the structure can not\nonly morph, but also hold the respective forms under external force\napplication. We achieve this through an iterative nonlinear optimization\nstrategy for shaping the potential energy of the structure in the two forms\nsimultaneously. Our approach guarantees first and second-order stability with\nrespect to the potential energy of the bistable structure.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06996v1"
    },
    {
        "title": "Half-Space Power Diagrams and Discrete Surface Offsets",
        "authors": [
            "Zhen Chen",
            "Daniele Panozzo",
            "Jeremie Dumas"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present an efficient, trivially parallelizable algorithm to compute offset\nsurfaces of shapes discretized using a dexel data structure. Our algorithm is\nbased on a two-stage sweeping procedure that is simple to implement and\nefficient, entirely avoiding volumetric distance field computations typical of\nexisting methods. Our construction is based on properties of half-space power\ndiagrams, where each seed is only visible by a half-space, which were never\nused before for the computation of surface offsets. The primary application of\nour method is interactive modeling for digital fabrication. Our technique\nenables a user to interactively process high-resolution models. It is also\nuseful in a plethora of other geometry processing tasks requiring fast,\napproximate offsets, such as topology optimization, collision detection, and\nskeleton extraction. We present experimental timings, comparisons with previous\napproaches, and provide a reference implementation in the supplemental\nmaterial.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.08968v2"
    },
    {
        "title": "Taichi: An Open-Source Computer Graphics Library",
        "authors": [
            "Yuanming Hu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  An ideal software system in computer graphics should be a combination of\ninnovative ideas, solid software engineering and rapid development. However, in\nreality these requirements are seldom met simultaneously. In this paper, we\npresent early results on an open-source library named Taichi\n(http://taichi.graphics), which alleviates this practical issue by providing an\naccessible, portable, extensible, and high-performance infrastructure that is\nreusable and tailored for computer graphics. As a case study, we share our\nexperience in building a novel physical simulation system using Taichi.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.09293v1"
    },
    {
        "title": "Hatching for 3D prints: line-based halftoning for dual extrusion fused\n  deposition modeling",
        "authors": [
            "Tim Kuipers",
            "Willemijn Elkhuizen",
            "Jouke Verlinden",
            "Eugeni Doubrovski"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This work presents a halftoning technique to manufacture 3D objects with the\nappearance of continuous grayscale imagery for Fused Deposition Modeling (FDM)\nprinters. While droplet-based dithering is a common halftoning technique, this\nis not applicable to FDM printing, since FDM builds up objects by extruding\nmaterial in semi-continuous paths. The line-based halftoning principle called\n'hatching' is applied to the line patterns naturally occuring in FDM prints,\nwhich are built up in a layer-by-layer fashion. The proposed halftoning\ntechnique isn't limited by the challenges existing techniques face; existing\nFDM coloring techniques greatly influence the surface geometry and deteriorate\nwith surface slopes deviating from vertical or greatly influence the basic\nparameters of the printing process and thereby the structural properties of the\nresulting product. Furthermore, the proposed technique has little effect on\nprinting time. Experiments on a dual-nozzle FDM printer show promising results.\nFuture work is required to calibrate the perceived tone.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.01375v2"
    },
    {
        "title": "A Radiative Transfer Framework for Spatially-Correlated Materials",
        "authors": [
            "Adrian Jarabo",
            "Carlos Aliaga",
            "Diego Gutierrez"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We introduce a non-exponential radiative framework that takes into account\nthe local spatial correlation of scattering particles in a medium. Most\nprevious works in graphics have ignored this, assuming uncorrelated media with\na uniform, random local distribution of particles. However, positive and\nnegative correlation lead to slower- and faster-than-exponential attenuation\nrespectively, which cannot be predicted by the Beer-Lambert law. As our results\nshow, this has a major effect on extinction, and thus appearance. From recent\nadvances in neutron transport, we first introduce our Extended Generalized\nBoltzmann Equation, and develop a general framework for light transport in\ncorrelated media. We lift the limitations of the original formulation,\nincluding an analysis of the boundary conditions, and present a model suitable\nfor computer graphics, based on optical properties of the media and statistical\ndistributions of scatterers. In addition, we present an analytic expression for\ntransmittance in the case of positive correlation, and show how to incorporate\nit efficiently into a Monte Carlo renderer. We show results with a wide range\nof both positive and negative correlation, and demonstrate the differences\ncompared to classic light transport.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.02651v1"
    },
    {
        "title": "LSTM-Based Facial Performance Capture Using Embedding Between\n  Expressions",
        "authors": [
            "Hsien-Yu Meng",
            "Tzu-heng Lin",
            "Xiubao Jiang",
            "Yao Lu",
            "Jiangtao Wen"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a novel end-to-end framework for facial performance capture given\na monocular video of an actor's face. Our framework are comprised of 2 parts.\nFirst, to extract the information in the frames, we optimize a triplet loss to\nlearn the embedding space which ensures the semantically closer facial\nexpressions are closer in the embedding space and the model can be transferred\nto distinguish the expressions that are not presented in the training dataset.\nSecond, the embeddings are fed into an LSTM network to learn the deformation\nbetween frames. In the experiments, we demonstrated that compared to other\nmethods, our method can distinguish the delicate motion around lips and\nsignificantly reduce jitters between the tracked meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03874v4"
    },
    {
        "title": "Building Anatomically Realistic Jaw Kinematics Model from Data",
        "authors": [
            "Wenwu Yang",
            "Nathan Marshak",
            "Daniel Sýkora",
            "Srikumar Ramalingam",
            "Ladislav Kavan"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This paper considers a different aspect of anatomical face modeling:\nkinematic modeling of the jaw, i.e., the Temporo-Mandibular Joint (TMJ).\nPrevious work often relies on simple models of jaw kinematics, even though the\nactual physiological behavior of the TMJ is quite complex, allowing not only\nfor mouth opening, but also for some amount of sideways (lateral) and\nfront-to-back (protrusion) motions. Fortuitously, the TMJ is the only joint\nwhose kinematics can be accurately measured with optical methods, because the\nbones of the lower and upper jaw are rigidly connected to the lower and upper\nteeth. We construct a person-specific jaw kinematic model by asking an actor to\nexercise the entire range of motion of the jaw while keeping the lips open so\nthat the teeth are at least partially visible. This performance is recorded\nwith three calibrated cameras. We obtain highly accurate 3D models of the teeth\nwith a standard dental scanner and use these models to reconstruct the rigid\nbody trajectories of the teeth from the videos (markerless tracking). The\nrelative rigid transformations samples between the lower and upper teeth are\nmapped to the Lie algebra of rigid body motions in order to linearize the\nrotational motion. Our main contribution is to fit these samples with a\nthree-dimensional nonlinear model parameterizing the entire range of motion of\nthe TMJ. We show that standard Principal Component Analysis (PCA) fails to\ncapture the nonlinear trajectories of the moving mandible. However, we found\nthese nonlinearities can be captured with a special modification of autoencoder\nneural networks known as Nonlinear PCA. By mapping back to the Lie group of\nrigid transformations, we obtain parameterization of the jaw kinematics which\nprovides an intuitive interface allowing the animators to explore realistic jaw\nmotions in a user-friendly way.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.05903v1"
    },
    {
        "title": "RLFC: Random Access Light Field Compression using Key Views and Bounded\n  Integer Encoding",
        "authors": [
            "Srihari Pratapa",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a new hierarchical compression scheme for encoding light field\nimages (LFI) that is suitable for interactive rendering. Our method (RLFC)\nexploits redundancies in the light field images by constructing a tree\nstructure. The top level (root) of the tree captures the common high-level\ndetails across the LFI, and other levels (children) of the tree capture\nspecific low-level details of the LFI. Our decompressing algorithm corresponds\nto tree traversal operations and gathers the values stored at different levels\nof the tree. Furthermore, we use bounded integer sequence encoding which\nprovides random access and fast hardware decoding for compressing the blocks of\nchildren of the tree. We have evaluated our method for 4D two-plane\nparameterized light fields. The compression rates vary from 0.08 - 2.5 bits per\npixel (bpp), resulting in compression ratios of around 200:1 to 20:1 for a PSNR\nquality of 40 to 50 dB. The decompression times for decoding the blocks of LFI\nare 1 - 3 microseconds per channel on an NVIDIA GTX-960 and we can render new\nviews with a resolution of 512X512 at 200 fps. Our overall scheme is simple to\nimplement and involves only bit manipulations and integer arithmetic\noperations.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06019v7"
    },
    {
        "title": "Improved Shortest Path Maps with GPU Shaders",
        "authors": [
            "Renato Farias",
            "Marcelo Kallmann"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present in this paper several improvements for computing shortest path\nmaps using OpenGL shaders. The approach explores GPU rasterization as a way to\npropagate optimal costs on a polygonal 2D environment, producing shortest path\nmaps which can efficiently be queried at run-time. Our improved method relies\non Compute Shaders for improved performance, does not require any CPU\npre-computation, and handles shortest path maps both with source points and\nwith line segment sources. The produced path maps partition the input\nenvironment into regions sharing a same parent point along the shortest path to\nthe closest source point or segment source. Our method produces paths with\nglobal optimality, a characteristic which has been mostly neglected in animated\nvirtual environments. The proposed approach is particularly suitable for the\nanimation of multiple agents moving toward the entrances or exits of a virtual\nenvironment, a situation which is efficiently represented with the proposed\npath maps.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.08500v1"
    },
    {
        "title": "Area-preserving parameterizations for spherical ellipses",
        "authors": [
            "Ibón Guillén",
            "Carlos Ureña",
            "Alan King",
            "Marcos Fajardo",
            "Iliyan Georgiev",
            "Jorge López-Moreno",
            "Adrian Jarabo"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present new methods for uniformly sampling the solid angle subtended by a\ndisk. To achieve this, we devise two novel area-preserving mappings from the\nunit square $[0,1]^2$ to a spherical ellipse (i.e. the projection of the disk\nonto the unit sphere). These mappings allow for low-variance stratified\nsampling of direct illumination from disk-shaped light sources. We discuss how\nto efficiently incorporate our methods into a production renderer and\ndemonstrate the quality of our maps, showing significantly lower variance than\nprevious work.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09048v1"
    },
    {
        "title": "The Vector Heat Method",
        "authors": [
            "Nicholas Sharp",
            "Yousuf Soliman",
            "Keenan Crane"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This paper describes a method for efficiently computing parallel transport of\ntangent vectors on curved surfaces, or more generally, any vector-valued data\non a curved manifold. More precisely, it extends a vector field defined over\nany region to the rest of the domain via parallel transport along shortest\ngeodesics. This basic operation enables fast, robust algorithms for\nextrapolating level set velocities, inverting the exponential map, computing\ngeometric medians and Karcher/Fr\\'{e}chet means of arbitrary distributions,\nconstructing centroidal Voronoi diagrams, and finding consistently ordered\nlandmarks. Rather than evaluate parallel transport by explicitly tracing\ngeodesics, we show that it can be computed via a short-time heat flow involving\nthe connection Laplacian. As a result, transport can be achieved by solving\nthree prefactored linear systems, each akin to a standard Poisson problem. To\nimplement the method we need only a discrete connection Laplacian, which we\ndescribe for a variety of geometric data structures (point clouds, polygon\nmeshes, etc). We also study the numerical behavior of our method, showing\nempirically that it converges under refinement, and augment the construction of\nintrinsic Delaunay triangulations (iDT) so that they can be used in the context\nof tangent vector field processing.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09170v3"
    },
    {
        "title": "Second-Order Occlusion-Aware Volumetric Radiance Caching",
        "authors": [
            "Julio Marco",
            "Adrian Jarabo",
            "Wojciech Jarosz",
            "Diego Gutierrez"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a second-order gradient analysis of light transport in\nparticipating media and use this to develop an improved radiance caching\nalgorithm for volumetric light transport. We adaptively sample and interpolate\nradiance from sparse points in the medium using a second-order Hessian-based\nerror metric to determine when interpolation is appropriate. We derive our\nmetric from each point's incoming light field, computed by using a proxy\ntriangulation-based representation of the radiance reflected by the surrounding\nmedium and geometry. We use this representation to efficiently compute the\nfirst- and second-order derivatives of the radiance at the cache points while\naccounting for occlusion changes.\n  We also propose a self-contained two-dimensional model for light transport in\nmedia and use it to validate and analyze our approach, demonstrating that our\nmethod outperforms previous radiance caching algorithms both in terms of\naccurate derivative estimates and final radiance extrapolation. We generalize\nthese findings to practical three-dimensional scenarios, where we show improved\nresults while reducing computation time by up to 30\\% compared to previous\nwork.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09258v1"
    },
    {
        "title": "DeepToF: Off-the-Shelf Real-Time Correction of Multipath Interference in\n  Time-of-Flight Imaging",
        "authors": [
            "Julio Marco",
            "Quercus Hernandez",
            "Adolfo Muñoz",
            "Yue Dong",
            "Adrian Jarabo",
            "Min H. Kim",
            "Xin Tong",
            "Diego Gutierrez"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Time-of-flight (ToF) imaging has become a widespread technique for depth\nestimation, allowing affordable off-the-shelf cameras to provide depth maps in\nreal time. However, multipath interference (MPI) resulting from indirect\nillumination significantly degrades the captured depth. Most previous works\nhave tried to solve this problem by means of complex hardware modifications or\ncostly computations. In this work we avoid these approaches, and propose a new\ntechnique that corrects errors in depth caused by MPI that requires no camera\nmodifications, and corrects depth in just 10 milliseconds per frame. By\nobserving that most MPI information can be expressed as a function of the\ncaptured depth, we pose MPI removal as a convolutional approach, and model it\nusing a convolutional neural network. In particular, given that the input and\noutput data present similar structure, we base our network in an autoencoder,\nwhich we train in two stages: first, we use the encoder (convolution filters)\nto learn a suitable basis to represent corrupted range images; then, we train\nthe decoder (deconvolution filters) to correct depth from the learned basis\nfrom synthetically generated scenes. This approach allows us to tackle the lack\nof reference data, by using a large-scale captured training set with corrupted\ndepth to train the encoder, and a smaller synthetic training set with ground\ntruth depth to train the corrector stage of the network, which we generate by\nusing a physically-based, time-resolved rendering. We demonstrate and validate\nour method on both synthetic and real complex scenarios, using an off-the-shelf\nToF camera, and with only the captured incorrect depth as input.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09305v1"
    },
    {
        "title": "VisemeNet: Audio-Driven Animator-Centric Speech Animation",
        "authors": [
            "Yang Zhou",
            "Zhan Xu",
            "Chris Landreth",
            "Evangelos Kalogerakis",
            "Subhransu Maji",
            "Karan Singh"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a novel deep-learning based approach to producing animator-centric\nspeech motion curves that drive a JALI or standard FACS-based production\nface-rig, directly from input audio. Our three-stage Long Short-Term Memory\n(LSTM) network architecture is motivated by psycho-linguistic insights:\nsegmenting speech audio into a stream of phonetic-groups is sufficient for\nviseme construction; speech styles like mumbling or shouting are strongly\nco-related to the motion of facial landmarks; and animator style is encoded in\nviseme motion curve profiles. Our contribution is an automatic real-time\nlip-synchronization from audio solution that integrates seamlessly into\nexisting animation pipelines. We evaluate our results by: cross-validation to\nground-truth data; animator critique and edits; visual comparison to recent\ndeep-learning lip-synchronization solutions; and showing our approach to be\nresilient to diversity in speaker and language.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09488v1"
    },
    {
        "title": "Progressive Transient Photon Beams",
        "authors": [
            "Julio Marco",
            "Ibón Guillén",
            "Wojciech Jarosz",
            "Diego Gutierrez",
            "Adrian Jarabo"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In this work we introduce a novel algorithm for transient rendering in\nparticipating media. Our method is consistent, robust, and is able to generate\nanimations of time-resolved light transport featuring complex caustic light\npaths in media. We base our method on the observation that the spatial\ncontinuity provides an increased coverage of the temporal domain, and\ngeneralize photon beams to transient-state. We extend the beam steady-state\nradiance estimates to include the temporal domain. Then, we develop a\nprogressive version of spatio-temporal density estimations, that converges to\nthe correct solution with finite memory requirements by iteratively averaging\nseveral realizations of independent renders with a progressively reduced kernel\nbandwidth. We derive the optimal convergence rates accounting for space and\ntime kernels, and demonstrate our method against previous consistent transient\nrendering methods for participating media.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09562v1"
    },
    {
        "title": "Designing Volumetric Truss Structures",
        "authors": [
            "Rahul Arora",
            "Alec Jacobson",
            "Timothy R. Langlois",
            "Yijiang Huang",
            "Caitlin Mueller",
            "Wojciech Matusik",
            "Ariel Shamir",
            "Karan Singh",
            "David I. W. Levin"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present the first algorithm for designing volumetric Michell Trusses. Our\nmethod uses a parametrization approach to generate trusses made of structural\nelements aligned with the primary direction of an object's stress field. Such\ntrusses exhibit high strength-to-weight ratios. We demonstrate the structural\nrobustness of our designs via a posteriori physical simulation. We believe our\nalgorithm serves as an important complement to existing structural optimization\ntools and as a novel standalone design tool itself.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.00706v3"
    },
    {
        "title": "Line Drawings from 3D Models",
        "authors": [
            "Pierre Bénard",
            "Aaron Hertzmann"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This tutorial describes the geometry and algorithms for generating line\ndrawings from 3D models, focusing on occluding contours.\n  The geometry of occluding contours on meshes and on smooth surfaces is\ndescribed in detail, together with algorithms for extracting contours,\ncomputing their visibility, and creating stylized renderings and animations.\nExact methods and hardware-accelerated fast methods are both described, and the\ntrade-offs between different methods are discussed. The tutorial brings\ntogether and organizes material that, at present, is scattered throughout the\nliterature. It also includes some novel explanations, and implementation tips.\n  A thorough survey of the field of non-photorealistic 3D rendering is also\nincluded, covering other kinds of line drawings and artistic shading.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.01175v2"
    },
    {
        "title": "Learning Bidirectional LSTM Networks for Synthesizing 3D Mesh Animation\n  Sequences",
        "authors": [
            "Yi-Ling Qiao",
            "Lin Gao",
            "Yu-Kun Lai",
            "Shihong Xia"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In this paper, we present a novel method for learning to synthesize 3D mesh\nanimation sequences with long short-term memory (LSTM) blocks and mesh-based\nconvolutional neural networks (CNNs). Synthesizing realistic 3D mesh animation\nsequences is a challenging and important task in computer animation. To achieve\nthis, researchers have long been focusing on shape analysis to develop new\ninterpolation and extrapolation techniques. However, such techniques have\nlimited learning capabilities and therefore can produce unrealistic animation.\nDeep architectures that operate directly on mesh sequences remain unexplored,\ndue to the following major barriers: meshes with irregular triangles, sequences\ncontaining rich temporal information and flexible deformations. To address\nthese, we utilize convolutional neural networks defined on triangular meshes\nalong with a shape deformation representation to extract useful features,\nfollowed by LSTM cells that iteratively process the features. To allow\ncompletion of a missing mesh sequence from given endpoints, we propose a new\nweight-shared bidirectional structure. The bidirectional generation loss also\nhelps mitigate error accumulation over iterations. Benefiting from all these\ntechnical advances, our approach outperforms existing methods in sequence\nprediction and completion both qualitatively and quantitatively. Moreover, this\nnetwork can also generate follow-up frames conditioned on initial shapes and\nimprove the accuracy as more bootstrap models are provided, which other works\nin the geometry processing domain cannot achieve.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.02042v1"
    },
    {
        "title": "Seamless Parametrization with Arbitrarily Prescribed Cones",
        "authors": [
            "Marcel Campen",
            "Hanxiao Shen",
            "Jiaran Zhou",
            "Denis Zorin"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Seamless global parametrization of surfaces is a key operation in geometry\nprocessing, e.g. for high-quality quad mesh generation. A common approach is to\nprescribe the parametric domain structure, in particular the locations of\nparametrization singularities (cones), and solve a non-convex optimization\nproblem minimizing a distortion measure, with local injectivity imposed through\neither constraints or barrier terms. In both cases, an initial valid\nparametrization is essential to serve as feasible starting point for obtaining\nan optimized solution. While convexified versions of the constraints eliminate\nthis initialization requirement, they narrow the range of solutions, causing\nsome problem instances that actually do have a solution to become infeasible.\nWe demonstrate that for arbitrary given sets of topologically admissible\nparametric cones with prescribed curvature, a global seamless parametrization\nalways exists (with the exception of one well-known case). Importantly, our\nproof is constructive and directly leads to a general algorithm for computing\nsuch parametrizations. Most distinctively, this algorithm is bootstrapped with\na convex optimization problem (solving for a conformal map), in tandem with a\nsimple linear equation system (determining a seamless modification of this\nmap). This initial map can then serve as valid starting point and be optimized\nwith respect to application specific distortion measures using existing\ninjectivity preserving methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.02460v1"
    },
    {
        "title": "Subdivision Directional Fields",
        "authors": [
            "Bram Custers",
            "Amir Vaxman"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a novel linear subdivision scheme for face-based tangent\ndirectional fields on triangle meshes. Our subdivision scheme is based on a\nnovel coordinate-free representation of directional fields as halfedge-based\nscalar quantities, bridging the finite-element representation with discrete\nexterior calculus. By commuting with differential operators, our subdivision is\nstructure-preserving: it reproduces curl-free fields precisely, and reproduces\ndivergence-free fields in the weak sense. Moreover, our subdivision scheme\ndirectly extends to directional fields with several vectors per face by working\non the branched covering space. Finally, we demonstrate how our scheme can be\napplied to directional-field design, advection, and robust earth mover's\ndistance computation, for efficient and robust computation.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.06884v2"
    },
    {
        "title": "Measuring the Effects of Scalar and Spherical Colormaps on Ensembles of\n  DMRI Tubes",
        "authors": [
            "Jian Chen",
            "Guohao Zhang",
            "Wesley Chiou",
            "David H. Laidlaw",
            "Alexander P. Auchus"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We report empirical study results on the color encoding of ensemble scalar\nand orientation to visualize diffusion magnetic resonance imaging (DMRI) tubes.\nThe experiment tested six scalar colormaps for average fractional anisotropy\n(FA) tasks (grayscale, blackbody, diverging, isoluminant-rainbow,\nextended-blackbody, and coolwarm) and four three-dimensional (3D) directional\nencodings for tract tracing tasks (uniform gray, absolute, eigenmap, and Boy's\nsurface embedding). We found that extended-blackbody, coolwarm, and blackbody\nremain the best three approaches for identifying ensemble average in 3D.\nIsoluminant-rainbow coloring led to the same ensemble mean accuracy as other\ncolormaps. However, more than 50% of the answers consistently had higher\nestimates of the ensemble average, independent of the mean values. Hue, not\nluminance, influences ensemble estimates of mean values. For ensemble\norientation-tracing tasks, we found that the Boy's surface embedding (greatest\nspatial resolution and contrast) and absolute color (lowest spatial resolution\nand contrast) schemes led to more accurate answers than the eigenmaps scheme\n(medium resolution and contrast), acting as the uncanny-valley phenomenon of\nvisualization design in terms of accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.07882v1"
    },
    {
        "title": "A System for Acquiring, Processing, and Rendering Panoramic Light Field\n  Stills for Virtual Reality",
        "authors": [
            "Ryan S. Overbeck",
            "Daniel Erickson",
            "Daniel Evangelakos",
            "Matt Pharr",
            "Paul Debevec"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a system for acquiring, processing, and rendering panoramic light\nfield still photography for display in Virtual Reality (VR). We acquire\nspherical light field datasets with two novel light field camera rigs designed\nfor portable and efficient light field acquisition. We introduce a novel\nreal-time light field reconstruction algorithm that uses a per-view geometry\nand a disk-based blending field. We also demonstrate how to use a light field\nprefiltering operation to project from a high-quality offline reconstruction\nmodel into our real-time model while suppressing artifacts. We introduce a\npractical approach for compressing light fields by modifying the VP9 video\ncodec to provide high quality compression with real-time, random access\ndecompression.\n  We combine these components into a complete light field system offering\nconvenient acquisition, compact file size, and high-quality rendering while\ngenerating stereo views at 90Hz on commodity VR hardware. Using our system, we\nbuilt a freely available light field experience application called Welcome to\nLight Fields featuring a library of panoramic light field stills for consumer\nVR which has been downloaded over 15,000 times.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.08860v1"
    },
    {
        "title": "Spherical Parameterization Balancing Angle and Area Distortions",
        "authors": [
            "Saad Nadeem",
            "Zhengyu Su",
            "Wei Zeng",
            "Arie Kaufman",
            "Xianfeng Gu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This work presents a novel framework for spherical mesh parameterization. An\nefficient angle-preserving spherical parameterization algorithm is introduced,\nwhich is based on dynamic Yamabe flow and the conformal welding method with\nsolid theoretic foundation. An area-preserving spherical parameterization is\nalso discussed, which is based on discrete optimal mass transport theory.\nFurthermore, a spherical parameterization algorithm, which is based on the\npolar decomposition method, balancing angle distortion and area distortion is\npresented. The algorithms are tested on 3D geometric data and the experiments\ndemonstrate the efficiency and efficacy of the proposed methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.09031v1"
    },
    {
        "title": "Single-Image SVBRDF Capture with a Rendering-Aware Deep Network",
        "authors": [
            "Valentin Deschaintre",
            "Miika Aittala",
            "Fredo Durand",
            "George Drettakis",
            "Adrien Bousseau"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Texture, highlights, and shading are some of many visual cues that allow\nhumans to perceive material appearance in single pictures. Yet, recovering\nspatially-varying bi-directional reflectance distribution functions (SVBRDFs)\nfrom a single image based on such cues has challenged researchers in computer\ngraphics for decades. We tackle lightweight appearance capture by training a\ndeep neural network to automatically extract and make sense of these visual\ncues. Once trained, our network is capable of recovering per-pixel normal,\ndiffuse albedo, specular albedo and specular roughness from a single picture of\na flat surface lit by a hand-held flash. We achieve this goal by introducing\nseveral innovations on training data acquisition and network design. For\ntraining, we leverage a large dataset of artist-created, procedural SVBRDFs\nwhich we sample and render under multiple lighting directions. We further\namplify the data by material mixing to cover a wide diversity of shading\neffects, which allows our network to work across many material classes.\nMotivated by the observation that distant regions of a material sample often\noffer complementary visual cues, we design a network that combines an\nencoder-decoder convolutional track for local feature extraction with a\nfully-connected track for global feature extraction and propagation. Many\nimportant material effects are view-dependent, and as such ambiguous when\nobserved in a single image. We tackle this challenge by defining the loss as a\ndifferentiable SVBRDF similarity metric that compares the renderings of the\npredicted maps against renderings of the ground truth from several lighting and\nviewing directions. Combined together, these novel ingredients bring clear\nimprovement over state of the art methods for single-shot capture of spatially\nvarying BRDFs.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.09718v1"
    },
    {
        "title": "Lightweight Structure Design Under Force Location Uncertainty",
        "authors": [
            "Erva Ulu",
            "James McCann",
            "Levent Burak Kara"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We introduce a lightweight structure optimization approach for problems in\nwhich there is uncertainty in the force locations. Such uncertainty may arise\ndue to force contact locations that change during use or are simply unknown a\npriori. Given an input 3D model, regions on its boundary where arbitrary normal\nforces may make contact, and a total force-magnitude budget, our algorithm\ngenerates a minimum weight 3D structure that withstands any force configuration\ncapped by the budget. Our approach works by repeatedly finding the most\ncritical force configuration and altering the internal structure accordingly. A\nkey issue, however, is that the critical force configuration changes as the\nstructure evolves, resulting in a significant computational challenge. To\naddress this, we propose an efficient critical instant analysis approach.\nCombined with a reduced order formulation, our method provides a practical\nsolution to the structural optimization problem. We demonstrate our method on a\nvariety of models and validate it with mechanical tests.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.11154v2"
    },
    {
        "title": "Content-Preserving Image Stitching with Regular Boundary Constraints",
        "authors": [
            "Yun Zhang",
            "Yu-Kun Lai",
            "Fang-Lue Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This paper proposes an approach to content-preserving stitching of images\nwith regular boundary constraints, which aims to stitch multiple images to\ngenerate a panoramic image with regular boundary. Existing methods treat image\nstitching and rectangling as two separate steps, which may result in suboptimal\nresults as the stitching process is not aware of the further warping needs for\nrectangling. We address these limitations by formulating image stitching with\nregular boundaries in a unified optimization. Starting from the initial\nstitching results produced by traditional warping-based optimization, we obtain\nthe irregular boundary from the warped meshes by polygon Boolean operations\nwhich robustly handle arbitrary mesh compositions, and by analyzing the\nirregular boundary construct a piecewise rectangular boundary. Based on this,\nwe further incorporate straight line preserving and regular boundary\nconstraints into the image stitching framework, and conduct iterative\noptimization to obtain an optimal piecewise rectangular boundary, thus can make\nthe panoramic boundary as close as possible to a rectangle, while reducing\nunwanted distortions. We further extend our method to panoramic videos and\nselfie photography, by integrating the temporal coherence and portrait\npreservation into the optimization. Experiments show that our method\nefficiently produces visually pleasing panoramas with regular boundaries and\nunnoticeable distortions.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.11220v2"
    },
    {
        "title": "Accurate control of a pan-tilt system based on parameterization of\n  rotational motion",
        "authors": [
            "JungHyun Byun",
            "SeungHo Chae",
            "TackDon Han"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  A pan-tilt camera system has been adopted by a variety of fields since it can\ncover a wide range of region compared to a single fixated camera setup. Yet\nmany studies rely on factory-assembled and calibrated platforms and assume an\nideal rotation where rotation axes are perfectly aligned with the optical axis\nof the local camera. However, in a user-created setup where a pan-tilting\nmechanism is arbitrarily assembled, the kinematic configurations may be\ninaccurate or unknown, violating ideal rotation. These discrepancies in the\nmodel with the real physics result in erroneous servo manipulation of the\npan-tilting system. In this paper, we propose an accurate control mechanism for\narbitrarily-assembled pan-tilt camera systems. The proposed method formulates\npan-tilt rotations as motion along great circle trajectories and calibrates its\nmodel parameters, such as positions and vectors of rotation axes, in 3D space.\nThen, one can accurately servo pan-tilt rotations with pose estimation from\ninverse kinematics of their transformation. The comparative experiment\ndemonstrates out-performance of the proposed method, in terms of accurately\nlocalizing target points in world coordinates, after being rotated from their\ncaptured camera frames.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.00232v1"
    },
    {
        "title": "AIR: Anywhere Immersive Reality with User-Perspective Projection",
        "authors": [
            "JungHyun Byun",
            "SeungHo Chae",
            "YoonSik Yang",
            "TackDon Han"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Projection-based augmented reality (AR) has much potential, but is limited in\nthat it requires burdensome installations and prone to geometric distortions on\ndisplay surface. To overcome these limitations, we propose AIR. It can be\ncarried and placed anywhere to project AR using pan/tilting motors, while\nproviding the user with distortion-free projection of a correct 3D view.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.00233v1"
    },
    {
        "title": "Fast and Accurate Reconstruction of Pan-Tilt RGB-D Scans via Axis Bound\n  Registration",
        "authors": [
            "Jung-Hyun Byun",
            "Tack-Don Han"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  A fast and accurate algorithm is presented for registering scans from an\nRGB-D camera on a pan-tilt platform. The pan-tilt RGB-D camera rotates and\nscans the entire scene in an automated fashion. The proposed algorithm exploits\nthe movement of the camera that is bound by the two rotation axes of the servo\nmotors so as to realize fast and accurate registration of acquired point\nclouds. The rotation parameters, including the rotation axes, pan-tilt\ntransformations and the servo control mechanism, are calibrated beforehand.\nSubsequently, fast global registration can be performed during online operation\nwith transformation matrices formed by the calibrated rotation axes and angles.\nIn local registration, features are extracted and matched between two scenes.\nFalse-positive correspondences, whose distances to the rotation trajectories\nexceed a threshold, are rejected. Then, a more accurate registration can be\nachieved by minimizing the residual distances between corresponding points,\nwhile transformations are bound to the rotation axes. Finally, the preliminary\nalignment result is input to the iterative closed point algorithm to compute\nthe final transformation. Results of comparative experiments validate that the\nproposed method outperforms state-of-the-art algorithms of various approaches\nbased on camera calibration, global registration, and\nsimultaneous-localization-and-mapping in terms of root-mean-square error and\ncomputation time.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.00240v3"
    },
    {
        "title": "Heter-Sim: Heterogeneous multi-agent systems simulation by interactive\n  data-driven optimization",
        "authors": [
            "Jiaping Ren",
            "Wei Xiang",
            "Yangxi Xiao",
            "Ruigang Yang",
            "Dinesh Manocha",
            "Xiaogang Jin"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Interactive multi-agent simulation algorithms are used to compute the\ntrajectories and behaviors of different entities in virtual reality scenarios.\nHowever, current methods involve considerable parameter tweaking to generate\nplausible behaviors. We introduce a novel approach (Heter-Sim) that combines\nphysics-based simulation methods with data-driven techniques using an\noptimization-based formulation. Our approach is general and can simulate\nheterogeneous agents corresponding to human crowds, traffic, vehicles, or\ncombinations of different agents with varying dynamics. We estimate motion\nstates from real-world datasets that include information about position,\nvelocity, and control direction. Our optimization algorithm considers several\nconstraints, including velocity continuity, collision avoidance, attraction,\nand direction control. To accelerate the computations, we reduce the search\nspace for both collision avoidance and optimal solution computation. Heter-Sim\ncan simulate tens or hundreds of agents at interactive rates and we compare its\naccuracy with real-world datasets and prior algorithms. We also perform user\nstudies that evaluate the plausible behaviors generated by our algorithm and a\nuser study that evaluates the plausibility of our algorithm via VR.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.00307v1"
    },
    {
        "title": "Learning to Predict Image-based Rendering Artifacts with Respect to a\n  Hidden Reference Image",
        "authors": [
            "Mojtaba Bemana",
            "Joachim Keinert",
            "Karol Myszkowski",
            "Michel Bätz",
            "Matthias Ziegler",
            "Hans-Peter Seidel",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Image metrics predict the perceived per-pixel difference between a reference\nimage and its degraded (e. g., re-rendered) version. In several important\napplications, the reference image is not available and image metrics cannot be\napplied. We devise a neural network architecture and training procedure that\nallows predicting the MSE, SSIM or VGG16 image difference from the distorted\nimage alone while the reference is not observed. This is enabled by two\ninsights: The first is to inject sufficiently many un-distorted natural image\npatches, which can be found in arbitrary amounts and are known to have no\nperceivable difference to themselves. This avoids false positives. The second\nis to balance the learning, where it is carefully made sure that all image\nerrors are equally likely, avoiding false negatives. Surprisingly, we observe,\nthat the resulting no-reference metric, subjectively, can even perform better\nthan the reference-based one, as it had to become robust against\nmis-alignments. We evaluate the effectiveness of our approach in an image-based\nrendering context, both quantitatively and qualitatively. Finally, we\ndemonstrate two applications which reduce light field capture time and provide\nguidance for interactive depth adjustment.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.02552v2"
    },
    {
        "title": "Techniques for modeling a high-quality B-spline curves by S-polygons in\n  a float format",
        "authors": [
            "Rushan Ziatdinov",
            "Valerijan Muftejev",
            "Rifkat Nabiyev",
            "Albert Mardanov",
            "Rustam Akhmetshin"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This article proposes a technique for the geometrically stable modeling of\nhigh-degree B-spline curves based on S-polygon in a float format, which will\nallow the accurate positioning of the end points of curves and the direction of\nthe tangent vectors. The method of shape approximation is described with the\npurpose of providing geometrical proximity between the original and\napproximating curve. The content of the notion of a harmonious, regular form of\nB-spline curve's S-polygon in a float format is revealed as a factor in\nachieving a high-quality of fit for the generated curve. The expediency of the\nshape modeling method based on S-polygon in a float format at the end portions\nof the curve for quality control of curve modeling and editing is\nsubstantiated. The results of a comparative test are presented, demonstrating\nthe superlative efficacy of using the Mineur-Farin configuration for\nconstructing constant and monotone curvature curves based on an S-polygon in a\nfloat format. The findings presented in this article confirm that it is\npreferable to employ the principle of \"constructing a control polygon of a\nharmonious form (or the Mineur-Farin configuration) of a parametric polynomial\"\nto a B-spline curve's S-polygon in a float format, and not to a B-polygon of\nthe Bezier curve. Recommendations are given for prospective studies in the\nfield of applying the technique of constructing a high-quality B-spline curves\nto the approximation of log-aesthetic curves, Ziatdinov's superspirals, etc.\nThe authors of the article developed a technique for constructing smooth\nconnections of B-spline curves with ensuring a high order of smoothness of the\ncomposite curve. The proposed techniques are implemented in the\nFairCurveModeler program as a plug-in to engineering CAD systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.04223v1"
    },
    {
        "title": "Antara: An Interactive 3D Volume Rendering and Visualization Framework",
        "authors": [
            "Pratik Kalshetti",
            "Parag Rahangdale",
            "Dinesh Jangra",
            "Manas Bundele",
            "Chiranjoy Chattopadhyay"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The goal of 3D visualization is to provide the user with an intuitive\ninterface which enables him to explore the 3D data in an interactive manner.\nThe aim of the exploration is to identify and analyze anomalies or to give\nproof of the non-anomaly of the visualized organic structures. For 3D Medical\nData, Magnetic Resonance Images (MRI) has been used. To create the 3D model, we\nused the Direct Volume Rendering technique. In the input 3D data, we have $x,\ny$ and $z$ coordinates and an intensity value for each voxel. The 3D data is\nused by Volume Ray Casting to compute 2D projections from 3D volumetric data\nsets. In ray casting, a ray of light is made to pass through the volume data.\nThe interaction of each voxel with this ray is used to assign RGB and alpha\nvalues for every voxel in the volume. As a result, we are able to generate the\n3D model of the region of interest using the 3D data. The 3D model is\ninteractive, thus enabling us to visualize the different layers of the 3D\nvolume by adjusting the transfer function.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.04233v1"
    },
    {
        "title": "Parallel and Scalable Heat Methods for Geodesic Distance Computation",
        "authors": [
            "Jiong Tao",
            "Juyong Zhang",
            "Bailin Deng",
            "Zheng Fang",
            "Yue Peng",
            "Ying He"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In this paper, we propose a parallel and scalable approach for geodesic\ndistance computation on triangle meshes. Our key observation is that the\nrecovery of geodesic distance with the heat method from [Crane et al. 2013] can\nbe reformulated as optimization of its gradients subject to integrability,\nwhich can be solved using an efficient first-order method that requires no\nlinear system solving and converges quickly. Afterward, the geodesic distance\nis efficiently recovered by parallel integration of the optimized gradients in\nbreadth-first order. Moreover, we employ a similar breadth-first strategy to\nderive a parallel Gauss-Seidel solver for the diffusion step in the heat\nmethod. To further lower the memory consumption from gradient optimization on\nfaces, we also propose a formulation that optimizes the projected gradients on\nedges, which reduces the memory footprint by about 50%. Our approach is\ntrivially parallelizable, with a low memory footprint that grows linearly with\nrespect to the model size. This makes it particularly suitable for handling\nlarge models. Experimental results show that it can efficiently compute\ngeodesic distance on meshes with more than 200 million vertices on a desktop PC\nwith 128GB RAM, outperforming the original heat method and other\nstate-of-the-art geodesic distance solvers.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.06060v3"
    },
    {
        "title": "Embedding Bilateral Filter in Least Squares for Efficient\n  Edge-preserving Image Smoothing",
        "authors": [
            "Wei Liu",
            "Pingping Zhang",
            "Xiaogang Chen",
            "Chunhua Shen",
            "Xiaolin Huang",
            "Jie Yang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Edge-preserving smoothing is a fundamental procedure for many computer vision\nand graphic applications. This can be achieved with either local methods or\nglobal methods. In most cases, global methods can yield superior performance\nover local ones. However, local methods usually run much faster than global\nones. In this paper, we propose a new global method that embeds the bilateral\nfilter in the least squares model for efficient edge-preserving smoothing. The\nproposed method can show comparable performance with the state-of-the-art\nglobal method. Meanwhile, since the proposed method can take advantages of the\nefficiency of the bilateral filter and least squares model, it runs much\nfaster. In addition, we show the flexibility of our method which can be easily\nextended by replacing the bilateral filter with its variants. They can be\nfurther modified to handle more applications. We validate the effectiveness and\nefficiency of the proposed method through comprehensive experiments in a range\nof applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07122v1"
    },
    {
        "title": "cellPACKexplorer: Interactive Model Building for Volumetric Data of\n  Complex Cells",
        "authors": [
            "M. Schwarzl",
            "L. Autin",
            "G. Johnson",
            "T. Torsney-Weir",
            "T. Möller"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Given an algorithm the quality of the output largely depends on a proper\nspecification of the input parameters. A lot of work has been done to analyze\ntasks related to using a fixed model [25] and finding a good set of inputs. In\nthis paper we present a different scenario, model building. In contrast to\nmodel usage the underlying algorithm, i.e. the underlying model, changes and\ntherefore the associated parameters also change. Developing a new algorithm\nrequires a particular set of parameters that, on the one hand, give access to\nan expected range of outputs and, on the other hand, are still interpretable.\nAs the model is developed and parameters are added, deleted, or changed\ndifferent features of the outputs are of interest. Therefore it is important to\nfind objective measures that quantify these features. In a model building\nprocess these features are prone to change and need to be adaptable as the\nmodel changes. We discuss these problems in the application of cellPACK, a tool\nthat generates virtual 3D cells. Our analysis is based on an output set\ngenerated by sampling the input parameter space. Hence we also present\ntechniques and metrics to analyze an ensemble of probabilistic volumes.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07273v1"
    },
    {
        "title": "Derivation of an Algorithm for Calculation of the Intersection Area of a\n  Circle with a Grid with Finite Fill Factor",
        "authors": [
            "Dmitrij Gendler",
            "Christian Eisele",
            "Dirk Seiffer",
            "Norbert Wendelstein"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The problem deals with an exact calculation of the intersection area of a\ncircle arbitrary placed on a grid of square shaped elements with gaps between\nthem (finite fill factor). Usually an approximation is used for the calculation\nof the intersection area of the circle and the squares of the grid. We analyze\nthe geometry of the problem and derive an algorithm for the exact computation\nof the intersection areas. The results of the analysis are summarized in the\ntally sheet. In a real world example this might be a CCD or CMOS chip, or the\ntile structure of a floor.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.10515v1"
    },
    {
        "title": "Computing Three-dimensional Constrained Delaunay Refinement Using the\n  GPU",
        "authors": [
            "Zhenghai Chen",
            "Tiow-Seng Tan"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose the first GPU algorithm for the 3D triangulation refinement\nproblem. For an input of a piecewise linear complex $\\mathcal{G}$ and a\nconstant $B$, it produces, by adding Steiner points, a constrained Delaunay\ntriangulation conforming to $\\mathcal{G}$ and containing tetrahedra mostly of\nradius-edge ratios smaller than $B$. Our implementation of the algorithm shows\nthat it can be an order of magnitude faster than the best CPU algorithm while\nusing a similar amount of Steiner points to produce triangulations of\ncomparable quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.03406v1"
    },
    {
        "title": "Robust Reference Frame Extraction from Unsteady 2D Vector Fields with\n  Convolutional Neural Networks",
        "authors": [
            "Byungsoo Kim",
            "Tobias Günther"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Robust feature extraction is an integral part of scientific visualization. In\nunsteady vector field analysis, researchers recently directed their attention\ntowards the computation of near-steady reference frames for vortex extraction,\nwhich is a numerically challenging endeavor. In this paper, we utilize a\nconvolutional neural network to combine two steps of the visualization pipeline\nin an end-to-end manner: the filtering and the feature extraction. We use\nneural networks for the extraction of a steady reference frame for a given\nunsteady 2D vector field. By conditioning the neural network to noisy inputs\nand resampling artifacts, we obtain numerically stabler results than existing\noptimization-based approaches. Supervised deep learning typically requires a\nlarge amount of training data. Thus, our second contribution is the creation of\na vector field benchmark data set, which is generally useful for any local deep\nlearning-based feature extraction. Based on Vatistas velocity profile, we\nformulate a parametric vector field mixture model that we parameterize based on\nnumerically-computed example vector fields in near-steady reference frames.\nGiven the parametric model, we can efficiently synthesize thousands of vector\nfields that serve as input to our deep learning architecture. The proposed\nnetwork is evaluated on an unseen numerical fluid flow simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.10255v1"
    },
    {
        "title": "Loopy Cuts: Surface-Field Aware Block Decomposition for Hex-Meshing",
        "authors": [
            "Marco Livesu",
            "Nico Pietroni",
            "Enrico Puppo",
            "Alla Sheffer",
            "Paolo Cignoni"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a new fully automatic block-decomposition hexahedral meshing\nalgorithm capable of producing high quality meshes that strictly preserve\nfeature curve networks on the input surface and align with an input surface\ncross-field. We produce all-hex meshes on the vast majority of inputs, and\nintroduce localized non-hex elements only when the surface feature network\nnecessitates those. The input to our framework is a closed surface with a\ncollection of geometric or user-demarcated feature curves and a feature-aligned\nsurface cross-field. Its output is a compact set of blocks whose edges\ninterpolate these features and are loosely aligned with this cross-field. We\nobtain this block decomposition by cutting the input model using a collection\nof simple cutting surfaces bounded by closed surface loops. The set of cutting\nloops spans the input feature curves, ensuring feature preservation, and is\nobtained using a field-space sampling process. The computed loops are uniformly\ndistributed across the surface, cross orthogonally, and are loosely aligned\nwith the cross-field directions, inducing the desired block decomposition. We\nvalidate our method by applying it to a large range of complex inputs and\ncomparing our results to those produced by state-of-the-art alternatives.\nContrary to prior approaches, our framework consistently produces high-quality\nfield aligned meshes while strictly preserving geometric or user-specified\nsurface features.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.10754v2"
    },
    {
        "title": "Implementing Noise with Hash functions for Graphics Processing Units",
        "authors": [
            "Matias Valdenegro-Toro",
            "Hector Pincheira"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a modification to Perlin noise which use computable hash functions\ninstead of textures as lookup tables. We implemented the FNV1, Jenkins and\nMurmur hashes on Shader Model 4.0 Graphics Processing Units for noise\ngeneration. Modified versions of the FNV1 and Jenkins hashes provide very close\nperformance compared to a texture based Perlin noise implementation. Our noise\nmodification enables noise function evaluation without any texture fetches,\ntrading computational power for memory bandwidth.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.12270v1"
    },
    {
        "title": "P-Cloth: Interactive Complex Cloth Simulation on Multi-GPU Systems using\n  Dynamic Matrix Assembly and Pipelined Implicit Integrators",
        "authors": [
            "Cheng Li",
            "Min Tang",
            "Ruofeng Tong",
            "Ming Cai",
            "Jieyi Zhao",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a novel parallel algorithm for cloth simulation that exploits\nmultiple GPUs for fast computation and the handling of very high resolution\nmeshes. To accelerate implicit integration, we describe new parallel algorithms\nfor sparse matrix-vector multiplication (SpMV) and for dynamic matrix assembly\non a multi-GPU workstation. Our algorithms use a novel work queue generation\nscheme for a fat-tree GPU interconnect topology. Furthermore, we present a\nnovel collision handling scheme that uses spatial hashing for discrete and\ncontinuous collision detection along with a non-linear impact zone solver. Our\nparallel schemes can distribute the computation and storage overhead among\nmultiple GPUs and enable us to perform almost interactive simulation on complex\ncloth meshes, which can hardly be handled on a single GPU due to memory\nlimitations. We have evaluated the performance with two multi-GPU workstations\n(with 4 and 8 GPUs, respectively) on cloth meshes with 0.5-1.65M triangles. Our\napproach can reliably handle the collisions and generate vivid wrinkles and\nfolds at 2-5 fps, which is significantly faster than prior cloth simulation\nsystems. We observe almost linear speedups with respect to the number of GPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.00409v2"
    },
    {
        "title": "Modeling of Personalized Anatomy using Plastic Strains",
        "authors": [
            "Bohan Wang",
            "George Matcuk",
            "Jernej Barbic"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We give a method for modeling solid objects undergoing large spatially\nvarying and/or anisotropic strains, and use it to reconstruct human anatomy\nfrom medical images. Our novel shape deformation method uses plastic strains\nand the Finite Element Method to successfully model shapes undergoing large\nand/or anisotropic strains, specified by sparse point constraints on the\nboundary of the object. We extensively compare our method to standard\nsecond-order shape deformation methods, variational methods and surface-based\nmethods and demonstrate that our method avoids the spikiness, wiggliness and\nother artefacts of previous methods. We demonstrate how to perform such shape\ndeformation both for attached and un-attached (\"free flying\") objects, using a\nnovel method to solve linear systems with singular matrices with a known\nnullspace. While our method is applicable to general large-strain shape\ndeformation modeling, we use it to create personalized 3D triangle and\nvolumetric meshes of human organs, based on MRI or CT scans. Given a medically\naccurate anatomy template of a generic individual, we optimize the geometry of\nthe organ to match the MRI or CT scan of a specific individual. Our examples\ninclude human hand muscles, a liver, a hip bone, and a gluteus medius muscle\n(\"hip abductor\").\n",
        "pdf_link": "http://arxiv.org/pdf/2008.00579v1"
    },
    {
        "title": "Mesh Processing Strategies and Fractals for Three Dimensional\n  Morphological Analysis of a Granitic Terrain using IRS LISS IV and Carto DEM",
        "authors": [
            "K. Seshadri",
            "M. Naresh Kumar"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Virtual Reality (VR) enabled applications are becoming very important to\nvisualize the terrain features in 3D. In general 3D datasets generated from\nhigh-resolution satellites and DEM occupy large volumes of data. However,\nlightweight datasets are required to create better user experiences on VR\nplatforms. So, the present study develops a methodology to generate datasets\ncompatible with VR using Indian Remote Sensing satellite (IRS) sensors. A\nLinear Imaging Self-Scanning System - IV (LISS IV) with 5.8 m spatial\nresolution and Carto DEM are used for generating the 3D view using the Arc\nenvironment and then converted into virtual reality modeling language (VRML)\nformat. In order to reduce the volume of the VRML dataset a quadratic edge\ncollapse decimation method is applied which reduces the number of faces in the\nmesh while preserving the boundary and/or normal. A granitic terrain in the\nsouth-west part of Hyderabad comprising of dyke intrusion is considered for the\ngeneration of 3D VR dataset, as it has high elevation differences thus\nrendering it most suitable for the present study. Further, the enhanced\ngeomorphological features such as hills and valleys, geological structures such\nas fractures, intrusive (dykes) are studied and found suitable for better\ninterpretation.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.01174v1"
    },
    {
        "title": "Segmentation-Driven Feature-Preserving Mesh Denoising",
        "authors": [
            "Weijia Wang",
            "Wei Pan",
            "Chaofan Dai",
            "Richard Dazeley",
            "Lei Wei",
            "Bernard Rolfe",
            "Xuequan Lu"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Feature-preserving mesh denoising has received noticeable attention in visual\nmedia, with the aim of recovering high-fidelity, clean mesh shapes from the\nones that are contaminated by noise. Existing denoising methods often design\nsmaller weights for anisotropic surfaces and larger weights for isotropic\nsurfaces in order to preserve sharp features, such as edges or corners, on the\nmesh shapes. However, they often disregard the fact that such small weights on\nanisotropic surfaces still pose negative impacts on the denoising outcomes and\ndetail preservation results on the shapes. In this paper, we propose a novel\nsegmentation-driven mesh denoising method which performs region-wise denoising,\nand thus avoids the disturbance of anisotropic neighbour faces for better\nfeature preservation results. Also, our backbone can be easily embedded into\ncommonly-used mesh denoising frameworks. Extensive experiments have\ndemonstrated that our method can enhance the denoising results on a wide range\nof synthetic and real mesh models, both quantitatively and visually.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.01358v3"
    },
    {
        "title": "Optimized Processing of Localized Collisions in Projective Dynamics",
        "authors": [
            "Qisi Wang",
            "Yutian Tao",
            "Eric Brandt",
            "Court Cutting",
            "Eftychios Sifakis"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a method for the efficient processing of contact and collision in\nvolumetric elastic models simulated using the Projective Dynamics paradigm. Our\napproach enables interactive simulation of tetrahedral meshes with more than\nhalf a million elements, provided that the model satisfies two fundamental\nproperties: the region of the model's surface that is susceptible to collision\nevents needs to be known in advance, and the simulation degrees of freedom\nassociated with that surface region should be limited to a small fraction (e.g.\n5\\%) of the total simulation nodes. Despite this conscious delineation of\nscope, our hypotheses hold true for common animation subjects, such as\nsimulated models of the human face and parts of the body. In such scenarios, a\npartial Cholesky factorization can abstract away the behavior of the\ncollision-safe subset of the face into the Schur Complement matrix with respect\nto the collision-prone region. We demonstrate how fast and accurate updates of\npenalty-based collision terms can be incorporated into this representation, and\nsolved with high efficiency on the GPU. We also demonstrate the opportunity to\niterate a partial update of the element rotations, akin to a selective\napplication of the local step, specifically on the smaller collision-prone\nregion without explicitly paying the cost associated with the rest of the\nsimulation mesh. We demonstrate efficient and robust interactive simulation in\ndetailed models from animation and medical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.01541v1"
    },
    {
        "title": "Deep Detail Enhancement for Any Garment",
        "authors": [
            "Meng Zhang",
            "Tuanfeng Wang",
            "Duygu Ceylan",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Creating fine garment details requires significant efforts and huge\ncomputational resources. In contrast, a coarse shape may be easy to acquire in\nmany scenarios (e.g., via low-resolution physically-based simulation, linear\nblend skinning driven by skeletal motion, portable scanners). In this paper, we\nshow how to enhance, in a data-driven manner, rich yet plausible details\nstarting from a coarse garment geometry. Once the parameterization of the\ngarment is given, we formulate the task as a style transfer problem over the\nspace of associated normal maps. In order to facilitate generalization across\ngarment types and character motions, we introduce a patch-based formulation,\nthat produces high-resolution details by matching a Gram matrix based style\nloss, to hallucinate geometric details (i.e., wrinkle density and shape). We\nextensively evaluate our method on a variety of production scenarios and show\nthat our method is simple, light-weight, efficient, and generalizes across\nunderlying garment types, sewing patterns, and body motion.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04367v1"
    },
    {
        "title": "Meshless Approximation and Helmholtz-Hodge Decomposition of Vector\n  Fields",
        "authors": [
            "Giuseppe Patanè"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The analysis of vector fields is crucial for the understanding of several\nphysical phenomena, such as natural events (e.g., analysis of waves), diffusive\nprocesses, electric and electromagnetic fields. While previous work has been\nfocused mainly on the analysis of 2D or 3D vector fields on volumes or\nsurfaces, we address the meshless analysis of a vector field defined on an\narbitrary domain, without assumptions on its dimension and discretisation. The\nmeshless approximation of the Helmholtz-Hodge decomposition of a vector field\nis achieved by expressing the potential of its components as a linear\ncombination of radial basis functions and by computing the corresponding\nconservative, irrotational, and harmonic components as solution to a\nleast-squares or to a differential problem. To this end, we identify the\nconditions on the kernel of the radial basis functions that guarantee the\nexistence of their derivatives. Finally, we demonstrate our approach on 2D and\n3D vector fields measured by sensors or generated through simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04411v1"
    },
    {
        "title": "Vid2Player: Controllable Video Sprites that Behave and Appear like\n  Professional Tennis Players",
        "authors": [
            "Haotian Zhang",
            "Cristobal Sciutto",
            "Maneesh Agrawala",
            "Kayvon Fatahalian"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a system that converts annotated broadcast video of tennis matches\ninto interactively controllable video sprites that behave and appear like\nprofessional tennis players. Our approach is based on controllable video\ntextures, and utilizes domain knowledge of the cyclic structure of tennis\nrallies to place clip transitions and accept control inputs at key\ndecision-making moments of point play. Most importantly, we use points from the\nvideo collection to model a player's court positioning and shot selection\ndecisions during points. We use these behavioral models to select video clips\nthat reflect actions the real-life player is likely to take in a given match\nplay situation, yielding sprites that behave realistically at the macro level\nof full points, not just individual tennis motions. Our system can generate\nnovel points between professional tennis players that resemble Wimbledon\nbroadcasts, enabling new experiences such as the creation of matchups between\nplayers that have not competed in real life, or interactive control of players\nin the Wimbledon final. According to expert tennis players, the rallies\ngenerated using our approach are significantly more realistic in terms of\nplayer behavior than video sprite methods that only consider the quality of\nmotion transitions during video synthesis.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04524v2"
    },
    {
        "title": "Interactive volume illumination of slice-based ray casting",
        "authors": [
            "Dening Luo"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Volume rendering always plays an important role in the field of medical\nimaging and industrial design. In recent years, the realistic and interactive\nvolume rendering of the global illumination can improve the perception of shape\nand depth of volumetric datasets. In this paper, a novel and flexible\nperformance method of slice-based ray casting is proposed to implement the\nvolume illumination effects, such as volume shadow and other scattering\neffects. This benefits from the slice-based illumination attenuation buffers of\nthe whole geometry slices at the viewpoint of the light source and the\nhigh-efficiency shadow or scattering coefficient calculation per sample in ray\ncasting. These tests show the method can obtain much better volume illumination\neffects and more scalable performance in contrast to the local volume\nillumination in ray casting volume rendering or other similar slice-based\nglobal volume illumination.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06134v1"
    },
    {
        "title": "Primary-Space Adaptive Control Variates using Piecewise-Polynomial\n  Approximations",
        "authors": [
            "Miguel Crespo",
            "Felix Bernal",
            "Adrian Jarabo",
            "Adolfo Muñoz"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present an unbiased numerical integration algorithm that handles both\nlow-frequency regions and high frequency details of multidimensional integrals.\nIt combines quadrature and Monte Carlo integration, by using a quadrature-base\napproximation as a control variate of the signal. We adaptively build the\ncontrol variate constructed as a piecewise polynomial, which can be\nanalytically integrated, and accurately reconstructs the low frequency regions\nof the integrand. We then recover the high-frequency details missed by the\ncontrol variate by using Monte Carlo integration of the residual. Our work\nleverages importance sampling techniques by working in primary space, allowing\nthe combination of multiple mappings; this enables multiple importance sampling\nin quadrature-based integration. Our algorithm is generic, and can be applied\nto any complex multidimensional integral. We demonstrate its effectiveness with\nfour applications with low dimensionality: transmittance estimation in\nheterogeneous participating media, low-order scattering in homogeneous media,\ndirect illumination computation, and rendering of distributed effects. Finally,\nwe show how our technique is extensible to integrands of higher dimensionality,\nby computing the control variate on Monte Carlo estimates of the\nhigh-dimensional signal, and accounting for such additional dimensionality on\nthe residual as well. In all cases, we show accurate results and faster\nconvergence compared to previous approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06722v1"
    },
    {
        "title": "Visual Analysis of Large Multivariate Scattered Data using Clustering\n  and Probabilistic Summaries",
        "authors": [
            "Tobias Rapp",
            "Christoph Peters",
            "Carsten Dachsbacher"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Rapidly growing data sizes of scientific simulations pose significant\nchallenges for interactive visualization and analysis techniques. In this work,\nwe propose a compact probabilistic representation to interactively visualize\nlarge scattered datasets. In contrast to previous approaches that represent\nblocks of volumetric data using probability distributions, we model clusters of\narbitrarily structured multivariate data. In detail, we discuss how to\nefficiently represent and store a high-dimensional distribution for each\ncluster. We observe that it suffices to consider low-dimensional marginal\ndistributions for two or three data dimensions at a time to employ common\nvisual analysis techniques. Based on this observation, we represent\nhigh-dimensional distributions by combinations of low-dimensional Gaussian\nmixture models. We discuss the application of common interactive visual\nanalysis techniques to this representation. In particular, we investigate\nseveral frequency-based views, such as density plots in 1D and 2D,\ndensity-based parallel coordinates, and a time histogram. We visualize the\nuncertainty introduced by the representation, discuss a level-of-detail\nmechanism, and explicitly visualize outliers. Furthermore, we propose a spatial\nvisualization by splatting anisotropic 3D Gaussians for which we derive a\nclosed-form solution. Lastly, we describe the application of brushing and\nlinking to this clustered representation. Our evaluation on several large,\nreal-world datasets demonstrates the scaling of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.09544v2"
    },
    {
        "title": "Geometry-guided Dense Perspective Network for Speech-Driven Facial\n  Animation",
        "authors": [
            "Jingying Liu",
            "Binyuan Hui",
            "Kun Li",
            "Yunke Liu",
            "Yu-Kun Lai",
            "Yuxiang Zhang",
            "Yebin Liu",
            "Jingyu Yang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Realistic speech-driven 3D facial animation is a challenging problem due to\nthe complex relationship between speech and face. In this paper, we propose a\ndeep architecture, called Geometry-guided Dense Perspective Network (GDPnet),\nto achieve speaker-independent realistic 3D facial animation. The encoder is\ndesigned with dense connections to strengthen feature propagation and encourage\nthe re-use of audio features, and the decoder is integrated with an attention\nmechanism to adaptively recalibrate point-wise feature responses by explicitly\nmodeling interdependencies between different neuron units. We also introduce a\nnon-linear face reconstruction representation as a guidance of latent space to\nobtain more accurate deformation, which helps solve the geometry-related\ndeformation and is good for generalization across subjects. Huber and HSIC\n(Hilbert-Schmidt Independence Criterion) constraints are adopted to promote the\nrobustness of our model and to better exploit the non-linear and high-order\ncorrelations. Experimental results on the public dataset and real scanned\ndataset validate the superiority of our proposed GDPnet compared with\nstate-of-the-art model.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.10004v1"
    },
    {
        "title": "Test Scene Design for Physically Based Rendering",
        "authors": [
            "Elias Brugger",
            "Christian Freude",
            "Michael Wimmer"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Physically based rendering is a discipline in computer graphics which aims at\nreproducing certain light and material appearances that occur in the real\nworld. Complex scenes can be difficult to compute for rendering algorithms.\nThis paper introduces a new comprehensive test database of scenes that treat\ndifferent light setups in conjunction with diverse materials and discusses its\ndesign principles. A lot of research is focused on the development of new\nalgorithms that can deal with difficult light conditions and materials\nefficiently. This database delivers a comprehensive foundation for evaluating\nexisting and newly developed rendering techniques. A final evaluation compares\ndifferent results of different rendering algorithms for all scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.11657v2"
    },
    {
        "title": "ClipFlip : Multi-view Clipart Design",
        "authors": [
            "I-Chao Shen",
            "Kuan-Hung Liu",
            "Li-Wen Su",
            "Yu-Ting Wu",
            "Bing-Yu Chen"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present an assistive system for clipart design by providing visual\nscaffolds from the unseen viewpoints. Inspired by the artists' creation\nprocess, our system constructs the visual scaffold by first synthesizing the\nreference 3D shape of the input clipart and rendering it from the desired\nviewpoint. The critical challenge of constructing this visual scaffold is to\ngenerate a reference 3Dshape that matches the user's expectation in terms of\nobject sizing and positioning while preserving the geometric style of the input\nclipart. To address this challenge, we propose a user-assisted curve extrusion\nmethod to obtain the reference 3D shape.We render the synthesized reference 3D\nshape with consistent style into the visual scaffold. By following the\ngenerated visual scaffold, the users can efficiently design clipart with their\ndesired viewpoints. The user study conducted by an intuitive user interface and\nour generated visual scaffold suggests that the users are able to design\nclipart from different viewpoints while preserving the original geometric style\nwithout losing its original shape.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.12933v3"
    },
    {
        "title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in\n  Virtual Screening",
        "authors": [
            "María Virginia Sabando",
            "Pavol Ulbrich",
            "Matías Selzer",
            "Jan Byška",
            "Jan Mičan",
            "Ignacio Ponzoni",
            "Axel J. Soto",
            "María Luján Ganuza",
            "Barbora Kozlíková"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In the modern drug discovery process, medicinal chemists deal with the\ncomplexity of analysis of large ensembles of candidate molecules. Computational\ntools, such as dimensionality reduction (DR) and classification, are commonly\nused to efficiently process the multidimensional space of features. These\nunderlying calculations often hinder interpretability of results and prevent\nexperts from assessing the impact of individual molecular features on the\nresulting representations. To provide a solution for scrutinizing such complex\ndata, we introduce ChemVA, an interactive application for the visual\nexploration of large molecular ensembles and their features. Our tool consists\nof multiple coordinated views: Hexagonal view, Detail view, 3D view, Table\nview, and a newly proposed Difference view designed for the comparison of DR\nprojections. These views display DR projections combined with biological\nactivity, selected molecular features, and confidence scores for each of these\nprojections. This conjunction of views allows the user to drill down through\nthe dataset and to efficiently select candidate compounds. Our approach was\nevaluated on two case studies of finding structurally similar ligands with\nsimilar binding affinity to a target protein, as well as on an external\nqualitative evaluation. The results suggest that our system allows effective\nvisual inspection and comparison of different high-dimensional molecular\nrepresentations. Furthermore, ChemVA assists in the identification of candidate\ncompounds while providing information on the certainty behind different\nmolecular representations.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.13150v1"
    },
    {
        "title": "Direct Volume Rendering with Nonparametric Models of Uncertainty",
        "authors": [
            "Tushar Athawale",
            "Bo Ma",
            "Elham Sakhaee",
            "Chris R. Johnson",
            "Alireza Entezari"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a nonparametric statistical framework for the quantification,\nanalysis, and propagation of data uncertainty in direct volume rendering (DVR).\nThe state-of-the-art statistical DVR framework allows for preserving the\ntransfer function (TF) of the ground truth function when visualizing uncertain\ndata; however, the existing framework is restricted to parametric models of\nuncertainty. In this paper, we address the limitations of the existing DVR\nframework by extending the DVR framework for nonparametric distributions. We\nexploit the quantile interpolation technique to derive probability\ndistributions representing uncertainty in viewing-ray sample intensities in\nclosed form, which allows for accurate and efficient computation. We evaluate\nour proposed nonparametric statistical models through qualitative and\nquantitative comparisons with the mean-field and parametric statistical models,\nsuch as uniform and Gaussian, as well as Gaussian mixtures. In addition, we\npresent an extension of the state-of-the-art rendering parametric framework to\n2D TFs for improved DVR classifications. We show the applicability of our\nuncertainty quantification framework to ensemble, downsampled, and bivariate\nversions of scalar field datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.13576v1"
    },
    {
        "title": "History-free Collision Response for Deformable Surfaces",
        "authors": [
            "Juntao Ye"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Continuous collision detection (CCD) and response methods are widely adopted\nin dynamics simulation of deformable models. They are history-based, as their\nsuccess is strictly based on an assumption of a collision-free state at the\nstart of each time interval. On the other hand, in many applications surfaces\nhave normals defined to designate their orientation (i.e. front- and\nback-face), yet CCD methods are totally blind to such orientation\nidentification (thus are orientation-free). We notice that if such information\nis utilized, many penetrations can be untangled. In this paper we present a\nhistory-free method for separation of two penetrating meshes, where at least\none of them has clarified surface orientation. This method first computes all\nedge-face (E-F) intersections with discrete collision detection (DCD), and then\nbuilds a number of penetration stencils. On response, the stencil vertices are\nrelocated into a penetration-free state, via a global displacement minimizer.\nOur method is very effective for handling penetration between two meshes, being\nit an initial configuration or in the middle of physics simulation. The major\nlimitation is that it is not applicable to self-collision within one mesh at\nthe time being.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.2081v1"
    },
    {
        "title": "Tracing Analytic Ray Curves for Light and Sound Propagation in\n  Non-linear Media",
        "authors": [
            "Qi Mo",
            "Hengchin Yeh",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  The physical world consists of spatially varying media, such as the\natmosphere and the ocean, in which light and sound propagates along non-linear\ntrajectories. This presents a challenge to existing ray-tracing based methods,\nwhich are widely adopted to simulate propagation due to their efficiency and\nflexibility, but assume linear rays. We present a novel algorithm that traces\nanalytic ray curves computed from local media gradients, and utilizes the\nclosed-form solutions of both the intersections of the ray curves with planar\nsurfaces, and the travel distance. By constructing an adaptive unstructured\nmesh, our algorithm is able to model general media profiles that vary in three\ndimensions with complex boundaries consisting of terrains and other scene\nobjects such as buildings. We trace the analytic ray curves using the adaptive\nunstructured mesh, which considerably improves the efficiency over prior\nmethods. We highlight the algorithm's application on simulation of sound and\nvisual propagation in outdoor scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.2235v3"
    },
    {
        "title": "Examplar-Based Face Colorization Using Image Morphing",
        "authors": [
            "Johannes Persch",
            "Fabien Pierre",
            "Gabriele Steidl"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Colorization of gray-scale images relies on prior color information.\nExamplar-based methods use a color image as source of such information. Then\nthe colors of the source image are transferred to the gray-scale image. In the\nliterature, this transfer is mainly guided by texture descriptors. Face images\nusually contain few texture so that the common approaches frequently fail. In\nthis paper we propose a new method based on image morphing. This technique is\nable to compute a correspondence map between images with similar shapes. It is\nbased on the geometric structure of the images rather than textures which is\nmore reliable for faces. Our numerical experiments show that our morphing based\napproach clearly outperforms state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.00648v1"
    },
    {
        "title": "Direct interactive visualization of locally refined spline volumes for\n  scalar and vector fields",
        "authors": [
            "Franz G. Fuchs",
            "Oliver J. D. Barrowclough",
            "Jon M. Hjelmervik",
            "Heidi E. I. Dahl"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a novel approach enabling interactive visualization of volumetric\nLocally Refined B-splines (LR-splines). To this end we propose a highly\nefficient algorithm for direct visualization of scalar and vector fields given\nby an LR-spline. In both cases, our main contribution to achieve interactive\nframe rates is an acceleration structure for fast element look-up and a change\nof basis for efficient evaluation. To further improve the efficiency, we\npresent a heuristic for adaptive sampling distance for the numerical\nintegration. A comparison with existing adaptive approaches is performed. The\nalgorithms are designed to fully utilize modern graphics processing unit (GPU)\ncapabilities. Important applications where LR-spline volumes emerge are given\nfor instance by approximation of large-scale simulation and sensor data, and\nIsogeometric Analysis (IGA). We showcase interactive rendering achieved by our\napproach on different representative use cases, stemming from simulations of\nwind flow around a telescope, Magnetic Resonance (MR) imaging of a human brain,\nand simulations of a fluidized bed used for mixing and coating particles in\nindustrial processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.01170v2"
    },
    {
        "title": "Nonlinear dance motion analysis and motion editing using Hilbert-Huang\n  transform",
        "authors": [
            "Ran Dong",
            "Dongsheng Cai",
            "Nobuyoshi Asai"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Human motions (especially dance motions) are very noisy, and it is hard to\nanalyze and edit the motions. To resolve this problem, we propose a new method\nto decompose and modify the motions using the Hilbert-Huang transform (HHT).\nFirst, HHT decomposes a chromatic signal into \"monochromatic\" signals that are\nthe so-called Intrinsic Mode Functions (IMFs) using an Empirical Mode\nDecomposition (EMD) [6]. After applying the Hilbert Transform to each IMF, the\ninstantaneous frequencies of the \"monochromatic\" signals can be obtained. The\nHHT has the advantage to analyze non-stationary and nonlinear signals such as\nhuman-joint-motions over FFT or Wavelet transform.\n  In the present paper, we propose a new framework to analyze and extract some\nnew features from a famous Japanese threesome pop singer group called\n\"Perfume\", and compare it with Waltz and Salsa dance. Using the EMD, their\ndance motions can be decomposed into motion (choreographic) primitives or IMFs.\nTherefore we can scale, combine, subtract, exchange, and modify those IMFs, and\ncan blend them into new dance motions self-consistently. Our analysis and\nframework can lead to a motion editing and blending method to create a new\ndance motion from different dance motions.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.01732v1"
    },
    {
        "title": "Localized Manifold Harmonics for Spectral Shape Analysis",
        "authors": [
            "Simone Melzi",
            "Emanuele Rodolà",
            "Umberto Castellani",
            "Michael M. Bronstein"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  The use of Laplacian eigenfunctions is ubiquitous in a wide range of computer\ngraphics and geometry processing applications. In particular, Laplacian\neigenbases allow generalizing the classical Fourier analysis to manifolds. A\nkey drawback of such bases is their inherently global nature, as the Laplacian\neigenfunctions carry geometric and topological structure of the entire\nmanifold. In this paper, we introduce a new framework for local spectral shape\nanalysis. We show how to efficiently construct localized orthogonal bases by\nsolving an optimization problem that in turn can be posed as the\neigendecomposition of a new operator obtained by a modification of the standard\nLaplacian. We study the theoretical and computational aspects of the proposed\nframework and showcase our new construction on the classical problems of shape\napproximation and correspondence. We obtain significant improvement compared to\nclassical Laplacian eigenbases as well as other alternatives for constructing\nlocalized bases.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02596v2"
    },
    {
        "title": "Natural Boundary Conditions for Smoothing in Geometry Processing",
        "authors": [
            "Oded Stein",
            "Eitan Grinspun",
            "Max Wardetzky",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In geometry processing, smoothness energies are commonly used to model\nscattered data interpolation, dense data denoising, and regularization during\nshape optimization. The squared Laplacian energy is a popular choice of energy\nand has a corresponding standard implementation: squaring the discrete\nLaplacian matrix. For compact domains, when values along the boundary are not\nknown in advance, this construction bakes in low-order boundary conditions.\nThis causes the geometric shape of the boundary to strongly bias the solution.\nFor many applications, this is undesirable. Instead, we propose using the\nsquared Frobenious norm of the Hessian as a smoothness energy. Unlike the\nsquared Laplacian energy, this energy's natural boundary conditions (those that\nbest minimize the energy) correspond to meaningful high-order boundary\nconditions. These boundary conditions model free boundaries where the shape of\nthe boundary should not bias the solution locally. Our analysis begins in the\nsmooth setting and concludes with discretizations using finite-differences on\n2D grids or mixed finite elements for triangle meshes. We demonstrate the core\nbehavior of the squared Hessian as a smoothness energy for various tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.04348v1"
    },
    {
        "title": "A Streamline Selection Technique Overlaying with Isosurfaces",
        "authors": [
            "Shiho Furuya",
            "Takayuki Itoh"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Integration of scalar and vector visualization has been an interesting topic.\nThis paper presents a technique to appropriately select and display multiple\nstreamlines while overlaying with isosurfaces, aiming an integrated scalar and\nvector field visualization. The technique visualizes a scalar field by multiple\nsemitransparent isosurfaces, and a vector field by multiple streamlines, while\nthe technique adequately selects the streamlines considering reduction of\ncluttering among the isosurfaces and streamlines. The technique first selects\nand renders isosurfaces, and then generates large number of streamlines from\nrandomly selected seed points. The technique evaluates each of the streamlines\naccording to their shapes on a 2D display space, distances to critical points\nof the given vector fields, and occlusion by isosurfaces. It then selects the\nspecified number of highly evaluated streamlines. As a result, we can visualize\nboth scalar and vector fields as a set of view-independently selected\nisosurfaces and view-dependently selected streamlines.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.04805v1"
    },
    {
        "title": "GPU accelerated computation of Polarized Subsurface BRDF for Flat\n  Particulate Layers",
        "authors": [
            "Charly Collin",
            "Sumanta Pattanaik"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  BRDF of most real world materials has two components, the surface BRDF due to\nthe light reflecting at the surface of the material and the subsurface BRDF due\nto the light entering and going through many scattering events inside the\nmaterial. Each of these events modifies light's path, power, polarization\nstate. Computing polarized subsurface BRDF of a material requires simulating\nthe light transport inside the material. The transport of polarized light is\nmodeled by the Vector Radiative Transfer Equation (VRTE), an\nintegro-differential equation. Computing solution to that equation is\nexpensive. The Discrete Ordinate Method (DOM) is a common approach to solving\nthe VRTE. Such solvers are very time consuming for complex uses such as BRDF\ncomputation, where one must solve VRTE for surface radiance distribution due to\nlight incident from every direction of the hemisphere above the surface. In\nthis paper, we present a GPU based DOM solution of the VRTE to expedite the\nsubsurface BRDF computation. As in other DOM based solutions, our solution is\nbased on Fourier expansions of the phase function and the radiance function.\nThis allows us to independently solve the VRTE for each order of expansion. We\ntake advantage of those repetitions and of the repetitions in each of the\nsub-steps of the solution process. Our solver is implemented to run mainly on\ngraphics hardware using the OpenCL library and runs up to seven times faster\nthan its CPU equivalent, allowing the computation of subsurface BRDF in a\nmatter of minutes. We compute and present the subsurface BRDF lobes due to\npowders and paints of a few materials. We also show the rendering of objects\nwith the computed BRDF. The solver is available for public use through the\nauthors' web site.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05882v1"
    },
    {
        "title": "Steklov Spectral Geometry for Extrinsic Shape Analysis",
        "authors": [
            "Yu Wang",
            "Mirela Ben-Chen",
            "Iosif Polterovich",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We propose using the Dirichlet-to-Neumann operator as an extrinsic\nalternative to the Laplacian for spectral geometry processing and shape\nanalysis. Intrinsic approaches, usually based on the Laplace-Beltrami operator,\ncannot capture the spatial embedding of a shape up to rigid motion, and many\nprevious extrinsic methods lack theoretical justification. Instead, we consider\nthe Steklov eigenvalue problem, computing the spectrum of the\nDirichlet-to-Neumann operator of a surface bounding a volume. A remarkable\nproperty of this operator is that it completely encodes volumetric geometry. We\nuse the boundary element method (BEM) to discretize the operator, accelerated\nby hierarchical numerical schemes and preconditioning; this pipeline allows us\nto solve eigenvalue and linear problems on large-scale meshes despite the\ndensity of the Dirichlet-to-Neumann discretization. We further demonstrate that\nour operators naturally fit into existing frameworks for geometry processing,\nmaking a shift from intrinsic to extrinsic geometry as simple as substituting\nthe Laplace-Beltrami operator with the Dirichlet-to-Neumann operator.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.07070v2"
    },
    {
        "title": "Pigmento: Pigment-Based Image Analysis and Editing",
        "authors": [
            "Jianchao Tan",
            "Stephen DiVerdi",
            "Jingwan Lu",
            "Yotam Gingold"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  The colorful appearance of a physical painting is determined by the\ndistribution of paint pigments across the canvas, which we model as a per-pixel\nmixture of a small number of pigments with multispectral absorption and\nscattering coefficients. We present an algorithm to efficiently recover this\nstructure from an RGB image, yielding a plausible set of pigments and a low RGB\nreconstruction error. We show that under certain circumstances we are able to\nrecover pigments that are close to ground truth, while in all cases our results\nare always plausible. Using our decomposition, we repose standard digital image\nediting operations as operations in pigment space rather than RGB, with\ninterestingly novel results. We demonstrate tonal adjustments, selection\nmasking, cut-copy-paste, recoloring, palette summarization, and edge\nenhancement.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.08323v3"
    },
    {
        "title": "Discrete Geodesic Nets for Modeling Developable Surfaces",
        "authors": [
            "Michael Rabinovich",
            "Tim Hoffmann",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a discrete theory for modeling developable surfaces as\nquadrilateral meshes satisfying simple angle constraints. The basis of our\nmodel is a lesser known characterization of developable surfaces as manifolds\nthat can be parameterized through orthogonal geodesics. Our model is simple,\nlocal, and, unlike previous works, it does not directly encode the surface\nrulings. This allows us to model continuous deformations of discrete\ndevelopable surfaces independently of their decomposition into torsal and\nplanar patches or the surface topology. We prove and experimentally demonstrate\nstrong ties to smooth developable surfaces, including a theorem stating that\nevery sampling of the smooth counterpart satisfies our constraints up to second\norder. We further present an extension of our model that enables a local\ndefinition of discrete isometry. We demonstrate the effectiveness of our\ndiscrete model in a developable surface editing system, as well as computation\nof an isometric interpolation between isometric discrete developable shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.08360v1"
    },
    {
        "title": "Research on Shape Mapping of 3D Mesh Models based on Hidden Markov\n  Random Field and EM Algorithm",
        "authors": [
            "Yong Wang",
            "Huai-yu Wu"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  How to establish the matching (or corresponding) between two different 3D\nshapes is a classical problem. This paper focused on the research on shape\nmapping of 3D mesh models, and proposed a shape mapping algorithm based on\nHidden Markov Random Field and EM algorithm, as introducing a hidden state\nrandom variable associated with the adjacent blocks of shape matching when\nestablishing HMRF. This algorithm provides a new theory and method to ensure\nthe consistency of the edge data of adjacent blocks, and the experimental\nresults show that the algorithm in this paper has a great improvement on the\nshape mapping of 3D mesh models.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09123v1"
    },
    {
        "title": "Continuous Global Optimization in Surface Reconstruction from an\n  Oriented Point Cloud",
        "authors": [
            "Rongjiang Pan",
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We introduce a continuous global optimization method to the field of surface\nreconstruction from discrete noisy cloud of points with weak information on\norientation. The proposed method uses an energy functional combining flux-based\ndata-fit measures and a regularization term. A continuous convex relaxation\nscheme assures the global minima of the geometric surface functional. The\nreconstructed surface is implicitly represented by the binary segmentation of\nvertices of a 3D uniform grid and a triangulated surface can be obtained by\nextracting an appropriate isosurface. Unlike the discrete graph-cut solution,\nthe continuous global optimization entails advantages like memory requirements,\nreduction of metrication errors for geometric quantities, allowing globally\noptimal surface reconstruction at higher grid resolutions. We demonstrate the\nperformance of the proposed method on several oriented point clouds captured by\nlaser scanners. Experimental results confirm that our approach is robust to\nnoise, large holes and non-uniform sampling density under the condition of very\ncoarse orientation information.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09366v1"
    },
    {
        "title": "Generation of concept-representative symbols",
        "authors": [
            "João Miguel Cunha",
            "Pedro Martins",
            "Amílcar Cardoso",
            "Penousal Machado"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  The visual representation of concepts or ideas through the use of simple\nshapes has always been explored in the history of Humanity, and it is believed\nto be the origin of writing. We focus on computational generation of visual\nsymbols to represent concepts. We aim to develop a system that uses background\nknowledge about the world to find connections among concepts, with the goal of\ngenerating symbols for a given concept. We are also interested in exploring the\nsystem as an approach to visual dissociation and visual conceptual blending.\nThis has a great potential in the area of Graphic Design as a tool to both\nstimulate creativity and aid in brainstorming in projects such as logo,\npictogram or signage design.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09432v1"
    },
    {
        "title": "Kernel Projection of Latent Structures Regression for Facial Animation\n  Retargeting",
        "authors": [
            "Christos Ouzounis",
            "Alex Kilias",
            "Christos Mousas"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Inspired by kernel methods that have been used extensively in achieving\nefficient facial animation retargeting, this paper presents a solution to\nretargeting facial animation in virtual character's face model based on the\nkernel projection of latent structure (KPLS) regression between semantically\nsimilar facial expressions. Specifically, a given number of corresponding\nsemantically similar facial expressions are projected into the latent space. By\nusing the Nonlinear Iterative Partial Least Square method, decomposition of the\nlatent variables is achieved. Finally, the KPLS is achieved by solving a\nkernalized version of the eigenvalue problem. By evaluating our methodology\nwith other kernel-based solutions, the efficiency of the presented methodology\nin transferring facial animation to face models with different morphological\nvariations is demonstrated.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09629v1"
    },
    {
        "title": "Fast 3D Indoor Scene Synthesis with Discrete and Exact Layout Pattern\n  Extraction",
        "authors": [
            "Song-Hai Zhang",
            "Shao-Kui Zhang",
            "Wei-Yu Xie",
            "Cheng-Yang Luo",
            "Hong-Bo Fu"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a fast framework for indoor scene synthesis, given a room geometry\nand a list of objects with learnt priors. Unlike existing data-driven\nsolutions, which often extract priors by co-occurrence analysis and statistical\nmodel fitting, our method measures the strengths of spatial relations by tests\nfor complete spatial randomness (CSR), and extracts complex priors based on\nsamples with the ability to accurately represent discrete layout patterns. With\nthe extracted priors, our method achieves both acceleration and plausibility by\npartitioning input objects into disjoint groups, followed by layout\noptimization based on the Hausdorff metric. Extensive experiments show that our\nframework is capable of measuring more reasonable relations among objects and\nsimultaneously generating varied arrangements in seconds.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.00328v2"
    },
    {
        "title": "SplitStreams: A Visual Metaphor for Evolving Hierarchies",
        "authors": [
            "Fabian Bolte",
            "Mahsan Nourani",
            "Eric D. Ragan",
            "Stefan Bruckner"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The visualization of hierarchically structured data over time is an ongoing\nchallenge and several approaches exist trying to solve it. Techniques such as\nanimated or juxtaposed tree visualizations are not capable of providing a good\noverview of the time series and lack expressiveness in conveying changes over\ntime. Nested streamgraphs provide a better understanding of the data evolution,\nbut lack the clear outline of hierarchical structures at a given timestep.\nFurthermore, these approaches are often limited to static hierarchies or\nexclude complex hierarchical changes in the data, limiting their use cases. We\npropose a novel visual metaphor capable of providing a static overview of all\nhierarchical changes over time, as well as clearly outlining the hierarchical\nstructure at each individual time step. Our method allows for smooth\ntransitions between tree maps and nested streamgraphs, enabling the exploration\nof the trade-off between dynamic behavior and hierarchical structure. As our\ntechnique handles topological changes of all types, it is suitable for a wide\nrange of applications. We demonstrate the utility of our method on several use\ncases, evaluate it with a user study, and provide its full source code.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.03891v1"
    },
    {
        "title": "Course notes Geometric Algebra for Computer Graphics, SIGGRAPH 2019",
        "authors": [
            "Charles G. Gunn"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  What is the best representation for doing euclidean geometry on computers?\nThese notes from a SIGGRAPH 2019 short course entitled \"Geometric algebra for\ncomputer graphics\" introduce projective geometric algebra (PGA) as a modern\nframework for this task. PGA features: uniform representation of points, lines,\nand planes; robust, parallel-safe join and meet operations; compact,\npolymorphic syntax for euclidean formulas and constructions; a single intuitive\nsandwich form for isometries; native support for automatic differentiation; and\ntight integration of kinematics and rigid body mechanics. PGA includes vector,\nquaternion, dual quaternion, and exterior algebras as sub-algebras, simplifying\nthe learning curve and transition path for experienced practitioners. On the\npractical side, it can be efficiently implemented, while its rich syntax\nenhances programming productivity. The basic ideas are introduced in the 2D\ncontext and developed selectively for 3D. Advantages to traditional approaches\nare collected in a table at the end. The article aims to be a self-contained\nintroduction for practitioners of euclidean geometry and includes numerous\nexamples, formulas, figures, and tables.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.04509v2"
    },
    {
        "title": "Pointfilter: Point Cloud Filtering via Encoder-Decoder Modeling",
        "authors": [
            "Dongbo Zhang",
            "Xuequan Lu",
            "Hong Qin",
            "Ying He"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Point cloud filtering is a fundamental problem in geometry modeling and\nprocessing. Despite of significant advancement in recent years, the existing\nmethods still suffer from two issues: 1) they are either designed without\npreserving sharp features or less robust in feature preservation; and 2) they\nusually have many parameters and require tedious parameter tuning. In this\npaper, we propose a novel deep learning approach that automatically and\nrobustly filters point clouds by removing noise and preserving their sharp\nfeatures. Our point-wise learning architecture consists of an encoder and a\ndecoder. The encoder directly takes points (a point and its neighbors) as\ninput, and learns a latent representation vector which goes through the decoder\nto relate the ground-truth position with a displacement vector. The trained\nneural network can automatically generate a set of clean points from a noisy\ninput. Extensive experiments show that our approach outperforms the\nstate-of-the-art deep learning techniques in terms of both visual quality and\nquantitative error metrics. The source code and dataset can be found at\nhttps://github.com/dongbo-BUAA-VR/Pointfilter.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.05968v2"
    },
    {
        "title": "Quantitative Evaluation of Time-Dependent Multidimensional Projection\n  Techniques",
        "authors": [
            "E. F. Vernier",
            "R. Garcia",
            "I. P. da Silva",
            "J. L. D. Comba",
            "A. C. Telea"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Dimensionality reduction methods are an essential tool for multidimensional\ndata analysis, and many interesting processes can be studied as time-dependent\nmultivariate datasets. There are, however, few studies and proposals that\nleverage on the concise power of expression of projections in the context of\ndynamic/temporal data. In this paper, we aim at providing an approach to assess\nprojection techniques for dynamic data and understand the relationship between\nvisual quality and stability. Our approach relies on an experimental setup that\nconsists of existing techniques designed for time-dependent data and new\nvariations of static methods. To support the evaluation of these techniques, we\nprovide a collection of datasets that has a wide variety of traits that encode\ndynamic patterns, as well as a set of spatial and temporal stability metrics\nthat assess the quality of the layouts. We present an evaluation of 11 methods,\n10 datasets, and 12 quality metrics, and elect the best-suited methods for\nprojecting time-dependent multivariate data, exploring the design choices and\ncharacteristics of each method. All our results are documented and made\navailable in a public repository to allow reproducibility of results.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.07481v1"
    },
    {
        "title": "A Survey on Deep Geometry Learning: From a Representation Perspective",
        "authors": [
            "Yun-Peng Xiao",
            "Yu-Kun Lai",
            "Fang-Lue Zhang",
            "Chunpeng Li",
            "Lin Gao"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Researchers have now achieved great success on dealing with 2D images using\ndeep learning. In recent years, 3D computer vision and Geometry Deep Learning\ngain more and more attention. Many advanced techniques for 3D shapes have been\nproposed for different applications. Unlike 2D images, which can be uniformly\nrepresented by regular grids of pixels, 3D shapes have various representations,\nsuch as depth and multi-view images, voxel-based representation, point-based\nrepresentation, mesh-based representation, implicit surface representation,\netc. However, the performance for different applications largely depends on the\nrepresentation used, and there is no unique representation that works well for\nall applications. Therefore, in this survey, we review recent development in\ndeep learning for 3D geometry from a representation perspective, summarizing\nthe advantages and disadvantages of different representations in different\napplications. We also present existing datasets in these representations and\nfurther discuss future research directions.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.07995v2"
    },
    {
        "title": "Toward the Graphics Turing Scale on a Blue Gene Supercomputer",
        "authors": [
            "Michael McGuigan"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  We investigate raytracing performance that can be achieved on a class of Blue\nGene supercomputers. We measure a 822 times speedup over a Pentium IV on a 6144\nprocessor Blue Gene/L. We measure the computational performance as a function\nof number of processors and problem size to determine the scaling performance\nof the raytracing calculation on the Blue Gene. We find nontrivial scaling\nbehavior at large number of processors. We discuss applications of this\ntechnology to scientific visualization with advanced lighting and high\nresolution. We utilize three racks of a Blue Gene/L in our calculations which\nis less than three percent of the the capacity of the worlds largest Blue Gene\ncomputer.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.1500v1"
    },
    {
        "title": "MathPSfrag 2: Convenient LaTeX Labels in Mathematica",
        "authors": [
            "Johannes Große"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  This article introduces the next version of MathPSfrag. MathPSfrag is a\nMathematica package that during export automatically replaces all expressions\nin a plot by corresponding LaTeX commands. The new version can also produce\nLaTeX independent images; e.g., PDF files for inclusion in pdfLaTeX. Moreover\nfrom these files a preview is generated and shown within Mathematica.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.2175v1"
    },
    {
        "title": "Computing a Compact Spline Representation of the Medial Axis Transform\n  of a 2D Shape",
        "authors": [
            "Yanshu Zhu",
            "Feng Sun",
            "Yi-King Choi",
            "Bert Jüttler",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We present a full pipeline for computing the medial axis transform of an\narbitrary 2D shape. The instability of the medial axis transform is overcome by\na pruning algorithm guided by a user-defined Hausdorff distance threshold. The\nstable medial axis transform is then approximated by spline curves in 3D to\nproduce a smooth and compact representation. These spline curves are computed\nby minimizing the approximation error between the input shape and the shape\nrepresented by the medial axis transform. Our results on various 2D shapes\nsuggest that our method is practical and effective, and yields faithful and\ncompact representations of medial axis transforms of 2D shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.0118v3"
    },
    {
        "title": "4-Dimensional Geometry Lens: A Novel Volumetric Magnification Approach",
        "authors": [
            "Bo Li",
            "Xin Zhao",
            "Hong Qin"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We present a novel methodology that utilizes 4-Dimensional (4D) space\ndeformation to simulate a magnification lens on versatile volume datasets and\ntextured solid models. Compared with other magnification methods (e.g.,\ngeometric optics, mesh editing), 4D differential geometry theory and its\npractices are much more flexible and powerful for preserving shape features\n(i.e., minimizing angle distortion), and easier to adapt to versatile solid\nmodels. The primary advantage of 4D space lies at the following fact: we can\nnow easily magnify the volume of regions of interest (ROIs) from the additional\ndimension, while keeping the rest region unchanged. To achieve this primary\ngoal, we first embed a 3D volumetric input into 4D space and magnify ROIs in\nthe 4th dimension. Then we flatten the 4D shape back into 3D space to\naccommodate other typical applications in the real 3D world. In order to\nenforce distortion minimization, in both steps we devise the high dimensional\ngeometry techniques based on rigorous 4D geometry theory for 3D/4D mapping back\nand forth to amend the distortion. Our system can preserve not only focus\nregion, but also context region and global shape. We demonstrate the\neffectiveness, robustness, and efficacy of our framework with a variety of\nmodels ranging from tetrahedral meshes to volume datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.0147v2"
    },
    {
        "title": "Progressive Blue Surfels",
        "authors": [
            "Claudius Jähn"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  In this paper we describe a new technique to generate and use surfels for\nrendering of highly complex, polygonal 3D scenes in real time. The basic idea\nis to approximate complex parts of the scene by rendering a set of points\n(surfels). The points are computed in a preprocessing step and offer two\nimportant properties: They are placed only on the visible surface of the\nscene's geometry and they are distributed and sorted in such a way, that every\nprefix of points is a good visual representation of the approximated part of\nthe scene. An early evaluation of the method shows that it is capable of\nrendering scenes consisting of several billions of triangles with high image\nquality.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.0247v1"
    },
    {
        "title": "New Julia and Mandelbrot Sets for Jungck Ishikawa Iterates",
        "authors": [
            "Suman Joshi",
            "Dr. Yashwant Singh Chauhan",
            "Dr. Ashish Negi"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  The generation of fractals and study of the dynamics of polynomials is one of\nthe emerging and interesting field of research nowadays. We introduce in this\npaper the dynamics of polynomials z^ n - z + c = 0 for n>=2 and applied Jungck\nIshikawa Iteration to generate new Relative Superior Mandelbrot sets and\nRelative Superior Julia sets. In order to solve this function by Jungck type\niterative schemes, we write it in the form of Sz = Tz, where the function T, S\nare defined as Tz = z^ n + c and Sz = z. Only mathematical explanations are\nderived by applying Jungck Ishikawa Iteration for polynomials in the literature\nbut in this paper we have generated Relative Mandelbrot sets and Relative Julia\nsets.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.0981v1"
    },
    {
        "title": "Expression driven Trignometric based Procedural Animation of Quadrupeds",
        "authors": [
            "Zeeshan Bhatti",
            "Asadullah Shah",
            "Mustafa Karabasi",
            "Waheed Mahesar"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  This research paper addresses the problem of generating involuntary and\nprecise animation of quadrupeds with automatic rigging system of various\ncharacter types. The technique proposed through this research is based on a two\ntier animation control curve with base simulation being driven through dynamic\nmathematical model using procedural algorithm and the top layer with a custom\nuser controlled animation provided with intuitive Graphical User Interface\n(GUI). The character rig is based on forward and inverse kinematics driven\nthrough trigonometric based motion equations. The User is provided with various\nmanipulators and attributes to control and handle the locomotion gaits of the\ncharacters and choose between various types of simulated motions from walking,\nrunning, trotting, ambling and galloping with complete custom controls to\neasily extend the base simulation as per requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.2053v1"
    },
    {
        "title": "Interactive Isogeometric Volume Visualization with Pixel-Accurate\n  Geometry",
        "authors": [
            "Franz G. Fuchs",
            "Jon M. Hjelmervik"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  A recent development, called isogeometric analysis, provides a unified\napproach for design, analysis and optimization of functional products in\nindustry. Traditional volume rendering methods for inspecting the results from\nthe numerical simulations cannot be applied directly to isogeometric models. We\npresent a novel approach for interactive visualization of isogeometric analysis\nresults, ensuring correct, i.e., pixel-accurate geometry of the volume\nincluding its bounding surfaces. The entire OpenGL pipeline is used in a\nmulti-stage algorithm leveraging techniques from surface rendering,\norder-independent transparency, as well as theory and numerical methods for\nordinary differential equations. We showcase the efficiency of our approach on\ndifferent models relevant to industry, ranging from quality inspection of the\nparametrization of the geometry, to stress analysis in linear elasticity, to\nvisualization of computational fluid dynamics results.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.3363v3"
    },
    {
        "title": "Piko: A Design Framework for Programmable Graphics Pipelines",
        "authors": [
            "Anjul Patney",
            "Stanley Tzeng",
            "Kerry A. Seitz Jr.",
            "John D. Owens"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  We present Piko, a framework for designing, optimizing, and retargeting\nimplementations of graphics pipelines on multiple architectures. Piko\nprogrammers express a graphics pipeline by organizing the computation within\neach stage into spatial bins and specifying a scheduling preference for these\nbins. Our compiler, Pikoc, compiles this input into an optimized implementation\ntargeted to a massively-parallel GPU or a multicore CPU.\n  Piko manages work granularity in a programmable and flexible manner, allowing\nprogrammers to build load-balanced parallel pipeline implementations, to\nexploit spatial and producer-consumer locality in a pipeline implementation,\nand to explore tradeoffs between these considerations. We demonstrate that Piko\ncan implement a wide range of pipelines, including rasterization, Reyes, ray\ntracing, rasterization/ray tracing hybrid, and deferred rendering. Piko allows\nus to implement efficient graphics pipelines with relative ease and to quickly\nexplore design alternatives by modifying the spatial binning configurations and\nscheduling preferences for individual stages, all while delivering real-time\nperformance that is within a factor six of state-of-the-art rendering systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.6293v2"
    },
    {
        "title": "Variance Analysis for Monte Carlo Integration: A\n  Representation-Theoretic Perspective",
        "authors": [
            "Michael Kazhdan",
            "Gurprit Singh",
            "Adrien Pilleboue",
            "David Coeurjolly",
            "Victor Ostromoukhov"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  In this report, we revisit the work of Pilleboue et al. [2015], providing a\nrepresentation-theoretic derivation of the closed-form expression for the\nexpected value and variance in homogeneous Monte Carlo integration. We show\nthat the results obtained for the variance estimation of Monte Carlo\nintegration on the torus, the sphere, and Euclidean space can be formulated as\nspecific instances of a more general theory. We review the related\nrepresentation theory and show how it can be used to derive a closed-form\nsolution.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.00021v1"
    },
    {
        "title": "Gradient-Domain Fusion for Color Correction in Large EM Image Stacks",
        "authors": [
            "Michael Kazhdan",
            "Kunal Lillaney",
            "William Roncal",
            "Davi Bock",
            "Joshua Vogelstein",
            "Randal Burns"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We propose a new gradient-domain technique for processing registered EM image\nstacks to remove inter-image discontinuities while preserving intra-image\ndetail. To this end, we process the image stack by first performing anisotropic\nsmoothing along the slice axis and then solving a Poisson equation within each\nslice to re-introduce the detail. The final image stack is continuous across\nthe slice axis and maintains sharp details within each slice. Adapting existing\nout-of-core techniques for solving the linear system, we describe a parallel\nalgorithm with time complexity that is linear in the size of the data and space\ncomplexity that is sub-linear, allowing us to process datasets as large as five\nteravoxels with a 600 MB memory footprint.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.02079v1"
    },
    {
        "title": "Pushing the Limits of 3D Color Printing: Error Diffusion with\n  Translucent Materials",
        "authors": [
            "Alan Brunton",
            "Can Ates Arikan",
            "Philipp Urban"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Accurate color reproduction is important in many applications of 3D printing,\nfrom design prototypes to 3D color copies or portraits. Although full color is\navailable via other technologies, multi-jet printers have greater potential for\ngraphical 3D printing, in terms of reproducing complex appearance properties.\nHowever, to date these printers cannot produce full color, and doing so poses\nsubstantial technical challenges, from the shear amount of data to the\ntranslucency of the available color materials. In this paper, we propose an\nerror diffusion halftoning approach to achieve full color with multi-jet\nprinters, which operates on multiple isosurfaces or layers within the object.\nWe propose a novel traversal algorithm for voxel surfaces, which allows the\ntransfer of existing error diffusion algorithms from 2D printing. The resulting\nprints faithfully reproduce colors, color gradients and fine-scale details.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.02400v2"
    },
    {
        "title": "Reviewing Data Visualization: an Analytical Taxonomical Study",
        "authors": [
            "Jose F. Rodrigues Jr.",
            "Agma J. M. Traina",
            "Maria Cristina F. de Oliveira",
            "Caetano Traina Jr"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  This paper presents an analytical taxonomy that can suitably describe, rather\nthan simply classify, techniques for data presentation. Unlike previous works,\nwe do not consider particular aspects of visualization techniques, but their\nmechanisms and foundational vision perception. Instead of just adjusting\nvisualization research to a classification system, our aim is to better\nunderstand its process. For doing so, we depart from elementary concepts to\nreach a model that can describe how visualization techniques work and how they\nconvey meaning.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.02976v1"
    },
    {
        "title": "A Clustering Based Approach for Realistic and Efficient Data-Driven\n  Crowd Simulation",
        "authors": [
            "Mingbi Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  In this paper, we present a data-driven approach to generate realistic\nsteering behaviors for virtual crowds in crowd simulation. We take advantage of\nboth rule-based models and data-driven models by applying the interaction\npatterns discovered from crowd videos. Unlike existing example-based models in\nwhich current states are matched to states extracted from crowd videos\ndirectly, our approach adopts a hierarchical mechanism to generate the steering\nbehaviors of agents. First, each agent is classified into one of the\ninteraction patterns that are automatically discovered from crowd video before\nsimulation. Then the most matched action is selected from the associated\ninteraction pattern to generate the steering behaviors of the agent. By doing\nso, agents can avoid performing a simple state matching as in the traditional\nexample-based approaches, and can perform a wider variety of steering behaviors\nas well as mimic the cognitive process of pedestrians. Simulation results on\nscenarios with different crowd densities and main motion directions demonstrate\nthat our approach performs better than two state-of-the-art simulation models,\nin terms of prediction accuracy. Besides, our approach is efficient enough to\nrun at interactive rates in real time simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.04480v2"
    },
    {
        "title": "A Novel Semantics and Feature Preserving Perspective for Content Aware\n  Image Retargeting",
        "authors": [
            "Sukrit Shankar",
            "Pier Luigi Dragotti"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  There is an increasing requirement for efficient image retargeting techniques\nto adapt the content to various forms of digital media. With rapid growth of\nmobile communications and dynamic web page layouts, one often needs to resize\nthe media content to adapt to the desired display sizes. For various layouts of\nweb pages and typically small sizes of handheld portable devices, the\nimportance in the original image content gets obfuscated after resizing it with\nthe approach of uniform scaling. Thus, there occurs a need for resizing the\nimages in a content aware manner which can automatically discard irrelevant\ninformation from the image and present the salient features with more\nmagnitude. There have been proposed some image retargeting techniques keeping\nin mind the content awareness of the input image. However, these techniques\nfail to prove globally effective for various kinds of images and desired sizes.\nThe major problem is the inefficiency of these algorithms to process these\nimages with minimal visual distortion while also retaining the meaning conveyed\nfrom the image. In this dissertation, we present a novel perspective for\ncontent aware image retargeting, which is well implementable in real time. We\nintroduce a novel method of analysing semantic information within the input\nimage while also maintaining the important and visually significant features.\nWe present the various nuances of our algorithm mathematically and logically,\nand show that the results prove better than the state-of-the-art techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.04806v3"
    },
    {
        "title": "The 12 prophets dataset",
        "authors": [
            "J. Rodrigues",
            "M. Gazziro",
            "N. Goncalves",
            "O. Neto",
            "Y. Fernandes",
            "A. Gimenes",
            "C. Alegre",
            "R. Assis"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  The \"Ajeijadinho 3D\" project is an initiative supported by the University of\nS\\~ao Paulo (Museum of Science and Dean of Culture and Extension), which\ninvolves the 3D digitization of art works of Brazilian sculptor Antonio\nFrancisco Lisboa, better known as Aleijadinho. The project made use of advanced\nacquisition and processing of 3D meshes for preservation and dissemination of\nthe cultural heritage. The dissemination occurs through a Web portal, so that\nthe population has the opportunity to meet the art works in detail using 3D\nvisualization and interaction. The portal address is\nhttp://www.aleijadinho3d.icmc.usp.br. The 3D acquisitions were conducted over a\nweek at the end of July 2013 in the cities of Ouro Preto, MG, Brazil and\nCongonhas do Campo, MG, Brazil. The scanning was done with a special equipment\nsupplied by company Leica Geosystems, which allowed the work to take place at\ndistances between 10 and 30 meters, defining a non-invasive procedure,\nsimplified logistics, and without the need for preparation or isolation of the\nsites. In Ouro Preto, we digitized the churches of Francisco of Assis, Our Lady\nof Carmo, and Our Lady of Mercy; in Congonhas do Campo we scanned the entire\nSanctuary of Bom Jesus de Matosinhos and his 12 prophets. Once scanned, the art\nworks went through a long process of preparation, which required careful\nhandling of meshes done by experts from the University of S\\~ao Paulo in\npartnership with company Imprimate.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.06098v1"
    },
    {
        "title": "3D Geometric Analysis of Tubular Objects based on Surface Normal\n  Accumulation",
        "authors": [
            "Bertrand Kerautret",
            "Adrien Krähenbühl",
            "Isabelle Debled-Rennesson",
            "Jacques-Olivier Lachaud"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  This paper proposes a simple and efficient method for the reconstruction and\nextraction of geometric parameters from 3D tubular objects. Our method\nconstructs an image that accumulates surface normal information, then peaks\nwithin this image are located by tracking. Finally, the positions of these are\noptimized to lie precisely on the tubular shape centerline. This method is very\nversatile, and is able to process various input data types like full or partial\nmesh acquired from 3D laser scans, 3D height map or discrete volumetric images.\nThe proposed algorithm is simple to implement, contains few parameters and can\nbe computed in linear time with respect to the number of surface faces. Since\nthe extracted tube centerline is accurate, we are able to decompose the tube\ninto rectilinear parts and torus-like parts. This is done with a new linear\ntime 3D torus detection algorithm, which follows the same principle of a\nprevious work on 2D arc circle recognition. Detailed experiments show the\nversatility, accuracy and robustness of our new method.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.06636v2"
    },
    {
        "title": "Modeling and Correspondence of Topologically Complex 3D Shapes",
        "authors": [
            "Ibraheem Alhashim"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  3D shape creation and modeling remains a challenging task especially for\nnovice users. Many methods in the field of computer graphics have been proposed\nto automate the often repetitive and precise operations needed during the\nmodeling of detailed shapes. This report surveys different approaches of shape\nmodeling and correspondence especially for shapes exhibiting topological\ncomplexity. We focus on methods designed to help generate or process shapes\nwith large number of interconnected components often found in man-made shapes.\nWe first discuss a variety of modeling techniques, that leverage existing\nshapes, in easy to use creative modeling systems. We then discuss possible\ncorrespondence strategies for topologically different shapes as it is a\nrequirement for such systems. Finally, we look at different shape\nrepresentations and tools that facilitate the modification of shape topology\nand we focus on those particularly useful in free-form 3D modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.06855v1"
    },
    {
        "title": "A Survey on Distributed Visualization Techniques over Clusters of\n  Personal Computers",
        "authors": [
            "Jose Rodrigues",
            "Andre Balan",
            "Luciana Zaina",
            "Agma Traina"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  In the last years, Distributed Visualization over Personal Computer (PC)\nclusters has become important for research and industrial communities. They\nhave made large-scale visualizations practical and more accessible. In this\nwork we survey Distributed Visualization techniques aiming at compiling last\ndecade's literature on the use of PC clusters as suitable alternatives to\nhigh-end workstations. We review the topic by defining basic concepts,\nenumerating system requirements and implementation challenges, and presenting\nup-to-date methodologies. Our work fulfills the needs of newcomers and seasoned\nprofessionals as an introductory compilation at the same time that it can help\nexperienced personnel by organizing ideas.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.06968v1"
    },
    {
        "title": "Ebb: A DSL for Physical Simulation on CPUs and GPUs",
        "authors": [
            "Gilbert Louis Bernstein",
            "Chinmayee Shah",
            "Crystal Lemire",
            "Zachary DeVito",
            "Matthew Fisher",
            "Philip Levis",
            "Pat Hanrahan"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Designing programming environments for physical simulation is challenging\nbecause simulations rely on diverse algorithms and geometric domains. These\nchallenges are compounded when we try to run efficiently on heterogeneous\nparallel architectures. We present Ebb, a domain-specific language (DSL) for\nsimulation, that runs efficiently on both CPUs and GPUs. Unlike previous DSLs,\nEbb uses a three-layer architecture to separate (1) simulation code, (2)\ndefinition of data structures for geometric domains, and (3) runtimes\nsupporting parallel architectures. Different geometric domains are implemented\nas libraries that use a common, unified, relational data model. By structuring\nthe simulation framework in this way, programmers implementing simulations can\nfocus on the physics and algorithms for each simulation without worrying about\ntheir implementation on parallel computers. Because the geometric domain\nlibraries are all implemented using a common runtime based on relations, new\ngeometric domains can be added as needed, without specifying the details of\nmemory management, mapping to different parallel architectures, or having to\nexpand the runtime's interface.\n  We evaluate Ebb by comparing it to several widely used simulations,\ndemonstrating comparable performance to hand-written GPU code where available,\nand surpassing existing CPU performance optimizations by up to 9$\\times$ when\nno GPU code exists.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.07577v3"
    },
    {
        "title": "Combining Visual Analytics and Content Based Data Retrieval Technology\n  for Efficient Data Analysis",
        "authors": [
            "Jose Rodrigues",
            "Luciana Romani",
            "Agma Traina",
            "Caetano Traina"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  One of the most useful techniques to help visual data analysis systems is\ninteractive filtering (brushing). However, visualization techniques often\nsuffer from overlap of graphical items and multiple attributes complexity,\nmaking visual selection inefficient. In these situations, the benefits of data\nvisualization are not fully observable because the graphical items do not pop\nup as comprehensive patterns. In this work we propose the use of content-based\ndata retrieval technology combined with visual analytics. The idea is to use\nthe similarity query functionalities provided by metric space systems in order\nto select regions of the data domain according to user-guidance and interests.\nAfter that, the data found in such regions feed multiple visualization\nworkspaces so that the user can inspect the correspondent datasets. Our\nexperiments showed that the methodology can break the visual analysis process\ninto smaller problems (views) and that the views hold the expectations of the\nanalyst according to his/her similarity query selection, improving data\nperception and analytical possibilities. Our contribution introduces a\nprinciple that can be used in all sorts of visualization techniques and\nsystems, this principle can be extended with different kinds of integration\nvisualization-metric-space, and with different metrics, expanding the\npossibilities of visual data analysis in aspects such as semantics and\nscalability.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.07915v1"
    },
    {
        "title": "On the Approximation Theory of Linear Variational Subspace Design",
        "authors": [
            "Jianbo Ye",
            "Zhixin Yan"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Solving large-scale optimization on-the-fly is often a difficult task for\nreal-time computer graphics applications. To tackle this challenge, model\nreduction is a well-adopted technique. Despite its usefulness, model reduction\noften requires a handcrafted subspace that spans a domain that hypothetically\nembodies desirable solutions. For many applications, obtaining such subspaces\ncase-by-case either is impossible or requires extensive human labors, hence\ndoes not readily have a scalable solution for growing number of tasks. We\npropose linear variational subspace design for large-scale constrained\nquadratic programming, which can be computed automatically without any human\ninterventions. We provide meaningful approximation error bound that\nsubstantiates the quality of calculated subspace, and demonstrate its empirical\nsuccess in interactive deformable modeling for triangular and tetrahedral\nmeshes.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.08459v1"
    },
    {
        "title": "Improving Style Similarity Metrics of 3D Shapes",
        "authors": [
            "Kapil Dev",
            "Manfred Lau"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  The idea of style similarity metrics has been recently developed for various\nmedia types such as 2D clip art and 3D shapes. We explore this style metric\nproblem and improve existing style similarity metrics of 3D shapes in four\nnovel ways. First, we consider the color and texture of 3D shapes which are\nimportant properties that have not been previously considered. Second, we\nexplore the effect of clustering a dataset of 3D models by comparing between\nstyle metrics for a single object type and style metrics that combine clusters\nof object types. Third, we explore the idea of user-guided learning for this\nproblem. Fourth, we introduce an iterative approach that can learn a metric\nfrom a general set of 3D models. We demonstrate these contributions with\nvarious classes of 3D shapes and with applications such as style-based\nsimilarity search and scene composition.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.08826v1"
    },
    {
        "title": "LayerBuilder: Layer Decomposition for Interactive Image and Video Color\n  Editing",
        "authors": [
            "Sharon Lin",
            "Matthew Fisher",
            "Angela Dai",
            "Pat Hanrahan"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Exploring and editing colors in images is a common task in graphic design and\nphotography. However, allowing for interactive recoloring while preserving\nsmooth color blends in the image remains a challenging problem. We present\nLayerBuilder, an algorithm that decomposes an image or video into a linear\ncombination of colored layers to facilitate color-editing applications. These\nlayers provide an interactive and intuitive means for manipulating individual\ncolors. Our approach reduces color layer extraction to a fast iterative linear\nsystem. Layer Builder uses locally linear embedding, which represents pixels as\nlinear combinations of their neighbors, to reduce the number of variables in\nthe linear solve and extract layers that can better preserve color blending\neffects. We demonstrate our algorithm on recoloring a variety of images and\nvideos, and show its overall effectiveness in recoloring quality and time\ncomplexity compared to previous approaches. We also show how this\nrepresentation can benefit other applications, such as automatic recoloring\nsuggestion, texture synthesis, and color-based filtering.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.03754v2"
    },
    {
        "title": "Poisson Vector Graphics (PVG) and Its Closed-Form Solver",
        "authors": [
            "Fei Hou",
            "Qian Sun",
            "Zheng Fang",
            "Yong-Jin Liu",
            "Shi-Min Hu",
            "Hong Qin",
            "Aimin Hao",
            "Ying He"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This paper presents Poisson vector graphics, an extension of the popular\nfirst-order diffusion curves, for generating smooth-shaded images. Armed with\ntwo new types of primitives, namely Poisson curves and Poisson regions, PVG can\neasily produce photorealistic effects such as specular highlights, core\nshadows, translucency and halos. Within the PVG framework, users specify color\nas the Dirichlet boundary condition of diffusion curves and control tone by\noffsetting the Laplacian, where both controls are simply done by mouse click\nand slider dragging. The separation of color and tone not only follows the\nbasic drawing principle that is widely adopted by professional artists, but\nalso brings three unique features to PVG, i.e., local hue change, ease of\nextrema control, and permit of intersection among geometric primitives, making\nPVG an ideal authoring tool.\n  To render PVG, we develop an efficient method to solve 2D Poisson's equations\nwith piecewise constant Laplacians. In contrast to the conventional finite\nelement method that computes numerical solutions only, our method expresses the\nsolution using harmonic B-spline, whose basis functions can be constructed\nlocally and the control coefficients are obtained by solving a small sparse\nlinear system. Our closed-form solver is numerically stable and it supports\nrandom access evaluation, zooming-in of arbitrary resolution and anti-aliasing.\nAlthough the harmonic B-spline based solutions are approximate, computational\nresults show that the relative mean error is less than 0.3%, which cannot be\ndistinguished by naked eyes.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.04303v1"
    },
    {
        "title": "User-guided free-form asset modelling",
        "authors": [
            "Daniel Beale"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In this paper a new system for piecewise primitive surface recovery on point\nclouds is presented, which allows a novice user to sketch areas of interest in\norder to guide the fitting process. The algorithm is demonstrated against a\nbenchmark technique for autonomous surface fitting, and, contrasted against\nexisting literature in user guided surface recovery, with empirical evidence.\nIt is concluded that the system is an improvement to the current documented\nliterature for its visual quality when modelling objects which are composed of\npiecewise primitive shapes, and, in its ability to fill large holes on occluded\nsurfaces using free-form input.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.05754v1"
    },
    {
        "title": "Plausible Shading Decomposition For Layered Photo Retouching",
        "authors": [
            "Carlo Innamorati",
            "Tobias Ritschel",
            "Tim Weyrich",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Photographers routinely compose multiple manipulated photos of the same scene\n(layers) into a single image, which is better than any individual photo could\nbe alone. Similarly, 3D artists set up rendering systems to produce layered\nimages to contain only individual aspects of the light transport, which are\ncomposed into the final result in post-production. Regrettably, both approaches\neither take considerable time to capture, or remain limited to synthetic\nscenes. In this paper, we suggest a system to allow decomposing a single image\ninto a plausible shading decomposition (PSD) that approximates effects such as\nshadow, diffuse illumination, albedo, and specular shading. This decomposition\ncan then be manipulated in any off-the-shelf image manipulation software and\nrecomposited back. We perform such a decomposition by learning a convolutional\nneural network trained using synthetic data. We demonstrate the effectiveness\nof our decomposition on synthetic (i.e., rendered) and real data (i.e.,\nphotographs), and use them for common photo manipulation, which are nearly\nimpossible to perform otherwise from single images.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06507v2"
    },
    {
        "title": "By chance is not enough: Preserving relative density through non uniform\n  sampling",
        "authors": [
            "Enrico Bertini",
            "Giuseppe Santucci"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Dealing with visualizations containing large data set is a challenging issue\nand, in the field of Information Visualization, almost every visual technique\nreveals its drawback when visualizing large number of items. To deal with this\nproblem we introduce a formal environment, modeling in a virtual space the\nimage features we are interested in (e.g, absolute and relative density,\nclusters, etc.) and we define some metrics able to characterize the image\ndecay. Such metrics drive our automatic techniques (i.e., not uniform sampling)\nrescuing the image features and making them visible to the user. In this paper\nwe focus on 2D scatter-plots, devising a novel non uniform data sampling\nstrategy able to preserve in an effective way relative densities.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.07110v1"
    },
    {
        "title": "Ray tracing method for stereo image synthesis using CUDA",
        "authors": [
            "Anas M. Al-Oraiqat",
            "S. A. Zori"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This paper presents a realization of the approach to spatial 3D stereo of\nvisualization of 3D images with use parallel Graphics processing unit (GPU).\nThe experiments of realization of synthesis of images of a 3D stage by a method\nof trace of beams on GPU with Compute Unified Device Architecture (CUDA) have\nshown that 60 % of the time is spent for the decision of a computing problem\napproximately, the major part of time (40 %) is spent for transfer of data\nbetween the central processing unit and GPU for computations and the\norganization process of visualization. The study of the influence of increase\nin the size of the GPU network at the speed of computations showed importance\nof the correct task of structure of formation of the parallel computer network\nand general mechanism of parallelization. Keywords: Volumetric 3D\nvisualization, stereo 3D visualization, ray tracing, parallel computing on GPU,\nCUDA\n",
        "pdf_link": "http://arxiv.org/pdf/1702.01530v2"
    },
    {
        "title": "Conceptual and algorithmic development of Pseudo 3D Graphics and Video\n  Content Visualization",
        "authors": [
            "Aladdein M. Amro",
            "S. A. Zori",
            "Anas M. Al-Oraiqat"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  The article presents a general concept of the organization of pseudo three\ndimension visualization of graphics and video content for three dimension\nvisualization systems. The steps of algorithms for solving the problem of\nsynthesis of three dimension stereo images based on two dimension images are\nintroduced. The features of synthesis organization of standard format of three\ndimension stereo frame are presented. Moreover, the performed experimental\nsimulation for generating complete stereoframes and the results of its time\ncomplexity are shown. Keywords:Three dimension visualization, pseudo three\ndimension stereo, a stereo pair, three dimension stereo format, algorithm,\nmodeling, time complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.01537v1"
    },
    {
        "title": "Generalized 3D Voxel Image Synthesis Architecture for Volumetric Spatial\n  Visualization",
        "authors": [
            "Anas M. Al-Oraiqat",
            "E. A. Bashkov",
            "S. A. Zori",
            "Aladdein M. Amro"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  A general concept of 3D volumetric visualization systems is described based\non 3D discrete voxel scenes (worlds) representation. Definitions of 3D discrete\nvoxel scene (world) basic elements and main steps of the image synthesis\nalgorithm are formulated. An algorithm for solving the problem of the voxelized\nworld 3D image synthesis, intended for the systems of volumetric spatial\nvisualization, is proposed. A computer-based architecture for 3D volumetric\nvisualization of 3D discrete voxel world is presented. On the basis of the\nproposed overall concept of discrete voxel representation, the proposed\narchitecture successfully adapts the ray tracing technique for the synthesis of\n3D volumetric images. Since it is algorithmically simple and effectively\nsupports parallelism, it can efficiently be implemented.\n  Key words:Volumetric spatial visualization, 3D volumetric imagesynthesis,\ndiscrete voxel world, ray tracing.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.01540v1"
    },
    {
        "title": "Towards Developing an Easy-To-Use Scripting Environment for Animating\n  Virtual Characters",
        "authors": [
            "Christos Mousas"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This paper presents the three scripting commands and main functionalities of\na novel character animation environment called CHASE. CHASE was developed for\nenabling inexperienced programmers, animators, artists, and students to animate\nin meaningful ways virtual reality characters. This is achieved by scripting\nsimple commands within CHASE. The commands identified, which are associated\nwith simple parameters, are responsible for generating a number of predefined\nmotions and actions of a character. Hence, the virtual character is able to\nanimate within a virtual environment and to interact with tasks located within\nit. An additional functionality of CHASE is supplied. It provides the ability\nto generate multiple tasks of a character, such as providing the user the\nability to generate scenario-related animated sequences. However, since\nmultiple characters may require simultaneous animation, the ability to script\nactions of different characters at the same time is also provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.03246v1"
    },
    {
        "title": "Visualization and Analysis of Large-Scale, Tree-Based, Adaptive Mesh\n  Refinement Simulations with Arbitrary Rectilinear Geometry",
        "authors": [
            "Guénolé Harel",
            "Jacques-Bernard Lekien",
            "Philippe P. Pébaÿ"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present here the first systematic treatment of the problems posed by the\nvisualization and analysis of large-scale, parallel adaptive mesh refinement\n(AMR) simulations on an Eulerian grid. When compared to those obtained by\nconstructing an intermediate unstructured mesh with fully described\nconnectivity, our primary results indicate a gain of at least 80\\% in terms of\nmemory footprint, with a better rendering while retaining similar execution\nspeed. In this article, we describe the key concepts that allow us to obtain\nthese results, together with the methodology that facilitates the design,\nimplementation, and optimization of algorithms operating directly on such\nrefined meshes. This native support for AMR meshes has been contributed to the\nopen source Visualization Toolkit (VTK). This work pertains to a broader\nlong-term vision, with the dual goal to both improve interactivity when\nexploring such data sets in 2 and 3 dimensions, and optimize resource\nutilization.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.04852v1"
    },
    {
        "title": "Redefining A in RGBA: Towards a Standard for Graphical 3D Printing",
        "authors": [
            "Philipp Urban",
            "Tejas Madan Tanksale",
            "Alan Brunton",
            "Bui Minh Vu",
            "Shigeki Nakauchi"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Advances in multimaterial 3D printing have the potential to reproduce various\nvisual appearance attributes of an object in addition to its shape. Since many\nexisting 3D file formats encode color and translucency by RGBA textures mapped\nto 3D shapes, RGBA information is particularly important for practical\napplications. In contrast to color (encoded by RGB), which is specified by the\nobject's reflectance, selected viewing conditions and a standard observer,\ntranslucency (encoded by A) is neither linked to any measurable physical nor\nperceptual quantity. Thus, reproducing translucency encoded by A is open for\ninterpretation.\n  In this paper, we propose a rigorous definition for A suitable for use in\ngraphical 3D printing, which is independent of the 3D printing hardware and\nsoftware, and which links both optical material properties and perceptual\nuniformity for human observers. By deriving our definition from the absorption\nand scattering coefficients of virtual homogeneous reference materials with an\nisotropic phase function, we achieve two important properties. First, a simple\nadjustment of A is possible, which preserves the translucency appearance if an\nobject is re-scaled for printing. Second, determining the value of A for a real\n(potentially non-homogeneous) material, can be achieved by minimizing a\ndistance function between light transport measurements of this material and\nsimulated measurements of the reference materials. Such measurements can be\nconducted by commercial spectrophotometers used in graphic arts.\n  Finally, we conduct visual experiments employing the method of constant\nstimuli, and derive from them an embedding of A into a nearly perceptually\nuniform scale of translucency for the reference materials.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.00546v2"
    },
    {
        "title": "Single-image Tomography: 3D Volumes from 2D Cranial X-Rays",
        "authors": [
            "Philipp Henzler",
            "Volker Rasche",
            "Timo Ropinski",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  As many different 3D volumes could produce the same 2D x-ray image, inverting\nthis process is challenging. We show that recent deep learning-based\nconvolutional neural networks can solve this task. As the main challenge in\nlearning is the sheer amount of data created when extending the 2D image into a\n3D volume, we suggest firstly to learn a coarse, fixed-resolution volume which\nis then fused in a second step with the input x-ray into a high-resolution\nvolume. To train and validate our approach we introduce a new dataset that\ncomprises of close to half a million computer-simulated 2D x-ray images of 3D\nvolumes scanned from 175 mammalian species. Applications of our approach\ninclude stereoscopic rendering of legacy x-ray images, re-rendering of x-rays\nincluding changes of illumination, view pose or geometry. Our evaluation\nincludes comparison to previous tomography work, previous learning methods\nusing our data, a user study and application to a set of real x-rays.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.04867v3"
    },
    {
        "title": "Robust Structure-based Shape Correspondence",
        "authors": [
            "Yanir Kleiman",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a robust method to find region-level correspondences between\nshapes, which are invariant to changes in geometry and applicable across\nmultiple shape representations. We generate simplified shape graphs by jointly\ndecomposing the shapes, and devise an adapted graph-matching technique, from\nwhich we infer correspondences between shape regions. The simplified shape\ngraphs are designed to primarily capture the overall structure of the shapes,\nwithout reflecting precise information about the geometry of each region, which\nenables us to find correspondences between shapes that might have significant\ngeometric differences. Moreover, due to the special care we take to ensure the\nrobustness of each part of our pipeline, our method can find correspondences\nbetween shapes with different representations, such as triangular meshes and\npoint clouds. We demonstrate that the region-wise matching that we obtain can\nbe used to find correspondences between feature points, reveal the intrinsic\nself-similarities of each shape, and even construct point-to-point maps across\nshapes. Our method is both time and space efficient, leading to a pipeline that\nis significantly faster than comparable approaches. We demonstrate the\nperformance of our approach through an extensive quantitative and qualitative\nevaluation on several benchmarks where we achieve comparable or superior\nperformance to existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.05592v2"
    },
    {
        "title": "Subtractive Color Mixture Computation",
        "authors": [
            "Scott Allen Burns"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Modeling subtractive color mixture (e.g., the way that paints mix) is\ndifficult when working with colors described only by three-dimensional color\nspace values, such as RGB. Although RGB values are sufficient to describe a\nspecific color sensation, they do not contain enough information to predict the\nRGB color that would result from a subtractive mixture of two specified RGB\ncolors. Methods do exist for accurately modeling subtractive mixture, such as\nthe Kubelka-Munk equations, but require extensive spectrophotometric\nmeasurements of the mixed components, making them unsuitable for many computer\ngraphics applications. This paper presents a strategy for modeling subtractive\ncolor mixture given only the RGB information of the colors being mixed, written\nfor a general audience. The RGB colors are first transformed to generic,\nrepresentative spectral distributions, and then this spectral information is\nused to perform the subtractive mixture, using the weighted\narithmetic-geometric mean. This strategy provides reasonable, representative\nsubtractive mixture colors with only modest computational effort and no\nexperimental measurements. As such, it provides a useful way to model\nsubtractive color mixture in computer graphics applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.06364v1"
    },
    {
        "title": "Joint Material and Illumination Estimation from Photo Sets in the Wild",
        "authors": [
            "Tuanfeng Y. Wang",
            "Tobias Ritschel",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Faithful manipulation of shape, material, and illumination in 2D Internet\nimages would greatly benefit from a reliable factorization of appearance into\nmaterial (i.e., diffuse and specular) and illumination (i.e., environment\nmaps). On the one hand, current methods that produce very high fidelity\nresults, typically require controlled settings, expensive devices, or\nsignificant manual effort. To the other hand, methods that are automatic and\nwork on 'in the wild' Internet images, often extract only low-frequency\nlighting or diffuse materials. In this work, we propose to make use of a set of\nphotographs in order to jointly estimate the non-diffuse materials and sharp\nlighting in an uncontrolled setting. Our key observation is that seeing\nmultiple instances of the same material under different illumination (i.e.,\nenvironment), and different materials under the same illumination provide\nvaluable constraints that can be exploited to yield a high-quality solution\n(i.e., specular materials and environment illumination) for all the observed\nmaterials and environments. Similar constraints also arise when observing\nmultiple materials in a single environment, or a single material across\nmultiple environments. The core of this approach is an optimization procedure\nthat uses two neural networks that are trained on synthetic images to predict\ngood gradients in parametric space given observation of reflected light. We\nevaluate our method on a range of synthetic and real examples to generate\nhigh-quality estimates, qualitatively compare our results against\nstate-of-the-art alternatives via a user study, and demonstrate\nphoto-consistent image manipulation that is otherwise very challenging to\nachieve.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.08313v2"
    },
    {
        "title": "Deep Illumination: Approximating Dynamic Global Illumination with\n  Generative Adversarial Network",
        "authors": [
            "Manu Mathew Thomas",
            "Angus G. Forbes"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present Deep Illumination, a novel machine learning technique for\napproximating global illumination (GI) in real-time applications using a\nConditional Generative Adversarial Network. Our primary focus is on generating\nindirect illumination and soft shadows with offline rendering quality at\ninteractive rates. Inspired from recent advancement in image-to-image\ntranslation problems using deep generative convolutional networks, we introduce\na variant of this network that learns a mapping from Gbuffers (depth map,\nnormal map, and diffuse map) and direct illumination to any global illumination\nsolution. Our primary contribution is showing that a generative model can be\nused to learn a density estimation from screen space buffers to an advanced\nillumination model for a 3D environment. Once trained, our network can\napproximate global illumination for scene configurations it has never\nencountered before within the environment it was trained on. We evaluate Deep\nIllumination through a comparison with both a state of the art real-time GI\ntechnique (VXGI) and an offline rendering GI technique (path tracing). We show\nthat our method produces effective GI approximations and is also\ncomputationally cheaper than existing GI techniques. Our technique has the\npotential to replace existing precomputed and screen-space techniques for\nproducing global illumination effects in dynamic scenes with physically-based\nrendering quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.09834v2"
    },
    {
        "title": "Scan-flood Fill(SCAFF): an Efficient Automatic Precise Region Filling\n  Algorithm for Complicated Regions",
        "authors": [
            "Yixuan He",
            "Tianyi Hu",
            "Delu Zeng"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Recently, instant level labeling for supervised machine learning requires a\nconsiderable number of filled masks. In this paper, we propose an efficient\nautomatic region filling algorithm for complicated regions. Distinguishing\nbetween adjacent connected regions, the Main Filling Process scans through all\npixels and fills all the pixels except boundary ones with either exterior or\ninterior label color. In this way, we succeed in classifying all the pixels\ninside the region except boundary ones in the given image to form two groups: a\nbackground group and a mask group. We then set all exterior label pixels to\nbackground color, and interior label pixels to mask color. With this algorithm,\nwe are able to generate output masks precisely and efficiently even for\ncomplicated regions as long as boundary pixels are given. Experimental results\nshow that the proposed algorithm can generate precise masks that allow for\nvarious machine learning tasks such as supervised training. This algorithm can\neffectively handle multiple regions, complicated `holes' and regions whose\nboundaries touch the image border. By testing the algorithm on both toy and\npractical images, we show that the performance of Scan-flood Fill(SCAFF) has\nachieved favorable results.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.03366v2"
    },
    {
        "title": "Multi-Resolution Rendering for Computationally Expensive Lighting\n  Effects",
        "authors": [
            "Simon Besenthal",
            "Sebastian Maisch",
            "Timo Ropinski"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Many lighting methods used in computer graphics such as indirect illumination\ncan have very high computational costs and need to be approximated for\nreal-time applications. These costs can be reduced by means of upsampling\ntechniques which tend to introduce artifacts and affect the visual quality of\nthe rendered image. This paper suggests a versatile approach for accelerating\nthe rendering of screen space methods while maintaining the visual quality.\nThis is achieved by exploiting the low frequency nature of many of these\nillumination methods and the geometrical continuity of the scene. First the\nscreen space is dynamically divided into separate sub-images, then the\nillumination is rendered for each sub-image in an adequate resolution and\nfinally the sub-images are put together in order to compose the final image.\nTherefore we identify edges in the scene and generate masks precisely\nspecifying which part of the image is included in which sub-image. The masks\ntherefore determine which part of the image is rendered in which resolution. A\nstep wise upsampling and merging process then allows optically soft transitions\nbetween the different resolution levels. For this paper, the introduced\nmulti-resolution rendering method was implemented and tested on three commonly\nused lighting methods. These are screen space ambient occlusion, soft shadow\nmapping and screen space global illumination.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.04576v1"
    },
    {
        "title": "Estimating Homogeneous Data-driven BRDF Parameters from a Reflectance\n  Map under Known Natural Lighting",
        "authors": [
            "Victoria L. Cooper",
            "James C. Bieron",
            "Pieter Peers"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper we demonstrate robust estimation of the model parameters of a\nfully-linear data-driven BRDF model from a reflectance map under known natural\nlighting. To regularize the estimation of the model parameters, we leverage the\nreflectance similarities within a material class. We approximate the space of\nhomogeneous BRDFs using a Gaussian mixture model, and assign a material class\nto each Gaussian in the mixture model. We formulate the estimation of the model\nparameters as a non-linear maximum a-posteriori optimization, and introduce a\nlinear approximation that estimates a solution per material class from which\nthe best solution is selected. We demonstrate the efficacy and robustness of\nour method using the MERL BRDF database under a variety of natural lighting\nconditions, and we provide a proof-of-concept real-world experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.04777v1"
    },
    {
        "title": "Blue-noise sampling for human retinal cone spatial distribution modeling",
        "authors": [
            "Matteo P. Lanaro",
            "Hélène Perrier",
            "David Coeurjolly",
            "Victor Ostromoukhov",
            "Alessandro Rizzi"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper proposes a novel method for modeling human retinal cone\ndistribution. It is based on Blue-noise sampling algorithms that share\ninteresting properties with the sampling performed by the mosaic formed by cone\nphotoreceptors in the retina. Here we present the method together with a series\nof examples of various real retinal patches. The same samples have also been\ncreated with alternative algorithms and compared with plots of the center of\nthe inner segments of cone photoreceptors from imaged retinas. Results are\nevaluated with different distance measure used in the field, like\nnearest-neighbor analysis and pair correlation function. The proposed method\ncan describe features of a human retinal cone distribution with a certain\ndegree of similarity to the available data and can be efficiently used for\nmodeling local patches of retina.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05075v1"
    },
    {
        "title": "VIPER: Volume Invariant Position-based Elastic Rods",
        "authors": [
            "Baptiste Angles",
            "Daniel Rebain",
            "Miles Macklin",
            "Brian Wyvill",
            "Loic Barthe",
            "JP Lewis",
            "Javier von der Pahlen",
            "Shahram Izadi",
            "Julien Valentin",
            "Sofien Bouaziz",
            "Andrea Tagliasacchi"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We extend the formulation of position-based rods to include elastic\nvolumetric deformations. We achieve this by introducing an additional degree of\nfreedom per vertex -- isotropic scale (and its velocity). Including scale\nenriches the space of possible deformations, allowing the simulation of\nvolumetric effects, such as a reduction in cross-sectional area when a rod is\nstretched. We rigorously derive the continuous formulation of its elastic\nenergy potentials, and hence its associated position-based dynamics (PBD)\nupdates to realize this model, enabling the simulation of up to 26000 DOFs at\n140 Hz in our GPU implementation. We further show how rods can provide a\ncompact alternative to tetrahedral meshes for the representation of complex\nmuscle deformations, as well as providing a convenient representation for\ncollision detection. This is achieved by modeling a muscle as a bundle of rods,\nfor which we also introduce a technique to automatically convert a muscle\nsurface mesh into a rods-bundle. Finally, we show how rods and/or bundles can\nbe skinned to a surface mesh to drive its deformation, resulting in an\nalternative to cages for real-time volumetric deformation.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05260v1"
    },
    {
        "title": "scenery: Flexible Virtual Reality Visualization on the Java VM",
        "authors": [
            "Ulrik Günther",
            "Tobias Pietzsch",
            "Aryaman Gupta",
            "Kyle I. S. Harrington",
            "Pavel Tomancak",
            "Stefan Gumhold",
            "Ivo F. Sbalzarini"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Life science today involves computational analysis of a large amount and\nvariety of data, such as volumetric data acquired by state-of-the-art\nmicroscopes, or mesh data from analysis of such data or simulations.\nVisualization is often the first step in making sense of data, and a crucial\npart of building and debugging analysis pipelines. It is therefore important\nthat visualizations can be quickly prototyped, as well as developed or embedded\ninto full applications. In order to better judge spatiotemporal relationships,\nimmersive hardware, such as Virtual or Augmented Reality (VR/AR) headsets and\nassociated controllers are becoming invaluable tools. In this work we introduce\nscenery, a flexible VR/AR visualization framework for the Java VM that can\nhandle mesh and large volumetric data, containing multiple views, timepoints,\nand color channels. scenery is free and open-source software, works on all\nmajor platforms, and uses the Vulkan or OpenGL rendering APIs. We introduce\nscenery's main features and example applications, such as its use in VR for\nmicroscopy, in the biomedical image analysis software Fiji, or for visualizing\nagent-based simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.06726v3"
    },
    {
        "title": "Camouflage Design of Analysis Based on HSV Color Statistics and K-means\n  Clustering",
        "authors": [
            "Xinyu Wei",
            "Mengjia Zhou",
            "Bernie Liu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Since ancient times, it has been essential to adopting camouflage on the\nbattlefield, whether it is in the forefront, in-depth or the rear. The\ntraditional evaluation method is made up of people opinion. By watching target\nor looking at the pictures, and determine the effect of camouflage, so it can\nbe more influenced by man's subjective factors. And now, in order to\nobjectively reflect the camouflage effect, we set up a model through using\nimages similarity to evaluate camouflage effect. Image similarity comparison is\ndivided into two main image feature comparison: image color features and\ntexture features of images. We now using computer design camouflage, camouflage\npattern design is divided into two aspects of design color and design plaques.\nFor the design of the color, we based on HSV color model, and as for the design\nof plague, the key steps are the background color edge extraction, we adopt\nalgorithm based on k-means clustering analysis of the method of background\ncolor edge extraction.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.07968v1"
    },
    {
        "title": "ZomeFab: Cost-effective Hybrid Fabrication with Zometools",
        "authors": [
            "I-Chao Shen",
            "Ming-Shiuan Chen",
            "Chun-Kai Huang",
            "Bing-Yu Chen"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In recent years, personalized fabrication has received considerable attention\nbecause of the widespread use of consumer-level three-dimensional (3D)\nprinters. However, such 3D printers have drawbacks, such as long production\ntime and limited output size, which hinder large-scale rapid-prototyping. In\nthis paper, for the time- and cost-effective fabrication of large-scale\nobjects, we propose a hybrid 3D fabrication method that combines 3D printing\nand the Zometool construction set, which is a compact, sturdy, and reusable\nstructure for infill fabrication. The proposed method significantly reduces\nfabrication cost and time by printing only thin 3D outer shells. In addition,\nwe design an optimization framework to generate both a Zometool structure and\nprinted surface partitions by optimizing several criteria, including\nprintability, material cost, and Zometool structure complexity. Moreover, we\ndemonstrate the effectiveness of the proposed method by fabricating various\nlarge-scale 3D models.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.09787v2"
    },
    {
        "title": "Flexible SVBRDF Capture with a Multi-Image Deep Network",
        "authors": [
            "Valentin Deschaintre",
            "Miika Aittala",
            "Fredo Durand",
            "George Drettakis",
            "Adrien Bousseau"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Empowered by deep learning, recent methods for material capture can estimate\na spatially-varying reflectance from a single photograph. Such lightweight\ncapture is in stark contrast with the tens or hundreds of pictures required by\ntraditional optimization-based approaches. However, a single image is often\nsimply not enough to observe the rich appearance of real-world materials. We\npresent a deep-learning method capable of estimating material appearance from a\nvariable number of uncalibrated and unordered pictures captured with a handheld\ncamera and flash. Thanks to an order-independent fusing layer, this\narchitecture extracts the most useful information from each picture, while\nbenefiting from strong priors learned from data. The method can handle both\nview and light direction variation without calibration. We show how our method\nimproves its prediction with the number of input pictures, and reaches high\nquality reconstructions with as little as 1 to 10 images -- a sweet spot\nbetween existing single-image and complex multi-image approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.11557v1"
    },
    {
        "title": "DVP: Data Visualization Platform",
        "authors": [
            "Waleed A. Yousef",
            "Ahmed A. Abouelkahire",
            "Omar S. Marzouk",
            "Sameh K. Mohamed",
            "Mohamed N. Alaggan"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We identify two major steps in data analysis, data exploration for\nunderstanding and observing patterns/relationships in data; and construction,\ndesign and assessment of various models to formalize these relationships. For\neach step, there exists a large set of tools and software. For the first step,\nmany visualization tools exist, such as, GGobi, Parallax, and Crystal Vision,\nand most recently tableau and plottly. For the second step, many Scientific\nComputing Environments (SCEs) exist, such as, Matlab, Mathematica, R and\nPython. However, there does not exist a tool which allows for seamless two-way\ninteraction between visualization tools and SCEs. We have designed and\nimplemented a data visualization platform (DVP) with an architecture and design\nthat attempts to bridge this gap. DVP connects seamlessly to SCEs to bring the\ncomputational capabilities to the visualization methods in a single coherent\nplatform. DVP is designed with two interfaces, the desktop stand alone version\nand the online interface. To illustrate the power of DVP design, a free demo\nfor the online interface of DVP is available \\citep{DVP} and very low-level\ndesign details are explained in this article. Since DVP was launched, circa\n2012, the present manuscript was not published since today for\ncommercialization and patent considerations.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.11738v2"
    },
    {
        "title": "Learning Manifold Patch-Based Representations of Man-Made Shapes",
        "authors": [
            "Dmitriy Smirnov",
            "Mikhail Bessmeltsev",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Choosing the right representation for geometry is crucial for making 3D\nmodels compatible with existing applications. Focusing on piecewise-smooth\nman-made shapes, we propose a new representation that is usable in conventional\nCAD modeling pipelines and can also be learned by deep neural networks. We\ndemonstrate its benefits by applying it to the task of sketch-based modeling.\nGiven a raster image, our system infers a set of parametric surfaces that\nrealize the input in 3D. To capture piecewise smooth geometry, we learn a\nspecial shape representation: a deformable parametric template composed of\nCoons patches. Naively training such a system, however, is hampered by\nnon-manifold artifacts in the parametric shapes and by a lack of data. To\naddress this, we introduce loss functions that bias the network to output\nnon-self-intersecting shapes and implement them as part of a fully\nself-supervised system, automatically generating both shape templates and\nsynthetic training data. We develop a testbed for sketch-based modeling,\ndemonstrate shape interpolation, and provide comparison to related work.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.12337v3"
    },
    {
        "title": "Efficient 2D Simulation on Moving 3D Surfaces",
        "authors": [
            "Dieter Morgenroth",
            "Stefan Reinhardt",
            "Daniel Weiskopf",
            "Bernhard Eberhardt"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a method to simulate fluid flow on evolving surfaces, e.g., an oil\nfilm on a water surface. Given an animated surface (e.g., extracted from a\nparticle-based fluid simulation) in three-dimensional space, we add a second\nsimulation on this base animation. In general, we solve a partial differential\nequation (PDE) on a level set surface obtained from the animated input surface.\nThe properties of the input surface are transferred to a sparse volume data\nstructure that is then used for the simulation. We introduce one-way coupling\nstrategies from input properties to our simulation and we add conservation of\nmass and momentum to existing methods that solve a PDE in a narrow-band using\nthe Closest Point Method. In this way, we efficiently compute high-resolution\n2D simulations on coarse input surfaces. Our approach helps visual effects\ncreators easily integrate a workflow to simulate material flow on evolving\nsurfaces into their existing production pipeline.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00408v1"
    },
    {
        "title": "Inspection of histological 3D reconstructions in virtual reality",
        "authors": [
            "Oleg Lobachev",
            "Moritz Berthold",
            "Henriette Pfeffer",
            "Michael Guthe",
            "Birte S. Steiniger"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  3D reconstruction is a challenging current topic in medical research. We\nperform 3D reconstructions from serial sections stained by immunohistological\nmethods. This paper presents an immersive visualisation solution to quality\ncontrol (QC), inspect, and analyse such reconstructions. QC is essential to\nestablish correct digital processing methodologies. Visual analytics, such as\nannotation placement, mesh painting, and classification utility, facilitates\nmedical research insights. We propose a visualisation in virtual reality (VR)\nfor these purposes. In this manner, we advance the microanatomical research of\nhuman bone marrow and spleen. Both 3D reconstructions and original data are\navailable in VR. Data inspection is streamlined by subtle implementation\ndetails and general immersion in VR.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00887v2"
    },
    {
        "title": "A Study of Opacity Ranges for Transparent Overlays in 3D Landscapes",
        "authors": [
            "Jan Hombeck",
            "Li Ji",
            "Kai Lawonn",
            "Charles Perin"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  {When visualizing data in a realistically rendered 3D virtual environment, it\nis often important to represent not only the 3D scene but also overlaid\ninformation about additional, abstract data. These overlays must be usefully\nvisible, i.e. be readable enough to convey the information they represent, but\nremain unobtrusive to avoid cluttering the view. We take a step toward\nestablishing guidelines for designing such overlays by studying the\nrelationship between three different patterns (filled, striped and dotted\npatterns), two pattern densities, the presence or not of a solid outline, two\ntypes of background (blank and with trees), and the opacity of the overlay. For\neach combination of factors, participants set the faintest and the strongest\nacceptable opacity values. Results from this first study suggest that i) ranges\nof acceptable opacities are around 20-70%, that ii) ranges can be extended by\n5% by using an outline, and that iii) ranges shift based on features like\npattern and density.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00931v1"
    },
    {
        "title": "DeformSyncNet: Deformation Transfer via Synchronized Shape Deformation\n  Spaces",
        "authors": [
            "Minhyuk Sung",
            "Zhenyu Jiang",
            "Panos Achlioptas",
            "Niloy J. Mitra",
            "Leonidas J. Guibas"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Shape deformation is an important component in any geometry processing\ntoolbox. The goal is to enable intuitive deformations of single or multiple\nshapes or to transfer example deformations to new shapes while preserving the\nplausibility of the deformed shape(s). Existing approaches assume access to\npoint-level or part-level correspondence or establish them in a preprocessing\nphase, thus limiting the scope and generality of such approaches. We propose\nDeformSyncNet, a new approach that allows consistent and synchronized shape\ndeformations without requiring explicit correspondence information.\nTechnically, we achieve this by encoding deformations into a class-specific\nidealized latent space while decoding them into an individual, model-specific\nlinear deformation action space, operating directly in 3D. The underlying\nencoding and decoding are performed by specialized (jointly trained) neural\nnetworks. By design, the inductive bias of our networks results in a\ndeformation space with several desirable properties, such as path invariance\nacross different deformation pathways, which are then also approximately\npreserved in real space. We qualitatively and quantitatively evaluate our\nframework against multiple alternative approaches and demonstrate improved\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.01456v1"
    },
    {
        "title": "Improving the Usability of Virtual Reality Neuron Tracing with\n  Topological Elements",
        "authors": [
            "Torin McDonald",
            "Will Usher",
            "Nate Morrical",
            "Attila Gyulassy",
            "Steve Petruzza",
            "Frederick Federer",
            "Alessandra Angelucci",
            "Valerio Pascucci"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Researchers in the field of connectomics are working to reconstruct a map of\nneural connections in the brain in order to understand at a fundamental level\nhow the brain processes information. Constructing this wiring diagram is done\nby tracing neurons through high-resolution image stacks acquired with\nfluorescence microscopy imaging techniques. While a large number of automatic\ntracing algorithms have been proposed, these frequently rely on local features\nin the data and fail on noisy data or ambiguous cases, requiring time-consuming\nmanual correction. As a result, manual and semi-automatic tracing methods\nremain the state-of-the-art for creating accurate neuron reconstructions. We\npropose a new semi-automatic method that uses topological features to guide\nusers in tracing neurons and integrate this method within a virtual reality\n(VR) framework previously used for manual tracing. Our approach augments both\nvisualization and interaction with topological elements, allowing rapid\nunderstanding and tracing of complex morphologies. In our pilot study,\nneuroscientists demonstrated a strong preference for using our tool over prior\napproaches, reported less fatigue during tracing, and commended the ability to\nbetter understand possible paths and alternatives. Quantitative evaluation of\nthe traces reveals that users' tracing speed increased, while retaining similar\naccuracy compared to a fully manual approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.01891v1"
    },
    {
        "title": "Chordal Decomposition for Spectral Coarsening",
        "authors": [
            "Honglin Chen",
            "Hsueh-Ti Derek Liu",
            "Alec Jacobson",
            "David I. W. Levin"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce a novel solver to significantly reduce the size of a geometric\noperator while preserving its spectral properties at the lowest frequencies. We\nuse chordal decomposition to formulate a convex optimization problem which\nallows the user to control the operator sparsity pattern. This allows for a\ntrade-off between the spectral accuracy of the operator and the cost of its\napplication. We efficiently minimize the energy with a change of variables and\nachieve state-of-the-art results on spectral coarsening. Our solver further\nenables novel applications including volume-to-surface approximation and\ndetaching the operator from the mesh, i.e., one can produce a mesh tailormade\nfor visualization and optimize an operator separately for computation.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02294v2"
    },
    {
        "title": "Complementary Dynamics",
        "authors": [
            "Jiayi Eris Zhang",
            "Seungbae Bang",
            "David I. W. Levin",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a novel approach to enrich arbitrary rig animations with\nelastodynamic secondary effects. Unlike previous methods which pit rig\ndisplacements and physical forces as adversaries against each other, we\nadvocate that physics should complement artists intentions. We propose\noptimizing for elastodynamic displacements in the subspace orthogonal to\ndisplacements that can be created by the rig. This ensures that the additional\ndynamic motions do not undo the rig animation. The complementary space is high\ndimensional, algebraically constructed without manual oversight, and capable of\nrich high-frequency dynamics. Unlike prior tracking methods, we do not require\nextra painted weights, segmentation into fixed and free regions or tracking\nclusters. Our method is agnostic to the physical model and plugs into\nnon-linear FEM simulations, geometric as-rigid-as-possible energies, or\nmass-spring models. Our method does not require a particular type of rig and\nadds secondary effects to skeletal animations, cage-based deformations, wire\ndeformers, motion capture data, and rigid-body simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02462v1"
    },
    {
        "title": "A curvature and density-based generative representation of shapes",
        "authors": [
            "Zi Ye",
            "Nobuyuki Umetani",
            "Takeo Igarashi",
            "Tim Hoffmann"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper introduces a generative model for 3D surfaces based on a\nrepresentation of shapes with mean curvature and metric, which are invariant\nunder rigid transformation. Hence, compared with existing 3D machine learning\nframeworks, our model substantially reduces the influence of translation and\nrotation. In addition, the local structure of shapes will be more precisely\ncaptured, since the curvature is explicitly encoded in our model. Specifically,\nevery surface is first conformally mapped to a canonical domain, such as a unit\ndisk or a unit sphere. Then, it is represented by two functions: the mean\ncurvature half-density and the vertex density, over this canonical domain.\nAssuming that input shapes follow a certain distribution in a latent space, we\nuse the variational autoencoder to learn the latent space representation. After\nthe learning, we can generate variations of shapes by randomly sampling the\ndistribution in the latent space. Surfaces with triangular meshes can be\nreconstructed from the generated data by applying isotropic remeshing and spin\ntransformation, which is given by Dirac equation. We demonstrate the\neffectiveness of our model on datasets of man-made and biological shapes and\ncompare the results with other methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02494v1"
    },
    {
        "title": "Palettailor: Discriminable Colorization for Categorical Data",
        "authors": [
            "Kecheng Lu",
            "Mi Feng",
            "Xin Chen",
            "Michael Sedlmair",
            "Oliver Deussen",
            "Dani Lischinski",
            "Zhanglin Cheng",
            "Yunhai Wang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present an integrated approach for creating and assigning color palettes\nto different visualizations such as multi-class scatterplots, line, and bar\ncharts. While other methods separate the creation of colors from their\nassignment, our approach takes data characteristics into account to produce\ncolor palettes, which are then assigned in a way that fosters better visual\ndiscrimination of classes. To do so, we use a customized optimization based on\nsimulated annealing to maximize the combination of three carefully designed\ncolor scoring functions: point distinctness, name difference, and color\ndiscrimination. We compare our approach to state-ofthe-art palettes with a\ncontrolled user study for scatterplots and line charts, furthermore we\nperformed a case study. Our results show that Palettailor, as a fully-automated\napproach, generates color palettes with a higher discrimination quality than\nexisting approaches. The efficiency of our optimization allows us also to\nincorporate user modifications into the color selection process.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02969v1"
    },
    {
        "title": "Nonlinear Spectral Geometry Processing via the TV Transform",
        "authors": [
            "Marco Fumero",
            "Michael Moeller",
            "Emanuele Rodolà"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce a novel computational framework for digital geometry processing,\nbased upon the derivation of a nonlinear operator associated to the total\nvariation functional. Such operator admits a generalized notion of spectral\ndecomposition, yielding a sparse multiscale representation akin to\nLaplacian-based methods, while at the same time avoiding undesirable\nover-smoothing effects typical of such techniques. Our approach entails\naccurate, detail-preserving decomposition and manipulation of 3D shape geometry\nwhile taking an especially intuitive form: non-local semantic details are well\nseparated into different bands, which can then be filtered and re-synthesized\nwith a straightforward linear step. Our computational framework is flexible,\ncan be applied to a variety of signals, and is easily adapted to different\ngeometry representations, including triangle meshes and point clouds. We\nshowcase our method throughout multiple applications in graphics, ranging from\nsurface and signal denoising to detail transfer and cubic stylization.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03044v1"
    },
    {
        "title": "A Fast Parametric Ellipse Algorithm",
        "authors": [
            "Jerry R. Van Aken"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper describes a 2-D graphics algorithm that uses shifts and adds to\nprecisely plot a series of points on an ellipse of any shape and orientation.\nThe algorithm can also plot an elliptic arc that starts and ends at arbitrary\nangles. The ellipse algorithm described here is largely based on earlier papers\nby Van Aken and Simar [1,2], which extend Marvin Minsky's well-known circle\nalgorithm [3,4,5] to ellipses, and show how to cancel out the sources of error\nin Minsky's original algorithm. A new flatness test is presented for\nautomatically controlling the spacing between points plotted on an ellipse or\nelliptic arc. Most of the calculations performed by the ellipse algorithm and\nflatness test use fixed-point addition and shift operations, and thus are\nwell-suited to run on less-powerful processors. C++ source code listings are\nincluded.\n  Keywords: parametric ellipse algorithm, rotated ellipse, Minsky circle\nalgorithm, flatness, elliptic arc, conjugate diameters, affine invariance\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03434v4"
    },
    {
        "title": "A GPU Parallel Algorithm for Computing Morse-Smale Complexes",
        "authors": [
            "Varshini Subhash",
            "Karran Pandey",
            "Vijay Natarajan"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The Morse-Smale complex is a well studied topological structure that\nrepresents the gradient flow behavior between critical points of a scalar\nfunction. It supports multi-scale topological analysis and visualization of\nfeature-rich scientific data. Several parallel algorithms have been proposed\ntowards the fast computation of the 3D Morse-Smale complex. Its computation\ncontinues to pose significant algorithmic challenges. In particular, the\nnon-trivial structure of the connections between the saddle critical points are\nnot amenable to parallel computation. This paper describes a fine grained\nparallel algorithm for computing the Morse-Smale complex and a GPU\nimplementation gMSC. The algorithm first determines the saddle-saddle\nreachability via a transformation into a sequence of vector operations, and\nnext computes the paths between saddles by transforming it into a sequence of\nmatrix operations. Computational experiments show that the method achieves up\nto 8.6x speedup over pyms3d and 6x speedup over TTK, the current shared memory\nimplementations. The paper also presents a comprehensive experimental analysis\nof different steps of the algorithm and reports on their contribution towards\nruntime performance. Finally, it introduces a CPU based data parallel algorithm\nfor simplifying the Morse-Smale complex via iterative critical point pair\ncancellation.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03707v3"
    },
    {
        "title": "Mode Surfaces of Symmetric Tensor Fields: Topological Analysis and\n  Seamless Extraction",
        "authors": [
            "Botong Qu",
            "Lawrence Roy",
            "Yue Zhang",
            "Eugene Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Mode surfaces are the generalization of degenerate curves and neutral\nsurfaces, which constitute 3D symmetric tensor field topology. Efficient\nanalysis and visualization of mode surfaces can provide additional insight into\nnot only degenerate curves and neutral surfaces, but also how these features\ntransition into each other. Moreover, the geometry and topology of mode\nsurfaces can help domain scientists better understand the tensor fields in\ntheir applications. Existing mode surface extraction methods can miss features\nin the surfaces. Moreover, the mode surfaces extracted from neighboring cells\nhave gaps, which make their subsequent analysis difficult. In this paper, we\nprovide novel analysis on the topological structures of mode surfaces,\nincluding a common parameterization of all mode surfaces of a tensor field\nusing 2D asymmetric tensors. This allows us to not only better understand the\nstructures in mode surfaces and their interactions with degenerate curves and\nneutral surfaces, but also develop an efficient algorithm to seamlessly extract\nmode surfaces, including neutral surfaces. The seamless mode surfaces enable\nefficient analysis of their geometric structures, such as the principal\ncurvature directions. We apply our analysis and visualization to a number of\nsolid mechanics data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.04601v1"
    },
    {
        "title": "Data-Driven Space-Filling Curves",
        "authors": [
            "Liang Zhou",
            "Chris R. Johnson",
            "Daniel Weiskopf"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose a data-driven space-filling curve method for 2D and 3D\nvisualization. Our flexible curve traverses the data elements in the spatial\ndomain in a way that the resulting linearization better preserves features in\nspace compared to existing methods. We achieve such data coherency by\ncalculating a Hamiltonian path that approximately minimizes an objective\nfunction that describes the similarity of data values and location coherency in\na neighborhood. Our extended variant even supports multiscale data via\nquadtrees and octrees. Our method is useful in many areas of visualization,\nincluding multivariate or comparative visualization, ensemble visualization of\n2D and 3D data on regular grids, or multiscale visual analysis of particle\nsimulations. The effectiveness of our method is evaluated with numerical\ncomparisons to existing techniques and through examples of ensemble and\nmultivariate datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06309v1"
    },
    {
        "title": "Smoothed Particle Hydrodynamics Techniques for the Physics Based\n  Simulation of Fluids and Solids",
        "authors": [
            "Dan Koschier",
            "Jan Bender",
            "Barbara Solenthaler",
            "Matthias Teschner"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Graphics research on Smoothed Particle Hydrodynamics (SPH) has produced\nfantastic visual results that are unique across the board of research\ncommunities concerned with SPH simulations. Generally, the SPH formalism serves\nas a spatial discretization technique, commonly used for the numerical\nsimulation of continuum mechanical problems such as the simulation of fluids,\nhighly viscous materials, and deformable solids. Recent advances in the field\nhave made it possible to efficiently simulate massive scenes with highly\ncomplex boundary geometries on a single PC [Com16b, Com16a]. Moreover, novel\ntechniques allow to robustly handle interactions among various materials\n[Com18,Com17]. As of today, graphics-inspired pressure solvers, neighborhood\nsearch algorithms, boundary formulations, and other contributions often serve\nas core components in commercial software for animation purposes [Nex17] as\nwell as in computer-aided engineering software [FIF16].\n  This tutorial covers various aspects of SPH simulations. Governing equations\nfor mechanical phenomena and their SPH discretizations are discussed. Concepts\nand implementations of core components such as neighborhood search algorithms,\npressure solvers, and boundary handling techniques are presented.\nImplementation hints for the realization of SPH solvers for fluids, elastic\nsolids, and rigid bodies are given. The tutorial combines the introduction of\ntheoretical concepts with the presentation of actual implementations.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06944v1"
    },
    {
        "title": "Differentiable Refraction-Tracing for Mesh Reconstruction of Transparent\n  Objects",
        "authors": [
            "Jiahui Lyu",
            "Bojian Wu",
            "Dani Lischinski",
            "Daniel Cohen-Or",
            "Hui Huang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Capturing the 3D geometry of transparent objects is a challenging task,\nill-suited for general-purpose scanning and reconstruction techniques, since\nthese cannot handle specular light transport phenomena. Existing\nstate-of-the-art methods, designed specifically for this task, either involve a\ncomplex setup to reconstruct complete refractive ray paths, or leverage a\ndata-driven approach based on synthetic training data. In either case, the\nreconstructed 3D models suffer from over-smoothing and loss of fine detail.\nThis paper introduces a novel, high precision, 3D acquisition and\nreconstruction method for solid transparent objects. Using a static background\nwith a coded pattern, we establish a mapping between the camera view rays and\nlocations on the background. Differentiable tracing of refractive ray paths is\nthen used to directly optimize a 3D mesh approximation of the object, while\nsimultaneously ensuring silhouette consistency and smoothness. Extensive\nexperiments and comparisons demonstrate the superior accuracy of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09144v1"
    },
    {
        "title": "3D Modeling and WebVR Implementation using Azure Kinect, Open3D, and\n  Three.js",
        "authors": [
            "Won Joon Yun",
            "Joongheon Kim"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper proposes a method of extracting an RGB-D image usingAzure Kinect,\na depth camera, creating afragment,i.e., 6D images (RGBXYZ), usingOpen3D,\ncreatingit as a point cloud object, and implementing webVR usingthree.js.\nFurthermore, it presents limitations and potentials for development.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09357v1"
    },
    {
        "title": "3D Primitives Gpgpu Generation for Volume Visualization in 3D Graphics\n  Systems",
        "authors": [
            "Anas M. Al-Oraiqat",
            "Sergii A. Zori"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This article discusses the study of 3D graphic volume primitive computer\nsystem generation (3D segments) based on General Purpose Graphics Processing\nUnit (GPGPU) technology for 3D volume visualization systems. It is based on the\ngeneral method of Volume 3D primitive generation and an algorithm for the\nvoxelization of 3D lines, previously proposed and studied by the authors. We\nconsidered the Compute Unified Device Architect (CUDA) implementation of a\nparametric method for generating 3D line segments and characteristics of\ngeneration on modern Graphics Processing Units. Experiments on the test bench\nshowed the relative inefficiency of generating a single 3D line segment and the\nefficiency of generating both fixed and arbitrary length of 3D segments on a\nGraphics Processing Unit (GPU). Experimental studies have proven the\neffectiveness and the quality of produced solutions by our method, when\ncompared to existing state-of-the-art approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09500v1"
    },
    {
        "title": "3D Pseudo Stereo Visualization with Gpgpu Support",
        "authors": [
            "Anas M. Al-Oraiqat",
            "Sergii A. Zori"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This article discusses the study of a computer system for creating 3D\npseudo-stereo images and videos using hardware and software support for\naccelerating a synthesis process based on General Purpose Graphics Processing\nUnit (GPGPU) technology. Based on the general strategy of 3D pseudo-stereo\nsynthesis previously proposed by the authors, Compute Unified Device Architect\n(CUDA) method considers the main implementation stages of 3D pseudo-stereo\nsynthesis: (i) the practical implementation study; (ii) the synthesis\ncharacteristics for obtaining images; (iii) the video in Ultra-High Definition\n(UHD) 4K resolution using the Graphics Processing Unit (GPU). Respectively with\nthese results of 4K content test on evaluation systems with a GPU the\nacceleration average of 60.6 and 6.9 times is obtained for images and videos.\nThe research results show consistency with previously identified forecasts for\nprocessing 4K image frames. They are confirming the possibility of synthesizing\n3D pseudo-stereo algorithms in real time using powerful support for modern\nGraphics Processing Unit/Graphics Processing Clusters (GPU/GPC).\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09501v1"
    },
    {
        "title": "Scaling Probe-Based Real-Time Dynamic Global Illumination for Production",
        "authors": [
            "Zander Majercik",
            "Adam Marrs",
            "Josef Spjut",
            "Morgan McGuire"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We contribute several practical extensions to the probe based\nirradiance-field-with-visibility representation to improve image quality,\nconstant and asymptotic performance, memory efficiency, and artist control. We\ndeveloped these extensions in the process of incorporating the previous work\ninto the global illumination solutions of the NVIDIA RTXGI SDK, the Unity and\nUnreal Engine 4 game engines, and proprietary engines for several commercial\ngames. These extensions include: a single, intuitive tuning parameter (the\n\"self-shadow\" bias); heuristics to speed transitions in the global\nillumination; reuse of irradiance data as prefiltered radiance for recursive\nglossy reflection; a probe state machine to prune work that will not affect the\nfinal image; and multiresolution cascaded volumes for large worlds.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.10796v3"
    },
    {
        "title": "Visualization of Human Spine Biomechanics for Spinal Surgery",
        "authors": [
            "Pepe Eulzer",
            "Sabine Bauer",
            "Francis Kilian",
            "Kai Lawonn"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose a visualization application, designed for the exploration of human\nspine simulation data. Our goal is to support research in biomechanical spine\nsimulation and advance efforts to implement simulation-backed analysis in\nsurgical applications. Biomechanical simulation is a state-of-the-art technique\nfor analyzing load distributions of spinal structures. Through the inclusion of\npatient-specific data, such simulations may facilitate personalized treatment\nand customized surgical interventions. Difficulties in spine modelling and\nsimulation can be partly attributed to poor result representation, which may\nalso be a hindrance when introducing such techniques into a clinical\nenvironment. Comparisons of measurements across multiple similar anatomical\nstructures and the integration of temporal data make commonly available\ndiagrams and charts insufficient for an intuitive and systematic display of\nresults. Therefore, we facilitate methods such as multiple coordinated views,\nabstraction and focus and context to display simulation outcomes in a dedicated\ntool. By linking the result data with patient-specific anatomy, we make\nrelevant parameters tangible for clinicians. Furthermore, we introduce new\nconcepts to show the directions of impact force vectors, which were not\naccessible before. We integrated our toolset into a spine segmentation and\nsimulation pipeline and evaluated our methods with both surgeons and\nbiomechanical researchers. When comparing our methods against standard\nrepresentations that are currently in use, we found increases in accuracy and\nspeed in data exploration tasks. In a qualitative review, domain experts deemed\nthe tool highly useful when dealing with simulation result data, which\ntypically combines time-dependent patient movement and the resulting force\ndistributions on spinal structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.11148v1"
    },
    {
        "title": "A Testing Environment for Continuous Colormaps",
        "authors": [
            "Pascal Nardini",
            "Min Chen",
            "Roxana Bujack",
            "Michael Böttinger",
            "Gerik Scheuermann"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Many computer science disciplines (e.g., combinatorial optimization, natural\nlanguage processing, and information retrieval) use standard or established\ntest suites for evaluating algorithms. In visualization, similar approaches\nhave been adopted in some areas (e.g., volume visualization), while user\ntestimonies and empirical studies have been the dominant means of evaluation in\nmost other areas, such as designing colormaps. In this paper, we propose to\nestablish a test suite for evaluating the design of colormaps. With such a\nsuite, the users can observe the effects when different continuous colormaps\nare applied to planar scalar fields that may exhibit various characteristic\nfeatures, such as jumps, local extrema, ridge or valley lines, different\ndistributions of scalar values, different gradients, different signal\nfrequencies, different levels of noise, and so on. The suite also includes an\nexpansible collection of real-world data sets including the most popular data\nfor colormap testing in the visualization literature. The test suite has been\nintegrated into a web-based application for creating continuous colormaps\n(https://ccctool.com/), facilitating close inter-operation between design and\nevaluation processes. This new facility complements traditional evaluation\nmethods such as user testimonies and empirical studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13133v1"
    },
    {
        "title": "A Large Scale Benchmark and an Inclusion-Based Algorithm for Continuous\n  Collision Detection",
        "authors": [
            "Bolun Wang",
            "Zachary Ferguson",
            "Teseo Schneider",
            "Xin Jiang",
            "Marco Attene",
            "Daniele Panozzo"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce a large scale benchmark for continuous collision detection (CCD)\nalgorithms, composed of queries manually constructed to highlight challenging\ndegenerate cases and automatically generated using existing simulators to cover\ncommon cases. We use the benchmark to evaluate the accuracy, correctness, and\nefficiency of state-of-the-art continuous collision detection algorithms, both\nwith and without minimal separation. We discover that, despite the widespread\nuse of CCD algorithms, existing algorithms are either: (1) correct but\nimpractically slow, (2) efficient but incorrect, introducing false negatives\nwhich will lead to interpenetration, or (3) correct but over conservative,\nreporting a large number of false positives which might lead to inaccuracies\nwhen integrated in a simulator. By combining the seminal interval root finding\nalgorithm introduced by Snyder in 1992 with modern predicate design techniques,\nwe propose a simple and efficient CCD algorithm. This algorithm is competitive\nwith state of the art methods in terms of runtime while conservatively\nreporting the time of impact and allowing explicit trade off between runtime\nefficiency and number of false positives reported.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13349v2"
    },
    {
        "title": "Asynchronous Liquids: Regional Time Stepping for Faster SPH and PCISPH",
        "authors": [
            "Prashant Goswami",
            "Christopher Batty"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper presents novel and efficient strategies to spatially adapt the\namount of computational effort applied based on the local dynamics of a free\nsurface flow, for both classic weakly compressible SPH (WCSPH) and\npredictive-corrective incompressible SPH (PCISPH). Using a convenient and\nreadily parallelizable block-based approach, different regions of the fluid are\nassigned differing time steps and solved at different rates to minimize\ncomputational cost. Our approach for WCSPH scheme extends an asynchronous SPH\ntechnique from compressible flow of astrophysical phenomena to the\nincompressible free surface setting, and further accelerates it by entirely\ndecoupling the time steps of widely spaced particles. Similarly, our approach\nto PCISPH adjusts the the number of iterations of density correction applied to\ndifferent regions, and asynchronously updates the neighborhood regions used to\nperform these corrections; this sharply reduces the computational cost of\nslowly deforming regions while preserving the standard density invariant. We\ndemonstrate our approaches on a number of highly dynamic scenarios,\ndemonstrating that they can typically double the speed of a simulation compared\nto standard methods while achieving visually consistent results.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.14514v1"
    },
    {
        "title": "Turbulent Details Simulation for SPH Fluids via Vorticity Refinement",
        "authors": [
            "Sinuo Liu",
            "Xiaokun Wang",
            "Xiaojuan Ban",
            "Yanrui Xu",
            "Jing Zhou",
            "Jiří Kosinka",
            "Alexandru C. Telea"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  A major issue in Smoothed Particle Hydrodynamics (SPH) approaches is the\nnumerical dissipation during the projection process, especially under coarse\ndiscretizations. High-frequency details, such as turbulence and vortices, are\nsmoothed out, leading to unrealistic results. To address this issue, we\nintroduce a Vorticity Refinement (VR) solver for SPH fluids with negligible\ncomputational overhead. In this method, the numerical dissipation of the\nvorticity field is recovered by the difference between the theoretical and the\nactual vorticity, so as to enhance turbulence details. Instead of solving the\nBiot-Savart integrals, a stream function, which is easier and more efficient to\nsolve, is used to relate the vorticity field to the velocity field. We obtain\nturbulence effects of different intensity levels by changing an adjustable\nparameter. Since the vorticity field is enhanced according to the curl field,\nour method can not only amplify existing vortices, but also capture additional\nturbulence. Our VR solver is straightforward to implement and can be easily\nintegrated into existing SPH methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.14535v1"
    },
    {
        "title": "Structured Regularization of Functional Map Computations",
        "authors": [
            "Jing Ren",
            "Mikhail Panine",
            "Peter Wonka",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We consider the problem of non-rigid shape matching using the functional map\nframework. Specifically, we analyze a commonly used approach for regularizing\nfunctional maps, which consists in penalizing the failure of the unknown map to\ncommute with the Laplace-Beltrami operators on the source and target shapes. We\nshow that this approach has certain undesirable fundamental theoretical\nlimitations, and can be undefined even for trivial maps in the smooth setting.\nInstead we propose a novel, theoretically well-justified approach for\nregularizing functional maps, by using the notion of the resolvent of the\nLaplacian operator. In addition, we provide a natural one-parameter family of\nregularizers, that can be easily tuned depending on the expected approximate\nisometry of the input shape pair. We show on a wide range of shape\ncorrespondence scenarios that our novel regularization leads to an improvement\nin the quality of the estimated functional, and ultimately pointwise\ncorrespondences before and after commonly-used refinement techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.14624v1"
    },
    {
        "title": "Perfectly normal type-2 fuzzy interpolation B-spline curve",
        "authors": [
            "Rozaimi Zakaria",
            "Abd. Fatah Wahab",
            "R. U. Gobithaasan"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  In this paper, we proposed another new form of type-2 fuzzy data\npoints(T2FDPs) that is perfectly normal type-2 data points(PNT2FDPs). These\nkinds of brand-new data were defined by using the existing type-2 fuzzy set\ntheory(T2FST) and type-2 fuzzy number(T2FN) concept since we dealt with the\nproblem of defining complex uncertainty data. Along with this restructuring, we\nincluded the fuzzification(alpha-cut operation), type-reduction and\ndefuzzification processes against PNT2FDPs. In addition, we used interpolation\nB-soline curve function to demonstrate the PNT2FDPs.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.0001v1"
    },
    {
        "title": "Parallel Chen-Han (PCH) Algorithm for Discrete Geodesics",
        "authors": [
            "Xiang Ying",
            "Shi-Qing Xin",
            "Ying He"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  In many graphics applications, the computation of exact geodesic distance is\nvery important. However, the high computational cost of the existing geodesic\nalgorithms means that they are not practical for large-scale models or\ntime-critical applications. To tackle this challenge, we propose the parallel\nChen-Han (or PCH) algorithm, which extends the classic Chen-Han (CH) discrete\ngeodesic algorithm to the parallel setting. The original CH algorithm and its\nvariant both lack a parallel solution because the windows (a key data structure\nthat carries the shortest distance in the wavefront propagation) are maintained\nin a strict order or a tightly coupled manner, which means that only one window\nis processed at a time. We propose dividing the CH's sequential algorithm into\nfour phases, window selection, window propagation, data organization, and\nevents processing so that there is no data dependence or conflicts in each\nphase and the operations within each phase can be carried out in parallel. The\nproposed PCH algorithm is able to propagate a large number of windows\nsimultaneously and independently. We also adopt a simple yet effective strategy\nto control the total number of windows. We implement the PCH algorithm on\nmodern GPUs (such as Nvidia GTX 580) and analyze the performance in detail. The\nperformance improvement (compared to the sequential algorithms) is highly\nconsistent with GPU double-precision performance (GFLOPS). Extensive\nexperiments on real-world models demonstrate an order of magnitude improvement\nin execution time compared to the state-of-the-art.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.1293v1"
    },
    {
        "title": "On the variety of planar spirals and their applications in computer\n  aided design",
        "authors": [
            "Rushan Ziatdinov",
            "Kenjiro T. Miura"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  In this paper we discuss the variety of planar spiral segments and their\napplications in objects in both the real and artificial world. The discussed\ncurves with monotonic curvature function are well-known in geometric modelling\nand computer aided geometric design as fair curves, and they are very\nsignificant in aesthetic shape modelling. Fair curve segments are used for\ntwo-point G1 and G2 Hermite interpolation, as well as for generating aesthetic\nsplines.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.1473v1"
    },
    {
        "title": "MC-curves and aesthetic measurements for pseudospiral curve segments",
        "authors": [
            "Rushan Ziatdinov",
            "Rifkat I. Nabiyev",
            "Kenjiro T. Miura"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  This article studies families of curves with monotonic curvature function\n(MC-curves) and their applications in geometric modelling and aesthetic design.\nAesthetic analysis and assessment of the structure and plastic qualities of\npseudospirals, which are curves with monotonic curvature function, are\nconducted for the first time in the field of geometric modelling from the\nposition of technical aesthetics laws. The example of car body surface\nmodelling with the use of aesthetics splines is given.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.1737v1"
    },
    {
        "title": "Parallel Coordinates Guided High Dimensional Transfer Function Design",
        "authors": [
            "Xin Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  High-dimensional transfer function design is widely used to provide\nappropriate data classification for direct volume rendering of various\ndatasets. However, its design is a complicated task. Parallel coordinate plot\n(PCP), as a powerful visualization tool, can efficiently display\nhigh-dimensional geometry and accurately analyze multivariate data. In this\npaper, we propose to combine parallel coordinates with dimensional reduction\nmethods to guide high-dimensional transfer function design. Our pipeline has\ntwo major advantages: (1) combine and display extracted high-dimensional\nfeatures in parameter space; and (2) select appropriate high-dimensional\nparameters, with the help of dimensional reduction methods, to obtain\nsophisticated data classification as transfer function for volume rendering. In\norder to efficiently design high-dimensional transfer functions, the\ncombination of both parallel coordinate components and dimension reduction\nresults is necessary to generate final visualization results. We demonstrate\nthe capability of our method for direct volume rendering using various CT and\nMRI datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.4583v2"
    },
    {
        "title": "A Survey on 3D CAD model quality assurance and testing tools",
        "authors": [
            "C. González-Lluch",
            "P. Company",
            "M. Contero",
            "J. D. Camba",
            "R. Plumed"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  A new taxonomy of issues related to CAD model quality is presented, which\ndistinguishes between explicit and procedural models. For each type of model,\nmorphologic, syntactic, and semantic errors are characterized. The taxonomy was\nvalidated successfully when used to classify quality testing tools, which are\naimed at detecting and repairing data errors that may affect the\nsimplification, interoperability, and reusability of CAD models. The study\nshows that low semantic level errors that hamper simplification are reasonably\ncovered in explicit representations, although many CAD quality testers are\nstill unaffordable for Small and Medium Enterprises, both in terms of cost and\ntraining time. Interoperability has been reasonably solved by standards like\nSTEP AP 203 and AP214, but model reusability is not feasible in explicit\nrepresentations. Procedural representations are promising, as interactive\nmodeling editors automatically prevent most morphologic errors derived from\nunsuitable modeling strategies. Interoperability problems between procedural\nrepresentations are expected to decrease dramatically with STEP AP242. Higher\nsemantic aspects of quality such as assurance of design intent, however, are\nhardly supported by current CAD quality testers.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.01765v1"
    },
    {
        "title": "Fractal Art Generation using GPUs",
        "authors": [
            "Will. D. Mayfield",
            "Justin. C. Eiland",
            "Taylor. J. Hutyra",
            "Matt. C. Paulsen",
            "Bryant. M. Wyatt"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Fractal image generation algorithms exhibit extreme parallelizability. Using\ngeneral purpose graphics processing unit (GPU) programming to implement\nescape-time algorithms for Julia sets of functions,parallel methods generate\nvisually attractive fractal images much faster than traditional methods. Vastly\nimproved speeds are achieved using this method of computation, which allow\nreal-time generation and display of images. A comparison is made between\nsequential and parallel implementations of the algorithm. An application\ncreated by the authors demonstrates using the increased speed to create dynamic\nimaging of fractals where the user may explore paths of parameter values\ncorresponding to a given function's Mandelbrot set. Examples are given of\nartistic and mathematical insights gained by experiencing fractals\ninteractively and from the ability to sample the parameter space quickly and\ncomprehensively.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.03079v1"
    },
    {
        "title": "Primal-Dual Optimization for Fluids",
        "authors": [
            "Tiffany Inglis",
            "Marie-Lena Eckert",
            "James Gregson",
            "Nils Thuerey"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We apply a novel optimization scheme from the image processing and machine\nlearning areas, a fast Primal-Dual method, to achieve controllable and\nrealistic fluid simulations. While our method is generally applicable to many\nproblems in fluid simulations, we focus on the two topics of fluid guiding and\nseparating solid-wall boundary conditions. Each problem is posed as an\noptimization problem and solved using our method, which contains acceleration\nschemes tailored to each problem. In fluid guiding, we are interested in\npartially guiding fluid motion to exert control while preserving fluid\ncharacteristics. With our method, we achieve explicit control over both\nlarge-scale motions and small-scale details which is valuable for many\napplications, such as level-of-detail adjustment (after running the coarse\nsimulation), spatially varying guiding strength, domain modification, and\nresimulation with different fluid parameters. For the separating solid-wall\nboundary conditions problem, our method effectively eliminates unrealistic\nartifacts of fluid crawling up solid walls and sticking to ceilings, requiring\nfew changes to existing implementations. We demonstrate the fast convergence of\nour Primal-Dual method with a variety of test cases for both model problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.03677v2"
    },
    {
        "title": "Navigable videos for presenting scientific data on head-mounted displays",
        "authors": [
            "Jacqueline Chu",
            "Leonardo Ferrer",
            "Min Shih",
            "Kwan-Liu Ma"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Immersive, stereoscopic viewing enables scientists to better analyze the\nspatial structures of visualized physical phenomena. However, their findings\ncannot be properly presented in traditional media, which lack these core\nattributes. Creating a presentation tool that captures this environment poses\nunique challenges, namely related to poor viewing accessibility. Immersive\nscientific renderings often require high-end equipment, which can be\nimpractical to obtain. We address these challenges with our authoring tool and\nnavigational interface, which is designed for affordable head-mounted displays.\nWith the authoring tool, scientists can show salient data features as connected\n360{\\deg} video paths, resulting in a \"choose-your-own-adventure\" experience.\nOur navigational interface features bidirectional video playback for added\nviewing control when users traverse the tailor-made content. We evaluate our\nsystem's benefits by authoring case studies on several data sets and conducting\na usability study on the navigational interface's design. In summary, our\napproach provides scientists an immersive medium to visually present their\nresearch to the intended audience--spanning from students to colleagues--on\naffordable virtual reality headsets.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.08947v1"
    },
    {
        "title": "A Qualitative and Quantitative Evaluation of 8 Clear Sky Models",
        "authors": [
            "Eric Bruneton"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We provide a qualitative and quantitative evaluation of 8 clear sky models\nused in Computer Graphics. We compare the models with each other as well as\nwith measurements and with a reference model from the physics community. After\na short summary of the physics of the problem, we present the measurements and\nthe reference model, and how we \"invert\" it to get the model parameters. We\nthen give an overview of each CG model, and detail its scope, its algorithmic\ncomplexity, and its results using the same parameters as in the reference\nmodel. We also compare the models with a perceptual study. Our quantitative\nresults confirm that the less simplifications and approximations are used to\nsolve the physical equations, the more accurate are the results. We conclude\nwith a discussion of the advantages and drawbacks of each model, and how to\nfurther improve their accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.04336v1"
    },
    {
        "title": "Orthogonal Edge Routing for the EditLens",
        "authors": [
            "Stefan Gladisch",
            "Valerius Weigandt",
            "Heidrun Schumann",
            "Christian Tominski"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  The EditLens is an interactive lens technique that supports the editing of\ngraphs. The user can insert, update, or delete nodes and edges while\nmaintaining an already existing layout of the graph. For the nodes and edges\nthat are affected by an edit operation, the EditLens suggests suitable\nlocations and routes, which the user can accept or adjust. For this purpose,\nthe EditLens requires an efficient routing algorithm that can compute results\nat interactive framerates. Existing algorithms cannot fully satisfy the needs\nof the EditLens. This paper describes a novel algorithm that can compute\northogonal edge routes for incremental edit operations of graphs. Tests\nindicate that, in general, the algorithm is better than alternative solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.05064v1"
    },
    {
        "title": "Charted Metropolis Light Transport",
        "authors": [
            "Jacopo Pantaleoni"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  In this manuscript, inspired by a simpler reformulation of primary sample\nspace Metropolis light transport, we derive a novel family of general Markov\nchain Monte Carlo algorithms called charted Metropolis-Hastings, that\nintroduces the notion of sampling charts to extend a given sampling domain and\nmaking it easier to sample the desired target distribution and escape from\nlocal maxima through coordinate changes. We further apply the novel algorithms\nto light transport simulation, obtaining a new type of algorithm called charted\nMetropolis light transport, that can be seen as a bridge between primary sample\nspace and path space Metropolis light transport. The new algorithms require to\nprovide only right inverses of the sampling functions, a property that we\nbelieve crucial to make them practical in the context of light transport\nsimulation. We further propose a method to integrate density estimation into\nthis framework through a novel scheme that uses it as an independence sampler.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.05395v7"
    },
    {
        "title": "Data-driven Shoulder Inverse Kinematics",
        "authors": [
            "YoungBeom Kim",
            "Byung-Ha Park",
            "Kwang-Mo Jung",
            "JungHyun Han"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  This paper proposes a shoulder inverse kinematics (IK) technique. Shoulder\ncomplex is comprised of the sternum, clavicle, ribs, scapula, humerus, and four\njoints. The shoulder complex shows specific motion pattern, such as Scapulo\nhumeral rhythm. As a result, if a motion of the shoulder isgenerated without\nthe knowledge of kinesiology, it will be seen as un-natural. The proposed\ntechnique generates motion of the shoulder complex about the orientation of the\nupper arm by interpolating the measurement data. The shoulder IK method allows\nnovice animators to generate natural shoulder motions easily. As a result, this\ntechnique improves the quality of character animation.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07353v1"
    },
    {
        "title": "Quantum Optimal Transport for Tensor Field Processing",
        "authors": [
            "Gabriel Peyré",
            "Lenaïc Chizat",
            "François-Xavier Vialard",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  This article introduces a new notion of optimal transport (OT) between tensor\nfields, which are measures whose values are positive semidefinite (PSD)\nmatrices. This \"quantum\" formulation of OT (Q-OT) corresponds to a relaxed\nversion of the classical Kantorovich transport problem, where the fidelity\nbetween the input PSD-valued measures is captured using the geometry of the\nVon-Neumann quantum entropy. We propose a quantum-entropic regularization of\nthe resulting convex optimization problem, which can be solved efficiently\nusing an iterative scaling algorithm. This method is a generalization of the\ncelebrated Sinkhorn algorithm to the quantum setting of PSD matrices. We extend\nthis formulation and the quantum Sinkhorn algorithm to compute barycenters\nwithin a collection of input tensor fields. We illustrate the usefulness of the\nproposed approach on applications to procedural noise generation, anisotropic\nmeshing, diffusion tensor imaging and spectral texture synthesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08731v4"
    },
    {
        "title": "Path Throughput Importance Weights",
        "authors": [
            "Johannes Jendersie"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Many Monte Carlo light transport simulations use multiple importance sampling\n(MIS) to weight between different path sampling strategies. We propose to use\nthe path throughput to compute the MIS weights instead of the commonly used\nprobability density per area measure. This new formulation is equivalent to the\nprevious approach and results in the same weights as well as implementation.\nHowever, it is more intuitive and can help in understanding the effects of\nmodifications to the weight function. We show some examples of required\nmodifications which are often neglected in implementations. Also, our new\nperspective might help to derive MIS strategies for new samplers in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01005v2"
    },
    {
        "title": "Latent Space Representation for Shape Analysis and Learning",
        "authors": [
            "Ruqi Huang",
            "Panos Achlioptas",
            "Leonidas Guibas",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We propose a novel shape representation useful for analyzing and processing\nshape collections, as well for a variety of learning and inference tasks.\nUnlike most approaches that capture variability in a collection by using a\ntemplate model or a base shape, we show that it is possible to construct a full\nshape representation by using the latent space induced by a functional map net-\nwork, allowing us to represent shapes in the context of a collection without\nthe bias induced by selecting a template shape. Key to our construction is a\nnovel analysis of latent functional spaces, which shows that after proper\nregularization they can be endowed with a natural geometric structure, giving\nrise to a well-defined, stable and fully informative shape representation. We\ndemonstrate the utility of our representation in shape analysis tasks, such as\nhighlighting the most distorted shape parts in a collection or separating\nvariability modes between shape classes. We further exploit our representation\nin learning applications by showing how it can naturally be used within deep\nlearning and convolutional neural networks for shape classi cation or\nreconstruction, signi cantly outperforming existing point-based techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.03967v2"
    },
    {
        "title": "Continuous and Orientation-preserving Correspondences via Functional\n  Maps",
        "authors": [
            "Jing Ren",
            "Adrien Poulenard",
            "Peter Wonka",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We propose a method for efficiently computing orientation-preserving and\napproximately continuous correspondences between non-rigid shapes, using the\nfunctional maps framework. We first show how orientation preservation can be\nformulated directly in the functional (spectral) domain without using landmark\nor region correspondences and without relying on external symmetry information.\nThis allows us to obtain functional maps that promote orientation preservation,\neven when using descriptors, that are invariant to orientation changes. We then\nshow how higher quality, approximately continuous and bijective pointwise\ncorrespondences can be obtained from initial functional maps by introducing a\nnovel refinement technique that aims to simultaneously improve the maps both in\nthe spectral and spatial domains. This leads to a general pipeline for\ncomputing correspondences between shapes that results in high-quality maps,\nwhile admitting an efficient optimization scheme. We show through extensive\nevaluation that our approach improves upon state-of-the-art results on\nchallenging isometric and non-isometric correspondence benchmarks according to\nboth measures of continuity and coverage as well as producing semantically\nmeaningful correspondences as measured by the distance to ground truth maps.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04455v3"
    },
    {
        "title": "Movie Editing and Cognitive Event Segmentation in Virtual Reality Video",
        "authors": [
            "Ana Serrano",
            "Vincent Sitzmann",
            "Jaime Ruiz-Borau",
            "Gordon Wetzstein",
            "Diego Gutierrez",
            "Belen Masia"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Traditional cinematography has relied for over a century on a\nwell-established set of editing rules, called continuity editing, to create a\nsense of situational continuity. Despite massive changes in visual content\nacross cuts, viewers in general experience no trouble perceiving the\ndiscontinuous flow of information as a coherent set of events. However, Virtual\nReality (VR) movies are intrinsically different from traditional movies in that\nthe viewer controls the camera orientation at all times. As a consequence,\ncommon editing techniques that rely on camera orientations, zooms, etc., cannot\nbe used. In this paper we investigate key relevant questions to understand how\nwell traditional movie editing carries over to VR. To do so, we rely on recent\ncognition studies and the event segmentation theory, which states that our\nbrains segment continuous actions into a series of discrete, meaningful events.\nWe first replicate one of these studies to assess whether the predictions of\nsuch theory can be applied to VR. We next gather gaze data from viewers\nwatching VR videos containing different edits with varying parameters, and\nprovide the first systematic analysis of viewers' behavior and the perception\nof continuity in VR. From this analysis we make a series of relevant findings;\nfor instance, our data suggests that predictions from the cognitive event\nsegmentation theory are useful guides for VR editing; that different types of\nedits are equally well understood in terms of continuity; and that spatial\nmisalignments between regions of interest at the edit boundaries favor a more\nexploratory behavior even after viewers have fixated on a new region of\ninterest. In addition, we propose a number of metrics to describe viewers'\nattentional behavior in VR. We believe the insights derived from our work can\nbe useful as guidelines for VR content creation.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04924v1"
    },
    {
        "title": "An intuitive control space for material appearance",
        "authors": [
            "Ana Serrano",
            "Diego Gutierrez",
            "Karol Myszkowski",
            "Hans-Peter Seidel",
            "Belen Masia"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Many different techniques for measuring material appearance have been\nproposed in the last few years. These have produced large public datasets,\nwhich have been used for accurate, data-driven appearance modeling. However,\nalthough these datasets have allowed us to reach an unprecedented level of\nrealism in visual appearance, editing the captured data remains a challenge. In\nthis paper, we present an intuitive control space for predictable editing of\ncaptured BRDF data, which allows for artistic creation of plausible novel\nmaterial appearances, bypassing the difficulty of acquiring novel samples. We\nfirst synthesize novel materials, extending the existing MERL dataset up to 400\nmathematically valid BRDFs. We then design a large-scale experiment, gathering\n56,000 subjective ratings on the high-level perceptual attributes that best\ndescribe our extended dataset of materials. Using these ratings, we build and\ntrain networks of radial basis functions to act as functionals mapping the\nperceptual attributes to an underlying PCA-based representation of BRDFs. We\nshow that our functionals are excellent predictors of the perceived attributes\nof appearance. Our control space enables many applications, including intuitive\nmaterial editing of a wide range of visual properties, guidance for gamut\nmapping, analysis of the correlation between perceptual attributes, or novel\nappearance similarity metrics. Moreover, our methodology can be used to derive\nfunctionals applicable to classic analytic BRDF representations. We release our\ncode and dataset publicly, in order to support and encourage further research\nin this direction.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04950v1"
    },
    {
        "title": "Perceptual Rasterization for Head-mounted Display Image Synthesis",
        "authors": [
            "Tobias Ritschel",
            "Sebastian Friston",
            "Anthony Steed"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We suggest a rasterization pipeline tailored towards the need of head-mounted\ndisplays (HMD), where latency and field-of-view requirements pose new\nchallenges beyond those of traditional desktop displays. Instead of rendering\nand warping for low latency, or using multiple passes for foveation, we show\nhow both can be produced directly in a single perceptual rasterization pass. We\ndo this with per-fragment ray-casting. This is enabled by derivations of tight\nspace-time-fovea pixel bounds, introducing just enough flexibility for\nrequisite geometric tests, but retaining most of the the simplicity and\nefficiency of the traditional rasterizaton pipeline. To produce foveated\nimages, we rasterize to an image with spatially varying pixel density. To\nreduce latency, we extend the image formation model to directly produce\n\"rolling\" images where the time at each pixel depends on its display location.\nOur approach overcomes limitations of warping with respect to disocclusions,\nobject motion and view-dependent shading, as well as geometric aliasing\nartifacts in other foveated rendering techniques. A set of perceptual user\nstudies demonstrates the efficacy of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.05385v1"
    },
    {
        "title": "TTHRESH: Tensor Compression for Multidimensional Visual Data",
        "authors": [
            "Rafael Ballester-Ripoll",
            "Peter Lindstrom",
            "Renato Pajarola"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Memory and network bandwidth are decisive bottlenecks when handling\nhigh-resolution multidimensional data sets in visualization applications, and\nthey increasingly demand suitable data compression strategies. We introduce a\nnovel lossy compression algorithm for multidimensional data over regular grids.\nIt leverages the higher-order singular value decomposition (HOSVD), a\ngeneralization of the SVD to three dimensions and higher, together with\nbit-plane, run-length and arithmetic coding to compress the HOSVD transform\ncoefficients. Our scheme degrades the data particularly smoothly and achieves\nlower mean squared error than other state-of-the-art algorithms at\nlow-to-medium bit rates, as it is required in data archiving and management for\nvisualization purposes. Further advantages of the proposed algorithm include\nvery fine bit rate selection granularity and the ability to manipulate data at\nvery small cost in the compression domain, for example to reconstruct filtered\nand/or subsampled versions of all (or selected parts) of the data set.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.05952v2"
    },
    {
        "title": "Mumford-Shah Mesh Processing using the Ambrosio-Tortorelli Functional",
        "authors": [
            "Nicolas Bonneel",
            "David Coeurjolly",
            "Pierre Gueth",
            "Jacques-Olivier Lachaud"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The Mumford-Shah functional approximates a function by a piecewise smooth\nfunction. Its versatility makes it ideal for tasks such as image segmentation\nor restoration, and it is now a widespread tool of image processing. Recent\nwork has started to investigate its use for mesh segmentation and feature lines\ndetection, but we take the stance that the power of this functional could reach\nfar beyond these tasks and integrate the everyday mesh processing toolbox. In\nthis paper, we discretize an Ambrosio-Tortorelli approximation via a Discrete\nExterior Calculus formulation. We show that, combined with a new shape\noptimization routine, several mesh processing problems can be readily tackled\nwithin the same framework. In particular, we illustrate applications in mesh\ndenoising, normal map embossing, mesh inpainting and mesh segmentation.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.05999v2"
    },
    {
        "title": "HexaLab.net: an online viewer for hexahedral meshes",
        "authors": [
            "Matteo Bracci",
            "Marco Tarini",
            "Nico Pietroni",
            "Marco Livesu",
            "Paolo Cignoni"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We introduce HexaLab: a WebGL application for real time visualization,\nexploration and assessment of hexahedral meshes. HexaLab can be used by simply\nopening www.hexalab.net. Our visualization tool targets both users and\nscholars. Practitioners who employ hexmeshes for Finite Element Analysis, can\nreadily check mesh quality and assess its usability for simulation. Researchers\ninvolved in mesh generation may use HexaLab to perform a detailed analysis of\nthe mesh structure, isolating weak points and testing new solutions to improve\non the state of the art and generate high quality images. To this end, we\nsupport a wide variety of visualization and volume inspection tools. Our system\noffers also immediate access to a repository containing all the publicly\navailable meshes produced with the most recent techniques for hexmesh\ngeneration. We believe HexaLab, providing a common tool for visualizing,\nassessing and distributing results, will push forward the recent strive for\nreplicability in our scientific community.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.06639v2"
    },
    {
        "title": "End-to-end Sampling Patterns",
        "authors": [
            "Thomas Leimkühler",
            "Gurprit Singh",
            "Karol Myszkowski",
            "Hans-Peter Seidel",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Sample patterns have many uses in Computer Graphics, ranging from procedural\nobject placement over Monte Carlo image synthesis to non-photorealistic\ndepiction. Their properties such as discrepancy, spectra, anisotropy, or\nprogressiveness have been analyzed extensively. However, designing methods to\nproduce sampling patterns with certain properties can require substantial\nhand-crafting effort, both in coding, mathematical derivation and compute time.\nIn particular, there is no systematic way to derive the best sampling algorithm\nfor a specific end-task.\n  Tackling this issue, we suggest another level of abstraction: a toolkit to\nend-to-end optimize over all sampling methods to find the one producing\nuser-prescribed properties such as discrepancy or a spectrum that best fit the\nend-task. A user simply implements the forward losses and the sampling method\nis found automatically -- without coding or mathematical derivation -- by\nmaking use of back-propagation abilities of modern deep learning frameworks.\nWhile this optimization takes long, at deployment time the sampling method is\nquick to execute as iterated unstructured non-linear filtering using radial\nbasis functions (RBFs) to represent high-dimensional kernels. Several important\nprevious methods are special cases of this approach, which we compare to\nprevious work and demonstrate its usefulness in several typical Computer\nGraphics applications. Finally, we propose sampling patterns with properties\nnot shown before, such as high-dimensional blue noise with projective\nproperties.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.06710v1"
    },
    {
        "title": "FrankenGAN: Guided Detail Synthesis for Building Mass-Models Using\n  Style-Synchonized GANs",
        "authors": [
            "Tom Kelly",
            "Paul Guerrero",
            "Anthony Steed",
            "Peter Wonka",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Coarse building mass models are now routinely generated at scales ranging\nfrom individual buildings through to whole cities. For example, they can be\nabstracted from raw measurements, generated procedurally, or created manually.\nHowever, these models typically lack any meaningful semantic or texture\ndetails, making them unsuitable for direct display. We introduce the problem of\nautomatically and realistically decorating such models by adding semantically\nconsistent geometric details and textures. Building on the recent success of\ngenerative adversarial networks (GANs), we propose FrankenGAN, a cascade of\nGANs to create plausible details across multiple scales over large\nneighborhoods. The various GANs are synchronized to produce consistent style\ndistributions over buildings and neighborhoods. We provide the user with direct\ncontrol over the variability of the output. We allow her to interactively\nspecify style via images and manipulate style-adapted sliders to control style\nvariability. We demonstrate our system on several large-scale examples. The\ngenerated outputs are qualitatively evaluated via a set of user studies and are\nfound to be realistic, semantically-plausible, and style-consistent.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07179v2"
    },
    {
        "title": "HairNet: Single-View Hair Reconstruction using Convolutional Neural\n  Networks",
        "authors": [
            "Yi Zhou",
            "Liwen Hu",
            "Jun Xing",
            "Weikai Chen",
            "Han-Wei Kung",
            "Xin Tong",
            "Hao Li"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We introduce a deep learning-based method to generate full 3D hair geometry\nfrom an unconstrained image. Our method can recover local strand details and\nhas real-time performance. State-of-the-art hair modeling techniques rely on\nlarge hairstyle collections for nearest neighbor retrieval and then perform\nad-hoc refinement. Our deep learning approach, in contrast, is highly efficient\nin storage and can run 1000 times faster while generating hair with 30K\nstrands. The convolutional neural network takes the 2D orientation field of a\nhair image as input and generates strand features that are evenly distributed\non the parameterized 2D scalp. We introduce a collision loss to synthesize more\nplausible hairstyles, and the visibility of each strand is also used as a\nweight term to improve the reconstruction accuracy. The encoder-decoder\narchitecture of our network naturally provides a compact and continuous\nrepresentation for hairstyles, which allows us to interpolate naturally between\nhairstyles. We use a large set of rendered synthetic hair models to train our\nnetwork. Our method scales to real images because an intermediate 2D\norientation field, automatically calculated from the real image, factors out\nthe difference between synthetic and real hairs. We demonstrate the\neffectiveness and robustness of our method on a wide range of challenging real\nInternet pictures and show reconstructed hair sequences from videos.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07467v2"
    },
    {
        "title": "Void Space Surfaces to Convey Depth in Vessel Visualizations",
        "authors": [
            "Julian Kreiser",
            "Pedro Hermosilla",
            "Timo Ropinski"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  To enhance depth perception and thus data comprehension, additional depth\ncues are often used in 3D visualizations of complex vascular structures.\nAccordingly, there is a variety of different approaches described in the\nliterature, ranging from chromadepth color coding over depth of field to\nglyph-based encodings. Unfortunately, the majority of existing approaches\nsuffers from the same problem. As these cues are directly applied to the\ngeometry's surface, the display of additional information, such as other\nmodalities or derived attributes, associated with a vessel is impaired. To\novercome this limitation we propose Void Space Surfaces which utilize the empty\nspace in between vessel branches to communicate depth and their relative\npositioning. This allows us to enhance the depth perception of vascular\nstructures without interfering with the spatial data and potentially\nsuperimposed parameter information. Within this paper we introduce Void Space\nSurfaces, describe their technical realization, and show their application to\nvarious vessel trees. Moreover, we report the outcome of a user study which we\nhave conducted in order to evaluate the perceptual impact of Void Space\nSurfaces as compared to existing vessel visualization techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07729v1"
    },
    {
        "title": "Combining Recurrent Neural Networks and Adversarial Training for Human\n  Motion Synthesis and Control",
        "authors": [
            "Zhiyong Wang",
            "Jinxiang Chai",
            "Shihong Xia"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This paper introduces a new generative deep learning network for human motion\nsynthesis and control. Our key idea is to combine recurrent neural networks\n(RNNs) and adversarial training for human motion modeling. We first describe an\nefficient method for training a RNNs model from prerecorded motion data. We\nimplement recurrent neural networks with long short-term memory (LSTM) cells\nbecause they are capable of handling nonlinear dynamics and long term temporal\ndependencies present in human motions. Next, we train a refiner network using\nan adversarial loss, similar to Generative Adversarial Networks (GANs), such\nthat the refined motion sequences are indistinguishable from real motion\ncapture data using a discriminative network. We embed contact information into\nthe generative deep learning model to further improve the performance of our\ngenerative model. The resulting model is appealing to motion synthesis and\ncontrol because it is compact, contact-aware, and can generate an infinite\nnumber of naturally looking motions with infinite lengths. Our experiments show\nthat motions generated by our deep learning model are always highly realistic\nand comparable to high-quality motion capture data. We demonstrate the power\nand effectiveness of our models by exploring a variety of applications, ranging\nfrom random motion synthesis, online/offline motion control, and motion\nfiltering. We show the superiority of our generative model by comparison\nagainst baseline models.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.08666v1"
    },
    {
        "title": "Golden interpolation",
        "authors": [
            "Ying He",
            "Jincai Chang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  For the classic aesthetic interpolation problem, we propose an entirely new\nthought: apply the golden section. For how to apply the golden section to\ninterpolation methods, we present three examples: the golden step\ninterpolation, the golden piecewise linear interpolation and the golden curve\ninterpolation, which respectively deal with the applications of golden section\nin the interpolation of degree 0, 1, and 2 in the plane. In each example, we\npresent our basic ideas, the specific methods, comparative examples and\napplications, and relevant criteria. And it is worth mentioning that for\naesthetics, we propose two novel concepts: the golden cuspidal hill and the\ngolden domed hill. This paper aims to provide the reference for the combination\nof golden section and interpolation, and stimulate more and better related\nresearches.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.09058v2"
    },
    {
        "title": "Learning a Shared Shape Space for Multimodal Garment Design",
        "authors": [
            "Tuanfeng Y. Wang",
            "Duygu Ceylan",
            "Jovan Popovic",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Designing real and virtual garments is becoming extremely demanding with\nrapidly changing fashion trends and increasing need for synthesizing realistic\ndressed digital humans for various applications. This necessitates creating\nsimple and effective workflows to facilitate authoring sewing patterns\ncustomized to garment and target body shapes to achieve desired looks.\nTraditional workflow involves a trial-and-error procedure wherein a mannequin\nis draped to judge the resultant folds and the sewing pattern iteratively\nadjusted until the desired look is achieved. This requires time and experience.\nInstead, we present a data-driven approach wherein the user directly indicates\ndesired fold patterns simply by sketching while our system estimates\ncorresponding garment and body shape parameters at interactive rates. The\nrecovered parameters can then be further edited and the updated draped garment\npreviewed. Technically, we achieve this via a novel shared shape space that\nallows the user to seamlessly specify desired characteristics across multimodal\ninput {\\em without} requiring to run garment simulation at design time. We\nevaluate our approach qualitatively via a user study and quantitatively against\ntest datasets, and demonstrate how our system can generate a rich quality of\non-body garments targeted for a range of body shapes while achieving desired\nfold characteristics.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.11335v2"
    },
    {
        "title": "Volumetric Spline Parameterization for Isogeometric Analysis",
        "authors": [
            "Maodong Pan",
            "Falai Chen",
            "Weihua Tong"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Given the spline representation of the boundary of a three dimensional\ndomain, constructing a volumetric spline parameterization of the domain (i.e.,\na map from a unit cube to the domain) with the given boundary is a fundamental\nproblem in isogeometric analysis. A good domain parameterization should satisfy\nthe following criteria: (1) the parameterization is a bijective map; and (2)\nthe map has lowest possible distortion. However, none of the state-of-the-art\nvolumetric parameterization methods has fully addressed the above issues. In\nthis paper, we propose a three-stage approach for constructing volumetric\nparameterization satisfying the above criteria. Firstly, a harmonic map is\ncomputed between a unit cube and the computational domain. Then a bijective map\nmodeled by a max-min optimization problem is computed in a coarse-to-fine way,\nand an algorithm based on divide and conquer strategy is proposed to solve the\noptimization problem efficiently. Finally, to ensure high quality of the\nparameterization, the MIPS (Most Isometric Parameterizations) method is adopted\nto reduce the conformal distortion of the bijective map. We provide several\nexamples to demonstrate the feasibility of our approach and to compare our\napproach with some state-of-the-art methods. The results show that our\nalgorithm produces bijective parameterization with high quality even for\ncomplex domains.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.00650v1"
    },
    {
        "title": "A Robust Volume Conserving Method for Character-Water Interaction",
        "authors": [
            "Minjae Lee",
            "David Hyde",
            "Kevin Li",
            "Ronald Fedkiw"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a novel volume conserving framework for character-water\ninteraction, using a novel volume-of-fluid solver on a skinned tetrahedral\nmesh, enabling the high degree of the spatial adaptivity in order to capture\nthin films and hair-water interactions. For efficiency, the bulk of the fluid\nvolume is simulated with a standard Eulerian solver which is two way coupled to\nour skinned arbitrary Lagrangian-Eulerian mesh using a fast, robust, and\nstraightforward to implement partitioned approach. This allows for a\nspecialized and efficient treatment of the volume-of-fluid solver, since it is\nonly required in a subset of the domain. The combination of conservation of\nfluid volume and a kinematically deforming skinned mesh allows us to robustly\nimplement interesting effects such as adhesion, and anisotropic porosity. We\nillustrate the efficacy of our method by simulating various water effects with\nsolid objects and animated characters.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.00801v1"
    },
    {
        "title": "Automated pebble mosaic stylization of images",
        "authors": [
            "Lars Doyle",
            "Forest Anderson",
            "Ehren Choy",
            "David Mould"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Digital mosaics have usually used regular tiles, simulating the historical\n\"tessellated\" mosaics. In this paper, we present a method for synthesizing\npebble mosaics, a historical mosaic style in which the tiles are rounded\npebbles. We address both the tiling problem, where pebbles are distributed over\nthe image plane so as to approximate the input image content, and the problem\nof geometry, creating a smooth rounded shape for each pebble. We adapt SLIC,\nsimple linear iterative clustering, to obtain elongated tiles conforming to\nimage content, and smooth the resulting irregular shapes into shapes resembling\npebble cross-sections. Then, we create an interior and exterior contour for\neach pebble and solve a Laplace equation over the region between them to obtain\nheight-field geometry. The resulting pebble set approximates the input image\nwhile presenting full geometry that can be rendered and textured for a highly\ndetailed representation of a pebble mosaic.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.02806v1"
    },
    {
        "title": "X3D in Urban Planning - Savannah in 3D",
        "authors": [
            " Faith-Anne",
            "L. Kocadag",
            "Felix G. Hamza-Lup"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Urban planning often raises complex issues that are difficult to visualize\nand challenging to communicate. The increasing availability of 3D modeling\nstandards has provided the opportunity for many developers, engineers,\ndesigners, planners, investors, and government officials to effectively\ncollaborate to bring projects to fruition. Because of its real-time\ninteractivity and widespread web-based content players, X3D proves to be a good\nchoice for developing and visualizing 3D city content on the Web for planning\npurposes.\n  Passenger rail is a viable and cost-effective transportation solution in many\nareas, especially in view of rising energy costs. The Savannah in 3D (or S3D)\nproject is a multimedia tool for a feasibility study designed to bring\npassenger rail to Savannah; thereby opening up the historic, tourist-friendly\ncity to a wider audience. The paper outlines the development process of an\ninteractive 3D train model as it journeys from Atlanta to Savannah, Georgia -\nfocusing on user interactivity and scene immersion to supplement the city and\ntransportation planning agenda.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.02906v1"
    },
    {
        "title": "Massively Parallel Path Space Filtering",
        "authors": [
            "Nikolaus Binder",
            "Sascha Fricke",
            "Alexander Keller"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Restricting path tracing to a small number of paths per pixel for performance\nreasons rarely achieves a satisfactory image quality for scenes of interest.\nHowever, path space filtering may dramatically improve the visual quality by\nsharing information across vertices of paths classified as proximate. Unlike\nscreen space-based approaches, these paths neither need to be present on the\nscreen, nor is filtering restricted to the first intersection with the scene.\nWhile searching proximate vertices had been more expensive than filtering in\nscreen space, we greatly improve over this performance penalty by storing,\nupdating, and looking up the required information in a hash table. The keys are\nconstructed from jittered and quantized information, such that only a single\nquery very likely replaces costly neighborhood searches. A massively parallel\nimplementation of the algorithm is demonstrated on a graphics processing unit\n(GPU).\n",
        "pdf_link": "http://arxiv.org/pdf/1902.05942v2"
    },
    {
        "title": "A Comprehensive Theory and Variational Framework for Anti-aliasing\n  Sampling Patterns",
        "authors": [
            "A. Cengiz Öztireli"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper, we provide a comprehensive theory of anti-aliasing sampling\npatterns that explains and revises known results, and show how patterns as\npredicted by the theory can be generated via a variational optimization\nframework. We start by deriving the exact spectral expression for expected\nerror in reconstructing an image in terms of power spectra of sampling\npatterns, and analyzing how the shape of power spectra is related to\nanti-aliasing properties. Based on this analysis, we then formulate the problem\nof generating anti-aliasing sampling patterns as constrained variational\noptimization on power spectra. This allows us to not rely on any parametric\nform, and thus explore the whole space of realizable spectra. We show that the\nresulting optimized sampling patterns lead to reconstructions with less visible\naliasing artifacts, while keeping low frequencies as clean as possible.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.08228v1"
    },
    {
        "title": "HMLFC: Hierarchical Motion-Compensated Light Field Compression for\n  Interactive Rendering",
        "authors": [
            "Srihar Pratapa",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a new motion-compensated hierarchical compression scheme (HMLFC)\nfor encoding light field images (LFI) that is suitable for interactive\nrendering. Our method combines two different approaches, motion compensation\nschemes and hierarchical compression methods, to exploit redundancies in LFI.\nThe motion compensation schemes capture the redundancies in local regions of\nthe LFI efficiently (local coherence) and hierarchical schemes capture the\nredundancies present across the entire LFI (global coherence). Our hybrid\napproach combines the two schemes effectively capturing both local as well as\nglobal coherence to improve the overall compression rate. We compute a tree\nfrom LFI using a hierarchical scheme and use phase shifted motion compensation\ntechniques at each level of the hierarchy. Our representation provides random\naccess to the pixel values of the light field, which makes it suitable for\ninteractive rendering applications using a small run-time memory footprint. Our\napproach is GPU friendly and allows parallel decoding of LF pixel values. We\nhighlight the performance on the two-plane parameterized light fields and\nobtain a compression ratio of 30-800X with a PSNR of 40-45 dB. Overall, we\nobserve a 2-5X improvement in compression rates using HMLFC over prior light\nfield compression schemes that provide random access capability. In practice,\nour algorithm can render new views of resolution 512X512 on an NVIDIA GTX-980\nat ~200 fps.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.09396v2"
    },
    {
        "title": "Chromatic Adaptation Transform by Spectral Reconstruction (Preprint)",
        "authors": [
            "Scott A Burns"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  A color appearance model (CAM) is an advanced colorimetric tool used to\npredict color appearance under a wide variety of viewing conditions. A\nchromatic adaptation transform (CAT) is an integral part of a CAM. Its role is\nto predict \"corresponding colors,\" that is, a pair of colors that have the same\ncolor appearance when viewed under different illuminants, after partial or full\nadaptation to each illuminant. Modern CATs perform well when applied to a\nlimited range of illuminant pairs and a limited range of source (test) colors.\nHowever, they can fail if operated outside these ranges. For imaging\napplications, it is important to have a CAT that can operate on any real color\nand illuminant pair without failure. This paper proposes a new CAT that does\nnot operate on the standard von Kries model of adaptation. Instead it relies on\nspectral reconstruction and how these reconstructions behave with respect to\ndifferent illuminants. It is demonstrated that the proposed CAT is immune to\nsome of the limitations of existing CATs (such as producing colors with\nnegative tristimulus values). The proposed CAT does not use established\nempirical corresponding-color datasets to optimize performance, as most modern\nCATs do, yet it performs as well as or better than the most recent CATs when\ntested against the corresponding-color datasets. This increase in robustness\ncomes at the expense of additional complexity and computational effort. If\nrobustness is of prime importance, then the proposed method may be justifiable.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10160v8"
    },
    {
        "title": "What Do People See in a Twenty-Second Glimpse of Bivariate Vector Field\n  Visualizations?",
        "authors": [
            "Henan Zhao",
            "Garnett W. Bryant",
            "Wesley Griffin",
            "Judith E. Terrill",
            "Jian Chen"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Little is known about how people learn from a brief glimpse of\nthree-dimensional (3D) bivariate vector field visualizations and about how well\nvisual features can guide behavior. Here we report empirical study results on\nthe use of color, texture, and length to guide viewing of bivariate glyphs:\nthese three visual features are mapped to the first integer variable (v1) and\nlength to the second quantitative variable (v2). Participants performed two\ntasks within 20 seconds: (1) MAX: find the largest v2 when v1 is fixed; (2)\nSEARCH: find a specific bivariate variable shown on the screen in a vector\nfield. Our first study with eighteen participants performing these tasks showed\nthat the randomized vector positions, although they lessened viewers' ability\nto group vectors, did not reduce task accuracy compared to structured vector\nfields. This result may support that these color, texture, and length can\nprovide to a certain degree, guide viewers' attention to task-relevant regions.\nThe second study measured eye movement to quantify viewers' behaviors with\nthree-errors (scanning, recognition, and decision errors) and one-behavior\n(refixation) metrics. Our results showed two dominant search strategies:\ndrilling and scanning. Coloring tended to restrict eye movement to the\ntask-relevant regions of interest, enabling drilling. Length tended to support\nscanners who quickly wandered around at different v1 levels. Drillers had\nsignificantly less errors than scanners and the error rates for color and\ntexture were also lowest. And length had limited discrimination power than\ncolor and texture as a 3D visual guidance. Our experiment results may suggest\nthat using categorical visual feature could help obtain the global structure of\na vector field visualization. We provide the first benchmark of the attention\ncost of seeing a bivariate vector on average about 5 items per second.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.02366v3"
    },
    {
        "title": "Picturing Bivariate Separable-Features for Univariate Vector Magnitudes\n  in Large-Magnitude-Range Quantum Physics Data",
        "authors": [
            "Henan Zhao",
            "Jian Chen"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present study results from two experiments to empirically validate that\nseparable bivariate pairs for univariate representations of\nlarge-magnitude-range vectors are more efficient than integral pairs. The first\nexperiment with 20 participants compared: one integral pair, three separable\npairs, and one redundant pair, which is a mix of the integral and separable\nfeatures. Participants performed three local tasks requiring reading numerical\nvalues, estimating ratio, and comparing two points. The second 18-participant\nstudy compared three separable pairs using three global tasks when participants\nmust look at the entire field to get an answer: find a specific target in 20\nseconds, find the maximum magnitude in 20 seconds, and estimate the total\nnumber of vector exponents within 2 seconds. Our results also reveal the\nfollowing: separable pairs led to the most accurate answers and the shortest\ntask execution time, while integral dimensions were among the least accurate;\nit achieved high performance only when a pop-out separable feature (here color)\nwas added. To reconcile this finding with the existing literature, our second\nexperiment suggests that the higher the separability, the higher the accuracy;\nthe reason is probably that the emergent global scene created by the separable\npairs reduces the subsequent search space.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.02586v1"
    },
    {
        "title": "Spectral Coarsening of Geometric Operators",
        "authors": [
            "Hsueh-Ti Derek Liu",
            "Alec Jacobson",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We introduce a novel approach to measure the behavior of a geometric operator\nbefore and after coarsening. By comparing eigenvectors of the input operator\nand its coarsened counterpart, we can quantitatively and visually analyze how\nwell the spectral properties of the operator are maintained. Using this\nmeasure, we show that standard mesh simplification and algebraic coarsening\ntechniques fail to maintain spectral properties. In response, we introduce a\nnovel approach for spectral coarsening. We show that it is possible to\nsignificantly reduce the sampling density of an operator derived from a 3D\nshape without affecting the low-frequency eigenvectors. By marrying techniques\ndeveloped within the algebraic multigrid and the functional maps literatures,\nwe successfully coarsen a variety of isotropic and anisotropic operators while\nmaintaining sparsity and positive semi-definiteness. We demonstrate the utility\nof this approach for applications including operator-sensitive sampling, shape\nmatching, and graph pooling for convolutional neural networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.05161v1"
    },
    {
        "title": "Statistical Analysis and Modeling of the Geometry and Topology of Plant\n  Roots",
        "authors": [
            "Guan Wang",
            "Hamid Laga",
            "Jinyuan Jia",
            "Stanley J. Miklavcic",
            "Anuj Srivastava"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The root is an important organ of a plant since it is responsible for water\nand nutrient uptake. Analyzing and modelling variabilities in the geometry and\ntopology of roots can help in assessing the plant's health, understanding its\ngrowth patterns, and modeling relations between plant species and between\nplants and their environment. In this article, we develop a framework for the\nstatistical analysis and modeling of the geometry and topology of plant roots.\nWe represent root structures as points in a tree-shape space equipped with a\nmetric that quantifies geometric and topological differences between pairs of\nroots. We then use these building blocks to compute geodesics, i.e., optimal\ndeformations under the metric between root structures, and to perform\nstatistical analysis on root populations. We demonstrate the utility of the\nproposed framework through an application to a dataset of wheat roots grown in\ndifferent environmental conditions. We also show that the framework can be used\nin various applications including classification and regression.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.06812v2"
    },
    {
        "title": "Smooth quasi-developable surfaces bounded by smooth curves",
        "authors": [
            "Pengbo Bo",
            "Yujian Zheng",
            "Caiming Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Computing a quasi-developable strip surface bounded by design curves finds\nwide industrial applications. Existing methods compute discrete surfaces\ncomposed of developable lines connecting sampling points on input curves which\nare not adequate for generating smooth quasi-developable surfaces. We propose\nthe first method which is capable of exploring the full solution space of\ncontinuous input curves to compute a smooth quasi-developable ruled surface\nwith as large developability as possible. The resulting surface is exactly\nbounded by the input smooth curves and is guaranteed to have no\nself-intersections. The main contribution is a variational approach to compute\na continuous mapping of parameters of input curves by minimizing a function\nevaluating surface developability. Moreover, we also present an algorithm to\nrepresent a resulting surface as a B-spline surface when input curves are\nB-spline curves.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.07518v1"
    },
    {
        "title": "A Smoothness Energy without Boundary Distortion for Curved Surfaces",
        "authors": [
            "Oded Stein",
            "Alec Jacobson",
            "Max Wardetzky",
            "Eitan Grinspun"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Current quadratic smoothness energies for curved surfaces either exhibit\ndistortions near the boundary due to zero Neumann boundary conditions, or they\ndo not correctly account for intrinsic curvature, which leads to\nunnatural-looking behavior away from the boundary. This leads to an unfortunate\ntrade-off: one can either have natural behavior in the interior, or a\ndistortion-free result at the boundary, but not both. We introduce a\ngeneralized Hessian energy for curved surfaces, expressed in terms of the\ncovariant one-form Dirichlet energy, the Gaussian curvature, and the exterior\nderivative. Energy minimizers solve the Laplace-Beltrami biharmonic equation,\ncorrectly accounting for intrinsic curvature, leading to natural-looking\nisolines. On the boundary, minimizers are as-linear-as-possible, which reduces\nthe distortion of isolines at the boundary. We discretize the covariant\none-form Dirichlet energy using Crouzeix-Raviart finite elements, arriving at a\ndiscrete formulation of the Hessian energy for applications on curved surfaces.\nWe observe convergence of the discretization in our experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.09777v2"
    },
    {
        "title": "ENIGMA: Evolutionary Non-Isometric Geometry Matching",
        "authors": [
            "Michal Edelstein",
            "Danielle Ezuz",
            "Mirela Ben-Chen"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper we propose a fully automatic method for shape correspondence\nthat is widely applicable, and especially effective for non isometric shapes\nand shapes of different topology. We observe that fully-automatic shape\ncorrespondence can be decomposed as a hybrid discrete/continuous optimization\nproblem, and we find the best sparse landmark correspondence, whose\nsparse-to-dense extension minimizes a local metric distortion. To tackle the\ncombinatorial task of landmark correspondence we use an evolutionary genetic\nalgorithm, where the local distortion of the sparse-to-dense extension is used\nas the objective function. We design novel geometrically guided genetic\noperators, which, when combined with our objective, are highly effective for\nnon isometric shape matching. Our method outperforms state of the art methods\nfor automatic shape correspondence both quantitatively and qualitatively on\nchallenging datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.10763v3"
    },
    {
        "title": "A SVBRDF Modeling Pipeline using Pixel Clustering",
        "authors": [
            "Bo Li",
            "Jie Feng",
            "Bingfeng Zhou"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a pipeline for modeling spatially varying BRDFs (svBRDFs) of\nplanar materials which only requires a mobile phone for data acquisition. With\na minimum of two photos under the ambient and point light source, our pipeline\nproduces svBRDF parameters, a normal map and a tangent map for the material\nsample. The BRDF fitting is achieved via a pixel clustering strategy and an\noptimization based scheme. Our method is light-weight, easy-to-use and capable\nof producing high-quality BRDF textures.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.00321v1"
    },
    {
        "title": "On the Optical Accuracy of the Salvator Mundi",
        "authors": [
            " Marco",
            " Liang",
            "Michael T. Goodrich",
            "Shuang Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  A debate in the scientific literature has arisen regarding whether the orb\ndepicted in Salvator Mundi, which has been attributed by some experts to\nLeonardo da Vinci, was rendered in a optically faithful manner or not. Some\nhypothesize that it was solid crystal while others hypothesize that it was\nhollow, with competing explanations for its apparent lack of background\ndistortion and its three white spots. In this paper, we study the optical\naccuracy of the Salvator Mundi using physically based rendering, a\nsophisticated computer graphics tool that produces optically accurate images by\nsimulating light transport in virtual scenes. We created a virtual model of the\ncomposition centered on the translucent orb in the subject's hand. By\nsynthesizing images under configurations that vary illuminations and orb\nmaterial properties, we tested whether it is optically possible to produce an\nimage that renders the orb similarly to how it appears in the painting. Our\nexperiments show that an optically accurate rendering qualitatively matching\nthat of the painting is indeed possible using materials, light sources, and\nscientific knowledge available to Leonardo da Vinci circa 1500. We additionally\ntested alternative theories regarding the composition of the orb, such as that\nit was a solid calcite ball, which provide empirical evidence that such\nalternatives are unlikely to produce images similar to the painting, and that\nthe orb is instead hollow.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.03416v1"
    },
    {
        "title": "RGB Point Cloud Manipulation with Triangular Structures for Artistic\n  Image Recoloring",
        "authors": [
            "Baptiste Delos",
            "Nicolas Mellado",
            "David Vanderhaeghe",
            "Remi Cozot"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Usual approaches for image recoloring, such as local filtering by transfer\nfunctions and global histogram remapping, lack of accurate control or miss\nsmall groups of important pixels. In this paper, we introduce a triangle-based\nstructuring of the colors of an image in the RGB space. We present an analysis\nof image colors in the RGB space showing the theoretical motivation of our\ntriangular abstraction. We illustrate the usefulness of our structure to\nrecolor images.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.04583v2"
    },
    {
        "title": "Spectral Domain Decomposition Method for Natural Lighting and Medieval\n  Glass Rendering",
        "authors": [
            "Guillaume Gbikpi-Benissan",
            "Remi Cerise",
            "Patrick Callet",
            "Frederic Magoules"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper, we use an original ray-tracing domain decomposition method to\naddress image rendering of naturally lighted scenes. This new method allows to\nparticularly analyze rendering problems on parallel architectures, in the case\nof interactions between light-rays and glass material. Numerical experiments,\nfor medieval glass rendering within the church of the Royaumont abbey,\nillustrate the performance of the proposed ray-tracing domain decomposition\nmethod (DDM) on multi-cores and multi-processors architectures. On one hand,\napplying domain decomposition techniques increases speedups obtained by\nparallelizing the computation. On the other hand, for a fixed number of\nparallel processes, we notice that speedups increase as the number of\nsub-domains do.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.05494v1"
    },
    {
        "title": "A Comparison of Rendering Techniques for Large 3D Line Sets with\n  Transparency",
        "authors": [
            "Michael Kern",
            "Christoph Neuhauser",
            "Torben Maack",
            "Mengjiao Han",
            "Will Usher",
            "Rüdiger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper presents a comprehensive study of interactive rendering techniques\nfor large 3D line sets with transparency. The rendering of transparent lines is\nwidely used for visualizing trajectories of tracer particles in flow fields.\nTransparency is then used to fade out lines deemed unimportant, based on, for\ninstance, geometric properties or attributes defined along them. Since accurate\nblending of transparent lines requires rendering the lines in back-to-front or\nfront-to-back order, enforcing this order for 3D line sets with tens or even\nhundreds of thousands of elements becomes challenging. In this paper, we study\nCPU and GPU rendering techniques for large transparent 3D line sets. We compare\naccurate and approximate techniques using optimized implementations and a\nnumber of benchmark data sets. We discuss the effects of data size and\ntransparency on quality, performance and memory consumption. Based on our\nstudy, we propose two improvements to per-pixel fragment lists and multi-layer\nalpha blending. The first improves the rendering speed via an improved GPU\nsorting operation, and the second improves rendering quality via a\ntransparency-based bucketing.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.08485v2"
    },
    {
        "title": "Neural Smoke Stylization with Color Transfer",
        "authors": [
            "Fabienne Christen",
            "Byungsoo Kim",
            "Vinicius C. Azevedo",
            "Barbara Solenthaler"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Artistically controlling fluid simulations requires a large amount of manual\nwork by an artist. The recently presented transportbased neural style transfer\napproach simplifies workflows as it transfers the style of arbitrary input\nimages onto 3D smoke simulations. However, the method only modifies the shape\nof the fluid but omits color information. In this work, we therefore extend the\nprevious approach to obtain a complete pipeline for transferring shape and\ncolor information onto 2D and 3D smoke simulations with neural networks. Our\nresults demonstrate that our method successfully transfers colored style\nfeatures consistently in space and time to smoke data for different input\ntextures.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.08757v1"
    },
    {
        "title": "Rendering Synthetic Objects into Legacy Photographs",
        "authors": [
            "Kevin Karsch",
            "Varsha Hedau",
            "David Forsyth",
            "Derek Hoiem"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a method to realistically insert synthetic objects into existing\nphotographs without requiring access to the scene or any additional scene\nmeasurements. With a single image and a small amount of annotation, our method\ncreates a physical model of the scene that is suitable for realistically\nrendering synthetic objects with diffuse, specular, and even glowing materials\nwhile accounting for lighting interactions between the objects and the scene.\nWe demonstrate in a user study that synthetic images produced by our method are\nconfusable with real scenes, even for people who believe they are good at\ntelling the difference. Further, our study shows that our method is competitive\nwith other insertion methods while requiring less scene information. We also\ncollected new illumination and reflectance datasets; renderings produced by our\nsystem compare well to ground truth. Our system has applications in the movie\nand gaming industry, as well as home decorating and user content creation,\namong others.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11565v1"
    },
    {
        "title": "ConstructAide: Analyzing and Visualizing Construction Sites through\n  Photographs and Building Models",
        "authors": [
            "Kevin Karsch",
            "Mani Golparvar-Fard",
            "David Forsyth"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We describe a set of tools for analyzing, visualizing, and assessing\narchitectural/construction progress with unordered photo collections and 3D\nbuilding models. With our interface, a user guides the registration of the\nmodel in one of the images, and our system automatically computes the alignment\nfor the rest of the photos using a novel Structure-from-Motion (SfM) technique;\nimages with nearby viewpoints are also brought into alignment with each other.\nAfter aligning the photo(s) and model(s), our system allows a user, such as a\nproject manager or facility owner, to explore the construction site seamlessly\nin time, monitor the progress of construction, assess errors and deviations,\nand create photorealistic architectural visualizations. These interactions are\nfacilitated by automatic reasoning performed by our system: static and dynamic\nocclusions are removed automatically, rendering information is collected, and\nsemantic selection tools help guide user input. We also demonstrate that our\nuser-assisted SfM method outperforms existing techniques on both real-world\nconstruction data and established multi-view datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11567v1"
    },
    {
        "title": "Adding Custom Intersectors to the C++ Ray Tracing Template Library\n  Visionaray",
        "authors": [
            "Stefan Zellmann"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Most ray tracing libraries allow the user to provide custom functionality\nthat is executed when a potential ray surface interaction was encountered to\ndetermine if the interaction was valid or traversal should be continued. This\nis e.g. useful for alpha mask validation and allows the user to reuse existing\nray object intersection routines rather than reimplementing them. Augmenting\nray traversal with custom intersection logic requires some kind of callback\nmechanism that injects user code into existing library routines. With template\nlibraries, this injection can happen statically since the user compiles the\nbinary code herself. We present an implementation of this \"custom intersector\"\napproach and its integration into the C++ ray tracing template library\nVisionaray.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12786v1"
    },
    {
        "title": "Lightform: Procedural Effects for Projected AR",
        "authors": [
            "Brittany Factura",
            "Laura LaPerche",
            "Phil Reyneri",
            "Brett Jones",
            "Kevin Karsch"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Projected augmented reality, also called projection mapping or video mapping,\nis a form of augmented reality that uses projected light to directly augment 3D\nsurfaces, as opposed to using pass-through screens or headsets. The value of\nprojected AR is its ability to add a layer of digital content directly onto\nphysical objects or environments in a way that can be instantaneously viewed by\nmultiple people, unencumbered by a screen or additional setup.\n  Because projected AR typically involves projecting onto non-flat, textured\nobjects (especially those that are conventionally not used as projection\nsurfaces), the digital content needs to be mapped and aligned to precisely fit\nthe physical scene to ensure a compelling experience. Current projected AR\ntechniques require extensive calibration at the time of installation, which is\nnot conducive to iteration or change, whether intentional (the scene is\nreconfigured) or not (the projector is bumped or settles). The workflows are\nundefined and fragmented, thus making it confusing and difficult for many to\napproach projected AR. For example, a digital artist may have the software\nexpertise to create AR content, but could not complete an installation without\nexperience in mounting, blending, and realigning projector(s); the converse is\ntrue for many A/V installation teams/professionals. Projection mapping has\ntherefore been limited to high-end event productions, concerts, and films,\nbecause it requires expensive, complex tools, and skilled teams ($100K+\nbudgets).\n  Lightform provides a technology that makes projected AR approachable,\npractical, intelligent, and robust through integrated hardware and\ncomputer-vision software. Lightform brings together and unites a currently\nfragmented workflow into a single cohesive process that provides users with an\napproachable and robust method to create and control projected AR experiences.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.00521v1"
    },
    {
        "title": "Digesting the Elephant -- Experiences with Interactive Production\n  Quality Path Tracing of the Moana Island Scene",
        "authors": [
            "Ingo Wald",
            "Bruce Cherniak",
            "Will Usher",
            "Carson Brownlee",
            "Attila Afra",
            "Johannes Guenther",
            "Jefferson Amstutz",
            "Tim Rowley",
            "Valerio Pascucci",
            "Chris R Johnson",
            "Jim Jeffers"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  New algorithmic and hardware developments over the past two decades have\nenabled interactive ray tracing of small to modest sized scenes, and are\nfinding growing popularity in scientific visualization and games. However,\ninteractive ray tracing has not been as widely explored in the context of\nproduction film rendering, where challenges due to the complexity of the models\nand, from a practical standpoint, their unavailability to the wider research\ncommunity, have posed significant challenges. The recent release of the Disney\nMoana Island Scene has made one such model available to the community for\nexperimentation. In this paper, we detail the challenges posed by this scene to\nan interactive ray tracer, and the solutions we have employed and developed to\nenable interactive path tracing of the scene with full geometric and shading\ndetail, with the goal of providing insight and guidance to other researchers.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.02620v1"
    },
    {
        "title": "Running on Raygun",
        "authors": [
            "Alexander Hirsch",
            "Peter Thoman"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  With the introduction of Nvidia RTX hardware, ray tracing is now viable as a\ngeneral real time rendering technique for complex 3D scenes. Leveraging this\nnew technology, we present Raygun, an open source rendering, simulation, and\ngame engine focusing on simplicity, expandability, and the topic of ray tracing\nrealized through Nvidia's Vulkan ray tracing extension.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.09792v1"
    },
    {
        "title": "Levitating Rigid Objects with Hidden Rods and Wires",
        "authors": [
            "Sarah Kushner",
            "Risa Ulinski",
            "Karan Singh",
            "David I. W. Levin",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose a novel algorithm to efficiently generate hidden structures to\nsupport arrangements of floating rigid objects. Our optimization finds a small\nset of rods and wires between objects and each other or a supporting surface\n(e.g., wall or ceiling) that hold all objects in force and torque equilibrium.\nOur objective function includes a sparsity inducing total volume term and a\nlinear visibility term based on efficiently pre-computed Monte-Carlo\nintegration, to encourage solutions that are as-hidden-as-possible. The\nresulting optimization is convex and the global optimum can be efficiently\nrecovered via a linear program. Our representation allows for a\nuser-controllable mixture of tension-, compression-, and shear-resistant rods\nor tension-only wires. We explore applications to theatre set design, museum\nexhibit curation, and other artistic endeavours.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.00074v2"
    },
    {
        "title": "FAME: 3D Shape Generation via Functionality-Aware Model Evolution",
        "authors": [
            "Yanran Guan",
            "Han Liu",
            "Kun Liu",
            "Kangxue Yin",
            "Ruizhen Hu",
            "Oliver van Kaick",
            "Yan Zhang",
            "Ersin Yumer",
            "Nathan Carr",
            "Radomir Mech",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce a modeling tool which can evolve a set of 3D objects in a\nfunctionality-aware manner. Our goal is for the evolution to generate large and\ndiverse sets of plausible 3D objects for data augmentation, constrained\nmodeling, as well as open-ended exploration to possibly inspire new designs.\nStarting with an initial population of 3D objects belonging to one or more\nfunctional categories, we evolve the shapes through part recombination to\nproduce generations of hybrids or crossbreeds between parents from the\nheterogeneous shape collection. Evolutionary selection of offsprings is guided\nboth by a functional plausibility score derived from functionality analysis of\nshapes in the initial population and user preference, as in a design gallery.\nSince cross-category hybridization may result in offsprings not belonging to\nany of the known functional categories, we develop a means for functionality\npartial matching to evaluate functional plausibility on partial shapes. We show\na variety of plausible hybrid shapes generated by our functionality-aware model\nevolution, which can complement existing datasets as training data and boost\nthe performance of contemporary data-driven segmentation schemes, especially in\nchallenging cases. Our tool supports constrained modeling, allowing users to\nrestrict or steer the model evolution with functionality labels. At the same\ntime, unexpected yet functional object prototypes can emerge during open-ended\nexploration owing to structure breaking when evolving a heterogeneous\ncollection.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.04464v3"
    },
    {
        "title": "Design and visualization of Riemannian metrics",
        "authors": [
            "Tiago Novello",
            "Vinícius da Silva",
            "Luiz Velho"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Local and global illumination were recently defined in Riemannian manifolds\nto visualize classical Non-Euclidean spaces. This work focuses on Riemannian\nmetric construction in $\\mathbb{R}^3$ to explore special effects like warping,\nmirages, and deformations. We investigate the possibility of using graphs of\nfunctions and diffeomorphism to produce such effects. For these, their\nRiemannian metrics and geodesics derivations are provided, and ways of\naccumulating such metrics. We visualize, in \"real-time\", the resulting\nRiemannian manifolds using a ray tracing implemented on top of Nvidia RTX GPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.05386v1"
    },
    {
        "title": "SpectralWeight: a spectral graph wavelet framework for weight prediction\n  of pork cuts",
        "authors": [
            "Majid Masoumi",
            "Marcel Marcoux",
            "Laurence Maignel",
            "Candido Pomar"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we propose a novel approach for the quality assessment of pork\ncarcasses using 3D shape analysis. First, we make a 3D model of a pork\nhalf-carcass using a 3D scanner and then we take advantage of spectral graph\nwavelet signature (SGWS) to build a local spectral descriptor. Next, we\naggregate the extracted features using the bag-of-geometric-words paradigm to\nglobally represent the half-carcass shape. We then employ partial least-squares\nregression to predict the weight of pork cuts for the quality assessment of\ncarcasses. Our results demonstrate that SpectralWeight can predict the weight\nof different pork cuts and tissues with high accuracy. Although in this study\nwe evaluate the performance of SGWS for the weight prediction of pork\ndissection, our framework is fairly general and enables new ways to estimate\nthe quality and economical value of carcasses of different animals.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.05406v1"
    },
    {
        "title": "Online path sampling control with progressive spatio-temporal filtering",
        "authors": [
            "Jacopo Pantaleoni"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This work introduces progressive spatio-temporal filtering, an efficient\nmethod to build all-frequency approximations to the light transport\ndistribution into a scene by filtering individual samples produced by an\nunderlying path sampler, using online, iterative algorithms and data-structures\nthat exploit both the spatial and temporal coherence of the approximated light\nfield. Unlike previous approaches, the proposed method is both more efficient,\ndue to its use of an iterative temporal feedback loop that massively improves\nconvergence to a noise-free approximant, and more flexible, due to its\nintroduction of a spatio-directional hashing representation that allows to\nencode directional variations like those due to glossy reflections. We then\nintroduce four different methods to employ the resulting approximations to\ncontrol the underlying path sampler and/or modify its associated estimator,\ngreatly reducing its variance and enhancing its robustness to complex lighting\nscenarios. The core algorithms are highly scalable and low-overhead, requiring\nonly minor modifications to an existing path tracer.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.07547v2"
    },
    {
        "title": "An error reduced and uniform parameter approximation in fitting of\n  B-spline curves to data points",
        "authors": [
            "Debashis Mukherjee"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Approximating data points in three or higher dimension space based on cubic\nB-spline curve is presented. Representations for planar curves, are merged and\nextended to the higher dimension. The curve is fitted to the order of data\npoints, or uniform parameter values are assumed for the points. Tangents are\nassumed at the data points, corresponding to the property used in cardinal\nsplines, for shape preserving and visually pleasing fit. Control points of\npiecewise continuous cubic bezier curves, meeting the boundary conditions of\ncardinal spline segments, are used for b-spline curve in corresponding\ncoordinate planes. Approximation using error computed in the least square\nsense, based on a fraction of data points, is also presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.08468v1"
    },
    {
        "title": "Non-Uniform Gaussian Blur of Hexagonal Bins in Cartesian Coordinates",
        "authors": [
            "Reinier Vleugels",
            "Magnus Palmblad"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In a recent application of the Bokeh Python library for visualizing\nphysico-chemical properties of chemical entities text-mined from the scientific\nliterature, we found ourselves facing the task of smoothing hexagonally binned\ndata in Cartesian coordinates. To the best of our knowledge, no documentation\nfor how to do this exist in the public domain. This short paper shows how to\naccomplish this in general and for Bokeh in particular. We illustrate the\nmethod with a real-world example and discuss some potential advantages of using\nhexagonal bins in these and similar applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09941v2"
    },
    {
        "title": "Software Implementation of Optimized Bicubic Interpolated Scan\n  Conversion in Echocardiography",
        "authors": [
            "Olivier Rukundo",
            "Samuel E. Schmidt",
            "Olaf T von Ramm"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper introduces a novel approach leveraging objective image quality\nassessment (IQA) metrics to optimize the outcomes of traditional bicubic (BIC)\nimage interpolation and interpolated scan conversion algorithms. Specifically,\nfeature selection through line chart data visualization and computing the IQA\nmetrics scores are used to estimate the IQA-guided coefficient-k that up-dates\nthe traditional BIC algorithm weighting function. The resulting optimized\nbicubic (OBIC) algorithm was subjectively and objectively evaluated using\nnatural and ultrasound images. Results showed that the overall performance of\nthe OBIC algorithm was equivalent to 92.22% of 180 occurrences when compared to\nthe BIC algorithm, while it was 57.22% of 180 occurrences when compared to\nother algorithms. On top of that, the OBIC interpolated scan conversion\nalgorithm generally produced crisper and better contrast cropped ultrasound\nsectored images than the BIC algorithm, as well as other interpolated scan\nconversion algorithms mentioned.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.11269v2"
    },
    {
        "title": "Constructing Human Motion Manifold with Sequential Networks",
        "authors": [
            "Deok-Kyeong Jang",
            "Sung-Hee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper presents a novel recurrent neural network-based method to\nconstruct a latent motion manifold that can represent a wide range of human\nmotions in a long sequence. We introduce several new components to increase the\nspatial and temporal coverage in motion space while retaining the details of\nmotion capture data. These include new regularization terms for the motion\nmanifold, combination of two complementary decoders for predicting joint\nrotations and joint velocities, and the addition of the forward kinematics\nlayer to consider both joint rotation and position errors. In addition, we\npropose a set of loss terms that improve the overall quality of the motion\nmanifold from various aspects, such as the capability of reconstructing not\nonly the motion but also the latent manifold vector, and the naturalness of the\nmotion through adversarial loss. These components contribute to creating\ncompact and versatile motion manifold that allows for creating new motions by\nperforming random sampling and algebraic operations, such as interpolation and\nanalogy, in the latent motion manifold.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.14370v1"
    },
    {
        "title": "Jacobians and Hessians of Mean Value Coordinates for Closed Triangular\n  Meshes",
        "authors": [
            "Jean-Marc Thiery",
            "Julien Tierny",
            "Tamy Boubekeur"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  In this technical note, we present the formulae of the derivatives of the\nMean Value Coordinates based transformations, using an enclosing triangle mesh,\nacting as a cage for the deformation of an interior object.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.1914v2"
    },
    {
        "title": "Evaluation of a Bundling Technique for Parallel Coordinates",
        "authors": [
            "Julian Heinrich",
            "Yuan Luo",
            "Arthur E. Kirkpatrick",
            "Hao Zhang",
            "Daniel Weiskopf"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  We describe a technique for bundled curve representations in\nparallel-coordinates plots and present a controlled user study evaluating their\neffectiveness. Replacing the traditional C^0 polygonal lines by C^1 continuous\npiecewise Bezier curves makes it easier to visually trace data points through\neach coordinate axis. The resulting Bezier curves can then be bundled to\nvisualize data with given cluster structures. Curve bundles are efficient to\ncompute, provide visual separation between data clusters, reduce visual\nclutter, and present a clearer overview of the dataset. A controlled user study\nwith 14 participants confirmed the effectiveness of curve bundling for\nparallel-coordinates visualization: 1) compared to polygonal lines, it is\nequally capable of revealing correlations between neighboring data attributes;\n2) its geometric cues can be effective in displaying cluster information. For\nsome datasets curve bundling allows the color perceptual channel to be applied\nto other data attributes, while for complex cluster patterns, bundling and\ncolor can represent clustering far more clearly than either alone.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.6073v1"
    },
    {
        "title": "A Survey of Ocean Simulation and Rendering Techniques in Computer\n  Graphics",
        "authors": [
            "Emmanuelle Darles",
            "Benoît Crespin",
            "Djamchid Ghazanfarpour",
            "Jean-Christophe Gonzato"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  This paper presents a survey of ocean simulation and rendering methods in\ncomputer graphics. To model and animate the ocean's surface, these methods\nmainly rely on two main approaches: on the one hand, those which approximate\nocean dynamics with parametric, spectral or hybrid models and use empirical\nlaws from oceanographic research. We will see that this type of methods\nessentially allows the simulation of ocean scenes in the deep water domain,\nwithout breaking waves. On the other hand, physically-based methods use\nNavier-Stokes Equations (NSE) to represent breaking waves and more generally\nocean surface near the shore. We also describe ocean rendering methods in\ncomputer graphics, with a special interest in the simulation of phenomena such\nas foam and spray, and light's interaction with the ocean surface.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.6494v1"
    },
    {
        "title": "On Intra Prediction for Screen Content Video Coding",
        "authors": [
            "Haoming Chen",
            "Ankur Saxena",
            "Felix Fernandes"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Screen content coding (SCC) is becoming increasingly important in various\napplications, such as desktop sharing, video conferencing, and remote\neducation. When compared to natural camera- captured content, screen content\nhas different characteristics, in particular sharper edges. In this paper, we\npropose a novel intra prediction scheme for screen content video. In the\nproposed scheme, bilinear interpolation in angular intra prediction in HEVC is\nselectively replaced by nearest-neighbor intra prediction to preserve the sharp\nedges in screen content video. We present three different variants of the\nproposed nearest neighbor prediction algorithm: two implicit methods where both\nthe encoder, and the decoder derive whether to perform nearest neighbor\nprediction or not based on either (a) the sum of the absolute difference, or\n(b) the difference between the boundary pixels from which prediction is\nperformed; and another variant where Rate-Distortion-Optimization (RDO) search\nis performed at the encoder to decide whether or not to use the nearest\nneighbor interpolation, and explicitly signaled to the decoder. We also discuss\nthe various underlying trade-offs in terms of the complexity of the three\nvariants. All the three proposed variants provide significant gains over HEVC,\nand simulation results show that average gains of 3.3% BD-bitrate in\nIntra-frame coding are achieved by the RDO variant for screen content video. To\nthe best of our knowledge, this is the first paper that 1) points out current\nHEVC intra prediction scheme with bilinear interpolation does not work\nefficiently for screen content video and 2) uses different filters adaptively\nin the HEVC intra prediction interpolation.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.01862v2"
    },
    {
        "title": "Procedural wood textures",
        "authors": [
            "Albert J. Liu",
            "Stephen R. Marschner",
            "Victoria E. Dye"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Existing bidirectional reflectance distribution function (BRDF) models are\ncapable of capturing the distinctive highlights produced by the fibrous nature\nof wood. However, capturing parameter textures for even a single specimen\nremains a laborious process requiring specialized equipment. In this paper we\ntake a procedural approach to generating parameters for the wood BSDF. We\ncharacterize the elements of trees that are important for the appearance of\nwood, discuss techniques appropriate for representing those features, and\npresent a complete procedural wood shader capable of reproducing the growth\npatterns responsible for the distinctive appearance of highly prized\n``figured'' wood. Our procedural wood shader is random-access, 3D, modular, and\nis fast enough to generate a preview for design.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.04224v2"
    },
    {
        "title": "Bezier curves and surfaces based on modified Bernstein polynomials",
        "authors": [
            "Khalid Khan",
            "D. K. Lobiyal",
            "Adem Kilicman"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  In this paper, we use the blending functions of Bernstein polynomials with\nshifted knots for construction of Bezier curves and surfaces. We study the\nnature of degree elevation and degree reduction for Bezier Bernstein functions\nwith shifted knots.\n  Parametric curves are represented using these modified Bernstein basis and\nthe concept of total positivity is applied to investigate the shape properties\nof the curve. We get Bezier curve defined on [0, 1] when we set the parameter\n\\alpha=\\beta to the value 0. We also present a de Casteljau algorithm to\ncompute Bernstein Bezier curves and surfaces with shifted knots. The new curves\nhave some properties similar to Bezier curves. Furthermore, some fundamental\nproperties for Bernstein Bezier curves and surfaces are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.06594v1"
    },
    {
        "title": "Embedding of Hypercube into Cylinder",
        "authors": [
            "Weixing Ji",
            "Qinghui Liu",
            "Guizhen Wang",
            "ZhuoJia Shen"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Task mapping in modern high performance parallel computers can be modeled as\na graph embedding problem, which simulates the mapping as embedding one graph\ninto another and try to find the minimum wirelength for the mapping. Though\nembedding problems have been considered for several regular graphs, such as\nhypercubes into grids, binary trees into grids, et al, it is still an open\nproblem for hypercubes into cylinders. In this paper, we consider the problem\nof embedding hypercubes into cylinders to minimize the wirelength. We obtain\nthe exact wirelength formula of embedding hypercube $Q^r$ into cylinder\n$C_{2^3}\\times P_{2^{r-3}}$ with $r\\ge3$.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.07932v1"
    },
    {
        "title": "SlicerPET: A workflow based software module for PET/CT guided needle\n  biopsy",
        "authors": [
            "Dženan Zukić",
            "Julien Finet",
            "Emmanuel Wilson",
            "Filip Banovac",
            "Giuseppe Esposito",
            "Kevin Cleary",
            "Andinet Enquobahrie"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Biopsy is commonly used to confirm cancer diagnosis when radiologically\nindicated. Given the ability of PET to localize malignancies in heterogeneous\ntumors and tumors that do not have a CT correlate, PET/CT guided biopsy may\nimprove the diagnostic yield of biopsies. To facilitate PET/CT guided needle\nbiopsy, we developed a workflow that allows us to bring PET image guidance into\nthe interventional CT suite. In this abstract, we present SlicerPET, a\nuser-friendly workflow based module developed using open source software\nlibraries to guide needle biopsy in the interventional suite.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.08118v1"
    },
    {
        "title": "Resolution Improvement of the Common Method for Presentating Arbitrary\n  Space Curves Voxel",
        "authors": [
            "Anas M. Al-Oraiqat",
            "E. A. Bashkov",
            "S. A. Zori"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The task of voxel resolution for a space curve in video memory of 3D display\nis set. Furthermore, an approach solution of voxel resolution of arbitrary\nspace curve, given in parametric form, is studied. Numerous numbers of\nintensive experiments are conducted and interesting results with significant\nrecommendations are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00073v1"
    },
    {
        "title": "Interactive Sound Rendering on Mobile Devices using Ray-Parameterized\n  Reverberation Filters",
        "authors": [
            "Carl Schissler",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a new sound rendering pipeline that is able to generate plausible\nsound propagation effects for interactive dynamic scenes. Our approach combines\nray-tracing-based sound propagation with reverberation filters using robust\nautomatic reverb parameter estimation that is driven by impulse responses\ncomputed at a low sampling rate.We propose a unified spherical harmonic\nrepresentation of directional sound in both the propagation and auralization\nmodules and use this formulation to perform a constant number of convolution\noperations for any number of sound sources while rendering spatial audio. In\ncomparison to previous geometric acoustic methods, we achieve a speedup of over\nan order of magnitude while delivering similar audio to high-quality\nconvolution rendering algorithms. As a result, our approach is the first\ncapable of rendering plausible dynamic sound propagation effects on commodity\nsmartphones.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00430v1"
    },
    {
        "title": "Light Transport Simulation via Generalized Multiple Importance Sampling",
        "authors": [
            "Qi Liu",
            "Yiheng Zhang",
            "Lizhuang Ma"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Multiple importance sampling (MIS) is employed to reduce variance of\nestimators, but when sampling and weighting are not suitable to the integrand,\nthe estimators would have extra variance. Therefore, robust light transport\nsimulation algorithms based on Monte Carlo sampling for different types of\nscenes are still uncompleted. In this paper, we address this problem by present\na general method, named generalized multiple importance sampling (GMIS), to\nenhance the robustness of light transport simulation based on MIS. GMIS\ncombines different sampling techniques and weighting functions, extending MIS\nto a more generalized framework. Meanwhile, we implement the GMIS in common\nrenderers and illustrate how it increase the robustness of light transport\nsimulation. Experiments show that, by applying GMIS, we obtain better\nconvergence performance and lower variance, and increase the rendering of\nambient light and specular shadow effects apparently.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04305v2"
    },
    {
        "title": "Procedural Planetary Multi-resolution Terrain Generation for Games",
        "authors": [
            "Ricardo B. D. d'Oliveira",
            "Antonio L. Apolinário Jr"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Terrains are the main part of an electronic game. To reduce human effort on\ngame development, procedural techniques are used to generate synthetic\nterrains. However rendering a terrain is not a trivial task. Their rendering\ntechniques must be optimal for gaming. Specially planetary terrains, which must\naccount for precision and scale conversion. Multi-resolution models are best\nfit to planetary terrains. An observer can change his point of view without\nnoticing any decrease in visual quality. There are several proposals regarding\nreal-time terrain rendering with multi-resolution models, and there are game\nengines capable of generating large scale terrains with fixed resolution.\nHowever for the best of our knowledge, it was noticed that there are no\ntechniques which combine both aspects. In this paper we present a new technique\ncapable of generating large-scale multi-resolution terrains, whichcan be\nrendered and viewed at different scales. Rendering large scale models with high\ndefinition and low scale areas with finer details added with the aid of\nprocedural content generation.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04612v1"
    },
    {
        "title": "Low Rank Matrix Approximation for Geometry Filtering",
        "authors": [
            "Xuequan Lu",
            "Scott Schaefer",
            "Jun Luo",
            "Lizhuang Ma",
            "Ying He"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We propose a robust normal estimation method for both point clouds and meshes\nusing a low rank matrix approximation algorithm. First, we compute a local\nfeature descriptor for each point and find similar, non-local neighbors that we\norganize into a matrix. We then show that a low rank matrix approximation\nalgorithm can robustly estimate normals for both point clouds and meshes.\nFurthermore, we provide a new filtering method for point cloud data to smooth\nthe position data to fit the estimated normals. We show applications of our\nmethod to point cloud filtering, point set upsampling, surface reconstruction,\nmesh denoising, and geometric texture removal. Our experiments show that our\nmethod outperforms current methods in both visual quality and accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.06783v2"
    },
    {
        "title": "DeepWarp: DNN-based Nonlinear Deformation",
        "authors": [
            "Ran Luo",
            "Tianjia Shao",
            "Huamin Wang",
            "Weiwei Xu",
            "Kun Zhou",
            "Yin Yang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  DeepWarp is an efficient and highly re-usable deep neural network (DNN) based\nnonlinear deformable simulation framework. Unlike other deep learning\napplications such as image recognition, where different inputs have a uniform\nand consistent format (e.g. an array of all the pixels in an image), the input\nfor deformable simulation is quite variable, high-dimensional, and\nparametrization-unfriendly. Consequently, even though DNN is known for its rich\nexpressivity of nonlinear functions, directly using DNN to reconstruct the\nforce-displacement relation for general deformable simulation is nearly\nimpossible. DeepWarp obviates this difficulty by partially restoring the\nforce-displacement relation via warping the nodal displacement simulated using\na simplistic constitutive model -- the linear elasticity. In other words,\nDeepWarp yields an incremental displacement fix based on a simplified\n(therefore incorrect) simulation result other than returning the unknown\ndisplacement directly. We contrive a compact yet effective feature vector\nincluding geodesic, potential and digression to sort training pairs of per-node\nlinear and nonlinear displacement. DeepWarp is robust under different model\nshapes and tessellations. With the assistance of deformation substructuring,\none DNN training is able to handle a wide range of 3D models of various\ngeometries including most examples shown in the paper. Thanks to the linear\nelasticity and its constant system matrix, the underlying simulator only needs\nto perform one pre-factorized matrix solve at each time step, and DeepWarp is\nable to simulate large models in real time.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09109v1"
    },
    {
        "title": "H-CNN: Spatial Hashing Based CNN for 3D Shape Analysis",
        "authors": [
            "Tianjia Shao",
            "Yin Yang",
            "Yanlin Weng",
            "Qiming Hou",
            "Kun Zhou"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a novel spatial hashing based data structure to facilitate 3D\nshape analysis using convolutional neural networks (CNNs). Our method well\nutilizes the sparse occupancy of 3D shape boundary and builds hierarchical hash\ntables for an input model under different resolutions. Based on this data\nstructure, we design two efficient GPU algorithms namely hash2col and col2hash\nso that the CNN operations like convolution and pooling can be efficiently\nparallelized. The spatial hashing is nearly minimal, and our data structure is\nalmost of the same size as the raw input. Compared with state-of-the-art\noctree-based methods, our data structure significantly reduces the memory\nfootprint during the CNN training. As the input geometry features are more\ncompactly packed, CNN operations also run faster with our data structure. The\nexperiment shows that, under the same network structure, our method yields\ncomparable or better benchmarks compared to the state-of-the-art while it has\nonly one-third memory consumption. Such superior memory performance allows the\nCNN to handle high-resolution shape analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.11385v1"
    },
    {
        "title": "$P_N$-Method for Multiple Scattering in Participating Media",
        "authors": [
            "David Koerner",
            "Jamie Portsmouth",
            "Wenzel Jakob"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Rendering highly scattering participating media using brute force path\ntracing is a challenge. The diffusion approximation reduces the problem to\nsolving a simple linear partial differential equation. Flux-limited diffusion\nintroduces non-linearities to improve the accuracy of the solution, especially\nin low optical depth media, but introduces several ad-hoc assumptions. Both\nmethods are based on a spherical harmonics expansion of the radiance field that\nis truncated after the first order. In this paper, we investigate the open\nquestion of whether going to higher spherical harmonic orders provides a viable\nimprovement to these two approaches. Increasing the order introduces a set of\ncomplex coupled partial differential equations (the $P_N$-equations), whose\ngrowing number make them difficult to work with at higher orders. We thus use a\ncomputer algebra framework for representing and manipulating the underlying\nmathematical equations, and use it to derive the real-valued $P_N$-equations\nfor arbitrary orders. We further present a staggered-grid $P_N$-solver and\ngenerate its stencil code directly from the expression tree of the\n$P_N$-equations. Finally, we discuss how our method compares to prior work for\nvarious standard problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00410v1"
    },
    {
        "title": "Simplifying Urban Data Fusion with BigSUR",
        "authors": [
            "Tom Kelly",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Our ability to understand data has always lagged behind our ability to\ncollect it. This is particularly true in urban environments, where mass data\ncapture is particularly valuable, but the objects captured are more varied,\ndenser, and complex. To understand the structure and content of the\nenvironment, we must process the unstructured data to a structured form. BigSUR\nis an urban reconstruction algorithm which fuses GIS data, photogrammetric\nmeshes, and street level photography, to create clean representative,\nsemantically labelled, geometry. However, we have identified three problems\nwith the system i) the street level photography is often difficult to acquire;\nii) novel fa\\c{c}ade styles often frustrate the detection of windows and doors;\niii) the computational requirements of the system are large, processing a large\ncity block can take up to 15 hours. In this paper we describe the process of\nsimplifying and validating the BigSUR semantic reconstruction system. In\nparticular, the requirement for street level images is removed, and greedy\npost-process profile assignment is introduced to accelerate the system. We\naccomplish this by modifying the binary integer programming (BIP) optimization,\nand re-evaluating the effects of various parameters. The new variant of the\nsystem is evaluated over a variety of urban areas. We objectively measure mean\nsquared error (MSE) terms over the unstructured geometry, showing that BigSUR\nis able to accurately recover omissions from the input meshes. Further, we\nevaluate the ability of the system to label the walls and roofs of input\nmeshes, concluding that our new BigSUR variant achieves highly accurate\nsemantic labelling with shorter computational time and less input data.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00687v1"
    },
    {
        "title": "Solid Geometry Processing on Deconstructed Domains",
        "authors": [
            "Silvia Sellán",
            "Herng Yi Cheng",
            "Yuming Ma",
            "Mitchell Dembowski",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Many tasks in geometry processing are modeled as variational problems solved\nnumerically using the finite element method. For solid shapes, this requires a\nvolumetric discretization, such as a boundary conforming tetrahedral mesh.\nUnfortunately, tetrahedral meshing remains an open challenge and existing\nmethods either struggle to conform to complex boundary surfaces or require\nmanual intervention to prevent failure. Rather than create a single volumetric\nmesh for the entire shape, we advocate for solid geometry processing on\ndeconstructed domains, where a large and complex shape is composed of\noverlapping solid subdomains. As each smaller and simpler part is now easier to\ntetrahedralize, the question becomes how to account for overlaps during problem\nmodeling and how to couple solutions on each subdomain together algebraically.\nWe explore how and why previous coupling methods fail, and propose a method\nthat couples solid domains only along their boundary surfaces. We demonstrate\nthe superiority of this method through empirical convergence tests and\nqualitative applications to solid geometry processing on a variety of popular\nsecond-order and fourth-order partial differential equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00866v1"
    },
    {
        "title": "Learning Fuzzy Set Representations of Partial Shapes on Dual Embedding\n  Spaces",
        "authors": [
            "Minhyuk Sung",
            "Anastasia Dubrovina",
            "Vladimir G. Kim",
            "Leonidas Guibas"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Modeling relations between components of 3D objects is essential for many\ngeometry editing tasks. Existing techniques commonly rely on labeled\ncomponents, which requires substantial annotation effort and limits components\nto a dictionary of predefined semantic parts. We propose a novel framework\nbased on neural networks that analyzes an uncurated collection of 3D models\nfrom the same category and learns two important types of semantic relations\namong full and partial shapes: complementarity and interchangeability. The\nformer helps to identify which two partial shapes make a complete plausible\nobject, and the latter indicates that interchanging two partial shapes from\ndifferent objects preserves the object plausibility. Our key idea is to jointly\nencode both relations by embedding partial shapes as fuzzy sets in dual\nembedding spaces. We model these two relations as fuzzy set operations\nperformed across the dual embedding spaces, and within each space,\nrespectively. We demonstrate the utility of our method for various retrieval\ntasks that are commonly needed in geometric modeling interfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.01519v1"
    },
    {
        "title": "Continuous-Scale Kinetic Fluid Simulation",
        "authors": [
            "Wei Li",
            "Kai Bai",
            "Xiaopei Liu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Kinetic approaches, i.e., methods based on the lattice Boltzmann equations,\nhave long been recognized as an appealing alternative for solving\nincompressible Navier-Stokes equations in computational fluid dynamics.\nHowever, such approaches have not been widely adopted in graphics mainly due to\nthe underlying inaccuracy, instability and inflexibility. In this paper, we try\nto tackle these problems in order to make kinetic approaches practical for\ngraphical applications. To achieve more accurate and stable simulations, we\npropose to employ the non-orthogonal central-moment-relaxation model, where we\ndevelop a novel adaptive relaxation method to retain both stability and\naccuracy in turbulent flows. To achieve flexibility, we propose a novel\ncontinuous-scale formulation that enables samples at arbitrary resolutions to\neasily communicate with each other in a more continuous sense and with loose\ngeometrical constraints, which allows efficient and adaptive sample\nconstruction to better match the physical scale. Such a capability directly\nleads to an automatic sample construction which generates static and dynamic\nscales at initialization and during simulation, respectively. This effectively\nmakes our method suitable for simulating turbulent flows with arbitrary\ngeometrical boundaries. Our simulation results with applications to smoke\nanimations show the benefits of our method, with comparisons for justification\nand verification.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.02284v1"
    },
    {
        "title": "Inferring Quality in Point Cloud-based 3D Printed Objects using\n  Topological Data Analysis",
        "authors": [
            "Paul Rosen",
            "Mustafa Hajij",
            "Junyi Tu",
            "Tanvirul Arafin",
            "Les Piegl"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Assessing the quality of 3D printed models before they are printed remains a\nchalleng- ing problem, particularly when considering point cloud-based models.\nThis paper introduces an approach to quality assessment, which uses techniques\nfrom the field of Topological Data Analy- sis (TDA) to compute a topological\nabstraction of the eventual printed model. Two main tools of TDA, Mapper and\npersistent homology, are used to analyze both the printed space and empty space\ncreated by the model. This abstraction enables investigating certain qualities\nof the model, with respect to print quality, and identifies potential anomalies\nthat may appear in the final product.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.02921v1"
    },
    {
        "title": "StyleBlit: Fast Example-Based Stylization with Local Guidance",
        "authors": [
            "Daniel Sýkora",
            "Ondřej Jamriška",
            "Jingwan Lu",
            "Eli Shechtman"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present StyleBlit---an efficient example-based style transfer algorithm\nthat can deliver high-quality stylized renderings in real-time on a single-core\nCPU. Our technique is especially suitable for style transfer applications that\nuse local guidance - descriptive guiding channels containing large spatial\nvariations. Local guidance encourages transfer of content from the source\nexemplar to the target image in a semantically meaningful way. Typical local\nguidance includes, e.g., normal values, texture coordinates or a displacement\nfield. Contrary to previous style transfer techniques, our approach does not\ninvolve any computationally expensive optimization. We demonstrate that when\nlocal guidance is used, optimization-based techniques converge to solutions\nthat can be well approximated by simple pixel-level operations. Inspired by\nthis observation, we designed an algorithm that produces results visually\nsimilar to, if not better than, the state-of-the-art, and is several orders of\nmagnitude faster. Our approach is suitable for scenarios with low computational\nbudget such as games and mobile applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.03249v1"
    },
    {
        "title": "CNNs based Viewpoint Estimation for Volume Visualization",
        "authors": [
            "Neng Shi",
            "Yubo Tao"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Viewpoint estimation from 2D rendered images is helpful in understanding how\nusers select viewpoints for volume visualization and guiding users to select\nbetter viewpoints based on previous visualizations. In this paper, we propose a\nviewpoint estimation method based on Convolutional Neural Networks (CNNs) for\nvolume visualization. We first design an overfit-resistant image rendering\npipeline to generate the training images with accurate viewpoint annotations,\nand then train a category-specific viewpoint classification network to estimate\nthe viewpoint for the given rendered image. Our method can achieve good\nperformance on images rendered with different transfer functions and rendering\nparameters in several categories. We apply our model to recover the viewpoints\nof the rendered images in publications, and show how experts look at volumes.\nWe also introduce a CNN feature-based image similarity measure for similarity\nvoting based viewpoint selection, which can suggest semantically meaningful\noptimal viewpoints for different volumes and transfer functions.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07449v3"
    },
    {
        "title": "Robust Edge-Preserved Surface Mesh Polycube Deformation",
        "authors": [
            "Hui Zhao",
            "Na Lei",
            "Xuan Li",
            "Peng Zeng",
            "Ke Xu",
            "Xianfeng Gu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The problem of polycube construction or deformation is an essential problem\nin computer graphics. In this paper, we present a robust, simple, efficient and\nautomatic algorithm to deform the meshes of arbitrary shapes into their\npolycube ones. We derive a clear relationship between a mesh and its\ncorresponding polycube shape. Our algorithm is edge-preserved, and works on\nsurface meshes with or without boundaries. Our algorithm outperforms previous\nones in speed, robustness, efficiency. Our method is simple to implement. To\ndemonstrate the robustness and effectiveness of our method, we apply it to\nhundreds of models of varying complexity and topology. We demonstrat that our\nmethod compares favorably to other state-of-the-art polycube deformation\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.08474v1"
    },
    {
        "title": "Conformal Mesh Parameterization Using Discrete Calabi Flow",
        "authors": [
            "Hui Zhao",
            "Xuan Li",
            "Huabin Ge",
            "Xianfeng Gu",
            "Na Lei"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In this paper, we introduce discrete Calabi flow to the graphics research\ncommunity and present a novel conformal mesh parameterization algorithm. Calabi\nenergy has a succinct and explicit format. Its corresponding flow is conformal\nand convergent under certain conditions. Our method is based on the Calabi\nenergy and Calabi flow with solid theoretical and mathematical base. We\ndemonstrate our approach on dozens of models and compare it with other related\nflow based methods, such as the well-known Ricci flow and CETM. Our experiments\nshow that the performance of our algorithm is comparably the same with other\nmethods. The discrete Calabi flow in our method provides another perspective on\nconformal flow and conformal parameterization.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.08486v1"
    },
    {
        "title": "GRAINS: Generative Recursive Autoencoders for INdoor Scenes",
        "authors": [
            "Manyi Li",
            "Akshay Gadi Patil",
            "Kai Xu",
            "Siddhartha Chaudhuri",
            "Owais Khan",
            "Ariel Shamir",
            "Changhe Tu",
            "Baoquan Chen",
            "Daniel Cohen-Or",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a generative neural network which enables us to generate plausible\n3D indoor scenes in large quantities and varieties, easily and highly\nefficiently. Our key observation is that indoor scene structures are inherently\nhierarchical. Hence, our network is not convolutional; it is a recursive neural\nnetwork or RvNN. Using a dataset of annotated scene hierarchies, we train a\nvariational recursive autoencoder, or RvNN-VAE, which performs scene object\ngrouping during its encoding phase and scene generation during decoding.\nSpecifically, a set of encoders are recursively applied to group 3D objects\nbased on support, surround, and co-occurrence relations in a scene, encoding\ninformation about object spatial properties, semantics, and their relative\npositioning with respect to other objects in the hierarchy. By training a\nvariational autoencoder (VAE), the resulting fixed-length codes roughly follow\na Gaussian distribution. A novel 3D scene can be generated hierarchically by\nthe decoder from a randomly sampled code from the learned distribution. We coin\nour method GRAINS, for Generative Recursive Autoencoders for INdoor Scenes. We\ndemonstrate the capability of GRAINS to generate plausible and diverse 3D\nindoor scenes and compare with existing methods for 3D scene synthesis. We show\napplications of GRAINS including 3D scene modeling from 2D layouts, scene\nediting, and semantic scene segmentation via PointNet whose performance is\nboosted by the large quantity and variety of 3D scenes generated by our method.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.09193v5"
    },
    {
        "title": "AniCode: Authoring Coded Artifacts for Network-Free Personalized\n  Animations",
        "authors": [
            "Zeyu Wang",
            "Shiyu Qiu",
            "Qingyang Chen",
            "Alexander Ringlein",
            "Julie Dorsey",
            "Holly Rushmeier"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Time-based media (videos, synthetic animations, and virtual reality\nexperiences) are used for communication, in applications such as manufacturers\nexplaining the operation of a new appliance to consumers and scientists\nillustrating the basis of a new conclusion. However, authoring time-based media\nthat are effective and personalized for the viewer remains a challenge. We\nintroduce AniCode, a novel framework for authoring and consuming time-based\nmedia. An author encodes a video animation in a printed code, and affixes the\ncode to an object. A consumer uses a mobile application to capture an image of\nthe object and code, and to generate a video presentation on the fly.\nImportantly, AniCode presents the video personalized in the consumer's visual\ncontext. Our system is designed to be low cost and easy to use. By not\nrequiring an internet connection, and through animations that decode correctly\nonly in the intended context, AniCode enhances privacy of communication using\ntime-based media. Animation schemes in the system include a series of 2D and 3D\ngeometric transformations, color transformation, and annotation. We demonstrate\nthe AniCode framework with sample applications from a wide range of domains,\nincluding product \"how to\" examples, cultural heritage, education, creative\nart, and design. We evaluate the ease of use and effectiveness of our system\nwith a user study.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.11627v1"
    },
    {
        "title": "On-the-Fly Power-Aware Rendering",
        "authors": [
            "Yunjin Zhang",
            "Marta Ortin",
            "Victor Arellano",
            "Rui Wang",
            "Diego Gutierrez",
            "Hujun Bao"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Power saving is a prevailing concern in desktop computers and, especially, in\nbattery-powered devices such as mobile phones. This is generating a growing\ndemand for power-aware graphics applications that can extend battery life,\nwhile preserving good quality. In this paper, we address this issue by\npresenting a real-time power-efficient rendering framework, able to dynamically\nselect the rendering configuration with the best quality within a given power\nbudget. Different from the current state of the art, our method does not\nrequire precomputation of the whole camera-view space, nor Pareto curves to\nexplore the vast power-error space; as such, it can also handle dynamic scenes.\nOur algorithm is based on two key components: our novel power prediction model,\nand our runtime quality error estimation mechanism. These components allow us\nto search for the optimal rendering configuration at runtime, being transparent\nto the user. We demonstrate the performance of our framework on two different\nplatforms: a desktop computer, and a mobile device. In both cases, we produce\nresults close to the maximum quality, while achieving significant power\nsavings.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.11760v1"
    },
    {
        "title": "Fast Sketch Segmentation and Labeling with Deep Learning",
        "authors": [
            "Lei Li",
            "Hongbo Fu",
            "Chiew-Lan Tai"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a simple and efficient method based on deep learning to\nautomatically decompose sketched objects into semantically valid parts. We\ntrain a deep neural network to transfer existing segmentations and labelings\nfrom 3D models to freehand sketches without requiring numerous well-annotated\nsketches as training data. The network takes the binary image of a sketched\nobject as input and produces a corresponding segmentation map with per-pixel\nlabelings as output. A subsequent post-process procedure with multi-label graph\ncuts further refines the segmentation and labeling result. We validate our\nproposed method on two sketch datasets. Experiments show that our method\noutperforms the state-of-the-art method in terms of segmentation and labeling\naccuracy and is significantly faster, enabling further integration in\ninteractive drawing systems. We demonstrate the efficiency of our method in a\nsketch-based modeling application that automatically transforms input sketches\ninto 3D models by part assembly.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.11847v1"
    },
    {
        "title": "Decomposition and Modeling in the Non-Manifold domain",
        "authors": [
            "Franco Morando"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The problem of decomposing non-manifold object has already been studied in\nsolid modeling. However, the few proposed solutions are limited to the problem\nof decomposing solids described through their boundaries. In this thesis we\nstudy the problem of decomposing an arbitrary non-manifold simplicial complex\ninto more regular components. A formal notion of decomposition is developed\nusing combinatorial topology. The proposed decomposition is unique, for a given\ncomplex, and is computable for complexes of any dimension. A decomposition\nalgorithm is proposed that is linear w.r.t. the size of the input. In three or\nhigher dimensions a decomposition into manifold parts is not always possible.\nThus, in higher dimensions, we decompose a non-manifold into a decidable super\nclass of manifolds, that we call, Initial-Quasi-Manifolds. We also defined a\ntwo-layered data structure, the Extended Winged data structure. This data\nstructure is a dimension independent data structure conceived to model\nnon-manifolds through their decomposition into initial-quasi-manifold parts.\nOur two layered data structure describes the structure of the decomposition and\neach component separately. In the second layer we encode the connectivity\nstructure of the decomposition. We analyze the space requirements of the\nExtended Winged data structure and give algorithms to build and navigate it.\nFinally, we discuss time requirements for the computation of topological\nrelations and show that, for surfaces and tetrahedralizations, embedded in real\n3D space, all topological relations can be extracted in optimal time. This\napproach offers a compact, dimension independent, representation for\nnon-manifolds that can be useful whenever the modeled object has few\nnon-manifold singularities.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00306v1"
    },
    {
        "title": "Learning Soft Tissue Behavior of Organs for Surgical Navigation with\n  Convolutional Neural Networks",
        "authors": [
            "Micha Pfeiffer",
            "Carina Riediger",
            "Jürgen Weitz",
            "Stefanie Speidel"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Purpose: In surgical navigation, pre-operative organ models are presented to\nsurgeons during the intervention to help them in efficiently finding their\ntarget. In the case of soft tissue, these models need to be deformed and\nadapted to the current situation by using intra-operative sensor data. A\npromising method to realize this are real-time capable biomechanical models.\n  Methods: We train a fully convolutional neural network to estimate a\ndisplacement field of all points inside an organ when given only the\ndisplacement of a part of the organ's surface. The network trains on entirely\nsynthetic data of random organ-like meshes, which allows us to generate much\nmore data than is otherwise available. The input and output data is discretized\ninto a regular grid, allowing us to fully utilize the capabilities of\nconvolutional operators and to train and infer in a highly parallelized manner.\n  Results: The system is evaluated on in-silico liver models, phantom liver\ndata and human in-vivo breathing data. We test the performance with varying\nmaterial parameters, organ shapes and amount of visible surface. Even though\nthe network is only trained on synthetic data, it adapts well to the various\ncases and gives a good estimation of the internal organ displacement. The\ninference runs at over 50 frames per second.\n  Conclusions: We present a novel method for training a data-driven, real-time\ncapable deformation model. The accuracy is comparable to other registration\nmethods, it adapts very well to previously unseen organs and does not need to\nbe re-trained for every patient. The high inferring speed makes this method\nuseful for many applications such as surgical navigation and real-time\nsimulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00722v1"
    },
    {
        "title": "The Discrete Fourier Transform for Golden Angle Linogram Sampling",
        "authors": [
            "Elias S. Helou",
            "Marcelo V. W. Zibetti",
            "Leon Axel",
            "Kai Tobias Block",
            "Ravinder R. Regatte",
            "Gabor T. Herman"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Estimation of the Discrete-Time Fourier Transform (DTFT) at points of a\nfinite domain arises in many imaging applications. A new approach to this task,\nthe Golden Angle Linogram Fourier Domain (GALFD), is presented, together with a\ncomputationally fast and accurate tool, named Golden Angle Linogram Evaluation\n(GALE), for approximating the DTFT at points of a GALFD. A GALFD resembles a\nLinogram Fourier Domain (LFD), which is efficient and accurate. A limitation of\nlinograms is that embedding an LFD into a larger one requires many extra\npoints, at least doubling the domain's cardinality. The GALFD, on the other\nhand, allows for incremental inclusion of relatively few data points.\nApproximation error bounds and floating point operations counts are presented\nto show that GALE computes accurately and efficiently the DTFT at the points of\na GALFD. The ability to extend the data collection in small increments is\nbeneficial in applications such as Magnetic Resonance Imaging. Experiments for\nsimulated and for real-world data are presented to substantiate the theoretical\nclaims. The mathematical analysis, algorithms, and software developed in the\npaper are equally suitable to other angular distributions of rays and therefore\nwe bring the benefits of linograms to arbitrary radial patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.01152v2"
    },
    {
        "title": "x3ogre: Connecting X3D to a state of the art rendering engine",
        "authors": [
            "Pavel Rojtberg",
            "Benjamin Audenrith"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We connect X3D to the state of the art OGRE renderer using our prototypical\nx3ogre implementation. At this we perform a comparison of both on a conceptual\nlevel, highlighting similarities and differences. Our implementation allows\nswapping X3D concepts for OGRE concepts and vice versa. We take advantage of\nthis to analyse current shortcomings in X3D and propose X3D extensions to\novercome those.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.02524v1"
    },
    {
        "title": "Computational Parquetry: Fabricated Style Transfer with Wood Pixels",
        "authors": [
            "Julian Iseringhausen",
            "Michael Weinmann",
            "Weizhen Huang",
            "Matthias B. Hullin"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Parquetry is the art and craft of decorating a surface with a pattern of\ndifferently colored veneers of wood, stone or other materials. Traditionally,\nthe process of designing and making parquetry has been driven by color, using\nthe texture found in real wood only for stylization or as a decorative effect.\nHere, we introduce a computational pipeline that draws from the rich natural\nstructure of strongly textured real-world veneers as a source of detail in\norder to approximate a target image as faithfully as possible using a\nmanageable number of parts. This challenge is closely related to the\nestablished problems of patch-based image synthesis and stylization in some\nways, but fundamentally different in others. Most importantly, the limited\navailability of resources (any piece of wood can only be used once) turns the\nrelatively simple problem of finding the right piece for the target location\ninto the combinatorial problem of finding optimal parts while avoiding resource\ncollisions. We introduce an algorithm that allows to efficiently solve an\napproximation to the problem. It further addresses challenges like gamut\nmapping, feature characterization and the search for fabricable cuts. We\ndemonstrate the effectiveness of the system by fabricating a selection of\n\"photo-realistic\" pieces of parquetry from different kinds of unstained wood\nveneer.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.04769v2"
    },
    {
        "title": "Curve and surface construction based on the generalized toric-Bernstein\n  basis functions",
        "authors": [
            "Jing-Gai Li",
            "Chun-Gang Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The construction of parametric curve and surface plays important role in\ncomputer aided geometric design (CAGD), computer aided design (CAD), and\ngeometric modeling. In this paper, we define a new kind of blending functions\nassociated with a real points set, called generalized toric-Bernstein\n(GT-Bernstein) basis functions. Then the generalized toric-Bezier (GT-B\\'ezier)\ncurves and surfaces are constructed based on the GT-Bernstein basis functions,\nwhich are the projections of the (irrational) toric varieties in fact and the\ngeneralizations of the classical rational B\\'ezier curves and surfaces and\ntoric surface patches. Furthermore, we also study the properties of the\npresented curves and surfaces, including the limiting properties of weights and\nknots. Some representative examples verify the properties and results.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.04954v1"
    },
    {
        "title": "ZoomOut: Spectral Upsampling for Efficient Shape Correspondence",
        "authors": [
            "Simone Melzi",
            "Jing Ren",
            "Emanuele Rodolà",
            "Abhishek Sharma",
            "Peter Wonka",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a simple and efficient method for refining maps or correspondences\nby iterative upsampling in the spectral domain that can be implemented in a few\nlines of code. Our main observation is that high quality maps can be obtained\neven if the input correspondences are noisy or are encoded by a small number of\ncoefficients in a spectral basis. We show how this approach can be used in\nconjunction with existing initialization techniques across a range of\napplication scenarios, including symmetry detection, map refinement across\ncomplete shapes, non-rigid partial shape matching and function transfer. In\neach application we demonstrate an improvement with respect to both the quality\nof the results and the computational speed compared to the best competing\nmethods, with up to two orders of magnitude speed-up in some applications. We\nalso demonstrate that our method is both robust to noisy input and is scalable\nwith respect to shape complexity. Finally, we present a theoretical\njustification for our approach, shedding light on structural properties of\nfunctional maps.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.07865v4"
    },
    {
        "title": "Rendering of Complex Heterogenous Scenes using Progressive Blue Surfels",
        "authors": [
            "Sascha Brandt",
            "Claudius Jähn",
            "Matthias Fischer",
            "Friedhelm Meyer auf der Heide"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a technique for rendering highly complex 3D scenes in real-time by\ngenerating uniformly distributed points on the scene's visible surfaces. The\ntechnique is applicable to a wide range of scene types, like scenes directly\nbased on complex and detailed CAD data consisting of billions of polygons (in\ncontrast to scenes handcrafted solely for visualization). This allows to\nvisualize such scenes smoothly even in VR on a HMD with good image quality,\nwhile maintaining the necessary frame-rates. In contrast to other point based\nrendering methods, we place points in an approximated blue noise distribution\nonly on visible surfaces and store them in a highly GPU efficient data\nstructure, allowing to progressively refine the number of rendered points to\nmaximize the image quality for a given target frame rate. Our evaluation shows\nthat scenes consisting of a high amount of polygons can be rendered with\ninteractive frame rates with good visual quality on standard hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.08225v1"
    },
    {
        "title": "Snaxels on a Plane",
        "authors": [
            "Kevin Karsch",
            "John C. Hart"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  While many algorithms exist for tracing various contours for illustrating a\nmeshed object, few algorithms organize these contours into region-bounding\nclosed loops. Tracing closed-loop boundaries on a mesh can be problematic due\nto switchbacks caused by subtle surface variation, and the organization of\nthese regions into a planar map can lead to many small region components due to\nimprecision and noise. This paper adapts \"snaxels,\" an energy minimizing active\ncontour method designed for robust mesh processing, and repurposes it to\ngenerate visual, shadow and shading contours, and a simplified visual-surface\nplanar map, useful for stylized vector art illustration of the mesh. The snaxel\nactive contours can also track contours as the mesh animates, and\nframe-to-frame correspondences between snaxels lead to a new method to convert\nthe moving contours on a 3-D animated mesh into 2-D SVG curve animations for\nefficient embedding in Flash, PowerPoint and other dynamic vector art\nplatforms.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.09530v1"
    },
    {
        "title": "Surface2Volume: Surface Segmentation Conforming Assemblable Volumetric\n  Partition",
        "authors": [
            "Chrystiano Araújo",
            "Daniela Cabiddu",
            "Marco Attene",
            "Marco Livesu",
            "Nicholas Vining",
            "Alla Sheffer"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Users frequently seek to fabricate objects whose outer surfaces consist of\nregions with different surface attributes, such as color or material.\nManufacturing such objects in a single piece is often challenging or even\nimpossible. The alternative is to partition them into single-attribute\nvolumetric parts that can be fabricated separately and then assembled to form\nthe target object. Facilitating this approach requires partitioning the input\nmodel into parts that conform to the surface segmentation and that can be moved\napart with no collisions. We propose Surface2Volume, a partition algorithm\ncapable of producing such assemblable parts, each of which is affiliated with a\nsingle attribute, the outer surface of whose assembly conforms to the input\nsurface geometry and segmentation. In computing the partition we strictly\nenforce conformity with surface segmentation and assemblability, and optimize\nfor ease of fabrication by minimizing part count, promoting part simplicity,\nand simplifying assembly sequencing. We note that computing the desired\npartition requires solving for three types of variables: per-part assembly\ntrajectories, partition topology, i.e. the connectivity of the interface\nsurfaces separating the different parts, and the geometry, or location, of\nthese interfaces. We efficiently produce the desired partitions by addressing\none type of variables at a time: first computing the assembly trajectories,\nthen determining interface topology, and finally computing interface locations\nthat allow parts assemblability. We algorithmically identify inputs that\nnecessitate sequential assembly, and partition these inputs gradually by\ncomputing and disassembling a subset of assemblable parts at a time. We\ndemonstrate our method....\n",
        "pdf_link": "http://arxiv.org/pdf/1904.10213v1"
    },
    {
        "title": "Mechanics-Aware Modeling of Cloth Appearance",
        "authors": [
            "Zahra Montazeri",
            "Chang Xiao",
            " Yun",
            " Fei",
            "Changxi Zheng",
            "Shuang Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Micro-appearance models have brought unprecedented fidelity and details to\ncloth rendering.\n  Yet, these models neglect fabric mechanics: when a piece of cloth interacts\nwith the environment, its yarn and fiber arrangement usually changes in\nresponse to external contact and tension forces.\n  Since subtle changes of a fabric's microstructures can greatly affect its\nmacroscopic appearance, mechanics-driven appearance variation of fabrics has\nbeen a phenomenon that remains to be captured.\n  We introduce a mechanics-aware model that adapts the microstructures of cloth\nyarns in a physics-based manner.\n  Our technique works on two distinct physical scales: using physics-based\nsimulations of individual yarns, we capture the rearrangement of yarn-level\nstructures in response to external forces.\n  These yarn structures are further enriched to obtain appearance-driving\nfiber-level details.\n  The cross-scale enrichment is made practical through a new parameter fitting\nalgorithm for simulation, an augmented procedural yarn model coupled with a\ncustom-design regression neural network.\n  We train the network using a dataset generated by joint simulations at both\nthe yarn and the fiber levels.\n  Through several examples, we demonstrate that our model is capable of\nsynthesizing photorealistic cloth appearance in a %dynamic and mechanically\nplausible way.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11116v3"
    },
    {
        "title": "SurfaceBrush: From Virtual Reality Drawings to Manifold Surfaces",
        "authors": [
            "Enrique Rosales",
            "Jafet Rodriguez",
            "Alla Sheffer"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Popular Virtual Reality (VR) tools allow users to draw varying-width,\nribbon-like 3D brush strokes by moving a hand-held controller in 3D space.\nArtists frequently use dense collections of such strokes to draw virtual 3D\nshapes. We propose SurfaceBrush, a surfacing method that converts such VR\ndrawings into user-intended manifold free-form 3D surfaces, providing a novel\napproach for modeling 3D shapes. The inputs to our method consist of dense\ncollections of artist-drawn stroke ribbons described by the positions and\nnormals of their central polylines, and ribbon widths. These inputs are highly\ndistinct from those handled by existing surfacing frameworks and exhibit\ndifferent sparsity and error patterns, necessitating a novel surfacing\napproach. We surface the input stroke drawings by identifying and leveraging\nlocal coherence between nearby artist strokes. In particular, we observe that\nstrokes intended to be adjacent on the artist imagined surface often have\nsimilar tangent directions along their respective polylines. We leverage this\nlocal stroke direction consistency by casting the computation of the\nuser-intended manifold surface as a constrained matching problem on stroke\npolyline vertices and edges. We first detect and smoothly connect adjacent\nsimilarly-directed sequences of stroke edges producing one or more manifold\npartial surfaces. We then complete the surfacing process by identifying and\nconnecting adjacent similarly directed edges along the borders of these partial\nsurfaces. We confirm the usability of the SurfaceBrush interface and the\nvalidity of our drawing analysis via an observational study. We validate our\nstroke surfacing algorithm by demonstrating an array of manifold surfaces\ncomputed by our framework starting from a range of inputs of varying\ncomplexity, and by comparing our outputs to reconstructions computed using\nalternative means.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.12297v1"
    },
    {
        "title": "Slope-Dependent Rendering of Parallel Coordinates to Reduce Density\n  Distortion and Ghost Clusters",
        "authors": [
            "David Pomerenke",
            "Frederik L. Dennig",
            "Daniel A. Keim",
            "Johannes Fuchs",
            "Michael Blumenschein"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Parallel coordinates are a popular technique to visualize multi-dimensional\ndata. However, they face a significant problem influencing the perception and\ninterpretation of patterns. The distance between two parallel lines differs\nbased on their slope. Vertical lines are rendered longer and closer to each\nother than horizontal lines. This problem is inherent in the technique and has\ntwo main consequences: (1) clusters which have a steep slope between two axes\nare visually more prominent than horizontal clusters. (2) Noise and clutter can\nbe perceived as clusters, as a few parallel vertical lines visually emerge as a\nghost cluster. Our paper makes two contributions: First, we formalize the\nproblem and show its impact. Second, we present a novel technique to reduce the\neffects by rendering the polylines of the parallel coordinates based on their\nslope: horizontal lines are rendered with the default width, lines with a steep\nslope with a thinner line. Our technique avoids density distortions of\nclusters, can be computed in linear time, and can be added on top of most\nparallel coordinate variations. To demonstrate the usefulness, we show examples\nand compare them to the classical rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.00500v2"
    },
    {
        "title": "Geometric Sample Reweighting for Monte Carlo Integration",
        "authors": [
            "Jerry Jinfeng Guo",
            "Elmar Eisemann"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a general sample reweighting scheme and its underlying theory for\nthe integration of an unknown function with low dimensionality. Our method\nproduces better results than standard weighting schemes for common sampling\nstrategies, while avoiding bias. Our main insight is to link the weight\nderivation to the function reconstruction process during integration. The\nimplementation of our solution is simple and results in an improved convergence\nbehavior. We illustrate its benefit by applying our method to multiple Monte\nCarlo rendering problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.01809v1"
    },
    {
        "title": "Efficient Space Skipping and Adaptive Sampling of Unstructured Volumes\n  Using Hardware Accelerated Ray Tracing",
        "authors": [
            "Nathan Morrical",
            "Will Usher",
            "Ingo Wald",
            "Valerio Pascucci"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Sample based ray marching is an effective method for direct volume rendering\nof unstructured meshes. However, sampling such meshes remains expensive, and\nstrategies to reduce the number of samples taken have received relatively\nlittle attention. In this paper, we introduce a method for rendering\nunstructured meshes using a combination of a coarse spatial acceleration\nstructure and hardware-accelerated ray tracing. Our approach enables efficient\nempty space skipping and adaptive sampling of unstructured meshes, and\noutperforms a reference ray marcher by up to 7x.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.01906v1"
    },
    {
        "title": "Heterogeneous porous scaffold generation in trivariate B-spline solid\n  with triply periodic minimal surface in the parametric domain",
        "authors": [
            "Chuanfeng Hu",
            "Hongwei Lin"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  A porous scaffold is a three-dimensional network structure composed of a\nlarge number of pores, and triply periodic minimal surfaces (TPMSs) are one of\nconventional tools for designing porous scaffolds. However, discontinuity,\nincompleteness, and high storage space requirements are the three main\nshortcomings of TPMSs for porous scaffold design. In this study, we developed\nan effective method for heterogeneous porous scaffold generation to overcome\nthe abovementioned shortcomings of TPMSs. The input of the proposed method is a\ntrivariate B-spline solid (TBSS) with a cubic parameter domain. The proposed\nmethod first constructs a threshold distribution field (TDF) in the cubic\nparameter domain, and then produces a continuous and complete TPMS within it.\nMoreover, by mapping the TPMS in the parametric domain to the TBSS, a\ncontinuous and complete porous scaffold is generated in the TBSS. In addition,\nif the TBSS does not satisfy engineering requirements, the TDF can be locally\nmodified in the parameter domain, and the porous scaffold in the TBSS can be\nrebuilt. We also defined a new storage space-saving file format based on the\nTDF to store porous scaffolds. The experimental results presented in this paper\ndemonstrate the effectiveness and efficiency of the method using a TBSS as well\nas the superior space-saving of the proposed storage format.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.01938v1"
    },
    {
        "title": "Rendering Point Clouds with Compute Shaders",
        "authors": [
            "Markus Schütz",
            "Michael Wimmer"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a compute shader based point cloud rasterizer with up to 10 times\nhigher performance than classic point-based rendering with the GL_POINT\nprimitive. In addition to that, our rasterizer offers 5 byte depth-buffer\nprecision with uniform or customizable distribution, and we show that it is\npossible to implement a high-quality splatting method that blends together\noverlapping fragments while still maintaining higher frame-rates than the\ntraditional approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02681v1"
    },
    {
        "title": "Fast Tetrahedral Meshing in the Wild",
        "authors": [
            "Yixin Hu",
            "Teseo Schneider",
            "Bolun Wang",
            "Denis Zorin",
            "Daniele Panozzo"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a new tetrahedral meshing method, fTetWild, to convert triangle\nsoups into high-quality tetrahedral meshes. Our method builds on the TetWild\nalgorithm, replacing the rational triangle insertion with a new incremental\napproach to construct and optimize the output mesh, interleaving triangle\ninsertion and mesh optimization. Our approach makes it possible to maintain a\nvalid floating-point tetrahedral mesh at all algorithmic stages, eliminating\nthe need for costly constructions with rational numbers used by TetWild, while\nmaintaining full robustness and similar output quality. This allows us to\nimprove on TetWild in two ways. First, our algorithm is significantly faster,\nwith running time comparable to less robust Delaunay-based tetrahedralization\nalgorithms. Second, our algorithm is guaranteed to produce a valid tetrahedral\nmesh with floating-point vertex coordinates, while TetWild produces a valid\nmesh with rational coordinates which is not guaranteed to be valid after\nfloating-point conversion. As a trade-off, our algorithm no longer guarantees\nthat all input triangles are present in the output mesh, but in practice, as\nconfirmed by our tests on the Thingi10k dataset, the algorithm always succeeds\nin inserting all input triangles.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.03581v2"
    },
    {
        "title": "Convolutional Humanoid Animation via Deformation",
        "authors": [
            "John Kanji",
            "David I. W. Levin"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper we present a new deep learning-driven approach to image-based\nsynthesis of animations involving humanoid characters. Unlike previous deep\napproaches to image-based animation our method makes no assumptions on the type\nof motion to be animated nor does it require dense temporal input to produce\nmotion. Instead we generate new animations by interpolating between user chosen\nkeyframes, arranged sparsely in time. Utilizing a novel configuration manifold\nlearning approach we interpolate suitable motions between these keyframes. In\ncontrast to previous methods, ours requires less data (animations can be\ngenerated from a single youtube video) and is broadly applicable to a wide\nrange of motions including facial motion, whole body motion and even scenes\nwith multiple characters. These improvements serve to significantly reduce the\ndifficulty in producing image-based animations of humanoid characters, allowing\neven broader audiences to express their creativity.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.04338v1"
    },
    {
        "title": "Algebraic Representations for Volumetric Frame Fields",
        "authors": [
            "David Palmer",
            "David Bommes",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Field-guided parametrization methods have proven effective for quad meshing\nof surfaces; these methods compute smooth cross fields to guide the meshing\nprocess and then integrate the fields to construct a discrete mesh. A key\nchallenge in extending these methods to three dimensions, however, is\nrepresentation of field values. Whereas cross fields can be represented by\ntangent vector fields that form a linear space, the 3D analog---an octahedral\nframe field---takes values in a nonlinear manifold. In this work, we describe\nthe space of octahedral frames in the language of differential and algebraic\ngeometry. With this understanding, we develop geometry-aware tools for\noptimization of octahedral fields, namely geodesic stepping and exact\nprojection via semidefinite relaxation. Our algebraic approach not only\nprovides an elegant and mathematically-sound description of the space of\noctahedral frames but also suggests a generalization to frames whose three axes\nscale independently, better capturing the singular behavior we expect to see in\nvolumetric frame fields. These new odeco frames, so-called as they are\nrepresented by orthogonally decomposable tensors, also admit a semidefinite\nprogram--based projection operator. Our description of the spaces of octahedral\nand odeco frames suggests computing frame fields via manifold-based\noptimization algorithms; we show that these algorithms efficiently produce\nhigh-quality fields while maintaining stability and smoothness.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.05411v2"
    },
    {
        "title": "Extending editing capabilities of subdivision schemes by refinement of\n  point-normal pairs",
        "authors": [
            "Evgeny Lipovetsky",
            "Nira Dyn"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper we extend the 2D circle average of [11] to a 3D binary average\nof point-normal pairs, and study its properties. We modify classical\nsurface-generating linear subdivision schemes with this average obtaining\nsurface-generating schemes refining point-normal pairs. The modified schemes\ngive the possibility to generate more geometries by editing the initial\nnormals. For the case of input data consisting of a mesh only, we present a\nmethod for computing \"naive\" initial normals from the initial mesh. The\nperformance of several modified schemes is compared to their linear variants,\nwhen operating on the same initial mesh, and examples of the editing\ncapabilities of the modified schemes are given. In addition we provide a link\nto our repository, where we store the initial and refined mesh files, and the\nimplementation code. Several videos, demonstrating the editing capabilities of\nthe initial normals are provided in our Youtube channel.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.06154v1"
    },
    {
        "title": "Adding quadric fillets to quador lattice structures",
        "authors": [
            "Fehmi Cirak",
            "Malcolm Sabin"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Gupta et al. [1, 2] describe a very beautiful application of algebraic\ngeometry to lattice structures composed of quadric of revolution (quador)\nimplicit surfaces. However, the shapes created have concave edges where the\nstubs meet, and such edges can be stress-raisers which can cause significant\nproblems with, for instance, fatigue under cyclic loading. This note describes\na way in which quadric fillets can be added to these models, thus relieving\nthis problem while retaining their computational simplicity and efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.06974v2"
    },
    {
        "title": "DeepSketchHair: Deep Sketch-based 3D Hair Modeling",
        "authors": [
            "Yuefan Shen",
            "Changgeng Zhang",
            "Hongbo Fu",
            "Kun Zhou",
            "Youyi Zheng"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present sketchhair, a deep learning based tool for interactive modeling of\n3D hair from 2D sketches. Given a 3D bust model as reference, our sketching\nsystem takes as input a user-drawn sketch (consisting of hair contour and a few\nstrokes indicating the hair growing direction within a hair region), and\nautomatically generates a 3D hair model, which matches the input sketch both\nglobally and locally. The key enablers of our system are two carefully designed\nneural networks, namely, S2ONet, which converts an input sketch to a dense 2D\nhair orientation field; and O2VNet, which maps the 2D orientation field to a 3D\nvector field. Our system also supports hair editing with additional sketches in\nnew views. This is enabled by another deep neural network, V2VNet, which\nupdates the 3D vector field with respect to the new sketches. All the three\nnetworks are trained with synthetic data generated from a 3D hairstyle\ndatabase. We demonstrate the effectiveness and expressiveness of our tool using\na variety of hairstyles and also compare our method with prior art.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.07198v1"
    },
    {
        "title": "A Flexible Neural Renderer for Material Visualization",
        "authors": [
            "Aakash KT",
            "Parikshit Sakurikar",
            "Saurabh Saini",
            "P. J. Narayanan"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Photo realism in computer generated imagery is crucially dependent on how\nwell an artist is able to recreate real-world materials in the scene. The\nworkflow for material modeling and editing typically involves manual tweaking\nof material parameters and uses a standard path tracing engine for visual\nfeedback. A lot of time may be spent in iterative selection and rendering of\nmaterials at an appropriate quality. In this work, we propose a convolutional\nneural network based workflow which quickly generates high-quality ray traced\nmaterial visualizations on a shaderball. Our novel architecture allows for\ncontrol over environment lighting and assists material selection along with the\nability to render spatially-varying materials. Additionally, our network\nenables control over environment lighting which gives an artist more freedom\nand provides better visualization of the rendered material. Comparison with\nstate-of-the-art denoising and neural rendering techniques suggests that our\nneural renderer performs faster and better. We provide a interactive\nvisualization tool and release our training dataset to foster further research\nin this area.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.09530v1"
    },
    {
        "title": "Next Event Backtracking",
        "authors": [
            "Johannes Jendersie"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In light transport simulation, challenging situations are caused by the\nvariety of materials and the relative length of path segments. Path Tracing can\nhandle many situations and scales well to parallel hardware. However, it is not\nable to produce paths which have a smooth surface in connection with a small\nlight source. Here, photon transports perform superior, which can be\nineffective if the smooth object is small compared to the scene size.\n  We propose to use the last segment of a Path Tracer path as the first segment\nof a photon path. As a result, the strengths of next event estimation are\ninherited by the photon transport and photons are guided toward the regions\nwhere they are most useful. To that end, we developed a lock-free sparse\noctree, which we use for fast and robust density estimates. Summarizing, the\nnew method can outperform state of the art algorithms like Vertex Connection\nand Merging in certain scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.00573v1"
    },
    {
        "title": "Learning Elastic Constitutive Material and Damping Models",
        "authors": [
            "Bin Wang",
            "Yuanmin Deng",
            "Paul Kry",
            "Uri Ascher",
            "Hui Huang",
            "Baoquan Chen"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Commonly used linear and nonlinear constitutive material models in\ndeformation simulation contain many simplifications and only cover a tiny part\nof possible material behavior. In this work we propose a framework for learning\ncustomized models of deformable materials from example surface trajectories.\nThe key idea is to iteratively improve a correction to a nominal model of the\nelastic and damping properties of the object, which allows new forward\nsimulations with the learned correction to more accurately predict the behavior\nof a given soft object. Space-time optimization is employed to identify gentle\ncontrol forces with which we extract necessary data for model inference and to\nfinally encapsulate the material correction into a compact parametric form.\nFurthermore, a patch based position constraint is proposed to tackle the\nchallenge of handling incomplete and noisy observations arising in real-world\nexamples. We demonstrate the effectiveness of our method with a set of\nsynthetic examples, as well with data captured from real world homogeneous\nelastic objects.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.01875v2"
    },
    {
        "title": "Real-time Deformation with Coupled Cages and Skeletons",
        "authors": [
            "Fabrizio Corda",
            "Jean-Marc Thiery",
            "Marco Livesu",
            "Enrico Puppo",
            "Tamy Boubekeur",
            "Riccardo Scateni"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Skeleton-based and cage-based deformation techniques represent the two most\npopular approaches to control real-time deformations of digital shapes and are,\nto a vast extent, complementary to one another. Despite their complementary\nroles, high-end modeling packages do not allow for seamless integration of such\ncontrol structures, thus inducing a considerable burden on the user to maintain\nthem synchronized. In this paper, we propose a framework that seamlessly\ncombines rigging skeletons and deformation cages, granting artists with a\nreal-time deformation system that operates using any smooth combination of the\ntwo approaches. By coupling the deformation spaces of cages and skeletons, we\naccess a much larger space, containing poses that are impossible to obtain by\nacting solely on a skeleton or a cage. Our method is oblivious to the specific\ntechniques used to perform skinning and cage-based deformation, securing it\ncompatible with pre-existing tools. We demonstrate the usefulness of our hybrid\napproach on a variety of examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.02807v1"
    },
    {
        "title": "GLoG: Laplacian of Gaussian for Spatial Pattern Detection in\n  Spatio-Temporal Data",
        "authors": [
            "Luis Gustavo Nonato",
            "Fabiano Petronetto e Claudio Silva"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Boundary detection has long been a fundamental tool for image processing and\ncomputer vision, supporting the analysis of static and time-varying data. In\nthis work, we built upon the theory of Graph Signal Processing to propose a\nnovel boundary detection filter in the context of graphs, having as main\napplication scenario the visual analysis of spatio-temporal data. More\nspecifically, we propose the equivalent for graphs of the so-called Laplacian\nof Gaussian edge detection filter, which is widely used in image processing.\nThe proposed filter is able to reveal interesting spatial patterns while still\nenabling the definition of entropy of time slices. The entropy reveals the\ndegree of randomness of a time slice, helping users to identify expected and\nunexpected phenomena over time. The effectiveness of our approach appears in\napplications involving synthetic and real data sets, which show that the\nproposed methodology is able to uncover interesting spatial and temporal\nphenomena. The provided examples and case studies make clear the usefulness of\nour approach as a mechanism to support visual analytic tasks involving\nspatio-temporal data.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03993v1"
    },
    {
        "title": "PTRM: Perceived Terrain Realism Metrics",
        "authors": [
            "Suren Deepak Rajasekaran",
            "Hao Kang",
            "Bedrich Benes",
            "Martin Čadík",
            "Eric Galin",
            "Eric Guérin",
            "Adrien Peytavie",
            "Pavel Slavík"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Terrains are visually important and commonly used in computer graphics. While\nmany algorithms for their generation exist, it is difficult to assess the\nrealism of a generated terrain. This paper presents a first step in the\ndirection of perceptual evaluation of terrain models. We gathered and\ncategorized several classes of real terrains and we generated synthetic\nterrains by using methods from computer graphics. We then conducted two large\nstudies ranking the terrains perceptually and showing that the synthetic\nterrains are perceived as lacking realism as compared to the real ones. Then we\nprovide insight into the features that affect the perceived realism by a\nquantitative evaluation based on localized geomorphology-based landform\nfeatures (geomorphons) that categorize terrain structures such as valleys,\nridges, hollows, etc. We show that the presence or absence of certain features\nhave a significant perceptual effect. We then introduce Perceived Terrain\nRealism Metrics (PTRM); a perceptual metrics that estimates perceived realism\nof a terrain represented as a digital elevation map by relating distribution of\nterrain features with their perceived realism. We validated PTRM on real and\nsynthetic data and compared it to the perceptual studies. To confirm the\nimportance of the presence of these features, we used a generative deep neural\nnetwork to transfer them between real terrains and synthetic ones and we\nperformed another perceptual experiment that further confirmed their importance\nfor perceived realism.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.04610v1"
    },
    {
        "title": "Measures in Visualization Space",
        "authors": [
            "Fabian Bolte",
            "Stefan Bruckner"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Measurement is an integral part of modern science, providing the fundamental\nmeans for evaluation, comparison, and prediction. In the context of\nvisualization, several different types of measures have been proposed, ranging\nfrom approaches that evaluate particular aspects of individual visualization\ntechniques, their perceptual characteristics, and even economic factors.\nFurthermore, there are approaches that attempt to provide means for measuring\ngeneral properties of the visualization process as a whole. Measures can be\nquantitative or qualitative, and one of the primary goals is to provide\nobjective means for reasoning about visualizations and their effectiveness. As\nsuch, they play a central role in the development of scientific theories for\nvisualization. In this chapter, we provide an overview of the current state of\nthe art, survey and classify different types of visualization measures,\ncharacterize their strengths and drawbacks, and provide an outline of open\nchallenges for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.05295v1"
    },
    {
        "title": "LOCALIS: Locally-adaptive Line Simplification for GPU-based Geographic\n  Vector Data Visualization",
        "authors": [
            "Alireza Amiraghdam",
            "Alexandra Diehl",
            "Renato Pajarola"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Visualization of large vector line data is a core task in geographic and\ncartographic systems. Vector maps are often displayed at different cartographic\ngeneralization levels, traditionally by using several discrete levels-of-detail\n(LODs). This limits the generalization levels to a fixed and predefined set of\nLODs, and generally does not support smooth LOD transitions. However, fast GPUs\nand novel line rendering techniques can be exploited to integrate dynamic\nvector map LOD management into GPU-based algorithms for locally-adaptive line\nsimplification and real-time rendering. We propose a new technique that\ninteractively visualizes large line vector datasets at variable LODs. It is\nbased on the Douglas-Peucker line simplification principle, generating an\nexhaustive set of line segments whose specific subsets represent the lines at\nany variable LOD. At run time, an appropriate and view-dependent error metric\nsupports screen-space adaptive LOD levels and the display of the correct subset\nof line segments accordingly. Our implementation shows that we can simplify and\ndisplay large line datasets interactively. We can successfully apply line style\npatterns, dynamic LOD selection lenses, and anti-aliasing techniques to our\nline rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.05511v2"
    },
    {
        "title": "Voting for Distortion Points in Geometric Processing",
        "authors": [
            "Shuangming Chai",
            "Xiao-Ming Fu",
            "Ligang Liu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Low isometric distortion is often required for mesh parameterizations. A\nconfiguration of some vertices, where the distortion is concentrated, provides\na way to mitigate isometric distortion, but determining the number and\nplacement of these vertices is non-trivial. We call these vertices distortion\npoints. We present a novel and automatic method to detect distortion points\nusing a voting strategy. Our method integrates two components: candidate\ngeneration and candidate voting. Given a closed triangular mesh, we generate\ncandidate distortion points by executing a three-step procedure repeatedly: (1)\nrandomly cut an input to a disk topology; (2) compute a low conformal\ndistortion parameterization; and (3) detect the distortion points. Finally, we\ncount the candidate points and generate the final distortion points by voting.\nWe demonstrate that our algorithm succeeds when employed on various closed\nmeshes with a genus of zero or higher. The distortion points generated by our\nmethod are utilized in three applications, including planar parameterization,\nsemi-automatic landmark correspondence, and isotropic remeshing. Compared to\nother state-of-the-art methods, our method demonstrates stronger practical\nrobustness in distortion point detection.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.13066v3"
    },
    {
        "title": "Shape Analysis via Functional Map Construction and Bases Pursuit",
        "authors": [
            "Omri Azencot",
            "Rongjie Lai"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a method to simultaneously compute scalar basis functions with an\nassociated functional map for a given pair of triangle meshes. Unlike previous\ntechniques that put emphasis on smoothness with respect to the\nLaplace--Beltrami operator and thus favor low-frequency eigenfunctions, we aim\nfor a spectrum that allows for better feature matching. This change of\nperspective introduces many degrees of freedom into the problem which we\nexploit to improve the accuracy of our computed correspondences. To effectively\nsearch in this high dimensional space of solutions, we incorporate into our\nminimization state-of-the-art regularizers. We solve the resulting highly\nnon-linear and non-convex problem using an iterative scheme via the Alternating\nDirection Method of Multipliers. At each step, our optimization involves simple\nto solve linear or Sylvester-type equations. In practice, our method performs\nwell in terms of convergence, and we additionally show that it is similar to a\nprovably convergent problem. We show the advantages of our approach by\nextensively testing it on multiple datasets in a few applications including\nshape matching, consistent quadrangulation and scalar function transfer.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.13200v1"
    },
    {
        "title": "Locking-free Simulation of Isometric Thin Plates",
        "authors": [
            "Hsiao-yu Chen",
            "Paul Kry",
            "Etienne Vouga"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  To efficiently simulate very thin, inextensible materials like cloth or\npaper, it is tempting to replace force-based thin-plate dynamics with hard\nisometry constraints. Unfortunately, naive formulations of the constraints\ninduce membrane locking---artificial stiffening of bending modes due to the\ninability of discrete kinematics to reproduce exact isometries. We propose a\nsimple set of meshless isometry constraints, based on moving-least-squares\naveraging of the strain tensor, which do not lock, and which can be easily\nincorporated into standard constrained Lagrangian dynamics integration.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.05204v1"
    },
    {
        "title": "Point Movement in a DSL for Higher-Order FEM Visualization",
        "authors": [
            "Teodoro Collin",
            "Charisee Chiw",
            "L. Ridgway Scott",
            "John Reppy",
            "Gordon L. Kindlmann"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Scientific visualization tools tend to be flexible in some ways (e.g., for\nexploring isovalues) while restricted in other ways, such as working only on\nregular grids, or only on unstructured meshes (as used in the finite element\nmethod, FEM). Our work seeks to expose the common structure of visualization\nmethods, apart from the specifics of how the fields being visualized are\nformed. Recognizing that previous approaches to FEM visualization depend on\nefficiently updating computed positions within a mesh, we took an existing\nvisualization domain-specific language, and added a mesh position type and\nassociated arithmetic operators. These are orthogonal to the visualization\nmethod itself, so existing programs for visualizing regular grid data work,\nwith minimal changes, on higher-order FEM data. We reproduce the efficiency\ngains of an earlier guided search method of mesh position update for computing\nstreamlines, and we demonstrate a novel ability to uniformly sample ridge\nsurfaces of higher-order FEM solutions defined on curved meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.05668v1"
    },
    {
        "title": "Efficient Direct Slicing Of Dilated And Eroded 3d Models For Additive\n  Manufacturing: Technical Report",
        "authors": [
            "Sylvain Lefebvre"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In the context of additive manufacturing we present a novel technique for\ndirect slicing of a dilated or eroded volume, where the input volume boundary\nis a triangle mesh. Rather than computing a 3D model of the boundary of the\ndilated or eroded volume, our technique directly produces its slices. This\nleads to a computationally and memory efficient algorithm, which is\nembarrassingly parallel. Contours can be extracted under an arbitrary chord\nerror, non-uniform dilation or erosion are also possible. Finally, the scheme\nis simple and robust to implement.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.05992v1"
    },
    {
        "title": "Efficient Animation of Sparse Voxel Octrees for Real-Time Ray Tracing",
        "authors": [
            "Asbjørn Engmark Espe",
            "Øystein Gjermundnes",
            "Sverre Hendseth"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  A considerable limitation of employing sparse voxels octrees (SVOs) as a\nmodel format for ray tracing has been that the octree data structure is\ninherently static. Due to traversal algorithms' dependence on the strict\nhierarchical structure of octrees, it has been challenging to achieve real-time\nperformance of SVO model animation in ray tracing since the octree data\nstructure would typically have to be regenerated every frame. Presented in this\narticle is a novel method for animation of models specified on the SVO format.\nThe method distinguishes itself by permitting model transformations such as\nrotation, translation, and anisotropic scaling, while preserving the\nhierarchical structure of SVO models so that they may be efficiently traversed.\nDue to its modest memory footprint and straightforward arithmetic operations,\nthe method is well-suited for implementation in hardware. A software ray\ntracing implementation of animated SVO models demonstrates real-time\nperformance on current-generation desktop GPUs, and shows that the animation\nmethod does not substantially slow down the rendering procedure compared to\nrendering static SVOs.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.06001v2"
    },
    {
        "title": "Applying Rational Envelope curves for skinning purposes",
        "authors": [
            "Kinga Kruppa"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Special curves in the Minkowski space such as Minkowski Pythagorean\nhodographs play an important role in Computer Aided Geometric Design, and their\nusages have been thoroughly studied in the recent years. Also, several papers\nhave been published which describe methods for interpolating Hermite data in\nR2,1 by MPH curves. Bizzarri et al.introduced the class of RE curves and\npresented an interpolation method for G1 Hermite data, where the resulting RE\ncurve yields a rational boundary for the represented domain. We now propose a\nnew application area for RE curves: skinning of a discrete set of input\ncircles. We find the appropriate Hermite data to interpolate so that the\nobtained rational envelope curves touch each circle at previously defined\npoints of contact. This way we overcome the problematic scenarios when the\nlocation of the touching points would not be appropriate for skinning purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.06906v1"
    },
    {
        "title": "Hierarchical Optimization Time Integration for CFL-rate MPM Stepping",
        "authors": [
            "Xinlei Wang",
            "Minchen Li",
            "Yu Fang",
            "Xinxin Zhang",
            "Ming Gao",
            "Min Tang",
            "Danny M. Kaufman",
            "Chenfanfu Jiang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose Hierarchical Optimization Time Integration (HOT) for efficient\nimplicit time-stepping of the Material Point Method (MPM) irrespective of\nsimulated materials and conditions. HOT is an MPM-specialized hierarchical\noptimization algorithm that solves nonlinear time step problems for large-scale\nMPM systems near the CFL-limit. HOT provides convergent simulations\n\"out-of-the-box\" across widely varying materials and computational resolutions\nwithout parameter tuning. As an implicit MPM time stepper accelerated by a\ncustom-designed Galerkin multigrid wrapped in a quasi-Newton solver, HOT is\nboth highly parallelizable and robustly convergent. As we show in our analysis,\nHOT maintains consistent and efficient performance even as we grow stiffness,\nincrease deformation, and vary materials over a wide range of finite strain,\nelastodynamic and plastic examples. Through careful benchmark ablation studies,\nwe compare the effectiveness of HOT against seemingly plausible alternative\ncombinations of MPM with standard multigrid and other Newton-Krylov models. We\nshow how these alternative designs result in severe issues and poor\nperformance. In contrast, HOT outperforms the existing state-of-the-art,\nheavily optimized implicit MPM codes with an up to 10x performance speedup\nacross a wide range of challenging benchmark test simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.07913v3"
    },
    {
        "title": "Inattentional Blindness for Redirected Walking Using Dynamic Foveated\n  Rendering",
        "authors": [
            "Yashas Joshi",
            "Charalambos Poullis"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Redirected walking is a Virtual Reality(VR) locomotion technique which\nenables users to navigate virtual environments (VEs) that are spatially larger\nthan the available physical tracked space. In this work we present a novel\ntechnique for redirected walking in VR based on the psychological phenomenon of\ninattentional blindness. Based on the user's visual fixation points we divide\nthe user's view into zones. Spatially-varying rotations are applied according\nto the zone's importance and are rendered using foveated rendering. Our\ntechnique is real-time and applicable to small and large physical spaces.\nFurthermore, the proposed technique does not require the use of stimulated\nsaccades but rather takes advantage of naturally occurring saccades and blinks\nfor a complete refresh of the framebuffer. We performed extensive testing and\npresent the analysis of the results of three user studies conducted for the\nevaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.12327v1"
    },
    {
        "title": "Integrating Deep Learning into CAD/CAE System: Generative Design and\n  Evaluation of 3D Conceptual Wheel",
        "authors": [
            "Soyoung Yoo",
            "Sunghee Lee",
            "Seongsin Kim",
            "Kwang Hyeon Hwang",
            "Jong Ho Park",
            "Namwoo Kang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Engineering design research integrating artificial intelligence (AI) into\ncomputer-aided design (CAD) and computer-aided engineering (CAE) is actively\nbeing conducted. This study proposes a deep learning-based CAD/CAE framework in\nthe conceptual design phase that automatically generates 3D CAD designs and\nevaluates their engineering performance. The proposed framework comprises seven\nstages: (1) 2D generative design, (2) dimensionality reduction, (3) design of\nexperiment in latent space, (4) CAD automation, (5) CAE automation, (6)\ntransfer learning, and (7) visualization and analysis. The proposed framework\nis demonstrated through a road wheel design case study and indicates that AI\ncan be practically incorporated into an end-use product design project.\nEngineers and industrial designers can jointly review a large number of\ngenerated 3D CAD models by using this framework along with the engineering\nperformance results estimated by AI and find conceptual design candidates for\nthe subsequent detailed design stage.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.02138v3"
    },
    {
        "title": "MapTree: Recovering Multiple Solutions in the Space of Maps",
        "authors": [
            "Jing Ren",
            "Simone Melzi",
            "Maks Ovsjanikov",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper we propose an approach for computing multiple high-quality\nnear-isometric dense correspondences between a pair of 3D shapes. Our method is\nfully automatic and does not rely on user-provided landmarks or descriptors.\nThis allows us to analyze the full space of maps and extract multiple diverse\nand accurate solutions, rather than optimizing for a single optimal\ncorrespondence as done in most previous approaches. To achieve this, we propose\na compact tree structure based on the spectral map representation for encoding\nand enumerating possible rough initializations, and a novel efficient approach\nfor refining them to dense pointwise maps. This leads to a new method capable\nof both producing multiple high-quality correspondences across shapes and\nrevealing the symmetry structure of a shape without a priori information. In\naddition, we demonstrate through extensive experiments that our method is\nrobust and results in more accurate correspondences than state-of-the-art for\nshape matching and symmetry detection.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.02532v2"
    },
    {
        "title": "Towards 3D Dance Motion Synthesis and Control",
        "authors": [
            "Wenlin Zhuang",
            "Yangang Wang",
            "Joseph Robinson",
            "Congyi Wang",
            "Ming Shao",
            "Yun Fu",
            "Siyu Xia"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  3D human dance motion is a cooperative and elegant social movement. Unlike\nregular simple locomotion, it is challenging to synthesize artistic dance\nmotions due to the irregularity, kinematic complexity and diversity. It\nrequires the synthesized dance is realistic, diverse and controllable. In this\npaper, we propose a novel generative motion model based on temporal convolution\nand LSTM,TC-LSTM, to synthesize realistic and diverse dance motion. We\nintroduce a unique control signal, dance melody line, to heighten\ncontrollability. Hence, our model, and its switch for control signals, promote\na variety of applications: random dance synthesis, music-to-dance, user\ncontrol, and more. Our experiments demonstrate that our model can synthesize\nartistic dance motion in various dance types. Compared with existing methods,\nour method achieved start-of-the-art results.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.05743v1"
    },
    {
        "title": "Least-Squares Affine Reflection Using Eigen Decomposition",
        "authors": [
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This note summarizes the steps to computing the best-fitting affine\nreflection that aligns two sets of corresponding points.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.06080v1"
    },
    {
        "title": "Repulsive Curves",
        "authors": [
            "Christopher Yu",
            "Henrik Schumacher",
            "Keenan Crane"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Curves play a fundamental role across computer graphics, physical simulation,\nand mathematical visualization, yet most tools for curve design do nothing to\nprevent crossings or self-intersections. This paper develops efficient\nalgorithms for (self-)repulsion of plane and space curves that are well-suited\nto problems in computational design. Our starting point is the so-called\ntangent-point energy, which provides an infinite barrier to self-intersection.\nIn contrast to local collision detection strategies used in, e.g., physical\nsimulation, this energy considers interactions between all pairs of points, and\nis hence useful for global shape optimization: local minima tend to be\naesthetically pleasing, physically valid, and nicely distributed in space. A\nreformulation of gradient descent, based on a Sobolev-Slobodeckij inner product\nenables us to make rapid progress toward local minima---independent of curve\nresolution. We also develop a hierarchical multigrid scheme that significantly\nreduces the per-step cost of optimization. The energy is easily integrated with\na variety of constraints and penalties (e.g., inextensibility, or obstacle\navoidance), which we use for applications including curve packing, knot\nuntangling, graph embedding, non-crossing spline interpolation, flow\nvisualization, and robotic path planning.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.07859v1"
    },
    {
        "title": "EMU: Efficient Muscle Simulation In Deformation Space",
        "authors": [
            "Vismay Modi",
            "Lawson Fulton",
            "Shinjiro Sueda",
            "Alec Jacobson",
            "David I. W. Levin"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  EMU is an efficient and scalable model to simulate bulk musculoskeletal\nmotion with heterogenous materials. First, EMU requires no model reductions, or\ngeometric coarsening, thereby producing results visually accurate when compared\nto an FEM simulation. Second, EMU is efficient and scales much better than\nstate-of-the-art FEM with the number of elements in the mesh, and is more\neasily parallelizable. Third, EMU can handle heterogeneously stiff meshes with\nan arbitrary constitutive model, thus allowing it to simulate soft muscles,\nstiff tendons and even stiffer bones all within one unified system. These three\nkey characteristics of EMU enable us to efficiently orchestrate muscle\nactivated skeletal movements. We demonstrate the efficacy of our approach via a\nnumber of examples with tendons, muscles, bones and joints.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.08821v4"
    },
    {
        "title": "An Evolutional Algorithm for Automatic 2D Layer Segmentation in\n  Laser-aided Additive Manufacturing",
        "authors": [
            "N. Liu",
            "K. Ren",
            "W. Zhang",
            "Y. F. Zhang",
            "Y. X. Chew",
            "J. Y. H. Fuh",
            "G. J. Bi"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Toolpath planning is an important task in laser aided additive manufacturing\n(LAAM) and other direct energy deposition (DED) processes. The deposition\ntoolpaths for complex geometries with slender structures can be further\noptimized by partitioning the sliced 2D layers into sub-regions, and enable the\ndesign of appropriate infill toolpaths for different sub-regions. However,\nreported approaches for 2D layer segmentation generally require manual\noperations that are tedious and time-consuming. To increase segmentation\nefficiency, this paper proposes an autonomous approach based on evolutional\ncomputation for 2D layer segmentation. The algorithm works in an\nidentify-and-segment manner. Specifically, the largest quasi-quadrilateral is\nidentified and segmented from the target layer iteratively. Results from case\nstudies have validated the effectiveness and efficacy of the developed\nalgorithm. To further improve its performance, a roughing-finishing strategy is\nproposed. Via multi-processing, the strategy can remarkably increase the\nsolution variety without affecting solution quality and search time, thus\nproviding great application potential in LAAM toolpath planning. To the best of\nthe authors knowledge, this work is the first to address automatic 2D layer\nsegmentation problem in LAAM process. Therefore, it may be a valuable\nsupplement to the state of the art in this area.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.09819v3"
    },
    {
        "title": "Ray-VR: Ray Tracing Virtual Reality in Falcor",
        "authors": [
            "Vinicius da Silva",
            "Luiz Velho"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  NVidia RTX platform has been changing and extending the possibilities for\nreal time Computer Graphics applications. It is the first time in history that\nretail graphics cards have full hardware support for ray tracing primitives. It\nstill a long way to fully understand and optimize its use and this task itself\nis a fertile field for scientific progression. However, another path is to\nexplore the platform as an expansion of paradigms for other problems. For\nexample, the integration of real time Ray Tracing and Virtual Reality can\nresult in interesting applications for visualization of Non-Euclidean Geometry\nand 3D Manifolds. In this paper we present Ray-VR, a novel algorithm for real\ntime stereo ray tracing, constructed on top of Falcor, NVidia's scientific\nprototyping framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.11348v1"
    },
    {
        "title": "Technical Note: Generating Realistic Fighting Scenes by Game Tree",
        "authors": [
            "Hubert P. H. Shum",
            "Taku Komura"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Recently, there have been a lot of researches to synthesize / edit the motion\nof a single avatar in the virtual environment. However, there has not been so\nmuch work of simulating continuous interactions of multiple avatars such as\nfighting. In this paper, we propose a new method to generate a realistic\nfighting scene based on motion capture data. We propose a new algorithm called\nthe temporal expansion approach which maps the continuous time action plan to a\ndiscrete causality space such that turn-based evaluation methods can be used.\nAs a result, it is possible to use many mature algorithms available in strategy\ngames such as the Minimax algorithm and $\\alpha-\\beta$ pruning. We also propose\na method to generate and use an offense/defense table, which illustrates the\nspatial-temporal relationship of attacks and dodges, to incorporate tactical\nmaneuvers of defense into the scene. Using our method, avatars will plan their\nstrategies taking into account the reaction of the opponent. Fighting scenes\nwith multiple avatars are generated to demonstrate the effectiveness of our\nalgorithm. The proposed method can also be applied to other kinds of continuous\nactivities that require strategy planning such as sport games.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.11620v1"
    },
    {
        "title": "SN-Engine, a Scale-free Geometric Modelling Environment",
        "authors": [
            "T. A. Zhukov"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a new scale-free geometric modelling environment designed by the\nauthor of the paper. It allows one to consistently treat geometric objects of\narbitrary size and offers extensive analytic and computational support for\nvisualization of both real and artificial sceneries.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.12661v1"
    },
    {
        "title": "Augmenting Image Warping-Based Remote Volume Rendering with Ray Tracing",
        "authors": [
            "Stefan Zellmann"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose an image warping-based remote rendering technique for volumes that\ndecouples the rendering and display phases. Our work builds on prior work that\nsamples the volume on the client using ray casting and reconstructs a z-value\nbased on some heuristic. The color and depth buffer are then sent to the client\nthat reuses this depth image as a stand-in for subsequent frames by warping it\naccording to the current camera position until new data was received from the\nserver. We augment that method by implementing the client renderer using ray\ntracing. By representing the pixel contributions as spheres, this allows us to\neffectively vary their footprint based on the distance to the viewer, which we\nfind to give better results than point-based rasterization when applied to\nvolumetric data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.14726v1"
    },
    {
        "title": "DNF-Net: a Deep Normal Filtering Network for Mesh Denoising",
        "authors": [
            "Xianzhi Li",
            "Ruihui Li",
            "Lei Zhu",
            "Chi-Wing Fu",
            "Pheng-Ann Heng"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper presents a deep normal filtering network, called DNF-Net, for mesh\ndenoising. To better capture local geometry, our network processes the mesh in\nterms of local patches extracted from the mesh. Overall, DNF-Net is an\nend-to-end network that takes patches of facet normals as inputs and directly\noutputs the corresponding denoised facet normals of the patches. In this way,\nwe can reconstruct the geometry from the denoised normals with feature\npreservation. Besides the overall network architecture, our contributions\ninclude a novel multi-scale feature embedding unit, a residual learning\nstrategy to remove noise, and a deeply-supervised joint loss function. Compared\nwith the recent data-driven works on mesh denoising, DNF-Net does not require\nmanual input to extract features and better utilizes the training data to\nenhance its denoising performance. Finally, we present comprehensive\nexperiments to evaluate our method and demonstrate its superiority over the\nstate of the art on both synthetic and real-scanned meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.15510v1"
    },
    {
        "title": "Simple Methods to Represent Shapes with Sample Spheres",
        "authors": [
            "Li-Yi Wei",
            "Arjun V Anand",
            "Shally Kumar",
            "Tarun Beri"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Representing complex shapes with simple primitives in high accuracy is\nimportant for a variety of applications in computer graphics and geometry\nprocessing. Existing solutions may produce suboptimal samples or are complex to\nimplement. We present methods to approximate given shapes with user-tunable\nnumber of spheres to balance between accuracy and simplicity: touching\nmedial/scale-axis polar balls and k-means smallest enclosing circles. Our\nmethods are easy to implement, run efficiently, and can approach quality\nsimilar to manual construction.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.02135v1"
    },
    {
        "title": "Perceptual error optimization for Monte Carlo rendering",
        "authors": [
            "Vassillen Chizhov",
            "Iliyan Georgiev",
            "Karol Myszkowski",
            "Gurprit Singh"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Synthesizing realistic images involves computing high-dimensional\nlight-transport integrals. In practice, these integrals are numerically\nestimated via Monte Carlo integration. The error of this estimation manifests\nitself as conspicuous aliasing or noise. To ameliorate such artifacts and\nimprove image fidelity, we propose a perception-oriented framework to optimize\nthe error of Monte Carlo rendering. We leverage models based on human\nperception from the halftoning literature. The result is an optimization\nproblem whose solution distributes the error as visually pleasing blue noise in\nimage space. To find solutions, we present a set of algorithms that provide\nvarying trade-offs between quality and speed, showing substantial improvements\nover prior state of the art. We perform evaluations using quantitative and\nerror metrics, and provide extensive supplemental material to demonstrate the\nperceptual improvements achieved by our methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.02344v6"
    },
    {
        "title": "EasyPBR: A Lightweight Physically-Based Renderer",
        "authors": [
            "Radu Alexandru Rosu",
            "Sven Behnke"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Modern rendering libraries provide unprecedented realism, producing real-time\nphotorealistic 3D graphics on commodity hardware. Visual fidelity, however,\ncomes at the cost of increased complexity and difficulty of usage, with many\nrendering parameters requiring a deep understanding of the pipeline. We propose\nEasyPBR as an alternative rendering library that strikes a balance between\nease-of-use and visual quality. EasyPBR consists of a deferred renderer that\nimplements recent state-of-the-art approaches in physically based rendering. It\noffers an easy-to-use Python and C++ interface that allows high-quality images\nto be created in only a few lines of code or directly through a graphical user\ninterface. The user can choose between fully controlling the rendering pipeline\nor letting EasyPBR automatically infer the best parameters based on the current\nscene composition. The EasyPBR library can help the community to more easily\nleverage the power of current GPUs to create realistic images. These can then\nbe used as synthetic data for deep learning or for creating animations for\nacademic purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.03325v1"
    },
    {
        "title": "Codimensional Incremental Potential Contact",
        "authors": [
            "Minchen Li",
            "Danny M. Kaufman",
            "Chenfanfu Jiang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We extend the incremental potential contact (IPC) model for contacting\nelastodynamics to resolve systems composed of codimensional DOFs in arbitrary\ncombination. This enables a unified, interpenetration-free, robust, and stable\nsimulation framework that couples codimension-0,1,2, and 3 geometries\nseamlessly with frictional contact. Extending IPC to thin structures poses new\nchallenges in computing strain, modeling thickness and determining collisions.\nTo address these challenges we propose three corresponding contributions.\nFirst, we introduce a C2 constitutive barrier model that directly enforces\nstrain limiting as an energy potential while preserving rest state. This\nprovides energetically-consistent strain limiting models (both isotropic and\nanisotropic) for cloth that enable strict satisfaction of strain-limit\ninequalities with direct coupling to both elastodynamics and contact via\nminimization of the incremental potential. Second, to capture the geometric\nthickness of codimensional domains we extend the IPC model to directly enforce\ndistance offsets. Our treatment imposes a strict guarantee that mid-surfaces\n(resp. mid-lines) of shells (resp. rods) will not move closer than applied\nthickness values. This enables us to account for thickness in the contact\nbehavior of codimensional structures and so robustly capture challenging\ncontacting geometries; a number of which, to our knowledge, have not been\nsimulated before. Third, codimensional models, especially with modeled\nthickness, mandate strict accuracy requirements that pose a severe challenge to\nall existing continuous collision detection (CCD) methods. To address these\nlimitations we develop a new, efficient, simple-to-implement additive CCD\n(ACCD) method that applies conservative advancement to iteratively refine a\nlower bound for deforming primitives, converging to time of impact.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.04457v3"
    },
    {
        "title": "Compressed Bounding Volume Hierarchies for Collision Detection &\n  Proximity Query",
        "authors": [
            "Toni Tan",
            "Rene Weller",
            "Gabriel Zachmann"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a novel representation of compressed data structure for\nsimultaneous bounding volume hierarchy (BVH) traversals like they appear for\ninstance in collision detection & proximity query. The main idea is to compress\nbounding volume (BV) descriptors and cluster BVH into a smaller parts 'treelet'\nthat fit into CPU cache while at the same time maintain random-access and\nautomatic cache-aware data structure layouts. To do that, we quantify BV and\ncompress 'treelet' using predictor-corrector scheme with the predictor at a\nspecific node in the BVH based on the chain of BVs upwards.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.05348v1"
    },
    {
        "title": "Continuous Curve Textures",
        "authors": [
            "Peihan Tu",
            "Li-Yi Wei",
            "Koji Yatani",
            "Takeo Igarashi",
            "Matthias Zwicker"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Repetitive patterns are ubiquitous in natural and human-made objects, and can\nbe created with a variety of tools and methods. Manual authoring provides\nunmatched degree of freedom and control, but can require significant artistic\nexpertise and manual labor. Computational methods can automate parts of the\nmanual creation process, but are mainly tailored for discrete pixels or\nelements instead of more general continuous structures. We propose an\nexample-based method to synthesize continuous curve patterns from exemplars.\nOur main idea is to extend prior sample-based discrete element synthesis\nmethods to consider not only sample positions (geometry) but also their\nconnections (topology). Since continuous structures can exhibit higher\ncomplexity than discrete elements, we also propose robust, hierarchical\nsynthesis to enhance output quality. Our algorithm can generate a variety of\ncontinuous curve patterns fully automatically. For further quality improvement\nand customization, we also present an autocomplete user interface to facilitate\ninteractive creation and iterative editing. We evaluate our methods and\ninterface via different patterns, ablation studies, and comparisons with\nalternative methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.07959v1"
    },
    {
        "title": "Proceduray -- A light-weight engine for procedural primitive ray tracing",
        "authors": [
            "Vinícius da Silva",
            "Tiago Novello",
            "Hélio Lopes",
            "Luiz Velho"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce Proceduray, an engine for real-time ray tracing of procedural\ngeometry. Its motivation is the current lack of mid-level abstraction tools for\nscenes with primitives involving intersection shaders. Those scenes impose\nstrict engine design choices since they need flexibility in the shader table\nsetup. Proceduray aims at providing a fair tradeoff between that flexibility\nand productivity. It also aims to be didactic. Shader table behavior can be\nconfusing because parameters for indexing come from different parts of a\nsystem, involving both host and device code. This is different in essence from\nray tracing triangle meshes (which must use a built-in intersection shader for\nall objects) or rendering with the traditional graphics or compute pipelines.\nAdditionals goals of the project include fomenting deeper discussions about\nDirectX RayTracing (DXR) host code and providing a good starting point for\ndevelopers trying to deal with procedural geometry using DXR.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.10357v2"
    },
    {
        "title": "Visualization of topology optimization designs with representative\n  subset selection",
        "authors": [
            "Daniel J Perry",
            "Vahid Keshavarzzadeh",
            "Shireen Y Elhabian",
            "Robert M Kirby",
            "Michael Gleicher",
            "Ross T Whitaker"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  An important new trend in additive manufacturing is the use of optimization\nto automatically design industrial objects, such as beams, rudders or wings.\nTopology optimization, as it is often called, computes the best configuration\nof material over a 3D space, typically represented as a grid, in order to\nsatisfy or optimize physical parameters. Designers using these automated\nsystems often seek to understand the interaction of physical constraints with\nthe final design and its implications for other physical characteristics. Such\nunderstanding is challenging because the space of designs is large and small\nchanges in parameters can result in radically different designs. We propose to\naddress these challenges using a visualization approach for exploring the space\nof design solutions. The core of our novel approach is to summarize the space\n(ensemble of solutions) by automatically selecting a set of examples and to\nrepresent the complete set of solutions as combinations of these examples. The\nrepresentative examples create a meaningful parameterization of the design\nspace that can be explored using standard visualization techniques for\nhigh-dimensional spaces. We present evaluations of our subset selection\ntechnique and that the overall approach addresses the needs of expert\ndesigners.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.14901v1"
    },
    {
        "title": "Hybrid Function Representation for Heterogeneous Objects",
        "authors": [
            "A. Tereshin",
            "A. Pasko",
            "O. Fryazinov",
            "V. Adzhiev"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Heterogeneous object modelling is an emerging area where geometric shapes are\nconsidered in concert with their internal physically-based attributes. This\npaper describes a novel theoretical and practical framework for modelling\nvolumetric heterogeneous objects on the basis of a novel unifying\nfunctionally-based hybrid representation called HFRep. This new representation\nallows for obtaining a continuous smooth distance field in Euclidean space and\npreserves the advantages of the conventional representations based on scalar\nfields of different kinds without their drawbacks. We systematically describe\nthe mathematical and algorithmic basics of HFRep. The steps of the basic\nalgorithm are presented in detail for both geometry and attributes. To solve\nsome problematic issues, we have suggested several practical solutions,\nincluding a new algorithm for solving the eikonal equation on hierarchical\ngrids. Finally, we show the practicality of the approach by modelling several\nrepresentative heterogeneous objects, including those of a time-variant nature.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.15176v1"
    },
    {
        "title": "PAVEL: Decorative Patterns with Packed Volumetric Elements",
        "authors": [
            "Filippo Andrea Fanni",
            "Fabio Pellacini",
            "Riccardo Scateni",
            "Andrea Giachetti"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Many real-world hand-crafted objects are decorated with elements that are\npacked onto the object's surface and deformed to cover it as much as possible.\nExamples are artisanal ceramics and metal jewelry. Inspired by these objects,\nwe present a method to enrich surfaces with packed volumetric decorations. Our\nalgorithm works by first determining the locations in which to add the\ndecorative elements and then removing the non-physical overlap between them\nwhile preserving the decoration volume. For the placement, we support several\nstrategies depending on the desired overall motif. To remove the overlap, we\nuse an approach based on implicit deformable models creating the qualitative\neffect of plastic warping while avoiding expensive and hard-to-control physical\nsimulations. Our decorative elements can be used to enhance virtual surfaces,\nas well as 3D-printed pieces, by assembling the decorations onto real-surfaces\nto obtain tangible reproductions.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.01029v1"
    },
    {
        "title": "Blue Noise Plots",
        "authors": [
            "Christian van Onzenoodt",
            "Gurprit Singh",
            "Timo Ropinski",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We propose Blue Noise Plots, two-dimensional dot plots that depict data\npoints of univariate data sets. While often one-dimensional strip plots are\nused to depict such data, one of their main problems is visual clutter which\nresults from overlap. To reduce this overlap, jitter plots were introduced,\nwhereby an additional, non-encoding plot dimension is introduced, along which\nthe data point representing dots are randomly perturbed. Unfortunately, this\nrandomness can suggest non-existent clusters, and often leads to visually\nunappealing plots, in which overlap might still occur. To overcome these\nshortcomings, we introduce BlueNoise Plots where random jitter along the\nnon-encoding plot dimension is replaced by optimizing all dots to keep a\nminimum distance in 2D i. e., Blue Noise. We evaluate the effectiveness as well\nas the aesthetics of Blue Noise Plots through both, a quantitative and a\nqualitative user study. The Python implementation of Blue Noise Plots is\navailable here.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04072v3"
    },
    {
        "title": "Meta-PU: An Arbitrary-Scale Upsampling Network for Point Cloud",
        "authors": [
            "Shuquan Ye",
            "Dongdong Chen",
            "Songfang Han",
            "Ziyu Wan",
            "Jing Liao"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Point cloud upsampling is vital for the quality of the mesh in\nthree-dimensional reconstruction. Recent research on point cloud upsampling has\nachieved great success due to the development of deep learning. However, the\nexisting methods regard point cloud upsampling of different scale factors as\nindependent tasks. Thus, the methods need to train a specific model for each\nscale factor, which is both inefficient and impractical for storage and\ncomputation in real applications. To address this limitation, in this work, we\npropose a novel method called ``Meta-PU\" to firstly support point cloud\nupsampling of arbitrary scale factors with a single model. In the Meta-PU\nmethod, besides the backbone network consisting of residual graph convolution\n(RGC) blocks, a meta-subnetwork is learned to adjust the weights of the RGC\nblocks dynamically, and a farthest sampling block is adopted to sample\ndifferent numbers of points. Together, these two blocks enable our Meta-PU to\ncontinuously upsample the point cloud with arbitrary scale factors by using\nonly a single model. In addition, the experiments reveal that training on\nmultiple scales simultaneously is beneficial to each other. Thus, Meta-PU even\noutperforms the existing methods trained for a specific scale factor only.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04317v1"
    },
    {
        "title": "Designing Personalized Garments with Body Movement",
        "authors": [
            "Katja Wolff",
            "Philipp Herholz",
            "Verena Ziegler",
            "Frauke Link",
            "Nico Brügel",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The standardized sizes used in the garment industry do not cover the range of\nindividual differences in body shape for most people, leading to ill-fitting\nclothes, high return rates and overproduction. Recent research efforts in both\nindustry and academia therefore focus on virtual try-on and on-demand\nfabrication of individually fitting garments. We propose an interactive design\ntool for creating custom-fit garments based on 3D body scans of the intended\nwearer. Our method explicitly incorporates transitions between various body\nposes to ensure a better fit and freedom of movement. The core of our method\nfocuses on tools to create a 3D garment shape directly on an avatar without an\nunderlying sewing pattern, and on the adjustment of that garment's rest shape\nwhile interpolating and moving through the different input poses. We alternate\nbetween cloth simulation and rest shape adjustment based on stretch to achieve\nthe final shape of the garment. At any step in the real-time process, we allow\nfor interactive changes to the garment. Once the garment shape is finalized for\nproduction, established techniques can be used to parameterize it into a 2D\nsewing pattern or transform it into a knitting pattern.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.05462v2"
    },
    {
        "title": "B/Surf: Interactive Bézier Splines on Surfaces",
        "authors": [
            "Claudio Mancinelli",
            "Giacomo Nazzaro",
            "Fabio Pellacini",
            "Enrico Puppo"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  B\\'ezier curves provide the basic building blocks of graphic design in 2D. In\nthis paper, we port B\\'ezier curves to manifolds. We support the interactive\ndrawing and editing of B\\'ezier splines on manifold meshes with millions of\ntriangles, by relying on just repeated manifold averages. We show that direct\nextensions of the De Casteljau and Bernstein evaluation algorithms to the\nmanifold setting are fragile, and prone to discontinuities when control\npolygons become large. Conversely, approaches based on subdivision are robust\nand can be implemented efficiently. We define B\\'ezier curves on manifolds, by\nextending both the recursive De Casteljau bisection and a new open-uniform\nLane-Riesenfeld subdivision scheme, which provide curves with different degrees\nof smoothness. For both schemes, we present algorithms for curve tracing, point\nevaluation, and point insertion. We test our algorithms for robustness and\nperformance on all watertight, manifold, models from the Thingi10k repository,\nwithout any pre-processing and with random control points. For interactive\nediting, we port all the basic user interface interactions found in 2D tools\ndirectly to the mesh. We also support mapping complex SVG drawings to the mesh\nand their interactive editing.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.05921v2"
    },
    {
        "title": "An All-In-One Geometric Algorithm for Cutting, Tearing, and Drilling\n  Deformable Models",
        "authors": [
            "Manos Kamarianakis",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Conformal Geometric Algebra (CGA) is a framework that allows the\nrepresentation of objects, such as points, planes and spheres, and\ndeformations, such as translations, rotations and dilations as uniform vectors,\ncalled multivectors. In this work, we demonstrate the merits of multivector\nusage with a novel, integrated rigged character simulation framework based on\nCGA. In such a framework, and for the first time, one may perform real-time\ncuts and tears as well as drill holes on a rigged 3D model. These operations\ncan be performed before and/or after model animation, while maintaining\ndeformation topology. Moreover, our framework permits generation of\nintermediate keyframes on-the-fly based on user input, apart from the frames\nprovided in the model data. We are motivated to use CGA as it is the\nlowest-dimension extension of dual-quaternion algebra that amends the\nshortcomings of the majority of existing animation and deformation techniques.\nSpecifically, we no longer need to maintain objects of multiple algebras and\nconstantly transmute between them, such as matrices, quaternions and\ndual-quaternions, and we can effortlessly apply dilations. Using such an\nall-in-one geometric framework allows for better maintenance and optimization\nand enables easier interpolation and application of all native deformations.\nFurthermore, we present these three novel algorithms in a single CGA\nrepresentation which enables cutting, tearing and drilling of the input rigged\nmodel, where the output model can be further re-deformed in interactive frame\nrates. These close to real-time cut,tear and drill algorithms can enable a new\nsuite of applications, especially under the scope of a medical VR simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.07499v2"
    },
    {
        "title": "An unbiased ray-marching transmittance estimator",
        "authors": [
            "Markus Kettunen",
            "Eugene d'Eon",
            "Jacopo Pantaleoni",
            "Jan Novak"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present an in-depth analysis of the sources of variance in\nstate-of-the-art unbiased volumetric transmittance estimators, and propose\nseveral new methods for improving their efficiency. These combine to produce a\nsingle estimator that is universally optimal relative to prior work, with up to\nseveral orders of magnitude lower variance at the same cost, and has zero\nvariance for any ray with non-varying extinction. We first reduce the variance\nof truncated power-series estimators using a novel efficient application of\nU-statistics. We then greatly reduce the average expansion order of the power\nseries and redistribute density evaluations to filter the optical depth\nestimates with an equidistant sampling comb. Combined with the use of an online\ncontrol variate built from a sampled mean density estimate, the resulting\nestimator effectively performs ray marching most of the time while using\nrarely-sampled higher order terms to correct the bias.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10294v1"
    },
    {
        "title": "GenFloor: Interactive Generative Space Layout System via Encoded Tree\n  Graphs",
        "authors": [
            "Mohammad Keshavarzi",
            "Mohammad Rahmani-Asl"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Automated floorplanning or space layout planning has been a long-standing\nNP-hard problem in the field of computer-aided design, with applications in\nintegrated circuits, architecture, urbanism, and operational research. In this\npaper, we introduce GenFloor, an interactive design system that takes\ngeometrical, topological, and performance goals and constraints as input and\nprovides optimized spatial design solutions as output. As part of our work, we\npropose three novel permutation methods for existing space layout graph\nrepresentations, namely O-Tree and B*-Tree representations. We implement our\nproposed floorplanning methods as a package for Dynamo, a visual programming\ntool, with a custom GUI and additional evaluation functionalities to facilitate\ndesigners in their generative design workflow. Furthermore, we illustrate the\nperformance of GenFloor in two sets of case-study experiments for residential\nfloorplanning tasks by (a) measuring the ability of the proposed system to find\na known optimal solution, and (b) observing how the system can generate diverse\nfloorplans while addressing given a constant residential design problem. Our\nresults indicate convergence to the global optimum is achieved while offering a\ndiverse set of solutions of a residential floorplan corresponding to the\nPareto-optimums of the solution landscape.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10320v1"
    },
    {
        "title": "Hybrid-driven Trajectory Prediction Based on Group Emotion",
        "authors": [
            "Chaochao Li",
            "Mingliang Xu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a hybrid-driven trajectory prediction method based on group\nemotion. The data driven and model driven methods are combined to make a\ncompromise between the controllability, generality, and efficiency of the\nmethod on the basis of simulating more real crowd movements. A hybrid driven\nmethod is proposed to improve the reliability of the calculation results based\non real crowd data, and ensure the controllability of the model. It reduces the\ndependence of our model on real data and realizes the complementary advantages\nof these two kinds of methods. In addition, we divide crowd into groups based\non human relations in society. So our method can calculate the movements in\ndifferent scales. We predict individual movement trajectories according to the\ntrajectories of group and fully consider the influence of the group movement\nstate on the individual movements. Besides we also propose a group emotion\ncalculation method and our method also considers the effect of group emotion on\ncrowd movements.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10375v1"
    },
    {
        "title": "Data to Physicalization: A Survey of the Physical Rendering Process",
        "authors": [
            "Hessam Djavaherpour",
            "Faramarz Samavati",
            "Ali Mahdavi-Amiri",
            "Fatemeh Yazdanbakhsh",
            "Samuel Huron",
            "Richard Levy",
            "Yvonne Jansen",
            "Lora Oehlberg"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Physical representations of data offer physical and spatial ways of looking\nat, navigating, and interacting with data. While digital fabrication has\nfacilitated the creation of objects with data-driven geometry, rendering data\nas a physically fabricated object is still a daunting leap for many\nphysicalization designers. Rendering in the scope of this research refers to\nthe back-and-forth process from digital design to digital fabrication and its\nspecific challenges. We developed a corpus of example data physicalizations\nfrom research literature and physicalization practice. This survey then unpacks\nthe \"rendering\" phase of the extended InfoVis pipeline in greater detail\nthrough these examples, with the aim of identifying ways that researchers,\nartists, and industry practitioners \"render\" physicalizations using digital\ndesign and fabrication tools.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.11175v1"
    },
    {
        "title": "Dev2PQ: Planar Quadrilateral Strip Remeshing of Developable Surfaces",
        "authors": [
            "Floor Verhoeven",
            "Amir Vaxman",
            "Tim Hoffmann",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We introduce an algorithm to remesh triangle meshes representing developable\nsurfaces to planar quad dominant meshes. The output of our algorithm consists\nof planar quadrilateral (PQ) strips that are aligned to principal curvature\ndirections and closely approximate the curved parts of the input developable,\nand planar polygons representing the flat parts of the input. Developable\nPQ-strip meshes are useful in many areas of shape modeling, thanks to the\nsimplicity of fabrication from flat sheet material. Unfortunately, they are\ndifficult to model due to their restrictive combinatorics and locking issues.\nOther representations of developable surfaces, such as arbitrary triangle or\nquad meshes, are more suitable for interactive freeform modeling, but generally\nhave non-planar faces or are not aligned to principal curvatures. Our method\nleverages the modeling flexibility of non-ruling based representations of\ndevelopable surfaces, while still obtaining developable, curvature aligned\nPQ-strip meshes. Our algorithm optimizes for a scalar function on the input\nmesh, such that its level sets are extrinsically straight and align well to the\nlocally estimated ruling directions. The condition that guarantees straight\nlevel sets is nonlinear of high order and numerically difficult to enforce in a\nstraightforward manner. We devise an alternating optimization method that makes\nour problem tractable and practical to compute. Our method works automatically\non any developable input, including multiple patches and curved folds, without\nexplicit domain decomposition. We demonstrate the effectiveness of our approach\non a variety of developable surfaces and show how our remeshing can be used\nalongside handle based interactive freeform modeling of developable shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.00239v2"
    },
    {
        "title": "LoBSTr: Real-time Lower-body Pose Prediction from Sparse Upper-body\n  Tracking Signals",
        "authors": [
            "Dongseok Yang",
            "Doyeon Kim",
            "Sung-Hee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  With the popularization of game and VR/AR devices, there is a growing need\nfor capturing human motion with a sparse set of tracking data. In this paper,\nwe introduce a deep neural-network (DNN) based method for real-time prediction\nof the lower-body pose only from the tracking signals of the upper-body joints.\nSpecifically, our Gated Recurrent Unit (GRU)-based recurrent architecture\npredicts the lower-body pose and feet contact probability from past sequence of\ntracking signals of the head, hands and pelvis. A major feature of our method\nis that the input signal is represented with the velocity of tracking signals.\nWe show that the velocity representation better models the correlation between\nthe upper-body and lower-body motions and increase the robustness against the\ndiverse scales and proportions of the user body than position-orientation\nrepresentations. In addition, to remove foot-skating and floating artifacts,\nour network predicts feet contact state, which is used to post-process the\nlower-body pose with inverse kinematics to preserve the contact. Our network is\nlightweight so as to run in real-time applications. We show the effectiveness\nof our method through several quantitative evaluations against other\narchitectures and input representations, with respect to wild tracking data\nobtained from commercial VR devices.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01500v2"
    },
    {
        "title": "An analytic BRDF for materials with spherical Lambertian scatterers",
        "authors": [
            "Eugene d'Eon"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a new analytic BRDF for porous materials comprised of spherical\nLambertian scatterers. The BRDF has a single parameter: the albedo of the\nLambertian particles. The resulting appearance exhibits strong back scattering\nand saturation effects that height-field-based models such as Oren-Nayar cannot\nreproduce.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01618v1"
    },
    {
        "title": "A Revisit of Shape Editing Techniques: from the Geometric to the Neural\n  Viewpoint",
        "authors": [
            "Yu-Jie Yuan",
            "Yu-Kun Lai",
            "Tong Wu",
            "Lin Gao",
            "Ligang Liu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  3D shape editing is widely used in a range of applications such as movie\nproduction, computer games and computer aided design. It is also a popular\nresearch topic in computer graphics and computer vision. In past decades,\nresearchers have developed a series of editing methods to make the editing\nprocess faster, more robust, and more reliable. Traditionally, the deformed\nshape is determined by the optimal transformation and weights for an energy\nterm. With increasing availability of 3D shapes on the Internet, data-driven\nmethods were proposed to improve the editing results. More recently as the deep\nneural networks became popular, many deep learning based editing methods have\nbeen developed in this field, which is naturally data-driven. We mainly survey\nrecent research works from the geometric viewpoint to those emerging neural\ndeformation techniques and categorize them into organic shape editing methods\nand man-made model editing methods. Both traditional methods and recent neural\nnetwork based methods are reviewed.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01694v1"
    },
    {
        "title": "Compact Tetrahedralization-based Acceleration Structure for Ray Tracing",
        "authors": [
            "Aytek Aman",
            "Serkan Demirci",
            "Uğur Güdükbay"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We propose a compact and efficient tetrahedral mesh representation to improve\nthe ray-tracing performance. We reorder tetrahedral mesh data using a\nspace-filling curve to improve cache locality. Most importantly, we propose an\nefficient ray traversal algorithm. We provide details of common ray tracing\noperations on tetrahedral meshes and give the GPU implementation of our\ntraversal method. We demonstrate our findings through a set of comprehensive\nexperiments. Our method outperforms existing tetrahedral mesh-based traversal\nmethods and yields comparable results to the traversal methods based on the\nstate of the art acceleration structures such as k-dimensional (k-d) trees and\nBounding Volume Hierarchies (BVHs).\n",
        "pdf_link": "http://arxiv.org/pdf/2103.02309v1"
    },
    {
        "title": "Clusterplot: High-dimensional Cluster Visualization",
        "authors": [
            "Or Malkai",
            "Min Lu",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present Clusterplot, a multi-class high-dimensional data visualization\ntool designed to visualize cluster-level information offering an intuitive\nunderstanding of the cluster inter-relations. Our unique plots leverage 2D\nblobs devised to convey the geometrical and topological characteristics of\nclusters within the high-dimensional data, and their pairwise relations, such\nthat general inter-cluster behavior is easily interpretable in the plot. Class\nidentity supervision is utilized to drive the measuring of relations among\nclusters in high-dimension, particularly, proximity and overlap, which are then\nreflected spatially through the 2D blobs. We demonstrate the strength of our\nclusterplots and their ability to deliver a clear and intuitive informative\nexploration experience for high-dimensional clusters characterized by complex\nstructure and significant overlap.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.02992v1"
    },
    {
        "title": "An Effective Approach to Minimize Error in Midpoint Ellipse Drawing\n  Algorithm",
        "authors": [
            "M. Javed Idrisi",
            "Aayesha Ashraf"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The present paper deals with the generalization of Midpoint Ellipse Drawing\nAlgorithm (MPEDA) to minimize the error in the existing MPEDA in cartesian\nform. In this method, we consider three different values of h, i.e., 1, 0.5 and\n0.1. For h = 1, all the results of MPEDA have been verified. For other values\nof h it is observed that as the value of h decreases, the number of iteration\nincreases but the error between the points generated and the original ellipse\npoints decreases and vice-versa.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.04033v1"
    },
    {
        "title": "FlowMesher: An automatic unstructured mesh generation algorithm with\n  applications from finite element analysis to medical simulations",
        "authors": [
            "Zhujiang Wang",
            "Arun R. Srinivasa",
            "J. N. Reddy",
            "Adam Dubrowski"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this work, we propose an automatic mesh generation algorithm, FlowMesher,\nwhich can be used to generate unstructured meshes for mesh domains in any shape\nwith minimum (or even no) user intervention. The approach can generate\nhigh-quality simplex meshes directly from scanned images in OBJ format in 2D\nand 3D or just from a line drawing in 2-D. Mesh grading can be easily\ncontrolled also. The FlowMesher is robust and easy to be implemented and is\nuseful for a variety of applications including surgical simulators.\n  The core idea of the FlowMesher is that a mesh domain is considered as an\n\"airtight container\" into which fluid particles are \"injected\" at one or\nmultiple selected interior points. The particles repel each other and occupy\nthe whole domain somewhat like blowing up a balloon. When the container is full\nof fluid particles and the flow is stopped, a Delaunay triangulation algorithm\nis employed to link the fluid particles together to generate an unstructured\nmesh (which is then optimized using a combination of automated mesh smoothing\nand element removal in 3D). The performance of the FlowMesher is demonstrated\nby generating meshes for several 2D and 3D mesh domains including a scanned\nimage of a bone.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.05640v1"
    },
    {
        "title": "Optimal Dual Schemes for Adaptive Grid Based Hexmeshing",
        "authors": [
            "Marco Livesu",
            "Luca Pitzalis",
            "Gianmarco Cherchi"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Hexahedral meshes are an ubiquitous domain for the numerical resolution of\npartial differential equations. Computing a pure hexahedral mesh from an\nadaptively refined grid is a prominent approach to automatic hexmeshing, and\nrequires the ability to restore the all hex property around the hanging nodes\nthat arise at the interface between cells having different size. The most\nadvanced tools to accomplish this task are based on mesh dualization. These\napproaches use topological schemes to regularize the valence of inner vertices\nand edges, such that dualizing the grid yields a pure hexahedral mesh. In this\npaper we study in detail the dual approach, and propose four main contributions\nto it: (i) we enumerate all the possible transitions that dual methods must be\nable to handle, showing that prior schemes do not natively cover all of them;\n(ii) we show that schemes are internally asymmetric, therefore not only their\nimplementation is ambiguous, but different implementation choices lead to\nhexahedral meshes with different singular structure; (iii) we explore the\ncombinatorial space of dual schemes, selecting the minimum set that covers all\nthe possible configurations and also yields the simplest singular structure in\nthe output hexmesh; (iv) we enlarge the class of adaptive grids that can be\ntransformed into pure hexahedral meshes, relaxing one of the tight requirements\nimposed by previous approaches, and ultimately permitting to obtain much\ncoarser meshes for same geometric accuracy. Last but not least, for the first\ntime we make grid-based hexmeshing truly reproducible, releasing our code and\nalso revealing a conspicuous amount of technical details that were always\noverlooked in previous literature, creating an entry barrier that was hard to\novercome for practitioners in the field.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.07745v1"
    },
    {
        "title": "Eigen Space of Mesh Distortion Energy Hessian",
        "authors": [
            "Yufeng Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Mesh distortion optimization is a popular research topic and has wide range\nof applications in computer graphics, including geometry modeling, variational\nshape interpolation, UV parameterization, elastoplastic simulation, etc. In\nrecent years, many solvers have been proposed to solve this nonlinear\noptimization efficiently, among which projected Newton has been shown to have\nbest convergence rate and work well in both 2D and 3D applications. Traditional\nNewton approach suffers from ill conditioning and indefiniteness of local\nenergy approximation. A crucial step in projected Newton is to fix this issue\nby projecting energy Hessian onto symmetric positive definite (SPD) cone so as\nto guarantee the search direction always pointing to decrease the energy\nlocally. Such step relies on time consuming eigen decomposition of element\nHessian, which has been addressed by several work before on how to obtain a\nconjugacy that is as diagonal as possible. In this report, we demonstrate an\nanalytic form of Hessian eigen system for distortion energy defined using\nprincipal stretches, which is the most general representation. Compared with\nexisting projected Newton diagonalization approaches, our formulation is more\ngeneral as it doesn't require the energy to be representable by tensor\ninvariants. In this report, we will only show the derivation for 3D and the\nextension to 2D case is straightforward.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.08141v2"
    },
    {
        "title": "Automatic Generation of Large-scale 3D Road Networks based on GIS Data",
        "authors": [
            "Hua Wang",
            "Yue Wu",
            "Xu Han",
            "Mingliang Xu",
            "Weizhe Chen",
            "Guoliang Chen"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  How to automatically generate a realistic large-scale 3D road network is a\nkey point for immersive and credible traffic simulations. Existing methods\ncannot automatically generate various kinds of intersections in 3D space based\non GIS data. In this paper, we propose a method to generate complex and\nlarge-scale 3D road networks automatically with the open source GIS data,\nincluding satellite imagery, elevation data and two-dimensional(2D) road center\naxis data, as input. We first introduce a semantic structure of road network to\nobtain high-detailed and well-formed networks in a 3D scene. We then generate\n2D shapes and topological data of the road network according to the semantic\nstructure and 2D road center axis data. At last, we segment the elevation data\nand generate the surface of the 3D road network according to the 2D semantic\ndata and satellite imagery data. Results show that our method does well in the\ngeneration of various types of intersections and the high-detailed features of\nroads. The traffic semantic structure, which must be provided in traffic\nsimulation, can also be generated automatically according to our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.08275v1"
    },
    {
        "title": "2D Points Curve Reconstruction Survey and Benchmark",
        "authors": [
            "Stefan Ohrhallinger",
            "Jiju Peethambaran",
            "Amal D. Parakkat",
            "Tamal K. Dey",
            "Ramanathan Muthuganapathy"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Curve reconstruction from unstructured points in a plane is a fundamental\nproblem with many applications that has generated research interest for\ndecades. Involved aspects like handling open, sharp, multiple and non-manifold\noutlines, run-time and provability as well as potential extension to 3D for\nsurface reconstruction have led to many different algorithms. We survey the\nliterature on 2D curve reconstruction and then present an open-sourced\nbenchmark for the experimental study. Our unprecedented evaluation on a\nselected set of planar curve reconstruction algorithms aims to give an overview\nof both quantitative analysis and qualitative aspects for helping users to\nselect the right algorithm for specific problems in the field. Our benchmark\nframework is available online to permit reproducing the results, and easy\nintegration of new algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.09583v1"
    },
    {
        "title": "Crowdsourcing Autonomous Traffic Simulation",
        "authors": [
            "Hua Wang",
            "Wenshan Zhao",
            "Zhigang Deng",
            "Mingliang Xu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present an innovative framework, Crowdsourcing Autonomous Traffic\nSimulation (CATS) framework, in order to safely implement and realize orderly\ntraffic flows. We firstly provide a semantic description of the CATS framework\nusing theories of economics to construct coupling constraints among drivers, in\nwhich drivers monitor each other by making use of transportation resources and\ndriving credit. We then introduce an emotion-based traffic simulation, which\nutilizes the Weber-Fechner law to integrate economic factors into drivers'\nbehaviors. Simulation results show that the CATS framework can significantly\nreduce traffic accidents and improve urban traffic conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.09988v2"
    },
    {
        "title": "HeterSkinNet: A Heterogeneous Network for Skin Weights Prediction",
        "authors": [
            "Xiaoyu Pan",
            "Jiancong Huang",
            "Jiaming Mai",
            "He Wang",
            "Honglin Li",
            "Tongkui Su",
            "Wenjun Wang",
            "Xiaogang Jin"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Character rigging is universally needed in computer graphics but notoriously\nlaborious. We present a new method, HeterSkinNet, aiming to fully automate such\nprocesses and significantly boost productivity. Given a character mesh and\nskeleton as input, our method builds a heterogeneous graph that treats the mesh\nvertices and the skeletal bones as nodes of different types and uses graph\nconvolutions to learn their relationships. To tackle the graph heterogeneity,\nwe propose a new graph network convolution operator that transfers information\nbetween heterogeneous nodes. The convolution is based on a new distance\nHollowDist that quantifies the relations between mesh vertices and bones. We\nshow that HeterSkinNet is robust for production characters by providing the\nability to incorporate meshes and skeletons with arbitrary topologies and\nmorphologies (e.g., out-of-body bones, disconnected mesh components, etc.).\nThrough exhaustive comparisons, we show that HeterSkinNet outperforms\nstate-of-the-art methods by large margins in terms of rigging accuracy and\nnaturalness. HeterSkinNet provides a solution for effective and robust\ncharacter rigging.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.10602v1"
    },
    {
        "title": "AVATAR: Blender add-on for fast creation of 3D human models",
        "authors": [
            "Jordi Sanchez-Riera",
            "Aniol Civit",
            "Marta Altarriba",
            "Francesc Moreno-Noguer"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Create an articulated and realistic human 3D model is a complicated task, not\nonly get a model with the right body proportions but also to the whole process\nof rigging the model with correct articulation points and vertices weights.\nHaving a tool that can create such a model with just a few clicks will be very\nadvantageous for amateurs developers to use in their projects, researchers to\neasily generate datasets to train neural networks and industry for game\ndevelopment. We present a software that is integrated in Blender in form of\nadd-on that allows us to design and animate a dressed 3D human models based on\nMakehuman with just a few clicks. Moreover, as it is already integrated in\nBlender, python scripts can be created to animate, render and further customize\nthe current available options.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14507v1"
    },
    {
        "title": "BrainPainter v2: Mouse Brain Visualization Software",
        "authors": [
            "Vedvayas Mallela",
            "Polina Golland",
            "Razvan V. Marinescu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  BrainPainter is a software for the 3D visualization of human brain\nstructures; it generates colored brain images using user-defined biomarker data\nfor each brain region. However, BrainPainter is only able to generate human\nbrain images. In this paper, we present updates to the existing BrainPainter\nsoftware which enables the generation of mouse brain images. We load meshes for\neach mouse brain region, based on the Allen Mouse Brain Atlas, into Blender, a\npowerful 3D computer graphics engine. We then use Blender to color each region\nand generate images of subcortical, outer-cortical, inner-cortical, top and\nbottom view renders. In addition to those views, we add new render angles and\nseparate visualization settings for the left and right hemispheres. While\nBrainPainter traditionally ran from the browser (\nhttps://brainpainter.csail.mit.edu ), we also created a graphical user\ninterface that launches image-generation requests in a user-friendly way, by\nconnecting to the Blender backend via a Docker API. We illustrate a use case of\nBrainPainter for modeling the progression of tau protein accumulation in a\nmouse study. Our contributions can help neuroscientists visualize brains in\nmouse studies and show disease progression. In addition, integration into\nBlender can subsequently enable the generation of complex animations using a\nmoving camera, generation of complex mesh deformations that simulate tumors and\nother pathologies, as well as visualization of toxic proteins using Blender's\nparticle system.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14696v1"
    },
    {
        "title": "Remeshing-Free Graph-Based Finite Element Method for Ductile and Brittle\n  Fracture",
        "authors": [
            "Avirup Mandal",
            "Parag Chaudhuri",
            "Subhasis Chaudhuri"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Fracture produces new mesh fragments that introduce additional degrees of\nfreedom in the system dynamics. Existing finite element method (FEM) based\nsolutions suffer from an explosion in computational cost as the system matrix\nsize increases. We solve this problem by presenting a graph-based FEM model for\nfracture simulation that is remeshing-free and easily scales to high-resolution\nmeshes. Our algorithm models fracture on the graph induced in a volumetric mesh\nwith tetrahedral elements. We relabel the edges of the graph using a computed\ndamage variable to initialize and propagate fracture. We prove that non-linear,\nhyper-elastic strain energy is expressible entirely in terms of the edge\nlengths of the induced graph. This allows us to reformulate the system dynamics\nfor the relabeled graph without changing the size of system dynamics matrix and\nthus prevents the computational cost from blowing up. The fractured surface has\nto be reconstructed explicitly only for visualization purposes. We simulate\nstandard laboratory experiments from structural mechanics and compare the\nresults with corresponding real-world experiments. We fracture objects made of\na variety of brittle and ductile materials, and show that our technique offers\nstability and speed that is unmatched in current literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14870v2"
    },
    {
        "title": "Countering Racial Bias in Computer Graphics Research",
        "authors": [
            "Theodore Kim",
            "Holly Rushmeier",
            "Julie Dorsey",
            "Derek Nowrouzezahrai",
            "Raqi Syed",
            "Wojciech Jarosz",
            "A. M. Darke"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Current computer graphics research practices contain racial biases that have\nresulted in investigations into \"skin\" and \"hair\" that focus on the hegemonic\nvisual features of Europeans and East Asians. To broaden our research horizons\nto encompass all of humanity, we propose a variety of improvements to\nquantitative measures and qualitative practices, and pose novel, open research\nproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.15163v3"
    },
    {
        "title": "View-Dependent Formulation of 2.5D Cartoon Models",
        "authors": [
            "Tsukasa Fukusato",
            "Akinobu Maejima"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  2.5D cartoon models are methods to simulate three-dimensional (3D)-like\nmovements, such as out-of-plane rotation, from two-dimensional (2D) shapes in\ndifferent views. However, cartoon objects and characters have several distorted\nparts which do not correspond to any real 3D positions (e.g., Mickey Mouse's\nears), that implies that existing systems are not suitable for designing such\nrepresentations. Hence, we formulate it as a view-dependent deformation (VDD)\nproblem, which has been proposed in the field of 3D character animation. The\ndistortions in an arbitrary viewpoint are automatically obtained by blending\nthe user-specified 2D shapes of key views. This model is simple enough to\neasily implement in an existing animation system. Several examples demonstrate\nthe robustness of our method over previous methods. In addition, we conduct a\nuser study and confirm that the proposed system is effective for animating\nclassic cartoon characters.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.15472v1"
    },
    {
        "title": "Learning and Exploring Motor Skills with Spacetime Bounds",
        "authors": [
            "Li-Ke Ma",
            "Zeshi Yang",
            "Xin Tong",
            "Baining Guo",
            "KangKang Yin"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Equipping characters with diverse motor skills is the current bottleneck of\nphysics-based character animation. We propose a Deep Reinforcement Learning\n(DRL) framework that enables physics-based characters to learn and explore\nmotor skills from reference motions. The key insight is to use loose space-time\nconstraints, termed spacetime bounds, to limit the search space in an early\ntermination fashion. As we only rely on the reference to specify loose\nspacetime bounds, our learning is more robust with respect to low quality\nreferences. Moreover, spacetime bounds are hard constraints that improve\nlearning of challenging motion segments, which can be ignored by imitation-only\nlearning. We compare our method with state-of-the-art tracking-based DRL\nmethods. We also show how to guide style exploration within the proposed\nframework\n",
        "pdf_link": "http://arxiv.org/pdf/2103.16807v1"
    },
    {
        "title": "DiffXPBD : Differentiable Position-Based Simulation of Compliant\n  Constraint Dynamics",
        "authors": [
            "Tuur Stuyck",
            "Hsiao-yu Chen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present DiffXPBD, a novel and efficient analytical formulation for the\ndifferentiable position-based simulation of compliant constrained dynamics\n(XPBD). Our proposed method allows computation of gradients of numerous\nparameters with respect to a goal function simultaneously leveraging a\nperformant simulation model. The method is efficient, thus enabling\ndifferentiable simulations of high resolution geometries and degrees of freedom\n(DoFs). Collisions are naturally included in the framework. Our differentiable\nmodel allows a user to easily add additional optimization variables. Every\ncontrol variable gradient requires the computation of only a few partial\nderivatives which can be computed using automatic differentiation code. We\ndemonstrate the efficacy of the method with examples such as elastic material\nparameter estimation, initial value optimization, optimizing for underlying\nbody shape and pose by only observing the clothing, and optimizing a\ntime-varying external force sequence to match sparse keyframe shapes at\nspecific times. Our approach demonstrates excellent efficiency and we\ndemonstrate this on high resolution meshes with optimizations involving over 26\nmillion degrees of freedom. Making an existing solver differentiable requires\nonly a few modifications and the model is compatible with both modern CPU and\nGPU multi-core hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01396v3"
    },
    {
        "title": "Robust Surface Reconstruction from Orthogonal Slices",
        "authors": [
            "Radek Svitak",
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The surface reconstruction problem from sets of planar parallel slices\nrepresenting cross sections through 3D objects is presented. The final result\nof surface reconstruction is always based on the correct estimation of the\nstructure of the original object. This paper is a case study of the problem of\nthe structure determination. We present a new approach, which is based on\nconsidering mutually orthogonal sets of slices. A new method for surface\nreconstruction from orthogonal slices is described and the benefit of\northogonal slices is discussed too.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01713v1"
    },
    {
        "title": "A Comparison of Fundamental Methods for Iso-surface Extraction",
        "authors": [
            "Jan Patera",
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper four fundamental methods for an iso-surface extraction are\ncompared, based on cell decomposition to tetrahedra. The methods are compared\nboth on mathematically generated data sets as well as on real data sets. The\ncomparison using mathematical data is made from different points of view such\nas area approximation, volume approximation. On the other hand, the Hausdorff\ndistance and root mean square are used to compare methods on real data sets.\nThe presented comparison can be helpful when deciding among tested methods\nwhich one to choose, as well as when we need to compare a newly developed\nmethod with other existing approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01715v1"
    },
    {
        "title": "Radiance Textures for Rasterizing Ray-Traced Data",
        "authors": [
            "Jakub Maksymilian Fober"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Presenting real-time rendering of 3D surfaces using radiance textures for\nfast synthesis of complex incidence-variable effects and environment\ninteractions. This includes iridescence, parallax occlusion and interior\nmapping, (specular, regular, diffuse, total-internal) reflections with many\nbounces, refraction, subsurface scattering, transparency, and possibly more.\nThis method divides textures into a matrix of radiance buckets, where each\nbucket represent some data at various incidence angles. Data can show final\npixel color, or deferred rendering ambient occlusion, reflections, shadow map,\netc. Resolution of the final synthesized output is the radiance bucket matrix\nsize. Technique can be implemented with a simple fragment shader. The\ncomputational footprint of this technique is of simple diffuse-only graphics,\nbut with visual fidelity of complex (off-line) ray-traced render at the cost of\nstorage memory footprint. Balance between computational footprint and storage\nmemory footprint can be easily achieved with variable compression ratio of\nrepetitive radiance scene textures.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01719v1"
    },
    {
        "title": "A Contact Proxy Splitting Method for Lagrangian Solid-Fluid Coupling",
        "authors": [
            "Tianyi Xie",
            "Minchen Li",
            "Yin Yang",
            "Chenfanfu Jiang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a robust and efficient method for simulating Lagrangian\nsolid-fluid coupling based on a new operator splitting strategy. We use\nvariational formulations to approximate fluid properties and solid-fluid\ninteractions, and introduce a unified two-way coupling formulation for SPH\nfluids and FEM solids using interior point barrier-based frictional contact. We\nsplit the resulting optimization problem into a fluid phase and a\nsolid-coupling phase using a novel time-splitting approach with augmented\ncontact proxies, and propose efficient custom linear solvers. Our technique\naccounts for fluids interaction with nonlinear hyperelastic objects of\ndifferent geometries and codimensions, while maintaining an algorithmically\nguaranteed non-penetrating criterion. Comprehensive benchmarks and experiments\ndemonstrate the efficacy of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01976v1"
    },
    {
        "title": "Line-Torus Intersection for Ray Tracing: Alternative Formulations",
        "authors": [
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Intersection algorithms are very important in computation of geometrical\nproblems. Algorithms for a line intersection with linear or quadratic surfaces\nare quite efficient. However, algorithms for a line intersection with other\nsurfaces are more complex and time consuming. In this case the object is\nusually closed into a simple bounding volume to speed up the cases when the\ngiven line cannot intersect the given object. In this paper new formulations of\nthe line-torus intersection problem are given and new specification of the\nbounding volume for a torus is given as well. The presented approach is based\non an idea of a line intersection with an envelope of rotating sphere that\nforms a torus. Due to this approach new bounding volume can be formulated which\nis more effective as it enables to detect cases when the line passes the \"hole\"\nof a torus, too.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.03191v1"
    },
    {
        "title": "Adaptive Dynamic Global Illumination",
        "authors": [
            "Sayantan Datta",
            "Negar Goli",
            "Jerry Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present an adaptive extension of probe based global illumination solution\nthat enhances the response to dynamic changes in the scene while while also\nenabling an order of magnitude increase in probe count. Our adaptive sampling\nstrategy carefully places samples in regions where we detect time varying\nchanges in radiosity either due to a change in lighting, geometry or both. Even\nwith large number of probes, our technique robustly updates the irradiance and\nvisibility cache to reflect the most up to date changes without stalling the\noverall algorithm. Our bandwidth aware approach is largely an improvement over\nthe original \\textit{Dynamic Diffuse Global Illumination} while also remaining\northogonal to the recent advancements in the technique.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.05125v1"
    },
    {
        "title": "Neural Shadow Mapping",
        "authors": [
            "Sayantan Datta",
            "Derek Nowrouzezahrai",
            "Christoph Schied",
            "Zhao Dong"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a neural extension of basic shadow mapping for fast, high quality\nhard and soft shadows. We compare favorably to fast pre-filtering shadow\nmapping, all while producing visual results on par with ray traced hard and\nsoft shadows. We show that combining memory bandwidth-aware architecture\nspecialization and careful temporal-window training leads to a fast, compact\nand easy-to-train neural shadowing method. Our technique is memory bandwidth\nconscious, eliminates the need for post-process temporal anti-aliasing or\ndenoising, and supports scenes with dynamic view, emitters and geometry while\nremaining robust to unseen objects.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.05262v1"
    },
    {
        "title": "Pose Metrics: a New Paradigm for Character Motion Edition",
        "authors": [
            "Léon Victor",
            "Alexandre Meyer",
            "Saïda Bouakaz"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In animation, style can be considered as a distinctive layer over the content\nof a motion, allowing a character to achieve the same gesture in various ways.\nEditing existing animation to modify the style while keeping the same content\nis an interesting task, which can facilitate the re-use of animation data and\ncut down on production time. Existing animation edition methods either work\ndirectly on the motion data, providing precise but tedious tools, or manipulate\nsemantic style categories, taking control away from the user. As a middle\nground, we propose a new character motion edition paradigm allowing\nhigher-level manipulations without sacrificing controllability. We describe the\nconcept of pose metrics, objective value functions which can be used to edit\nanimation, leaving the style interpretation up to the user. We then propose an\nedition pipeline to edit animation data using pose metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.06514v1"
    },
    {
        "title": "A Combined Finite Element and Finite Volume Method for Liquid Simulation",
        "authors": [
            "Tatsuya Koike",
            "Shigeo Morishima",
            "Ryoichi Ando"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce a new Eulerian simulation framework for liquid animation that\nleverages both finite element and finite volume methods. In contrast to\nprevious methods where the whole simulation domain is discretized either using\nthe finite volume method or finite element method, our method spatially merges\nthem together using two types of discretization being tightly coupled on its\nseams while enforcing second order accurate boundary conditions at free\nsurfaces. We achieve our formulation via a variational form using new shape\nfunctions specifically designed for this purpose. By enabling a mixture of the\ntwo methods, we can take advantage of the best of two worlds. For example,\nfinite volume method (FVM) result in sparse linear systems; however, complexity\nis encountered when unstructured grids such as tetrahedral or Voronoi elements\nare used. Finite element method (FEM), on the other hand, result in comparably\ndenser linear systems, but the complexity remains the same even if unstructured\nelements are chosen; thereby facilitating spatial adaptivity. In this paper, we\npropose to use FVM for the majority parts to retain the sparsity of linear\nsystems and FEM for parts where the grid elements are allowed to be freely\ndeformed. An example of this application is locally moving grids. We show that\nby adapting the local grid movement to an underlying nearly rigid motion,\nnumerical diffusion is noticeably reduced; leading to better preservation of\nstructural details such as sharp edges, thin sheets and spindles of liquids.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.06816v1"
    },
    {
        "title": "From medical imaging to virtual reality for archaeology",
        "authors": [
            "Théophane Nicolas",
            "Ronan Gaugne",
            "Bruno Arnaldi",
            "Valérie Gouranton"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The IRMA project aims to design innovative methodologies for research in the\nfield of historical and archaeological heritage based on a combination of\nmedical imaging technologies and interactive 3D restitution modalities (virtual\nreality, augmented reality, haptics, additive manufacturing). These tools are\nbased on recent research results from a collaboration between IRISA, Inrap and\nthe company Image ET and are intended for cultural heritage professionals such\nas museums, curators, restorers and archaeologists.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.11006v1"
    },
    {
        "title": "Screen Space Indirect Lighting with Visibility Bitmask",
        "authors": [
            "Olivier Therrien",
            "Yannick Levesque",
            "Guillaume Gilet"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Horizon-based indirect illumination efficiently estimates a diffuse light\nbounce in screen space by analytically integrating the horizon angle difference\nbetween samples along a given direction. Like other horizon-based methods, this\ntechnique cannot properly simulate light passing behind thin surfaces. We\npropose the concept of a visibility bitmask that replaces the two horizon\nangles by a bit field representing the binary state (occluded / un-occluded) of\nN sectors uniformly distributed around the hemisphere slice. It allows light to\npass behind surfaces of constant thickness while keeping the efficiency of\nhorizon-based methods. It can also do more accurate ambient lighting than bent\nnormal by sampling more than one visibility cone. This technique improves the\nvisual quality of ambient occlusion, indirect diffuse, and ambient light\ncompared to previous screen space methods while minimizing noise and keeping a\nlow performance overhead.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.11376v2"
    },
    {
        "title": "Information Entropy-based Camera Path Estimation for In-Situ\n  Visualization",
        "authors": [
            "Ken Iwata",
            "Naohisa Sakamoto",
            "Jorji Nonaka",
            "Chongke Bi"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In-situ processing has widely been recognized as an effective approach for\nthe visualization and analysis of large-scale simulation outputs from modern\nHPC systems. One of the most common approaches for batch-based in-situ\nvisualization is the image- or video-based approach. In this kind of approach,\na large number of rendered images are generated from different viewpoints at\neach time step and has proven useful for detailed analysis of the main\nsimulation results. However, during test runs and model calibration runs before\nthe main simulation run, a quick overview might be sufficient and useful. In\nthis work, we focused on selecting the viewpoints which provide as much\ninformation as possible by using information entropy to maximize the subsequent\nvisual analysis task. However, by simply following the selected viewpoints at\neach of the visualization time steps will probably lead to a rapidly changing\nvideo, which can impact the understanding. Therefore, we have also worked on an\nefficient camera path estimation approach for connecting selected viewpoints,\nat regular intervals, to generate a smooth video. This resulting video is\nexpected to assist in rapid understanding of the underlying simulation\nphenomena and can be helpful to narrow down the temporal region of interest to\nminimize the turnaround time during detailed visual exploration via image- or\nvideo-based visual analysis of the main simulation run. We implemented and\nevaluated the proposed approach using the OpenFOAM CFD application, on an\nx86-based Server and an ARM A64FX-based supercomputer (Fugaku), and we obtained\npositive evaluations from domain scientists.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.11591v2"
    },
    {
        "title": "Fast B-spline Curve Fitting by L-BFGS",
        "authors": [
            "Wenni Zheng",
            "Pengbo Bo",
            "Yang Liu",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  We propose a novel method for fitting planar B-spline curves to unorganized\ndata points. In traditional methods, optimization of control points and foot\npoints are performed in two very time-consuming steps in each iteration: 1)\ncontrol points are updated by setting up and solving a linear system of\nequations; and 2) foot points are computed by projecting each data point onto a\nB-spline curve. Our method uses the L-BFGS optimization method to optimize\ncontrol points and foot points simultaneously and therefore it does not need to\nperform either matrix computation or foot point projection in every iteration.\nAs a result, our method is much faster than existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.0070v1"
    },
    {
        "title": "Connectivity-preserving Geometry Images",
        "authors": [
            "Shaofan Wang",
            "Dehui Kong",
            "Juan Xue",
            "Weijia Zhu",
            "Min Xu",
            "Baocai Yin",
            "Hubert Roth"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We propose connectivity-preserving geometry images (CGIMs), which map a\nthree-dimensional mesh onto a rectangular regular array of an image, such that\nthe reconstructed mesh produces no sampling errors, but merely round-off\nerrors. We obtain a V-matrix with respect to the original mesh, whose elements\nare vertices of the mesh, which intrinsically preserves the vertex-set and the\nconnectivity of the original mesh in the sense of allowing round-off errors. We\ngenerate a CGIM array by using the Cartesian coordinates of corresponding\nvertices of the V-matrix. To reconstruct a mesh, we obtain a vertex-set and an\nedge-set by collecting all the elements with different pixels, and all\ndifferent pairwise adjacent elements from the CGIM array respectively. Compared\nwith traditional geometry images, CGIMs achieve minimum reconstruction errors\nwith an efficient parametrization-free algorithm via elementary permutation\ntechniques. We apply CGIMs to lossy compression of meshes, and the experimental\nresults show that CGIMs perform well in reconstruction precision and detail\npreservation.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.0113v3"
    },
    {
        "title": "Forward and Inverse Kinematics Seamless Matching Using Jacobian",
        "authors": [
            "Zeeshan Bhatti",
            "Asadullah Shah",
            "Farruh Shahidi",
            "Mostafa Karbasi"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  In this paper the problem of matching Forward Kinematics (FK) motion of a 3\nDimensional (3D) joint chain to the Inverse Kinematics (IK) movement and vice\nversa has been addressed. The problem lies at the heart of animating a 3D\ncharacter having controller and manipulator based rig for animation within any\n3D modeling and animation software. The seamless matching has been achieved\nthrough the use of pseudo-inverse of Jacobian Matrix. The Jacobian Matrix is\nused to determine the rotation values of each joint of character body part such\nas arms, between the inverse kinematics and forward kinematics motion. Then\nmoving the corresponding kinematic joint system to the desired place,\nautomatically eliminating the jumping or popping effect which would reduce the\ncomplexity of the system.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.1488v1"
    },
    {
        "title": "A General Framework for Bilateral and Mean Shift Filtering",
        "authors": [
            "Justin Solomon",
            "Keenan Crane",
            "Adrian Butscher",
            "Chris Wojtan"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  We present a generalization of the bilateral filter that can be applied to\nfeature-preserving smoothing of signals on images, meshes, and other domains\nwithin a single unified framework. Our discretization is competitive with\nstate-of-the-art smoothing techniques in terms of both accuracy and speed, is\neasy to implement, and has parameters that are straightforward to understand.\nUnlike previous bilateral filters developed for meshes and other irregular\ndomains, our construction reduces exactly to the image bilateral on rectangular\ndomains and comes with a rigorous foundation in both the smooth and discrete\nsettings. These guarantees allow us to construct unconditionally convergent\nmean-shift schemes that handle a variety of extremely noisy signals. We also\napply our framework to geometric edge-preserving effects like feature\nenhancement and show how it is related to local histogram techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.4734v1"
    },
    {
        "title": "Incorporating Sharp Features in the General Solid Sweep Framework",
        "authors": [
            "Bharat Adsul",
            "Jinesh Machchhar",
            "Milind Sohoni"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  This paper extends a recently proposed robust computational framework for\nconstructing the boundary representation (brep) of the volume swept by a given\nsmooth solid moving along a one parameter family $h$ of rigid motions. Our\nextension allows the input solid to have sharp features, i.e., to be of class\nG0 wherein, the unit outward normal to the solid may be discontinuous. In the\nearlier framework, the solid to be swept was restricted to be G1, and thus this\nis a significant and useful extension of that work. This naturally requires a\nprecise description of the geometry of the surface generated by the sweep of a\nsharp edge supported by two intersecting smooth faces. We uncover the geometry\nalong with the related issues like parametrization, self-intersection and\nsingularities via a novel mathematical analysis. Correct trimming of such a\nsurface is achieved by a delicate analysis of the interplay between the cone of\nnormals at a sharp point and its trajectory under $h$. The overall topology is\nexplicated by a key lifting theorem which allows us to compute the adjacency\nrelations amongst entities in the swept volume by relating them to\ncorresponding adjacencies in the input solid. Moreover, global issues related\nto body-check such as orientation are efficiently resolved. Many examples from\na pilot implementation illustrate the efficiency and effectiveness of our\nframework.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.7457v1"
    },
    {
        "title": "Visualization of Large Volumetric Multi-Channel Microscopy Data Streams\n  on Standard PCs",
        "authors": [
            "Tobias Brix",
            "Jörg-Stefan Praßni",
            "Klaus Hinrichs"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Background: Visualization of multi-channel microscopy data plays a vital role\nin biological research. With the ever-increasing resolution of modern\nmicroscopes the data set size of the scanned specimen grows steadily. On\ncommodity hardware this size easily exceeds the available main memory and the\neven more limited GPU memory. Common volume rendering techniques require the\nentire data set to be present in the GPU memory. Existing out-of-core rendering\napproaches for large volume data sets either are limited to single-channel\nvolumes, or require a computer cluster, or have long preprocessing times.\nResults: We introduce a ray-casting technique for rendering large volumetric\nmulti-channel microscopy data streams on commodity hardware. The volumetric\ndata is managed at different levels of detail by an octree structure. In\ncontrast to previous octree-based techniques, the octree is built incrementally\nand therefore supports streamed microscopy data as well as data set sizes\nexceeding the available main memory. Furthermore, our approach allows the user\nto interact with the partially rendered data set at all stages of the octree\nconstruction. After a detailed description of our method, we present\nperformance results for different multi-channel data sets with a size of up to\n24 GB on a standard desktop PC. Conclusions: Our rendering technique allows\nbiologists to visualize their scanned specimen on their standard desktop\ncomputers without high-end hardware requirements. Furthermore, the user can\ninteract with the data set during the initial loading to explore the already\nloaded parts, change rendering parameters like color maps or adjust clipping\nplanes. Thus, the time of biologists being idle is reduced. Also, streamed data\ncan be visualized to detect and stop flawed scans early during the scan\nprocess.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.2074v1"
    },
    {
        "title": "Visualization and Correction of Automated Segmentation, Tracking and\n  Lineaging from 5-D Stem Cell Image Sequences",
        "authors": [
            "Eric Wait",
            "Mark Winter",
            "Chris Bjornsson",
            "Erzsebet Kokovay",
            "Yue Wang",
            "Susan Goderie",
            "Sally Temple",
            "Andrew Cohen"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Results: We present an application that enables the quantitative analysis of\nmultichannel 5-D (x, y, z, t, channel) and large montage confocal fluorescence\nmicroscopy images. The image sequences show stem cells together with blood\nvessels, enabling quantification of the dynamic behaviors of stem cells in\nrelation to their vascular niche, with applications in developmental and cancer\nbiology. Our application automatically segments, tracks, and lineages the image\nsequence data and then allows the user to view and edit the results of\nautomated algorithms in a stereoscopic 3-D window while simultaneously viewing\nthe stem cell lineage tree in a 2-D window. Using the GPU to store and render\nthe image sequence data enables a hybrid computational approach. An\ninference-based approach utilizing user-provided edits to automatically correct\nrelated mistakes executes interactively on the system CPU while the GPU handles\n3-D visualization tasks. Conclusions: By exploiting commodity computer gaming\nhardware, we have developed an application that can be run in the laboratory to\nfacilitate rapid iteration through biological experiments. There is a pressing\nneed for visualization and analysis tools for 5-D live cell image data. We\ncombine accurate unsupervised processes with an intuitive visualization of the\nresults. Our validation interface allows for each data set to be corrected to\n100% accuracy, ensuring that downstream data analysis is accurate and\nverifiable. Our tool is the first to combine all of these aspects, leveraging\nthe synergies obtained by utilizing validation information from stereo\nvisualization to improve the low level image processing tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.2089v1"
    },
    {
        "title": "SketchBio: A Scientist's 3D Interface for Molecular Modeling and\n  Animation",
        "authors": [
            "Shawn M. Waldon",
            "Peter M. Thompson",
            "Patrick J. Hahn",
            "Russell M. Taylor II"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Background: Because of the difficulties involved in learning and using 3D\nmodeling and rendering software, many scientists hire programmers or animators\nto create models and animations. This both slows the discovery process and\nprovides opportunities for miscommunication. Working with multiple\ncollaborators, we developed a set of design goals for a tool that would enable\nthem to directly construct models and animations. Results: We present\nSketchBio, a tool that incorporates state-of-the-art bimanual interaction and\ndrop shadows to enable rapid construction of molecular structures and\nanimations. It includes three novel features: crystal by example, pose-mode\nphysics, and spring-based layout that accelerate operations common in the\nformation of molecular models. We present design decisions and their\nconsequences, including cases where iterative design was required to produce\neffective approaches. Conclusions: The design decisions, novel features, and\ninclusion of state-of-the-art techniques enabled SketchBio to meet all of its\ndesign goals. These features and decisions can be incorporated into existing\nand new tools to improve their effectiveness\n",
        "pdf_link": "http://arxiv.org/pdf/1407.3145v1"
    },
    {
        "title": "Real-time animation of human characters with fuzzy controllers",
        "authors": [
            "Koen Samyn",
            "Sofie Van Hoecke",
            "Bart Pieters",
            "Charles Hollemeersch",
            "Aljosha Demeulemeester",
            "Rik van de Walle"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  The production of animation is a resource intensive process in game\ncompanies. Therefore, techniques to synthesize animations have been developed.\nHowever, these procedural techniques offer limited adaptability by animation\nartists. In order to solve this, a fuzzy neural network model of the animation\nis proposed, where the parameters can be tuned either by machine learning\ntechniques that use motion capture data as training data or by the animation\nartist himself. This paper illustrates how this real time procedural animation\nsystem can be developed, taking the human gait on flat terrain and inclined\nsurfaces as example. Currently, the parametric model is capable of synthesizing\nanimations for various limb sizes and step sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.1130v1"
    },
    {
        "title": "A mathematical design and evaluation of Bernstein-Bezier curves' shape\n  features using the laws of technical aesthetics",
        "authors": [
            "Rifkat I. Nabiyev",
            "Rushan Ziatdinov"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  We present some notes on the definition of mathematical design as well as on\nthe methods of mathematical modeling which are used in the process of the\nartistic design of the environment and its components. For the first time in\nthe field of geometric modeling, we perform an aesthetic analysis of planar\nBernstein-Bezier curves from the standpoint of the laws of technical\naesthetics. The shape features of the curve segments' geometry were evaluated\nusing the following criteria: conciseness-integrity, expressiveness,\nproportional consistency, compositional balance, structural organization,\nimagery, rationality, dynamism, scale, flexibility and harmony. In the\nnon-Russian literature, Bernstein-Bezier curves using a monotonic curvature\nfunction (i.e., a class A Bezier curve) are considered to be fair (i.e.,\nbeautiful) curves, but their aesthetic analysis has never been performed. The\naesthetic analysis performed by the authors of this work means that this is no\nlonger the case. To confirm the conclusions of the authors' research, a survey\nof the \"aesthetic appropriateness\" of certain Bernstein-Bezier curve segments\nwas conducted among 240 children, aged 14-17. The results of this survey have\nshown themselves to be in full accordance with the authors' results.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.3018v1"
    },
    {
        "title": "Efficient Distance Computation Algorithm between Nearly Intersected\n  Objects Using Dynamic Pivot Point in Virtual Environment Application",
        "authors": [
            "Hamzah Asyrani Sulaiman",
            "Abdullah Bade",
            "Mohd Harun Abdullah"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Finding nearly accurate distance between two or more nearly intersecting\nthree-dimensional (3D) objects is vital especially for collision determination\nsuch as in virtual surgeon simulation and real-time car crash simulation.\nInstead of performing broad phase collision detection, we need to check for\naccuracy of detection by running narrow phase collision detection. One of the\nimportant elements for narrow phase collision detection is to determine the\nprecise distance between two or more nearly intersecting objects or polygons in\norder to prepare the area for potential colliding. Distance computation plays\nimportant roles in determine the exact point of contact between two or more\nnearly intersecting polygons where the preparation for collision detection is\ndetermined at the earlier stage. In this paper, we describes our current works\nof determining the distance between objects using dynamic pivot point that will\nbe used as reference point to reduce the complexity searching for potential\npoint of contacts. By using Axis-Aligned Bounding Box for each polygon, we\ncalculate a dynamic pivot point that will become our reference point to\ndetermine the potential candidates for distance computation. The test our\nfinding distance will be simplified by using our method instead of performing\nunneeded operations. Our method provides faster solution than the previous\nmethod where it helps to determine the point of contact efficiently and faster\nthan the other method.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.4603v1"
    },
    {
        "title": "$G^{k,l}$-constrained multi-degree reduction of Bézier curves",
        "authors": [
            "Przemysław Gospodarczyk",
            "Stanisław Lewanowicz",
            "Paweł Woźny"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We present a new approach to the problem of $G^{k,l}$-constrained ($k,l \\leq\n3$) multi-degree reduction of B\\'{e}zier curves with respect to the least\nsquares norm. First, to minimize the least squares error, we consider two\nmethods of determining the values of geometric continuity parameters. One of\nthem is based on quadratic and nonlinear programming, while the other uses some\nsimplifying assumptions and solves a system of linear equations. Next, for\nprescribed values of these parameters, we obtain control points of the\nmulti-degree reduced curve, using the properties of constrained dual Bernstein\nbasis polynomials. Assuming that the input and output curves are of degree $n$\nand $m$, respectively, we determine these points with the complexity $O(mn)$,\nwhich is significantly less than the cost of other known methods. Finally, we\ngive several examples to demonstrate the effectiveness of our algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.03032v3"
    },
    {
        "title": "Feature Lines for Illustrating Medical Surface Models: Mathematical\n  Background and Survey",
        "authors": [
            "Kai Lawonn",
            "Bernhard Preim"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  This paper provides a tutorial and survey for a specific kind of illustrative\nvisualization technique: feature lines. We examine different feature line\nmethods. For this, we provide the differential geometry behind these concepts\nand adapt this mathematical field to the discrete differential geometry. All\ndiscrete differential geometry terms are explained for triangulated surface\nmeshes. These utilities serve as basis for the feature line methods. We provide\nthe reader with all knowledge to re-implement every feature line method.\nFurthermore, we summarize the methods and suggest a guideline for which kind of\nsurface which feature line algorithm is best suited. Our work is motivated by,\nbut not restricted to, medical and biological surface models.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.03605v1"
    },
    {
        "title": "Which tone-mapping operator is the best? A comparative study of\n  perceptual quality",
        "authors": [
            "Xim Cerdá-Company",
            "C. Alejandro Párraga",
            "Xavier Otazu"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Tone-mapping operators (TMO) are designed to generate perceptually similar\nlow-dynamic range images from high-dynamic range ones. We studied the\nperformance of fifteen TMOs in two psychophysical experiments where observers\ncompared the digitally generated tone-mapped images to their corresponding\nphysical scenes. All experiments were performed in a controlled environment and\nthe setups were designed to emphasise different image properties: in the first\nexperiment we evaluated the local relationships among intensity-levels, and in\nthe second one we evaluated global visual appearance among physical scenes and\ntone-mapped images, which were presented side by side. We ranked the TMOs\naccording to how well they reproduce the results obtained in the physical\nscene. Our results show that ranking position clearly depends on the adopted\nevaluation criteria, which implies that, in general, these tone-mapping\nalgorithms consider either local or global image attributes but rarely both. We\nconclude that a more thorough and standardized evaluation criteria are needed\nto study all the characteristics of TMOs, as there is ample room for\nimprovement in future developments.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04450v1"
    },
    {
        "title": "3D digital reassembling of archaeological ceramic pottery fragments\n  based on their thickness profile",
        "authors": [
            "Michail I. Stamatopoulos",
            "Christos-Nikolaos Anagnostopoulos"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  The reassembly of a broken archaeological ceramic pottery is an open and\ncomplex problem, which remains a scientific process of extreme interest for the\narchaeological community. Usually, the solutions suggested by various research\ngroups and universities depend on various aspects such as the matching process\nof the broken surfaces, the outline of sherds or their colors and geometric\ncharacteris-tics, their axis of symmetry, the corners of their contour, the\ntheme portrayed on the surface, the concentric circular rills that are left\nduring the base construction in the inner pottery side by the fingers of the\npotter artist etc. In this work the reassembly process is based on a different\nand more secure idea, since it is based on the thick-ness profile, which is\nappropriately identified in every fragment. Specifically, our approach is based\non information encapsulated in the inner part of the sherd (i.e. thickness),\nwhich is not -or at least not heavily- affected by the presence of harsh\nenvironmental conditions, but is safely kept within the sherd itself. Our\nmethod is verified in various use case experiments, using cutting edge\ntechnologies such as 3D representations and precise measurements on surfaces\nfrom the acquired 3D models.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.05824v1"
    },
    {
        "title": "SculptStat: Statistical Analysis of Digital Sculpting Workflows",
        "authors": [
            "Christian Santoni",
            "Claudio Calabrese",
            "Francesco Di Renzo",
            "Fabio Pellacini"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Targeted user studies are often employed to measure how well artists can\nperform specific tasks. But these studies cannot properly describe editing\nworkflows as wholes, since they guide the artists both by choosing the tasks\nand by using simplified interfaces. In this paper, we investigate digital\nsculpting workflows used to produce detailed models. In our experiment design,\nartists can choose freely what and how to model. We recover whole-workflow\ntrends with sophisticated statistical analyzes and validate these trends with\ngoodness-of-fits measures. We record brush strokes and mesh snapshots by\ninstrumenting a sculpting program and analyze the distribution of these\nproperties and their spatial and temporal characteristics. We hired expert\nartists that can produce relatively sophisticated models in short time, since\ntheir workflows are representative of best practices. We analyze 13 meshes\ncorresponding to roughly 25 thousand strokes in total. We found that artists\nwork mainly with short strokes, with average stroke length dependent on model\nfeatures rather than the artist itself. Temporally, artists do not work\ncoarse-to-fine but rather in bursts. Spatially, artists focus on some selected\nregions by dedicating different amounts of edits and by applying different\ntechniques. Spatio-temporally, artists return to work on the same area multiple\ntimes without any apparent periodicity. We release the entire dataset and all\ncode used for the analyzes as reference for the community.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.07765v1"
    },
    {
        "title": "Boolean Operations using Generalized Winding Numbers",
        "authors": [
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  The generalized winding number function measures insideness for arbitrary\noriented triangle meshes. Exploiting this, I similarly generalize binary\nboolean operations to act on such meshes. The resulting operations for union,\nintersection, difference, etc. avoid volumetric discretization or\npre-processing.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.07953v1"
    },
    {
        "title": "LevelMerge: Collaborative Game Level Editing by Merging Labeled Graphs",
        "authors": [
            "Christian Santoni",
            "Gabriele Salvati",
            "Valentina Tibaldo",
            "Fabio Pellacini"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Game level editing is the process of constructing a full game level starting\nfrom 3D asset libraries, e.g. 3d models, textures, shaders, scripts. In level\nediting, designers define the look and behavior of the whole level by placing\nobjects, assigning materials and lighting parameters, setting animations and\nphysics properties and customizing the objects AI and behavior by editing\nscripts. The heterogeneity of the task usually translates to a workflow where a\nteam of people, experts on separate aspects, cooperate to edit the game level,\noften working on the same objects (e.g.: a programmer working on the AI of a\ncharacter, while an artist works on its 3D model or its materials). Today this\ncollaboration is established by using version control systems designed for text\ndocuments, such as Git, to manage different versions and share them amongst\nusers. The merge algorithms used in these systems though does not perform well\nin our case since it does not respect the relations between game objects\nnecessary to maintain the semantic of the game level behavior and look. This is\na known problem and commercial systems for game level merging exists, e.g.\nPlasticSCM, but these are only slightly more robust than text-based ones. This\ncauses designers to often merge scenes manually, essentially reapplying others\nedits in the game level editor.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.00713v1"
    },
    {
        "title": "Modelling Developable Ribbons Using Ruling Bending Coordinates",
        "authors": [
            "Zherong Pan",
            "Jin Huang",
            "Hujun Bao"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  This paper presents a new method for modelling the dynamic behaviour of\ndevelopable ribbons, two dimensional strips with much smaller width than\nlength. Instead of approximating such surface with a general triangle mesh, we\ncharacterize it by a set of creases and bending angles across them. This\nrepresentation allows the developability to be satisfied everywhere while still\nleaves enough degree of freedom to represent salient global deformation. We\nshow how the potential and kinetic energies can be properly discretized in this\nconfiguration space and time integrated in a fully implicit manner. The result\nis a dynamic simulator with several desirable features: We can model\nnon-trivial deformation using much fewer elements than conventional FEM method.\nIt is stable under extreme deformation, external force or large timestep size.\nAnd we can readily handle various user constraints in Euclidean space.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.04060v1"
    },
    {
        "title": "Degree reduction of composite Bézier curves",
        "authors": [
            "Przemysław Gospodarczyk",
            "Stanisław Lewanowicz",
            "Paweł Woźny"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  This paper deals with the problem of multi-degree reduction of a composite\nB\\'ezier curve with the parametric continuity constraints at the endpoints of\nthe segments. We present a novel method which is based on the idea of using\nconstrained dual Bernstein polynomials to compute the control points of the\nreduced composite curve. In contrast to other methods, ours minimizes the\n$L_2$-error for the whole composite curve instead of minimizing the\n$L_2$-errors for each segment separately. As a result, an additional\noptimization is possible. Examples show that the new method gives much better\nresults than multiple application of the degree reduction of a single B\\'ezier\ncurve.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.05433v2"
    },
    {
        "title": "A Report on Shape Deformation with a Stretching and Bending Energy",
        "authors": [
            "Hui Zhao",
            "Steven J. Gortler"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  In this report we describe a mesh editing system that we implemented that\nuses a natural stretching and bending energy defined over smooth surfaces. As\nsuch, this energy behaves uniformly under various mesh resolutions. All of the\nelements of our approach already exist in the literature. We hope that our\ndiscussions of these energies helps to shed light on the behaviors of these\nmethods and provides a unified discussion of these methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06821v1"
    },
    {
        "title": "Graphs Drawing through Fuzzy Clustering",
        "authors": [
            "Mohammadreza Ashouri",
            "Ali Golshani",
            "Dara Moazzmi",
            "Mandana Ghasemi"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Many problems can be presented in an abstract form through a wide range of\nbinary objects and relations which are defined over problem domain. In these\nproblems, graphical demonstration of defined binary objects and solutions is\nthe most suitable representation approach. In this regard, graph drawing\nproblem discusses the methods for transforming combinatorial graphs to\ngeometrical drawings in order to visualize them. This paper studies the\nforce-directed algorithms and multi-surface techniques for drawing general\nundirected graphs. Particularly, this research describes force-directed\napproach to model the drawing of a general graph as a numerical optimization\nproblem. So, it can use rich knowledge which is presented as an established\nsystem by the numerical optimization. Moreover, this research proposes the\nmulti-surface approach as an efficient tool for overcoming local minimums in\nstandard force-directed algorithms. Next, we introduce a new method for\nmulti-surface approach based on fuzzy clustering algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.07011v1"
    },
    {
        "title": "Curve Networks for Surface Reconstruction",
        "authors": [
            "Yuanhao Cao",
            "Liangliang Nan",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Man-made objects usually exhibit descriptive curved features (i.e., curve\nnetworks). The curve network of an object conveys its high-level geometric and\ntopological structure. We present a framework for extracting feature curve\nnetworks from unstructured point cloud data. Our framework first generates a\nset of initial curved segments fitting highly curved regions. We then optimize\nthese curved segments to respect both data fitting and structural regularities.\nFinally, the optimized curved segments are extended and connected into curve\nnetworks using a clustering method. To facilitate effectiveness in case of\nsevere missing data and to resolve ambiguities, we develop a user interface for\ncompleting the curve networks. Experiments on various imperfect point cloud\ndata validate the effectiveness of our curve network extraction framework. We\ndemonstrate the usefulness of the extracted curve networks for surface\nreconstruction from incomplete point clouds.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.08753v1"
    },
    {
        "title": "Adaptive Mesh Booleans",
        "authors": [
            "Ryan Schmidt",
            "Tyson Brochu"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We present a new method for performing Boolean operations on volumes\nrepresented as triangle meshes. In contrast to existing methods which treat\nmeshes as 3D polyhedra and try to partition the faces at their exact\nintersection curves, we treat meshes as adaptive surfaces which can be\narbitrarily refined. Rather than depending on computing precise face\nintersections, our approach refines the input meshes in the intersection\nregions, then discards intersecting triangles and fills the resulting holes\nwith high-quality triangles. The original intersection curves are approximated\nto a user-definable precision, and our method can identify and preserve creases\nand sharp features. Advantages of our approach include the ability to trade\nspeed for accuracy, support for open meshes, and the ability to incorporate\ntolerances to handle cases where large numbers of faces are slightly\ninter-penetrating or near-coincident.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.01760v1"
    },
    {
        "title": "Thingi10K: A Dataset of 10,000 3D-Printing Models",
        "authors": [
            "Qingnan Zhou",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Empirically validating new 3D-printing related algorithms and implementations\nrequires testing data representative of inputs encountered \\emph{in the wild}.\nAn ideal benchmarking dataset should not only draw from the same distribution\nof shapes people print in terms of class (e.g., toys, mechanisms, jewelry),\nrepresentation type (e.g., triangle soup meshes) and complexity (e.g., number\nof facets), but should also capture problems and artifacts endemic to 3D\nprinting models (e.g., self-intersections, non-manifoldness). We observe that\nthe contextual and geometric characteristics of 3D printing models differ\nsignificantly from those used for computer graphics applications, not to\nmention standard models (e.g., Stanford bunny, Armadillo, Fertility). We\npresent a new dataset of 10,000 models collected from an online 3D printing\nmodel-sharing database. Via analysis of both geometric (e.g., triangle aspect\nratios, manifoldness) and contextual (e.g., licenses, tags, classes)\ncharacteristics, we demonstrate that this dataset represents a more concise\nsummary of real-world models used for 3D printing compared to existing\ndatasets. To facilitate future research endeavors, we also present an online\nquery interface to select subsets of the dataset according to project-specific\ncharacteristics. The complete dataset and per-model statistical data are freely\navailable to the public.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.04797v2"
    },
    {
        "title": "3D Printed Stencils for Texturing Flat Surfaces",
        "authors": [
            "Vaibhav Vavilala"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We address the problem of texturing flat surfaces by spray-painting through\n3D printed stencils. We propose a system that (1) decomposes an image into\nalpha-blended layers; (2) computes a stippling given a transparency channel;\n(3) generates a 3D printed stencil given a stippling and (4) simulates the\neffects of spray-painting through the stencil.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.09737v1"
    },
    {
        "title": "On the Design and Invariants of a Ruled Surface",
        "authors": [
            "Ferhat Taş"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This paper deals with a kind of design of a ruled surface. It combines\nconcepts from the fields of computer aided geometric design and kinematics. A\ndual unit spherical B\\'ezier-like curve on the dual unit sphere (DUS) is\nobtained with respect the control points by a new method. So, with the aid of\nStudy [1] transference principle, a dual unit spherical B\\'ezier-like curve\ncorresponds to a ruled surface. Furthermore, closed ruled surfaces are\ndetermined via control points and integral invariants of these surfaces are\ninvestigated. The results are illustrated by examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00267v1"
    },
    {
        "title": "Approximate Program Smoothing Using Mean-Variance Statistics, with\n  Application to Procedural Shader Bandlimiting",
        "authors": [
            "Yuting Yang",
            "Connelly Barnes"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This paper introduces a general method to approximate the convolution of an\narbitrary program with a Gaussian kernel. This process has the effect of\nsmoothing out a program. Our compiler framework models intermediate values in\nthe program as random variables, by using mean and variance statistics. Our\napproach breaks the input program into parts and relates the statistics of the\ndifferent parts, under the smoothing process. We give several approximations\nthat can be used for the different parts of the program. These include the\napproximation of Dorn et al., a novel adaptive Gaussian approximation, Monte\nCarlo sampling, and compactly supported kernels. Our adaptive Gaussian\napproximation is accurate up to the second order in the standard deviation of\nthe smoothing kernel, and mathematically smooth. We show how to construct a\ncompiler that applies chosen approximations to given parts of the input\nprogram. Because each expression can have multiple approximation choices, we\nuse a genetic search to automatically select the best approximations. We apply\nthis framework to the problem of automatically bandlimiting procedural shader\nprograms. We evaluate our method on a variety of complex shaders, including\nshaders with parallax mapping, animation, and spatially varying statistics. The\nresulting smoothed shader programs outperform previous approaches both\nnumerically, and aesthetically, due to the smoothing properties of our\napproximations.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01208v1"
    },
    {
        "title": "QuickCSG: Fast Arbitrary Boolean Combinations of N Solids",
        "authors": [
            "Matthijs Douze",
            "Jean-Sébastien Franco",
            "Bruno Raffin"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  QuickCSG computes the result for general N-polyhedron boolean expressions\nwithout an intermediate tree of solids. We propose a vertex-centric view of the\nproblem, which simplifies the identification of final geometric contributions,\nand facilitates its spatial decomposition. The problem is then cast in a single\nKD-tree exploration, geared toward the result by early pruning of any region of\nspace not contributing to the final surface. We assume strong regularity\nproperties on the input meshes and that they are in general position. This\nsimplifying assumption, in combination with our vertex-centric approach,\nimproves the speed of the approach. Complemented with a task-stealing\nparallelization, the algorithm achieves breakthrough performance, one to two\norders of magnitude speedups with respect to state-of-the-art CPU algorithms,\non boolean operations over two to dozens of polyhedra. The algorithm also\noutperforms GPU implementations with approximate discretizations, while\nproducing an output without redundant facets. Despite the restrictive\nassumptions on the input, we show the usefulness of QuickCSG for applications\nwith large CSG problems and strong temporal constraints, e.g. modeling for 3D\nprinters, reconstruction from visual hulls and collision detection.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01558v1"
    },
    {
        "title": "A Physically Plausible Model for Rendering Highly Scattering Fluorescent\n  Participating Media",
        "authors": [
            "Marwan Abdellah",
            "Ahmet Bilgili",
            "Stefan Eilemann",
            "Henry Markram",
            "Felix Schürmann"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a novel extension of the path tracing algorithm that is capable of\ntreating highly scattering participating media in the presence of fluorescent\nstructures. The extension is based on the formulation of the full radiative\ntransfer equation when solved on a per-wavelength-basis, resulting in accurate\nmodel and unbiased algorithm for rendering highly scattering fluorescent\nparticipating media. The model accounts for the intrinsic properties of\nfluorescent dyes including their absorption and emission spectra, molar\nabsorptivity and quantum yield and also their concentration. Our algorithm is\napplied to render highly scattering isotropic fluorescent solutions under\ndifferent illumination conditions. The spectral performance of the model is\nvalidated against emission spectra of different fluorescent dyes that are of\nsignificance in spectroscopy.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.03024v2"
    },
    {
        "title": "Procedural Wang Tile Algorithm for Stochastic Wall Patterns",
        "authors": [
            "Alexandre Derouet-Jourdan",
            "Marc Salvati",
            "Theo Jonchier"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  The game and movie industries always face the challenge of reproducing\nmaterials. This problem is tackled by combining illumination models and various\ntextures (painted or procedural patterns). Gnerating stochastic wall patterns\nis crucial in the creation of a wide range of backgrounds (castles, temples,\nruins...). A specific Wang tile set was introduced previously to tackle this\nproblem, in a non-procedural fashion. Long lines may appear as visual\nartifacts. We use this tile set in a new procedural algorithm to generate\nstochastic wall patterns. For this purpose, we introduce specific hash\nfunctions implementing a constrained Wang tiling. This technique makes possible\nthe generation of boundless textures while giving control over the maximum line\nlength. The algorithm is simple and easy to implement, and the wall structure\nwe get from the tiles allows to achieve visuals that reproduce all the small\ndetails of artist painted walls.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.03950v1"
    },
    {
        "title": "Interactive Shape Perturbation",
        "authors": [
            "Juan C. Quiroz",
            "Sergiu M. Dascalu"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a web application for the procedural generation of perturbations\nof 3D models. We generate the perturbations by generating vertex shaders that\nchange the positions of vertices that make up the 3D model. The vertex shaders\nare created with an interactive genetic algorithm, which displays to the user\nthe visual effect caused by each vertex shader, allows the user to select the\nvisual effect the user likes best, and produces a new generation of vertex\nshaders using the user feedback as the fitness measure of the genetic\nalgorithm. We use genetic programming to represent each vertex shader as a\ncomputer program. This paper presents details of requirements specification,\nsoftware architecture, high and low-level design, and prototype user interface.\nWe discuss the project's current status and development challenges.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.04077v1"
    },
    {
        "title": "Degenerations of NURBS curves while all of weights approaching infinity",
        "authors": [
            "Yue Zhang",
            "Chun-Gang Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  NURBS curve is widely used in Computer Aided Design and Computer Aided\nGeometric Design. When a single weight approaches infinity, the limit of a\nNURBS curve tends to the corresponding control point. In this paper, a kind of\ncontrol structure of a NURBS curve, called regular control curve, is defined.\nWe prove that the limit of the NURBS curve is exactly its regular control curve\nwhen all of weights approach infinity, where each weight is multiplied by a\ncertain one-parameter function tending to infinity, different for each control\npoint. Moreover, some representative examples are presented to show this\nproperty and indicate its application for shape deformation.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.08262v1"
    },
    {
        "title": "Way to Go! Automatic Optimization of Wayfinding Design",
        "authors": [
            "Haikun Huang",
            "Ni-Ching Lin",
            "Lorenzo Barrett",
            "Darian Springer",
            "Hsueh-Cheng Wang",
            "Marc Pomplun",
            "Lap-Fai Yu"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Wayfinding signs play an important role in guiding users to navigate in a\nvirtual environment and in helping pedestrians to find their ways in a\nreal-world architectural site. Conventionally, the wayfinding design of a\nvirtual environment is created manually, so as the wayfinding design of a\nreal-world architectural site. The many possible navigation scenarios, as well\nas the interplay between signs and human navigation, can make the manual design\nprocess overwhelming and non-trivial. As a result, creating a wayfinding design\nfor a typical layout can take months to several years. In this paper, we\nintroduce the Way to Go! approach for automatically generating a wayfinding\ndesign for a given layout. The designer simply has to specify some navigation\nscenarios; our approach will automatically generate an optimized wayfinding\ndesign with signs properly placed considering human agents' visibility and\npossibility of making mistakes during a navigation. We demonstrate the\neffectiveness of our approach in generating wayfinding designs for different\nlayouts such as a train station, a downtown and a canyon. We evaluate our\nresults by comparing different wayfinding designs and show that our optimized\nwayfinding design can guide pedestrians to their destinations effectively and\nefficiently. Our approach can also help the designer visualize the\naccessibility of a destination from different locations, and correct any \"blind\nzone\" with additional signs.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.08891v1"
    },
    {
        "title": "Analysis and Modeling of 3D Indoor Scenes",
        "authors": [
            "Rui Ma"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We live in a 3D world, performing activities and interacting with objects in\nthe indoor environments everyday. Indoor scenes are the most familiar and\nessential environments in everyone's life. In the virtual world, 3D indoor\nscenes are also ubiquitous in 3D games and interior design. With the fast\ndevelopment of VR/AR devices and the emerging applications, the demand of\nrealistic 3D indoor scenes keeps growing rapidly. Currently, designing detailed\n3D indoor scenes requires proficient 3D designing and modeling skills and is\noften time-consuming. For novice users, creating realistic and complex 3D\nindoor scenes is even more difficult and challenging.\n  Many efforts have been made in different research communities, e.g. computer\ngraphics, vision and robotics, to capture, analyze and generate the 3D indoor\ndata. This report mainly focuses on the recent research progress in graphics on\ngeometry, structure and semantic analysis of 3D indoor data and different\nmodeling techniques for creating plausible and realistic indoor scenes. We\nfirst review works on understanding and semantic modeling of scenes from\ncaptured 3D data of the real world. Then, we focus on the virtual scenes\ncomposed of 3D CAD models and study methods for 3D scene analysis and\nprocessing. After that, we survey various modeling paradigms for creating 3D\nindoor scenes and investigate human-centric scene analysis and modeling, which\nbridge indoor scene studies of graphics, vision and robotics. At last, we\ndiscuss open problems in indoor scene processing that might bring interests to\ngraphics and all related communities.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.09577v1"
    },
    {
        "title": "There is more to PCG than Meets the Eye: NPC AI, Dynamic Camera, PVS and\n  Lightmaps",
        "authors": [
            "Anthony Savidis"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Procedural content generation (PCG) concerns all sorts of algorithms and\ntools which automatically produce game content, without requiring manual\nauthoring by game artists. Besides generating com-plex static meshes, the PCG\ncore usually encompasses geometrical information about the game world that can\nbe useful in supporting other critical subsystems of the game engine. We\ndiscuss our experi-ence from the development of the iOS game title named\n\"Fallen God: Escape Underworld\", and show how our PCG produced extra metadata\nregarding the game world, in particular: (i) an annotated dun-geon graph to\nsupport path finding for NPC AI to attack or avoid the player (working for\nbipeds, birds, insects and serpents); (ii) a quantized voxel space to allow\ndiscrete A* for the dynamic camera system to work in the continuous 3d space;\n(iii) dungeon portals to support a dynamic PVS; and (iv) procedural ambient\nocclusion and tessellation of a separate set of simplified meshes to support\nvery-fast and high-quality light mapping.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.00328v1"
    },
    {
        "title": "Toward A Deep Understanding of What Makes a Scientific Visualization\n  Memorable",
        "authors": [
            "Rui Li",
            "Jian Chen"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We report results from a preliminary study exploring the memorability of\nspatial scientific visualizations, the goal of which is to understand the\nvisual features that contribute to memorability. The evaluation metrics include\nthree objective measures (entropy, feature congestion, the number of edges),\nfour subjective ratings (clutter, the number of distinct colors, familiarity,\nand realism), and two sentiment ratings (interestingness and happiness). We\ncurate 1142 scientific visualization (SciVis) images from the original 2231\nimages in published IEEE SciVis papers from 2008 to 2017 and compute\nmemorability scores of 228 SciVis images from data collected on Amazon\nMechanical Turk (MTurk). Results showed that the memorability of SciVis images\nis mostly correlated with clutter and the number of distinct colors. We further\ninvestigate the differences between scientific visualization and infographics\nas a means to understand memorability differences by data attributes.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.00607v2"
    },
    {
        "title": "The Normal Map Based on Area-Preserving Parameterization",
        "authors": [
            "Hui Zhao",
            "Kehua Su",
            "Ming Ma",
            "Na Lei",
            "Li Cui",
            "Xianfeng Gu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In this paper, we present an approach to enhance and improve the current\nnormal map rendering technique. Our algorithm is based on semi-discrete Optimal\nMass Transportation (OMT) theory and has a solid theoretical base. The key\ndifference from previous normal map method is that we preserve the local area\nwhen we unwrap a disk-like 3D surface onto 2D plane. Compared to the currently\nused techniques which is based on conformal parameterization, our method does\nnot need to cut a surface into many small pieces to avoid the large area\ndistortion. The following charts packing step is also unnecessary in our\nframework. Our method is practical and makes the normal map technique more\nrobust and efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.01308v2"
    },
    {
        "title": "Cinematic Visualization of Multiresolution Data: Ytini for Adaptive Mesh\n  Refinement in Houdini",
        "authors": [
            "Kalina Borkiewicz",
            "J. P. Naiman",
            "Haoming Lai"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We have entered the era of large multidimensional datasets represented by\nincreasingly complex data structures. Current tools for scientific\nvisualization are not optimized to efficiently and intuitively create cinematic\nproduction quality, time-evolving representations of numerical data for broad\nimpact science communication via film, media, or journalism. To present such\ndata in a cinematic environment, it is advantageous to develop methods that\nintegrate these complex data structures into industry standard visual effects\nsoftware packages, which provide a myriad of control features otherwise\nunavailable in traditional scientific visualization software. In this paper, we\npresent the general methodology for the import and visualization of nested\nmultiresolution datasets into commercially available visual effects software.\nWe further provide a specific example of importing Adaptive Mesh Refinement\ndata into the software Houdini. This paper builds on our previous work, which\ndescribes a method for using Houdini to visualize uniform Cartesian datasets.\nWe summarize a tutorial available on the website www.ytini.com, which includes\nsample data downloads, Python code, and various other resources to simplify the\nprocess of importing and rendering multiresolution data.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.02860v2"
    },
    {
        "title": "SAGNet:Structure-aware Generative Network for 3D-Shape Modeling",
        "authors": [
            "Zhijie Wu",
            "Xiang Wang",
            "Di Lin",
            "Dani Lischinski",
            "Daniel Cohen-Or",
            "Hui Huang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present SAGNet, a structure-aware generative model for 3D shapes. Given a\nset of segmented objects of a certain class, the geometry of their parts and\nthe pairwise relationships between them (the structure) are jointly learned and\nembedded in a latent space by an autoencoder. The encoder intertwines the\ngeometry and structure features into a single latent code, while the decoder\ndisentangles the features and reconstructs the geometry and structure of the 3D\nmodel. Our autoencoder consists of two branches, one for the structure and one\nfor the geometry. The key idea is that during the analysis, the two branches\nexchange information between them, thereby learning the dependencies between\nstructure and geometry and encoding two augmented features, which are then\nfused into a single latent code. This explicit intertwining of information\nenables separately controlling the geometry and the structure of the generated\nmodels. We evaluate the performance of our method and conduct an ablation\nstudy. We explicitly show that encoding of shapes accounts for both\nsimilarities in structure and geometry. A variety of quality results generated\nby SAGNet are presented. The data and code are at\nhttps://github.com/zhijieW-94/SAGNet.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.03981v4"
    },
    {
        "title": "Image Inpainting Based on a Novel Criminisi Algorithm",
        "authors": [
            "Song Yuheng",
            "Yan Hao"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In view of the problem of image inpainting error continuation and the\ndeviation of finding best match block, an improved Criminisi algorithm is\nproposed. The improvement was mainly embodied in two aspects. In the repairing\norder aspect, we redefine the calculation formula of the priority. In order to\nsolve the problem of error continuation caused by local confidence item\nupdating, the mean value of Manhattan distance is used for replace the\nconfidence item. In the matching strategy aspect, finding the best match block\nnot only depend on the difference of the two pixels, but also consider the\nmatching region. Therefore, Euclidean distance is introduced. Experiments\nconfirm that the improved algorithm can overcome the insufficiencies of the\noriginal algorithm. The repairing effect has been improved, and the results\nhave a better visual appearance.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.04121v1"
    },
    {
        "title": "Neural Material: Learning Elastic Constitutive Material and Damping\n  Models from Sparse Data",
        "authors": [
            "Bin Wang",
            "Paul Kry",
            "Yuanmin Deng",
            "Uri Ascher",
            "Hui Huang",
            "Baoquan Chen"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The accuracy and fidelity of deformation simulations are highly dependent\nupon the underlying constitutive material model. Commonly used linear or\nnonlinear constitutive material models only cover a tiny part of possible\nmaterial behavior. In this work we propose a unified framework for modeling\ndeformable material. The key idea is to use a neural network to correct a\nnominal model of the elastic and damping properties of the object. The neural\nnetwork encapsulates a complex function that is hard to explicitly model. It\ninjects force corrections that help the forward simulation to more accurately\npredict the true behavior of a given soft object, which includes non-linear\nelastic forces and damping. Attempting to satisfy the requirement from real\nmaterial interference and animation design scenarios, we learn material models\nfrom examples of dynamic behavior of a deformable object's surface. The\nchallenge is that such data is sparse as it is consistently given only on part\nof the surface. Sparse reduced space-time optimization is employed to gradually\ngenerate increasingly accurate training data, which further refines and\nenhances the neural network. We evaluate our choice of network architecture and\nshow evidence that the modest amount of training data we use is suitable for\nthe problem tackled. Our method is demonstrated with a set of synthetic\nexamples.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.04931v1"
    },
    {
        "title": "Universal software platform for visualizing class F curves,\n  log-aesthetic curves and development of applied CAD systems",
        "authors": [
            "Rushan Ziatdinov",
            "Valerijan G. Muftejev",
            "Rustam I. Akhmetshin",
            "Alexander P. Zelev",
            "Rifkat I. Nabiyev",
            "Albert R. Mardanov"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This article describes the capabilities of a universal software platform for\nvisualizing class F curves and developing specialized applications for CAD\nsystems based on Microsoft Excel VBA, the software complex FairCurveModeler,\nand computer algebra systems. Additionally, it demonstrates the use of a\nsoftware platform for visualizing functional and log-aesthetic curves\nintegrated with CAD Fusion360. The value of the curves is evident in\nvisualizing the qualitative geometry of the product shape in industrial design.\nMoreover, the requirements for the characteristics of class F curves are\nemphasized to form a visual purity of shape in industrial design and to provide\na positive emotional perception of the visual image of the product by a person.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.06060v1"
    },
    {
        "title": "Intelligent Middle-Level Game Control",
        "authors": [
            "Amin Babadi",
            "Kourosh Naderi",
            "Perttu Hämäläinen"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We propose the concept of intelligent middle-level game control, which lies\non a continuum of control abstraction levels between the following two dual\nopposites: 1) high-level control that translates player's simple commands into\ncomplex actions (such as pressing Space key for jumping), and 2) low-level\ncontrol which simulates real-life complexities by directly manipulating, e.g.,\njoint rotations of the character as it is done in the runner game QWOP. We\nposit that various novel control abstractions can be explored using recent\nadvances in movement intelligence of game characters. We demonstrate this\nthrough design and evaluation of a novel 2-player martial arts game prototype.\nIn this game, each player guides a simulated humanoid character by clicking and\ndragging body parts. This defines the cost function for an online continuous\ncontrol algorithm that executes the requested movement. Our control algorithm\nuses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) in a rolling\nhorizon manner with custom population seeding techniques. Our playtesting data\nindicates that intelligent middle-level control results in producing novel and\ninnovative gameplay without frustrating interface complexities.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.06201v1"
    },
    {
        "title": "Image-based remapping of spatially-varying material appearance",
        "authors": [
            "Alejandro Sztrajman",
            "Jaroslav Krivanek",
            "Alexander Wilkie",
            "Tim Weyrich"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  BRDF models are ubiquitous tools for the representation of material\nappearance. However, there is now an astonishingly large number of different\nmodels in practical use. Both a lack of BRDF model standardisation across\nimplementations found in different renderers, as well as the often semantically\ndifferent capabilities of various models, have grown to be a major hindrance to\nthe interchange of production assets between different rendering systems.\nCurrent attempts to solve this problem rely on manually finding visual\nsimilarities between models, or mathematical ones between their functional\nshapes, which requires access to the shader implementation, usually unavailable\nin commercial renderers. We present a method for automatic translation of\nmaterial appearance between different BRDF models, which uses an image-based\nmetric for appearance comparison, and that delegates the interaction with the\nmodel to the renderer. We analyse the performance of the method, both with\nrespect to robustness and visual differences of the fits for multiple\ncombinations of BRDF models. While it is effective for individual BRDFs, the\ncomputational cost does not scale well for spatially-varying BRDFs. Therefore,\nwe further present a parametric regression scheme that approximates the shape\nof the transformation function and generates a reduced representation which\nevaluates instantly and without further interaction with the renderer. We\npresent respective visual comparisons of the remapped SVBRDF models for\ncommonly used renderers and shading models, and show that our approach is able\nto extrapolate transformed BRDF parameters better than other complex regression\nschemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.06715v1"
    },
    {
        "title": "Deep Portrait Image Completion and Extrapolation",
        "authors": [
            "Xian Wu",
            "Rui-Long Li",
            "Fang-Lue Zhang",
            "Jian-Cheng Liu",
            "Jue Wang",
            "Ariel Shamir",
            "Shi-Min Hu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  General image completion and extrapolation methods often fail on portrait\nimages where parts of the human body need to be recovered - a task that\nrequires accurate human body structure and appearance synthesis. We present a\ntwo-stage deep learning framework for tacking this problem. In the first stage,\ngiven a portrait image with an incomplete human body, we extract a complete,\ncoherent human body structure through a human parsing network, which focuses on\nstructure recovery inside the unknown region with the help of pose estimation.\nIn the second stage, we use an image completion network to fill the unknown\nregion, guided by the structure map recovered in the first stage. For realistic\nsynthesis the completion network is trained with both perceptual loss and\nconditional adversarial loss. We evaluate our method on public portrait image\ndatasets, and show that it outperforms other state-of-art general image\ncompletion methods. Our method enables new portrait image editing applications\nsuch as occlusion removal and portrait extrapolation. We further show that the\nproposed general learning framework can be applied to other types of images,\ne.g. animal images.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.07757v2"
    },
    {
        "title": "StretchDenoise: Parametric Curve Reconstruction with Guarantees by\n  Separating Connectivity from Residual Uncertainty of Samples",
        "authors": [
            "Stefan Ohrhallinger",
            "Michael Wimmer"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We reconstruct a closed denoised curve from an unstructured and highly noisy\n2D point cloud. Our proposed method uses a two- pass approach: Previously\nrecovered manifold connectivity is used for ordering noisy samples along this\nmanifold and express these as residuals in order to enable parametric\ndenoising. This separates recovering low-frequency features from denoising high\nfrequencies, which avoids over-smoothing. The noise probability density\nfunctions (PDFs) at samples are either taken from sensor noise models or from\nestimates of the connectivity recovered in the first pass. The output curve\nbalances the signed distances (inside/outside) to the samples. Additionally,\nthe angles between edges of the polygon representing the connectivity become\nminimized in the least-square sense. The movement of the polygon's vertices is\nrestricted to their noise extent, i.e., a cut-off distance corresponding to a\nmaximum variance of the PDFs. We approximate the resulting optimization model,\nwhich consists of higher-order functions, by a linear model with good\ncorrespondence. Our algorithm is parameter-free and operates fast on the local\nneighborhoods determined by the connectivity. We augment a least-squares solver\nconstrained by a linear system to also handle bounds. This enables us to\nguarantee stochastic error bounds for sampled curves corrupted by noise, e.g.,\nsilhouettes from sensed data, and we improve on the reconstruction error from\nground truth. Open source to reproduce figures and tables in this paper is\navailable at: https://github.com/stefango74/stretchdenoise\n",
        "pdf_link": "http://arxiv.org/pdf/1808.07778v1"
    },
    {
        "title": "Modeling Surface Appearance from a Single Photograph using\n  Self-augmented Convolutional Neural Networks",
        "authors": [
            "Xiao Li",
            "Yue Dong",
            "Pieter Peers",
            "Xin Tong"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a convolutional neural network (CNN) based solution for modeling\nphysically plausible spatially varying surface reflectance functions (SVBRDF)\nfrom a single photograph of a planar material sample under unknown natural\nillumination. Gathering a sufficiently large set of labeled training pairs\nconsisting of photographs of SVBRDF samples and corresponding reflectance\nparameters, is a difficult and arduous process. To reduce the amount of\nrequired labeled training data, we propose to leverage the appearance\ninformation embedded in unlabeled images of spatially varying materials to\nself-augment the training process. Starting from an initial approximative\nnetwork obtained from a small set of labeled training pairs, we estimate\nprovisional model parameters for each unlabeled training exemplar. Given this\nprovisional reflectance estimate, we then synthesize a novel temporary labeled\ntraining pair by rendering the exact corresponding image under a new lighting\ncondition. After refining the network using these additional training samples,\nwe re-estimate the provisional model parameters for the unlabeled data and\nrepeat the self-augmentation process until convergence. We demonstrate the\nefficacy of the proposed network structure on spatially varying wood, metals,\nand plastics, as well as thoroughly validate the effectiveness of the\nself-augmentation training process.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.00886v1"
    },
    {
        "title": "Extending Mandelbox Fractals with Shape Inversions",
        "authors": [
            "Gregg Helt"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The Mandelbox is a recently discovered class of escape-time fractals which\nuse a conditional combination of reflection, spherical inversion, scaling, and\ntranslation to transform points under iteration. In this paper we introduce a\nnew extension to Mandelbox fractals which replaces spherical inversion with a\nmore generalized shape inversion. We then explore how this technique can be\nused to generate new fractals in 2D, 3D, and 4D.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.01720v1"
    },
    {
        "title": "Texturing and Deforming Meshes with Casual Images",
        "authors": [
            "I-Chao Shen",
            "Yi-Hau Wang",
            "Yu-Mei Chen",
            "Bing-Yu Chen"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Using (casual) images to texture 3D models is a common way to create\nrealistic 3D models, which is a very important task in computer graphics.\nHowever, if the shape of the casual image does not look like the target model\nor the target mapping area, the textured model will become strange since the\nimage will be distorted very much. In this paper, we present a novel texturing\nand deforming approach for mapping the pattern and shape of a casual image to a\n3D model at the same time based on an alternating least-square approach.\nThrough a photogrammetric method, we project the target model onto the source\nimage according to the estimated camera model. Then, the target model is\ndeformed according to the shape of the source image using a surface-based\ndeformation method while minimizing the image distortion simultaneously. The\nprocesses are performed iteratively until convergence. Hence, our method can\nachieve texture mapping, shape deformation, and detail-preserving at once, and\ncan obtain more reasonable texture mapped results than traditional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.03144v1"
    },
    {
        "title": "AlSub: Fully Parallel and Modular Subdivision",
        "authors": [
            "Daniel Mlakar",
            "Martin Winter",
            "Hans-Peter Seidel",
            "Markus Steinberger",
            "Rhaleb Zayer"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In recent years, mesh subdivision---the process of forging smooth free-form\nsurfaces from coarse polygonal meshes---has become an indispensable production\ninstrument. Although subdivision performance is crucial during simulation,\nanimation and rendering, state-of-the-art approaches still rely on serial\nimplementations for complex parts of the subdivision process. Therefore, they\noften fail to harness the power of modern parallel devices, like the graphics\nprocessing unit (GPU), for large parts of the algorithm and must resort to\ntime-consuming serial preprocessing. In this paper, we show that a complete\nparallelization of the subdivision process for modern architectures is\npossible. Building on sparse matrix linear algebra, we show how to structure\nthe complete subdivision process into a sequence of algebra operations. By\nrestructuring and grouping these operations, we adapt the process for different\nuse cases, such as regular subdivision of dynamic meshes, uniform subdivision\nfor immutable topology, and feature-adaptive subdivision for efficient\nrendering of animated models. As the same machinery is used for all use cases,\nidentical subdivision results are achieved in all parts of the production\npipeline. As a second contribution, we show how these linear algebra\nformulations can effectively be translated into efficient GPU kernels. Applying\nour strategies to $\\sqrt{3}$, Loop and Catmull-Clark subdivision shows\nsignificant speedups of our approach compared to state-of-the-art solutions,\nwhile we completely avoid serial preprocessing.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.06047v3"
    },
    {
        "title": "Next Generation of Star Patterns",
        "authors": [
            "Hadi Mansourifar",
            "Weidong Shi"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In this paper we present two new ideas for generating star patterns and\nfilling the gaps during the tile operation. Firstly, we introduce a novel\nparametric method based on concentric circles for generating stars and\nrosettes. Using proposed method, completely different stars and rosettes and a\nset of new and complex star patterns convert to each other only by changing\nnine parameters. Secondly, we demonstrate how three equal tangent circles can\nbe used as a base for generating tile elements. For this reason a surrounded\ncircle is created among tangent circles, which represents the gaps in hexagonal\npacking. Afterwards, we use our first idea for filling the tangent circles and\nsurrounded circle. This parametric approach can be used for generating infinite\nnew star patterns, which some of them will be presented in result section.Two\nAndroid apps of proposed method called Starking and Tilerking are available in\nGoogle app store.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.09270v1"
    },
    {
        "title": "3D Face Synthesis Driven by Personality Impression",
        "authors": [
            "Yining Lang",
            "Wei Liang",
            "Yujia Wang",
            "Lap-Fai Yu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Synthesizing 3D faces that give certain personality impressions is commonly\nneeded in computer games, animations, and virtual world applications for\nproducing realistic virtual characters. In this paper, we propose a novel\napproach to synthesize 3D faces based on personality impression for creating\nvirtual characters. Our approach consists of two major steps. In the first\nstep, we train classifiers using deep convolutional neural networks on a\ndataset of images with personality impression annotations, which are capable of\npredicting the personality impression of a face. In the second step, given a 3D\nface and a desired personality impression type as user inputs, our approach\noptimizes the facial details against the trained classifiers, so as to\nsynthesize a face which gives the desired personality impression. We\ndemonstrate our approach for synthesizing 3D faces giving desired personality\nimpressions on a variety of 3D face models. Perceptual studies show that the\nperceived personality impressions of the synthesized faces agree with the\ntarget personality impressions specified for synthesizing the faces. Please\nrefer to the supplementary materials for all results.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.10402v1"
    },
    {
        "title": "Fast and Scalable Position-Based Layout Synthesis",
        "authors": [
            "Tomer Weiss",
            "Alan Litteneker",
            "Noah Duncan",
            "Masaki Nakada",
            "Chenfanfu Jiang",
            "Lap-Fai Yu",
            "Demetri Terzopoulos"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The arrangement of objects into a layout can be challenging for non-experts,\nas is affirmed by the existence of interior design professionals. Recent\nresearch into the automation of this task has yielded methods that can\nsynthesize layouts of objects respecting aesthetic and functional constraints\nthat are non-linear and competing. These methods usually adopt a stochastic\noptimization scheme, which samples from different layout configurations, a\nprocess that is slow and inefficient. We introduce an physics-motivated,\ncontinuous layout synthesis technique, which results in a significant gain in\nspeed and is readily scalable. We demonstrate our method on a variety of\nexamples and show that it achieves results similar to conventional layout\nsynthesis based on Markov chain Monte Carlo (McMC) state-search, but is faster\nby at least an order of magnitude and can handle layouts of unprecedented size\nas well as tightly-packed layouts that can overwhelm McMC.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.10526v1"
    },
    {
        "title": "Functional Maps Representation on Product Manifolds",
        "authors": [
            "Emanuele Rodolà",
            "Zorah Lähner",
            "Alex M. Bronstein",
            "Michael M. Bronstein",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We consider the tasks of representing, analyzing and manipulating maps\nbetween shapes. We model maps as densities over the product manifold of the\ninput shapes; these densities can be treated as scalar functions and therefore\nare manipulable using the language of signal processing on manifolds. Being a\nmanifold itself, the product space endows the set of maps with a geometry of\nits own, which we exploit to define map operations in the spectral domain; we\nalso derive relationships with other existing representations (soft maps and\nfunctional maps). To apply these ideas in practice, we discretize product\nmanifolds and their Laplace--Beltrami operators, and we introduce localized\nspectral analysis of the product manifold as a novel tool for map processing.\nOur framework applies to maps defined between and across 2D and 3D shapes\nwithout requiring special adjustment, and it can be implemented efficiently\nwith simple operations on sparse matrices.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.10940v2"
    },
    {
        "title": "NeuralDrop: DNN-based Simulation of Small-Scale Liquid Flows on Solids",
        "authors": [
            "Rajaditya Mukherjee",
            "Qingyang Li",
            "Zhili Chen",
            "Shicheng Chu",
            "Huamin Wang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Small-scale liquid flows on solid surfaces provide convincing details in\nliquid animation, but they are difficult to be simulated with efficiency and\nfidelity, mostly due to the complex nature of the surface tension at the\ncontact front where liquid, air, and solid meet. In this paper, we propose to\nsimulate the dynamics of new liquid drops from captured real-world liquid flow\ndata, using deep neural networks. To achieve this goal, we develop a data\ncapture system that acquires liquid flow patterns from hundreds of real-world\nwater drops. We then convert raw data into compact data for training neural\nnetworks, in which liquid drops are represented by their contact fronts in a\nLagrangian form. Using the LSTM units based on recurrent neural networks, our\nneural networks serve three purposes in our simulator: predicting the contour\nof a contact front, predicting the color field gradient of a contact front, and\nfinally predicting whether a contact front is going to break or not. Using\nthese predictions, our simulator recovers the overall shape of a liquid drop at\nevery time step, and handles merging and splitting events by simple operations.\nThe experiment shows that our trained neural networks are able to perform\npredictions well. The whole simulator is robust, convenient to use, and capable\nof generating realistic small-scale liquid effects in animation.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.02517v1"
    },
    {
        "title": "Printable Aggregate Elements",
        "authors": [
            "Jérémie Dumas",
            "Jonàs Martínez",
            "Sylvain Lefebvre",
            "Li-Yi Wei"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Aggregating base elements into rigid objects such as furniture or sculptures\nis a great way for designers to convey a specific look and feel. Unfortunately,\nthere is no existing solution to help model structurally sound aggregates. The\nchallenges stem from the fact that the final shape and its structural\nproperties emerge from the arrangements of the elements, whose sizes are large\nso that they remain easily identifiable. Therefore there is a very tight\ncoupling between the object shape, structural properties, and the precise\nlayout of the elements.\n  We present the first method to create aggregates of elements that are\nstructurally sound and can be manufactured on 3D printers. Rather than having\nto assemble an aggregate shape by painstakingly positioning elements one by\none, users of our method only have to describe the structural purpose of the\ndesired object. This is done by specifying a set of external forces and\nattachment points. The algorithm then automatically optimizes a layout of\nuser-provided elements that answers the specified scenario. The elements can\nhave arbitrary shapes: convex, concave, elongated, and can be allowed to\ndeform.\n  Our approach creates connections between elements through small overlaps\npreserving their appearance, while optimizing for the global rigidity of the\nresulting aggregate. We formulate a topology optimization problem whose design\nvariables are the positions and orientations of individual elements. Global\nrigidity is maximized through a dedicated gradient descent scheme. Due to the\nchallenging setting -- number of elements, arbitrary shapes, orientation, and\nconstraints in 3D -- we propose several novel steps to achieve convergence.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.02626v1"
    },
    {
        "title": "Fast, High Precision Ray/Fiber Intersection using Tight, Disjoint\n  Bounding Volumes",
        "authors": [
            "Nikolaus Binder",
            "Alexander Keller"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Analyzing and identifying the shortcomings of current subdivision methods for\nfinding intersections of rays with fibers defined by the surface of a circular\ncontour swept along a B\\'ezier curve, we present a new algorithm that improves\nprecision and performance. Instead of the inefficient pruning using overlapping\naxis aligned bounding boxes and determining the closest point of approach of\nthe ray and the curve, we prune using disjoint bounding volumes defined by\ncylinders and calculate the intersections on the limit surface. This in turn\nallows for computing accurate parametric position and normal in the point of\nintersection. The iteration requires only one bit per subdivision to avoid\ncostly stack memory operations. At a low number of subdivisions, the\nperformance of the high precision algorithm is competitive, while for a high\nnumber of subdivisions it dramatically outperforms the state-of-the-art.\nBesides an extensive mathematical analysis, source code is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03374v1"
    },
    {
        "title": "Massively Parallel Stackless Ray Tracing of Catmull-Clark Subdivision\n  Surfaces",
        "authors": [
            "Nikolaus Binder",
            "Alexander Keller"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a fast and efficient method for intersecting rays with\nCatmull-Clark subdivision surfaces. It takes advantage of the approximation\ndemocratized by OpenSubdiv, in which regular patches are represented by tensor\nproduct B\\'ezier surfaces and irregular ones are approximated using Gregory\npatches. Our algorithm operates solely on the original patch data and can\nprocess both patch types simultaneously with only a small amount of control\nflow divergence. Besides introducing an optimized method to determine axis\naligned bounding boxes of Gregory patches restricted in the parametric domain,\nseveral techniques are introduced that accelerate the recursive subdivision\nprocess including stackless operation, efficient work distribution, and control\nflow optimizations. The algorithm is especially useful for quick turnarounds\nduring patch editing and animation playback.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03510v1"
    },
    {
        "title": "VV-Net: Voxel VAE Net with Group Convolutions for Point Cloud\n  Segmentation",
        "authors": [
            "Hsien-Yu Meng",
            "Lin Gao",
            "YuKun Lai",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a novel algorithm for point cloud segmentation. Our approach\ntransforms unstructured point clouds into regular voxel grids, and further uses\na kernel-based interpolated variational autoencoder (VAE) architecture to\nencode the local geometry within each voxel. Traditionally, the voxel\nrepresentation only comprises Boolean occupancy information which fails to\ncapture the sparsely distributed points within voxels in a compact manner. In\norder to handle sparse distributions of points, we further employ radial basis\nfunctions (RBF) to compute a local, continuous representation within each\nvoxel. Our approach results in a good volumetric representation that\neffectively tackles noisy point cloud datasets and is more robust for learning.\nMoreover, we further introduce group equivariant CNN to 3D, by defining the\nconvolution operator on a symmetry group acting on $\\mathbb{Z}^3$ and its\nisomorphic sets. This improves the expressive capacity without increasing\nparameters, leading to more robust segmentation results. We highlight the\nperformance on standard benchmarks and show that our approach outperforms\nstate-of-the-art segmentation algorithms on the ShapeNet and S3DIS datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04337v2"
    },
    {
        "title": "Web3D Graphics enabled through Sensor Networks for Cost-Effective\n  Assessment and Management of Energy Efficiency in Buildings",
        "authors": [
            "Felix Hamza-Lup",
            "Marcel Maghiar"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The past decade has seen the advent of numerous building energy efficiency\nvisualization and simulation systems; however, most of them rely on theoretical\nthermal models to suggest building structural design for new constructions and\nmodifications for existing ones. Sustainable methods of construction have made\ntremendous progress. The example of the German Energy-Plus- House technology\nuses a combination of (almost) zero-carbon passive heating technologies. A\nweb-enabled X3D visualization and simulation system coupled with a\ncost-effective set of temperature/humidity sensors can provide valuable\ninsights into building design, materials and construction that can lead to\nsignificant energy savings and an improved thermal comfort for residents,\nresulting in superior building energy efficiency. A cost-effective\nhardware-software prototype system is proposed in this paper that can provide\nreal-time data driven visualization or offline simulation of 3D thermal maps\nfor residential and/or commercial buildings on the Web.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.05046v1"
    },
    {
        "title": "Total Positivity of A Kind of Generalized Toric-Bernstein Basis",
        "authors": [
            "Ying-Ying Yu",
            "Hui Ma",
            "Chun-Gang Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The normalized totally positive bases are widely used in many fields.Based on\nthe generalized Vandermonde determinant, the normalized total positivity of a\nkind of generalized toric-Bernstein basis is proved, which is defined on a set\nof real points. By this result, the progressive iterative approximation\nproperty of the generalized toric-B\\'{e}zier curve is obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.05674v2"
    },
    {
        "title": "Hair-GANs: Recovering 3D Hair Structure from a Single Image",
        "authors": [
            "Meng Zhang",
            "Youyi Zheng"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We introduce Hair-GANs, an architecture of generative adversarial networks,\nto recover the 3D hair structure from a single image. The goal of our networks\nis to build a parametric transformation from 2D hair maps to 3D hair structure.\nThe 3D hair structure is represented as a 3D volumetric field which encodes\nboth the occupancy and the orientation information of the hair strands. Given a\nsingle hair image, we first align it with a bust model and extract a set of 2D\nmaps encoding the hair orientation information in 2D, along with the bust depth\nmap to feed into our Hair-GANs. With our generator network, we compute the 3D\nvolumetric field as the structure guidance for the final hair synthesis. The\nmodeling results not only resemble the hair in the input image but also\npossesses many vivid details in other views. The efficacy of our method is\ndemonstrated by using a variety of hairstyles and comparing with the prior art.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.06229v1"
    },
    {
        "title": "Iso-parametric tool path planning for point clouds",
        "authors": [
            "Qiang Zou",
            "Jibin Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The computational consuming and non-robust reconstruction from point clouds\nto either meshes or spline surfaces motivates the direct tool path planning for\npoint clouds. In this paper, a novel approach for planning iso-parametric tool\npath from a point cloud is presented. The planning depends on the\nparameterization of point clouds. Accordingly, a conformal map is employed to\nbuild the parameterization which leads to a significant simplification of\ncomputing tool path parameters and boundary conformed paths. Then, Tool path is\ngenerated through linear interpolation with the forward and side step computed\nagainst specified chord deviation and scallop height, respectively.\nExperimental results are given to illustrate effectiveness of the proposed\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.06600v1"
    },
    {
        "title": "Fast quasi-conformal regional flattening of the left atrium",
        "authors": [
            "Marta Nuñez-Garcia",
            "Gabriel Bernardino",
            "Francisco Alarcón",
            "Gala Caixal",
            "Lluís Mont",
            "Oscar Camara",
            "Constantine Butakoff"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Two-dimensional representation of 3D anatomical structures is a simple and\nintuitive way for analysing patient information across populations and image\nmodalities. It also allows convenient visualizations that can be included in\nclinical reports for a fast overview of the whole structure. While cardiac\nventricles, especially the left ventricle, have an established standard\nrepresentation (e.g. bull's eye plot), the 2D depiction of the left atrium (LA)\nis challenging due to its sub-structural complexity including the pulmonary\nveins (PV) and the left atrial appendage (LAA). Quasi-conformal flattening\ntechniques, successfully applied to cardiac ventricles, require additional\nconstraints in the case of the LA to place the PV and LAA in the same\ngeometrical 2D location for different cases. Some registration-based methods\nhave been proposed but 3D (or 2D) surface registration is time-consuming and\nprone to errors if the geometries are very different. We propose a novel atrial\nflattening methodology where a quasi-conformal 2D map of the LA is obtained\nquickly and without errors related to registration. In our approach, the LA is\ndivided into 5 regions which are then mapped to their analogue two-dimensional\nregions. A dataset of 67 human left atria from magnetic resonance images (MRI)\nwas studied to derive a population-based 2D LA template representing the\naveraged relative locations of the PVs and LAA. The clinical application of the\nproposed methodology is illustrated on different use cases including the\nintegration of MRI and electroanatomical data.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.06896v2"
    },
    {
        "title": "STOAViz: Visualizing Saturated Thickness of Ogallala Aquifer",
        "authors": [
            "Tommy Dang",
            "Long Nguyen",
            "Abdullah Karim",
            "Venkatesh Uddameri"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In this paper, we introduce STOAViz, a visual analytics tool for analyzing\nthe saturated thickness of the Ogallala aquifer. The saturated thicknesses are\nmonitored by sensors integrated on wells distributed on a vast geographic area.\nOur analytics application also captures the trends and patterns (such as\naverage/standard deviation over time, sudden increase/decrease of saturated\nthicknesses) of water on an individual well and a group of wells based on their\ngeographic locations. To highlight the usefulness and effectiveness of STOAViz,\nwe demonstrate it on the Southern High Plains Aquifer of Texas. The work was\ndeveloped using feedback from experts at the water resource center at a\nuniversity. Moreover, our technique can be applied on any geographic areas\nwhere wells and their measurements are available.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.07385v1"
    },
    {
        "title": "A Study on 3D Surface Graph Representations",
        "authors": [
            "Long Nguyen",
            "Abdullah Karim"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Surface graphs have been used in many application domains to represent\nthree-dimensional (3D) data. Another approach to representing 3D data is making\nprojections onto two-dimensional (2D) graphs. This approach will result in\nmultiple displays, which is time-consuming in switching between different\nscreens for a different perspective. In this work, we study the performance of\n3D version of popular 2D visualization techniques for time series: horizon\ngraph, small multiple, and simple line graph. We explore discrimination tasks\nwith respect to each visualization technique that requires simultaneous\nrepresentations. We demonstrate our study by visualizing saturated thickness of\nthe Ogallala aquifer - the Southern High Plains Aquifer of Texas in multiple\nyears. For the evaluation, we design comparison and discrimination tasks and\nautomatically record result performed by a group of students at a university.\nOur results show that 3D small multiples perform well with stable accuracy over\nnumbers of occurrences. On the other hand, shared-space visualization within a\nsingle 3D coordinate system is more efficient with small number of simultaneous\ngraphs. 3D horizon graph loses its competence in the 3D coordinate system with\nthe lowest accuracy comparing to other techniques. Our demonstration of 3D\nspatial-temporal is also presented on the Southern High Plains Aquifer of Texas\nfrom 2010 to 2016.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.07390v1"
    },
    {
        "title": "Iso-level tool path planning for free-form surfaces",
        "authors": [
            "Qiang Zou",
            "Juyong Zhang",
            "Bailin Deng",
            "Jibin Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The aim of tool path planning is to maximize the efficiency against some\ngiven precision criteria. In practice, scallop height should be kept constant\nto avoid unnecessary cutting, while the tool path should be smooth enough to\nmaintain a high feed rate. However, iso-scallop and smoothness often conflict\nwith each other. Existing methods smooth iso-scallop paths one-by-one, which\nmake the final tool path far from being globally optimal. This paper proposes a\nnew framework for tool path optimization. It views a family of iso-level curves\nof a scalar function defined over the surface as tool path so that desired tool\npath can be generated by finding the function that minimizes certain energy\nfunctional and different objectives can be considered simultaneously. We use\nthe framework to plan globally optimal tool path with respect to iso-scallop\nand smoothness. The energy functionals for planning iso-scallop, smoothness,\nand optimal tool path are respectively derived, and the path topology is\nstudied too. Experimental results are given to show the effectiveness of the\nproposed methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.07580v1"
    },
    {
        "title": "Generating Classes of 3D Virtual Mandibles for AR-Based Medical\n  Simulation",
        "authors": [
            "Neha R. Hippalgaonkar",
            "Alexa D. Sider",
            "Felix G. Hamza-Lup",
            "Anand P. Santhanam",
            "Bala Jaganathan",
            "Celina Imielinska",
            "Jannick P. Rolland"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Simulation and modeling represent promising tools for several application\ndomains from engineering to forensic science and medicine. Advances in 3D\nimaging technology convey paradigms such as augmented reality (AR) and mixed\nreality inside promising simulation tools for the training industry. Motivated\nby the requirement for superimposing anatomically correct 3D models on a Human\nPatient Simulator (HPS) and visualizing them in an AR environment, the purpose\nof this research effort is to derive method for scaling a source human mandible\nto a target human mandible. Results show that, given a distance between two\nsame landmarks on two different mandibles, a relative scaling factor may be\ncomputed. Using this scaling factor, results show that a 3D virtual mandible\nmodel can be made morphometrically equivalent to a real target-specific\nmandible within a 1.30 millimeter average error bound. The virtual mandible may\nbe further used as a reference target for registering other anatomical models,\nsuch as the lungs, on the HPS. Such registration will be made possible by\nphysical constraints among the mandible and the spinal column in the horizontal\nnormal rest position.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.08053v1"
    },
    {
        "title": "Procedural Crowd Generation for Semantically Augmented Virtual Cities",
        "authors": [
            "O. Rogla",
            "N. Pelechano",
            "G. Patow"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Authoring realistic behaviors to populate a large virtual city can be a\ncumbersome, time-consuming and error-prone task. Believable crowds require the\neffort of storytellers and programming experts working together for long\nperiods of time. In this work, we present a new framework to allow users to\ngenerate populated environments in an easier and faster way, by relying on the\nuse of procedural techniques. Our framework consists of the procedural\ngeneration of semantically-augmented virtual cities to drive the procedural\ngeneration and simulation of crowds. The main novelty lies in the generation of\nagendas for each individual inhabitant (alone or as part of a family) by using\na rule-based grammar that combines city semantics with the autonomous persons'\ncharacteristics. Real-world data can be used to accommodate the generation of a\nvirtual population, thus enabling the recreation of more realistic scenarios.\nUsers can author a new population or city by editing rule files with the\nflexibility of re-using, combining or extending the rules of previous\npopulations. The results show how logical and consistent behaviors can be\neasily generated for a large crowd providing a good starting point to bring\nvirtual cities to life.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.10036v1"
    },
    {
        "title": "Interactive X-ray and proton therapy training and simulation",
        "authors": [
            "Felix G. Hamza-Lup",
            "Shane Farrar",
            "Erik Leon"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  External beam X-ray therapy (XRT) and proton therapy (PT) are effective and\nwidely accepted forms of treatment for many types of cancer. However, the\nprocedures require extensive computerized planning. Current planning systems\nfor both XRT and PT have insufficient visual aid to combine real patient data\nwith the treatment device geometry to account for unforeseen collisions among\nsystem components and the patient. We are proposing a cost-effective method to\nextract patient specific S-reps in real time, and combine them with the\ntreatment system geometry to provide a comprehensive simulation of the XRT/PT\ntreatment room. The X3D standard is used to implement and deploy the simulator\non the web, enabling its use not only for remote specialists' collaboration,\nsimulation, and training, but also for patient education.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.11941v1"
    },
    {
        "title": "Online External Beam Radiation Treatment Simulator",
        "authors": [
            "Felix G. Hamza-Lup",
            "Ivan Sopin",
            "Omar Zeidan"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Radiation therapy is an effective and widely accepted form of treatment for\nmany types of cancer that requires extensive computerized planning.\nUnfortunately, current treatment planning systems have limited or no visual aid\nthat combines patient volumetric models extracted from patient-specific CT data\nwith the treatment device geometry in a 3D interactive simulation. We\nillustrate the potential of 3D simulation in radiation therapy with a web-based\ninteractive system that combines novel standards and technologies. We discuss\nrelated research efforts in this area and present in detail several components\nof the simulator. An objective assessment of the accuracy of the simulator and\na usability study prove the potential of such a system for simulation and\ntraining.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.11947v1"
    },
    {
        "title": "Increasing the Capability of Neural Networks for Surface Reconstruction\n  from Noisy Point Clouds",
        "authors": [
            "Adam R White",
            "Li Bai"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This paper builds upon the current methods to increase their capability and\nautomation for 3D surface construction from noisy and potentially sparse point\nclouds. It presents an analysis of an artificial neural network surface\nregression and mapping method, describing caveats, improvements and\njustification for the different approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.12464v2"
    },
    {
        "title": "Inviwo -- A Visualization System with Usage Abstraction Levels",
        "authors": [
            "Daniel Jönsson",
            "Peter Steneteg",
            "Erik Sundén",
            "Rickard Englund",
            "Sathish Kottravel",
            "Martin Falk",
            "Anders Ynnerman",
            "Ingrid Hotz",
            "Timo Ropinski"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The complexity of today's visualization applications demands specific\nvisualization systems tailored for the development of these applications.\nFrequently, such systems utilize levels of abstraction to improve the\napplication development process, for instance by providing a data flow network\neditor. Unfortunately, these abstractions result in several issues, which need\nto be circumvented through an abstraction-centered system design. Often, a high\nlevel of abstraction hides low level details, which makes it difficult to\ndirectly access the underlying computing platform, which would be important to\nachieve an optimal performance. Therefore, we propose a layer structure\ndeveloped for modern and sustainable visualization systems allowing developers\nto interact with all contained abstraction levels. We refer to this interaction\ncapabilities as usage abstraction levels, since we target application\ndevelopers with various levels of experience. We formulate the requirements for\nsuch a system, derive the desired architecture, and present how the concepts\nhave been exemplary realized within the Inviwo visualization system.\nFurthermore, we address several specific challenges that arise during the\nrealization of such a layered architecture, such as communication between\ndifferent computing platforms, performance centered encapsulation, as well as\nlayer-independent development by supporting cross layer documentation and\ndebugging capabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.12517v2"
    },
    {
        "title": "Constructing Trivariate B-splines with Positive Jacobian by Pillow\n  Operation and Geometric Iterative Fitting",
        "authors": [
            "Hongwei Lin",
            "Hao Huang",
            "Chuanfeng Hu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The advent of isogeometric analysis has prompted a need for methods to\ngenerate Trivariate B-spline Solids (TBS) with positive Jacobian. However, it\nis difficult to guarantee a positive Jacobian of a TBS since the geometric\npre-condition for ensuring the positive Jacobian is very complicated. In this\npaper, we propose a method for generating TBSs with guaranteed positive\nJacobian. For the study, we used a tetrahedral (tet) mesh model and segmented\nit into sub-volumes using the pillow operation. Then, to reduce the difficulty\nin ensuring a positive Jacobian, we separately fitted the boundary curves and\nsurfaces and the sub-volumes using a geometric iterative fitting algorithm.\nFinally, the smoothness between adjacent TBSs is improved. The experimental\nexamples presented in this paper demonstrate the effectiveness and efficiency\nof the developed algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.12597v1"
    },
    {
        "title": "Gregory Solid Construction for Polyhedral Volume Parameterization by\n  Sparse Optimization",
        "authors": [
            "Chuanfeng Hu",
            "Hongwei Lin"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In isogeometric analysis, it is frequently required to handle the geometric\nmodels enclosed by four-sided or non-four-sided boundary patches, such as\ntrimmed surfaces. In this paper, we develop a Gregory solid based method to\nparameterize those models. First, we extend the Gregory patch representation to\nthe trivariate Gregory solid representation. Second, the trivariate Gregory\nsolid representation is employed to interpolate the boundary patches of a\ngeometric model, thus generating the polyhedral volume parametrization. To\nimprove the regularity of the polyhedral volume parametrization, we formulate\nthe construction of the trivariate Gregory solid as a sparse optimization\nproblem, where the optimization objective function is a linear combination of\nsome terms, including a sparse term aiming to reduce the negative Jacobian area\nof the Gregory solid. Then, the alternating direction method of multipliers\n(ADMM) is used to solve the sparse optimization problem. Lots of experimental\nexamples illustrated in this paper demonstrate the effectiveness and efficiency\nof the developed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.12599v1"
    },
    {
        "title": "Modeling Data-Driven Dominance Traits for Virtual Characters using Gait\n  Analysis",
        "authors": [
            "Tanmay Randhavane",
            "Aniket Bera",
            "Emily Kubin",
            "Kurt Gray",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a data-driven algorithm for generating gaits of virtual characters\nwith varying dominance traits. Our formulation utilizes a user study to\nestablish a data-driven dominance mapping between gaits and dominance labels.\nWe use our dominance mapping to generate walking gaits for virtual characters\nthat exhibit a variety of dominance traits while interacting with the user.\nFurthermore, we extract gait features based on known criteria in visual\nperception and psychology literature that can be used to identify the dominance\nlevels of any walking gait. We validate our mapping and the perceived dominance\ntraits by a second user study in an immersive virtual environment. Our gait\ndominance classification algorithm can classify the dominance traits of gaits\nwith ~73% accuracy. We also present an application of our approach that\nsimulates interpersonal relationships between virtual characters. To the best\nof our knowledge, ours is the first practical approach to classifying gait\ndominance and generate dominance traits in virtual characters.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.02037v1"
    },
    {
        "title": "Collaborative 3D modeling system based on blockchain",
        "authors": [
            "Hunmin Park",
            "Sung-Eui Yoon"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a collaborative 3D modeling system, which is based on the\nblockchain technology. Our approach uses the blockchain to communicate with\nmodeling tools and to provide them a decentralized database of the mesh\nmodification history. This approach also provides a server-less version control\nsystem: users can commit their modifications to the blockchain and checkout\nothers' modifications from the blockchain. As a result, our system enables\nusers to do collaborative modeling without any central server.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.02629v1"
    },
    {
        "title": "Stroke-based sketched symbol reconstruction and segmentation",
        "authors": [
            "Kurmanbek Kaiyrbekov",
            "Metin Sezgin"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Hand-drawn objects usually consist of multiple semantically meaningful parts.\nFor example, a stick figure consists of a head, a torso, and pairs of legs and\narms. Efficient and accurate identification of these subparts promises to\nsignificantly improve algorithms for stylization, deformation, morphing and\nanimation of 2D drawings. In this paper, we propose a neural network model that\nsegments symbols into stroke-level components. Our segmentation framework has\ntwo main elements: a fixed feature extractor and a Multilayer Perceptron (MLP)\nnetwork that identifies a component based on the feature. As the feature\nextractor we utilize an encoder of a stroke-rnn, which is our newly proposed\ngenerative Variational Auto-Encoder (VAE) model that reconstructs symbols on a\nstroke by stroke basis. Experiments show that a single encoder could be reused\nfor segmenting multiple categories of sketched symbols with negligible effects\non segmentation accuracies. Our segmentation scores surpass existing\nmethodologies on an available small state of the art dataset. Moreover,\nextensive evaluations on our newly annotated big dataset demonstrate that our\nframework obtains significantly better accuracies as compared to baseline\nmodels. We release the dataset to the community.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.03427v2"
    },
    {
        "title": "Joint Stabilization and Direction of 360°Videos",
        "authors": [
            "Chengzhou Tang",
            "Oliver Wang",
            "Feng Liu",
            "Ping Tan"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  360{\\deg} video provides an immersive experience for viewers, allowing them\nto freely explore the world by turning their head. However, creating\nhigh-quality 360{\\deg} video content can be challenging, as viewers may miss\nimportant events by looking in the wrong direction, or they may see things that\nruin the immersion, such as stitching artifacts and the film crew. We take\nadvantage of the fact that not all directions are equally likely to be\nobserved; most viewers are more likely to see content located at ``true\nnorth'', i.e. in front of them, due to ergonomic constraints. We therefore\npropose 360{\\deg} video direction, where the video is jointly optimized to\norient important events to the front of the viewer and visual clutter behind\nthem, while producing smooth camera motion. Unlike traditional video, viewers\ncan still explore the space as desired, but with the knowledge that the most\nimportant content is likely to be in front of them. Constraints can be user\nguided, either added directly on the equirectangular projection or by recording\n``guidance'' viewing directions while watching the video in a VR headset, or\nautomatically computed, such as via visual saliency or forward motion\ndirection. To accomplish this, we propose a new motion estimation technique\nspecifically designed for 360{\\deg} video which outperforms the commonly used\n5-point algorithm on wide angle video. We additionally formulate the direction\nproblem as an optimization where a novel parametrization of spherical warping\nallows us to correct for some degree of parallax effects. We compare our\napproach to recent methods that address stabilization-only and converting\n360{\\deg} video to narrow field-of-view video.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04161v1"
    },
    {
        "title": "PointWise: An Unsupervised Point-wise Feature Learning Network",
        "authors": [
            "Matan Shoef",
            "Sharon Fogel",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a novel approach to learning a point-wise, meaningful embedding\nfor point-clouds in an unsupervised manner, through the use of neural-networks.\nThe domain of point-cloud processing via neural-networks is rapidly evolving,\nwith novel architectures and applications frequently emerging. Within this\nfield of research, the availability and plethora of unlabeled point-clouds as\nwell as their possible applications make finding ways of characterizing this\ntype of data appealing. Though significant advancement was achieved in the\nrealm of unsupervised learning, its adaptation to the point-cloud\nrepresentation is not trivial. Previous research focuses on the embedding of\nentire point-clouds representing an object in a meaningful manner. We present a\ndeep learning framework to learn point-wise description from a set of shapes\nwithout supervision. Our approach leverages self-supervision to define a\nrelevant loss function to learn rich per-point features. We train a\nneural-network with objectives based on context derived directly from the raw\ndata, with no added annotation. We use local structures of point-clouds to\nincorporate geometric information into each point's latent representation. In\naddition to using local geometric information, we encourage adjacent points to\nhave similar representations and vice-versa, creating a smoother, more\ndescriptive representation. We demonstrate the ability of our method to capture\nmeaningful point-wise features through three applications. By clustering the\nlearned embedding space, we perform unsupervised part-segmentation on point\nclouds. By calculating euclidean distance in the latent space we derive\nsemantic point-analogies. Finally, by retrieving nearest-neighbors in our\nlearned latent space we present meaningful point-correspondence within and\namong point-clouds.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04544v2"
    },
    {
        "title": "Image Synthesis and Style Transfer",
        "authors": [
            "Somnuk Phon-Amnuaisuk"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Affine transformation, layer blending, and artistic filters are popular\nprocesses that graphic designers employ to transform pixels of an image to\ncreate a desired effect. Here, we examine various approaches that synthesize\nnew images: pixel-based compositing models and in particular, distributed\nrepresentations of deep neural network models. This paper focuses on\nsynthesizing new images from a learned representation model obtained from the\nVGG network. This approach offers an interesting creative process from its\ndistributed representation of information in hidden layers of a deep VGG\nnetwork i.e., information such as contour, shape, etc. are effectively captured\nin hidden layers of neural networks. Conceptually, if $\\Phi$ is the function\nthat transforms input pixels into distributed representations of VGG layers\n${\\bf h}$, a new synthesized image $X$ can be generated from its inverse\nfunction, $X = \\Phi^{-1}({\\bf h})$. We describe the concept behind the\napproach, present some representative synthesized images and style-transferred\nimage examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04686v1"
    },
    {
        "title": "Computational Design of Lightweight Trusses",
        "authors": [
            "Caigui Jiang",
            "Chengcheng Tang",
            "Hans-Peter Seidel",
            "Renjie Chen",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Trusses are load-carrying light-weight structures consisting of bars\nconnected at joints ubiquitously applied in a variety of engineering scenarios.\nDesigning optimal trusses that satisfy functional specifications with a minimal\namount of material has interested both theoreticians and practitioners for more\nthan a century. In this paper, we introduce two main ideas to improve upon the\nstate of the art. First, we formulate an alternating linear programming problem\nfor geometry optimization. Second, we introduce two sets of complementary\ntopological operations, including a novel subdivision scheme for global\ntopology refinement inspired by Michell's famed theoretical study. Based on\nthese two ideas, we build an efficient computational framework for the design\nof lightweight trusses. \\AD{We illustrate our framework with a variety of\nfunctional specifications and extensions. We show that our method achieves\ntrusses with smaller volumes and is over two orders of magnitude faster\ncompared with recent state-of-the-art approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.05637v1"
    },
    {
        "title": "Automatic normal orientation in point clouds of building interiors",
        "authors": [
            "Sebastian Ochmann",
            "Reinhard Klein"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Orienting surface normals correctly and consistently is a fundamental problem\nin geometry processing. Applications such as visualization, feature detection,\nand geometry reconstruction often rely on the availability of correctly\noriented normals. Many existing approaches for automatic orientation of normals\non meshes or point clouds make severe assumptions on the input data or the\ntopology of the underlying object which are not applicable to real-world\nmeasurements of urban scenes. In contrast, our approach is specifically\ntailored to the challenging case of unstructured indoor point cloud scans of\nmulti-story, multi-room buildings. We evaluate the correctness and speed of our\napproach on multiple real-world point cloud datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.06487v2"
    },
    {
        "title": "Periodic-corrected data driven coupling of blood flow and vessel wall\n  for virtual surgery",
        "authors": [
            "Xuejie Mai",
            "Zhiyong Yuan",
            "Qianqian Tong",
            "Tianchen Yuan",
            "Jianhui Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Fast and realistic coupling of blood flow and vessel wall is of great\nimportance to virtual surgery. In this paper, we propose a novel data-driven\ncoupling method that formulates physics-based blood flow simulation as a\nregression problem, using an improved periodic-corrected neural network\n(PcNet), estimating the acceleration of every particle at each frame to obtain\nfast, stable and realistic simulation. We design a particle state feature\nvector based on smoothed particle hydrodynamics (SPH), modeling the mixed\ncontribution of neighboring proxy particles on the blood vessel wall and\nneighboring blood particles, giving the extrapolation ability to deal with more\ncomplex couplings. We present a semi-supervised training strategy to improve\nthe traditional BP neural network, which corrects the error periodically to\nensure long term stability. Experimental results demonstrate that our method is\nable to implement stable and vivid coupling of blood flow and vessel wall while\ngreatly improving computational efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.08397v1"
    },
    {
        "title": "Automatic reconstruction of fully volumetric 3D building models from\n  point clouds",
        "authors": [
            "Sebastian Ochmann",
            "Richard Vock",
            "Reinhard Klein"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a novel method for reconstructing parametric, volumetric,\nmulti-story building models from unstructured, unfiltered indoor point clouds\nby means of solving an integer linear optimization problem. Our approach\novercomes limitations of previous methods in several ways: First, we drop\nassumptions about the input data such as the availability of separate scans as\nan initial room segmentation. Instead, a fully automatic room segmentation and\noutlier removal is performed on the unstructured point clouds. Second,\nrestricting the solution space of our optimization approach to arrangements of\nvolumetric wall entities representing the structure of a building enforces a\nconsistent model of volumetric, interconnected walls fitted to the observed\ndata instead of unconnected, paper-thin surfaces. Third, we formulate the\noptimization as an integer linear programming problem which allows for an exact\nsolution instead of the approximations achieved with most previous techniques.\nLastly, our optimization approach is designed to incorporate hard constraints\nwhich were difficult or even impossible to integrate before. We evaluate and\ndemonstrate the capabilities of our proposed approach on a variety of complex\nreal-world point clouds.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.00631v1"
    },
    {
        "title": "Computational Design of Skinned Quad-Robots",
        "authors": [
            "Xudong Feng",
            "Jiafeng Liu",
            "Huamin Wang",
            "Yin Yang",
            "Hujun Bao",
            "Bernd Bickel",
            "Weiwei Xu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a computational design system that assists users to model,\noptimize, and fabricate quad-robots with soft skins.Our system addresses the\nchallenging task of predicting their physical behavior by fully integrating the\nmultibody dynamics of the mechanical skeleton and the elastic behavior of the\nsoft skin. The developed motion control strategy uses an alternating\noptimization scheme to avoid expensive full space time-optimization,\ninterleaving space-time optimization for the skeleton and frame-by-frame\noptimization for the full dynamics. The output are motor torques to drive the\nrobot to achieve a user prescribed motion trajectory.We also provide a\ncollection of convenient engineering tools and empirical manufacturing guidance\nto support the fabrication of the designed quad-robot. We validate the\nfeasibility of designs generated with our system through physics simulations\nand with a physically-fabricated prototype.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.00893v1"
    },
    {
        "title": "Progressive Refinement Imaging",
        "authors": [
            "Markus Kluge",
            "Tim Weyrich",
            "Andreas Kolb"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper presents a novel technique for progressive online integration of\nuncalibrated image sequences with substantial geometric and/or photometric\ndiscrepancies into a single, geometrically and photometrically consistent\nimage. Our approach can handle large sets of images, acquired from a nearly\nplanar or infinitely distant scene at different resolutions in object domain\nand under variable local or global illumination conditions. It allows for\nefficient user guidance as its progressive nature provides a valid and\nconsistent reconstruction at any moment during the online refinement process.\nOur approach avoids global optimization techniques, as commonly used in the\nfield of image refinement, and progressively incorporates new imagery into a\ndynamically extendable and memory-efficient Laplacian pyramid. Our image\nregistration process includes a coarse homography and a local refinement stage\nusing optical flow. Photometric consistency is achieved by retaining the\nphotometric intensities given in a reference image, while it is being refined.\nGlobally blurred imagery and local geometric inconsistencies due to, e.g.\nmotion are detected and removed prior to image fusion. We demonstrate the\nquality and robustness of our approach using several image and video sequences,\nincluding handheld acquisition with mobile phones and zooming sequences with\nconsumer cameras.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.04099v2"
    },
    {
        "title": "Void-and-Cluster Sampling of Large Scattered Data and Trajectories",
        "authors": [
            "Tobias Rapp",
            "Christoph Peters",
            "Carsten Dachsbacher"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a data reduction technique for scattered data based on statistical\nsampling. Our void-and-cluster sampling technique finds a representative subset\nthat is optimally distributed in the spatial domain with respect to the blue\nnoise property. In addition, it can adapt to a given density function, which we\nuse to sample regions of high complexity in the multivariate value domain more\ndensely. Moreover, our sampling technique implicitly defines an ordering on the\nsamples that enables progressive data loading and a continuous level-of-detail\nrepresentation. We extend our technique to sample time-dependent trajectories,\nfor example pathlines in a time interval, using an efficient and iterative\napproach. Furthermore, we introduce a local and continuous error measure to\nquantify how well a set of samples represents the original dataset. We apply\nthis error measure during sampling to guide the number of samples that are\ntaken. Finally, we use this error measure and other quantities to evaluate the\nquality, performance, and scalability of our algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.05073v2"
    },
    {
        "title": "Towards Robust Direction Invariance in Character Animation",
        "authors": [
            "Li-Ke Ma",
            "Zeshi Yang",
            "Baining Guo",
            "KangKang Yin"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In character animation, direction invariance is a desirable property. That\nis, a pose facing north and the same pose facing south are considered the same;\na character that can walk to the north is expected to be able to walk to the\nsouth in a similar style. To achieve such direction invariance, the current\npractice is to remove the facing direction's rotation around the vertical axis\nbefore further processing. Such a scheme, however, is not robust for rotational\nbehaviors in the sagittal plane. In search of a smooth scheme to achieve\ndirection invariance, we prove that in general a singularity free scheme does\nnot exist. We further connect the problem with the hairy ball theorem, which is\nbetter-known to the graphics community. Due to the nonexistence of a\nsingularity free scheme, a general solution does not exist and we propose a\nremedy by using a properly-chosen motion direction that can avoid singularities\nfor specific motions at hand. We perform comparative studies using two\ndeep-learning based methods, one builds kinematic motion representations and\nthe other learns physics-based controls. The results show that with our robust\ndirection invariant features, both methods can achieve better results in terms\nof learning speed and/or final quality. We hope this paper can not only boost\nperformance for character animation methods, but also help related communities\ncurrently not fully aware of the direction invariance problem to achieve more\nrobust results.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.06790v4"
    },
    {
        "title": "LightGuider: Guiding Interactive Lighting Design using Suggestions,\n  Provenance, and Quality Visualization",
        "authors": [
            "Andreas Walch",
            "Michael Schwärzler",
            "Christian Luksch",
            "Elmar Eisemann",
            "Theresia Gschwandtner"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  LightGuider is a novel guidance-based approach to interactive lighting\ndesign, which typically consists of interleaved 3D modeling operations and\nlight transport simulations. Rather than having designers use a trial-and-error\napproach to match their illumination constraints and aesthetic goals,\nLightGuider supports the process by simulating potential next modeling steps\nthat can deliver the most significant improvements. LightGuider takes\npredefined quality criteria and the current focus of the designer into account\nto visualize suggestions for lighting-design improvements via a specialized\nprovenance tree. This provenance tree integrates snapshot visualizations of how\nwell a design meets the given quality criteria weighted by the designer's\npreferences. This integration facilitates the analysis of quality improvements\nover the course of a modeling workflow as well as the comparison of alternative\ndesign solutions. We evaluate our approach with three lighting designers to\nillustrate its usefulness.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.08553v2"
    },
    {
        "title": "DeepOrganNet: On-the-Fly Reconstruction and Visualization of 3D / 4D\n  Lung Models from Single-View Projections by Deep Deformation Network",
        "authors": [
            "Yifan Wang",
            "Zichun Zhong",
            "Jing Hua"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper introduces a deep neural network based method, i.e., DeepOrganNet,\nto generate and visualize high-fidelity 3D / 4D organ geometric models from\nsingle-view medical image in real time. Traditional 3D / 4D medical image\nreconstruction requires near hundreds of projections, which cost insufferable\ncomputational time and deliver undesirable high imaging / radiation dose to\nhuman subjects. Moreover, it always needs further notorious processes to\nextract the accurate 3D organ models subsequently. To our knowledge, there is\nno method directly and explicitly reconstructing multiple 3D organ meshes from\na single 2D medical grayscale image on the fly. Given single-view 2D medical\nimages, e.g., 3D / 4D-CT projections or X-ray images, our end-to-end\nDeepOrganNet framework can efficiently and effectively reconstruct 3D / 4D lung\nmodels with a variety of geometric shapes by learning the smooth deformation\nfields from multiple templates based on a trivariate tensor-product deformation\ntechnique, leveraging an informative latent descriptor extracted from input 2D\nimages. The proposed method can guarantee to generate high-quality and\nhigh-fidelity manifold meshes for 3D / 4D lung models. The major contributions\nof this work are to accurately reconstruct the 3D organ shapes from 2D\nsingle-view projection, significantly improve the procedure time to allow\non-the-fly visualization, and dramatically reduce the imaging dose for human\nsubjects. Experimental results are evaluated and compared with the traditional\nreconstruction method and the state-of-the-art in deep learning, by using\nextensive 3D and 4D examples from synthetic phantom and real patient datasets.\nThe proposed method only needs several milliseconds to generate organ meshes\nwith 10K vertices, which has a great potential to be used in real-time image\nguided radiation therapy (IGRT).\n",
        "pdf_link": "http://arxiv.org/pdf/1907.09375v1"
    },
    {
        "title": "A system for efficient 3D printed stop-motion face animation",
        "authors": [
            "Rinat Abdrashitov",
            "Alec Jacobson",
            "Karan Singh"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Computer animation in conjunction with 3D printing has the potential to\npositively impact traditional stop-motion animation. As 3D printing every frame\nof a computer animation is prohibitively slow and expensive, 3D printed\nstop-motion can only be viable if animations can be faithfully reproduced using\na compact library of 3D printed and efficiently assemblable parts. We thus\npresent the first system for processing computer animation sequences (typically\nfaces) to produce an optimal set of replacement parts for use in 3D printed\nstop-motion animation. Given an input animation sequence of topology invariant\ndeforming meshes, our problem is to output a library of replacement parts and\nper-animation-frame assignment of the parts, such that we maximally approximate\nthe input animation, while minimizing the amount of 3D printing and assembly.\nInspired by current stop-motion workflows, a user manually indicates which\nparts of the model are preferred for segmentation; then, we find curves with\nminimal deformation along which to segment the mesh. We then present a novel\nalgorithm to zero out deformations along the segment boundaries, so that\nreplacement sets for each part can be interchangeably and seamlessly assembled\ntogether. The part boundaries are designed to ease 3D printing and\ninstrumentation for assembly. Each part is then independently optimized using a\ngraph-cut technique to find a set of replacements, whose size can be user\ndefined, or automatically computed to adhere to a printing budget or allowed\ndeviation from the original animation. Our evaluation is threefold: we show\nresults on a variety of facial animations, both digital and 3D printed,\ncritiqued by a professional animator; we show the impact of various algorithmic\nparameters; and compare our results to naive solutions. Our approach can reduce\nthe printing time and cost significantly for stop-motion animated films.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.10163v1"
    },
    {
        "title": "Spectral Visualization Sharpening",
        "authors": [
            "Liang Zhou",
            "Rudolf Netzel",
            "Daniel Weiskopf",
            "Chris Johnson"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper, we propose a perceptually-guided visualization sharpening\ntechnique. We analyze the spectral behavior of an established comprehensive\nperceptual model to arrive at our approximated model based on an adapted\nweighting of the bandpass images from a Gaussian pyramid. The main benefit of\nthis approximated model is its controllability and predictability for\nsharpening color-mapped visualizations. Our method can be integrated into any\nvisualization tool as it adopts generic image-based post-processing, and it is\nintuitive and easy to use as viewing distance is the only parameter. Using\nhighly diverse datasets, we show the usefulness of our method across a wide\nrange of typical visualizations.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.10208v1"
    },
    {
        "title": "Data-Driven Physical Face Inversion",
        "authors": [
            "Yeara Kozlov",
            "Hongyi Xu",
            "Moritz Bächer",
            "Derek Bradley",
            "Markus Gross",
            "Thabo Beeler"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Facial animation is one of the most challenging problems in computer\ngraphics, and it is often solved using linear heuristics like blend-shape\nrigging. More expressive approaches like physical simulation have emerged, but\nthese methods are very difficult to tune, especially when simulating a real\nactor's face. We propose to use a simple finite element simulation approach for\nface animation, and present a novel method for recovering the required\nsimulation parameters in order to best match a real actor's face motion. Our\nmethod involves reconstructing a very small number of head poses of the actor\nin 3D, where the head poses span different configurations of force directions\ndue to gravity. Our algorithm can then automatically recover both the\ngravity-free rest shape of the face as well as the spatially-varying physical\nmaterial stiffness such that a forward simulation will match the captured\ntargets as closely as possible. As a result, our system can produce\nactor-specific, physical parameters that can be immediately used in recent\nphysical simulation methods for faces. Furthermore, as the simulation results\ndepend heavily on the chosen spatial layout of material clusters, we analyze\nand compare different spatial layouts.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.10402v1"
    },
    {
        "title": "FAKIR: An algorithm for revealing the anatomy and pose of statues from\n  raw point sets",
        "authors": [
            "Tong Fu",
            "Raphaëlle Chaine",
            "Julie Digne"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  3D acquisition of archaeological artefacts has become an essential part of\ncultural heritage research for preservation or restoration purpose. Statues, in\nparticular, have been at the center of many projects. In this paper, we\nintroduce a way to improve the understanding of acquired statues representing\nreal or imaginary creatures by registering a simple and pliable articulated\nmodel to the raw point set data. Our approach performs a Forward And bacKward\nIterative Registration (FAKIR) which proceeds joint by joint, needing only a\nfew iterations to converge. We are thus able to detect the pose and elementary\nanatomy of sculptures, with possibly non realistic body proportions. By\nadapting our simple skeleton, our method can work on animals and imaginary\ncreatures.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11721v3"
    },
    {
        "title": "Overlap-free Drawing of Generalized Pythagoras Trees for Hierarchy\n  Visualization",
        "authors": [
            "Tanja Munz",
            "Michael Burch",
            "Toon van Benthem",
            "Yoeri Poels",
            "Fabian Beck",
            "Daniel Weiskopf"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Generalized Pythagoras trees were developed for visualizing hierarchical\ndata, producing organic, fractal-like representations. However, the drawback of\nthe original layout algorithm is visual overlap of tree branches. To avoid such\noverlap, we introduce an adapted drawing algorithm using ellipses instead of\ncircles to recursively place tree nodes representing the subhierarchies. Our\ntechnique is demonstrated by resolving overlap in diverse real-world and\ngenerated datasets, while comparing the results to the original approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12845v1"
    },
    {
        "title": "A Comparison of Radial and Linear Charts for Visualizing Daily Pattern",
        "authors": [
            "Manuela Waldner",
            "Alexandra Diehl",
            "Denis Gracanin",
            "Rainer Splechtna",
            "Claudio Delrieux",
            "Kresimir Matkovic"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Radial charts are generally considered less effective than linear charts.\nPerhaps the only exception is in visualizing periodical time-dependent data,\nwhich is believed to be naturally supported by the radial layout. It has been\ndemonstrated that the drawbacks of radial charts outweigh the benefits of this\nnatural mapping. Visualization of daily patterns, as a special case, has not\nbeen systematically evaluated using radial charts. In contrast to yearly or\nweekly recurrent trends, the analysis of daily patterns on a radial chart may\nbenefit from our trained skill on reading radial clocks that are ubiquitous in\nour culture. In a crowd-sourced experiment with 92 non-expert users, we\nevaluated the accuracy, efficiency, and subjective ratings of radial and linear\ncharts for visualizing daily traffic accident patterns. We systematically\ncompared juxtaposed 12-hours variants and single 24-hours variants for both\nlayouts in four low-level tasks and one high-level interpretation task. Our\nresults show that over all tasks, the most elementary 24-hours linear bar chart\nis most accurate and efficient and is also preferred by the users. This\nprovides strong evidence for the use of linear layouts - even for visualizing\nperiodical daily patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.13534v1"
    },
    {
        "title": "Sparse Surface Constraints for Combining Physics-based Elasticity\n  Simulation and Correspondence-Free Object Reconstruction",
        "authors": [
            "Sebastian Weiss",
            "Robert Maier",
            "Rüdiger Westermann",
            "Daniel Cremers",
            "Nils Thuerey"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We address the problem to infer physical material parameters and boundary\nconditions from the observed motion of a homogeneous deformable object via the\nsolution of an inverse problem. Parameters are estimated from potentially\nunreliable real-world data sources such as sparse observations without\ncorrespondences. We introduce a novel Lagrangian-Eulerian optimization\nformulation, including a cost function that penalizes differences to\nobservations during an optimization run. This formulation matches\ncorrespondence-free, sparse observations from a single-view depth sequence with\na finite element simulation of deformable bodies. In conjunction with an\nefficient hexahedral discretization and a stable, implicit formulation of\ncollisions, our method can be used in demanding situation to recover a variety\nof material parameters, ranging from Young's modulus and Poisson ratio to\ngravity and stiffness damping, and even external boundaries. In a number of\ntests using synthetic datasets and real-world measurements, we analyse the\nrobustness of our approach and the convergence behavior of the numerical\noptimization scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.01812v1"
    },
    {
        "title": "Deep Radiance Caching: Convolutional Autoencoders Deeper in Ray Tracing",
        "authors": [
            "Giulio Jiang",
            "Bernhard Kainz"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Rendering realistic images with global illumination is a computationally\ndemanding task and often requires dedicated hardware for feasible runtime.\nRecent research uses Deep Neural Networks to predict indirect lighting on image\nlevel, but such methods are commonly limited to diffuse materials and require\ntraining on each scene.We present Deep Radiance Caching (DRC), an efficient\nvariant of Radiance Caching utilizing Convolutional Autoencoders for rendering\nglobal illumination. DRC employs a denoising neural network with Radiance\nCaching to support a wide range of material types, without the requirement of\noffline pre-computation or training for each scene.This offers high performance\nCPU rendering for maximum accessibility. Our method has been evaluated on\ninterior scenes, and is able to produce high-quality images within 180 seconds\non a single CPU.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.02480v2"
    },
    {
        "title": "Cubic Stylization",
        "authors": [
            "Hsueh-Ti Derek Liu",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a 3D stylization algorithm that can turn an input shape into the\nstyle of a cube while maintaining the content of the original shape. The key\ninsight is that cubic style sculptures can be captured by the\nas-rigid-as-possible energy with an l1-regularization on rotated surface\nnormals. Minimizing this energy naturally leads to a detail-preserving, cubic\ngeometry. Our optimization can be solved efficiently without any mesh surgery.\nOur method serves as a non-realistic modeling tool where one can incorporate\nmany artistic controls to create stylized geometries.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.02926v3"
    },
    {
        "title": "Visual Abstraction",
        "authors": [
            "Ivan Viola",
            "Min Chen",
            "Tobias Isenberg"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this article we revisit the concept of abstraction as it is used in\nvisualization and put it on a solid formal footing. While the term\n\\emph{abstraction} is utilized in many scientific disciplines, arts, as well as\neveryday life, visualization inherits the notion of data abstraction or class\nabstraction from computer science, topological abstraction from mathematics,\nand visual abstraction from arts. All these notions have a lot in common, yet\nthere is a major discrepancy in the terminology and basic understanding about\nvisual abstraction in the context of visualization. We thus root the notion of\nabstraction in the philosophy of science, clarify the basic terminology, and\nprovide crisp definitions of visual abstraction as a process. Furthermore, we\nclarify how it relates to similar terms often used interchangeably in the field\nof visualization. Visual abstraction is characterized by a conceptual space\nwhere this process exists, by the purpose it should serve, and by the\nperceptual and cognitive qualities of the beholder. These characteristics can\nbe used to control the process of visual abstraction to produce effective and\ninformative visual representations.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.03310v2"
    },
    {
        "title": "Triangle Mesh Slicing and Contour Construction for Three-Dimensional\n  Printing on a Rotating Mandrel",
        "authors": [
            "Kyle Reeser",
            "Christopher Conlon",
            "Amber L. Doiron"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Three-dimensional (3D) printing is a powerful development tool both in\nindustry, as well as in biomedical research. Additive-lathe 3D printing is an\nemerging sub-class of 3D printing whereby material is layered outward from the\nsurface of a rotating cylindrical mandrel. While established additive\nmanufacturing technologies have developed robust toolpath generation software,\nadditive-lathe publications to date have been relegated to the most basic of\nproof-of-concept structures. This paper details the theory and implementation\nof a method for slicing a triangulated surface with a series of concentric,\nopen, right circular cylinders that represents a crucial step in creating\ntoolpaths to print complex models with additive-lathe technology. Valid edge\ncases are detailed which must be addressed when implementing a cylindrical\nslicer to produce non-intersecting closed contours; two classes of resultant\nclosed contour are described. Methodologies for generating infill patterns,\nsupport structures and other considerations for toolpath construction are\nrequired prior to full implementation of a machine capable of printing complex\ngeometry from a digital model onto a rotating cylindrical surface. This work\nrepresents the first thorough examination of the mathematics and algorithmic\nimplementation of triangle mesh slicing with concentric cylinders and offers\ninsights for future works in toolpath generation for the additive-lathe type 3D\nprinter.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.04037v1"
    },
    {
        "title": "Addressing Troubles with Double Bubbles: Convergence and Stability at\n  Multi-Bubble Junctions",
        "authors": [
            "Yun Fei",
            "Christopher Batty",
            "Eitan Grinspun"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this report we discuss and propose a correction to a convergence and\nstability issue occurring in the work of Da et al.[2015], in which they\nproposed a numerical model to simulate soap bubbles.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.06402v2"
    },
    {
        "title": "Animation Synthesis Triggered by Vocal Mimics",
        "authors": [
            "Adrien Nivaggioli",
            "Damien Rohmer"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a method leveraging the naturally time-related expressivity of our\nvoice to control an animation composed of a set of short events. The user\nrecords itself mimicking onomatopoeia sounds such as \"Tick\", \"Pop\", or \"Chhh\"\nwhich are associated with specific animation events. The recorded soundtrack is\nautomatically analyzed to extract every instant and types of sounds. We finally\nsynthesize an animation where each event type and timing correspond with the\nsoundtrack. In addition to being a natural way to control animation timing, we\ndemonstrate that multiple stories can be efficiently generated by recording\ndifferent voice sequences. Also, the use of more than one soundtrack allows us\nto control different characters with overlapping actions.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08462v1"
    },
    {
        "title": "Top-Down Shape Abstraction Based on Greedy Pole Selection",
        "authors": [
            "Zhiyang Dou",
            "Shiqing Xin",
            "Rui Xu",
            "Jian Xu",
            "Yuanfeng Zhou",
            "Shuangmin Chen",
            "Wenping Wang",
            "Xiuyang Zhao",
            "Changhe Tu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Motivated by the fact that the medial axis transform is able to encode nearly\nthe complete shape, we propose to use as few medial balls as possible to\napproximate the original enclosed volume by the boundary surface. We\nprogressively select new medial balls, in a top-down style, to enlarge the\nregion spanned by the existing medial balls. The key spirit of the selection\nstrategy is to encourage large medial balls while imposing given geometric\nconstraints. We further propose a speedup technique based on a provable\nobservation that the intersection of medial balls implies the adjacency of\npower cells (in the sense of the power crust). We further elaborate the\nselection rules in combination with two closely related applications. One\napplication is to develop an easy-to-use ball-stick modeling system that helps\nnon-professional users to quickly build a shape with only balls and wires, but\nany penetration between two medial balls must be suppressed. The other\napplication is to generate porous structures with convex, compact (with a high\nisoperimetric quotient) and shape-aware pores where two adjacent spherical\npores may have penetration as long as the mechanical rigidity can be well\npreserved.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08954v3"
    },
    {
        "title": "Dynamic Upsampling of Smoke through Dictionary-based Learning",
        "authors": [
            "Kai Bai",
            "Wei Li",
            "Mathieu Desbrun",
            "Xiaopei Liu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Simulating turbulent smoke flows is computationally intensive due to their\nintrinsic multiscale behavior, thus requiring relatively high resolution grids\nto fully capture their complexity. For iterative editing or simply faster\ngeneration of smoke flows, dynamic upsampling of an input low-resolution\nnumerical simulation is an attractive, yet currently unattainable goal. In this\npaper, we propose a novel dictionary-based learning approach to the dynamic\nupsampling of smoke flows. For each frame of an input coarse animation, we seek\na sparse representation of small, local velocity patches of the flow based on\nan over-complete dictionary, and use the resulting sparse coefficients to\ngenerate a high-resolution smoke animation sequence. We propose a novel\ndictionary-based neural network which learns both a fast evaluation of sparse\npatch encoding and a dictionary of corresponding coarse and fine patches from a\nsequence of example simulations computed with any numerical solver. Our\nupsampling network then injects into coarse input sequences physics-driven fine\ndetails, unlike most previous approaches that only employed fast procedural\nmodels to add high frequency to the input. We present a variety of upsampling\nresults for smoke flows and offer comparisons to their corresponding\nhigh-resolution simulations to demonstrate the effectiveness of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.09166v1"
    },
    {
        "title": "Style-compatible Object Recommendation for Multi-room Indoor Scene\n  Synthesis",
        "authors": [
            "Yu He",
            "Yun Cai",
            "Yuan-Chen Guo",
            "Zheng-Ning Liu",
            "Shao-Kui Zhang",
            "Song-Hai Zhang",
            "Hong-Bo Fu",
            "Sheng-Yong Chen"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Traditional indoor scene synthesis methods often take a two-step approach:\nobject selection and object arrangement. Current state-of-the-art object\nselection approaches are based on convolutional neural networks (CNNs) and can\nproduce realistic scenes for a single room. However, they cannot be directly\nextended to synthesize style-compatible scenes for multiple rooms with\ndifferent functions. To address this issue, we treat the object selection\nproblem as combinatorial optimization based on a Labeled LDA (L-LDA) model. We\nfirst calculate occurrence probability distribution of object categories\naccording to a topic model, and then sample objects from each category\nconsidering their function diversity along with style compatibility, while\nregarding not only separate rooms, but also associations among rooms. User\nstudy shows that our method outperforms the baselines by incorporating\nmulti-function and multi-room settings with style constraints, and sometimes\neven produces plausible scenes comparable to those produced by professional\ndesigners.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.04187v2"
    },
    {
        "title": "Geodesic Distance Field-based Curved Layer Volume Decomposition for\n  Multi-Axis Support-free Printing",
        "authors": [
            "Yamin Li",
            "Dong He",
            "Xiangyu Wang",
            "Kai Tang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper presents a new curved layer volume decomposition method for\nmulti-axis support-free printing of freeform solid parts. Given a solid model\nto be printed that is represented as a tetrahedral mesh, we first establish a\ngeodesic distance field embedded on the mesh, whose value at any vertex is the\ngeodesic distance to the base of the model. Next, the model is naturally\ndecomposed into curved layers by interpolating a number of iso-geodesic\ndistance surfaces (IGDSs). These IGDSs morph from bottom-up in an intrinsic and\nsmooth way owing to the nature of geodesics, which will be used as the curved\nprinting layers that are friendly to multi-axis printing. In addition, to cater\nto the collision-free requirement and to improve the printing efficiency, we\nalso propose a printing sequence optimization algorithm for determining the\nprinting order of the IGDSs, which helps reduce the air-move path length. Ample\nexperiments in both computer simulation and physical printing are performed,\nand the experimental results confirm the advantages of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05938v1"
    },
    {
        "title": "Real-time Image Smoothing via Iterative Least Squares",
        "authors": [
            "Wei Liu",
            "Pingping Zhang",
            "Xiaolin Huang",
            "Jie Yang",
            "Chunhua Shen",
            "Ian Reid"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Edge-preserving image smoothing is a fundamental procedure for many computer\nvision and graphic applications. There is a tradeoff between the smoothing\nquality and the processing speed: the high smoothing quality usually requires a\nhigh computational cost which leads to the low processing speed. In this paper,\nwe propose a new global optimization based method, named iterative least\nsquares (ILS), for efficient edge-preserving image smoothing. Our approach can\nproduce high-quality results but at a much lower computational cost.\nComprehensive experiments demonstrate that the propose method can produce\nresults with little visible artifacts. Moreover, the computation of ILS can be\nhighly parallel, which can be easily accelerated through either multi-thread\ncomputing or the GPU hardware. With the acceleration of a GTX 1080 GPU, it is\nable to process images of 1080p resolution ($1920\\times1080$) at the rate of\n20fps for color images and 47fps for gray images. In addition, the ILS is\nflexible and can be modified to handle more applications that require different\nsmoothing properties. Experimental results of several applications show the\neffectiveness and efficiency of the proposed method. The code is available at\n\\url{https://github.com/wliusjtu/Real-time-Image-Smoothing-via-Iterative-Least-Squares}\n",
        "pdf_link": "http://arxiv.org/pdf/2003.07504v2"
    },
    {
        "title": "Gaussian Curvature Filter on 3D Meshes",
        "authors": [
            "Wenming Tang",
            "Yuanhao Gong",
            "Kanglin Liu",
            "Jun Liu",
            "Wei Pan",
            "Bozhi Liu",
            "Guoping Qiu"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Minimizing the Gaussian curvature of meshes can play a fundamental role in 3D\nmesh processing. However, there is a lack of computationally efficient and\nrobust Gaussian curvature optimization method. In this paper, we present a\nsimple yet effective method that can efficiently reduce Gaussian curvature for\n3D meshes. We first present the mathematical foundation of our method. Then, we\nintroduce a simple and robust implicit Gaussian curvature optimization method\nnamed Gaussian Curvature Filter (GCF). GCF implicitly minimizes Gaussian\ncurvature without the need to explicitly calculate the Gaussian curvature\nitself. GCF is highly efficient and this method can be used in a large range of\napplications that involve Gaussian curvature. We conduct extensive experiments\nto demonstrate that GCF significantly outperforms state-of-the-art methods in\nminimizing Gaussian curvature, and geometric feature preserving soothing on 3D\nmeshes. GCF program is available at https://github.com/tangwenming/GCF-filter.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.09178v2"
    },
    {
        "title": "Rig-space Neural Rendering",
        "authors": [
            "Dominik Borer",
            "Lu Yuhang",
            "Laura Wuelfroth",
            "Jakob Buhmann",
            "Martin Guay"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Movie productions use high resolution 3d characters with complex proprietary\nrigs to create the highest quality images possible for large displays.\nUnfortunately, these 3d assets are typically not compatible with real-time\ngraphics engines used for games, mixed reality and real-time pre-visualization.\nConsequently, the 3d characters need to be re-modeled and re-rigged for these\nnew applications, requiring weeks of work and artistic approval. Our solution\nto this problem is to learn a compact image-based rendering of the original 3d\ncharacter, conditioned directly on the rig parameters. Our idea is to render\nthe character in many different poses and views, and to train a deep neural\nnetwork to render high resolution images, from the rig parameters directly.\nMany neural rendering techniques have been proposed to render from 2d\nskeletons, or geometry and UV maps. However these require manual work, and to\ndo not remain compatible with the animator workflow of manipulating rig\nwidgets, as well as the real-time game engine pipeline of interpolating rig\nparameters. We extend our architecture to support dynamic re-lighting and\ncomposition with other 3d objects in the scene. We designed a network that\nefficiently generates multiple scene feature maps such as normals, depth,\nalbedo and mask, which are composed with other scene objects to form the final\nimage.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.09820v1"
    },
    {
        "title": "Pose to Seat: Automated Design of Body-Supporting Surfaces",
        "authors": [
            "Kurt Leimer",
            "Andreas Winkler",
            "Stefan Ohrhallinger",
            "Przemyslaw Musialski"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The design of functional seating furniture is a complicated process which\noften requires extensive manual design effort and empirical evaluation. We\npropose a computational design framework for pose-driven automated generation\nof body-supports which are optimized for comfort of sitting. Given a human body\nin a specified pose as input, our method computes an approximate pressure\ndistribution that also takes frictional forces and body torques into\nconsideration which serves as an objective measure of comfort. Utilizing this\ninformation to find out where the body needs to be supported in order to\nmaintain comfort of sitting, our algorithm can create a supporting mesh suited\nfor a person in that specific pose. This is done in an automated fitting\nprocess, using a template model capable of supporting a large variety of\nsitting poses. The results can be used directly or can be considered as a\nstarting point for further interactive design.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.10435v1"
    },
    {
        "title": "Perspective picture from Visual Sphere: a new approach to image\n  rasterization",
        "authors": [
            "Jakub Maksymilian Fober"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper alternative method for real-time 3D model rasterization is\ngiven. Surfaces are drawn in perspective-map space which acts as a virtual\ncamera lens. It can render single-pass 360{\\deg} angle of view (AOV) image of\nunlimited shape, view-directions count and unrestrained projection geometry\n(e.g. direct lens distortion, projection mapping, curvilinear perspective),\nnatively aliasing-free. In conjunction to perspective vector map, visual-sphere\nperspective model is proposed. A model capable of combining pictures from\nsources previously incompatible, like fish-eye camera and wide-angle lens\npicture. More so, method is proposed for measurement and simulation of a real\noptical system variable no-parallax point (NPP). This study also explores\nphilosophical and historical aspects of picture perception and presents a guide\nfor perspective design.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.10558v4"
    },
    {
        "title": "Global Illumination of non-Euclidean spaces",
        "authors": [
            "Tiago Novello",
            "Vinicius da Silva",
            "Luiz Velho"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper presents a path tracer algorithm to compute the global\nillumination of non-Euclidean manifolds. We use the 3D torus as an example.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.11133v1"
    },
    {
        "title": "Human Motion Transfer with 3D Constraints and Detail Enhancement",
        "authors": [
            "Yang-Tian Sun",
            "Qian-Cheng Fu",
            "Yue-Ren Jiang",
            "Zitao Liu",
            "Yu-Kun Lai",
            "Hongbo Fu",
            "Lin Gao"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose a new method for realistic human motion transfer using a\ngenerative adversarial network (GAN), which generates a motion video of a\ntarget character imitating actions of a source character, while maintaining\nhigh authenticity of the generated results. We tackle the problem by decoupling\nand recombining the posture information and appearance information of both the\nsource and target characters. The innovation of our approach lies in the use of\nthe projection of a reconstructed 3D human model as the condition of GAN to\nbetter maintain the structural integrity of transfer results in different\nposes. We further introduce a detail enhancement net to enhance the details of\ntransfer results by exploiting the details in real source frames. Extensive\nexperiments show that our approach yields better results both qualitatively and\nquantitatively than the state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13510v4"
    },
    {
        "title": "Multi-feature super-resolution network for cloth wrinkle synthesis",
        "authors": [
            "Lan Chen",
            "Juntao Ye",
            "Xiaopeng Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Existing physical cloth simulators suffer from expensive computation and\ndifficulties in tuning mechanical parameters to get desired wrinkling\nbehaviors. Data-driven methods provide an alternative solution. It typically\nsynthesizes cloth animation at a much lower computational cost, and also\ncreates wrinkling effects that highly resemble the much controllable training\ndata. In this paper we propose a deep learning based method for synthesizing\ncloth animation with high resolution meshes. To do this we first create a\ndataset for training: a pair of low and high resolution meshes are simulated\nand their motions are synchronized. As a result the two meshes exhibit similar\nlarge-scale deformation but different small wrinkles. Each simulated mesh pair\nare then converted into a pair of low and high resolution \"images\" (a 2D array\nof samples), with each sample can be interpreted as any of three features: the\ndisplacement, the normal and the velocity. With these image pairs, we design a\nmulti-feature super-resolution (MFSR) network that jointly train an upsampling\nsynthesizer for the three features. The MFSR architecture consists of two key\ncomponents: a sharing module that takes multiple features as input to learn\nlow-level representations from corresponding super-resolution tasks\nsimultaneously; and task-specific modules focusing on various high-level\nsemantics. Frame-to-frame consistency is well maintained thanks to the proposed\nkinematics-based loss function. Our method achieves realistic results at high\nframe rates: 12-14 times faster than traditional physical simulation. We\ndemonstrate the performance of our method with various experimental scenes,\nincluding a dressed character with sophisticated collisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.04351v2"
    },
    {
        "title": "Pulsar: Efficient Sphere-based Neural Rendering",
        "authors": [
            "Christoph Lassner",
            "Michael Zollhöfer"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose Pulsar, an efficient sphere-based differentiable renderer that is\norders of magnitude faster than competing techniques, modular, and easy-to-use\ndue to its tight integration with PyTorch. Differentiable rendering is the\nfoundation for modern neural rendering approaches, since it enables end-to-end\ntraining of 3D scene representations from image observations. However,\ngradient-based optimization of neural mesh, voxel, or function representations\nsuffers from multiple challenges, i.e., topological inconsistencies, high\nmemory footprints, or slow rendering speeds. To alleviate these problems,\nPulsar employs: 1) a sphere-based scene representation, 2) an efficient\ndifferentiable rendering engine, and 3) neural shading. Pulsar executes orders\nof magnitude faster than existing techniques and allows real-time rendering and\noptimization of representations with millions of spheres. Using spheres for the\nscene representation, unprecedented speed is obtained while avoiding topology\nproblems. Pulsar is fully differentiable and thus enables a plethora of\napplications, ranging from 3D reconstruction to general neural rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.07484v3"
    },
    {
        "title": "A Simple, General, and GPU Friendly Method for Computing Dual Mesh and\n  Iso-Surfaces of Adaptive Mesh Refinement (AMR) Data",
        "authors": [
            "Ingo Wald"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose a novel approach to extracting crack-free iso-surfaces from\nStructured AMR data that is more general than previous techniques, is trivially\nsimple to implement, requires no information other than the list of AMR cells,\nand works, in particular, for different AMR formats including octree AMR,\nblock-structured AMR with arbitrary level differences at level boundaries, and\nAMR data that consist of individual cells without any existing grid structure.\nWe describe both the technique itself and a CUDA-based GPU implementation of\nthis technique, and evaluate it on several non-trivial AMR data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.08475v1"
    },
    {
        "title": "Developable B-spline surface generation from control rulings",
        "authors": [
            "Zixuan Hu",
            "Pengbo Bo"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  An intuitive design method is proposed for generating developable ruled\nB-spline surfaces from a sequence of straight line segments indicating the\nsurface shape. The first and last line segments are enforced to be the head and\ntail ruling lines of the resulting surface while the interior lines are\nrequired to approximate rulings on the resulting surface as much as possible.\nThis manner of developable surface design is conceptually similar to the\npopular way of the freeform curve and surface design in the CAD community,\nobserving that a developable ruled surface is a single parameter family of\nstraight lines. This new design mode of the developable surface also provides\nmore flexibility than the widely employed way of developable surface design\nfrom two boundary curves of the surface. The problem is treated by numerical\noptimization methods with which a particular level of distance error is\nallowed. We thus provide an effective tool for creating surfaces with a high\ndegree of developability when the input control rulings do not lie in exact\ndevelopable surfaces. We consider this ability as the superiority over\nanalytical methods in that it can deal with arbitrary design inputs and find\npractically useful results.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.09038v2"
    },
    {
        "title": "Tales from the Trenches: Developing sciview, a new 3D viewer for the\n  ImageJ community",
        "authors": [
            "Ulrik Günther",
            "Kyle I. S. Harrington"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  ImageJ/Fiji is a widely-used tool in the biomedical community for performing\neveryday image analysis tasks. However, its 3D viewer component (aptly named 3D\nViewer) has become dated and is no longer actively maintained. We set out to\ncreate an alternative tool that not only brings modern concepts and APIs from\ncomputer graphics to ImageJ, but is designed to be robust to long-term,\nopen-source development. To achieve this we divided the visualization logic\ninto two parts: the rendering framework, scenery, and the user-facing\napplication, sciview. In this paper we describe the development process and\ndesign decisions made, putting an emphasis on sustainable development,\ncommunity building, and software engineering best practises. We highlight the\nmotivation for the Java Virtual Machine (JVM) as a target platform for\nvisualisation applications. We conclude by discussing the remaining milestones\nand strategy for long-term sustainability.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11897v1"
    },
    {
        "title": "Organic Narrative Charts",
        "authors": [
            "Fabian Bolte",
            "Stefan Bruckner"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Storyline visualizations display the interactions of groups and entities and\ntheir development over time. Existing approaches have successfully adopted the\ngeneral layout from hand-drawn illustrations to automatically create similar\ndepictions. Ward Shelley is the author of several diagrammatic paintings that\nshow the timeline of art-related subjects, such as Downtown Body, a history of\nart scenes. His drawings include many stylistic elements that are not covered\nby existing storyline visualizations, like links between entities, splits and\nmerges of streams, and tags or labels to describe the individual elements. We\npresent a visualization method that provides a visual mapping for the complex\nrelationships in the data, creates a layout for their display, and adopts a\nsimilar styling of elements to imitate the artistic appeal of such\nillustrations. We compare our results to the original drawings and provide an\nopen-source authoring tool prototype.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.13896v1"
    },
    {
        "title": "Visualization of Unsteady Flow Using Heat Kernel Signatures",
        "authors": [
            "Kairong Jiang",
            "Matthew Berger",
            "Joshua A. Levine"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce a new technique to visualize complex flowing phenomena by using\nconcepts from shape analysis. Our approach uses techniques that examine the\nintrinsic geometry of manifolds through their heat kernel, to obtain\nrepresentations of such manifolds that are isometry-invariant and multi-scale.\nThese representations permit us to compute heat kernel signatures of each point\non that manifold, and we can use these signatures as features for\nclassification and segmentation that identify points that have similar\nstructural properties.\n  Our approach adapts heat kernel signatures to unsteady flows by formulating a\nnotion of shape where pathlines are observations of a manifold living in a\nhigh-dimensional space.\n  We use this space to compute and visualize heat kernel signatures associated\nwith each pathline.\n  Besides being able to capture the structural features of a pathline, heat\nkernel signatures allow the comparison of pathlines from different flow\ndatasets through a shape matching pipeline. We demonstrate the analytic power\nof heat kernel signatures by comparing both (1) different timesteps from the\nsame unsteady flow as well as (2) flow datasets taken from ensemble simulations\nwith varying simulation parameters. Our analysis only requires the pathlines\nthemselves, and thus it does not utilize the underlying vector field directly.\nWe make minimal assumptions on the pathlines: while we assume they are sampled\nfrom a continuous, unsteady flow, our computations can tolerate pathlines that\nhave varying density and potential unknown boundaries. We evaluate our approach\nthrough visualizations of a variety of two-dimensional unsteady flows.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.14381v1"
    },
    {
        "title": "Real-World Textured Things: a Repository of Textured Models Generated\n  with Modern Photo-Reconstruction Tools",
        "authors": [
            "Andrea Maggiordomo",
            "Federico Ponchio",
            "Paolo Cignoni",
            "Marco Tarini"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We are witnessing a proliferation of textured 3D models captured from the\nreal world with automatic photo-reconstruction tools. Digital 3D models of this\nclass come with a unique set of characteristics and defects -- especially\nconcerning their parametrization -- setting them starkly apart from 3D models\noriginating from other, more traditional, sources. We study this class of 3D\nmodels by collecting a significant number of representatives and quantitatively\nevaluating their quality according to several metrics. These include a new\ninvariant metric we design to assess the fragmentation of the UV map, one of\nthe main weaknesses hindering the usability of these models. Our results back\nthe widely shared notion that such models are not fit for direct use in\ndownstream applications (such as videogames), and require challenging\nprocessing steps. Regrettably, existing automatic geometry processing tools are\nnot always up to the task: for example, we verify that available tools for UV\noptimization often fail due mesh inconsistencies, geometric and topological\nnoise, excessive resolution, or other factors; moreover, even when an output is\nproduced, it is rarely a significant improvement over the input (according to\nthe aforementioned measures). Therefore, we argue that further advancements are\nrequired specifically targeted at this class of models. Towards this goal, we\nshare the models we collected in the form of a new public repository,\nReal-World Textured Things (RWTT), a benchmark to systematic field-test and\ncompare algorithms. RWTT consists of 568 carefully selected textured 3D models\nrepresentative of all the main modern off-the-shelf photo-reconstruction tools.\nThe repository is available at http://texturedmesh.isti.cnr.it/ and is\nbrowsable by metadata collected during experiments, and comes with a tool,\nTexMetro, providing the same set of measures for generic UV mapped datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.14753v1"
    },
    {
        "title": "Polar Stroking: New Theory and Methods for Stroking Paths",
        "authors": [
            "Mark J. Kilgard"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Stroking and filling are the two basic rendering operations on paths in\nvector graphics. The theory of filling a path is well-understood in terms of\ncontour integrals and winding numbers, but when path rendering standards\nspecify stroking, they resort to the analogy of painting pixels with a brush\nthat traces the outline of the path. This means important standards such as\nPDF, SVG, and PostScript lack a rigorous way to say what samples are inside or\noutside a stroked path. Our work fills this gap with a principled theory of\nstroking.\n  Guided by our theory, we develop a novel polar stroking method to render\nstroked paths robustly with an intuitive way to bound the tessellation error\nwithout needing recursion. Because polar stroking guarantees small uniform\nsteps in tangent angle, it provides an efficient way to accumulate arc length\nalong a path for texturing or dashing. While this paper focuses on developing\nthe theory of our polar stroking method, we have successfully implemented our\nmethods on modern programmable GPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00308v3"
    },
    {
        "title": "Ordinary Facet Angles of a Stroked Path Tessellated by Uniform Tangent\n  Angle Steps Are Bounded by Twice the Step Angle",
        "authors": [
            "Mark J. Kilgard"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We explain geometrically why ordinary facet angles of a stroked path\ntessellated from uniform tangent angle steps are bounded by twice the step\nangle. This fact means---excluding a small number of extraordinary facet angles\nstraddling offset cusps---our polar stroking method bounds the facet angle size\nto less than $2 \\theta$ where $\\theta$ is the tangent step angle.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.01481v1"
    },
    {
        "title": "A Discrete Probabilistic Approach to Dense Flow Visualization",
        "authors": [
            "Daniel Preuß",
            "Tino Weinkauf",
            "Jens Krüger"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Dense flow visualization is a popular visualization paradigm. Traditionally,\nthe various models and methods in this area use a continuous formulation,\nresting upon the solid foundation of functional analysis. In this work, we\nexamine a discrete formulation of dense flow visualization. From probability\ntheory, we derive a similarity matrix that measures the similarity between\ndifferent points in the flow domain, leading to the discovery of a whole new\nclass of visualization models. Using this matrix, we propose a novel\nvisualization approach consisting of the computation of spectral embeddings,\ni.e., characteristic domain maps, defined by particle mixture probabilities.\nThese embeddings are scalar fields that give insight into the mixing processes\nof the flow on different scales. The approach of spectral embeddings is already\nwell studied in image segmentation, and we see that spectral embeddings are\nconnected to Fourier expansions and frequencies. We showcase the utility of our\nmethod using different 2D and 3D flows.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.01629v1"
    },
    {
        "title": "Deform, Cut and Tear a skinned model using Conformal Geometric Algebra",
        "authors": [
            "Manos Kamarianakis",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this work, we present a novel, integrated rigged character simulation\nframework in Conformal Geometric Algebra (CGA) that supports, for the first\ntime, real-time cuts and tears, before and/or after the animation, while\nmaintaining deformation topology. The purpose of using CGA is to lift several\nrestrictions posed by current state-of-the-art character animation &\ndeformation methods. Previous implementations originally required weighted\nmatrices to perform deformations, whereas, in the current state-of-the-art,\ndual-quaternions handle both rotations and translations, but cannot handle\ndilations. CGA is a suitable extension of dual-quaternion algebra that amends\nthese two major previous shortcomings: the need to constantly transmute between\nmatrices and dual-quaternions as well as the inability to properly dilate a\nmodel during animation. Our CGA algorithm also provides easy interpolation and\napplication of all deformations in each intermediate steps, all within the same\ngeometric framework. Furthermore we also present two novel algorithms that\nenable cutting and tearing of the input rigged, animated model, while the\noutput model can be further re-deformed. These interactive, real-time cut and\ntear operations can enable a new suite of applications, especially under the\nscope of a medical surgical simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.04464v2"
    },
    {
        "title": "Octahedral Frames for Feature-Aligned Cross-Fields",
        "authors": [
            "Paul Zhang",
            "Josh Vekhter",
            "Edward Chien",
            "David Bommes",
            "Etienne Vouga",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a method for designing smooth cross fields on surfaces that\nautomatically align to sharp features of an underlying geometry. Our approach\nintroduces a novel class of energies based on a representation of cross fields\nin the spherical harmonic basis. We provide theoretical analysis of these\nenergies in the smooth setting, showing that they penalize deviations from\nsurface creases while otherwise promoting intrinsically smooth fields. We\ndemonstrate the applicability of our method to quad-meshing and include an\nextensive benchmark comparing our fields to other automatic approaches for\ngenerating feature-aligned cross fields on triangle meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.09740v1"
    },
    {
        "title": "DecoSurf: Recursive Geodesic Patterns on Triangle Meshes",
        "authors": [
            "Giacomo Nazzaro",
            "Enrico Puppo",
            "Fabio Pellacini"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we show that many complex patterns, which characterize the\ndecorative style of many artisanal objects, can be generated by the recursive\napplication of only four operators. Each operator is derived from tracing the\nisolines or the integral curves of geodesics fields generated from selected\nseeds on the surface. Based on this formulation, we present an interactive\napplication that lets designers model complex recursive patterns directly on\nthe object surface, without relying on parametrization. We support interaction\non commodity hardware on meshes of a few million triangles, by combining light\ndata structures together with an efficient approximate graph-based geodesic\nsolver. We validate our approach by matching decoration styles from real-world\nphotos, by analyzing the speed and accuracy of our geodesic solver, and by\nvalidating the interface with a user study.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.10918v1"
    },
    {
        "title": "FASTSWARM: A Data-driven FrAmework for Real-time Flying InSecT SWARM\n  Simulation",
        "authors": [
            "Wei Xiang",
            "Xinran Yao",
            "He Wang",
            "Xiaogang Jin"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Insect swarms are common phenomena in nature and therefore have been actively\npursued in computer animation. Realistic insect swarm simulation is difficult\ndue to two challenges: high-fidelity behaviors and large scales, which make the\nsimulation practice subject to laborious manual work and excessive\ntrial-and-error processes. To address both challenges, we present a novel\ndata-driven framework, FASTSWARM, to model complex behaviors of flying insects\nbased on real-world data and simulate plausible animations of flying insect\nswarms. FASTSWARM has a linear time complexity and achieves real-time\nperformance for large swarms. The high-fidelity behavior model of FASTSWARM\nexplicitly takes into consideration the most common behaviors of flying\ninsects, including the interactions among insects such as repulsion and\nattraction, the self-propelled behaviors such as target following and obstacle\navoidance, and other characteristics such as the random movements. To achieve\nscalability, an energy minimization problem is formed with different behaviors\nmodelled as energy terms, where the minimizer is the desired behavior. The\nminimizer is computed from the real-world data, which ensures the plausibility\nof the simulation results. Extensive simulation results and evaluations show\nthat FASTSWARM is versatile in simulating various swarm behaviors, high\nfidelity measured by various metrics, easily controllable in inducing user\ncontrols and highly scalable.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11250v1"
    },
    {
        "title": "Wavelet-based Heat Kernel Derivatives: Towards Informative Localized\n  Shape Analysis",
        "authors": [
            "M. Kirgo",
            "S. Melzi",
            "G. Patanè",
            "E. Rodolà",
            "M. Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we propose a new construction for the Mexican hat wavelets on\nshapes with applications to partial shape matching. Our approach takes its main\ninspiration from the well-established methodology of diffusion wavelets. This\nnovel construction allows us to rapidly compute a multiscale family of Mexican\nhat wavelet functions, by approximating the derivative of the heat kernel. We\ndemonstrate that it leads to a family of functions that inherit many attractive\nproperties of the heat kernel (e.g., a local support, ability to recover\nisometries from a single point, efficient computation). Due to its natural\nability to encode high-frequency details on a shape, the proposed method\nreconstructs and transfers $\\delta$-functions more accurately than the\nLaplace-Beltrami eigenfunction basis and other related bases. Finally, we apply\nour method to the challenging problems of partial and large-scale shape\nmatching. An extensive comparison to the state-of-the-art shows that it is\ncomparable in performance, while both simpler and much faster than competing\napproaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11632v3"
    },
    {
        "title": "Signed Distance Fields Dynamic Diffuse Global Illumination",
        "authors": [
            "Jinkai Hu",
            "Milo Yip",
            "G. Elias Alonso",
            "Shihao Gu",
            "Xiangjun Tang",
            "Xiaogang Jin"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Global Illumination (GI) is of utmost importance in the field of\nphoto-realistic rendering. However, its computation has always been very\ncomplex, especially diffuse GI. State of the art real-time GI methods have\nlimitations of different nature, such as light leaking, performance issues,\nspecial hardware requirements, noise corruption, bounce number limitations,\namong others. To overcome these limitations, we propose a novel approach of\ncomputing dynamic diffuse GI with a signed distance fields approximation of the\nscene and discretizing the space domain of the irradiance function. With this\napproach, we are able to estimate real-time diffuse GI for dynamic lighting and\ngeometry, without any precomputations and supporting multi-bounce GI, providing\ngood quality lighting and high performance at the same time. Our algorithm is\nalso able to achieve better scalability, and manage both large open scenes and\nindoor high-detailed scenes without being corrupted by noise.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.14394v1"
    },
    {
        "title": "Functionality-Driven Musculature Retargeting",
        "authors": [
            "Hoseok Ryu",
            "Minseok Kim",
            "Seunghwan Lee",
            "Moon Seok Park",
            "Kyoungmin Lee",
            "Jehee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a novel retargeting algorithm that transfers the musculature of a\nreference anatomical model to new bodies with different sizes, body\nproportions, muscle capability, and joint range of motion while preserving the\nfunctionality of the original musculature as closely as possible. The geometric\nconfiguration and physiological parameters of musculotendon units are estimated\nand optimized to adapt to new bodies. The range of motion around joints is\nestimated from a motion capture dataset and edited further for individual\nmodels. The retargeted model is simulation-ready, so we can physically simulate\nmuscle-actuated motor skills with the model. Our system is capable of\ngenerating a wide variety of anatomical bodies that can be simulated to walk,\nrun, jump and dance while maintaining balance under gravity. We will also\ndemonstrate the construction of individualized musculoskeletal models from\nbi-planar X-ray images and medical examinations.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.15311v3"
    },
    {
        "title": "Cartographic Relief Shading with Neural Networks",
        "authors": [
            "Bernhard Jenny",
            "Magnus Heitzler",
            "Dilpreet Singh",
            "Marianna Farmakis-Serebryakova",
            "Jeffery Chieh Liu",
            "Lorenz Hurni"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Shaded relief is an effective method for visualising terrain on topographic\nmaps, especially when the direction of illumination is adapted locally to\nemphasise individual terrain features. However, digital shading algorithms are\nunable to fully match the expressiveness of hand-crafted masterpieces, which\nare created through a laborious process by highly specialised cartographers. We\nreplicate hand-drawn relief shading using U-Net neural networks. The deep\nneural networks are trained with manual shaded relief images of the Swiss\ntopographic map series and terrain models of the same area. The networks\ngenerate shaded relief that closely resemble hand-drawn shaded relief art. The\nnetworks learn essential design principles from manual relief shading such as\nremoving unnecessary terrain details, locally adjusting the illumination\ndirection to accentuate individual terrain features, and varying brightness to\nemphasise larger landforms. Neural network shadings are generated from digital\nelevation models in a few seconds, and a study with 18 relief shading experts\nfound that they are of high quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.01256v1"
    },
    {
        "title": "Generating Emotive Gaits for Virtual Agents Using Affect-Based\n  Autoregression",
        "authors": [
            "Uttaran Bhattacharya",
            "Nicholas Rewkowski",
            "Pooja Guhan",
            "Niall L. Williams",
            "Trisha Mittal",
            "Aniket Bera",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a novel autoregression network to generate virtual agents that\nconvey various emotions through their walking styles or gaits. Given the 3D\npose sequences of a gait, our network extracts pertinent movement features and\naffective features from the gait. We use these features to synthesize\nsubsequent gaits such that the virtual agents can express and transition\nbetween emotions represented as combinations of happy, sad, angry, and neutral.\nWe incorporate multiple regularizations in the training of our network to\nsimultaneously enforce plausible movements and noticeable emotions on the\nvirtual agents. We also integrate our approach with an AR environment using a\nMicrosoft HoloLens and can generate emotive gaits at interactive rates to\nincrease the social presence. We evaluate how human observers perceive both the\nnaturalness and the emotions from the generated gaits of the virtual agents in\na web-based study. Our results indicate around 89% of the users found the\nnaturalness of the gaits satisfactory on a five-point Likert scale, and the\nemotions they perceived from the virtual agents are statistically similar to\nthe intended emotions of the virtual agents. We also use our network to augment\nexisting gait datasets with emotive gaits and will release this augmented\ndataset for future research in emotion prediction and emotive gait synthesis.\nOur project website is available at https://gamma.umd.edu/gen_emotive_gaits/.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.01615v3"
    },
    {
        "title": "Cinema Darkroom: A Deferred Rendering Framework for Large-Scale Datasets",
        "authors": [
            "Jonas Lukasczyk",
            "Christoph Garth",
            "Matthew Larsen",
            "Wito Engelke",
            "Ingrid Hotz",
            "David Rogers",
            "James Ahrens",
            "Ross Maciejewski"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper presents a framework that fully leverages the advantages of a\ndeferred rendering approach for the interactive visualization of large-scale\ndatasets. Geometry buffers (G-Buffers) are generated and stored in situ, and\nshading is performed post hoc in an interactive image-based rendering front\nend. This decoupled framework has two major advantages. First, the G-Buffers\nonly need to be computed and stored once---which corresponds to the most\nexpensive part of the rendering pipeline. Second, the stored G-Buffers can\nlater be consumed in an image-based rendering front end that enables users to\ninteractively adjust various visualization parameters---such as the applied\ncolor map or the strength of ambient occlusion---where suitable choices are\noften not known a priori. This paper demonstrates the use of Cinema Darkroom on\nseveral real-world datasets, highlighting CD's ability to effectively decouple\nthe complexity and size of the dataset from its visualization.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.03936v1"
    },
    {
        "title": "Temporally-smooth Antialiasing and Lens Distortion with Rasterization\n  Map",
        "authors": [
            "Jakub Maximilian Fober"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Current GPU rasterization procedure is limited to narrow views in rectilinear\nperspective. While industries demand curvilinear perspective in wide-angle\nviews, like Virtual Reality and Virtual Film Production industry. This paper\ndelivers new rasterization method using industry-standard STMaps. Additionally\nnew antialiasing rasterization method is proposed, which outperforms MSAA in\nboth quality and performance. It is an improvement upon previous solutions\nfound in paper Perspective picture from Visual Sphere by yours truly.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.04077v2"
    },
    {
        "title": "LSMAT Least Squares Medial Axis Transform",
        "authors": [
            "Daniel Rebain",
            "Baptiste Angles",
            "Julien Valentin",
            "Nicholas Vining",
            "Jiju Peethambaran",
            "Shahram Izadi",
            "Andrea Tagliasacchi"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The medial axis transform has applications in numerous fields including\nvisualization, computer graphics, and computer vision. Unfortunately,\ntraditional medial axis transformations are usually brittle in the presence of\noutliers, perturbations and/or noise along the boundary of objects. To overcome\nthis limitation, we introduce a new formulation of the medial axis transform\nwhich is naturally robust in the presence of these artifacts. Unlike previous\nwork which has approached the medial axis from a computational geometry angle,\nwe consider it from a numerical optimization perspective. In this work, we\nfollow the definition of the medial axis transform as \"the set of maximally\ninscribed spheres\". We show how this definition can be formulated as a least\nsquares relaxation where the transform is obtained by minimizing a continuous\noptimization problem. The proposed approach is inherently parallelizable by\nperforming independant optimization of each sphere using Gauss-Newton, and its\nleast-squares form allows it to be significantly more robust compared to\ntraditional computational geometry approaches. Extensive experiments on 2D and\n3D objects demonstrate that our method provides superior results to the state\nof the art on both synthetic and real-data.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.05066v1"
    },
    {
        "title": "Fat Pad Cages for Facial Posing",
        "authors": [
            "Adèle Colas",
            "Florent Guiotte",
            "Fabien Danieau",
            "François Le Clerc",
            "Quentin Avril"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce Fat Pad cages for posing facial meshes. It combines cage\nrepresentation and facial anatomical elements, and enables users with no\nartistic skill to quickly sketch realistic facial expressions. The model relies\non one or several cage(s) that deform(s) the mesh following the human fat pads\nmap. We propose a new function to filter Green Coordinates using geodesic\ndistances preventing global deformation while ensuring smooth deformations at\nthe borders. Lips, nostrils and eyelids are processed slightly differently to\nallow folding up and opening. Cages are automatically created and fit any new\nunknown facial mesh. To validate our approach, we present a user study\ncomparing our Fat Pad cages to regular Green Coordinates. Results show that Fat\nPad cages bring a significant improvement in reproducing existing facial\nexpressions.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.05528v1"
    },
    {
        "title": "Intuitive Facial Animation Editing Based On A Generative RNN Framework",
        "authors": [
            "Eloïse Berson",
            "Catherine Soladié",
            "Nicolas Stoiber"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  For the last decades, the concern of producing convincing facial animation\nhas garnered great interest, that has only been accelerating with the recent\nexplosion of 3D content in both entertainment and professional activities. The\nuse of motion capture and retargeting has arguably become the dominant solution\nto address this demand. Yet, despite high level of quality and automation\nperformance-based animation pipelines still require manual cleaning and editing\nto refine raw results, which is a time- and skill-demanding process. In this\npaper, we look to leverage machine learning to make facial animation editing\nfaster and more accessible to non-experts. Inspired by recent image inpainting\nmethods, we design a generative recurrent neural network that generates\nrealistic motion into designated segments of an existing facial animation,\noptionally following user-provided guiding constraints. Our system handles\ndifferent supervised or unsupervised editing scenarios such as motion filling\nduring occlusions, expression corrections, semantic content modifications, and\nnoise filtering. We demonstrate the usability of our system on several\nanimation editing use cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.05655v1"
    },
    {
        "title": "TM-NET: Deep Generative Networks for Textured Meshes",
        "authors": [
            "Lin Gao",
            "Tong Wu",
            "Yu-Jie Yuan",
            "Ming-Xian Lin",
            "Yu-Kun Lai",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce TM-NET, a novel deep generative model for synthesizing textured\nmeshes in a part-aware manner. Once trained, the network can generate novel\ntextured meshes from scratch or predict textures for a given 3D mesh, without\nimage guidance. Plausible and diverse textures can be generated for the same\nmesh part, while texture compatibility between parts in the same shape is\nachieved via conditional generation. Specifically, our method produces texture\nmaps for individual shape parts, each as a deformable box, leading to a natural\nUV map with minimal distortion. The network separately embeds part geometry\n(via a PartVAE) and part texture (via a TextureVAE) into their respective\nlatent spaces, so as to facilitate learning texture probability distributions\nconditioned on geometry. We introduce a conditional autoregressive model for\ntexture generation, which can be conditioned on both part geometry and textures\nalready generated for other parts to achieve texture compatibility. To produce\nhigh-frequency texture details, our TextureVAE operates in a high-dimensional\nlatent space via dictionary-based vector quantization. We also exploit\ntransparencies in the texture as an effective means to model complex shape\nstructures including topological details. Extensive experiments demonstrate the\nplausibility, quality, and diversity of the textures and geometries generated\nby our network, while avoiding inconsistency issues that are common to novel\nview synthesis methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.06217v3"
    },
    {
        "title": "Design and Fabrication of Elastic Geodesic Grid Structures",
        "authors": [
            "Stefan Pillwein",
            "Johanna Kübert",
            "Florian Rist",
            "Przemyslaw Musialski"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Elastic geodesic grids (EGG) are lightweight structures that can be easily\ndeployed to approximate designer provided free-form surfaces. In the initial\nconfiguration the grids are perfectly flat, during deployment, though,\ncurvature is induced to the structure, as grid elements bend and twist. Their\nlayout is found geometrically, it is based on networks of geodesic curves on\nfree-form design-surfaces. Generating a layout with this approach encodes an\nelasto-kinematic mechanism to the grid that creates the curved shape during\ndeployment. In the final state the grid can be fixed to supports and serve for\nall kinds of purposes like free-form sub-structures, paneling, sun and rain\nprotectors, pavilions, etc. However, so far these structures have only been\ninvestigated using small-scale desktop models. We investigate the scalability\nof such structures, presenting a medium sized model. It was designed by an\narchitecture student without expert knowledge on elastic structures or\ndifferential geometry, just using the elastic geodesic grids design-pipeline.\nWe further present a fabrication-process for EGG-models. They can be built\nquickly and with a small budget.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08062v1"
    },
    {
        "title": "Real-time High-Quality Rendering of Non-Rotating Black Holes",
        "authors": [
            "Eric Bruneton"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose a real-time method to render high-quality images of a non-rotating\nblack hole with an accretion disc and background stars. Our method is based on\nbeam tracing, but uses precomputed tables to find the intersections of each\ncurved light beam with the scene in constant time per pixel. It also uses a\nspecific texture filtering scheme to integrate the contribution of the light\nsources to each beam. Our method is simple to implement and achieves high frame\nrates.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08735v1"
    },
    {
        "title": "Probabilistic Character Motion Synthesis using a Hierarchical Deep\n  Latent Variable Model",
        "authors": [
            "Saeed Ghorbani",
            "Calden Wloka",
            "Ali Etemad",
            "Marcus A. Brubaker",
            "Nikolaus F. Troje"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a probabilistic framework to generate character animations based\non weak control signals, such that the synthesized motions are realistic while\nretaining the stochastic nature of human movement. The proposed architecture,\nwhich is designed as a hierarchical recurrent model, maps each sub-sequence of\nmotions into a stochastic latent code using a variational autoencoder extended\nover the temporal domain. We also propose an objective function which respects\nthe impact of each joint on the pose and compares the joint angles based on\nangular distance. We use two novel quantitative protocols and human qualitative\nassessment to demonstrate the ability of our model to generate convincing and\ndiverse periodic and non-periodic motion sequences without the need for strong\ncontrol signals.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.09950v1"
    },
    {
        "title": "A Characterization of 3D Printability",
        "authors": [
            "Ioannis Fudos",
            "Margarita Ntousia",
            "Vasiliki Stamati",
            "Paschalis Charalampous",
            "Theodora Kontodina",
            "Ioannis Kostavelis",
            "Dimitrios Tzovaras",
            "Leonardo Bilalis"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Additive manufacturing technologies are positioned to provide an\nunprecedented innovative transformation in how products are designed and\nmanufactured. Due to differences in the technical specifications of AM\ntechnologies, the final fabricated parts can vary significantly from the\noriginal CAD models, therefore raising issues regarding accuracy, surface\nfinish, robustness, mechanical properties, functional and geometrical\nconstraints. Various researchers have studied the correlation between AM\ntechnologies and design rules.\n  In this work we propose a novel approach to assessing the capability of a 3D\nmodel to be printed successfully (a.k.a printability) on a specific AM machine.\nThis is utilized by taking into consideration the model mesh complexity and\ncertain part characteristics. A printability score is derived for a model in\nreference to a specific 3D printing technology, expressing the probability of\nobtaining a robust and accurate end result for 3D printing on a specific AM\nmachine. The printability score can be used either to determine which 3D\ntechnology is more suitable for manufacturing a specific model or as a guide to\nredesign the model to ensure printability. We verify this framework by\nconducting 3D printing experiments for benchmark models which are printed on\nthree AM machines employing different technologies: Fused Deposition Modeling\n(FDM), Binder Jetting (3DP), and Material Jetting (Polyjet).\n",
        "pdf_link": "http://arxiv.org/pdf/2010.12930v1"
    },
    {
        "title": "Optimal Textures: Fast and Robust Texture Synthesis and Style Transfer\n  through Optimal Transport",
        "authors": [
            "Eric Risser"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper presents a light-weight, high-quality texture synthesis algorithm\nthat easily generalizes to other applications such as style transfer and\ntexture mixing. We represent texture features through the deep neural\nactivation vectors within the bottleneck layer of an auto-encoder and frame the\ntexture synthesis problem as optimal transport between the activation values of\nthe image being synthesized and those of an exemplar texture. To find this\noptimal transport mapping, we utilize an N-dimensional probability density\nfunction (PDF) transfer process that iterates over multiple random rotations of\nthe PDF basis and matches the 1D marginal distributions across each dimension.\nThis achieves quality and flexibility on par with expensive back-propagation\nbased neural texture synthesis methods, but with the potential of achieving\ninteractive rates. We demonstrate that first order statistics offer a more\nrobust representation for texture than the second order statistics that are\nused today. We propose an extension of this algorithm that reduces the\ndimensionality of the neural feature space. We utilize a multi-scale\ncoarse-to-fine synthesis pyramid to capture and preserve larger image features;\nunify color and style transfer under one framework; and further augment this\nsystem with a novel masking scheme that re-samples and re-weights the feature\ndistribution for user-guided texture painting and targeted style transfer.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.14702v1"
    },
    {
        "title": "Uncertainty-Oriented Ensemble Data Visualization and Exploration using\n  Variable Spatial Spreading",
        "authors": [
            "Mingdong Zhang",
            "Li Chen",
            "Quan Li",
            "Xiaoru Yuan",
            "Junhai Yong"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  As an important method of handling potential uncertainties in numerical\nsimulations, ensemble simulation has been widely applied in many disciplines.\nVisualization is a promising and powerful ensemble simulation analysis method.\nHowever, conventional visualization methods mainly aim at data simplification\nand highlighting important information based on domain expertise instead of\nproviding a flexible data exploration and intervention mechanism.\nTrial-and-error procedures have to be repeatedly conducted by such approaches.\nTo resolve this issue, we propose a new perspective of ensemble data analysis\nusing the attribute variable dimension as the primary analysis dimension.\nParticularly, we propose a variable uncertainty calculation method based on\nvariable spatial spreading. Based on this method, we design an interactive\nensemble analysis framework that provides a flexible interactive exploration of\nthe ensemble data. Particularly, the proposed spreading curve view, the region\nstability heat map view, and the temporal analysis view, together with the\ncommonly used 2D map view, jointly support uncertainty distribution perception,\nregion selection, and temporal analysis, as well as other analysis\nrequirements. We verify our approach by analyzing a real-world ensemble\nsimulation dataset. Feedback collected from domain experts confirms the\nefficacy of our framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.01497v1"
    },
    {
        "title": "PCEDNet : A Lightweight Neural Network for Fast and Interactive Edge\n  Detection in 3D Point Clouds",
        "authors": [
            "Chems-Eddine Himeur",
            "Thibault Lejemble",
            "Thomas Pellegrini",
            "Mathias Paulin",
            "Loic Barthe",
            "Nicolas Mellado"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In recent years, Convolutional Neural Networks (CNN) have proven to be\nefficient analysis tools for processing point clouds, e.g., for reconstruction,\nsegmentation and classification. In this paper, we focus on the classification\nof edges in point clouds, where both edges and their surrounding are described.\nWe propose a new parameterization adding to each point a set of differential\ninformation on its surrounding shape reconstructed at different scales. These\nparameters, stored in a Scale-Space Matrix (SSM), provide a well suited\ninformation from which an adequate neural network can learn the description of\nedges and use it to efficiently detect them in acquired point clouds. After\nsuccessfully applying a multi-scale CNN on SSMs for the efficient\nclassification of edges and their neighborhood, we propose a new lightweight\nneural network architecture outperforming the CNN in learning time, processing\ntime and classification capabilities. Our architecture is compact, requires\nsmall learning sets, is very fast to train and classifies millions of points in\nseconds.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.01630v4"
    },
    {
        "title": "Learning Multiple-Scattering Solutions for Sphere-Tracing of Volumetric\n  Subsurface Effects",
        "authors": [
            "Ludwig Leonard",
            "Kevin Hoehlein",
            "Ruediger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Accurate subsurface scattering solutions require the integration of optical\nmaterial properties along many complicated light paths. We present a method\nthat learns a simple geometric approximation of random paths in a homogeneous\nvolume of translucent material. The generated representation allows determining\nthe absorption along the path as well as a direct lighting contribution, which\nis representative of all scattering events along the path. A sequence of\nconditional variational auto-encoders (CVAEs) is trained to model the\nstatistical distribution of the photon paths inside a spherical region in\npresence of multiple scattering events. A first CVAE learns to sample the\nnumber of scattering events, occurring on a ray path inside the sphere, which\neffectively determines the probability of the ray being absorbed. Conditioned\non this, a second model predicts the exit position and direction of the light\nparticle. Finally, a third model generates a representative sample of photon\nposition and direction along the path, which is used to approximate the\ncontribution of direct illumination due to in-scattering. To accelerate the\ntracing of the light path through the volumetric medium toward the solid\nboundary, we employ a sphere-tracing strategy that considers the light\nabsorption and is able to perform statistically accurate next-event estimation.\nWe demonstrate efficient learning using shallow networks of only three layers\nand no more than 16 nodes. In combination with a GPU shader that evaluates the\nCVAEs' predictions, performance gains can be demonstrated for a variety of\ndifferent scenarios. A quality evaluation analyzes the approximation error that\nis introduced by the data-driven scattering simulation and sheds light on the\nmajor sources of error in the accelerated path tracing process.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.03082v1"
    },
    {
        "title": "Flexible Virtual Reality System for Neurorehabilitation and Quality of\n  Life Improvement",
        "authors": [
            "Iulia-Cristina Stanica",
            "Florica Moldoveanu",
            "Giovanni-Paul Portelli",
            "Maria-Iuliana Dascalu",
            "Alin Moldoveanu",
            "Mariana Georgiana Ristea"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  As life expectancy is mostly increasing, the incidence of many neurological\ndisorders is also constantly growing. For improving the physical functions\naffected by a neurological disorder, rehabilitation procedures are mandatory,\nand they must be performed regularly. Unfortunately, neurorehabilitation\nprocedures have disadvantages in terms of costs, accessibility and a lack of\ntherapists. This paper presents Immersive Neurorehabilitation Exercises Using\nVirtual Reality (INREX-VR), our innovative immersive neurorehabilitation system\nusing virtual reality. The system is based on a thorough research methodology\nand is able to capture real-time user movements and evaluate joint mobility for\nboth upper and lower limbs, record training sessions and save electromyography\ndata. The use of the first-person perspective increases immersion, and the\njoint range of motion is calculated with the help of both the HTC Vive system\nand inverse kinematics principles applied on skeleton rigs. Tutorial exercises\nare demonstrated by a virtual therapist, as they were recorded with real-life\nphysicians, and sessions can be monitored and configured through tele-medicine.\nComplex movements are practiced in gamified settings, encouraging\nself-improvement and competition. Finally, we proposed a training plan and\npreliminary tests which show promising results in terms of accuracy and user\nfeedback. As future developments, we plan to improve the system's accuracy and\ninvestigate a wireless alternative based on neural networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.03596v1"
    },
    {
        "title": "LCollision: Fast Generation of Collision-Free Human Poses using Learned\n  Non-Penetration Constraints",
        "authors": [
            "Qingyang Tan",
            "Zherong Pan",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present LCollision, a learning-based method that synthesizes\ncollision-free 3D human poses. At the crux of our approach is a novel deep\narchitecture that simultaneously decodes new human poses from the latent space\nand predicts colliding body parts. These two components of our architecture are\nused as the objective function and surrogate hard constraints in a constrained\noptimization for collision-free human pose generation. A novel aspect of our\napproach is the use of a bilevel autoencoder that decomposes whole-body\ncollisions into groups of collisions between localized body parts. By solving\nthe constrained optimizations, we show that a significant amount of collision\nartifacts can be resolved. Furthermore, in a large test set of $2.5\\times 10^6$\nrandomized poses from SCAPE, our architecture achieves a collision-prediction\naccuracy of $94.1\\%$ with $80\\times$ speedup over exact collision detection\nalgorithms. To the best of our knowledge, LCollision is the first approach that\naccelerates collision detection and resolves penetrations using a neural\nnetwork.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.03632v4"
    },
    {
        "title": "Diffusion Structures for Architectural Stripe Pattern Generation",
        "authors": [
            "Abhishek Madan",
            "Alec Jacobson",
            "David I. W. Levin"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present Diffusion Structures, a family of resilient shell structures from\nthe eigenfunctions of a pair of novel diffusion operators. This approach is\nbased on Michell's theorem but avoids expensive non-linear optimization with\ncomputation that amounts to constructing and solving two generalized eigenvalue\nproblems to generate two sets of stripe patterns. This structure family can be\ngenerated quickly, and navigated in real-time using a small number of tuneable\nparameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.05550v1"
    },
    {
        "title": "Assembling a Pipeline for 3D Face Interpolation",
        "authors": [
            "Yusuke Niiro",
            "Marcelo Kallmann"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper describes a pipeline built with open source tools for\ninterpolating 3D facial expressions taken from images. The presented approach\nallows anyone to create 3D face animations from 2 input photos: one from the\nstart face expression, and the other from the final face expression. Given the\ninput photos, corresponding 3D face models are constructed and texture-mapped\nusing the photos as textures aligned with facial features. Animations are then\ngenerated by morphing the models by interpolation of the geometries and\ntextures of the models. This work was performed as a MS project at the\nUniversity of California, Merced.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.09657v1"
    },
    {
        "title": "Perceptual Evaluation of Liquid Simulation Methods",
        "authors": [
            "Kiwon Um",
            "Xiangyu Hu",
            "Nils Thuerey"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper proposes a novel framework to evaluate fluid simulation methods\nbased on crowd-sourced user studies in order to robustly gather large numbers\nof opinions. The key idea for a robust and reliable evaluation is to use a\nreference video from a carefully selected real-world setup in the user study.\nBy conducting a series of controlled user studies and comparing their\nevaluation results, we observe various factors that affect the perceptual\nevaluation. Our data show that the availability of a reference video makes the\nevaluation consistent. We introduce this approach for computing scores of\nsimulation methods as visual accuracy metric. As an application of the proposed\nframework, a variety of popular simulation methods are evaluated.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.10257v1"
    },
    {
        "title": "GraphSeam: Supervised Graph Learning Framework for Semantic UV Mapping",
        "authors": [
            "Fatemeh Teimury",
            "Bruno Roy",
            "Juan Sebastián Casallas",
            "David MacDonald",
            "Mark Coates"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Recently there has been a significant effort to automate UV mapping, the\nprocess of mapping 3D-dimensional surfaces to the UV space while minimizing\ndistortion and seam length. Although state-of-the-art methods, Autocuts and\nOptCuts, addressed this task via energy-minimization approaches, they fail to\nproduce semantic seam styles, an essential factor for professional artists. The\nrecent emergence of Graph Neural Networks (GNNs), and the fact that a mesh can\nbe represented as a particular form of a graph, has opened a new bridge to\nnovel graph learning-based solutions in the computer graphics domain. In this\nwork, we use the power of supervised GNNs for the first time to propose a fully\nautomated UV mapping framework that enables users to replicate their desired\nseam styles while reducing distortion and seam length. To this end, we provide\naugmentation and decimation tools to enable artists to create their dataset and\ntrain the network to produce their desired seam style. We provide a\ncomplementary post-processing approach for reducing the distortion based on\ngraph algorithms to refine low-confidence seam predictions and reduce seam\nlength (or the number of shells in our supervised case) using a skeletonization\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.13748v2"
    },
    {
        "title": "A Marching Cube Algorithm Based on Edge Growth",
        "authors": [
            "Xin Wang",
            "Su Gao",
            "Monan Wang",
            "Zhenghua Duan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Marching Cube algorithm is currently one of the most popular 3D\nreconstruction surface rendering algorithms. It forms cube voxels through the\ninput image, and then uses 15 basic topological configurations to extract the\niso-surfaces in the voxels. It processes each cube voxel in a traversal manner,\nbut it does not consider the relationship between iso-surfaces in adjacent\ncubes. Due to ambiguity, the final reconstructed model may have holes. We\npropose a Marching Cube algorithm based on edge growth. The algorithm first\nextracts seed triangles, then grows the seed triangles and reconstructs the\nentire 3D model. According to the position of the growth edge, we propose 17\ntopological configurations with iso-surfaces. From the reconstruction results,\nthe algorithm can reconstruct the 3D model well. When only the main contour of\nthe 3D model needs to be organized, the algorithm performs well. In addition,\nwhen there are multiple scattered parts in the data, the algorithm can extract\nonly the 3D contours of the parts connected to the seed by setting the region\nselected by the seed.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.00631v3"
    },
    {
        "title": "Enhanced Direct Delta Mush",
        "authors": [
            "Serguei Kalentchouk",
            "Michael Hutchinson",
            "Deepak Tolani"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Direct Delta Mush is a novel skinning deformation technique introduced by Le\nand Lewis (2019). It generalizes the iterative Delta Mush algorithm of\nMancewicz et al (2014), providing a direct solution with improved efficiency\nand control. Compared to Linear Blend Skinning, Direct Delta Mush offers better\nquality of deformations and ease of authoring at comparable performance.\nHowever, Direct Delta Mush does not handle non-rigid joint transformations\ncorrectly which limits its application for most production environments. This\npaper presents an extension to Direct Delta Mush that integrates the non-rigid\npart of joint transformations into the algorithm. In addition, the paper also\ndescribes practical considerations for computing the orthogonal component of\nthe transformation and stability issues observed during the implementation and\ntesting.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.02798v1"
    },
    {
        "title": "Color Contrast Enhanced Rendering for Optical See-through Head-mounted\n  Displays",
        "authors": [
            "Yunjin Zhang",
            "Rui Wang",
            " Yifan",
            " Peng",
            "Wei Hua",
            "Hujun Bao"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Most commercially available optical see-through head-mounted displays\n(OST-HMDs) utilize optical combiners to simultaneously visualize the physical\nbackground and virtual objects. The displayed images perceived by users are a\nblend of rendered pixels and background colors. Enabling high fidelity color\nperception in mixed reality (MR) scenarios using OST-HMDs is an important but\nchallenging task. We propose a real-time rendering scheme to enhance the color\ncontrast between virtual objects and the surrounding background for OST-HMDs.\nInspired by the discovery of color perception in psychophysics, we first\nformulate the color contrast enhancement as a constrained optimization problem.\nWe then design an end-to-end algorithm to search the optimal complementary\nshift in both chromaticity and luminance of the displayed color. This aims at\nenhancing the contrast between virtual objects and the real background as well\nas keeping the consistency with the original color. We assess the performance\nof our approach using a simulated OST-HMD environment and an off-the-shelf\nOST-HMD. Experimental results from objective evaluations and subjective user\nstudies demonstrate that the proposed approach makes rendered virtual objects\nmore distinguishable from the surrounding background, thereby bringing a better\nvisual experience.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.02847v1"
    },
    {
        "title": "Geometry-Based Layout Generation with Hyper-Relations AMONG Objects",
        "authors": [
            "Shao-Kui Zhang",
            "Wei-Yu Xie",
            "Song-Hai Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Recent studies show increasing demands and interests in automatically\ngenerating layouts, while there is still much room for improving the\nplausibility and robustness. In this paper, we present a data-driven layout\nframework without model formulation and loss term optimization. We achieve and\norganize priors directly based on samples from datasets instead of sampling\nprobabilistic models. Therefore, our method enables expressing and generating\nmathematically inexpressible relations among three or more objects.\nSubsequently, a non-learning geometric algorithm attempts arranging objects\nplausibly considering constraints such as walls, windows, etc. Experiments\nwould show our generated layouts outperform the state-of-art and our framework\nis competitive to human designers.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.02903v1"
    },
    {
        "title": "Example-based Real-time Clothing Synthesis for Virtual Agents",
        "authors": [
            "Nannan Wu",
            "Qianwen Chao",
            "Yanzhen Chen",
            "Weiwei Xu",
            "Chen Liu",
            "Dinesh Manocha",
            "Wenxin Sun",
            "Yi Han",
            "Xinran Yao",
            "Xiaogang Jin"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a real-time cloth animation method for dressing virtual humans of\nvarious shapes and poses. Our approach formulates the clothing deformation as a\nhigh-dimensional function of body shape parameters and pose parameters. In\norder to accelerate the computation, our formulation factorizes the clothing\ndeformation into two independent components: the deformation introduced by body\npose variation (Clothing Pose Model) and the deformation from body shape\nvariation (Clothing Shape Model). Furthermore, we sample and cluster the poses\nspanning the entire pose space and use those clusters to efficiently calculate\nthe anchoring points. We also introduce a sensitivity-based distance\nmeasurement to both find nearby anchoring points and evaluate their\ncontributions to the final animation. Given a query shape and pose of the\nvirtual agent, we synthesize the resulting clothing deformation by blending the\nTaylor expansion results of nearby anchoring points. Compared to previous\nmethods, our approach is general and able to add the shape dimension to any\nclothing pose model. %and therefore it is more general. Furthermore, we can\nanimate clothing represented with tens of thousands of vertices at 50+ FPS on a\nCPU. Moreover, our example database is more representative and can be generated\nin parallel, and thereby saves the training time. We also conduct a user\nevaluation and show that our method can improve a user's perception of dressed\nvirtual agents in an immersive virtual environment compared to a conventional\nlinear blend skinning method.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03088v1"
    },
    {
        "title": "TopoKnit : A Process-Oriented Representation for Modeling the Topology\n  of Yarns in Weft-Knitted Textiles",
        "authors": [
            "Levi Kapllani",
            "Chelsea Amanatides",
            "Genevieve Dion",
            "Vadim Shapiro",
            "David E. Breen"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Machine knitted textiles are complex multi-scale material structures\nincreasingly important in many industries, including consumer products,\narchitecture, composites, medical, and military. Computational modeling,\nsimulation, and design of industrial fabrics require efficient representations\nof the spatial, material, and physical properties of such structures. We\npropose a process-oriented representation, TopoKnit, that defines a\nfoundational data structure for representing the topology of weft-knitted\ntextiles at the yarn scale. Process space serves as an intermediary between the\nmachine and fabric spaces, and supports a concise, computationally efficient\nevaluation approach based on on-demand, near constant-time queries. In this\npaper, we define the properties of the process space, and design a data\nstructure to represent it and algorithms to evaluate it. We demonstrate the\neffectiveness of the representation scheme by providing results of evaluations\nof the data structure in support of common topological operations in the fabric\nspace.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.04560v1"
    },
    {
        "title": "Human Centric Accessibility Graph For Environment Analysis",
        "authors": [
            "Mathew Schwartz"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Understanding design decisions in relation to the future occupants of a\nbuilding is a crucial part of good design. However, limitations in tools and\nexpertise hinder meaningful human-centric decisions during the design process.\nIn this paper, a novel Spatial Human Accessibility graph for Planning and\nEnvironment Analysis (SHAPE) is introduced that brings together the technical\nchallenges of discrete representations of digital models, with human-based\nmetrics for evaluating the environment. SHAPE: does not need labeled geometry\nas input, works with multi-level buildings, captures surface variations (e.g.,\nslopes in a terrain), and can be used with existing graph theory (e.g.,\ngravity, centrality) techniques. SHAPE uses ray-casting to perform a search,\ngenerating a dense graph of all accessible locations within the environment and\nstoring the type of travel required in a graph (e.g., up a slope, down a step).\nThe ability to simultaneously evaluate and plan paths from multiple human\nfactors is shown to work on digital models across room, building, and\ntopography scales. The results enable designers and planners to evaluate\noptions of the built environment in new ways, and at higher fidelity, that will\nlead to more human-friendly and accessible environments.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.10503v1"
    },
    {
        "title": "A causal convolutional neural network for multi-subject motion modeling\n  and generation",
        "authors": [
            "Shuaiying Hou",
            "Congyi Wang",
            "Wenlin Zhuang",
            "Yu Chen",
            "Yangang Wang",
            "Hujun Bao",
            "Jinxiang Chai",
            "Weiwei Xu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Inspired by the success of WaveNet in multi-subject speech synthesis, we\npropose a novel neural network based on causal convolutions for multi-subject\nmotion modeling and generation. The network can capture the intrinsic\ncharacteristics of the motion of different subjects, such as the influence of\nskeleton scale variation on motion style. Moreover, after fine-tuning the\nnetwork using a small motion dataset for a novel skeleton that is not included\nin the training dataset, it is able to synthesize high-quality motions with a\npersonalized style for the novel skeleton. The experimental results demonstrate\nthat our network can model the intrinsic characteristics of motions well and\ncan be applied to various motion modeling and synthesis tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.12276v2"
    },
    {
        "title": "Real-Time Global Illumination Using OpenGL And Voxel Cone Tracing",
        "authors": [
            "Benjamin Kahl"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Building systems capable of replicating global illumination models with\ninteractive frame-rates has long been one of the toughest conundrums facing\ncomputer graphics researchers. Voxel Cone Tracing, as proposed by Cyril Crassin\net al. in 2011, makes use of mipmapped 3D textures containing a voxelized\nrepresentation of an environments direct light component to trace diffuse,\nspecular and occlusion cones in linear time to extrapolate a surface fragments\nindirect light emitted towards a given photo-receptor. Seemingly providing a\nwell-disposed balance between performance and physical fidelity, this thesis\nexamines the algorithms theoretical side on the basis of the rendering equation\nas well as its practical side in the context of a self-implemented,\nOpenGL-based variant. Whether if it can compete with long standing alternatives\nsuch as radiosity and raytracing will be determined in the subsequent\nevaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.00618v1"
    },
    {
        "title": "Cortical Morphometry Analysis based on Worst Transportation Theory",
        "authors": [
            "Min Zhang",
            "Dongsheng An",
            "Na Lei",
            "Jianfeng Wu",
            "Tong Zhao",
            "Xiaoyin Xu",
            "Yalin Wang",
            "Xianfeng Gu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Biomarkers play an important role in early detection and intervention in\nAlzheimer's disease (AD). However, obtaining effective biomarkers for AD is\nstill a big challenge. In this work, we propose to use the worst transportation\ncost as a univariate biomarker to index cortical morphometry for tracking AD\nprogression. The worst transportation (WT) aims to find the least economical\nway to transport one measure to the other, which contrasts to the optimal\ntransportation (OT) that finds the most economical way between measures. To\ncompute the WT cost, we generalize the Brenier theorem for the OT map to the WT\nmap, and show that the WT map is the gradient of a concave function satisfying\nthe Monge-Ampere equation. We also develop an efficient algorithm to compute\nthe WT map based on computational geometry. We apply the algorithm to analyze\ncortical shape difference between dementia due to AD and normal aging\nindividuals. The experimental results reveal the effectiveness of our proposed\nmethod which yields better statistical performance than other competiting\nmethods including the OT.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.00637v1"
    },
    {
        "title": "Curl-Flow: Boundary-Respecting Pointwise Incompressible Velocity\n  Interpolation for Grid-Based Fluids",
        "authors": [
            "Jumyung Chang",
            "Ruben Partono",
            "Vinicius C. Azevedo",
            "Christopher Batty"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We propose to augment standard grid-based fluid solvers with pointwise\ndivergence-free velocity interpolation, thereby ensuring exact\nincompressibility down to the sub-cell level. Our method takes as input a\ndiscretely divergence-free velocity field generated by a staggered grid\npressure projection, and first recovers a corresponding discrete vector\npotential. Instead of solving a costly vector Poisson problem for the\npotential, we develop a fast parallel sweeping strategy to find a candidate\npotential and apply a gauge transformation to enforce the Coulomb gauge\ncondition and thereby make it numerically smooth. Interpolating this discrete\npotential generates a pointwise vector potential whose analytical curl is a\npointwise incompressible velocity field. Our method further supports irregular\nsolid geometry through the use of level set-based cut-cells and a novel\nCurl-Noise-inspired potential ramping procedure that simultaneously offers\nstrictly non-penetrating velocities and incompressibility. Experimental\ncomparisons demonstrate that the vector potential reconstruction procedure at\nthe heart of our approach is consistently faster than prior such reconstruction\nschemes, especially those that solve vector Poisson problems. Moreover, in\nexchange for its modest extra cost, our overall Curl-Flow framework produces\nsignificantly improved particle trajectories that closely respect irregular\nobstacles, do not suffer from spurious sources or sinks, and yield superior\nparticle distributions over time.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.00867v2"
    },
    {
        "title": "Appearance-Driven Automatic 3D Model Simplification",
        "authors": [
            "Jon Hasselgren",
            "Jacob Munkberg",
            "Jaakko Lehtinen",
            "Miika Aittala",
            "Samuli Laine"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a suite of techniques for jointly optimizing triangle meshes and\nshading models to match the appearance of reference scenes. This capability has\na number of uses, including appearance-preserving simplification of extremely\ncomplex assets, conversion between rendering systems, and even conversion\nbetween geometric scene representations.\n  We follow and extend the classic analysis-by-synthesis family of techniques:\nenabled by a highly efficient differentiable renderer and modern nonlinear\noptimization algorithms, our results are driven to minimize the image-space\ndifference to the target scene when rendered in similar viewing and lighting\nconditions. As the only signals driving the optimization are differences in\nrendered images, the approach is highly general and versatile: it easily\nsupports many different forward rendering models such as normal mapping,\nspatially-varying BRDFs, displacement mapping, etc. Supervision through images\nonly is also key to the ability to easily convert between rendering systems and\nscene representations.\n  We output triangle meshes with textured materials to ensure that the models\nrender efficiently on modern graphics hardware and benefit from, e.g.,\nhardware-accelerated rasterization, ray tracing, and filtered texture lookups.\nOur system is integrated in a small Python code base, and can be applied at\nhigh resolutions and on large models. We describe several use cases, including\nmesh decimation, level of detail generation, seamless mesh filtering and\napproximations of aggregate geometry.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.03989v1"
    },
    {
        "title": "Velocity Skinning for Real-time Stylized Skeletal Animation",
        "authors": [
            "Damien Rohmer",
            "Marco Tarini",
            "Niranjan Kalyanasundaram",
            "Faezeh Moshfeghifar",
            "Marie-Paule Cani",
            "Victor Zordan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Secondary animation effects are essential for liveliness. We propose a\nsimple, real-time solution for adding them on top of standard skinning,\nenabling artist-driven stylization of skeletal motion. Our method takes a\nstandard skeleton animation as input, along with a skin mesh and rig weights.\nIt then derives per-vertex deformations from the different linear and angular\nvelocities along the skeletal hierarchy. We highlight two specific applications\nof this general framework, namely the cartoonlike \"squashy\" and \"floppy\"\neffects, achieved from specific combinations of velocity terms. As our results\nshow, combining these effects enables to mimic, enhance and stylize\nphysical-looking behaviours within a standard animation pipeline, for arbitrary\nskinned characters. Interactive on CPU, our method allows for GPU\nimplementation, yielding real-time performances even on large meshes. Animator\ncontrol is supported through a simple interface toolkit, enabling to refine the\ndesired type and magnitude of deformation at relevant vertices by simply\npainting weights. The resulting rigged character automatically responds to new\nskeletal animation, without further input.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.04934v1"
    },
    {
        "title": "Shapes In A Box -- Disassembling 3D objects for efficient packing and\n  fabrication",
        "authors": [
            "Marco Attene"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Modern 3D printing technologies and the upcoming mass-customization paradigm\ncall for efficient methods to produce and distribute arbitrarily-shaped 3D\nobjects. This paper introduces an original algorithm to split a 3D model in\nparts that can be efficiently packed within a box, with the objective of\nreassembling them after delivery. The first step consists in the creation of a\nhierarchy of possible parts that can be tightly packed within their minimum\nbounding boxes. In a second step, the hierarchy is exploited to extract the\n(single) segmentation whose parts can be most tightly packed. The fact that\nshape packing is an NP-complete problem justifies the use of heuristics and\napproximated solutions whose efficacy and efficiency must be assessed.\nExtensive experimentation demonstrates that our algorithm produces satisfactory\nresults for arbitrarily-shaped objects while being comparable to ad-hoc methods\nwhen specific shapes are considered.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.05281v1"
    },
    {
        "title": "Rendering Point Clouds with Compute Shaders and Vertex Order\n  Optimization",
        "authors": [
            "Markus Schütz",
            "Bernhard Kerbl",
            "Michael Wimmer"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  While commodity GPUs provide a continuously growing range of features and\nsophisticated methods for accelerating compute jobs, many state-of-the-art\nsolutions for point cloud rendering still rely on the provided point primitives\n(GL_POINTS, POINTLIST, ...) of graphics APIs for image synthesis. In this\npaper, we present several compute-based point cloud rendering approaches that\noutperform the hardware pipeline by up to an order of magnitude and achieve\nsignificantly better frame times than previous compute-based methods. Beyond\nbasic closest-point rendering, we also introduce a fast, high-quality variant\nto reduce aliasing. We present and evaluate several variants of our proposed\nmethods with different flavors of optimization, in order to ensure their\napplicability and achieve optimal performance on a range of platforms and\narchitectures with varying support for novel GPU hardware features. During our\nexperiments, the observed peak performance was reached rendering 796 million\npoints (12.7GB) at rates of 62 to 64 frames per second (50 billion points per\nsecond, 802GB/s) on an RTX 3090 without the use of level-of-detail structures.\n  We further introduce an optimized vertex order for point clouds to boost the\nefficiency of GL_POINTS by a factor of 5x in cases where hardware rendering is\ncompulsory. We compare different orderings and show that Morton sorted buffers\nare faster for some viewpoints, while shuffled vertex buffers are faster in\nothers. In contrast, combining both approaches by first sorting according to\nMorton-code and shuffling the resulting sequence in batches of 128 points leads\nto a vertex buffer layout with high rendering performance and low sensitivity\nto viewpoint changes.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.07526v1"
    },
    {
        "title": "Soft Walks: Real-Time, Two-Ways Interaction between a Character and\n  Loose Grounds",
        "authors": [
            "Chloé Paliard",
            "Eduardo Alvarado",
            "Damien Rohmer",
            "Marie-Paule Cani"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  When walking on loose terrains, possibly covered with vegetation, the ground\nand grass should deform, but the character's gait should also change\naccordingly. We propose a method for modeling such two-ways interactions in\nreal-time. We first complement a layered character model by a high-level\ncontroller, which uses position and angular velocity inputs to improve dynamic\noscillations when walking on various slopes. Secondly, at a refined level, the\nfeet are set to locally deform the ground and surrounding vegetation using\nefficient procedural functions, while the character's response to such\ndeformations is computed through adapted inverse kinematics. While simple to\nset up, our method is generic enough to adapt to any character morphology.\nMoreover, its ability to generate in real time, consistent gaits on a variety\nof loose grounds of arbitrary slope, possibly covered with grass, makes it an\ninteresting solution to enhance films and games.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.10898v2"
    },
    {
        "title": "Normal-Driven Spherical Shape Analogies",
        "authors": [
            "Hsueh-Ti Derek Liu",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This paper introduces a new method to stylize 3D geometry. The key\nobservation is that the surface normal is an effective instrument to capture\ndifferent geometric styles. Centered around this observation, we cast\nstylization as a shape analogy problem, where the analogy relationship is\ndefined on the surface normal. This formulation can deform a 3D shape into\ndifferent styles within a single framework. One can plug-and-play different\ntarget styles by providing an exemplar shape or an energy-based style\ndescription (e.g., developable surfaces). Our surface stylization methodology\nenables Normal Captures as a geometric counterpart to material captures\n(MatCaps) used in rendering, and the prototypical concept of Spherical Shape\nAnalogies as a geometric counterpart to image analogies in image processing.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.11993v2"
    },
    {
        "title": "HodgeNet: Learning Spectral Geometry on Triangle Meshes",
        "authors": [
            "Dmitriy Smirnov",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Constrained by the limitations of learning toolkits engineered for other\napplications, such as those in image processing, many mesh-based learning\nalgorithms employ data flows that would be atypical from the perspective of\nconventional geometry processing. As an alternative, we present a technique for\nlearning from meshes built from standard geometry processing modules and\noperations. We show that low-order eigenvalue/eigenvector computation from\noperators parameterized using discrete exterior calculus is amenable to\nefficient approximate backpropagation, yielding spectral per-element or\nper-mesh features with similar formulas to classical descriptors like the\nheat/wave kernel signatures. Our model uses few parameters, generalizes to\nhigh-resolution meshes, and exhibits performance and time complexity on par\nwith past work.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.12826v1"
    },
    {
        "title": "Efficacy of Images Versus Data Buffers: Optimizing Interactive\n  Applications Utilizing OpenCL for Scientific Visualization",
        "authors": [
            "Donald W. Johnson",
            "T. J. Jankun-Kelly"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This paper examines an algorithm using dual OpenCL image buffers to optimize\ndata streaming for ensemble processing and visualization. Image buffers were\nutilized because they allow cached memory access, unlike simple data buffers,\nwhich are more commonly used. OpenCL image object performance was improved by\nallowing upload and mapping into one buffer to occur concurrently with mapping\nand/or processing of data in another buffer. This technique was applied in an\ninteractive application allowing multiple flood extent maps to be combined into\na single image, and allowing users to vary input image sets in real time. The\nefficiency of this technique was tested by varying both dimensions of input\nimages and number of iterations; computation scaled linearly with number of\ninput images, with best results achieved using ~4k images. Tests were performed\nto determine the rate at which data could be moved from data buffers to image\nbuffers, examining a large range of possible image buffer dimensions.\nAdditional tests examined kernel runtimes with different image and buffer\nvariants. Limitations of the algorithm and possible applications are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14667v1"
    },
    {
        "title": "Reliving the Dataset: Combining the Visualization of Road Users'\n  Interactions with Scenario Reconstruction in Virtual Reality",
        "authors": [
            "Lars Töttel",
            "Maximilian Zipfl",
            "Daniel Bogdoll",
            "Marc René Zofka",
            "J. Marius Zöllner"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  One core challenge in the development of automated vehicles is their\ncapability to deal with a multitude of complex trafficscenarios with many, hard\nto predict traffic participants. As part of the iterative development process,\nit is necessary to detect criticalscenarios and generate knowledge from them to\nimprove the highly automated driving (HAD) function. In order to tackle this\nchallenge,numerous datasets have been released in the past years, which act as\nthe basis for the development and testing of such algorithms.Nevertheless, the\nremaining challenges are to find relevant scenes, such as safety-critical\ncorner cases, in these datasets and tounderstand them completely.Therefore,\nthis paper presents a methodology to process and analyze naturalistic motion\ndatasets in two ways: On the one hand, ourapproach maps scenes of the datasets\nto a generic semantic scene graph which allows for a high-level and objective\nanalysis. Here,arbitrary criticality measures, e.g. TTC, RSS or SFF, can be set\nto automatically detect critical scenarios between traffic participants.On the\nother hand, the scenarios are recreated in a realistic virtual reality (VR)\nenvironment, which allows for a subjective close-upanalysis from multiple,\ninteractive perspectives.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01610v3"
    },
    {
        "title": "A Practical Ply-Based Appearance Modeling for Knitted Fabrics",
        "authors": [
            "Zahra Montazeri",
            "Soren Gammelmark",
            "Henrik W. Jensen",
            "Shuang Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Modeling the geometry and the appearance of knitted fabrics has been\nchallenging due to their complex geometries and interactions with light.\n  Previous surface-based models have difficulties capturing fine-grained knit\ngeometries; Micro-appearance models, on the other hands, typically store\nindividual cloth fibers explicitly and are expensive to be generated and\nrendered.\n  Further, neither of the models have been matched the photographs to capture\nboth the reflection and the transmission of light simultaneously.\n  In this paper, we introduce an efficient technique to generate knit models\nwith user-specified knitting patterns.\n  Our model stores individual knit plies with fiber-level detailed depicted\nusing normal and tangent mapping.\n  We evaluate our generated models using a wide array of knitting patterns.\nFurther, we compare qualitatively renderings to our models to photos of real\nsamples.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.02475v1"
    },
    {
        "title": "PH-CPF: Planar Hexagonal Meshing using Coordinate Power Fields",
        "authors": [
            "Kacper Pluta",
            "Michal Edelstein",
            "Amir Vaxman",
            "Mirela Ben-Chen"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a new approach for computing planar hexagonal meshes that\napproximate a given surface, represented as a triangle mesh. Our method is\nbased on two novel technical contributions. First, we introduce Coordinate\nPower Fields, which are a pair of tangent vector fields on the surface that\nfulfill a certain continuity constraint. We prove that the fulfillment of this\nconstraint guarantees the existence of a seamless parameterization with\nquantized rotational jumps, which we then use to regularly remesh the surface.\nWe additionally propose an optimization framework for finding Coordinate Power\nFields, which also fulfill additional constraints, such as alignment, sizing\nand bijectivity. Second, we build upon this framework to address a challenging\nmeshing problem: planar hexagonal meshing. To this end, we suggest a\ncombination of conjugacy, scaling and alignment constraints, which together\nlead to planarizable hexagons. We demonstrate our approach on a variety of\nsurfaces, automatically generating planar hexagonal meshes on complicated\nmeshes, which were not achievable with existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.02507v1"
    },
    {
        "title": "DeepFaceEditing: Deep Face Generation and Editing with Disentangled\n  Geometry and Appearance Control",
        "authors": [
            "Shu-Yu Chen",
            "Feng-Lin Liu",
            "Yu-Kun Lai",
            "Paul L. Rosin",
            "Chunpeng Li",
            "Hongbo Fu",
            "Lin Gao"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Recent facial image synthesis methods have been mainly based on conditional\ngenerative models. Sketch-based conditions can effectively describe the\ngeometry of faces, including the contours of facial components, hair\nstructures, as well as salient edges (e.g., wrinkles) on face surfaces but lack\neffective control of appearance, which is influenced by color, material,\nlighting condition, etc. To have more control of generated results, one\npossible approach is to apply existing disentangling works to disentangle face\nimages into geometry and appearance representations. However, existing\ndisentangling methods are not optimized for human face editing, and cannot\nachieve fine control of facial details such as wrinkles. To address this issue,\nwe propose DeepFaceEditing, a structured disentanglement framework specifically\ndesigned for face images to support face generation and editing with\ndisentangled control of geometry and appearance. We adopt a local-to-global\napproach to incorporate the face domain knowledge: local component images are\ndecomposed into geometry and appearance representations, which are fused\nconsistently using a global fusion module to improve generation quality. We\nexploit sketches to assist in extracting a better geometry representation,\nwhich also supports intuitive geometry editing via sketching. The resulting\nmethod can either extract the geometry and appearance representations from face\nimages, or directly extract the geometry representation from face sketches.\nSuch representations allow users to easily edit and synthesize face images,\nwith decoupled control of their geometry and appearance. Both qualitative and\nquantitative evaluations show the superior detail and appearance control\nabilities of our method compared to state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.08935v2"
    },
    {
        "title": "Projection matrices and related viewing frustums: new ways to create and\n  apply",
        "authors": [
            "Nikita Glushkov",
            "Tyuleneva Emiliya"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In computer graphics, the field of view of a camera is represented by a\nviewing frustum and a corresponding projection matrix, the properties of which,\nin the absence of restrictions on rectangular shape of the near plane and its\nparallelism to the far plane are currently not fully explored and structured.\nThis study aims to consider the properties of arbitrary affine frustums, as\nwell as various techniques for their transformation for practical use in\ndevices with limited resources. Additionally, this article explores the methods\nof working with the visible volume as an arbitrary frustum that is not\nassociated with the projection matrix. To study the properties of affine\nfrustums, the dependencies between its planes and formulas for obtaining key\npoints from the inverse projection matrix were derived. Methods of constructing\nfrustum by key points and given planes were also considered. Moreover, frustum\ntransformation formulas were obtained to simulate the effects of reflection,\nrefraction and cropping in devices with limited resources. In conclusion, a\nmethod was proposed for applying an arbitrary frustum, which does not have a\ncorresponding projection matrix, to limit the visible volume and then transform\nthe points into NDC space.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.09476v1"
    },
    {
        "title": "Lessons Learned and Improvements when Building Screen-Space Samplers\n  with Blue-Noise Error Distribution",
        "authors": [
            "Laurent Belcour",
            "Eric Heitz"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Recent work has shown that the error of Monte-Carlo rendering is visually\nmore acceptable when distributed as blue-noise in screen-space. Despite recent\nefforts, building a screen-space sampler is still an open problem. In this\ntalk, we present the lessons we learned while improving our previous\nscreen-space sampler. Specifically: we advocate for a new criterion to assess\nthe quality of such samplers; we introduce a new screen-space sampler based on\nrank-1 lattices; we provide a parallel optimization method that is compatible\nwith a GPU implementation and that achieves better quality; we detail the\npitfalls of using such samplers in renderers and how to cope with many\ndimensions; and we provide empirical proofs of the versatility of the\noptimization process.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.12620v2"
    },
    {
        "title": "Robust Voxelization and Visualization by Improved Tetrahedral Mesh\n  Generation",
        "authors": [
            "Joseph Chen",
            "Ko-Wei Tai",
            "Wen-Chin Chen",
            "Ming Ouhyoung"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  When obtaining interior 3D voxel data from triangular meshes, most existing\nmethods fail to handle low quality meshes which happens to take up a big\nportion on the internet. In this work we present a robust voxelization method\nthat is based on tetrahedral mesh generation within a user defined error bound.\nComparing to other tetrahedral mesh generation methods, our method produces\nmuch higher quality tetrahedral meshes as the intermediate outcome, which\nallows us to utilize a faster voxelization algorithm that is based on a\nstronger assumption. We show the results comparing to various methods including\nthe state-of-the-art. Our contribution includes a framework which takes\ntriangular mesh as an input and produces voxelized data, a proof to an unproved\nalgorithm that performs better than the state-of-the-art, and various\nexperiments including parallelization built on the GPU and CPU. We further\ntested our method on various dataset including Princeton ModelNet and Thingi10k\nto show the robustness of the framework, where near 100% availability is\nachieved, while others can only achieve around 50%.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.01326v2"
    },
    {
        "title": "ClipGen: A Deep Generative Model for Clipart Vectorization and Synthesis",
        "authors": [
            "I-Chao Shen",
            "Bing-Yu Chen"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This paper presents a novel deep learning-based approach for automatically\nvectorizing and synthesizing the clipart of man-made objects. Given a raster\nclipart image and its corresponding object category (e.g., airplanes), the\nproposed method sequentially generates new layers, each of which is composed of\na new closed path filled with a single color. The final result is obtained by\ncompositing all layers together into a vector clipart image that falls into the\ntarget category. The proposed approach is based on an iterative generative\nmodel that (i) decides whether to continue synthesizing a new layer and (ii)\ndetermines the geometry and appearance of the new layer. We formulated a joint\nloss function for training our generative model, including the shape\nsimilarity, symmetry, and local curve smoothness losses, as well as vector\ngraphics rendering accuracy loss for synthesizing clipart recognizable by\nhumans. We also introduced a collection of man-made object clipart, ClipNet,\nwhich is composed of closed-path layers, and two designed preprocessing tasks\nto clean up and enrich the original raw clipart. To validate the proposed\napproach, we conducted several experiments and demonstrated its ability to\nvectorize and synthesize various clipart categories. We envision that our\ngenerative model can facilitate efficient and intuitive clipart designs for\nnovice users and graphic designers.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.04912v1"
    },
    {
        "title": "Real-Time Denoising of Volumetric Path Tracing for Direct Volume\n  Rendering",
        "authors": [
            "Jose A. Iglesias-Guitian",
            "Prajita Mane",
            "Bochang Moon"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Direct Volume Rendering (DVR) using Volumetric Path Tracing (VPT) is a\nscientific visualization technique that simulates light transport with objects'\nmatter using physically-based lighting models. Monte Carlo (MC) path tracing is\noften used with surface models, yet its application for volumetric models is\ndifficult due to the complexity of integrating MC light-paths in volumetric\nmedia with none or smooth material boundaries. Moreover, auxiliary\ngeometry-buffers (G-buffers) produced for volumes are typically very noisy,\nfailing to guide image denoisers relying on that information to preserve image\ndetails. This makes existing real-time denoisers, which take noise-free\nG-buffers as their input, less effective when denoising VPT images. We propose\nthe necessary modifications to an image-based denoiser previously used when\nrendering surface models, and demonstrate effective denoising of VPT images. In\nparticular, our denoising exploits temporal coherence between frames, without\nrelying on noise-free G-buffers, which has been a common assumption of existing\ndenoisers for surface-models. Our technique preserves high-frequency details\nthrough a weighted recursive least squares that handles heterogeneous noise for\nvolumetric models. We show for various real data sets that our method improves\nthe visual fidelity and temporal stability of VPT during classic DVR operations\nsuch as camera movements, modifications of the light sources, and editions to\nthe volume transfer function.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.08034v1"
    },
    {
        "title": "Construction of Planar and Symmetric Truss Structures with Interlocking\n  Edge Elements",
        "authors": [
            "Anantha Natarajan",
            "Jiaqi Cui",
            "Ergun Akleman",
            "Vinayak Krishnamurthy"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this paper, we present an algorithmic approach to design and construct\nplanar truss structures based on symmetric lattices using modular elements. The\nmethod of assembly is similar to Leonardo grids as they both rely on the\nproperty of interlocking. In theory, our modular elements can be assembled by\nthe same type of binary operations. Our modular elements embody the principle\nof geometric interlocking, a principle recently introduced in literature that\nallows for pieces of an assembly to be interlocked in a way that they can\nneither be assembled nor disassembled unless the pieces are subjected to\ndeformation or breakage. We demonstrate that breaking the pieces can indeed\nfacilitate the effective assembly of these pieces through the use of a simple\nkey-in-hole concept. As a result, these modular elements can be assembled\ntogether to form an interlocking structure, in which the locking pieces apply\nthe force necessary to hold the entire assembly together.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.10363v1"
    },
    {
        "title": "ExplorerTree: a focus+context exploration approach for 2D embeddings",
        "authors": [
            "Wilson E. Marcílio-Jr",
            "Danilo M. Eler",
            "Fernando V. Paulovich",
            "José F. Rodrigues-Jr",
            "Almir O. Artero"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In exploratory tasks involving high-dimensional datasets, dimensionality\nreduction (DR) techniques help analysts to discover patterns and other useful\ninformation. Although scatter plot representations of DR results allow for\ncluster identification and similarity analysis, such a visual metaphor presents\nproblems when the number of instances of the dataset increases, resulting in\ncluttered visualizations. In this work, we propose a scatter plot-based\nmultilevel approach to display DR results and address clutter-related problems\nwhen visualizing large datasets, together with the definition of a methodology\nto use focus+context interaction on non-hierarchical embeddings. The proposed\ntechnique, called ExplorerTree, uses a sampling selection technique on scatter\nplots to reduce visual clutter and guide users through exploratory tasks. We\ndemonstrate ExplorerTree's effectiveness through a use case, where we visually\nexplore activation images of the convolutional layers of a neural network.\nFinally, we also conducted a user experiment to evaluate ExplorerTree's ability\nto convey embedding structures using different sampling strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.10592v2"
    },
    {
        "title": "Design and Fabrication of Multi-Patch Elastic Geodesic Grid Structures",
        "authors": [
            "Stefan Pillwein",
            "Johanna Kübert",
            "Florian Rist",
            "Przemyslaw Musialski"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Elastic geodesic grids (EGG) are lightweight structures that can be deployed\nto approximate designer-provided free-form surfaces. Initially, the grids are\nperfectly flat, during deployment, a curved shape emerges, as grid elements\nbend and twist. Their layout is based on networks of geodesic curves and is\nfound geometrically. Encoded in the planar grids is the intrinsic shape of the\ndesign surface. Such structures may serve purposes like free-form\nsub-structures, panels, sun and rain protectors, pavilions, etc. However, so\nfar the EGG have only been investigated using a generic set of design surfaces\nand small-scale desktop models. Some limitations become apparent when\nconsidering more sophisticated design surfaces, like from free-form\narchitecture. Due to characteristics like high local curvature or non-geodesic\nboundaries, they may be captured only poorly by a single EGG.\n  We show how decomposing such surfaces into smaller patches serves as an\neffective strategy to tackle these problems. We furthermore show that elastic\ngeodesic grids are in fact well suited for this approach. Finally, we present a\nshowcase model of some meters in size and discuss practical aspects concerning\nfabrication, size, and easy deployment.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.12643v1"
    },
    {
        "title": "Half-body Portrait Relighting with Overcomplete Lighting Representation",
        "authors": [
            "Guoxian Song",
            "Tat-Jen Cham",
            "Jianfei Cai",
            "Jianmin Zheng"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a neural-based model for relighting a half-body portrait image by\nsimply referring to another portrait image with the desired lighting condition.\nRather than following classical inverse rendering methodology that involves\nestimating normals, albedo and environment maps, we implicitly encode the\nsubject and lighting in a latent space, and use these latent codes to generate\nrelighted images by neural rendering. A key technical innovation is the use of\na novel overcomplete lighting representation, which facilitates lighting\ninterpolation in the latent space, as well as helping regularize the\nself-organization of the lighting latent space during training. In addition, we\npropose a novel multiplicative neural render that more effectively combines the\nsubject and lighting latent codes for rendering. We also created a large-scale\nphotorealistic rendered relighting dataset for training, which allows our model\nto generalize well to real images. Extensive experiments demonstrate that our\nsystem not only outperforms existing methods for referral-based portrait\nrelighting, but also has the capability generate sequences of relighted images\nvia lighting rotations.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.13425v1"
    },
    {
        "title": "Frame Field Operators",
        "authors": [
            "David R. Palmer",
            "Oded Stein",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Differential operators are widely used in geometry processing for problem\ndomains like spectral shape analysis, data interpolation, parametrization and\nmapping, and meshing. In addition to the ubiquitous cotangent Laplacian,\nanisotropic second-order operators, as well as higher-order operators such as\nthe Bilaplacian, have been discretized for specialized applications. In this\npaper, we study a class of operators that generalizes the fourth-order\nBilaplacian to support anisotropic behavior. The anisotropy is parametrized by\na symmetric frame field, first studied in connection with quadrilateral and\nhexahedral meshing, which allows for fine-grained control of local directions\nof variation. We discretize these operators using a mixed finite element\nscheme, verify convergence of the discretization, study the behavior of the\noperator under pullback, and present potential applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.14360v1"
    },
    {
        "title": "Accessible Color Sequences for Data Visualization",
        "authors": [
            "Matthew A. Petroff"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Color sequences, ordered sets of colors for data visualization, that balance\naesthetics with accessibility considerations are presented. In order to model\naesthetic preference, data were collected with an online survey, and the\nresults were used to train a machine-learning model. To ensure accessibility,\nthis model was combined with minimum-perceptual-distance constraints, including\nfor simulated color-vision deficiencies, as well as with\nminimum-lightness-distance constraints for grayscale printing,\nmaximum-lightness constraints for maintaining contrast with a white background,\nand scores from a color-saliency model for ease of use of the colors in verbal\nand written descriptions. Optimal color sequences containing six, eight, and\nten colors were generated using the data-driven aesthetic-preference model and\naccessibility constraints. Due to the balance of aesthetics and accessibility\nconsiderations, the resulting color sequences can serve as reasonable defaults\nin data-plotting codes, e.g., for use in scatter plots and line plots.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.02270v3"
    },
    {
        "title": "Exact Analytical Parallel Vectors",
        "authors": [
            "Hanqi Guo",
            "Tom Peterka"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This paper demonstrates that parallel vector curves are piecewise cubic\nrational curves in 3D piecewise linear vector fields. Parallel vector curves --\nloci of points where two vector fields are parallel -- have been widely used to\nextract features including ridges, valleys, and vortex core lines in scientific\ndata. We define the term \\emph{generalized and underdetermined eigensystem} in\nthe form of\n$\\mathbf{A}\\mathbf{x}+\\mathbf{a}=\\lambda(\\mathbf{B}\\mathbf{x}+\\mathbf{b})$ in\norder to derive the piecewise rational representation of 3D parallel vector\ncurves. We discuss how singularities of the rationals lead to different types\nof intersections with tetrahedral cells.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.02708v1"
    },
    {
        "title": "A Splitting Scheme for Flip-Free Distortion Energies",
        "authors": [
            "Oded Stein",
            "Jiajin Li",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We introduce a robust optimization method for flip-free distortion energies\nused, for example, in parametrization, deformation, and volume correspondence.\nThis method can minimize a variety of distortion energies, such as the\nsymmetric Dirichlet energy and our new symmetric gradient energy. We identify\nand exploit the special structure of distortion energies to employ an operator\nsplitting technique, leading us to propose a novel Alternating Direction Method\nof Multipliers (ADMM) algorithm to deal with the non-convex, non-smooth nature\nof distortion energies. The scheme results in an efficient method where the\nglobal step involves a single matrix multiplication and the local steps are\nclosed-form per-triangle/per-tetrahedron expressions that are highly\nparallelizable. The resulting general-purpose optimization algorithm exhibits\nrobustness to flipped triangles and tetrahedra in initial data as well as\nduring the optimization. We establish the convergence of our proposed algorithm\nunder certain conditions and demonstrate applications to parametrization,\ndeformation, and volume correspondence.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.05200v2"
    },
    {
        "title": "CurveFusion: Reconstructing Thin Structures from RGBD Sequences",
        "authors": [
            "Lingjie Liu",
            "Nenglun Chen",
            "Duygu Ceylan",
            "Christian Theobalt",
            "Wenping Wang",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We introduce CurveFusion, the first approach for high quality scanning of\nthin structures at interactive rates using a handheld RGBD camera. Thin\nfilament-like structures are mathematically just 1D curves embedded in R^3, and\nintegration-based reconstruction works best when depth sequences (from the thin\nstructure parts) are fused using the object's (unknown) curve skeleton. Thus,\nusing the complementary but noisy color and depth channels, CurveFusion first\nautomatically identifies point samples on potential thin structures and groups\nthem into bundles, each being a group of a fixed number of aligned consecutive\nframes. Then, the algorithm extracts per-bundle skeleton curves using L1 axes,\nand aligns and iteratively merges the L1 segments from all the bundles to form\nthe final complete curve skeleton. Thus, unlike previous methods,\nreconstruction happens via integration along a data-dependent fusion primitive,\ni.e., the extracted curve skeleton. We extensively evaluate CurveFusion on a\nrange of challenging examples, different scanner and calibration settings, and\npresent high fidelity thin structure reconstructions previously just not\npossible from raw RGBD sequences.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.05284v1"
    },
    {
        "title": "Particle Merging-and-Splitting",
        "authors": [
            "Nghia Truong",
            "Cem Yuksel",
            "Chakrit Watcharopas",
            "Joshua A. Levine",
            "Robert M. Kirby"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Robustly handling collisions between individual particles in a large\nparticle-based simulation has been a challenging problem. We introduce particle\nmerging-and-splitting, a simple scheme for robustly handling collisions between\nparticles that prevents inter-penetrations of separate objects without\nintroducing numerical instabilities. This scheme merges colliding particles at\nthe beginning of the time-step and then splits them at the end of the\ntime-step. Thus, collisions last for the duration of a time-step, allowing\nneighboring particles of the colliding particles to influence each other. We\nshow that our merging-and-splitting method is effective in robustly handling\ncollisions and avoiding penetrations in particle-based simulations. We also\nshow how our merging-and-splitting approach can be used for coupling different\nsimulation systems using different and otherwise incompatible integrators. We\npresent simulation tests involving complex solid-fluid interactions, including\nsolid fractures generated by fluid interactions.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.08093v1"
    },
    {
        "title": "Efficient Dataflow Modeling of Peripheral Encoding in the Human Visual\n  System",
        "authors": [
            "Rachel Brown",
            "Vasha DuTell",
            "Bruce Walter",
            "Ruth Rosenholtz",
            "Peter Shirley",
            "Morgan McGuire",
            "David Luebke"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Computer graphics seeks to deliver compelling images, generated within a\ncomputing budget, targeted at a specific display device, and ultimately viewed\nby an individual user. The foveated nature of human vision offers an\nopportunity to efficiently allocate computation and compression to appropriate\nareas of the viewer's visual field, especially with the rise of high resolution\nand wide field-of-view display devices. However, while the ongoing study of\nfoveal vision is advanced, much less is known about how humans process imagery\nin the periphery of their vision -- which comprises, at any given moment, the\nvast majority of the pixels in the image. We advance computational models for\nperipheral vision aimed toward their eventual use in computer graphics. In\nparticular, we present a dataflow computational model of peripheral encoding\nthat is more efficient than prior pooling - based methods and more compact than\ncontrast sensitivity-based methods. Further, we account for the explicit\nencoding of \"end stopped\" features in the image, which was missing from\nprevious methods. Finally, we evaluate our model in the context of perception\nof textures in the periphery. Our improved peripheral encoding may simplify\ndevelopment and testing of more sophisticated, complete models in more robust\nand realistic settings relevant to computer graphics.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.11505v1"
    },
    {
        "title": "Co-Optimization of Design and Fabrication Plans for Carpentry",
        "authors": [
            "Haisen Zhao",
            "Max Willsey",
            "Amy Zhu",
            "Chandrakana Nandi",
            "Zachary Tatlock",
            "Justin Solomon",
            "Adriana Schulz"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Past work on optimizing fabrication plans given a carpentry design can\nprovide Pareto-optimal plans trading off between material waste, fabrication\ntime, precision, and other considerations. However, when developing fabrication\nplans, experts rarely restrict to a single design, instead considering families\nof design variations, sometimes adjusting designs to simplify fabrication.\nJointly exploring the design and fabrication plan spaces for each design is\nintractable using current techniques. We present a new approach to jointly\noptimize design and fabrication plans for carpentered objects. To make this\nbi-level optimization tractable, we adapt recent work from program synthesis\nbased on equality graphs (e-graphs), which encode sets of equivalent programs.\nOur insight is that subproblems within our bi-level problem share significant\nsubstructures. By representing both designs and fabrication plans in a new bag\nof parts(BOP) e-graph, we amortize the cost of optimizing design components\nshared among multiple candidates. Even using BOP e-graphs, the optimization\nspace grows quickly in practice. Hence, we also show how a feedback-guided\nsearch strategy dubbed Iterative Contraction and Expansion on E-graphs(ICEE)\ncan keep the size of the e-graph manage-able and direct the search toward\npromising candidates. We illustrate the advantages of our pipeline through\nexamples from the carpentry domain.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.12265v2"
    },
    {
        "title": "Differentiable Direct Volume Rendering",
        "authors": [
            "Sebastian Weiss",
            "Rüdiger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a differentiable volume rendering solution that provides\ndifferentiability of all continuous parameters of the volume rendering process.\nThis differentiable renderer is used to steer the parameters towards a setting\nwith an optimal solution of a problem-specific objective function. We have\ntailored the approach to volume rendering by enforcing a constant memory\nfootprint via analytic inversion of the blending functions. This makes it\nindependent of the number of sampling steps through the volume and facilitates\nthe consideration of small-scale changes. The approach forms the basis for\nautomatic optimizations regarding external parameters of the rendering process\nand the volumetric density field itself. We demonstrate its use for automatic\nviewpoint selection using differentiable entropy as objective, and for\noptimizing a transfer function from rendered images of a given volume.\nOptimization of per-voxel densities is addressed in two different ways: First,\nwe mimic inverse tomography and optimize a 3D density field from images using\nan absorption model. This simplification enables comparisons with algebraic\nreconstruction techniques and state-of-the-art differentiable path tracers.\nSecond, we introduce a novel approach for tomographic reconstruction from\nimages using an emission-absorption model with post-shading via an arbitrary\ntransfer function.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.12672v1"
    },
    {
        "title": "Projector Pixel Redirection Using Phase-Only Spatial Light Modulator",
        "authors": [
            "Haruka Terai",
            "Daisuke Iwai",
            "Kosuke Sato"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In projection mapping from a projector to a non-planar surface, the pixel\ndensity on the surface becomes uneven. This causes the critical problem of\nlocal spatial resolution degradation. We confirmed that the pixel density\nuniformity on the surface was improved by redirecting projected rays using a\nphase-only spatial light modulator.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.12900v1"
    },
    {
        "title": "Stochastic Geometric Iterative Method for Loop Subdivision Surface\n  Fitting",
        "authors": [
            "Chenkai Xu",
            "Yaqi He",
            "Hui Hu",
            "Hongwei Lin"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this paper, we propose a stochastic geometric iterative method to\napproximate the high-resolution 3D models by finite Loop subdivision surfaces.\nGiven an input mesh as the fitting target, the initial control mesh is\ngenerated using the mesh simplification algorithm. Then, our method adjusts the\ncontrol mesh iteratively to make its finite Loop subdivision surface\napproximates the input mesh. In each geometric iteration, we randomly select\npart of points on the subdivision surface to calculate the difference vectors\nand distribute the vectors to the control points. Finally, the control points\nare updated by adding the weighted average of these difference vectors. We\nprove the convergence of our method and verify it by demonstrating error curves\nin the experiment. In addition, compared with an existing geometric iterative\nmethod, our method has a faster fitting speed and higher fitting precision.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14024v1"
    },
    {
        "title": "Spherical Cap Harmonic Analysis (SCHA) for Characterising the Morphology\n  of Rough Surface Patches",
        "authors": [
            "Mahmoud Shaqfa",
            "Gary P. T. Choi",
            "Katrin Beyer"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We use spherical cap harmonic (SCH) basis functions to analyse and\nreconstruct the morphology of scanned genus-0 rough surface patches with open\nedges. We first develop a novel one-to-one conformal mapping algorithm with\nminimal area distortion for parameterising a surface onto a polar spherical cap\nwith a prescribed half angle. We then show that as a generalisation of the\nhemispherical harmonic analysis, the SCH analysis provides the most added value\nfor small half angles, i.e., for nominally flat surfaces where the distortion\nintroduced by the parameterisation algorithm is smaller when the surface is\nprojected onto a spherical cap with a small half angle than onto a hemisphere.\nFrom the power spectral analysis of the expanded SCH coefficients, we estimate\na direction-independent Hurst exponent. We also estimate the wavelengths\nassociated with the orders of the SCH basis functions from the dimensions of\nthe first degree ellipsoidal cap. By windowing the spectral domain, we limit\nthe bandwidth of wavelengths included in a reconstructed surface geometry. This\nbandlimiting can be used for modifying surfaces, such as for generating finite\nor discrete element meshes for contact problems. The codes and data developed\nin this paper are made available under the GNU LGPLv2.1 license.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14629v1"
    },
    {
        "title": "Co-Optimization of Design and Fabrication Plans for Carpentry:\n  Supplemental Material",
        "authors": [
            "Haisen Zhao",
            "Max Willsey",
            "Amy Zhu",
            "Chandrakana Nandi",
            "Zachary Tatlock",
            "Justin Solomon",
            "Adriana Schulz"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Past work on optimizing fabrication plans given a carpentry design can\nprovide Pareto-optimal plans trading off between material waste, fabrication\ntime, precision, and other considerations. However, when developing fabrication\nplans, experts rarely restrict to a single design, instead considering families\nof design variations, sometimes adjusting designs to simplify fabrication.\nJointly exploring the design and fabrication plan spaces for each design is\nintractable using current techniques. We present a new approach to jointly\noptimize design and fabrication plans for carpentered objects. To make this\nbi-level optimization tractable, we adapt recent work from program synthesis\nbased on equality graphs (e-graphs), which encode sets of equivalent programs.\nOur insight is that subproblems within our bi-level problem share significant\nsubstructures. By representing both designs and fabrication plans in a new bag\nof parts(BOP) e-graph, we amortize the cost of optimizing design components\nshared among multiple candidates. Even using BOP e-graphs, the optimization\nspace grows quickly in practice. Hence, we also show how a feedback-guided\nsearch strategy dubbed Iterative Contraction and Expansion on E-graphs (ICEE)\ncan keep the size of the e-graph manage-able and direct the search toward\npromising candidates. We illustrate the advantages of our pipeline through\nexamples from the carpentry domain.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14745v1"
    },
    {
        "title": "A-ULMPM: An Arbitrary Updated Lagrangian Material Point Method for\n  Efficient Simulation of Solids and Fluids",
        "authors": [
            "Haozhe Su",
            "Tao Xue",
            "Chengguizi Han",
            "Mridul Aanjaneya"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present an arbitrary updated Lagrangian Material Point Method (A-ULMPM) to\nalleviate issues, such as the cell-crossing instability and numerical fracture,\nthat plague state of the art Eulerian formulations of MPM, while still allowing\nfor large deformations that arise in fluid simulations. Our proposed framework\nspans MPM discretizations from total Lagrangian formulations to Eulerian\nformulations. We design an easy-to-implement physics-based criterion that\nallows A-ULMPM to update the reference configuration adaptively for measuring\nphysical states including stress, strain, interpolation kernels and their\nderivatives. For better efficiency and conservation of angular momentum, we\nfurther integrate the APIC[Jiang et al. 2015] and MLS-MPM[Hu et al. 2018]\nformulations in A-ULMPM by augmenting the accuracy of velocity rasterization\nusing both the local velocity and its first-order derivatives. Our theoretical\nderivations use a nodal discretized Lagrangian, instead of the weak form\ndiscretization in MLS-MPM[Hu et al. 2018], and naturally lead to a \"modified\"\nMLS-MPM in A-ULMPM, which can recover MLS-MPM using a completely Eulerian\nformulation. A-ULMPM does not require significant changes to traditional\nEulerian formulations of MPM, and is computationally more efficient since it\nonly updates interpolation kernels and their derivatives when large topology\nchanges occur. We present end-to-end 3D simulations of stretching and twisting\nhyperelastic solids, splashing liquids, and multi-material interactions with\nlarge deformations to demonstrate the efficacy of our novel A-ULMPM framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.00388v1"
    },
    {
        "title": "Automatic Polygon Layout for Primal-Dual Visualization of Hypergraphs",
        "authors": [
            "Botong Qu",
            "Eugene Zhang",
            "Yue Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  N-ary relationships, which relate N entities where N is not necessarily two,\ncan be visually represented as polygons whose vertices are the entities of the\nrelationships. Manually generating a high-quality layout using this\nrepresentation is labor-intensive. In this paper, we provide an automatic\npolygon layout generation algorithm for the visualization of N-ary\nrelationships. At the core of our algorithm is a set of objective functions\nmotivated by a number of design principles that we have identified. These\nobjective functions are then used in an optimization framework that we develop\nto achieve high-quality layouts. Recognizing the duality between entities and\nrelationships in the data, we provide a second visualization in which the roles\nof entities and relationships in the original data are reversed. This can lead\nto additional insight about the data. Furthermore, we enhance our framework for\na joint optimization on the primal layout (original data) and the dual layout\n(where the roles of entities and relationships are reversed). This allows users\nto inspect their data using two complementary views. We apply our visualization\napproach to a number of datasets that include co-authorship data and social\ncontact pattern data.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.00671v1"
    },
    {
        "title": "Feature Curves and Surfaces of 3D Asymmetric Tensor Fields",
        "authors": [
            "Shih-Hsuan Hung",
            "Yue Zhang",
            "Harry Yeh",
            "Eugene Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  3D asymmetric tensor fields have found many applications in science and\nengineering domains, such as fluid dynamics and solid mechanics. 3D asymmetric\ntensors can have complex eigenvalues, which makes their analysis and\nvisualization more challenging than 3D symmetric tensors. Existing research in\ntensor field visualization focuses on 2D asymmetric tensor fields and 3D\nsymmetric tensor fields. In this paper, we address the analysis and\nvisualization of 3D asymmetric tensor fields. We introduce six topological\nsurfaces and one topological curve, which lead to an eigenvalue space based on\nthe tensor mode that we define. In addition, we identify several\nnon-topological feature surfaces that are nonetheless physically important.\nIncluded in our analysis are the realizations that triple degenerate tensors\nare structurally stable and form curves, unlike the case for 3D symmetric\ntensors fields. Furthermore, there are two different ways of measuring the\nrelative strengths of rotation and angular deformation in the tensor fields,\nunlike the case for 2D asymmetric tensor fields. We extract these feature\nsurfaces using the A-patches algorithm. However, since three of our feature\nsurfaces are quadratic, we develop a method to extract quadratic surfaces at\nany given accuracy. To facilitate the analysis of eigenvector fields, we\nvisualize a hyperstreamline as a tree stem with the other two eigenvectors\nrepresented as thorns in the real domain or the dual-eigenvectors as leaves in\nthe complex domain. To demonstrate the effectiveness of our analysis and\nvisualization, we apply our approach to datasets from solid mechanics and fluid\ndynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.02114v1"
    },
    {
        "title": "Uncertainty Visualization of the Marching Squares and Marching Cubes\n  Topology Cases",
        "authors": [
            "Tushar M. Athawale",
            "Sudhanshu Sane",
            "Chris R. Johnson"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Marching squares (MS) and marching cubes (MC) are widely used algorithms for\nlevel-set visualization of scientific data. In this paper, we address the\nchallenge of uncertainty visualization of the topology cases of the MS and MC\nalgorithms for uncertain scalar field data sampled on a uniform grid. The\nvisualization of the MS and MC topology cases for uncertain data is challenging\ndue to their exponential nature and the possibility of multiple topology cases\nper cell of a grid. We propose the topology case count and entropy-based\ntechniques for quantifying uncertainty in the topology cases of the MS and MC\nalgorithms when noise in data is modeled with probability distributions. We\ndemonstrate the applicability of our techniques for independent and correlated\nuncertainty assumptions. We visualize the quantified topological uncertainty\nvia color mapping proportional to uncertainty, as well as with interactive\nprobability queries in the MS case and entropy isosurfaces in the MC case. We\ndemonstrate the utility of our uncertainty quantification framework in\nidentifying the isovalues exhibiting relatively high topological uncertainty.\nWe illustrate the effectiveness of our techniques via results on synthetic,\nsimulation, and hixel datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.03066v1"
    },
    {
        "title": "GCN-Denoiser: Mesh Denoising with Graph Convolutional Networks",
        "authors": [
            "Yuefan Shen",
            "Hongbo Fu",
            "Zhongshuo Du",
            "Xiang Chen",
            "Evgeny Burnaev",
            "Denis Zorin",
            "Kun Zhou",
            "Youyi Zheng"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this paper, we present GCN-Denoiser, a novel feature-preserving mesh\ndenoising method based on graph convolutional networks (GCNs). Unlike previous\nlearning-based mesh denoising methods that exploit hand-crafted or voxel-based\nrepresentations for feature learning, our method explores the structure of a\ntriangular mesh itself and introduces a graph representation followed by graph\nconvolution operations in the dual space of triangles. We show such a graph\nrepresentation naturally captures the geometry features while being lightweight\nfor both training and inference. To facilitate effective feature learning, our\nnetwork exploits both static and dynamic edge convolutions, which allow us to\nlearn information from both the explicit mesh structure and potential implicit\nrelations among unconnected neighbors. To better approximate an unknown noise\nfunction, we introduce a cascaded optimization paradigm to progressively\nregress the noise-free facet normals with multiple GCNs. GCN-Denoiser achieves\nthe new state-of-the-art results in multiple noise datasets, including CAD\nmodels often containing sharp features and raw scan models with real noise\ncaptured from different devices. We also create a new dataset called PrintData\ncontaining 20 real scans with their corresponding ground-truth meshes for the\nresearch community. Our code and data are available in\nhttps://github.com/Jhonve/GCN-Denoiser.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.05128v1"
    },
    {
        "title": "Jurassic Mark: Inattentional Blindness for a Datasaurus Reveals that\n  Visualizations are Explored, not Seen",
        "authors": [
            "Tal Boger",
            "Steven B. Most",
            "Steven L. Franconeri"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Graphs effectively communicate data because they capitalize on the visual\nsystem's ability to rapidly extract patterns. Yet, this pattern extraction does\nnot occur in a single glance. Instead, research on visual attention suggests\nthat the visual system iteratively applies a sequence of filtering operations\non an image, extracting patterns from subsets of visual information over time,\nwhile selectively inhibiting other information at each of these moments. To\ndemonstrate that this powerful series of filtering operations also occurs\nduring the perception of visualized data, we designed a task where participants\nmade judgments from one class of marks on a scatterplot, presumably\nincentivizing them to relatively ignore other classes of marks. Participants\nconsistently missed a conspicuous dinosaur in the ignored collection of marks\n(93% for a 1s presentation, and 61% for 2.5s), but not in a control condition\nwhere the incentive to ignore that collection was removed (25% for a 1s\npresentation, and 11% for 2.5s), revealing that data visualizations are not\n\"seen\" in a single glance, and instead require an active process of\nexploration.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.05182v2"
    },
    {
        "title": "Dynamic Diffuse Global Illumination Resampling",
        "authors": [
            "Zander Majercik",
            "Thomas Müller",
            "Alexander Keller",
            "Derek Nowrouzezahrai",
            "Morgan McGuire"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Interactive global illumination remains a challenge in radiometrically- and\ngeometrically-complex scenes. Specialized sampling strategies are effective for\nspecular and near-specular transport because the scattering has relatively low\ndirectional variance per scattering event. In contrast, the high variance from\ntransport paths comprising multiple rough glossy or diffuse scattering events\nremains notoriously difficult to resolve with a small number of samples. We\nextend unidirectional path tracing to address this by combining screen-space\nreservoir resampling and sparse world-space probes, significantly improving\nsample efficiency for transport contributions that terminate on diffuse\nscattering events. Our experiments demonstrate a clear improvement -- at equal\ntime and equal quality -- over purely path traced and purely probe-based\nbaselines. Moreover, when combined with commodity denoisers, we are able to\ninteractively render global illumination in complex scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.05263v1"
    },
    {
        "title": "Edge-Path Bundling: A Less Ambiguous Edge Bundling Approach",
        "authors": [
            "Markus Wallinger",
            "Daniel Archambault",
            "David Auber",
            "Martin Nöllenburg",
            "Jaakko Peltonen"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Edge bundling techniques cluster edges with similar attributes (i.e.\nsimilarity in direction and proximity) together to reduce the visual clutter.\nAll edge bundling techniques to date implicitly or explicitly cluster groups of\nindividual edges, or parts of them, together based on these attributes. These\nclusters can result in ambiguous connections that do not exist in the data.\nConfluent drawings of networks do not have these ambiguities, but require the\nlayout to be computed as part of the bundling process. We devise a new bundling\nmethod, Edge-Path bundling, to simplify edge clutter while greatly reducing\nambiguities compared to previous bundling techniques. Edge-Path bundling takes\na layout as input and clusters each edge along a weighted, shortest path to\nlimit its deviation from a straight line. Edge-Path bundling does not incur\nindependent edge ambiguities typically seen in all edge bundling methods, and\nthe level of bundling can be tuned through shortest path distances, Euclidean\ndistances, and combinations of the two. Also, directed edge bundling naturally\nemerges from the model. Through metric evaluations, we demonstrate the\nadvantages of Edge-Path bundling over other techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.05467v1"
    },
    {
        "title": "Fast Evaluation of Smooth Distance Constraints on Co-Dimensional\n  Geometry",
        "authors": [
            "Abhishek Madan",
            "David I. W. Levin"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a new method for computing a smooth minimum distance function\nbased on the LogSumExp function for point clouds, edge meshes, triangle meshes,\nand combinations of all three. We derive blending weights and a modified\nBarnes-Hut acceleration approach that ensure our method approximates the true\ndistance, and is conservative (points outside the zero isosurface are\nguaranteed to be outside the surface) and efficient to evaluate for all the\nabove data types. This, in combination with its ability to smooth sparsely\nsampled and noisy data, like point clouds, shortens the gap between data\nacquisition and simulation, and thereby enables new applications such as\ndirect, co-dimensional rigid body simulation using unprocessed lidar data.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.10480v2"
    },
    {
        "title": "A functional skeleton transfer",
        "authors": [
            "Pietro Musoni",
            "Riccardo Marin",
            "Simone Melzi",
            "Umberto Castellani"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The animation community has spent significant effort trying to ease rigging\nprocedures. This is necessitated because the increasing availability of 3D data\nmakes manual rigging infeasible. However, object animations involve\nunderstanding elaborate geometry and dynamics, and such knowledge is hard to\ninfuse even with modern data-driven techniques. Automatic rigging methods do\nnot provide adequate control and cannot generalize in the presence of unseen\nartifacts. As an alternative, one can design a system for one shape and then\ntransfer it to other objects. In previous work, this has been implemented by\nsolving the dense point-to-point correspondence problem. Such an approach\nrequires a significant amount of supervision, often placing hundreds of\nlandmarks by hand. This paper proposes a functional approach for skeleton\ntransfer that uses limited information and does not require a complete match\nbetween the geometries. To do so, we suggest a novel representation for the\nskeleton properties, namely the functional regressor, which is compact and\ninvariant to different discretizations and poses. We consider our functional\nregressor a new operator to adopt in intrinsic geometry pipelines for encoding\nthe pose information, paving the way for several new applications. We\nnumerically stress our method on a large set of different shapes and object\nclasses, providing qualitative and numerical evaluations of precision and\ncomputational efficiency. Finally, we show a preliminar transfer of the\ncomplete rigging scheme, introducing a promising direction for future\nexplorations.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.12041v1"
    },
    {
        "title": "GeodesicEmbedding (GE): A High-Dimensional Embedding Approach for Fast\n  Geodesic Distance Queries",
        "authors": [
            "Qianwei Xia",
            "Juyong Zhang",
            "Zheng Fang",
            "Jin Li",
            "Mingyue Zhang",
            "Bailin Deng",
            "Ying He"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this paper, we develop a novel method for fast geodesic distance queries.\nThe key idea is to embed the mesh into a high-dimensional space, such that the\nEuclidean distance in the high-dimensional space can induce the geodesic\ndistance in the original manifold surface. However, directly solving the\nhigh-dimensional embedding problem is not feasible due to the large number of\nvariables and the fact that the embedding problem is highly nonlinear. We\novercome the challenges with two novel ideas. First, instead of taking all\nvertices as variables, we embed only the saddle vertices, which greatly reduces\nthe problem complexity. We then compute a local embedding for each non-saddle\nvertex. Second, to reduce the large approximation error resulting from the\npurely Euclidean embedding, we propose a cascaded optimization approach that\nrepeatedly introduces additional embedding coordinates with a non-Euclidean\nfunction to reduce the approximation residual. Using the precomputation data,\nour approach can determine the geodesic distance between any two vertices in\nnear-constant time. Computational testing results show that our method is more\ndesirable than previous geodesic distance queries methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.13821v2"
    },
    {
        "title": "Wanderlust: 3D Impressionism in Human Journeys",
        "authors": [
            "Guangyu Du",
            "Lei Dong",
            "Fabio Duarte",
            "Carlo Ratti"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The movements of individuals are fundamental to building and maintaining\nsocial connections. This pictorial presents Wanderlust, an experimental\nthree-dimensional data visualization on the universal visitation pattern\nrevealed from large-scale mobile phone tracking data. It explores ways of\nvisualizing recurrent flows and the attractive places they implied. Inspired by\nthe 19th-century art movement Impressionism, we develop a multi-layered effect,\nan impression, of mountains emerging from consolidated flows, to capture the\nessence of human journeys and urban spatial structure.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.00058v2"
    },
    {
        "title": "Volume Preserving Simulation of Soft Tissue with Skin",
        "authors": [
            "Seung Heon Sheen",
            "Egor Larionov",
            "Dinesh K. Pai"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Simulation of human soft tissues in contact with their environment is\nessential in many fields, including visual effects and apparel design.\nBiological tissues are nearly incompressible. However, standard methods employ\ncompressible elasticity models and achieve incompressibility indirectly by\nsetting Poisson's ratio to be close to 0.5. This approach can produce results\nthat are plausible qualitatively but inaccurate quantatively. This approach\nalso causes numerical instabilities and locking in coarse discretizations or\notherwise poses a prohibitive restriction on the size of the time step. We\npropose a novel approach to alleviate these issues by replacing indirect volume\npreservation using Poisson's ratios with direct enforcement of zonal volume\nconstraints, while controlling fine-scale volumetric deformation through a\ncell-wise compression penalty. To increase realism, we propose an epidermis\nmodel to mimic the dramatically higher surface stiffness on real skinned\nbodies. We demonstrate that our method produces stable realistic deformations\nwith precise volume preservation but without locking artifacts. Due to the\nvolume preservation not being tied to mesh discretization, our method also\nallows a resolution consistent simulation of incompressible materials. Our\nmethod improves the stability of the standard neo-Hookean model and the general\ncompression recovery in the Stable neo-Hookean model.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.01170v1"
    },
    {
        "title": "Rectangle-based Approximation for Rendering Glossy Interreflections",
        "authors": [
            "Chunbiao Guo"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This study introduces an approximation for rendering one bounce glossy\ninterreflection in real time. The solution is based on the most representative\npoint (MRP) and extends to a sampling disk near the MRP. Our algorithm\nrepresents geometry as rectangle proxies and specular reflections using a\nspherical Gaussian. The reflected radiance from the disk was efficiently\napproximated by selecting a representative attenuation axis in the sampling\ndisk. We provide an efficient approximation of the glossy interreflection and\ncan efficiently perform the approximation at runtime. Our method uses forward\nrendering (without using GBuffer), which is more suitable for platforms that\nfavor forward rendering, such as mobile applications and virtual reality.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.05805v1"
    },
    {
        "title": "Interactive All-Hex Meshing via Cuboid Decomposition",
        "authors": [
            "Lingxiao Li",
            "Paul Zhang",
            "Dmitriy Smirnov",
            "S. Mazdak Abulnaga",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Standard PolyCube-based hexahedral (hex) meshing methods aim to deform the\ninput domain into an axis-aligned PolyCube volume with integer corners; if this\ndeformation is bijective, then applying the inverse map to the voxelized\nPolyCube yields a valid hex mesh. A key challenge in these methods is to\nmaintain the bijectivity of the PolyCube deformation, thus reducing the\nrobustness of these algorithms. In this work, we present an interactive\npipeline for hex meshing that sidesteps this challenge by using a new\nrepresentation of PolyCubes as unions of cuboids. We begin by deforming the\ninput tetrahedral mesh into a near-PolyCube domain whose faces are close but\nnot perfectly aligned to the major axis directions. We then build a PolyCube by\noptimizing the layout of a set of cuboids with user guidance to closely fit the\ndeformed domain. Finally, we construct an inversion-free pullback map from the\nvoxelized PolyCube to the input domain while optimizing for mesh quality\nmetrics. We allow extensive user control over each stage, such as editing the\nvoxelized PolyCube, positioning surface vertices, and exploring the trade-off\namong competing quality metrics, while also providing automatic alternatives.\nWe validate our method on over one hundred shapes, including models that are\nchallenging for past PolyCube-based and frame-field-based methods. Our pipeline\nreliably produces hex meshes with quality on par with or better than\nstate-of-the-art. We additionally conduct a user study with 20 participants in\nwhich the majority prefer hex meshes they make using our tool to the ones from\nautomatic state-of-the-art methods. This demonstrates the need for intuitive\ninteractive hex meshing tools where the user can dictate the priorities of\ntheir mesh.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.06279v1"
    },
    {
        "title": "An Inverse Procedural Modeling Pipeline for SVBRDF Maps",
        "authors": [
            "Yiwei Hu",
            "Chengan He",
            "Valentin Deschaintre",
            "Julie Dorsey",
            "Holly Rushmeier"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Procedural modeling is now the de facto standard of material modeling in\nindustry. Procedural models can be edited and are easily extended, unlike\npixel-based representations of captured materials. In this paper, we present a\nsemi-automatic pipeline for general material proceduralization. Given\nSpatially-Varying Bidirectional Reflectance Distribution Functions (SVBRDFs)\nrepresented as sets of pixel maps, our pipeline decomposes them into a tree of\nsub-materials whose spatial distributions are encoded by their associated mask\nmaps. This semi-automatic decomposition of material maps progresses\nhierarchically, driven by our new spectrum-aware material matting and\ninstance-based decomposition methods. Each decomposed sub-material is\nproceduralized by a novel multi-layer noise model to capture local variations\nat different scales. Spatial distributions of these sub-materials are modeled\neither by a by-example inverse synthesis method recovering Point Process\nTexture Basis Functions (PPTBF) or via random sampling. To reconstruct\nprocedural material maps, we propose a differentiable rendering-based\noptimization that recomposes all generated procedures together to maximize the\nsimilarity between our procedural models and the input material pixel maps. We\nevaluate our pipeline on a variety of synthetic and real materials. We\ndemonstrate our method's capacity to process a wide range of material types,\neliminating the need for artist designed material graphs required in previous\nwork. As fully procedural models, our results expand to arbitrary resolution\nand enable high level user control of appearance.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.06395v2"
    },
    {
        "title": "Deep 3D Mesh Watermarking with Self-Adaptive Robustness",
        "authors": [
            "Feng Wang",
            "Hang Zhou",
            "Han Fang",
            "Xiaojuan Dong",
            "Weiming Zhang",
            "Xi Yang",
            "Nenghai Yu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Robust 3D mesh watermarking is a traditional research topic in computer\ngraphics, which provides an efficient solution to the copyright protection for\n3D meshes. Traditionally, researchers need manually design watermarking\nalgorithms to achieve sufficient robustness for the actual application\nscenarios. In this paper, we propose the first deep learning-based 3D mesh\nwatermarking framework, which can solve this problem once for all. In detail,\nwe propose an end-to-end network, consisting of a watermark embedding\nsub-network, a watermark extracting sub-network and attack layers. We adopt the\ntopology-agnostic graph convolutional network (GCN) as the basic convolution\noperation for 3D meshes, so our network is not limited by registered meshes\n(which share a fixed topology). For the specific application scenario, we can\nintegrate the corresponding attack layers to guarantee adaptive robustness\nagainst possible attacks. To ensure the visual quality of watermarked 3D\nmeshes, we design a curvature-based loss function to constrain the local\ngeometry smoothness of watermarked meshes. Experimental results show that the\nproposed method can achieve more universal robustness and faster watermark\nembedding than baseline methods while guaranteeing comparable visual quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.07202v1"
    },
    {
        "title": "Intuitive and Efficient Roof Modeling for Reconstruction and Synthesis",
        "authors": [
            "Jing Ren",
            "Biao Zhang",
            "Bojian Wu",
            "Jianqiang Huang",
            "Lubin Fan",
            "Maks Ovsjanikov",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We propose a novel and flexible roof modeling approach that can be used for\nconstructing planar 3D polygon roof meshes. Our method uses a graph structure\nto encode roof topology and enforces the roof validity by optimizing a simple\nbut effective planarity metric we propose. This approach is significantly more\nefficient than using general purpose 3D modeling tools such as 3ds Max or\nSketchUp, and more powerful and expressive than specialized tools such as the\nstraight skeleton. Our optimization-based formulation is also flexible and can\naccommodate different styles and user preferences for roof modeling. We\nshowcase two applications. The first application is an interactive roof editing\nframework that can be used for roof design or roof reconstruction from aerial\nimages. We highlight the efficiency and generality of our approach by\nconstructing a mesh-image paired dataset consisting of 2539 roofs. Our second\napplication is a generative model to synthesize new roof meshes from scratch.\nWe use our novel dataset to combine machine learning and our roof optimization\ntechniques, by using transformers and graph convolutional networks to model\nroof topology, and our roof optimization methods to enforce the planarity\nconstraint.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.07683v1"
    },
    {
        "title": "Egocentric Network Exploration for Immersive Analytics",
        "authors": [
            "Johannes Sorger",
            "Alessio Arleo",
            "Peter Kán",
            "Wolfgang Knecht",
            "Manuela Waldner"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  To exploit the potential of immersive network analytics for engaging and\neffective exploration, we promote the metaphor of \"egocentrism\", where data\ndepiction and interaction are adapted to the perspective of the user within a\n3D network. Egocentrism has the potential to overcome some of the inherent\ndownsides of virtual environments, e.g., visual clutter and cyber-sickness. To\ninvestigate the effect of this metaphor on immersive network exploration, we\ndesigned and evaluated interfaces of varying degrees of egocentrism. In a user\nstudy, we evaluated the effect of these interfaces on visual search tasks,\nefficiency of network traversal, spatial orientation, as well as\ncyber-sickness. Results show that a simple egocentric interface considerably\nimproves visual search efficiency and navigation performance, yet does not\ndecrease spatial orientation or increase cyber-sickness. An occlusion-free\nEgo-Bubble view of the neighborhood only marginally improves the user's\nperformance. We tie our findings together in an open online tool for egocentric\nnetwork exploration, providing actionable insights on the benefits of the\negocentric network exploration metaphor.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.09547v1"
    },
    {
        "title": "The Parallel Coordinates Plot Revisited: Visual Extensions from Hive\n  Plots, Heterogeneous Correlations, and an Exploration of Covid-19 Data in the\n  United States",
        "authors": [
            "Gary Koplik",
            "Ashlee Valente"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This paper extends an existing visualization, the Parallel Coordinates Plot\n(PCP), specifically its polar coordinate representation, the $\\textit{Polar\nParallel Coordinates Plot (P2CP)}$. With the additional incorporation of\ntechniques borrowed from Hive Plot network visualizations, we demonstrate\nimproved capabilities to explore multidimensional data in flatland, with a\nparticular emphasis on the unique ability to represent 3-dimensional data. To\ndemonstrate these techniques on P2CPs, we consider toy data, the Iris dataset,\nand socioeconomic data for counties in the United States. We conclude with an\nexploration of Covid-19 data from counties in the contiguous United States.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.10193v1"
    },
    {
        "title": "Common Artifacts in Volume Rendering",
        "authors": [
            "Daniel Ruijters"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Direct Volume Rendering is a popular and powerful visualization method for\nvoxel data and other volumetric scalar data sets. Particularly, in medical\napplications volume rendering is very commonly used, and has become one of the\nstate of the art methods for 3D visualization of medical data. In this article,\nsome of the most common artifacts encountered will be discussed, and their\npossible remedies.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.13704v2"
    },
    {
        "title": "Position-free Multiple-bounce Computations for Smith Microfacet BSDFs",
        "authors": [
            "Beibei Wang",
            "Wenhua Jin",
            "Jiahui Fan",
            "Jian Yang",
            "Nicolas Holzschuch",
            "Ling-Qi Yan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Bidirectional Scattering Distribution Functions (BSDFs) encode how a material\nreflects or transmits the incoming light. The most commonly used model is the\nMicrofacet BSDF. It computes material response from the micro-geometry of the\nsurface assuming a single bounce on specular microfacets. The original model\nignores multiple bounces on the micro-geometry, resulting in energy loss,\nespecially with large roughness. In this paper, we present a position-free\nformulation of multiple bounces inside the micro-geometry, which eliminates\nthis energy loss. We use an explicit mathematical definition of path space that\ndescribes single and multiple bounces in a uniform way. We then study the\nbehavior of light on the different vertices and segments in path space, leading\nto an accurate and reciprocal multiple-bounce description of BSDFs. We also\npresent practical, unbiased Monte-Carlo estimators to compute multiple\nscattering. Our method is less noisy than existing algorithms for computing\nmultiple scattering. It is almost noise-free with a very-low sampling rate,\nfrom 2 to 4 samples per pixel.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.14398v1"
    },
    {
        "title": "Convex polyhedral meshing for robust solid modeling",
        "authors": [
            "Lorenzo Diazzi",
            "Marco Attene"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We introduce a new technique to create a mesh of convex polyhedra\nrepresenting the interior volume of a triangulated input surface. Our approach\nis particularly tolerant to defects in the input, which is allowed to\nself-intersect, to be non-manifold, disconnected, and to contain surface holes\nand gaps. We guarantee that the input surface is exactly represented as the\nunion of polygonal facets of the output volume mesh. Thanks to our algorithm,\ntraditionally difficult solid modeling operations such as mesh booleans and\nMinkowski sums become surprisingly robust and easy to implement, even if the\ninput has defects. Our technique leverages on the recent concept of indirect\ngeometric predicate to provide an unprecedented combination of guaranteed\nrobustness and speed, thus enabling the practical implementation of robust\nthough flexible solid modeling systems. We have extensively tested our method\non all the 10000 models of the Thingi10k dataset, and concluded that no\nexisting method provides comparable robustness, precision and performances.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.14434v1"
    },
    {
        "title": "Constant-Cost Spatio-Angular Prefiltering of Glinty Appearance Using\n  Tensor Decomposition",
        "authors": [
            "Hong Deng",
            "Yang Liu",
            "Beibei Wang",
            "Jian Yang",
            "Lei Ma",
            "Nicolas Holzschuch",
            "Ling-Qi Yan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The detailed glinty appearance from complex surface microstructures enhances\nthe level of realism, but is both space- and time-consuming to render,\nespecially when viewed from far away (large spatial coverage) and/or\nilluminated by area lights (large angular coverage). In this paper, we\nformulate the glinty appearance rendering process as a spatio-angular range\nquery problem of the Normal Distribution Functions (NDFs), and introduce an\nefficient spatio-angular prefiltering solution to it. We start by exhaustively\nprecomputing all possible NDFs with differently sized positional coverages.\nThen we compress the precomputed data using tensor rank decomposition, which\nenables accurate and fast angular range queries. With our spatio-angular\nprefiltering scheme, we are able to solve both the storage and performance\nissues at the same time, leading to efficient rendering of glinty appearance\nwith both constant storage and constant performance, regardless of the range of\nspatio-angular queries. Finally, we demonstrate that our method easily applies\nto practical rendering applications that were traditionally considered\ndifficult. For example, efficient bidirectional reflection distribution\nfunction (BRDF) evaluation accurate NDF importance sampling, fast global\nillumination between glinty objects, high-frequency preserving rendering with\nenvironment lighting, and tile-based synthesis of glinty appearance.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.14807v1"
    },
    {
        "title": "Coverage Axis: Inner Point Selection for 3D Shape Skeletonization",
        "authors": [
            "Zhiyang Dou",
            "Cheng Lin",
            "Rui Xu",
            "Lei Yang",
            "Shiqing Xin",
            "Taku Komura",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this paper, we present a simple yet effective formulation called Coverage\nAxis for 3D shape skeletonization. Inspired by the set cover problem, our key\nidea is to cover all the surface points using as few inside medial balls as\npossible. This formulation inherently induces a compact and expressive\napproximation of the Medial Axis Transform (MAT) of a given shape. Different\nfrom previous methods that rely on local approximation error, our method allows\na global consideration of the overall shape structure, leading to an efficient\nhigh-level abstraction and superior robustness to noise. Another appealing\naspect of our method is its capability to handle more generalized input such as\npoint clouds and poor-quality meshes. Extensive comparisons and evaluations\ndemonstrate the remarkable effectiveness of our method for generating compact\nand expressive skeletal representation to approximate the MAT.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.00965v3"
    },
    {
        "title": "Differentiable 3D CAD Programs for Bidirectional Editing",
        "authors": [
            "Dan Cascaval",
            "Mira Shalah",
            "Phillip Quinn",
            "Rastislav Bodik",
            "Maneesh Agrawala",
            "Adriana Schulz"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Modern CAD tools represent 3D designs not only as geometry, but also as a\nprogram composed of geometric operations, each of which depends on a set of\nparameters. Program representations enable meaningful and controlled shape\nvariations via parameter changes. However, achieving desired modifications\nsolely through parameter editing is challenging when CAD models have not been\nexplicitly authored to expose select degrees of freedom in advance. We\nintroduce a novel bidirectional editing system for 3D CAD programs. In addition\nto editing the CAD program, users can directly manipulate 3D geometry and our\nsystem infers parameter updates to keep both representations in sync. We\nformulate inverse edits as a set of constrained optimization objectives,\nreturning plausible updates to program parameters that both match user intent\nand maintain program validity. Our approach implements an automatically\ndifferentiable domain-specific language for CAD programs, providing derivatives\nfor this optimization to be performed quickly on any expressed program. Our\nsystem enables rapid, interactive exploration of a constrained 3D design space\nby allowing users to manipulate the program and geometry interchangeably during\ndesign iteration. While our approach is not designed to optimize across changes\nin geometric topology, we show it is expressive and performant enough for users\nto produce a diverse set of design variants, even when the CAD program contains\na large number of parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.01182v1"
    },
    {
        "title": "A Survey on Deep Learning for Skeleton-Based Human Animation",
        "authors": [
            "L. Mourot",
            "L. Hoyet",
            "F. Le Clerc",
            "François Schnitzler",
            "Pierre Hellier"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Human character animation is often critical in entertainment content\nproduction, including video games, virtual reality or fiction films. To this\nend, deep neural networks drive most recent advances through deep learning and\ndeep reinforcement learning. In this article, we propose a comprehensive survey\non the state-of-the-art approaches based on either deep learning or deep\nreinforcement learning in skeleton-based human character animation. First, we\nintroduce motion data representations, most common human motion datasets and\nhow basic deep models can be enhanced to foster learning of spatial and\ntemporal patterns in motion data. Second, we cover state-of-the-art approaches\ndivided into three large families of applications in human animation pipelines:\nmotion synthesis, character control and motion editing. Finally, we discuss the\nlimitations of the current state-of-the-art methods based on deep learning\nand/or deep reinforcement learning in skeletal human character animation and\npossible directions of future research to alleviate current limitations and\nmeet animators' needs.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.06901v2"
    },
    {
        "title": "SpongeCake: A Layered Microflake Surface Appearance Model",
        "authors": [
            "Beibei Wang",
            "Wenhua Jin",
            "Miloš Hašan",
            "Ling-Qi Yan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this paper, we propose SpongeCake: a layered BSDF model where each layer\nis a volumetric scattering medium, defined using microflake or other phase\nfunctions. We omit any reflecting and refracting interfaces between the layers.\nThe first advantage of this formulation is that an exact and analytic solution\nfor single scattering, regardless of the number of volumetric layers, can be\nderived. We propose to approximate multiple scattering by an additional\nsingle-scattering lobe with modified parameters and a Lambertian lobe. We use a\nparameter mapping neural network to find the parameters of the newly added\nlobes to closely approximate the multiple scattering effect. Despite the\nabsence of layer interfaces, we demonstrate that many common material effects\ncan be achieved with layers of SGGX microflake and other volumes with\nappropriate parameters. A normal mapping effect can also be achieved through\nmapping of microflake orientations, which avoids artifacts common in standard\nnormal maps. Thanks to the analytical formulation, our model is very fast to\nevaluate and sample. Through various parameter settings, our model is able to\nhandle many types of materials, like plastics, wood, cloth, etc., opening a\nnumber of practical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.07145v2"
    },
    {
        "title": "Relighting Humans in the Wild: Monocular Full-Body Human Relighting with\n  Domain Adaptation",
        "authors": [
            "Daichi Tajima",
            "Yoshihiro Kanamori",
            "Yuki Endo"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The modern supervised approaches for human image relighting rely on training\ndata generated from 3D human models. However, such datasets are often small\n(e.g., Light Stage data with a small number of individuals) or limited to\ndiffuse materials (e.g., commercial 3D scanned human models). Thus, the human\nrelighting techniques suffer from the poor generalization capability and\nsynthetic-to-real domain gap. In this paper, we propose a two-stage method for\nsingle-image human relighting with domain adaptation. In the first stage, we\ntrain a neural network for diffuse-only relighting. In the second stage, we\ntrain another network for enhancing non-diffuse reflection by learning\nresiduals between real photos and images reconstructed by the diffuse-only\nnetwork. Thanks to the second stage, we can achieve higher generalization\ncapability against various cloth textures, while reducing the domain gap.\nFurthermore, to handle input videos, we integrate illumination-aware deep video\nprior to greatly reduce flickering artifacts even with challenging settings\nunder dynamic illuminations.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.07272v2"
    },
    {
        "title": "Real Time Cluster Path Tracing",
        "authors": [
            "Feng Xie",
            "Petro Mishchuk",
            "Warren Hunt"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Photorealistic rendering effects are common in films, but most real time\ngraphics today still rely on scan-line based multi-pass rendering to deliver\nrich visual experiences. While there have been prior works in distributed path\ntracing for static scene and objects under rigid motion, real time path tracing\nof deforming characters has to support per-frame dynamic BVH changes. We\npresent the architecture and implementation of the first real-time production\nquality cluster path tracing renderer that supports film quality animation and\ndeformation. We build our cluster path tracing system using the open source\nBlender and its GPU accelerated production quality renderer Cycles. Our\nsystem's rendering performance and quality scales linearly with the number of\nRTX cluster nodes used. It is able to generate and deliver path traced images\nwith global illumination effects to remote light-weight client systems at 15-30\nframes per second for a variety of Blender scenes including animated digital\nhuman characters with skin deformation and virtual objects.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.08913v2"
    },
    {
        "title": "From river flow to spatial flow: flow map via river flow directions\n  assignment algorithm",
        "authors": [
            "Zhiwei Wei",
            "Su Ding",
            "Yang Wang",
            "Yuanben Zhang",
            "Wenjia Xu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Flow maps are thematic maps that visualize object movements across space with\na tree layout, in which the underlying tree structure is similar to a natural\nriver system. In this paper, we present a novel and automated approach named\nRFDA-FM for flow maps from one origin to multiple destinations using a river\nextraction algorithm in digital elevation models (DEM). The RFDA-FM first\nmodels the mapping space as a flat surface by a DEM. A maze-solving algorithm\n(MSA) for river extraction is then adapted to calculate the flow path from one\ndestination to the origin by constraining its searching directions, direction\nweights, and searching ranges according to the quality criteria of flow maps.\nAll flow paths from the destinations to the origin are obtained iteratively\nbased on the MSA according to their importance, which is defined by considering\ntheir length. Finally, these paths are smoothly rendered with varying widths\naccording to their volume using B\\'ezier curves. A comparison with existing\napproaches indicates that the flow maps generated by RFDA-FM can be better at\nkeeping nodes away from edges without node overlaps and edge crosses. Two\nextension experiments demonstrate that RFDA-FM is applicable to heterogeneous\nmapping space or mapping space with obstacle areas. The parameter analysis\nshows that RFDA-FM can intuitively control the layouts of flow maps. Project\nwebsite: https://github.com/TrentonWei/FlowMap\n",
        "pdf_link": "http://arxiv.org/pdf/2110.09395v2"
    },
    {
        "title": "Learning a perceptual manifold with deep features for animation video\n  resequencing",
        "authors": [
            "Charles C. Morace",
            "Thi-Ngoc-Hanh Le",
            "Sheng-Yi Yao",
            "Shang-Wei Zhang",
            "Tong-Yee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We propose a novel deep learning framework for animation video resequencing.\nOur system produces new video sequences by minimizing a perceptual distance of\nimages from an existing animation video clip. To measure perceptual distance,\nwe utilize the activations of convolutional neural networks and learn a\nperceptual distance by training these features on a small network with data\ncomprised of human perceptual judgments. We show that with this perceptual\nmetric and graph-based manifold learning techniques, our framework can produce\nnew smooth and visually appealing animation video results for a variety of\nanimation video styles. In contrast to previous work on animation video\nresequencing, the proposed framework applies to wide range of image styles and\ndoes not require hand-crafted feature extraction, background subtraction, or\nfeature correspondence. In addition, we also show that our framework has\napplications to appealing arrange unordered collections of images.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.01455v1"
    },
    {
        "title": "An immersive Open Source environment using Godot",
        "authors": [
            "Francesca Santucci",
            "Federico Frenguelli",
            "Alessandro De Angelis",
            "Ilaria Cuccaro",
            "Damiano Perri",
            "Marco Simonetti"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a sample implementation of a Virtual and Augmented Reality\nimmersive environment based on Free and Libre Open Source Hardware and Software\nand the HTC Vive system, used to enhance the immersive experience of the user\nand to track her/his movements. The sense of immersion has increased and\nstimulated using a footplate and a Tibetan bridge, connected to the virtual\nworld as Augmented Reality applications and implemented through an Arduino\nboard, thereby adopting a low cost, open source hardware and software approach.\nThe proposed architecture is relatively affordable from the cost point of view,\neasy to implement, configure and adapt to different contexts. It can be of\ngreat help for organizing laboratory classes for young students to afford the\nimplementation of virtual worlds and Augmented Reality applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.01974v1"
    },
    {
        "title": "ActFloor-GAN: Activity-Guided Adversarial Networks for Human-Centric\n  Floorplan Design",
        "authors": [
            "Shidong Wang",
            "Wei Zeng",
            "Xi Chen",
            "Yu Ye",
            "Yu Qiao",
            "Chi-Wing Fu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a novel two-stage approach for automated floorplan design in\nresidential buildings with a given exterior wall boundary. Our approach has the\nunique advantage of being human-centric, that is, the generated floorplans can\nbe geometrically plausible, as well as topologically reasonable to enhance\nresident interaction with the environment. From the input boundary, we first\nsynthesize a human-activity map that reflects both the spatial configuration\nand human-environment interaction in an architectural space. We propose to\nproduce the human-activity map either automatically by a pre-trained generative\nadversarial network (GAN) model, or semi-automatically by synthesizing it with\nuser manipulation of the furniture. Second, we feed the human-activity map into\nour deep framework ActFloor-GAN to guide a pixel-wise prediction of room types.\nWe adopt a re-formulated cycle-consistency constraint in ActFloor-GAN to\nmaximize the overall prediction performance, so that we can produce\nhigh-quality room layouts that are readily convertible to vectorized\nfloorplans. Experimental results show several benefits of our approach. First,\na quantitative comparison with prior methods shows superior performance of\nleveraging the human-activity map in predicting piecewise room types. Second, a\nsubjective evaluation by architects shows that our results have compelling\nquality as professionally-designed floorplans and much better than those\ngenerated by existing methods in terms of the room layout topology. Last, our\napproach allows manipulating the furniture placement, considers the human\nactivities in the environment, and enables the incorporation of user-design\npreferences.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.03545v1"
    },
    {
        "title": "Content-aware media retargeting based on deep importance map",
        "authors": [
            "Thi-Ngoc-Hanh Le",
            "Shih-Syun Lin",
            "Weiming Dong",
            "Tong-Yee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a neural network to estimate the visual information of important\npixels in image and video, which is used in content-aware media retargeting\napplications. Existing techniques are successful in proposing retargeting\nmethods. Yet, the serious distortion and the shrunk problem in the retargeted\nresults still need to be investigated due to the limitations in the methods\nused to analyze visual attention. To accomplish this, we propose a network to\ndefine the importance map, which is sufficient to describe the energy of the\nsignificant regions in image/video. With this strategy, more ideal results are\nobtained from our system. Besides, the objective evaluation presented in this\npaper shows that our media retargeting system can achieve better and more\nplausible results than those of other works. In addition, our proposed\nimportance map performs well in the enlarge operator and on the\n\"difficult-to-resize\" images.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.04396v1"
    },
    {
        "title": "Breaking Good: Fracture Modes for Realtime Destruction",
        "authors": [
            "Silvia Sellán",
            "Jack Luong",
            "Leticia Mattos Da Silva",
            "Aravind Ramakrishnan",
            "Yuchuan Yang",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Drawing a direct analogy with the well-studied vibration or elastic modes, we\nintroduce an object's fracture modes, which constitute its preferred or most\nnatural ways of breaking. We formulate a sparsified eigenvalue problem, which\nwe solve iteratively to obtain the n lowest-energy modes. These can be\nprecomputed for a given shape to obtain a prefracture pattern that can\nsubstitute the state of the art for realtime applications at no runtime cost\nbut significantly greater realism. Furthermore, any realtime impact can be\nprojected onto our modes to obtain impact-dependent fracture patterns without\nthe need for any online crack propagation simulation. We not only introduce\nthis theoretically novel concept, but also show its fundamental and practical\nsuperiority in a diverse set of examples and contexts.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.05249v5"
    },
    {
        "title": "Path Verification for Dynamic Indirect Illumination",
        "authors": [
            "Pierre Moreau",
            "Michael Doggett",
            "Erik Sintorn"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this paper we present a technique that improves rendering performance for\nreal-time scenes with ray traced lighting in the presence of dynamic lights and\nobjects. In particular we verify photon paths from the previous frame against\ndynamic objects in the current frame, and show how most photon paths are still\nvalid. When using area lights, we use a data structure to store light\ndistribution that tracks light paths allowing photons to be reused when the\nlight source is moving in the scene. We also show that by reusing paths when\nthe error in the reflected energy is below a threshold value, even more paths\ncan be reused. We apply this technique to Indirect Illumination using a screen\nspace photon splatting rendering engine. By reusing photon paths and applying\nour error threshold, our method can reduce the number of rays traced by up to\n5x, and improve performance by up to 2x.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.06906v1"
    },
    {
        "title": "Distortion Reduction for Off-Center Perspective Projection of Panoramas",
        "authors": [
            "Chi-Han Peng",
            "Jiayao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  A single Panorama can be drawn perspectively without distortions in arbitrary\nviewing directions and field-of-views when the camera position is at the\norigin. This is a key advantage in VR and virtual tour applications because it\nenables the user to freely\"look around\" in a virtual world with just a single\npanorama, albeit at a fixed position. However, when the camera moves away from\nthe center, barrel distortions appear and realism breaks. We propose\nmodifications to the equirectangular-to-perspective(E2P) projection that\nsignificantly reduce distortions when the camera position is away from the\norigin. This enables users to not only \"look around\" but also \"walk around\"\nvirtually in a single panorama with more convincing renderings. We compare with\nother techniques that aim to augment panoramas with 3D information, including:\n1) panoramas with depth information and 2) panoramas augmented with room\nlayouts, and show that our approach provides more visually convincing results\n",
        "pdf_link": "http://arxiv.org/pdf/2111.12018v1"
    },
    {
        "title": "Interpolating Rotations with Non-abelian Kuramoto Model on the 3-Sphere",
        "authors": [
            "Zinaid Kapić",
            "Aladin Crnkić"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The paper presents a novel method for interpolating rotations based on the\nnon-Abelian Kuramoto model on sphere S3. The algorithm, introduced in this\npaper, finds the shortest and most direct path between two rotations. We have\ndiscovered that it gives approximately the same results as a Spherical Linear\nInterpolation algorithm. Simulation results of our algorithm are visualized on\nS2 using Hopf fibration. In addition, in order to gain a better insight, we\nhave provided one short video illustrating the rotation of an object between\ntwo positions.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.12549v1"
    },
    {
        "title": "Physics-based Mesh Deformation with Haptic Feedback and Material\n  Anisotropy",
        "authors": [
            "Avirup Mandal",
            "Parag Chaudhuri",
            "Subhasis Chaudhuri"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a physics-based framework to simulate porous, deformable materials\nand interactive tools with haptic feedback that can reshape it. In order to\nallow the material to be moulded non-homogeneously, we propose an algorithm to\nchange the material properties of the object depending on its water content. We\npresent a multi-resolution, multi-timescale simulation framework to enable\nstable visual and haptic feedback at interactive rates. We test our model for\nphysical consistency, accuracy, interactivity and appeal through a user study\nand quantitative performance evaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.04362v2"
    },
    {
        "title": "Stackless Ray-Object Intersections Using Approximate Minimum Weight\n  Triangulations: Results in 2D That Outperform Roped KD-Trees (And Massively\n  Outperform BVHs)",
        "authors": [
            "Roald Frederickx",
            "Philip Dutré"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The kd-tree and Bounding Volume Hierarchy (BVH) are well-known data\nstructures for computing ray-object intersections. Less known is the\nConstrained Convex Space Partitioning (CCSP), which partitions space and makes\nthe geometric primitives exactly overlap with the boundaries of its cells.\nConsequently, it is robust against ill-fitting cells that plague methods with\naxis-aligned cells (kd-tree, BVH) and it permits an efficient, stackless\ntraversal.\n  Within computer graphics, CCSPs have received some attention in both 2D and\n3D, but their construction methods were never directly aimed at minimizing\ntheir traversal cost -- even having fundamentally opposing goals for\nDelaunay-type methods. Instead, for an isotropic and translation-invariant ray\ndistribution the traversal cost is minimized by minimizing the weight: the\ntotal boundary size of all cells in the structure.\n  We study the 2D case using triangulations as CCSPs and minimize their total\nedge length using a simulated annealing process that allows for topological\nchanges and varying vertex count. Standard Delaunay-based triangulation\ntechniques show total edge lengths ranging from 10% higher to twice as high as\nour optimized triangulations for a variety of scenes, with a similar difference\nin traversal cost when using the triangulations for ray tracing. Compared to a\nroped kd-tree, our triangulations require less traversal steps for all scenes\nthat we tested and they are robust against the kd-tree's pathological behaviour\nwhen geometry becomes misaligned with the world axes. Moreover, the stackless\ntraversal strongly outperforms a BVH, which always requires a top-down descent\nin the hierarchy. In fact, we show several scenes where the number of traversal\noperations for our triangulations decreases(!) as the number of geometric\nprimitives $N$ increases, in contrast to the increasing $\\log N$ behaviour of a\nBVH.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.05570v1"
    },
    {
        "title": "The 3D Motorcycle Complex for Structured Volume Decomposition",
        "authors": [
            "Hendrik Brückler",
            "Ojaswi Gupta",
            "Manish Mandad",
            "Marcel Campen"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The so-called motorcycle graph has been employed in recent years for various\npurposes in the context of structured and aligned block decomposition of 2D\nshapes and 2-manifold surfaces. Applications are in the fields of surface\nparametrization, spline space construction, semi-structured quad mesh\ngeneration, or geometry data compression. We describe a generalization of this\nmotorcycle graph concept to the three-dimensional volumetric setting. Through\ncareful extensions aware of topological intricacies of this higher-dimensional\nsetting, we are able to guarantee important block decomposition properties also\nin this case. We describe algorithms for the construction of this 3D motorcycle\ncomplex on the basis of either hexahedral meshes or seamless volumetric\nparametrizations. Its utility is illustrated on examples in hexahedral mesh\ngeneration and volumetric T-spline construction.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.05793v1"
    },
    {
        "title": "GenMotion: Data-driven Motion Generators for Real-time Animation\n  Synthesis",
        "authors": [
            "Yizhou Zhao",
            "Wensi Ai",
            "Liang Qiu",
            "Pan Lu",
            "Feng Shi",
            "Tian Han",
            "Song-Chun Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  With the recent success of deep learning algorithms, many researchers have\nfocused on generative models for human motion animation. However, the research\ncommunity lacks a platform for training and benchmarking various algorithms,\nand the animation industry needs a toolkit for implementing advanced motion\nsynthesizing techniques. To facilitate the study of deep motion synthesis\nmethods for skeleton-based human animation and their potential applications in\npractical animation making, we introduce \\genmotion: a library that provides\nunified pipelines for data loading, model training, and animation sampling with\nvarious deep learning algorithms. Besides, by combining Python coding in the\nanimation software \\genmotion\\ can assist animators in creating real-time 3D\ncharacter animation. Source code is available at\nhttps://github.com/realvcla/GenMotion/.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.06060v1"
    },
    {
        "title": "Time of Impact Dataset for Continuous Collision Detection and a Scalable\n  Conservative Algorithm",
        "authors": [
            "David Belgrod",
            "Bolun Wang",
            "Zachary Ferguson",
            "Xin Zhao",
            "Marco Attene",
            "Daniele Panozzo",
            "Teseo Schneider"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We introduce a large-scale benchmark for broad- and narrow-phase continuous\ncollision detection (CCD) over linearized trajectories with exact time of\nimpacts and use it to evaluate the accuracy, correctness, and efficiency of 13\nstate-of-the-art CCD algorithms. Our analysis shows that several methods\nexhibit problems either in efficiency or accuracy.\n  To overcome these limitations, we introduce an algorithm for CCD designed to\nbe scalable on modern parallel architectures and provably correct when\nimplemented using floating point arithmetic. We integrate our algorithm within\nthe Incremental Potential Contact solver [Li et al . 2021] and evaluate its\nimpact on various simulation scenarios. Our approach includes a broad-phase CCD\nto quickly filter out primitives having disjoint bounding boxes and a\nnarrow-phase CCD that establishes whether the remaining primitive pairs indeed\ncollide. Our broad-phase algorithm is efficient and scalable thanks to the\nexperimental observation that sweeping along a coordinate axis performs\nsurprisingly well on modern parallel architectures. For narrow-phase CCD, we\nre-design the recently proposed interval-based algorithm of Wang et al. [2021]\nto work on massively parallel hardware.\n  To foster the adoption and development of future linear CCD algorithms, and\nto evaluate their correctness, scalability, and overall performance, we release\nthe dataset with analytic ground truth, the implementation of all the\nalgorithms tested, and our testing framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.06300v4"
    },
    {
        "title": "Scalar Spatiotemporal Blue Noise Masks",
        "authors": [
            "Alan Wolfe",
            "Nathan Morrical",
            "Tomas Akenine-Möller",
            "Ravi Ramamoorthi"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Blue noise error patterns are well suited to human perception, and when\napplied to stochastic rendering techniques, blue noise masks (blue noise\ntextures) minimize unwanted low-frequency noise in the final image. Current\nmethods of applying blue noise masks at each frame independently produce white\nnoise frequency spectra temporally. This white noise results in slower\nintegration convergence over time and unstable results when filtered\ntemporally. Unfortunately, achieving temporally stable blue noise distributions\nis non-trivial since 3D blue noise does not exhibit the desired 2D blue noise\nproperties, and alternative approaches degrade the spatial blue noise\nqualities.\n  We propose novel blue noise patterns that, when animated, produce values at a\npixel that are well distributed over time, converge rapidly for Monte Carlo\nintegration, and are more stable under TAA, while still retaining spatial blue\nnoise properties. To do so, we propose an extension to the well-known void and\ncluster algorithm that reformulates the underlying energy function to produce\nspatiotemporal blue noise masks.\n  These masks exhibit blue noise frequency spectra in both the spatial and\ntemporal domains, resulting in visually pleasing error patterns, rapid\nconvergence speeds, and increased stability when filtered temporally. We\ndemonstrate these improvements on a variety of applications, including\ndithering, stochastic transparency, ambient occlusion, and volumetric\nrendering.\n  By extending spatial blue noise to spatiotemporal blue noise, we overcome the\nconvergence limitations of prior blue noise works, enabling new applications\nfor blue noise distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.09629v1"
    },
    {
        "title": "Real-Time Path-Guiding Based on Parametric Mixture Models",
        "authors": [
            "Mikhail Derevyannykh"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Path-Guiding algorithms for sampling scattering directions can drastically\ndecrease the variance of Monte Carlo estimators of Light Transport Equation,\nbut their usage was limited to offline rendering because of memory and\ncomputational limitations. We introduce a new robust screen-space technique\nthat is based on online learning of parametric mixture models for guiding the\nreal-time path-tracing algorithm. It requires storing of 8 parameters for every\npixel, achieves a reduction of FLIP metric up to 4 times with 1 spp rendering.\nAlso, it consumes less than 1.5ms on RTX 2070 for 1080p and reduces\npath-tracing timings by generating more coherent rays by about 5% on average.\nMoreover, it leads to significant bias reduction and a lower level of\nflickering of SVGF output.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.09728v1"
    },
    {
        "title": "Field-Based Toolpath Generation for 3D Printing Continuous Fibre\n  Reinforced Thermoplastic Composites",
        "authors": [
            "Xiangjia Chen",
            "Guoxin Fang",
            "Wei-Hsin Liao",
            "Charlie C. L. Wang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a field-based method of toolpath generation for 3D printing\ncontinuous fibre reinforced thermoplastic composites. Our method employs the\nstrong anisotropic material property of continuous fibres by generating\ntoolpaths along the directions of tensile stresses in the critical regions.\nMoreover, the density of toolpath distribution is controlled in an adaptive way\nproportionally to the values of stresses. Specifically, a vector field is\ngenerated from the stress tensors under given loads and processed to have\nbetter compatibility between neighboring vectors. An optimal scalar field is\ncomputed later by making its gradients approximate the vector field. After\nthat, isocurves of the scalar field are extracted to generate the toolpaths for\ncontinuous fibre reinforcement, which are also integrated with the boundary\nconformal toolpaths in user selected regions. The performance of our method has\nbeen verified on a variety of models in different loading conditions.\nExperimental tests are conducted on specimens by 3D printing continuous carbon\nfibres (CCF) in a polylactic acid (PLA) matrix. Compared to reinforcement by\nload-independent toolpaths, the specimens fabricated by our method show up to\n71.4% improvement on the mechanical strength in physical tests when using the\nsame (or even slightly smaller) amount of continuous fibres.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.12057v1"
    },
    {
        "title": "Wavelet Transparency",
        "authors": [
            "Maksim Aizenshtein",
            "Niklas Smal",
            "Morgan McGuire"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Order-independent transparency schemes rely on low-order approximations of\ntransmittance as a function of depth. We introduce a new wavelet representation\nof this function and an algorithm for building and evaluating it efficiently on\na GPU. We then extend the order-independent Phenomenological Transparency\nalgorithm to our representation and introduce a new phenomenological\napproximation of chromatic aberration under refraction. This generates\ncomparable image quality to reference A-buffering for challenging cases such as\nsmoke coverage, more realistic refraction, and comparable or better performance\nand bandwidth to the state-of-the-art Moment transparency with a simpler\nimplementation.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.00094v1"
    },
    {
        "title": "ENI: Quantifying Environment Compatibility for Natural Walking in\n  Virtual Reality",
        "authors": [
            "Niall L. Williams",
            "Aniket Bera",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a novel metric to analyze the similarity between the physical\nenvironment and the virtual environment for natural walking in virtual reality.\nOur approach is general and can be applied to any pair of physical and virtual\nenvironments. We use geometric techniques based on conforming constrained\nDelaunay triangulations and visibility polygons to compute the Environment\nNavigation Incompatibility (ENI) metric that can be used to measure the\ncomplexity of performing simultaneous navigation. We demonstrate applications\nof ENI for highlighting regions of incompatibility for a pair of environments,\nguiding the design of the virtual environments to make them more compatible\nwith a fixed physical environment, and evaluating the performance of different\nredirected walking controllers. We validate the ENI metric using simulations\nand two user studies. Results of our simulations and user studies show that in\nthe environment pair that our metric identified as more navigable, users were\nable to walk for longer before colliding with objects in the physical\nenvironment. Overall, ENI is the first general metric that can automatically\nidentify regions of high and low compatibility in physical and virtual\nenvironments. Our project website is available at https://gamma.umd.edu/eni/.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.01261v3"
    },
    {
        "title": "Harmonics Virtual Lights : fast projection of luminance field on\n  spherical harmonics for efficient rendering",
        "authors": [
            "Pierre Mézières",
            "François Desrichard",
            "David Vanderhaeghe",
            "Mathias Paulin"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this paper, we introduce Harmonics Virtual Lights (HVL), to model indirect\nlight sources for interactive global illumination of dynamic 3D scenes. Virtual\nPoint Lights (VPL) are an efficient approach to define indirect light sources\nand to evaluate the resulting indirect lighting. Nonetheless, VPL suffer from\ndisturbing artifacts, especially with high frequency materials. Virtual\nSpherical Lights (VSL) avoid these artifacts by considering spheres instead of\npoints but estimates the lighting integral using Monte Carlo which results to\nnoise in the final image. We define HVL as an extension of VSL in a Spherical\nHarmonics (SH) framework, defining a closed form of the lighting integral\nevaluation. We propose an efficient SH projection of spherical lights\ncontribution faster than existing methods. Computing the outgoing luminance\nrequires $\\mathcal{O}(n)$ operations when using materials with circular\nsymmetric lobes, and $\\mathcal{O}(n^2)$ operations for the general case, where\n$n$ is the number of SH bands. HVL can be used with either parametric or\nmeasured BRDF without extra cost and offers control over rendering time and\nimage quality, by either decreasing or increasing the band limit used for SH\nprojection. Our approach is particularly well designed to render\nmedium-frequency one-bounce global illumination with arbitrary BRDF in\ninteractive time.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.01487v1"
    },
    {
        "title": "As-Continuous-As-Possible Extrusion Fabrication of Surface Models",
        "authors": [
            "Fanchao Zhong",
            "Yonglai Xu",
            "Haisen Zhao",
            "Lin Lu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose a novel computational framework for optimizing the toolpath\ncontinuity in fabricating surface models on an extrusion-based 3D printer.\nToolpath continuity has been a critical issue for extrusion-based fabrications\nthat affects both quality and efficiency. Transfer moves cause non-smoothor\nbumpy surfaces and get worse for materials with large inertia like clay. For\nsurface models, the effects of continuity are even more severe, in terms of\nsurface quality and model stability. In this paper, we introduce an original\ncriterion \"one-path-patch\" (OPP), for representing a shell surface patch that\ncan be traversed in one path considering fabrication constraints. We study the\nproperties of an OPP and the merging operations for OPPs, and propose a\nbottom-up OPP merging procedure for decomposing the given shell surface into a\nminimal number of OPPs and generating the \"as-continuous-as-possible\" (ACAP)\ntoolpath. Furthermore, we customize the path planning algorithm with a curved\nlayer printing scheme, which reduces the staircase defect and improves the\ntoolpath continuity via possibly connecting multiple segments. We evaluate the\nACAP algorithm for both ceramic and thermoplastic materials, and results\ndemonstrate that it improves the fabrication of surface models in both surface\nquality and efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.02374v3"
    },
    {
        "title": "JSOL: JavaScript Open-source Library for Grammar of Graphics",
        "authors": [
            "Waleed A. Yousef",
            "Hisham E. Mohammed",
            "Andrew A. Naguib",
            "Rafat S. Eid",
            "Sherif E. Emabrak",
            "Ahmed F. Hamed",
            "Yusuf M. Khalifa",
            "Shrouk T. AbdElrheem",
            "Eman A. Awad",
            "Sara G. Gaafar",
            "Alaa M. Mamdoh",
            "Nada A. Shawky"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this paper, we introduce the JavaScript Open-source Library (\\libname), a\nhigh-level grammar for representing data in visualization graphs and plots.\n\\libname~perspective on the grammar of graphics is unique; it provides\nstate-of-art rules for encoding visual primitives that can be used to generate\na known scene or to invent a new one. \\libname~has ton rules developed\nspecifically for data-munging, mapping, and visualization through many layers,\nsuch as algebra, scales, and geometries. Additionally, it has a compiler that\nincorporates and combines all rules specified by a user and put them in a flow\nto validate it as a visualization grammar and check its requisites. Users can\ncustomize scenes through a pipeline that either puts customized rules or comes\nwith new ones. We evaluated \\libname~on a multitude of plots to check rules\nspecification of customizing a specific plot. Although the project is still\nunder development and many enhancements are under construction, this paper\ndescribes the first developed version of \\libname, circa 2016, where an\nopen-source version of it is available. One immediate practical deployment for\nJSOl is to be integrated with the open-source version of the Data Visualization\nPlatform (DVP) \\citep{Yousef2019DVP-arxiv}\n",
        "pdf_link": "http://arxiv.org/pdf/2201.04205v1"
    },
    {
        "title": "A Robust Grid-Based Meshing Algorithm for Embedding Self-Intersecting\n  Surfaces",
        "authors": [
            "Steven W. Gagniere",
            "Yushan Han",
            "Yizhou Chen",
            "David A. B. Hyde",
            "Alan Marquez-Razon",
            "Joseph Teran",
            "Ronald Fedkiw"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The creation of a volumetric mesh representing the interior of an input\npolygonal mesh is a common requirement in graphics and computational mechanics\napplications. Most mesh creation techniques assume that the input surface is\nnot self-intersecting. However, due to numerical and/or user error, input\nsurfaces are commonly self-intersecting to some degree. The removal of\nself-intersection is a burdensome task that complicates workflow and generally\nslows down the process of creating simulation-ready digital assets. We present\na method for the creation of a volumetric embedding hexahedron mesh from a\nself-intersecting input triangle mesh. Our method is designed for efficiency by\nminimizing use of computationally expensive exact/adaptive precision\narithmetic. Although our approach allows for nearly no limit on the degree of\nself-intersection in the input surface, our focus is on efficiency in the most\ncommon case: many minimal self-intersections. The embedding hexahedron mesh is\ncreated from a uniform background grid and consists of hexahedron elements that\nare geometrical copies of grid cells. Multiple copies of a single grid cell are\nused to resolve regions of self-intersection/overlap. Lastly, we develop a\nnovel topology-aware embedding mesh coarsening technique to allow for\nuser-specified mesh resolution as well as a topology-aware tetrahedralization\nof the hexahedron mesh.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.06256v1"
    },
    {
        "title": "HPCGen: Hierarchical K-Means Clustering and Level Based Principal\n  Components for Scan Path Genaration",
        "authors": [
            "Wolfgang Fuhl"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this paper, we present a new approach for decomposing scan paths and its\nutility for generating new scan paths. For this purpose, we use the K-Means\nclustering procedure to the raw gaze data and subsequently iteratively to find\nmore clusters in the found clusters. The found clusters are grouped for each\nlevel in the hierarchy, and the most important principal components are\ncomputed from the data contained in them. Using this tree hierarchy and the\nprincipal components, new scan paths can be generated that match the human\nbehavior of the original data. We show that this generated data is very useful\nfor generating new data for scan path classification but can also be used to\ngenerate fake scan paths.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.08354v1"
    },
    {
        "title": "Neural Implicit Mapping via Nested Neighborhoods",
        "authors": [
            "Vinícius da Silva",
            "Tiago Novello",
            "Guilherme Schardong",
            "Luiz Schirmer",
            "Hélio Lopes",
            "Luiz Velho"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce a novel approach for rendering static and dynamic 3D neural\nsigned distance functions (SDF) in real-time. We rely on nested neighborhoods\nof zero-level sets of neural SDFs, and mappings between them. This framework\nsupports animations and achieves real-time performance without the use of\nspatial data-structures. It consists of three uncoupled algorithms representing\nthe rendering steps. The multiscale sphere tracing focuses on minimizing\niteration time by using coarse approximations on earlier iterations. The neural\nnormal mapping transfers details from a fine neural SDF to a surface nested on\na neighborhood of its zero-level set. It is smooth and it does not depend on\nsurface parametrizations. As a result, it can be used to fetch smooth normals\nfor discrete surfaces such as meshes and to skip later iterations when sphere\ntracing level sets. Finally, we propose an algorithm for analytic normal\ncalculation for MLPs and describe ways to obtain sequences of neural SDFs to\nuse with the algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.09147v2"
    },
    {
        "title": "Real-Time Computer-Generated EIA for Light Field Display by\n  Pre-Calculating and Pre-Storing the Invariable Voxel-Pixel Mapping",
        "authors": [
            "Quanzhen Wan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The elemental image array (EIA) for light field display, especially integral\nimaging light field display, was reliant on a virtual camera array, novel\nsampling algorithms, high-performance hardware or corresponding complex\nalgorithms, which hinder its application. Without sacrificing accuracy and\nprecision, we innovate a novel algorithm set to achieve video-level EIA\ngeneration. The invariable voxel to pixel relationship is pre-calculated and\npre-stored as a lookup table or mapping. Benefiting from the very lookup table,\nthe voxel array could be fast mapped to an EIA without contingent upon any\nhigh-end hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.09658v4"
    },
    {
        "title": "Affine Body Dynamics: Fast, Stable & Intersection-free Simulation of\n  Stiff Materials",
        "authors": [
            "Lei Lan",
            "Danny M. Kaufman",
            "Minchen Li",
            "Chenfanfu Jiang",
            "Yin Yang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Simulating stiff materials in applications where deformations are either not\nsignificant or can safely be ignored is a pivotal task across fields. Rigid\nbody modeling has thus long remained a fundamental tool and is, by far, the\nmost popular simulation strategy currently employed for modeling stiff solids.\nAt the same time, numerical models of a rigid body continue to pose a number of\nknown challenges and trade-offs including intersections, instabilities,\ninaccuracies, and/or slow performances that grow with contact-problem\ncomplexity. In this paper we revisit this problem and present ABD, a simple and\nhighly effective affine body dynamics framework, which significantly improves\nstate-of-the-art stiff simulations. We trace the challenges in the rigid-body\nIPC (incremental potential contact) method to the necessity of linearizing\npiecewise-rigid (SE(3)) trajectories and subsequent constraints. ABD instead\nrelaxes the unnecessary (and unrealistic) constraint that each body's motion be\nexactly rigid with a stiff orthogonality potential, while preserving the rigid\nbody model's key feature of a small coordinate representation. In doing so ABD\nreplaces piecewise linearization with piecewise linear trajectories. This, in\nturn, combines the best from both parties: compact coordinates ensure small,\nsparse system solves, while piecewise-linear trajectories enable efficient and\naccurate constraint (contact and joint) evaluations. Beginning with this simple\nfoundation, ABD preserves all guarantees of the underlying IPC model e.g.,\nsolution convergence, guaranteed non-intersection, and accurate frictional\ncontact. Over a wide range and scale of simulation problems we demonstrate that\nABD brings orders of magnitude performance gains (two- to three-order on the\nCPU and an order more utilizing the GPU, which is 10,000x speedups) over prior\nIPC-based methods with a similar or higher simulation quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.10022v2"
    },
    {
        "title": "An Attempt of Adaptive Heightfield Rendering with Complex Interpolants\n  Using Ray Casting",
        "authors": [
            "Daniel Cornel",
            "Zsolt Horváth",
            "Jürgen Waser"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this technical report, we document our attempt to visualize adaptive\nheightfields with smooth interpolation using ray casting in real time. The\nperformance of ray casting depends strongly on the used interpolant and its\nefficient evaluation. Unfortunately, analytical solutions for ray-surface\nintersections are only given in the literature for very few simple, piece-wise\npolynomial surfaces. In our use case, we approximate the heightfield with\nradial basis functions defined on an adaptive grid, for which we propose a\ntwo-step solution: First, we reconstruct and discretize the currently visible\nportion of the surface with smooth approximation into a set of off-screen\nbuffers. In a second step, we interpret these off-screen buffers as regular\nheightfields that can be rendered efficiently with ray casting using a simple\nbilinear interpolant. While our approach works, our quantitative evaluation\nshows that the performance depends strongly on the complexity and size of the\nheightfield. Real-time performance cannot be achieved for arbitrary\nheightfields, which is why we report our findings as a failed attempt to use\nray casting for practical geospatial visualization in real time.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.10887v1"
    },
    {
        "title": "Closed-Loop Control of Direct Ink Writing via Reinforcement Learning",
        "authors": [
            "Michal Piovarci",
            "Michael Foshey",
            "Jie Xu",
            "Timothy Erps",
            "Vahid Babaei",
            "Piotr Didyk",
            "Szymon Rusinkiewicz",
            "Wojciech Matusik",
            "Bernd Bickel"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Enabling additive manufacturing to employ a wide range of novel, functional\nmaterials can be a major boost to this technology. However, making such\nmaterials printable requires painstaking trial-and-error by an expert operator,\nas they typically tend to exhibit peculiar rheological or hysteresis\nproperties. Even in the case of successfully finding the process parameters,\nthere is no guarantee of print-to-print consistency due to material differences\nbetween batches. These challenges make closed-loop feedback an attractive\noption where the process parameters are adjusted on-the-fly. There are several\nchallenges for designing an efficient controller: the deposition parameters are\ncomplex and highly coupled, artifacts occur after long time horizons,\nsimulating the deposition is computationally costly, and learning on hardware\nis intractable. In this work, we demonstrate the feasibility of learning a\nclosed-loop control policy for additive manufacturing using reinforcement\nlearning. We show that approximate, but efficient, numerical simulation is\nsufficient as long as it allows learning the behavioral patterns of deposition\nthat translate to real-world experiences. In combination with reinforcement\nlearning, our model can be used to discover control policies that outperform\nbaseline controllers. Furthermore, the recovered policies have a minimal\nsim-to-real gap. We showcase this by applying our control policy in-vivo on a\nsingle-layer, direct ink writing printer.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.11819v2"
    },
    {
        "title": "Mixed Variational Finite Elements for Implicit, General-Purpose\n  Simulation of Deformables",
        "authors": [
            "Ty Trusty",
            "Danny M. Kaufman",
            "David I W Levin"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose and explore a new, general-purpose method for the implicit time\nintegration of elastica. Key to our approach is the use of a mixed variational\nprinciple. In turn its finite element discretization leads to an efficient\nalternating projections solver with a superset of the desirable properties of\nmany previous fast solution strategies. This framework fits a range of elastic\nconstitutive models and remains stable across a wide span of timestep sizes,\nmaterial parameters (including problems that are quasi-static and approximately\nrigid). It is efficient to evaluate and easily applicable to volume, surface,\nand rods models. We demonstrate the efficacy of our approach on a number of\nsimulated examples across all three codomains.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.00183v2"
    },
    {
        "title": "Dr.Jit: A Just-In-Time Compiler for Differentiable Rendering",
        "authors": [
            "Wenzel Jakob",
            "Sébastien Speierer",
            "Nicolas Roussel",
            "Delio Vicini"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Dr.Jit is a new just-in-time compiler for physically based rendering and its\nderivative. Dr.Jit expedites research on these topics in two ways: first, it\ntraces high-level simulation code (e.g., written in Python) and aggressively\nsimplifies and specializes the resulting program representation, producing\ndata-parallel kernels with state-of-the-art performance on CPUs and GPUs.\n  Second, it simplifies the development of differentiable rendering algorithms.\nEfficient methods in this area turn the derivative of a simulation into a\nsimulation of the derivative. Dr.Jit provides fine-grained control over the\nprocess of automatic differentiation to help with this transformation.\n  Specialization is particularly helpful in the context of differentiation,\nsince large parts of the simulation ultimately do not influence the computed\ngradients. Dr.Jit tracks data dependencies globally to find and remove\nredundant computation.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.01284v2"
    },
    {
        "title": "Neural Collision Detection for Deformable Objects",
        "authors": [
            "Ryan S. Zesch",
            "Bethany R. Witemeyer",
            "Ziyan Xiong",
            "David I. W. Levin",
            "Shinjiro Sueda"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose a neural network-based approach for collision detection with\ndeformable objects. Unlike previous approaches based on bounding volume\nhierarchies, our neural approach does not require an update of the spatial data\nstructure when the object deforms. Our network is trained on the reduced\ndegrees of freedom of the object, so that we can use the same network to query\nfor collisions even when the object deforms. Our approach is simple to use and\nimplement, and it can readily be employed on the GPU. We demonstrate our\napproach with two concrete examples: a haptics application with a finite\nelement mesh, and cloth simulation with a skinned character.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02309v1"
    },
    {
        "title": "Differentiable Simulation of Inertial Musculotendons",
        "authors": [
            "Ying Wang",
            "Jasper Verheul",
            "Sang-Hoon Yeo",
            "Nima Khademi Kalantari",
            "Shinjiro Sueda"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose a simple and practical approach for incorporating the effects of\nmuscle inertia, which has been ignored by previous musculoskeletal simulators\nin both graphics and biomechanics. We approximate the inertia of the muscle by\nassuming that muscle mass is distributed along the centerline of the muscle. We\nexpress the motion of the musculotendons in terms of the motion of the skeletal\njoints using a chain of Jacobians, so that at the top level, only the reduced\ndegrees of freedom of the skeleton are used to completely drive both bones and\nmusculotendons. Our approach can handle all commonly used musculotendon path\ntypes, including those with multiple path points and wrapping surfaces. For\nmuscle paths involving wrapping surfaces, we use neural networks to model the\nJacobians, trained using existing wrapping surface libraries, which allows us\nto effectively handle the Jacobian discontinuities that occur when\nmusculotendon paths collide with wrapping surfaces. We demonstrate support for\nhigher-order time integrators, complex joints, inverse dynamics, Hill-type\nmuscle models, and differentiability. In the limit, as the muscle mass is\nreduced to zero, our approach gracefully degrades to traditional simulators\nwithout support for muscle inertia. Finally, it is possible to mix and match\ninertial and non-inertial musculotendons, depending on the application.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02344v2"
    },
    {
        "title": "Condensation Jacobian with Adaptivity",
        "authors": [
            "Nicholas J. Weidner",
            "Theodore Kim",
            "Shinjiro Sueda"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a new approach that allows large time steps in dynamic\nsimulations. Our approach, ConJac, is based on condensation, a technique for\neliminating many degrees of freedom (DOFs) by expressing them in terms of the\nremaining degrees of freedom. In this work, we choose a subset of nodes to be\ndynamic nodes, and apply condensation at the velocity level by defining a\nlinear mapping from the velocities of these chosen dynamic DOFs to the\nvelocities of the remaining quasistatic DOFs. We then use this mapping to\nderive reduced equations of motion involving only the dynamic DOFs. We also\nderive a novel stabilization term that enables us to use complex nonlinear\nmaterial models. ConJac remains stable at large time steps, exhibits highly\ndynamic motion, and displays minimal numerical damping. In marked contrast to\nsubspace approaches, ConJac gives exactly the same configuration as the full\nspace approach once the static state is reached. Furthermore, ConJac can\nautomatically choose which parts of the object are to be simulated dynamically\nor quasistatically. Finally, ConJac works with a wide range of moderate to\nstiff materials, supports anisotropy and heterogeneity, handles topology\nchanges, and can be combined with existing solvers including rigid body\ndynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02390v1"
    },
    {
        "title": "Textured Mesh Quality Assessment: Large-Scale Dataset and Deep\n  Learning-based Quality Metric",
        "authors": [
            "Yana Nehmé",
            "Johanna Delanoy",
            "Florent Dupont",
            "Jean-Philippe Farrugia",
            "Patrick Le Callet",
            "Guillaume Lavoué"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Over the past decade, 3D graphics have become highly detailed to mimic the\nreal world, exploding their size and complexity. Certain applications and\ndevice constraints necessitate their simplification and/or lossy compression,\nwhich can degrade their visual quality. Thus, to ensure the best Quality of\nExperience (QoE), it is important to evaluate the visual quality to accurately\ndrive the compression and find the right compromise between visual quality and\ndata size. In this work, we focus on subjective and objective quality\nassessment of textured 3D meshes. We first establish a large-scale dataset,\nwhich includes 55 source models quantitatively characterized in terms of\ngeometric, color, and semantic complexity, and corrupted by combinations of 5\ntypes of compression-based distortions applied on the geometry, texture mapping\nand texture image of the meshes. This dataset contains over 343k distorted\nstimuli. We propose an approach to select a challenging subset of 3000 stimuli\nfor which we collected 148929 quality judgments from over 4500 participants in\na large-scale crowdsourced subjective experiment. Leveraging our subject-rated\ndataset, a learning-based quality metric for 3D graphics was proposed. Our\nmetric demonstrates state-of-the-art results on our dataset of textured meshes\nand on a dataset of distorted meshes with vertex colors. Finally, we present an\napplication of our metric and dataset to explore the influence of distortion\ninteractions and content characteristics on the perceived quality of compressed\ntextured meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02397v3"
    },
    {
        "title": "Alpha Blending with No Division Operations",
        "authors": [
            "Jerry R. Van Aken"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Highly accurate alpha blending can be performed entirely with integer\noperations, and no divisions. To reduce the number of integer multiplications,\nmultiple color components can be blended in parallel in the same 32-bit or\n64-bit register. This tutorial explains how to avoid division operations when\nalpha blending with 32-bit RGBA pixels. An RGBA pixel contains four 8-bit\ncomponents (red, green, blue, and alpha) whose values range from 0 to 255.\nAlpha blending requires multiplication of the color components by an alpha\nvalue, after which (for greatest accuracy) each of these products is divided by\n255 and then rounded to the nearest integer. This tutorial presents an\napproximate alpha-blending formula that replaces the division operation with an\ninteger shift and add -- and also enables the number of multiplications to be\nreduced. When the same blending calculation is carried out to high precision\nusing double-precision floating-point division operations, the results are\nfound to exactly match those produced by this approximation. C++ code examples\nare included.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02864v2"
    },
    {
        "title": "Real-time Monte Carlo Denoising with Weight Sharing Kernel Prediction\n  Network",
        "authors": [
            "Hangming Fan",
            "Rui Wang",
            "Yuchi Huo",
            "Hujun Bao"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Real-time Monte Carlo denoising aims at removing severe noise under low\nsamples per pixel (spp) in a strict time budget. Recently, kernel-prediction\nmethods use a neural network to predict each pixel's filtering kernel and have\nshown a great potential to remove Monte Carlo noise. However, the heavy\ncomputation overhead blocks these methods from real-time applications. This\npaper expands the kernel-prediction method and proposes a novel approach to\ndenoise very low spp (e.g., 1-spp) Monte Carlo path traced images at real-time\nframe rates. Instead of using the neural network to directly predict the kernel\nmap, i.e., the complete weights of each per-pixel filtering kernel, we predict\nan encoding of the kernel map, followed by a high-efficiency decoder with\nunfolding operations for a high-quality reconstruction of the filtering\nkernels. The kernel map encoding yields a compact single-channel representation\nof the kernel map, which can significantly reduce the kernel-prediction\nnetwork's throughput. In addition, we adopt a scalable kernel fusion module to\nimprove denoising quality. The proposed approach preserves kernel prediction\nmethods' denoising quality while roughly halving its denoising time for 1-spp\nnoisy inputs. In addition, compared with the recent neural bilateral grid-based\nreal-time denoiser, our approach benefits from the high parallelism of\nkernel-based reconstruction and produces better denoising results at equal\ntime.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05977v2"
    },
    {
        "title": "Application of Color Block Code in Image Scaling",
        "authors": [
            "Hao Wang",
            "Yu Bai",
            "Jie Liu",
            "Guangmin Sun",
            "Yanjun Zhang",
            "Jie Li"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Aiming at the high cost of embedding annotation watermark in a narrow small\narea and the information distortion caused by the change of annotation\nwatermark image resolution, this paper proposes a color block code technology,\nwhich uses location information and color code to form recognizable graphics,\nwhich can not only simplify the annotation graphics, but also ensure the\nrecognition efficiency. First, the constituent elements of color block code are\ndesigned, and then the coding and decoding method of color block code is\nproposed. Experiments show that color block code has high anti-scaling and\nanti-interference, and can be widely used in the labeling of small object\nsurface and low resolution image.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.06252v1"
    },
    {
        "title": "Computational Pattern Making from 3D Garment Models",
        "authors": [
            "Nico Pietroni",
            "Corentin Dumery",
            "Raphael Guenot-Falque",
            "Mark Liu",
            "Teresa Vidal-Calleja",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose a method for computing a sewing pattern of a given 3D garment\nmodel. Our algorithm segments an input 3D garment shape into patches and\ncomputes their 2D parameterization, resulting in pattern pieces that can be cut\nout of fabric and sewn together to manufacture the garment. Unlike the general\nstate-of-the-art approaches for surface cutting and flattening, our method\nexplicitly targets garment fabrication. It accounts for the unique properties\nand constraints of tailoring, such as seam symmetry, the usage of darts, fabric\ngrain alignment, and a flattening distortion measure that models woven fabric\ndeformation, respecting its anisotropic behavior. We bootstrap a recent patch\nlayout approach developed for quadrilateral remeshing and adapt it to the\npurpose of computational pattern making, ensuring that the deformation of each\npattern piece stays within prescribed bounds of cloth stress. While our\nalgorithm can automatically produce the sewing patterns, it is fast enough to\nadmit user input to creatively iterate on the pattern design. Our method can\ntake several target poses of the 3D garment into account and integrate them\ninto the sewing pattern design. We demonstrate results on both skintight and\nloose garments, showcasing the versatile application possibilities of our\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.10272v1"
    },
    {
        "title": "Point Containment Queries on Ray Tracing Cores for AMR Flow\n  Visualization",
        "authors": [
            "Stefan Zellmann",
            "Daniel Seifried",
            "Nate Morrical",
            "Ingo Wald",
            "Will Usher",
            "Jamie A. P. Law-Smith",
            "Stefanie Walch-Gassner",
            "André Hinkenjann"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Modern GPUs come with dedicated hardware to perform ray/triangle\nintersections and bounding volume hierarchy (BVH) traversal. While the primary\nuse case for this hardware is photorealistic 3D computer graphics, with careful\nalgorithm design scientists can also use this special-purpose hardware to\naccelerate general-purpose computations such as point containment queries. This\narticle explains the principles behind these techniques and their application\nto vector field visualization of large simulation data using particle tracing.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12020v1"
    },
    {
        "title": "Sparse Sampling and Completion for Light Transport in VPL-based\n  Rendering",
        "authors": [
            "Yuchi Huo",
            "Rui Wang",
            "Xinguo Liu",
            "Hujun Bao"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The many-light formulation provides a general framework for rendering various\nillumination effects using hundreds of thousands of virtual point lights\n(VPLs). To efficiently gather the contributions of the VPLs, lightcuts and its\nextensions cluster the VPLs, which implicitly approximates the lighting matrix\nwith some representative blocks similar to vector quantization. In this paper,\nwe propose a new approximation method based on the previous lightcut method and\na low-rank matrix factorization model. As many researchers pointed out, the\nlighting matrix is low rank, which implies that it can be completed from a\nsmall set of known entries.\n  We first generate a conservative global light cut with bounded error and\npartition the lighting matrix into slices by the coordinate and normal of the\nsurface points using the method of lightslice. Then we perform two passes of\nrandomly sampling on each matrix slice. In the first pass, uniformly\ndistributed random entries are sampled to coarsen the global light cut, further\nclustering the similar light for the spatially localized surface points of the\nslices. In the second pass, more entries are sampled according to the\npossibility distribution function estimated from the first sampling result.\nThen each matrix slice is factorized into a product of two smaller low-rank\nmatrices constrained by the sampled entries, which delivers a completion of the\nlighting matrix. The factorized form provides an additional speedup for adding\nup the matrix columns which is more GPU friendly. Compared with the previous\nlightcut based methods, we approximate the lighting matrix with some signal\nspecialized bases via factorization. The experimental results shows that we can\nachieve significant acceleration than the state of the art many-light methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12567v2"
    },
    {
        "title": "Hex-Mesh Generation and Processing: a Survey",
        "authors": [
            "Nico Pietroni",
            "Marcel Campen",
            "Alla Sheffer",
            "Gianmarco Cherchi",
            "David Bommes",
            "Xifeng Gao",
            "Riccardo Scateni",
            "Franck Ledoux",
            "Jean-Francois Remacle",
            "Marco Livesu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this article, we provide a detailed survey of techniques for hexahedral\nmesh generation. We cover the whole spectrum of alternative approaches to mesh\ngeneration, as well as post processing algorithms for connectivity editing and\nmesh optimization. For each technique, we highlight capabilities and\nlimitations, also pointing out the associated unsolved challenges. Recent\nrelaxed approaches, aiming to generate not pure-hex but hex-dominant meshes,\nare also discussed. The required background, pertaining to geometrical as well\nas combinatorial aspects, is introduced along the way.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12670v1"
    },
    {
        "title": "Weighted Simultaneous Algebra Reconstruction Technique (wSART) for\n  Additive Light Field Synthesis",
        "authors": [
            "Chen Gao",
            "Linqi Dong",
            "Liang Xu",
            "Xu Liu",
            "Haifeng Li"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We apply an iterative weighting scheme for additive light field synthesis.\nUnlike previous work optimizing additive light field evenly over viewpoints, we\nconstrain the optimization to deliver a reconstructed light field of high image\nquality for viewpoints of large weight.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.02141v1"
    },
    {
        "title": "Morphological Anti-Aliasing Method for Boundary Slope Prediction",
        "authors": [
            "Yuchen Zhong",
            "Yuchi Huo",
            "Rui Wang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Image pixel aliasing caused by insufficient sampling is a long-standing\nproblem in the field of computer graphics. It has always been the goal of\nresearchers to seek anti-aliasing algorithms with high speed and good effect.\nDue to the deficiencies in local detection and reconstruction of sloping line\nboundaries, a morphological anti-aliasing method for boundary slope prediction\nis proposed. This method uses the information of the local line boundary slope\nto predict and test the end positions of the line boundary in the global scope,\nthereby reconstructing The boundary information more consistent with the actual\nboundary is obtained, and a more accurate linear boundary shape is obtained\nwith only a small increase in the amount of calculation. Compared with the\nprevious morphological anti-aliasing algorithm, the proposed method is based on\nthe global morphological boundary. , can reconstruct the straight line boundary\nmore accurately, and apply it to the anti-aliasing calculation, which can\nfurther improve the color transition of the straight line boundary, make the\ninclined straight line boundary have higher continuity, and obtain a better\nanti-aliasing effect.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.03870v1"
    },
    {
        "title": "Dynamic Grass Color Scale Display Technique Based on Grass Length for\n  Green Landscape-Friendly Animation Display",
        "authors": [
            "Kojiro Tanaka",
            "Yuichi Kato",
            "Akito Mizuno",
            "Masahiko Mikawa",
            "Makoto Fujisawa"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Recently, public displays such as liquid crystal displays (LCDs) are often\nused in urban green spaces, however, the display devices can spoil green\nlandscape of urban green spaces because they look like artificial materials. We\npreviously proposed a green landscape-friendly grass animation display method\nby controlling a pixel-by-pixel grass color dynamically. The grass color can be\nchanged by moving a green grass length in yellow grass, and the grass animation\ndisplay can play simple animations using grayscale images. In the previous\nresearch, the color scale was mapped to the green grass length subjectively,\nhowever, this method has not achieved displaying the grass colors corresponding\nto the color scale based on objective evaluations. Here, we introduce a dynamic\ngrass color scale display technique based on a grass length. In this paper, we\ndeveloped a grass color scale setting procedure to map the grass length to the\ncolor scale with five levels through image processing. Through the outdoor\nexperiment of the grass color scale setting procedure, the color scale can\ncorrespond to the green grass length based on a viewpoint. After the\nexperiments, we demonstrated a grass animation display to show the animations\nwith the color scale using the experiment results.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08496v2"
    },
    {
        "title": "Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion\n  Tracking from Sparse Inertial Sensors",
        "authors": [
            "Xinyu Yi",
            "Yuxiao Zhou",
            "Marc Habermann",
            "Soshi Shimada",
            "Vladislav Golyanik",
            "Christian Theobalt",
            "Feng Xu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Motion capture from sparse inertial sensors has shown great potential\ncompared to image-based approaches since occlusions do not lead to a reduced\ntracking quality and the recording space is not restricted to be within the\nviewing frustum of the camera. However, capturing the motion and global\nposition only from a sparse set of inertial sensors is inherently ambiguous and\nchallenging. In consequence, recent state-of-the-art methods can barely handle\nvery long period motions, and unrealistic artifacts are common due to the\nunawareness of physical constraints. To this end, we present the first method\nwhich combines a neural kinematics estimator and a physics-aware motion\noptimizer to track body motions with only 6 inertial sensors. The kinematics\nmodule first regresses the motion status as a reference, and then the physics\nmodule refines the motion to satisfy the physical constraints. Experiments\ndemonstrate a clear improvement over the state of the art in terms of capture\naccuracy, temporal stability, and physical correctness.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08528v2"
    },
    {
        "title": "Variational Hierarchical Directed Bounding Box Construction for Solid\n  Mesh Models",
        "authors": [
            "Rui Wang",
            "Wei Hua",
            "Gaofeng Xu",
            "Yuchi Huo",
            "Hujun Bao"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Object oriented bounding box tree (OBB-Tree for short) has many applications\nin collision detection, real-time rendering, etc. It has a wide range of\napplications. The construction of the hierarchical directed bounding box of the\nsolid mesh model is studied, and a new optimization solution method is\nproposed. But this part of the external space volume that does not belong to\nthe solid mesh model is used as the error, and an error calculation method\nbased on hardware acceleration is given. Secondly, the hierarchical bounding\nbox construction problem is transformed into a variational approximation\nproblem, and the optimal hierarchical directed bounding box is obtained by\nsolving the global error minimum. In the optimization calculation, we propose\nthat combining Lloyd clustering iteration in the same layer and MultiGrid-like\nreciprocating iteration between layers. Compared with previous results, this\nmethod can generate aired original solid mesh models are more tightly packed\nwith hierarchical directed bounding box approximation. In the practical\napplication of collision detection, the results constructed using this method\ncan reduce the computational time of collision detection and improve detection\nefficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.10521v1"
    },
    {
        "title": "Less Is More: Efficient Networked VR Transformation Handling Using\n  Geometric Algebra",
        "authors": [
            "Manos Kamarianakis",
            "Ilias Chrysovergis",
            "Nick Lydatakis",
            "Mike Kentros",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  As shared, collaborative, networked, virtual environments become increasingly\npopular, various challenges arise regarding the efficient transmission of model\nand scene transformation data over the network. As user immersion and real-time\ninteractions heavily depend on VR stream synchronization, transmitting the\nentire data sat does not seem a suitable approach, especially for sessions\ninvolving a large number of users. Session recording is another\nmomentum-gaining feature of VR applications that also faces the same challenge.\nThe selection of a suitable data format can reduce the occupied volume, while\nit may also allow effective replication of the VR session and optimized\npost-processing for analytics and deep-learning algorithms. In this work, we\npropose two algorithms that can be applied in the context of a networked\nmultiplayer VR session, to efficiently transmit the displacement and\norientation data from the users' hand-based VR HMDs. Moreover, we present a\nnovel method describing effective VR recording of the data exchanged in such a\nsession. Our algorithms, based on the use of dual-quaternions and multivectors,\nimpact the network consumption rate and are highly effective in scenarios\ninvolving multiple users. By sending less data over the network and\ninterpolating the in-between frames locally, we manage to obtain better visual\nresults than current state-of-the-art methods. Lastly, we prove that, for\nrecording purposes, storing less data and interpolating them on-demand yields a\ndata set quantitatively close to the original one.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.10988v2"
    },
    {
        "title": "A Hybrid Lagrangian-Eulerian Model for the Structural Analysis of\n  Multifield Datasets",
        "authors": [
            "Zi'ang Ding",
            "Xavier Tricoche"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Multifields datasets are common in a large number of research and engineering\napplications of computational science. The effective visualization of the\ncorresponding datasets can facilitate their analysis by elucidating the complex\nand dynamic interactions that exist between the attributes that describe the\nphysics of the phenomenon. We present in this paper a new hybrid\nLagrangian-Eulerian model that extends existing Lagrangian visualization\ntechniques to the analysis of multifields problems. In particular, our approach\nfactors in the entire data space to reveal the structure of multifield\ndatasets, thereby combining both Eulerian and Lagrangian perspectives. We\nevaluate our technique in the context of several fluid dynamics applications.\nOur results indicate that our proposed approach is able to characterize\nimportant structural features that are missed by existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.11398v1"
    },
    {
        "title": "A Virtual Point Light Generation Method in Close-Range Area",
        "authors": [
            "Shihao Jin",
            "Rui Wang",
            "Wenting Zheng",
            "Wei Hua",
            "Yuchi Huo"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This paper proposes a new hybrid algorithm for sampling virtual point light\n(VPL). The indirect lighting calculation of the scene is used to distribute the\nVPL reasonably. In the process of generating VPL, we divide the scene into two\nparts according to the camera position and orientation. The close-range part:\nthe part that the camera pays attention to. The distant-range part: the part\nthat the camera does not pay attention to or rarely pays attention to. For the\nclose-range part, we use a patch-based vPL sampling method to distribute the\nVPL as evenly as possible on the patch in the near-field area; for the\ndistant-range part, we use sparse instant radiosity (IR) for sampling. It turns\nout that, in contrast to conventional multiple instant radiance Compared with\nthe VPL generation algorithm, the method proposed in this paper can greatly\nimprove the quality of the final result graph when the number of VPLs is the\nsame; Under the same rendering quality, the rendering speed can be greatly\nimproved.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.11484v1"
    },
    {
        "title": "Rendering Layered Materials with Diffuse Interfaces",
        "authors": [
            "Héloïse de Dinechin",
            "Laurent Belcour"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this work, we introduce a novel method to render, in real-time, Lambertian\nsurfaces with a rough dieletric coating. We show that the appearance of such\nconfigurations is faithfully represented with two microfacet lobes accounting\nfor direct and indirect interactions respectively. We numerically fit these\nlobes based on the first order directional statistics (energy, mean and\nvariance) of light transport using 5D tables and narrow them down to 2D + 1D\nwith analytical forms and dimension reduction. We demonstrate the quality of\nour method by efficiently rendering rough plastics and ceramics, closely\nmatching ground truth. In addition, we improve a state-of-the-art layered\nmaterial model to include Lambertian interfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.11835v1"
    },
    {
        "title": "Bringing Linearly Transformed Cosines to Anisotropic GGX",
        "authors": [
            "Aakash KT",
            "Eric Heitz",
            "Jonathan Dupuy",
            "P. J. Narayanan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Linearly Transformed Cosines (LTCs) are a family of distributions that are\nused for real-time area-light shading thanks to their analytic integration\nproperties. Modern game engines use an LTC approximation of the ubiquitous GGX\nmodel, but currently this approximation only exists for isotropic GGX and thus\nanisotropic GGX is not supported. While the higher dimensionality presents a\nchallenge in itself, we show that several additional problems arise when\nfitting, post-processing, storing, and interpolating LTCs in the anisotropic\ncase. Each of these operations must be done carefully to avoid rendering\nartifacts. We find robust solutions for each operation by introducing and\nexploiting invariance properties of LTCs. As a result, we obtain a small $8^4$\nlook-up table that provides a plausible and artifact-free LTC approximation to\nanisotropic GGX and brings it to real-time area-light shading.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.11904v2"
    },
    {
        "title": "Real-time Rendering and Editing of Scattering Effects for Translucent\n  Objects",
        "authors": [
            "Rui Wang",
            "Wei Hua",
            "Yuchi Huo",
            "Hujun Bao"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The photorealistic rendering of the transparent effect of translucent objects\nis a hot research topic in recent years. A real-time photorealistic rendering\nand material dynamic editing method for the diffuse scattering effect of\ntranslucent objects is proposed based on the bidirectional surface scattering\nreflectance function's (BSSRDF) Dipole approximation. The diffuse scattering\nmaterial function in the Dipo le approximation is decomposed into the product\nform of the shape-related function and the translucent material-related\nfunction through principal component analysis; using this decomposition\nrepresentation, under the real-time photorealistic rendering framework of\npre-radiative transmission and the scattering transmission to realize real-time\nediting of translucent object materials under various light sources. In\naddition, a method for quadratic wavelet compression of precomputed radiative\ntransfer data in the spatial domain is also proposed. Using the correlation of\nsurface points in the spatial distribution position, on the premise of ensuring\nthe rendering quality, the data is greatly compressed and the rendering is\nefficiently improved. The experimental results show that the method in this\npaper can generate a highly realistic translucent effect and ensure the\nreal-time rendering speed.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12339v1"
    },
    {
        "title": "PRTT: Precomputed Radiance Transfer Textures",
        "authors": [
            "Sirikonda Dhawal",
            "Aakash KT",
            "P. J. Narayanan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Precomputed Radiance Transfer (PRT) can achieve high quality renders of\nglossy materials at real-time framerates. PRT involves precomputing a\nk-dimensional transfer vector of Spherical Harmonic (SH) coefficients at\nspecific points for a scene. Most prior art precomputes transfer at vertices of\nthe mesh and interpolates color for interior points. They require finer mesh\ntessellations for high quality renderings. In this paper, we explore and\npresent the use of textures for storing transfer. Using transfer textures\ndecouples mesh resolution from transfer storage and sampling which is useful\nespecially for glossy renders. We further demonstrate glossy inter-reflections\nby precomputing additional textures. We thoroughly discuss practical aspects of\ntransfer textures and analyze their performance in real-time rendering\napplications. We show equivalent or higher render quality and FPS and\ndemonstrate results on several challenging scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12399v1"
    },
    {
        "title": "Pupil-aware Holography",
        "authors": [
            "Praneeth Chakravarthula",
            "Seung-Hwan Baek",
            "Florian Schiffers",
            "Ethan Tseng",
            "Grace Kuo",
            "Andrew Maimone",
            "Nathan Matsuda",
            "Oliver Cossairt",
            "Douglas Lanman",
            "Felix Heide"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Holographic displays promise to deliver unprecedented display capabilities in\naugmented reality applications, featuring a wide field of view, wide color\ngamut, spatial resolution, and depth cues all in a compact form factor. While\nemerging holographic display approaches have been successful in achieving large\netendue and high image quality as seen by a camera, the large etendue also\nreveals a problem that makes existing displays impractical: the sampling of the\nholographic field by the eye pupil. Existing methods have not investigated this\nissue due to the lack of displays with large enough etendue, and, as such, they\nsuffer from severe artifacts with varying eye pupil size and location.\n  We show that the holographic field as sampled by the eye pupil is highly\nvarying for existing display setups, and we propose pupil-aware holography that\nmaximizes the perceptual image quality irrespective of the size, location, and\norientation of the eye pupil in a near-eye holographic display. We validate the\nproposed approach both in simulations and on a prototype holographic display\nand show that our method eliminates severe artifacts and significantly\noutperforms existing approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.14939v2"
    },
    {
        "title": "Forward and backward mapping of image to 2D vector field using fiber\n  bundle color space",
        "authors": [
            "A. A. Snarskii",
            "I. V. Bezsudnov"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce the concept of a fiber bundle color space, which acts according\nto the psychophysiological rules of trichromacy perception of colors by a\nhuman. The image resides in the fiber bundle base space and the fiber color\nspace contains color vectors. Further we propose the decomposition of color\nvectors into spectral and achromatic parts. A homomorphism of a color image and\nconstructed two-dimensional vector field is demonstrated that allows us to\napply well-known advanced methods of vector analysis to a color image, i.e.\nultimately give new numerical characteristics of the image. Appropriate image\nto vector field forward mapping is constructed. The proposed backward mapping\nalgorithm converts a two-dimensional vector field to color image. The type of\nimage filter is described using sequential forward and backward mapping\nalgorithms. An example of the color image formation on the base of\ntwo-dimensional magnetic vector field scattered by a typical pipe line defect\nis given.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.01065v1"
    },
    {
        "title": "Software Rasterization of 2 Billion Points in Real Time",
        "authors": [
            "Markus Schütz",
            "Bernhard Kerbl",
            "Michael Wimmer"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose a software rasterization pipeline for point clouds that is capable\nof brute-force rendering up to two billion points in real time (60fps).\nImprovements over the state of the art are achieved by batching points in a way\nthat a number of batch-level optimizations can be computed before rasterizing\nthe points within the same rendering pass. These optimizations include frustum\nculling, level-of-detail rendering, and choosing the appropriate coordinate\nprecision for a given batch of points directly within a compute workgroup.\nAdaptive coordinate precision, in conjunction with visibility buffers, reduces\nthe number of loaded bytes for the majority of points down to 4, thus making\nour approach several times faster than the bandwidth-limited state of the art.\nFurthermore, support for LOD rendering makes our software-rasterization\napproach suitable for rendering arbitrarily large point clouds, and to meet the\nincreased performance demands of virtual reality rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.01287v1"
    },
    {
        "title": "Sensitive vPSA -- Exploring Sensitivity in Visual Parameter Space\n  Analysis",
        "authors": [
            "Bernhard Fröhler",
            "Tim Elberfeld",
            "Torsten Möller",
            "Hans-Christian Hege",
            "Julia Maurer",
            "Christoph Heinzl"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The sensitivity of parameters in computational science problems is difficult\nto assess, especially for algorithms with multiple input parameters and diverse\noutputs. This work seeks to explore sensitivity analysis in the visualization\ndomain, introducing novel techniques for respective visual analyses of\nparameter sensitivity in multi-dimensional algorithms. First, the sensitivity\nanalysis background is revisited, highlighting the definition of sensitivity\nanalysis and approaches analyzing global and local sensitivity as well as the\ndifferences of sensitivity analysis to the more common uncertainty analysis. We\nintroduce and explore parameter sensitivity using visualization techniques from\noverviews to details on demand, covering the analysis of all aspects of\nsensitivity in a prototypical implementation. The respective visualization\ntechniques outline the algorithmic in- and outputs including indications, on\nhow sensitive an input is with regard to the outputs. The detailed sensitivity\ninformation is communicated through constellation plots for the exploration of\ninput and output spaces. A matrix view is discussed for localized information\non the sensitivity of specific outputs to specific inputs. A 3D view provides\nthe link of the parameter sensitivity to the spatial domain, in which the\nresults of the multi-dimensional algorithms are embedded. The proposed\nsensitivity analysis techniques are implemented and evaluated in a prototype\ncalled Sensitivity Explorer. We show that Sensitivity Explorer reliably\nidentifies the most influential parameters and provides insights into which of\nthe output characteristics these affect as well as to which extent.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.01823v1"
    },
    {
        "title": "Rule-based Procedural Tree Modeling Approach",
        "authors": [
            "Yinhui Yang",
            "Rui Wang",
            "Yuchi Huo"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In some entertainment and virtual reality applications, it is necessary to\nmodel and draw the real world realistically, so as to improve the fidelity of\nnatural scenes and make users have a better sense of immersion. However, due to\nthe morphological structure of trees The complexity and variety present many\nchallenges for photorealistic modeling and rendering of trees. This paper\nreviews the progress achieved in photorealistic modeling and rendering of tree\nbranches, leaves, and bark over the past few decades. The main achievement is\nmainly a rule-based procedural tree modeling method.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.03237v1"
    },
    {
        "title": "Nanomatrix: Scalable Construction of Crowded Biological Environments",
        "authors": [
            "Ruwayda Alharbi",
            "Ondřej Strnad",
            "Tobias Klein",
            "Ivan Viola"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a novel method for the interactive construction and rendering of\nextremely large molecular scenes, capable of representing multiple biological\ncells in atomistic detail. Our method is tailored for scenes, which are\nprocedurally constructed, based on a given set of building rules. Rendering of\nlarge scenes normally requires the entire scene available in-core, or\nalternatively, it requires out-of-core management to load data into the memory\nhierarchy as a part of the rendering loop. Instead of out-of-core memory\nmanagement, we propose to procedurally generate the scene on-demand on the fly.\nThe key idea is a positional- and view-dependent procedural scene-construction\nstrategy, where only a fraction of the atomistic scene around the camera is\navailable in the GPU memory at any given time. The atomistic detail is\npopulated into a uniform-space partitioning using a grid that covers the entire\nscene. Most of the grid cells are not filled with geometry, only those are\npopulated that are potentially seen by the camera. The atomistic detail is\npopulated in a compute shader and its representation is connected with\nacceleration data structures for hardware ray-tracing of modern GPUs. Objects\nwhich are far away, where atomistic detail is not perceivable from a given\nviewpoint, are represented by a triangle mesh mapped with a seamless texture,\ngenerated from the rendering of geometry from atomistic detail. The algorithm\nconsists of two pipelines, the construction-compute pipeline, and the rendering\npipeline, which work together to render molecular scenes at an atomistic\nresolution far beyond the limit of the GPU memory containing trillions of\natoms. We demonstrate our technique on multiple models of SARS-CoV-2 and the\nred blood cell.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05762v3"
    },
    {
        "title": "Plots.jl -- a user extendable plotting API for the julia programming\n  language",
        "authors": [
            "Simon Christ",
            "Daniel Schwabeneder",
            "Christopher Rackauckas",
            "Michael Krabbe Borregaard",
            "Thomas Breloff"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  There are plenty of excellent plotting libraries. Each excels at a different\nuse case: one is good for printed 2D publication figures, the other at\ninteractive 3D graphics, a third has excellent L A TEX integration or is good\nfor creating dashboards on the web. The aim of Plots.jl is to enable the user\nto use the same syntax to interact with many different plotting libraries, such\nthat it is possible to change the library \"backend\" without needing to touch\nthe code that creates the content -- and without having to learn yet another\napplication programming interface (API). This is achieved by the separation of\nthe plot specification from the implementation of the actual graphical backend.\nThese plot specifications may be extended by a \"recipe\" system, which allows\npackage authors and users to define how to plot any new type (be it a\nstatistical model, a map, a phylogenetic tree or the solution to a system of\ndifferential equations) and create new types of plots -- without depending on\nthe Plots.jl package. This supports a modular ecosystem structure for plotting\nand yields a high reuse potential across the entire julia package ecosystem.\nPlots.jl is publicly available at https://github.com/JuliaPlots/Plots.jl.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.08775v3"
    },
    {
        "title": "Metappearance: Meta-Learning for Visual Appearance Reproduction",
        "authors": [
            "Michael Fischer",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  There currently exist two main approaches to reproducing visual appearance\nusing Machine Learning (ML): The first is training models that generalize over\ndifferent instances of a problem, e.g., different images of a dataset. As\none-shot approaches, these offer fast inference, but often fall short in\nquality. The second approach does not train models that generalize across\ntasks, but rather over-fit a single instance of a problem, e.g., a flash image\nof a material. These methods offer high quality, but take long to train. We\nsuggest to combine both techniques end-to-end using meta-learning: We over-fit\nonto a single problem instance in an inner loop, while also learning how to do\nso efficiently in an outer-loop across many exemplars. To this end, we derive\nthe required formalism that allows applying meta-learning to a wide range of\nvisual appearance reproduction problems: textures, BRDFs, svBRDFs, illumination\nor the entire light transport of a scene. The effects of meta-learning\nparameters on several different aspects of visual appearance are analyzed in\nour framework, and specific guidance for different tasks is provided.\nMetappearance enables visual quality that is similar to over-fit approaches in\nonly a fraction of their runtime while keeping the adaptivity of general\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.08993v2"
    },
    {
        "title": "SILVR: A Synthetic Immersive Large-Volume Plenoptic Dataset",
        "authors": [
            "Martijn Courteaux",
            "Julie Artois",
            "Stijn De Pauw",
            "Peter Lambert",
            "Glenn Van Wallendael"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In six-degrees-of-freedom light-field (LF) experiences, the viewer's freedom\nis limited by the extent to which the plenoptic function was sampled. Existing\nLF datasets represent only small portions of the plenoptic function, such that\nthey either cover a small volume, or they have limited field of view.\nTherefore, we propose a new LF image dataset \"SILVR\" that allows for\nsix-degrees-of-freedom navigation in much larger volumes while maintaining full\npanoramic field of view. We rendered three different virtual scenes in various\nconfigurations, where the number of views ranges from 642 to 2226. One of these\nscenes (called Zen Garden) is a novel scene, and is made publicly available. We\nchose to position the virtual cameras closely together in large cuboid and\nspherical organisations ($2.2m^3$ to $48m^3$), equipped with 180{\\deg} fish-eye\nlenses. Every view is rendered to a color image and depth map of 2048px\n$\\times$ 2048px. Additionally, we present the software used to automate the\nmulti-view rendering process, as well as a lens-reprojection tool that converts\nbetween images with panoramic or fish-eye projection to a standard rectilinear\n(i.e., perspective) projection. Finally, we demonstrate how the proposed\ndataset and software can be used to evaluate LF coding/rendering techniques(in\nthis case for training NeRFs with instant-ngp). As such, we provide the first\npublicly-available LF dataset for large volumes of light with full panoramic\nfield of view\n",
        "pdf_link": "http://arxiv.org/pdf/2204.09523v1"
    },
    {
        "title": "TopoEmbedding, a web tool for the interactive analysis of persistent\n  homology",
        "authors": [
            "Xueyi Bao",
            "Guoxi Liu",
            "Federico Iuricich"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Software libraries for Topological Data Analysis (TDA) offer limited support\nfor interactive visualization. Most libraries only allow to visualize\ntopological descriptors (e.g., persistence diagrams), and lose the connection\nwith the original domain of data. This makes it challenging for users to\ninterpret the results of a TDA pipeline in an exploratory context. In this\npaper, we present TopoEmbedding, a web-based tool that simplifies the\ninteractive visualization and analysis of persistence-based descriptors.\nTopoEmbedding allows non-experts in TDA to explore similarities and differences\nfound by TDA descriptors with simple yet effective visualization techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.09783v1"
    },
    {
        "title": "Computational Design of Kinesthetic Garments",
        "authors": [
            "Velko Vechev",
            "Juan Zarate",
            "Bernhard Thomaszewski",
            "Otmar Hilliges"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Kinesthetic garments provide physical feedback on body posture and motion\nthrough tailored distributions of reinforced material. Their ability to\nselectively stiffen a garment's response to specific motions makes them\nappealing for rehabilitation, sports, robotics, and many other application\nfields. However, finding designs that distribute a given amount of\nreinforcement material to maximally stiffen the response to specified motions\nis a challenging problem. In this work, we propose an optimization-driven\napproach for automated design of reinforcement patterns for kinesthetic\ngarments. Our main contribution is to cast this design task as an on-body\ntopology optimization problem. Our method allows designers to explore a\ncontinuous range of designs corresponding to various amounts of reinforcement\ncoverage. Our model captures both tight contact and lift-off separation between\ncloth and body. We demonstrate our method on a variety of reinforcement design\nproblems for different body sites and motions. Optimal designs lead to a two-\nto threefold improvement in performance in terms of energy density. A set of\nmanufactured designs were consistently rated as providing more resistance than\nbaselines in a comparative user study\n",
        "pdf_link": "http://arxiv.org/pdf/2204.09996v1"
    },
    {
        "title": "Nested Papercrafts for Anatomical and Biological Edutainment",
        "authors": [
            "Marwin Schindler",
            "Thorsten Korpitsch",
            "Renata G. Raidou",
            "Hsiang-Yun Wu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this paper, we present a new workflow for the computer-aided generation of\nphysicalizations, addressing nested configurations in anatomical and biological\nstructures. Physicalizations are an important component of anatomical and\nbiological education and edutainment. However, existing approaches have mainly\nrevolved around creating data sculptures through digital fabrication. Only a\nfew recent works proposed computer-aided pipelines for generating sculptures,\nsuch as papercrafts, with affordable and readily available materials.\nPapercraft generation remains a challenging topic by itself. Yet, anatomical\nand biological applications pose additional challenges, such as reconstruction\ncomplexity and insufficiency to account for multiple, nested structures--often\npresent in anatomical and biological structures. Our workflow comprises the\nfollowing steps: (i) define the nested configuration of the model and detect\nits levels, (ii) calculate the viewpoint that provides optimal, unobstructed\nviews on inner levels, (iii) perform cuts on the outer levels to reveal the\ninner ones based on the viewpoint selection, (iv) estimate the stability of the\ncut papercraft to ensure a reliable outcome, (v) generate textures at each\nlevel, as a smart visibility mechanism that provides additional information on\nthe inner structures, and (vi) unfold each textured mesh guaranteeing\nreconstruction. Our novel approach exploits the interactivity of nested\npapercraft models for edutainment purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.10901v1"
    },
    {
        "title": "GAMORRA: An API-Level Workload Model for Rasterization-based Graphics\n  Pipeline Architecture",
        "authors": [
            "Iman Soltani Mohammadi",
            "Mohammad Ghanbari",
            "Mahmoud Reza Hashemi"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The performance of applications that require frame rendering time estimation\nor dynamic frequency scaling, rely on the accuracy of the workload model that\nis utilized within these applications. Existing models lack sufficient accuracy\nin their core model. Hence, they require changes to the target application or\nthe hardware to produce accurate results. This paper introduces a mathematical\nworkload model for a rasterization-based graphics Application Programming\nInterface (API) pipeline, named GAMORRA, which works based on the load and\ncomplexity of each stage of the pipeline. Firstly, GAMORRA models each stage of\nthe pipeline based on their operation complexity and the input data size. Then,\nthe calculated workloads of the stages are fed to a Multiple Linear Regression\n(MLR) model as explanatory variables. A hybrid offline/online training scheme\nis proposed as well to train the model. A suite of benchmarks is also designed\nto tune the model parameters based on the performance of the target system. The\nexperiments were performed on Direct3D 11 and on two different rendering\nplatforms comparing GAMORRA to an AutoRegressive (AR) model, a Frame Complexity\nModel (FCM) and a frequency-based (FRQ) model. The experiments show an average\nof 1.27 ms frame rendering time estimation error (9.45%) compared to an average\nof 1.87 ms error (13.23%) for FCM which is the best method among the three\nchosen methods. However, this comes at the cost of 0.54 ms (4.58%) increase in\ntime complexity compared to FCM. Furthermore, GAMMORA improves frametime\nunderestimations by 1.1% compared to FCM.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.11025v2"
    },
    {
        "title": "MFA-DVR: Direct Volume Rendering of MFA Models",
        "authors": [
            "Jianxin Sun",
            "David Lenz",
            "Hongfeng Yu",
            "Tom Peterka"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  3D volume rendering is widely used to reveal insightful intrinsic patterns of\nvolumetric datasets across many domains. However, the complex structures and\nvarying scales of volumetric data can make efficiently generating high-quality\nvolume rendering results a challenging task. Multivariate functional\napproximation (MFA) is a new data model that addresses some of the critical\nchallenges: high-order evaluation of both value and derivative anywhere in the\nspatial domain, compact representation for large-scale volumetric data, and\nuniform representation of both structured and unstructured data. In this paper,\nwe present MFA-DVR, the first direct volume rendering pipeline utilizing the\nMFA model, for both structured and unstructured volumetric datasets. We\ndemonstrate improved rendering quality using MFA-DVR on both synthetic and real\ndatasets through a comparative study. We show that MFA-DVR not only generates\nmore faithful volume rendering than using local filters but also performs\nfaster on high-order interpolations on structured and unstructured datasets.\nMFA-DVR is implemented in the existing volume rendering pipeline of the\nVisualization Toolkit (VTK) to be accessible by the scientific visualization\ncommunity.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.11762v2"
    },
    {
        "title": "Towards Quantum Ray Tracing",
        "authors": [
            "Luís Paulo Santos",
            "Thomas Bashford-Rogers",
            "João Barbosa",
            "Paul Navrátil"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Rendering on conventional computers is capable of generating realistic\nimagery, but the computational complexity of these light transport algorithms\nis a limiting factor of image synthesis. Quantum computers have the potential\nto significantly improve rendering performance through reducing the underlying\ncomplexity of the algorithms behind light transport. This paper investigates\nhybrid quantum-classical algorithms for ray tracing, a core component of most\nrendering techniques. Through a practical implementation of quantum ray tracing\nin a 3D environment, we show quantum approaches provide a quadratic improvement\nin query complexity compared to the equivalent classical approach. Based on\ndomain specific knowledge, we then propose algorithms to significantly reduce\nthe computation required for quantum ray tracing through exploiting image space\ncoherence and a principled termination criteria for quantum searching. We show\nresults for both Whitted style ray tracing, and for accelerating ray tracing\noperations when performing classical Monte Carlo integration for area lights\nand indirect illumination.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12797v1"
    },
    {
        "title": "Perceptual Visibility Model for Temporal Contrast Changes in Periphery",
        "authors": [
            "Cara Tursun",
            "Piotr Didyk"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Modeling perception is critical for many applications and developments in\ncomputer graphics to optimize and evaluate content generation techniques. Most\nof the work to date has focused on central (foveal) vision. However, this is\ninsufficient for novel wide-field-of-view display devices, such as virtual and\naugmented reality headsets. Furthermore, the perceptual models proposed for the\nfovea do not readily extend to the off-center, peripheral visual field, where\nhuman perception is drastically different. In this paper, we focus on modeling\nthe temporal aspect of visual perception in the periphery. We present new\npsychophysical experiments that measure the sensitivity of human observers to\ndifferent spatio-temporal stimuli across a wide field of view. We use the\ncollected data to build a perceptual model for the visibility of temporal\nchanges at different eccentricities in complex video content. Finally, we\ndiscuss, demonstrate, and evaluate several problems that can be addressed using\nour technique. First, we show how our model enables injecting new content into\nthe periphery without distracting the viewer, and we discuss the link between\nthe model and human attention. Second, we demonstrate how foveated rendering\nmethods can be evaluated and optimized to limit the visibility of temporal\naliasing.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00108v2"
    },
    {
        "title": "A Position-Free Path Integral for Homogeneous Slabs and Multiple\n  Scattering on Smith Microfacets",
        "authors": [
            "Benedikt Bitterli",
            "Eugene d'Eon"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We consider the problem of multiple scattering on Smith microfacets. This\nproblem is equivalent to computing volumetric light transport in a homogeneous\nslab. Although the symmetry of the slab allows for significant simplification,\nfully analytic solutions are scarce and not general enough for most\napplications. Standard Monte Carlo simulation, although general, is expensive\nand leads to variance that must be dealt with.\n  We present the first unbiased, truly position-free path integral for\nevaluating the BSDF of a homogeneous slab. We collapse the spatially-1D path\nintegral of previous works to a position-free form using an analytical\npreintegration of collision distances. Evaluation of the resulting path\nintegral, which now contains only directions, reduces to simple recursive\nmanipulation of exponential distributions. Applying Monte Carlo to solve the\nreduced integration problem leads to lower variance.\n  Our new algorithm allows us to render multiple scattering on Smith\nmicrofacets with less variance than prior work, and, in the case of conductors,\nto do so without any bias. Additionally, our algorithm can also be used to\naccelerate the rendering of BSDFs containing volumetrically scattering layers,\nat reduced variance compared to standard Monte Carlo integration.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00587v1"
    },
    {
        "title": "Evocube: a Genetic Labeling Framework for Polycube-Maps",
        "authors": [
            "Corentin Dumery",
            "François Protais",
            "Sébastien Mestrallet",
            "Christophe Bourcier",
            "Franck Ledoux"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Polycube-maps are used as base-complexes in various fields of computational\ngeometry, including the generation of regular all-hexahedral meshes free of\ninternal singularities. However, the strict alignment constraints behind\npolycube-based methods make their computation challenging for CAD models used\nin numerical simulation via Finite Element Method (FEM). We propose a novel\napproach based on an evolutionary algorithm to robustly compute polycube-maps\nin this context. We address the labeling problem, which aims to precompute\npolycube alignment by assigning one of the base axes to each boundary face on\nthe input. Previous research has described ways to initialize and improve a\nlabeling via greedy local fixes. However, such algorithms lack robustness and\noften converge to inaccurate solutions for complex geometries. Our proposed\nframework alleviates this issue by embedding labeling operations in an\nevolutionary heuristic, defining fitness, crossover, and mutations in the\ncontext of labeling optimization. We evaluate our method on a thousand smooth\nand CAD meshes, showing Evocube converges to valid labelings on a wide range of\nshapes. The limitations of our method are also discussed thoroughly.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00738v3"
    },
    {
        "title": "Realistic soft-body tearing under 10ms in VR",
        "authors": [
            "Manos Kamarianakis",
            "Antonis Protopsaltis",
            "Michail Tamiolakis",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a novel integration of a real-time continuous tearing algorithm\nfor 3D meshes in VR, suitable for devices of low CPU/GPU specifications, along\nwith a suitable particle decomposition that allows soft-body deformations on\nboth the original and the torn model.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00914v1"
    },
    {
        "title": "Recording and replaying psychomotor user actions in VR",
        "authors": [
            "Manos Kamarianakis",
            "Ilias Chrysovergis",
            "Mike Kentros",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce a novel method that describes the functionality and\ncharacteristics of an efficient VR recorder with replay capabilities,\nimplemented in a modern game engine, publicly available for free.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00923v1"
    },
    {
        "title": "Physics-Based Inverse Rendering using Combined Implicit and Explicit\n  Geometries",
        "authors": [
            "Guangyan Cai",
            "Kai Yan",
            "Zhao Dong",
            "Ioannis Gkioulekas",
            "Shuang Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Mathematically representing the shape of an object is a key ingredient for\nsolving inverse rendering problems. Explicit representations like meshes are\nefficient to render in a differentiable fashion but have difficulties handling\ntopology changes. Implicit representations like signed-distance functions, on\nthe other hand, offer better support of topology changes but are much more\ndifficult to use for physics-based differentiable rendering. We introduce a new\nphysics-based inverse rendering pipeline that uses both implicit and explicit\nrepresentations. Our technique enjoys the benefit of both representations by\nsupporting both topology changes and differentiable rendering of complex\neffects such as environmental illumination, soft shadows, and interreflection.\nWe demonstrate the effectiveness of our technique using several synthetic and\nreal examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.01242v2"
    },
    {
        "title": "Encoding of direct 4D printing of isotropic single-material system for\n  double-curvature and multimodal morphing",
        "authors": [
            "Bihui Zou",
            "Chao Song",
            "Zipeng He",
            "Jaehyung Ju"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The ability to morph flat sheets into complex 3D shapes is extremely useful\nfor fast manufacturing and saving materials while also allowing volumetrically\nefficient storage and shipment and a functional use. Direct 4D printing is a\ncompelling method to morph complex 3D shapes out of as-printed 2D plates.\nHowever, most direct 4D printing methods require multi-material systems\ninvolving costly machines. Moreover, most works have used an open-cell design\nfor shape shifting by encoding a collection of 1D rib deformations, which\ncannot remain structurally stable. Here, we demonstrate the direct 4D printing\nof an isotropic single-material system to morph 2D continuous bilayer plates\ninto doubly curved and multimodal 3D complex shapes whose geometry can also be\nlocked after deployment. We develop an inverse-design algorithm that integrates\nextrusion-based 3D printing of a single-material system to directly morph a raw\nprinted sheet into complex 3D geometries such as a doubly curved surface with\nshape locking. Furthermore, our inverse-design tool encodes the localized\nshape-memory anisotropy during the process, providing the processing conditions\nfor a target 3D morphed geometry. Our approach could be used for conventional\nextrusion-based 3D printing for various applications including biomedical\ndevices, deployable structures, smart textiles, and pop-up Kirigami structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.02510v1"
    },
    {
        "title": "Foveated Rendering: Motivation, Taxonomy, and Research Directions",
        "authors": [
            "Susmija Jabbireddy",
            "Xuetong Sun",
            "Xiaoxu Meng",
            "Amitabh Varshney"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  With the recent interest in virtual reality and augmented reality, there is a\nnewfound demand for displays that can provide high resolution with a wide field\nof view (FOV). However, such displays incur significantly higher costs for\nrendering the larger number of pixels. This poses the challenge of rendering\nrealistic real-time images that have a wide FOV and high resolution using\nlimited computing resources. The human visual system does not need every pixel\nto be rendered at a uniformly high quality. Foveated rendering methods provide\nperceptually high-quality images while reducing computational workload and are\nbecoming a crucial component for large-scale rendering. In this paper, we\npresent key motivations, research directions, and challenges for leveraging the\nlimitations of the human visual system as they relate to foveated rendering. We\nprovide a taxonomy to compare and contrast various foveated techniques based on\nkey factors. We also review aliasing artifacts arising due to foveation methods\nand discuss several approaches that attempt to mitigate such effects. Finally,\nwe present several open problems and possible future research directions that\ncan further reduce computational costs while generating perceptually\nhigh-quality renderings.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04529v2"
    },
    {
        "title": "HDR Lighting Dilation for Dynamic Range Reduction on Virtual Production\n  Stages",
        "authors": [
            "Paul Debevec",
            "Chloe LeGendre"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a technique to reduce the dynamic range of an HDRI lighting\nenvironment map in an efficient, energy-preserving manner by spreading out the\nlight of concentrated light sources. This allows us to display a reasonable\napproximation of the illumination of an HDRI map in a lighting reproduction\nsystem with limited dynamic range such as virtual production LED Stage. The\ntechnique identifies regions of the HDRI map above a given pixel threshold,\ndilates these regions until the average pixel value within each is below the\nthreshold, and finally replaces each dilated region's pixels with the region's\naverage pixel value. The new HDRI map contains the same energy as the original,\nspreads the light as little as possible, and avoids chromatic fringing.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07873v2"
    },
    {
        "title": "A Fully Implicit Method for Robust Frictional Contact Handling in\n  Elastic Rods",
        "authors": [
            "Dezhong Tong",
            "Andrew Choi",
            "Jungseock Joo",
            "M. Khalid Jawed"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Accurate frictional contact is critical in simulating the assembly of\nrod-like structures in the practical world, such as knots, hairs, flagella, and\nmore. Due to their high geometric nonlinearity and elasticity, rod-on-rod\ncontact remains a challenging problem tackled by researchers in both\ncomputational mechanics and computer graphics. Typically, frictional contact is\nregarded as constraints for the equations of motions of a system. Such\nconstraints are often computed independently at every time step in a dynamic\nsimulation, thus slowing down the simulation and possibly introducing numerical\nconvergence issues. This paper proposes a fully implicit penalty-based\nfrictional contact method, Implicit Contact Model (IMC), that efficiently and\nrobustly captures accurate frictional contact responses. We showcase our\nalgorithm's performance in achieving visually realistic results for the\nchallenging and novel contact scenario of flagella bundling in fluid medium, a\nsignificant phenomenon in biology that motivates novel engineering applications\nin soft robotics. In addition to this, we offer a side-by-side comparison with\nIncremental Potential Contact (IPC), a state-of-the-art contact handling\nalgorithm. We show that IMC possesses comparable performance to IPC while\nconverging at a faster rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.10309v3"
    },
    {
        "title": "Fast GPU bounding boxes on tree-structured scenes",
        "authors": [
            "Raph Levien"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Computation of bounding boxes is a fundamental problem in high performance\nrendering, as it is an input to visibility culling and binning operations. In a\nscene description structured as a tree, clip nodes and blend nodes entail\nintersection and union of bounding boxes, respectively. These are\nstraightforward to compute on the CPU using a sequential algorithm, but an\nefficient, parallel GPU algorithm is more elusive. This paper presents a fast\nand practical solution, with a new algorithm for the classic parentheses\nmatching problem at its core. The core algorithm is presented abstractly (in\nterms of a PRAM abstraction), then with a concrete mapping to the thread,\nworkgroup, and dispatch levels of real GPU hardware. The algorithm is\nimplemented portably using compute shaders, and performance results show a\ndramatic speedup over a sequential CPU version, and indeed a reasonable\nfraction of maximum theoretical throughput of the GPU hardware. The immediate\nmotivating application is 2D rendering, but the algorithms generalize to other\ndomains, and the core parentheses matching problem has other applications\nincluding parsing.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.11659v1"
    },
    {
        "title": "Semantically Supervised Appearance Decomposition for Virtual Staging\n  from a Single Panorama",
        "authors": [
            "Tiancheng Zhi",
            "Bowei Chen",
            "Ivaylo Boyadzhiev",
            "Sing Bing Kang",
            "Martial Hebert",
            "Srinivasa G. Narasimhan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We describe a novel approach to decompose a single panorama of an empty\nindoor environment into four appearance components: specular, direct sunlight,\ndiffuse and diffuse ambient without direct sunlight. Our system is weakly\nsupervised by automatically generated semantic maps (with floor, wall, ceiling,\nlamp, window and door labels) that have shown success on perspective views and\nare trained for panoramas using transfer learning without any further\nannotations. A GAN-based approach supervised by coarse information obtained\nfrom the semantic map extracts specular reflection and direct sunlight regions\non the floor and walls. These lighting effects are removed via a similar\nGAN-based approach and a semantic-aware inpainting step. The appearance\ndecomposition enables multiple applications including sun direction estimation,\nvirtual furniture insertion, floor material replacement, and sun direction\nchange, providing an effective tool for virtual home staging. We demonstrate\nthe effectiveness of our approach on a large and recently released dataset of\npanoramas of empty homes.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.13150v1"
    },
    {
        "title": "Differentiable solver for time-dependent deformation problems with\n  contact",
        "authors": [
            "Zizhou Huang",
            "Davi Colli Tozoni",
            "Arvi Gjoka",
            "Zachary Ferguson",
            "Teseo Schneider",
            "Daniele Panozzo",
            "Denis Zorin"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce a general differentiable solver for time-dependent deformation\nproblems with contact and friction. Our approach uses a finite element\ndiscretization with a high-order time integrator coupled with the recently\nproposed incremental potential contact method for handling contact and friction\nforces to solve ODE- and PDE-constrained optimization problems on scenes with\ncomplex geometry. It supports static and dynamic problems and differentiation\nwith respect to all physical parameters involved in the physical problem\ndescription, which include shape, material parameters, friction parameters, and\ninitial conditions. Our analytically derived adjoint formulation is efficient,\nwith a small overhead (typically less than 10% for nonlinear problems) over the\nforward simulation, and shares many similarities with the forward problem,\nallowing the reuse of large parts of existing forward simulator code.\n  We implement our approach on top of the open-source PolyFEM library and\ndemonstrate the applicability of our solver to shape design, initial condition\noptimization, and material estimation on both simulated results and physical\nvalidations.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.13643v5"
    },
    {
        "title": "High-Order Incremental Potential Contact for Elastodynamic Simulation on\n  Curved Meshes",
        "authors": [
            "Zachary Ferguson",
            "Pranav Jain",
            "Denis Zorin",
            "Teseo Schneider",
            "Daniele Panozzo"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  High-order bases provide major advantages over linear ones in terms of\nefficiency, as they provide (for the same physical model) higher accuracy for\nthe same running time, and reliability, as they are less affected by locking\nartifacts and mesh quality. Thus, we introduce a high-order finite element (FE)\nformulation (high-order bases) for elastodynamic simulation on high-order\n(curved) meshes with contact handling based on the recently proposed\nIncremental Potential Contact (IPC) model.\n  Our approach is based on the observation that each IPC optimization step used\nto minimize the elasticity, contact, and friction potentials leads to linear\ntrajectories even in the presence of nonlinear meshes or nonlinear FE bases. It\nis thus possible to retain the strong non-penetration guarantees and large time\nsteps of the original formulation while benefiting from the high-order bases\nand high-order geometry. We accomplish this by mapping displacements and\nresulting contact forces between a linear collision proxy and the underlying\nhigh-order representation.\n  We demonstrate the effectiveness of our approach in a selection of problems\nfrom graphics, computational fabrication, and scientific computing.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.13727v3"
    },
    {
        "title": "Angle-Uniform Parallel Coordinates",
        "authors": [
            "Kaiyi Zhang",
            "Liang Zhou",
            "Lu Chen",
            "Shitong He",
            "Daniel Weiskopf",
            "Yunhai Wang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present angle-uniform parallel coordinates, a data-independent technique\nthat deforms the image plane of parallel coordinates so that the angles of\nlinear relationships between two variables are linearly mapped along the\nhorizontal axis of the parallel coordinates plot. Despite being a common method\nfor visualizing multidimensional data, parallel coordinates are ineffective for\nrevealing positive correlations since the associated parallel coordinates\npoints of such structures may be located at infinity in the image plane and the\nasymmetric encoding of negative and positive correlations may lead to\nunreliable estimations. To address this issue, we introduce a transformation\nthat bounds all points horizontally using an angle-uniform mapping and shrinks\nthem vertically in a structure-preserving fashion; polygonal lines become\nsmooth curves and a symmetric representation of data correlations is achieved.\nWe further propose a combined subsampling and density visualization approach to\nreduce visual clutter caused by overdrawing. Our method enables accurate visual\npattern interpretation of data correlations, and its data-independent nature\nmakes it applicable to all multidimensional datasets. The usefulness of our\nmethod is demonstrated using examples of synthetic and real-world datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.14430v2"
    },
    {
        "title": "Text/Speech-Driven Full-Body Animation",
        "authors": [
            "Wenlin Zhuang",
            "Jinwei Qi",
            "Peng Zhang",
            "Bang Zhang",
            "Ping Tan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Due to the increasing demand in films and games, synthesizing 3D avatar\nanimation has attracted much attention recently. In this work, we present a\nproduction-ready text/speech-driven full-body animation synthesis system. Given\nthe text and corresponding speech, our system synthesizes face and body\nanimations simultaneously, which are then skinned and rendered to obtain a\nvideo stream output. We adopt a learning-based approach for synthesizing facial\nanimation and a graph-based approach to animate the body, which generates\nhigh-quality avatar animation efficiently and robustly. Our results demonstrate\nthe generated avatar animations are realistic, diverse and highly\ntext/speech-correlated.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.15573v1"
    },
    {
        "title": "Improved Direct Voxel Grid Optimization for Radiance Fields\n  Reconstruction",
        "authors": [
            "Cheng Sun",
            "Min Sun",
            "Hwann-Tzong Chen"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this technical report, we improve the DVGO framework (called DVGOv2),\nwhich is based on Pytorch and uses the simplest dense grid representation.\nFirst, we re-implement part of the Pytorch operations with cuda, achieving 2-3x\nspeedup. The cuda extension is automatically compiled just in time. Second, we\nextend DVGO to support Forward-facing and Unbounded Inward-facing capturing.\nThird, we improve the space time complexity of the distortion loss proposed by\nmip-NeRF 360 from O(N^2) to O(N). The distortion loss improves our quality and\ntraining speed. Our efficient implementation could allow more future works to\nbenefit from the loss.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.05085v4"
    },
    {
        "title": "Differentiable Transient Rendering",
        "authors": [
            "Shinyoung Yi",
            "Donggun Kim",
            "Kiseok Choi",
            "Adrian Jarabo",
            "Diego Gutierrez",
            "Min H. Kim"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Recent differentiable rendering techniques have become key tools to tackle\nmany inverse problems in graphics and vision. Existing models, however, assume\nsteady-state light transport, i.e., infinite speed of light. While this is a\nsafe assumption for many applications, recent advances in ultrafast imaging\nleverage the wealth of information that can be extracted from the exact time of\nflight of light. In this context, physically-based transient rendering allows\nto efficiently simulate and analyze light transport considering that the speed\nof light is indeed finite. In this paper, we introduce a novel differentiable\ntransient rendering framework, to help bring the potential of differentiable\napproaches into the transient regime. To differentiate the transient path\nintegral we need to take into account that scattering events at path vertices\nare no longer independent; instead, tracking the time of flight of light\nrequires treating such scattering events at path vertices jointly as a\nmultidimensional, evolving manifold. We thus turn to the generalized transport\ntheorem, and introduce a novel correlated importance term, which links the\ntime-integrated contribution of a path to its light throughput, and allows us\nto handle discontinuities in the light and sensor functions. Last, we present\nresults in several challenging scenarios where the time of flight of light\nplays an important role such as optimizing indices of refraction,\nnon-line-of-sight tracking with nonplanar relay walls, and non-line-of-sight\ntracking around two corners.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.06193v1"
    },
    {
        "title": "Volume Conductor: Interactive Visibility Management for Crowded Volumes",
        "authors": [
            "Žiga Lesar",
            "Ruwayda Alharbi",
            "Ciril Bohak",
            "Ondřej Strnad",
            "Christoph Heinzl",
            "Matija Marolt",
            "Ivan Viola"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a novel smart visibility system for visualizing crowded volumetric\ndata containing many object instances. The presented approach allows users to\nform groups of objects through membership predicates and to individually\ncontrol the visibility of the instances in each group. Unlike previous smart\nvisibility approaches, our approach controls the visibility on a per-instance\nbasis and decides which instances are displayed or hidden based on the\nmembership predicates and the current view. Thus, cluttered and dense volumes\nthat are notoriously difficult to explore effectively are automatically\nsparsified so that the essential information is extracted and presented to the\nuser. The proposed system is generic and can be easily integrated into existing\nvolume rendering applications and applied to many different domains. We\ndemonstrate the use of the volume conductor for visualizing fiber-reinforced\npolymers and intracellular organelle structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.07392v1"
    },
    {
        "title": "Predicting Geometric Errors and Failures in Additive Manufacturing",
        "authors": [
            "Margarita Ntousia",
            "Ioannis Fudos",
            "Spyridon Moschopoulos",
            "Vasiliki Stamati"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Additive manufacturing is a process that has facilitated the cost effective\nproduction of complicated designs. Objects fabricated via additive\nmanufacturing technologies often suffer from dimensional accuracy issues and\nother part specific problems such as thin part robustness, overhang geometries\nthat may collapse, support structures that cannot be removed, engraved and\nembossed details that are indistinguishable. In this work we present an\napproach to predict the dimensional accuracy per vertex and per part.\nFurthermore, we provide a framework for estimating the probability that a model\nis fabricated correctly via an additive manufacturing technology for a specific\napplication. This framework can be applied to several 3D printing technologies\nand applications. In the context of this paper, a thorough experimental\nevaluation is presented for binder jetting technology and applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.07951v2"
    },
    {
        "title": "Efficient Raycasting of Volumetric Depth Images for Remote Visualization\n  of Large Volumes at High Frame Rates",
        "authors": [
            "Aryaman Gupta",
            "Ulrik Günther",
            "Pietro Incardona",
            "Guido Reina",
            "Steffen Frey",
            "Stefan Gumhold",
            "Ivo F. Sbalzarini"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present an efficient raycasting algorithm for rendering Volumetric Depth\nImages (VDIs), and we show how it can be used in a remote visualization setting\nwith VDIs generated and streamed from a remote server. VDIs are compact\nview-dependent volume representations that enable interactive visualization of\nlarge volumes at high frame rates by decoupling viewpoint changes from\nexpensive rendering calculations. However, current rendering approaches for\nVDIs struggle with achieving interactive frame rates at high image resolutions.\nHere, we exploit the properties of perspective projection to simplify\nintersections of rays with the view-dependent frustums in a VDI and leverage\nspatial smoothness in the volume data to minimize memory accesses. Benchmarks\nshow that responsive frame rates can be achieved close to the viewpoint of\ngeneration for HD display resolutions, providing high-fidelity approximate\nrenderings of Gigabyte-sized volumes. We also propose a method to subsample the\nVDI for preview rendering, maintaining high frame rates even for large\nviewpoint deviations. We provide our implementation as an extension of an\nestablished open-source visualization library.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.08660v3"
    },
    {
        "title": "Towards computing complete parameter ranges in parametric modeling",
        "authors": [
            "Zhihong Tang",
            "Qiang Zou",
            "Shuming Gao"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In parametric design, the geometric model is edited by changing relevant\nparameters in the parametric model, which is commonly done sequentially on\nmultiple parameters. Without guidance on allowable parameter ranges that can\nguarantee the solvability of the geometric constraint system, the user could\nassign improper parameter values to the model's parameters, which would further\nlead to a failure in model updating. However, current commercial CAD systems\nprovide little support for the proper parameter assignments. Although the\nexisting methods can compute allowable ranges for individual parameters, they\nface difficulties in handling multi-parameter situations. In particular, these\nmethods could miss some feasible parameter values and provide incomplete\nallowable parameter ranges. To solve this problem, an automatic approach is\nproposed in this paper to compute complete parameter ranges in multi-parameter\nediting. In the approach, a set of variable parameters are first selected to be\nsequentially edited by the user; before each editing operation, the\none-dimensional ranges of the variable parameters are presented as guidance. To\ncompute the one-dimensional ranges, each variable parameter is expressed as an\nequality-constrained function, and its one-dimensional allowable range is\nobtained by calculating the function range. To effectively obtain the function\nrange which can hardly be calculated in a normal way, the function range\nproblem is converted into a constrained optimization problem, and is then\nsolved by Lagrange multiplier method and the Niching particle swarm\noptimization algorithm (the NichePSO). The effectiveness and efficiency of the\nproposed approach is verified by several experimental results.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.08698v1"
    },
    {
        "title": "A Data-Driven Paradigm for Precomputed Radiance Transfer",
        "authors": [
            "Laurent Belcour",
            "Thomas Deliot",
            "Wilhem Barbier",
            "Cyril Soler"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this work, we explore a change of paradigm to build Precomputed Radiance\nTransfer (PRT) methods in a data-driven way. This paradigm shift allows us to\nalleviate the difficulties of building traditional PRT methods such as defining\na reconstruction basis, coding a dedicated path tracer to compute a transfer\nfunction, etc. Our objective is to pave the way for Machine Learned methods by\nproviding a simple baseline algorithm. More specifically, we demonstrate\nreal-time rendering of indirect illumination in hair and surfaces from a few\nmeasurements of direct lighting. We build our baseline from pairs of direct and\nindirect illumination renderings using only standard tools such as Singular\nValue Decomposition (SVD) to extract both the reconstruction basis and transfer\nfunction.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.13112v1"
    },
    {
        "title": "Parallel Compositing of Volumetric Depth Images for Interactive\n  Visualization of Distributed Volumes at High Frame Rates",
        "authors": [
            "Aryaman Gupta",
            "Pietro Incardona",
            "Anton Brock",
            "Guido Reina",
            "Steffen Frey",
            "Stefan Gumhold",
            "Ulrik Günther",
            "Ivo F. Sbalzarini"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a parallel compositing algorithm for Volumetric Depth Images\n(VDIs) of large three-dimensional volume data. Large distributed volume data\nare routinely produced in both numerical simulations and experiments, yet it\nremains challenging to visualize them at smooth, interactive frame rates. VDIs\nare view-dependent piecewise constant representations of volume data that offer\na potential solution. They are more compact and less expensive to render than\nthe original data. So far, however, there is no method for generating VDIs from\ndistributed data. We propose an algorithm that enables this by sort-last\nparallel generation and compositing of VDIs with automatically chosen\ncontent-adaptive parameters. The resulting composited VDI can then be streamed\nfor remote display, providing responsive visualization of large, distributed\nvolume data.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.14503v3"
    },
    {
        "title": "Controlling Material Appearance by Examples",
        "authors": [
            "Yiwei Hu",
            "Miloš Hašan",
            "Paul Guerrero",
            "Holly Rushmeier",
            "Valentin Deschaintre"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Despite the ubiquitousness of materials maps in modern rendering pipelines,\ntheir editing and control remains a challenge. In this paper, we present an\nexample-based material control method to augment input material maps based on\nuser-provided material photos. We train a tileable version of MaterialGAN and\nleverage its material prior to guide the appearance transfer, optimizing its\nlatent space using differentiable rendering. Our method transfers the micro and\nmeso-structure textures of user provided target(s) photographs, while\npreserving the structure of the input and quality of the input material. We\nshow our methods can control existing material maps, increasing realism or\ngenerating new, visually appealing materials.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.14970v1"
    },
    {
        "title": "Stochastic Poisson Surface Reconstruction",
        "authors": [
            "Silvia Sellán",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce a statistical extension of the classic Poisson Surface\nReconstruction algorithm for recovering shapes from 3D point clouds. Instead of\noutputting an implicit function, we represent the reconstructed shape as a\nmodified Gaussian Process, which allows us to conduct statistical queries\n(e.g., the likelihood of a point in space being on the surface or inside a\nsolid). We show that this perspective: improves PSR's integration into the\nonline scanning process, broadens its application realm, and opens the door to\nother lines of research such as applying task-specific priors.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.15236v2"
    },
    {
        "title": "Corner-based implicit patches",
        "authors": [
            "Ágoston Sipos"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Multi-sided surfaces are often defined by side interpolants (also called\nribbons), i.e. the surface has to connect to the ribbons with a prescribed\ndegree of smoothness. The I-patch is such a family of implicit surfaces capable\nof interpolating an arbitrary number of ribbons and can be used in design and\napproximation. While in the case of parametric surfaces describing ribbons is a\nwell-discussed problem, defining implicit ribbons is a different task. This\npaper will introduce corner I-patches, a new representation that describes\nimplicit surfaces based on corner interpolants. Those may be defined with much\nsimpler surfaces, while the shape of the patch will depend on a handful of\nscalar parameters. Continuity between patches will be enforced via constraints\non these parameters. Corner I-patches have several favorable properties that\ncan be exploited for example in volume rendering or approximation.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00806v1"
    },
    {
        "title": "MatFormer: A Generative Model for Procedural Materials",
        "authors": [
            "Paul Guerrero",
            "Miloš Hašan",
            "Kalyan Sunkavalli",
            "Radomír Měch",
            "Tamy Boubekeur",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Procedural material graphs are a compact, parameteric, and\nresolution-independent representation that are a popular choice for material\nauthoring. However, designing procedural materials requires significant\nexpertise and publicly accessible libraries contain only a few thousand such\ngraphs. We present MatFormer, a generative model that can produce a diverse set\nof high-quality procedural materials with complex spatial patterns and\nappearance. While procedural materials can be modeled as directed (operation)\ngraphs, they contain arbitrary numbers of heterogeneous nodes with\nunstructured, often long-range node connections, and functional constraints on\nnode parameters and connections. MatFormer addresses these challenges with a\nmulti-stage transformer-based model that sequentially generates nodes, node\nparameters, and edges, while ensuring the semantic validity of the graph. In\naddition to generation, MatFormer can be used for the auto-completion and\nexploration of partial material graphs. We qualitatively and quantitatively\ndemonstrate that our method outperforms alternative approaches, in both\ngenerated graph and material quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.01044v2"
    },
    {
        "title": "Automatic Quantization for Physics-Based Simulation",
        "authors": [
            "Jiafeng Liu",
            "Haoyang Shi",
            "Siyuan Zhang",
            "Yin Yang",
            "Chongyang Ma",
            "Weiwei Xu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Quantization has proven effective in high-resolution and large-scale\nsimulations, which benefit from bit-level memory saving. However, identifying a\nquantization scheme that meets the requirement of both precision and memory\nefficiency requires trial and error. In this paper, we propose a novel\nframework to allow users to obtain a quantization scheme by simply specifying\neither an error bound or a memory compression rate. Based on the error\npropagation theory, our method takes advantage of auto-diff to estimate the\ncontributions of each quantization operation to the total error. We formulate\nthe task as a constrained optimization problem, which can be efficiently solved\nwith analytical formulas derived for the linearized objective function. Our\nworkflow extends the Taichi compiler and introduces dithering to improve the\nprecision of quantized simulations. We demonstrate the generality and\nefficiency of our method via several challenging examples of physics-based\nsimulation, which achieves up to 2.5x memory compression without noticeable\ndegradation of visual quality in the results. Our code and data are available\nat https://github.com/Hanke98/AutoQuantizer.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.04658v2"
    },
    {
        "title": "Htex: Per-Halfedge Texturing for Arbitrary Mesh Topologies",
        "authors": [
            "Wilhem Barbier",
            "Jonathan Dupuy"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce per-halfedge texturing (Htex) a GPU-friendly method for\ntexturing arbitrary polygon-meshes without an explicit parameterization. Htex\nbuilds upon the insight that halfedges encode an intrinsic triangulation for\npolygon meshes, where each halfedge spans a unique triangle with direct\nadjacency information. Rather than storing a separate texture per face of the\ninput mesh as is done by previous parameterization-free texturing methods, Htex\nstores a square texture for each halfedge and its twin. We show that this\nsimple change from face to halfedge induces two important properties for high\nperformance parameterization-free texturing. First, Htex natively supports\narbitrary polygons without requiring dedicated code for, e.g, non-quad faces.\nSecond, Htex leads to a straightforward and efficient GPU implementation that\nuses only three texture-fetches per halfedge to produce continuous texturing\nacross the entire mesh. We demonstrate the effectiveness of Htex by rendering\nproduction assets in real time.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.05618v1"
    },
    {
        "title": "Node Graph Optimization Using Differentiable Proxies",
        "authors": [
            "Yiwei Hu",
            "Paul Guerrero",
            "Miloš Hašan",
            "Holly Rushmeier",
            "Valentin Deschaintre"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Graph-based procedural materials are ubiquitous in content production\nindustries. Procedural models allow the creation of photorealistic materials\nwith parametric control for flexible editing of appearance. However, designing\na specific material is a time-consuming process in terms of building a model\nand fine-tuning parameters. Previous work [Hu et al. 2022; Shi et al. 2020]\nintroduced material graph optimization frameworks for matching target material\nsamples. However, these previous methods were limited to optimizing\ndifferentiable functions in the graphs. In this paper, we propose a fully\ndifferentiable framework which enables end-to-end gradient based optimization\nof material graphs, even if some functions of the graph are non-differentiable.\nWe leverage the Differentiable Proxy, a differentiable approximator of a\nnon-differentiable black-box function. We use our framework to match structure\nand appearance of an output material to a target material, through a\nmulti-stage differentiable optimization. Differentiable Proxies offer a more\ngeneral optimization solution to material appearance matching than previous\nwork.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.07684v1"
    },
    {
        "title": "Voreen -- An Open-source Framework for Interactive Visualization and\n  Processing of Large Volume Data",
        "authors": [
            "Dominik Drees",
            "Simon Leistikow",
            "Xiaoyi Jiang",
            "Lars Linsen"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Technological advances for measuring or simulating volume data have led to\nlarge data sizes in many research areas such as biology, medicine, physics, and\ngeoscience. Here, large data can refer to individual data sets with high\nspatial and/or temporal resolution as well as collections of data sets in the\nsense of cohorts or ensembles. Therefore, general-purpose and customizable\nvolume visualization and processing systems have to provide out-of-core\nmechanisms that allow for handling and analyzing such data. Voreen is an\nopen-source rapid-prototyping framework that was originally designed to quickly\ncreate custom visualization applications for volumetric imaging data using the\nmeanwhile quite common data flow graph paradigm. In recent years, Voreen has\nbeen used in various interdisciplinary research projects with an increasing\ndemand for large data processing capabilities without relying on cluster\ncompute resources. In its latest release, Voreen has thus been extended by\nout-of-core techniques for processing and visualization of volume data with\nvery high spatial resolution as well as collections of volume data sets\nincluding spatio-temporal multi-field simulation ensembles. In this paper we\ncompare state-of-the-art volume processing and visualization systems and\nconclude that Voreen is the first system combining out-of-core processing and\nrendering capabilities for large volume data on consumer hardware with features\nimportant for interdisciplinary research. We describe how Voreen achieves these\ngoals and show-case its use, performance, and capability to support\ninterdisciplinary research by presenting typical workflows within two large\nvolume data case studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.12746v1"
    },
    {
        "title": "Dynamic modeling of a sliding ring on an elastic rod with incremental\n  potential formulation",
        "authors": [
            "Weicheng Huang",
            "Peifei Xu",
            "Zhaowei Liu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Mechanical interactions between rigid rings and flexible cables find broad\napplication in both daily life (hanging clothes) and engineering systems\n(closing a tether-net). A reduced-order method for the dynamic analysis of\nsliding rings on a deformable one-dimensional (1D) rod-like object is proposed.\nIn contrast to the conventional approach of discretizing joint rings into\nmultiple nodes and edges for contact detection and numerical simulation, a\nsingle point is used to reduce the order of the model. To ensure that the\nsliding ring and flexible rod do not deviate from their desired positions, a\nnew barrier function is formulated using the incremental potential theory.\nSubsequently, the interaction between tangent frictional forces is obtained\nthrough a delayed dissipative approach. The proposed barrier functional and the\nassociated frictional functional are C2 continuous, hence the nonlinear\nelastodynamic system can be solved variationally by an implicit time-stepping\nscheme. The numerical framework is initially applied to simple examples where\nthe analytical solutions are available for validation. Then, multiple complex\npractical engineering examples are considered to showcase the effectiveness of\nthe proposed method. The simplified ring-to-rod interaction model has the\ncapacity to enhance the realism of visual effects in image animations, while\nsimultaneously facilitating the optimization of designs for space debris\nremoval systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.01238v4"
    },
    {
        "title": "Procedural Generation and Rendering of Realistic, Navigable Forest\n  Environments: An Open-Source Tool",
        "authors": [
            "Callum Newlands",
            "Klaus-Peter Zauner"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Simulation of forest environments has applications from entertainment and art\ncreation to commercial and scientific modelling. Due to the unique features and\nlighting in forests, a forest-specific simulator is desirable, however many\ncurrent forest simulators are proprietary or highly tailored to a particular\napplication. Here we review several areas of procedural generation and\nrendering specific to forest generation, and utilise this to create a\ngeneralised, open-source tool for generating and rendering interactive,\nrealistic forest scenes. The system uses specialised L-systems to generate\ntrees which are distributed using an ecosystem simulation algorithm. The\nresulting scene is rendered using a deferred rendering pipeline, a Blinn-Phong\nlighting model with real-time leaf transparency and post-processing lighting\neffects. The result is a system that achieves a balance between high natural\nrealism and visual appeal, suitable for tasks including training computer\nvision algorithms for autonomous robots and visual media generation.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.01471v1"
    },
    {
        "title": "Differentiable Subdivision Surface Fitting",
        "authors": [
            "Tianhao Xie"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this paper, we present a powerful differentiable surface fitting technique\nto derive a compact surface representation for a given dense point cloud or\nmesh, with application in the domains of graphics and CAD/CAM. We have chosen\nthe Loop subdivision surface, which in the limit yields the smooth surface\nunderlying the point cloud, and can handle complex surface topology better than\nother popular compact representations, such as NURBS. The principal idea is to\nfit the Loop subdivision surface not directly to the point cloud, but to the\nIMLS (implicit moving least squares) surface defined over the point cloud. As\nboth Loop subdivision and IMLS have analytical expressions, we are able to\nformulate the problem as an unconstrained minimization problem of a completely\ndifferentiable function that can be solved with standard numerical solvers.\nDifferentiability enables us to integrate the subdivision surface into any deep\nlearning method for point clouds or meshes. We demonstrate the versatility and\npotential of this approach by using it in conjunction with a differentiable\nrenderer to robustly reconstruct compact surface representations of\nspatial-temporal sequences of dense meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.01685v2"
    },
    {
        "title": "A Dataset and Benchmark for Mesh Parameterization",
        "authors": [
            "Georgia Shay",
            "Justin Solomon",
            "Oded Stein"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  UV parameterization is a core task in computer graphics, with applications in\nmesh texturing, remeshing, mesh repair, mesh editing, and more. It is thus an\nactive area of research, which has led to a wide variety of parameterization\nmethods that excel according to different measures of quality. There is no\nsingle metric capturing parameterization quality in practice, since the quality\nof a parameterization heavily depends on its application; hence,\nparameterization methods can best be judged by the actual users of the computed\nresult. In this paper, we present a dataset of meshes together with UV maps\ncollected from various sources and intended for real-life use. Our dataset can\nbe used to test parameterization methods in realistic environments. We also\nintroduce a benchmark to compare parameterization methods with artist-provided\nUV parameterizations using a variety of metrics. This strategy enables us to\nevaluate the performance of a parameterization method by computing the quality\nindicators that are valued by the designers of a mesh.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.01772v1"
    },
    {
        "title": "Report on the software \"SemanticModellingFramework\"",
        "authors": [
            "Andreas Scalas"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The evolution of 3D visual content calls for innovative methods for modelling\nshapes based on their intended usage, function and role in a complex scenario.\nEven if different attempts have been done in this direction, shape modelling\nstill mainly focuses on geometry. However, 3D models have a structure, given by\nthe arrangement of salient parts, and shape and structure are deeply related to\nsemantics and functionality. Changing geometry without semantic clues may\ninvalidate such functionalities or the meaning of objects or their parts. Here,\nthe problem is approached by considering semantics as the formalised knowledge\nrelated to a category of objects; the geometry can vary provided that the\nsemantics is preserved. The semantics and the variable geometry of a class of\nshapes is represented through the parametric template: an annotated 3D model\nwhose geometry can be deformed provided that some semantic constraints remain\nsatisfied. In this work, the design and development of a framework for the\nsemantics-aware modelling of shapes is presented, offering the user a single\napplication environment where the whole workflow of defining the parametric\ntemplate and applying semantics-aware deformations can take place. In\nparticular, the system provides tools for the selection and annotation of\ngeometry based on a formalised contextual knowledge; shape analysis methods to\nderive new knowledge implicitly encoded in the geometry, and possibly enrich\nthe given semantics; a set of constraints that the user can apply to salient\nparts and a deformation operation that takes into account the semantic\nconstraints and provides an optimal solution. The framework is modular so that\nnew tools can be continuously added.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.02577v1"
    },
    {
        "title": "PC-Expo: A Metrics-Based Interactive Axes Reordering Method for Parallel\n  Coordinate Displays",
        "authors": [
            "Anjul Tyagi",
            "Tyler Estro",
            "Geoff Kuenning",
            "Erez Zadok",
            "Klaus Mueller"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The axes ordering in PCP presents a particular story from the data based on\nthe user perception of PCP polylines. Existing works focus on directly\noptimizing for PCP axes ordering based on some common analysis tasks like\nclustering, neighborhood, and correlation. However, direct optimization for PCP\naxes based on these common properties is restrictive because it does not\naccount for multiple properties occurring between the axes, and for local\nproperties that occur in small regions in the data. Also, many of these\ntechniques do not support the human-in-the-loop (HIL) paradigm, which is\ncrucial (i) for explainability and (ii) in cases where no single reordering\nscheme fits the user goals. To alleviate these problems, we present PC-Expo, a\nreal-time visual analytics framework for all-in-one PCP line pattern detection,\nand axes reordering. We studied the connection of line patterns in PCPs with\ndifferent data analysis tasks and datasets. PC-Expo expands prior work on PCP\naxes reordering by developing real-time, local detection schemes for the 12\nmost common analysis tasks (properties). Users can choose the story they want\nto present with PCPs by optimizing directly over their choice of properties.\nThese properties can be ranked, or combined using individual weights, creating\na custom optimization scheme for axes reordering. Users can control the\ngranularity at which they want to work with their detection scheme in the data,\nallowing exploration of local regions. PC-Expo also supports HIL axes\nreordering via local-property visualization, which shows the regions of\ngranular activity for every axis pair. Local-property visualization is helpful\nfor PCP axes reordering based on multiple properties, when no single reordering\nscheme fits the user goals.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03430v1"
    },
    {
        "title": "Projective Geometry, Duality and Plucker Coordinates for Geometric\n  Computations with Determinants on GPUs",
        "authors": [
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Many algorithms used are based on geometrical computation. There are several\ncriteria in selecting appropriate algorithm from already known. Recently, the\nfastest algorithms have been preferred. Nowadays, algorithms with a high\nstability are preferred. Also technology and computer architecture, like GPU\netc., plays a significant role for large data processing. However, some\nalgorithms are ill-conditioned due to numerical representation used; result of\nthe floating point representation. In this paper, relations between projective\nrepresentation, duality and Plucker coordinates will be explored with\ndemonstration on simple geometric examples. The presented approach is\nconvenient especially for application on GPUs or vector-vector computational\narchitectures\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03674v1"
    },
    {
        "title": "A New Robust Algorithm for Computation of a Triangle Circumscribed\n  Sphere in E3 and a Hypersphere Simplex",
        "authors": [
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  There are many applications in which a bounding sphere containing the given\ntriangle E3 is needed, e.g. fast collision detection, ray-triangle intersecting\nin raytracing etc. This is a typical geometrical problem in E3 and it has also\napplications in computational problems in general. In this paper a new fast and\nrobust algorithm of circumscribed sphere computation in the -dimensional space\nis presented and specification for the E3 space is given, too. The presented\nmethod is convenient for use on GPU or with SSE or Intel AVX instructions on a\nstandard CPU\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03677v1"
    },
    {
        "title": "Point-in-Convex Polygon and Point-in-Convex Polyhedron Algorithms with\n  O(1) Complexity using Space Subdivision",
        "authors": [
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  There are many space subdivision and space partitioning techniques used in\nmany algorithms to speed up computations. They mostly rely on orthogonal space\nsubdivision, resp. using hierarchical data structures, e.g. BSP trees,\nquadtrees, octrees, kd-trees, bounding volume hierarchies, etc. However in some\napplications a non-orthogonal space subdivision can offer new ways for actual\nspeed up. In the case of convex polygon in E3 a simple Point-in-Polygon test is\nof the O(N) complexity and the optimal algorithm is of O(lg N) computational\ncomplexity. In the E3 case, the complexity is O(N) even for the convex\npolyhedron as no ordering is defined. New Point-in-Convex Polygon and\nPoint-in-Convex Polyhedron algorithms are presented based on space subdivision\nin the preprocessing stage resulting to O(1) run-time complexity. The presented\napproach is simple to implement. Due to the principle of duality, dual\nproblems, e.g. line-convex polygon, line clipping, can be solved in a\nsimilarly.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03682v1"
    },
    {
        "title": "Interpretable Disentangled Parametrization of Measured BRDF with\n  $β$-VAE",
        "authors": [
            "Alexis Benamira",
            "Sachin Shah",
            "Sumanta Pattanaik"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Finding a low dimensional parametric representation of measured BRDF remains\nchallenging. Currently available solutions are either not interpretable, or\nrely on limited analytical solutions, or require expensive test subject based\ninvestigations. In this work, we strive to establish a parametrization space\nthat affords the data-driven representation variance of measured BRDF models\nwhile still offering the artistic control of parametric analytical BRDFs. We\npresent a machine learning approach that generates an interpretable\ndisentangled parameter space. A disentangled representation is one in which\neach parameter is responsible for a unique generative factor and is insensitive\nto the ones encoded by the other parameters. To that end, we resort to a\n$\\beta$-Variational AutoEncoder ($\\beta$-VAE), a specific architecture of Deep\nNeural Network (DNN). After training our network, we analyze the\nparametrization space and interpret the learned generative factors utilizing\nour visual perception. It should be noted that perceptual analysis is called\nupon downstream of the system for interpretation purposes compared to most\nother existing methods where it is used upfront to elaborate the\nparametrization. In addition to that, we do not need a test subject\ninvestigation. A novel feature of our interpretable disentangled\nparametrization is the post-processing capability to incorporate new parameters\nalong with the learned ones, thus expanding the richness of producible\nappearances. Furthermore, our solution allows more flexible and controllable\nmaterial editing possibilities than manifold exploration. Finally, we provide a\nrendering interface, for real-time material editing and interpolation based on\nthe presented new parametrization system.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03914v1"
    },
    {
        "title": "A Rotation-Strain Method to Model Surfaces using Plasticity",
        "authors": [
            "Jiahao Wen",
            "Bohan Wang",
            "Jernej Barbič"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Modeling arbitrarily large deformations of surfaces smoothly embedded in\nthree-dimensional space is challenging. The difficulties come from two aspects:\nthe existing geometry processing or forward simulation methods penalize the\ndifference between the current status and the rest configuration to maintain\nthe initial shape, which will lead to sharp spikes or wiggles for large\ndeformations; the co-dimensional nature of the problem makes it more\ncomplicated because the deformed surface has to locally satisfy compatibility\nconditions on fundamental forms to guarantee a feasible solution exists. To\naddress these two challenges, we propose a rotation-strain method to modify the\nfundamental forms in a compatible way, and model the large deformation of\nsurface meshes smoothly using plasticity. The user prescribes the positions of\na few vertices, and our method finds a smooth strain and rotation field under\nwhich the surface meets the target positions. We demonstrate several examples\nwhereby triangle meshes are smoothly deformed to large strains while meeting\nuser constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.04364v1"
    },
    {
        "title": "CLIP-based Neural Neighbor Style Transfer for 3D Assets",
        "authors": [
            "Shailesh Mishra",
            "Jonathan Granskog"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a method for transferring the style from a set of images to a 3D\nobject. The texture appearance of an asset is optimized with a differentiable\nrenderer in a pipeline based on losses using pretrained deep neural networks.\nMore specifically,\n  we utilize a nearest-neighbor feature matching loss with CLIP-ResNet50 to\nextract the style from images. We show that a CLIP- based style loss provides a\ndifferent appearance over a VGG-based loss by focusing more on texture over\ngeometric shapes.\n  Additionally, we extend the loss to support multiple images and enable\nloss-based control over the color palette combined with automatic color palette\nextraction from style images.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.04370v1"
    },
    {
        "title": "UnderPressure: Deep Learning for Foot Contact Detection, Ground Reaction\n  Force Estimation and Footskate Cleanup",
        "authors": [
            "Lucas Mourot",
            "Ludovic Hoyet",
            "François Le Clerc",
            "Pierre Hellier"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Human motion synthesis and editing are essential to many applications like\nfilm post-production. However, they often introduce artefacts in motions, which\ncan be detrimental to the perceived realism. In particular, footskating is a\nfrequent and disturbing artefact requiring foot contacts knowledge to be\ncleaned up. Current approaches to obtain foot contact labels rely either on\nunreliable threshold-based heuristics or on tedious manual annotation. In this\narticle, we address foot contact label detection from motion with a deep\nlearning. To this end, we first publicly release UnderPressure, a novel motion\ncapture database labelled with pressure insoles data serving as reliable\nknowledge of foot contact with the ground. Then, we design and train a deep\nneural network to estimate ground reaction forces exerted on the feet from\nmotion data and then derive accurate foot contact labels. The evaluation of our\nmodel shows that we significantly outperform heuristic approaches based on\nheight and velocity thresholds and that our approach is much more robust on\nmotion sequences suffering from perturbations like noise or footskate. We\nfurther propose a fully automatic workflow for footskate cleanup: foot contact\nlabels are first derived from estimated ground reaction forces. Then, footskate\nis removed by solving foot constraints through an optimisation-based inverse\nkinematics (IK) approach that ensures consistency with the estimated ground\nreaction forces. Beyond footskate cleanup, both the database and the method we\npropose could help to improve many approaches based on foot contact labels or\nground reaction forces, including inverse dynamics problems like motion\nreconstruction and learning of deep motion models in motion synthesis or\ncharacter animation. Our implementation, pre-trained model as well as links to\ndatabase can be found at https://github.com/InterDigitalInc/UnderPressure.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.04598v1"
    },
    {
        "title": "A New Approach to Line-Sphere and Line-Quadrics Intersection Detection\n  and Computation",
        "authors": [
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Line intersection with convex and un-convex polygons or polyhedron algorithms\nare well known as line clipping algorithms and very often used in computer\ngraphics. Rendering of geometrical problems often leads to ray tracing\ntechniques, when an intersection of many lines with spheres or quadrics is a\ncritical issue due to ray-tracing algorithm complexity. A new formulation of\ndetection and computation of the intersection of line (ray) with a quadric\nsurface is presented, which separates geometric properties of the line and\nquadrics that enables pre-computation. The presented approach is especially\nconvenient for implementation with SSE instructions or on GPU\n",
        "pdf_link": "http://arxiv.org/pdf/2208.04728v1"
    },
    {
        "title": "New Geometric Continuity Solution of Parametric Surfaces",
        "authors": [
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This paper presents a new approach to computation of geometric continuity for\nparametric bi-cubic patches, based on a simple mathematical reformulation which\nleads to simple additional conditions to be applied in the patching\ncomputation. The paper presents an Hermite formulation of a bicubic parametric\npatch, but reformulations can be made also for B\\'ezier and B-Spline patches as\nwell. The presented approach is convenient for the cases when valencies of\ncorners are different from the value 4, in general.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.04729v1"
    },
    {
        "title": "Separable Shape Tensors for Aerodynamic Design",
        "authors": [
            "Zachary Grey",
            "Olga Doronina",
            "Andrew Glaws"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Airfoil shape design is a classical problem in engineering and manufacturing.\nIn this work, we combine principled physics-based considerations for the shape\ndesign problem with modern computational techniques using a data-driven\napproach. Modern and traditional analyses of 2D and 3D aerodynamic shapes\nreveal a flow-based sensitivity to specific deformations that can be\nrepresented generally by affine transformations (rotation, scaling, shearing,\ntranslation). We present a novel representation of shapes that decouples\naffine-style deformations over a submanifold and a product submanifold\nprincipally of the Grassmannian. As an analytic generative model, the separable\nrepresentation, informed by a database of physically relevant airfoils, offers\n(i) a rich set of novel 2D airfoil deformations not previously captured in the\ndata, (ii) an improved low-dimensional parameter domain for inferential\nstatistics informing design/manufacturing, and (iii) consistent 3D blade\nrepresentation and perturbation over a sequence of nominal 2D shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.04743v2"
    },
    {
        "title": "CreatureShop: Interactive 3D Character Modeling and Texturing from a\n  Single Color Drawing",
        "authors": [
            "Congyi Zhang",
            "Lei Yang",
            "Nenglun Chen",
            "Nicholas Vining",
            "Alla Sheffer",
            "Francis C. M. Lau",
            "Guoping Wang",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Creating 3D shapes from 2D drawings is an important problem with applications\nin content creation for computer animation and virtual reality. We introduce a\nnew sketch-based system, CreatureShop, that enables amateurs to create\nhigh-quality textured 3D character models from 2D drawings with ease and\nefficiency. CreatureShop takes an input bitmap drawing of a character (such as\nan animal or other creature), depicted from an arbitrary descriptive pose and\nviewpoint, and creates a 3D shape with plausible geometric details and textures\nfrom a small number of user annotations on the 2D drawing. Our key\ncontributions are a novel oblique view modeling method, a set of systematic\napproaches for producing plausible textures on the invisible or occluded parts\nof the 3D character (as viewed from the direction of the input drawing), and a\nuser-friendly interactive system. We validate our system and methods by\ncreating numerous 3D characters from various drawings, and compare our results\nwith related works to show the advantages of our method. We perform a user\nstudy to evaluate the usability of our system, which demonstrates that our\nsystem is a practical and efficient approach to create fully-textured 3D\ncharacter models for novice users.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.05572v1"
    },
    {
        "title": "MosaicSets: Embedding Set Systems into Grid Graphs",
        "authors": [
            "Peter Rottmann",
            "Markus Wallinger",
            "Annika Bonerath",
            "Sven Gedicke",
            "Martin Nöllenburg",
            "Jan-Henrik Haunert"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Visualizing sets of elements and their relations is an important research\narea in information visualization. In this paper, we present MosaicSets: a\nnovel approach to create Euler-like diagrams from non-spatial set systems such\nthat each element occupies one cell of a regular hexagonal or square grid. The\nmain challenge is to find an assignment of the elements to the grid cells such\nthat each set constitutes a contiguous region. As use case, we consider the\nresearch groups of a university faculty as elements, and the departments and\njoint research projects as sets. We aim at finding a suitable mapping between\nthe research groups and the grid cells such that the department structure forms\na base map layout. Our objectives are to optimize both the compactness of the\nentirety of all cells and of each set by itself. We show that computing the\nmapping is NP-hard. However, using integer linear programming we can solve\nreal-world instances optimally within a few seconds. Moreover, we propose a\nrelaxation of the contiguity requirement to visualize otherwise non-embeddable\nset systems. We present and discuss different rendering styles for the set\noverlays. Based on a case study with real-world data, our evaluation comprises\nquantitative measures as well as expert interviews.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.07982v1"
    },
    {
        "title": "3D Anatomical Representations and Analysis: an Application to the Spine",
        "authors": [
            "Martina Paccini",
            "Giuseppe Patanè",
            "Michela Spagnuolo"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This work proposes a framework for the patient-specific characterization of\nthe spine, which integrates information on the tissues with geometric\ninformation on the spine morphology. Key elements are the extraction of 3D\npatient-specific models of each vertebra and the intervertebral space from 3D\nCT images, the segmentation of each vertebra in its three functional regions,\nand the analysis of the tissue condition in the functional regions based on\ngeometrical parameters. The localization of anomalies obtained in the results\nand the proposed visualization support the applicability of our tool for\nquantitative and visual evaluation of possible damages, for surgery planning,\nand early diagnosis or follow-up studies. Finally, we discuss the main\nproperties of the proposed framework in terms of characterisation of the\nmorphology and pathology of the spine on benchmarks of the spine district.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.08983v1"
    },
    {
        "title": "Generalized Projection Matrices",
        "authors": [
            "S. J. D. MacIntosh"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Projection matrices are necessary for a large portion of rendering computer\ngraphics. There are primarily two different types of projection matrices --\nperspective and orthographic -- which are used frequently, and are\ntraditionally treated as mutually incompatible with each other in how they are\ndefined. Here, we bridge the gap between the two different forms of projection\nmatrices to present a single generalized projection matrix that can represent\nboth.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.09549v1"
    },
    {
        "title": "Dual Space Coupling Model Guided Overlap-Free Scatterplot",
        "authors": [
            "Zeyu Li",
            "Ruizhi Shi",
            "Yan Liu",
            "Shizhuo Long",
            "Ziheng Guo",
            "Shichao Jia",
            "Jiawan Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The overdraw problem of scatterplots seriously interferes with the visual\ntasks. Existing methods, such as data sampling, node dispersion, subspace\nmapping, and visual abstraction, cannot guarantee the correspondence and\nconsistency between the data points that reflect the intrinsic original data\ndistribution and the corresponding visual units that reveal the presented data\ndistribution, thus failing to obtain an overlap-free scatterplot with unbiased\nand lossless data distribution. A dual space coupling model is proposed in this\npaper to represent the complex bilateral relationship between data space and\nvisual space theoretically and analytically. Under the guidance of the model,\nan overlap-free scatterplot method is developed through integration of the\nfollowing: a geometry-based data transformation algorithm, namely\nDistributionTranscriptor; an efficient spatial mutual exclusion guided view\ntransformation algorithm, namely PolarPacking; an overlap-free oriented visual\nencoding configuration model and a radius adjustment tool, namely\n$f_{r_{draw}}$. Our method can ensure complete and accurate information\ntransfer between the two spaces, maintaining consistency between the newly\ncreated scatterplot and the original data distribution on global and local\nfeatures. Quantitative evaluation proves our remarkable progress on\ncomputational efficiency compared with the state-of-the-art methods. Three\napplications involving pattern enhancement, interaction improvement, and\noverdraw mitigation of trajectory visualization demonstrate the broad prospects\nof our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.09706v1"
    },
    {
        "title": "Parameterization-Independent Importance Sampling of Environment Maps",
        "authors": [
            "Martin Lambers"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Environment maps with high dynamic range lighting, such as daylight sky maps,\nrequire importance sampling to keep the balance between noise and number of\nsamples per pixel manageable. Typically, importance sampling schemes for\nenvironment maps are based directly on the map parameterization, e.g.\nequirectangular maps, and do not work with alternative parameterizations that\nmight provide better sampling quality. In this paper, an importance sampling\nscheme based on an equal-area projection of the sphere is proposed that is easy\nto implement and works independently of the environment map parameterization or\nresolution. This allows to apply the same scheme to equirectangular maps, cube\nmap variants, or any other map representation, and to adapt the importance\nsampling granularity to the requirements of the map contents.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.10815v1"
    },
    {
        "title": "Wavelet-Based Fast Decoding of 360-Degree Videos",
        "authors": [
            "Colin Groth",
            "Sascha Fricke",
            "Susana Castillo",
            "Marcus Magnor"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this paper, we propose a wavelet-based video codec specifically designed\nfor VR displays that enables real-time playback of high-resolution 360{\\deg}\nvideos. Our codec exploits the fact that only a fraction of the full 360{\\deg}\nvideo frame is visible on the display at any time. To load and decode the video\nviewport-dependently in real time, we make use of the wavelet transform for\nintra- as well as inter-frame coding. Thereby, the relevant content is directly\nstreamed from the drive, without the need to hold the entire frames in memory.\nWith an average of 193 frames per second at 8192x8192-pixel full-frame\nresolution, the conducted evaluation demonstrates that our codec's decoding\nperformance is up to 272% higher than that of the state-of-the-art video codecs\nH.265 and AV1 for typical VR displays. By means of a perceptual study, we\nfurther illustrate the necessity of high frame rates for a better VR\nexperience. Finally, we demonstrate how our wavelet-based codec can also\ndirectly be used in conjunction with foveation for further performance\nincrease.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.10859v2"
    },
    {
        "title": "DualSmoke: Sketch-Based Smoke Illustration Design with Two-Stage\n  Generative Model",
        "authors": [
            "Haoran Xie",
            "Keisuke Arihara",
            "Syuhei Sato",
            "Kazunori Miyata"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The dynamic effects of smoke are impressive in illustration design, but it is\na troublesome and challenging issue for common users to design the smoke effect\nwithout domain knowledge of fluid simulations. In this work, we propose\nDualSmoke, two stage global-to-local generation framework for the interactive\nsmoke illustration design. For the global stage, the proposed approach utilizes\nfluid patterns to generate Lagrangian coherent structure from the user's\nhand-drawn sketches. For the local stage, the detailed flow patterns are\nobtained from the generated coherent structure. Finally, we apply the guiding\nforce field to the smoke simulator to design the desired smoke illustration. To\nconstruct the training dataset, DualSmoke generates flow patterns using the\nfinite-time Lyapunov exponents of the velocity fields. The synthetic sketch\ndata is generated from the flow patterns by skeleton extraction. From our user\nstudy, it is verified that the proposed design interface can provide various\nsmoke illustration designs with good user usability. Our code is available at:\nhttps://github.com/shasph/DualSmoke\n",
        "pdf_link": "http://arxiv.org/pdf/2208.10906v1"
    },
    {
        "title": "Interactive Visual Analysis of Structure-borne Noise Data",
        "authors": [
            "Rainer Splechtna",
            "Denis Gracanin",
            "Goran Todorovic",
            "Stanislav Goja",
            "Boris Bedic",
            "Helwig Hauser",
            "Kresimir Matkovic"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Numerical simulation has become omnipresent in the automotive domain, posing\nnew challenges such as high-dimensional parameter spaces and large as well as\nincomplete and multi-faceted data. In this design study, we show how\ninteractive visual exploration and analysis of high-dimensional, spectral data\nfrom noise simulation can facilitate design improvements in the context of\nconflicting criteria. Here, we focus on structure-borne noise, i.e., noise from\nvibrating mechanical parts. Detecting problematic noise sources early in the\ndesign and production process is essential for reducing a product's development\ncosts and its time to market. In a close collaboration of visualization and\nautomotive engineering, we designed a new, interactive approach to quickly\nidentify and analyze critical noise sources, also contributing to an improved\nunderstanding of the analyzed system. Several carefully designed, interactive\nlinked views enable the exploration of noises, vibrations, and harshness at\nmultiple levels of detail, both in the frequency and spatial domain. This\nenables swift and smooth changes of perspective; selections in the frequency\ndomain are immediately reflected in the spatial domain, and vice versa. Noise\nsources are quickly identified and shown in the context of their neighborhood,\nboth in the frequency and spatial domain. We propose a novel drill-down view,\nespecially tailored to noise data analysis. Split boxplots and synchronized 3D\ngeometry views support comparison tasks. With this solution, engineers iterate\nover design optimizations much faster, while maintaining a good overview at\neach iteration. We evaluated the new approach in the automotive industry,\nstudying noise simulation data for an internal combustion engine.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.03083v1"
    },
    {
        "title": "Neural3Points: Learning to Generate Physically Realistic Full-body\n  Motion for Virtual Reality Users",
        "authors": [
            "Yongjing Ye",
            "Libin Liu",
            "Lei Hu",
            "Shihong Xia"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Animating an avatar that reflects a user's action in the VR world enables\nnatural interactions with the virtual environment. It has the potential to\nallow remote users to communicate and collaborate in a way as if they met in\nperson. However, a typical VR system provides only a very sparse set of up to\nthree positional sensors, including a head-mounted display (HMD) and optionally\ntwo hand-held controllers, making the estimation of the user's full-body\nmovement a difficult problem. In this work, we present a data-driven\nphysics-based method for predicting the realistic full-body movement of the\nuser according to the transformations of these VR trackers and simulating an\navatar character to mimic such user actions in the virtual world in real-time.\nWe train our system using reinforcement learning with carefully designed\npretraining processes to ensure the success of the training and the quality of\nthe simulation. We demonstrate the effectiveness of the method with an\nextensive set of examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.05753v1"
    },
    {
        "title": "A Transfer Function Design Using A Knowledge Database based on Deep\n  Image and Primitive Intensity Profile Features Retrieval",
        "authors": [
            "Younhyun Jung",
            "Jim Kong",
            "Jinman Kim"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Transfer function (TF) plays a key role for the generation of direct volume\nrendering (DVR), by enabling accurate identification of structures of interest\n(SOIs) interactively as well as ensuring appropriate visibility of them.\nAttempts at mitigating the repetitive manual process of TF design have led to\napproaches that make use of a knowledge database consisting of pre-designed TFs\nby domain experts. In these approaches, a user navigates the knowledge database\nto find the most suitable pre-designed TF for their input volume to visualize\nthe SOIs. Although these approaches potentially reduce the workload to generate\nthe TFs, they, however, require manual TF navigation of the knowledge database,\nas well as the likely fine tuning of the selected TF to suit the input. In this\nwork, we propose a TF design approach where we introduce a new content-based\nretrieval (CBR) to automatically navigate the knowledge database. Instead of\npre-designed TFs, our knowledge database contains image volumes with SOI\nlabels. Given an input image volume, our CBR approach retrieves relevant image\nvolumes (with SOI labels) from the knowledge database; the retrieved labels are\nthen used to generate and optimize TFs of the input. This approach does not\nneed any manual TF navigation and fine tuning. For improving SOI retrieval\nperformance, we propose a two-stage CBR scheme to enable the use of local\nintensity and regional deep image feature representations in a complementary\nmanner. We demonstrate the capabilities of our approach with comparison to a\nconventional CBR approach in visualization, where an intensity profile matching\nalgorithm is used, and also with potential use-cases in medical image volume\nvisualization where DVR plays an indispensable role for different clinical\nusages.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.06421v1"
    },
    {
        "title": "Progressive tearing and cutting of soft-bodies in high-performance\n  virtual reality",
        "authors": [
            "Manos Kamarianakis",
            "Antonis Protopsaltis",
            "Dimitris Angelis",
            "Michail Tamiolakis",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present an algorithm that allows a user within a virtual environment to\nperform real-time unconstrained cuts or consecutive tears, i.e., progressive,\ncontinuous fractures on a deformable rigged and soft-body mesh model in\nhigh-performance 10ms. In order to recreate realistic results for different\nphysically-principled materials such as sponges, hard or soft tissues, we\nincorporate a novel soft-body deformation, via a particle system layered on-top\nof a linear-blend skinning model. Our framework allows the simulation of\nrealistic, surgical-grade cuts and continuous tears, especially valuable in the\ncontext of medical VR training. In order to achieve high performance in VR, our\nalgorithms are based on Euclidean geometric predicates on the rigged mesh,\nwithout requiring any specific model pre-processing. The contribution of this\nwork lies on the fact that current frameworks supporting similar kinds of model\ntearing, either do not operate in high-performance real-time or only apply to\npredefined tears. The framework presented allows the user to freely cut or tear\na 3D mesh model in a consecutive way, under 10ms, while preserving its\nsoft-body behaviour and/or allowing further animation.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.08531v1"
    },
    {
        "title": "MAGES 4.0: Accelerating the world's transition to VR training and\n  democratizing the authoring of the medical metaverse",
        "authors": [
            "Paul Zikas",
            "Antonis Protopsaltis",
            "Nick Lydatakis",
            "Mike Kentros",
            "Stratos Geronikolakis",
            "Steve Kateros",
            "Manos Kamarianakis",
            "Giannis Evangelou",
            "Achilleas Filippidis",
            "Eleni Grigoriou",
            "Dimitris Angelis",
            "Michail Tamiolakis",
            "Michael Dodis",
            "George Kokiadis",
            "John Petropoulos",
            "Maria Pateraki",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this work, we propose MAGES 4.0, a novel Software Development Kit (SDK) to\naccelerate the creation of collaborative medical training applications in\nVR/AR. Our solution is essentially a low-code metaverse authoring platform for\ndevelopers to rapidly prototype high-fidelity and high-complexity medical\nsimulations. MAGES breaks the authoring boundaries across extended reality,\nsince networked participants can also collaborate using different\nvirtual/augmented reality as well as mobile and desktop devices, in the same\nmetaverse world. With MAGES we propose an upgrade to the outdated 150-year-old\nmaster-apprentice medical training model. Our platform incorporates, in a\nnutsell, the following novelties: a) 5G edge-cloud remote rendering and physics\ndissection layer, b) realistic real-time simulation of organic tissues as\nsoft-bodies under 10ms, c) a highly realistic cutting and tearing algorithm, d)\nneural network assessment for user profiling and, e) a VR recorder to record\nand replay or debrief the training simulation from any perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.08819v2"
    },
    {
        "title": "Iterative Poisson Surface Reconstruction (iPSR) for Unoriented Points",
        "authors": [
            "Fei Hou",
            "Chiyu Wang",
            "Wencheng Wang",
            "Hong Qin",
            "Chen Qian",
            "Ying He"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Poisson surface reconstruction (PSR) remains a popular technique for\nreconstructing watertight surfaces from 3D point samples thanks to its\nefficiency, simplicity, and robustness. Yet, the existing PSR method and\nsubsequent variants work only for oriented points. This paper intends to\nvalidate that an improved PSR, called iPSR, can completely eliminate the\nrequirement of point normals and proceed in an iterative manner. In each\niteration, iPSR takes as input point samples with normals directly computed\nfrom the surface obtained in the preceding iteration, and then generates a new\nsurface with better quality. Extensive quantitative evaluation confirms that\nthe new iPSR algorithm converges in 5-30 iterations even with randomly\ninitialized normals. If initialized with a simple visibility based heuristic,\niPSR can further reduce the number of iterations. We conduct comprehensive\ncomparisons with PSR and other powerful implicit-function based methods.\nFinally, we confirm iPSR's effectiveness and scalability on the AIM@SHAPE\ndataset and challenging (indoor and outdoor) scenes. Code and data for this\npaper are at https://github.com/houfei0801/ipsr.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.09510v1"
    },
    {
        "title": "Generating Upper-Body Motion for Real-Time Characters Making their Way\n  through Dynamic Environments",
        "authors": [
            "Eduardo Alvarado",
            "Damien Rohmer",
            "Marie-Paule Cani"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Real-time character animation in dynamic environments requires the generation\nof plausible upper-body movements regardless of the nature of the environment,\nincluding non-rigid obstacles such as vegetation. We propose a flexible model\nfor upper-body interactions, based on the anticipation of the character's\nsurroundings, and on antagonistic controllers to adapt the amount of muscular\nstiffness and response time to better deal with obstacles. Our solution relies\non a hybrid method for character animation that couples a keyframe sequence\nwith kinematic constraints and lightweight physics. The dynamic response of the\ncharacter's upper-limbs leverages antagonistic controllers, allowing us to tune\ntension/relaxation in the upper-body without diverging from the reference\nkeyframe motion. A new sight model, controlled by procedural rules, enables\nhigh-level authoring of the way the character generates interactions by\nadapting its stiffness and reaction time. As results show, our real-time method\noffers precise and explicit control over the character's behavior and style,\nwhile seamlessly adapting to new situations. Our model is therefore well suited\nfor gaming applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.10214v1"
    },
    {
        "title": "Real-Time Locomotion on Soft Grounds With Dynamic Footprints",
        "authors": [
            "Eduardo Alvarado",
            "Chloé Paliard",
            "Damien Rohmer",
            "Marie-Paule Cani"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  When we move on snow, sand, or mud, the ground deforms under our feet,\nimmediately affecting our gait. We propose a physically based model for\ncomputing such interactions in real time, from only the kinematic motion of a\nvirtual character. The force applied by each foot on the ground during contact\nis estimated from the weight of the character, its current balance, the foot\nspeed at the time of contact, and the nature of the ground. We rely on a\nstandard stress-strain relationship to compute the dynamic deformation of the\nsoil under this force, where the amount of compression and lateral displacement\nof material are, respectively, parameterized by the soil's Young modulus and\nPoisson ratio. The resulting footprint is efficiently applied to the terrain\nthrough procedural deformations of refined terrain patches, while the addition\nof a simple controller on top of a kinematic character enables capturing the\neffect of ground deformation on the character's gait. As our results show, the\nresulting footprints greatly improve visual realism, while ground compression\nresults in consistent changes in the character's motion. Readily applicable to\nany locomotion gait and soft soil material, our real-time model is ideal for\nenhancing the visual realism of outdoor scenes in video games and virtual\nreality applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.10215v1"
    },
    {
        "title": "Fluidic Topology Optimization with an Anisotropic Mixture Model",
        "authors": [
            "Yifei Li",
            "Tao Du",
            "Sangeetha Grama Srinivasan",
            "Kui Wu",
            "Bo Zhu",
            "Eftychios Sifakis",
            "Wojciech Matusik"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Fluidic devices are crucial components in many industrial applications\ninvolving fluid mechanics. Computational design of a high-performance fluidic\nsystem faces multifaceted challenges regarding its geometric representation and\nphysical accuracy. We present a novel topology optimization method to design\nfluidic devices in a Stokes flow context. Our approach is featured by its\ncapability in accommodating a broad spectrum of boundary conditions at the\nsolid-fluid interface. Our key contribution is an anisotropic and\ndifferentiable constitutive model that unifies the representation of different\nphases and boundary conditions in a Stokes model, enabling a topology\noptimization method that can synthesize novel structures with accurate boundary\nconditions from a background grid discretization. We demonstrate the efficacy\nof our approach by conducting several fluidic system design tasks with over\nfour million design parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.10736v2"
    },
    {
        "title": "GPU-based Data-parallel Rendering of Large, Unstructured, and\n  Non-convexly Partitioned Data",
        "authors": [
            "Alper Sahistan",
            "Serkan Demirci",
            "Ingo Wald",
            "Stefan Zellmann",
            "João Barbosa",
            "Nathan Morrical",
            "Uğur Güdükbay"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Computational fluid dynamic simulations often produce large clusters of\nfinite elements with non-trivial, non-convex boundaries and uneven\ndistributions among compute nodes, posing challenges to compositing during\ninteractive volume rendering. Correct, in-place visualization of such clusters\nbecomes difficult because viewing rays straddle domain boundaries across\nmultiple compute nodes. We propose a GPU-based, scalable, memory-efficient\ndirect volume visualization framework suitable for in~situ and post~hoc usage.\nOur approach reduces memory usage of the unstructured volume elements by\nleveraging an exclusive or-based index reduction scheme and provides fast\nray-marching-based traversal without requiring large external data structures\nbuilt over the elements themselves. Moreover, we present a GPU-optimized deep\ncompositing scheme that allows correct order compositing of intermediate color\nvalues accumulated across different ranks that works even for non-convex\nclusters. Our method scales well on large data-parallel systems and achieves\ninteractive frame rates during visualization. We can interactively render both\nFun3D Small Mars Lander (14 GB / 798.4 million finite elements) and Huge Mars\nLander (111.57 GB / 6.4 billion finite elements) data sets at 14 and 10 frames\nper second using 72 and 80 GPUs, respectively, on TACC's Frontera\nsupercomputer.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.14537v1"
    },
    {
        "title": "RTSDF: Generating Signed Distance Fields in Real Time for Soft Shadow\n  Rendering",
        "authors": [
            "Yu Wei Tan",
            "Nicholas Chua",
            "Clarence Koh",
            "Anand Bhojan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Signed Distance Fields (SDFs) for surface representation are commonly\ngenerated offline and subsequently loaded into interactive applications like\ngames. Since they are not updated every frame, they only provide a rigid\nsurface representation. While there are methods to generate them quickly on\nGPU, the efficiency of these approaches is limited at high resolutions. This\npaper showcases a novel technique that combines jump flooding and ray tracing\nto generate approximate SDFs in real-time for soft shadow approximation,\nachieving prominent shadow penumbras while maintaining interactive frame rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.04449v1"
    },
    {
        "title": "Hybrid DoF: Ray-Traced and Post-Processed Hybrid Depth of Field Effect\n  for Real-Time Rendering",
        "authors": [
            "Yu Wei Tan",
            "Nicholas Chua",
            "Nathan Biette",
            "Anand Bhojan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Depth of Field (DoF) in games is usually achieved as a post-process effect by\nblurring pixels in the sharp rasterized image based on the defined focus plane.\nThis paper describes a novel real-time DoF technique that uses ray tracing with\nimage filtering to achieve more accurate partial occlusion semi-transparencies\non edges of blurry foreground geometry. This hybrid rendering technique\nleverages ray tracing hardware acceleration as well as spatio-temporal\nreconstruction techniques to achieve interactive frame rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.04981v1"
    },
    {
        "title": "Hybrid MBlur: Using Ray Tracing to Solve the Partial Occlusion Artifacts\n  in Real-Time Rendering of Motion Blur Effect",
        "authors": [
            "Yu Wei Tan",
            "Xiaohan Cui",
            "Anand Bhojan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  For a foreground object in motion, details of its background which would\notherwise be hidden are uncovered through its inner blur. This paper presents a\nnovel hybrid motion blur rendering technique combining post-process image\nfiltering and hardware-accelerated ray tracing. In each frame, we advance rays\nrecursively into the scene to retrieve background information for inner blur\nregions and apply a post-process filtering pass on the ray-traced background\nand rasterized colour before compositing them together. Our approach achieves\nmore accurate partial occlusion semi-transparencies for moving objects while\nmaintaining interactive frame rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.05364v1"
    },
    {
        "title": "Cloud-Assisted Hybrid Rendering for Thin-Client Games and VR\n  Applications",
        "authors": [
            "Yu Wei Tan",
            "Louiz Kim-Chan",
            "Anthony Halim",
            "Anand Bhojan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce a novel distributed rendering approach to generate high-quality\ngraphics in thin-client games and VR applications. Many mobile devices have\nlimited computational power to achieve ray tracing in real-time. Hence,\nhardware-accelerated cloud servers can perform ray tracing instead and have\ntheir output streamed to clients in remote rendering. Applying the approach of\ndistributed hybrid rendering, we leverage the computational capabilities of\nboth the thin client and powerful server by performing rasterization locally\nwhile offloading ray tracing to the server. With advancements in 5G technology,\nthe server and client can communicate effectively over the network and work\ntogether to produce a high-quality output while maintaining interactive frame\nrates. Our approach can achieve better visuals as compared to local rendering\nbut faster performance as compared to remote rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.05365v1"
    },
    {
        "title": "A Hybrid System for Real-time Rendering of Depth of Field Effect in\n  Games",
        "authors": [
            "Yu Wei Tan",
            "Nicholas Chua",
            "Nathan Biette",
            "Anand Bhojan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Real-time depth of field in game cinematics tends to approximate the\nsemi-transparent silhouettes of out-of-focus objects through post-processing\ntechniques. We leverage ray tracing hardware acceleration and spatio-temporal\nreconstruction to improve the realism of such semi-transparent regions through\nhybrid rendering, while maintaining interactive frame rates for immersive\ngaming. This paper extends our previous work with a complete presentation of\nour technique and details on its design, implementation, and future work.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.06158v1"
    },
    {
        "title": "Hybrid MBlur: A Systematic Approach to Augment Rasterization with Ray\n  Tracing for Rendering Motion Blur in Games",
        "authors": [
            "Yu Wei Tan",
            "Xiaohan Cui",
            "Anand Bhojan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Motion blur is commonly used in game cinematics to achieve photorealism by\nmodelling the behaviour of the camera shutter and simulating its effect\nassociated with the relative motion of scene objects. A common real-time\npost-process approach is spatial sampling, where the directional blur of a\nmoving object is rendered by integrating its colour based on velocity\ninformation within a single frame. However, such screen space approaches\ntypically cannot produce accurate partial occlusion semi-transparencies. Our\nreal-time hybrid rendering technique leverages hardware-accelerated ray tracing\nto correct post-process partial occlusion artifacts by advancing rays\nrecursively into the scene to retrieve background information for\nmotion-blurred regions, with reasonable additional performance cost for\nrendering game contents. We extend our previous work with details on the\ndesign, implementation, and future work of the technique as well as performance\ncomparisons with post-processing.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.06159v1"
    },
    {
        "title": "RTSDF: Real-time Signed Distance Fields for Soft Shadow Approximation in\n  Games",
        "authors": [
            "Yu Wei Tan",
            "Nicholas Chua",
            "Clarence Koh",
            "Anand Bhojan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Signed distance fields (SDFs) are a form of surface representation widely\nused in computer graphics, having applications in rendering, collision\ndetection and modelling. In interactive media such as games, high-resolution\nSDFs are commonly produced offline and subsequently loaded into the\napplication, representing rigid meshes only. This work develops a novel\ntechnique that combines jump flooding and ray tracing to generate approximate\nSDFs in real-time. Our approach can produce relatively accurate scene\nrepresentation for rendering soft shadows while maintaining interactive frame\nrates. We extend our previous work with details on the design and\nimplementation as well as visual quality and performance evaluation of the\ntechnique.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.06160v1"
    },
    {
        "title": "Computing Medial Axis Transform with Feature Preservation via Restricted\n  Power Diagram",
        "authors": [
            "Ningna Wang",
            "Bin Wang",
            "Wenping Wang",
            "Xiaohu Guo"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose a novel framework for computing the medial axis transform of 3D\nshapes while preserving their medial features via restricted power diagram\n(RPD). Medial features, including external features such as the sharp edges and\ncorners of the input mesh surface and internal features such as the seams and\njunctions of medial axis, are important shape descriptors both topologically\nand geometrically. However, existing medial axis approximation methods fail to\ncapture and preserve them due to the fundamentally under-sampling in the\nvicinity of medial features, and the difficulty to build their correct\nconnections. In this paper we use the RPD of medial spheres and its affiliated\nstructures to help solve these challenges. The dual structure of RPD provides\nthe connectivity of medial spheres. The surface restricted power cell (RPC) of\neach medial sphere provides the tangential surface regions that these spheres\nhave contact with. The connected components (CC) of surface RPC give us the\nclassification of each sphere, to be on a medial sheet, a seam, or a junction.\nThey allow us to detect insufficient sphere sampling around medial features and\ndevelop necessary conditions to preserve them. Using this RPD-based framework,\nwe are able to construct high quality medial meshes with features preserved.\nCompared with existing sampling-based or voxel-based methods, our method is the\nfirst one that can preserve not only external features but also internal\nfeatures of medial axes.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.13676v1"
    },
    {
        "title": "BSDF Importance Baking: A Lightweight Neural Solution to Importance\n  Sampling General Parametric BSDFs",
        "authors": [
            "Yaoyi Bai",
            "Songyin Wu",
            "Zheng Zeng",
            "Beibei Wang",
            "Ling-Qi Yan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Parametric Bidirectional Scattering Distribution Functions (BSDFs) are\npervasively used because of their flexibility to represent a large variety of\nmaterial appearances by simply tuning the parameters. While efficient\nevaluation of parametric BSDFs has been well-studied, high-quality importance\nsampling techniques for parametric BSDFs are still scarce. Existing sampling\nstrategies either heavily rely on approximations, resulting in high variance,\nor solely perform sampling on a portion of the whole BSDF slice. Moreover, many\nof the sampling approaches are specifically paired with certain types of BSDFs.\nIn this paper, we seek an efficient and general way for importance sampling\nparametric BSDFs. We notice that the nature of importance sampling is the\nmapping between a uniform distribution and the target distribution.\nSpecifically, when BSDF parameters are given, the mapping that performs\nimportance sampling on a BSDF slice can be simply recorded as a 2D image that\nwe name as importance map. Following this observation, we accurately precompute\nthe importance maps using a mathematical tool named optimal transport. Then we\npropose a lightweight neural network to efficiently compress the precomputed\nimportance maps. In this way, we have brought parametric BSDF important\nsampling to the precomputation stage, avoiding heavy runtime computation. Since\nthis process is similar to light baking where a set of images are precomputed,\nwe name our method importance baking. Together with a BSDF evaluation network\nand a PDF (probability density function) query network, our method enables full\nmultiple importance sampling (MIS) without any revision to the rendering\npipeline. Our method essentially performs perfect importance sampling. Compared\nwith previous methods, we demonstrate reduced noise levels on rendering results\nwith a rich set of appearances.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.13681v3"
    },
    {
        "title": "Visually Improved Erosion Algorithm for the Procedural Generation of\n  Tile-based Terrain",
        "authors": [
            "Fong Yuan Lim",
            "Yu Wei Tan",
            "Anand Bhojan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Procedural terrain generation is the process of generating a digital\nrepresentation of terrain using a computer program or procedure, with little to\nno human guidance. This paper proposes a procedural terrain generation\nalgorithm based on a graph representation of fluvial erosion that offers\nseveral novel improvements over existing algorithms. Namely, the use of a\nheight constraint map with two types of locally defined constraint strengths;\nthe ability to specify a realistic erosion strength via level of rainfall; and\nthe ability to carve realistic gorges. These novelties allow it to generate\nmore varied and realistic terrain by integrating additional parameters and\nsimulation processes, while being faster and offering more flexibility and ease\nof use to terrain designers due to the nature and intuitiveness of these new\nparameters and processes. This paper additionally reviews some common metrics\nused to evaluate terrain generators, and suggests a completely new one that\ncontributes to a more holistic evaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.14496v1"
    },
    {
        "title": "DHR: Distributed Hybrid Rendering for Metaverse Experiences",
        "authors": [
            "Yu Wei Tan",
            "Alden Tan",
            "Nicholas Nge",
            "Anand Bhojan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Classically, rasterization techniques are performed for real-time rendering\nto meet the constraint of interactive frame rates. However, such techniques do\nnot produce realistic results as compared to ray tracing approaches. Hence,\nhybrid rendering has emerged to improve the graphics fidelity of rasterization\nwith ray tracing in real-time. We explore the approach of distributed rendering\nin incorporating real-time hybrid rendering into metaverse experiences for\nimmersive graphics. In standalone extended reality (XR) devices, such ray\ntracing-enabled graphics is only feasible through pure cloud-based remote\nrendering systems that rely on low-latency networks to transmit real-time\nray-traced data in response to interactive user input. Under high network\nlatency conditions, remote rendering might not be able to maintain interactive\nframe rates for the client, adversely affecting the user experience. We adopt\nhybrid rendering via a distributed rendering approach by integrating ray\ntracing on powerful remote hardware with raster-based rendering on user access\ndevices. With this hybrid approach, our technique can help standalone XR\ndevices achieve ray tracing-incorporated graphics and maintain interactive\nframe rates even under high-latency conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.15835v1"
    },
    {
        "title": "Decorrelating ReSTIR Samplers via MCMC Mutations",
        "authors": [
            "Rohan Sawhney",
            "Daqi Lin",
            "Markus Kettunen",
            "Benedikt Bitterli",
            "Ravi Ramamoorthi",
            "Chris Wyman",
            "Matt Pharr"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Monte Carlo rendering algorithms often utilize correlations between pixels to\nimprove efficiency and enhance image quality. For real-time applications in\nparticular, repeated reservoir resampling offers a powerful framework to reuse\nsamples both spatially in an image and temporally across multiple frames. While\nsuch techniques achieve equal-error up to 100 times faster for real-time direct\nlighting and global illumination, they are still far from optimal. For\ninstance, unchecked spatiotemporal resampling often introduces noticeable\ncorrelation artifacts, while reservoirs holding more than one sample suffer\nfrom impoverishment in the form of duplicate samples. We demonstrate how\ninterleaving Markov Chain Monte Carlo (MCMC) mutations with reservoir\nresampling helps alleviate these issues, especially in scenes with glossy\nmaterials and difficult-to-sample lighting. Moreover, our approach does not\nintroduce any bias, and in practice we find considerable improvement in image\nquality with just a single mutation per reservoir sample in each frame.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00166v1"
    },
    {
        "title": "Real-Time Character Inverse Kinematics using the Gauss-Seidel Iterative\n  Approximation Method",
        "authors": [
            "Ben Kenwright"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a realistic, robust, and computationally fast method of solving\nhighly non-linear inverse kinematic problems with angular limits using the\nGauss-Seidel iterative method. Our method is ideally suited towards character\nbased interactive applications such as games. To achieve interactive simulation\nspeeds, numerous acceleration techniques are employed, including spatial\ncoherent starting approximations and projected angular clamping. The method has\nbeen tested on a continuous range of poses for animated articulated characters\nand successfully performed in all cases and produced good visual outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00330v1"
    },
    {
        "title": "AmiGo: Computational Design of Amigurumi Crochet Patterns",
        "authors": [
            "Michal Edelstein",
            "Hila Peleg",
            "Shachar Itzhaky",
            "Mirela Ben-Chen"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose an approach for generating crochet instructions (patterns) from an\ninput 3D model. We focus on Amigurumi, which are knitted stuffed toys. Given a\nclosed triangle mesh, and a single point specified by the user, we generate\ncrochet instructions, which when knitted and stuffed result in a toy similar to\nthe input geometry. Our approach relies on constructing the geometry and\nconnectivity of a Crochet Graph, which is then translated into a crochet\npattern. We segment the shape automatically into chrochetable components, which\nare connected using the join-as-you-go method, requiring no additional sewing.\nWe demonstrate that our method is applicable to a large variety of shapes and\ngeometries, and yields easily crochetable patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.01178v1"
    },
    {
        "title": "An Image-Space Split-Rendering Approach to Accelerate Low-Powered\n  Virtual Reality",
        "authors": [
            "Ville Cantory",
            "Nathan Ringo"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Virtual Reality systems provide many opportunities for scientific research\nand consumer enjoyment; however, they are more demanding than traditional\ndesktop applications and require a wired connection to desktops in order to\nenjoy maximum quality. Standalone options that are not connected to computers\nexist, yet they are powered by mobile GPUs, which provide limited power in\ncomparison to desktop rendering. Alternative approaches to improve performance\non mobile devices use server rendering to render frames for a client and treat\nthe client largely as a display device. However, current streaming solutions\nlargely suffer from high end-to-end latency due to processing and networking\nrequirements, as well as underutilization of the client. We propose a networked\nsplit-rendering approach to achieve faster end-to-end image presentation rates\non the mobile device while preserving image quality. Our proposed solution uses\nan image-space division of labour between the server-side GPU and the mobile\nclient, and achieves a significantly faster runtime than client-only rendering\nand than using a thin-client approach, which is mostly reliant on the server.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.02529v2"
    },
    {
        "title": "Integration-free Learning of Flow Maps",
        "authors": [
            "Saroj Sahoo",
            "Matthew Berger"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a method for learning neural representations of flow maps from\ntime-varying vector field data. The flow map is pervasive within the area of\nflow visualization, as it is foundational to numerous visualization techniques,\ne.g. integral curve computation for pathlines or streaklines, as well as\ncomputing separation/attraction structures within the flow field. Yet\nbottlenecks in flow map computation, namely the numerical integration of vector\nfields, can easily inhibit their use within interactive visualization settings.\nIn response, in our work we seek neural representations of flow maps that are\nefficient to evaluate, while remaining scalable to optimize, both in\ncomputation cost and data requirements. A key aspect of our approach is that we\ncan frame the process of representation learning not in optimizing for samples\nof the flow map, but rather, a self-consistency criterion on flow map\nderivatives that eliminates the need for flow map samples, and thus numerical\nintegration, altogether. Central to realizing this is a novel neural network\ndesign for flow maps, coupled with an optimization scheme, wherein our\nrepresentation only requires the time-varying vector field for learning,\nencoded as instantaneous velocity. We show the benefits of our method over\nprior works in terms of accuracy and efficiency across a range of 2D and 3D\ntime-varying vector fields, while showing how our neural representation of flow\nmaps can benefit unsteady flow visualization techniques such as streaklines,\nand the finite-time Lyapunov exponent.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.03192v2"
    },
    {
        "title": "Fast GPU-Based Two-Way Continuous Collision Handling",
        "authors": [
            "Tianyu Wang",
            "Jiong Chen",
            "Dongping Li",
            "Xiaowei Liu",
            "Huamin Wang",
            "Kun Zhou"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Step-and-project is a popular way to simulate non-penetrated deformable\nbodies in physically-based animation. First integrating the system in time\nregardless of contacts and post resolving potential intersections practically\nstrike a good balance between plausibility and efficiency. However, existing\nmethods could be defective and unsafe when the time step is large, taking risks\nof failures or demands of repetitive collision testing and resolving that\nseverely degrade performance. In this paper, we propose a novel two-way method\nfor fast and reliable continuous collision handling. Our method launches the\noptimization at both ends of the intermediate time-integrated state and the\nprevious intersection-free state, progressively generating a piecewise-linear\npath and finally reaching a feasible solution for the next time step.\nTechnically, our method interleaves between a forward step and a backward step\nat a low cost, until the result is conditionally converged. Due to a set of\nunified volume-based contact constraints, our method can flexibly and reliably\nhandle a variety of codimensional deformable bodies, including volumetric\nbodies, cloth, hair and sand. The experiments show that our method is safe,\nrobust, physically faithful and numerically efficient, especially suitable for\nlarge deformations or large time steps.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.04045v1"
    },
    {
        "title": "Scalable multi-class sampling via filtered sliced optimal transport",
        "authors": [
            "Corentin Salaün",
            "Iliyan Georgiev",
            "Hans-Peter Seidel",
            "Gurprit Singh"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose a multi-class point optimization formulation based on continuous\nWasserstein barycenters. Our formulation is designed to handle hundreds to\nthousands of optimization objectives and comes with a practical optimization\nscheme. We demonstrate the effectiveness of our framework on various sampling\napplications like stippling, object placement, and Monte-Carlo integration. We\na derive multi-class error bound for perceptual rendering error which can be\nminimized using our optimization. We provide source code at\nhttps://github.com/iribis/filtered-sliced-optimal-transport.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.04314v1"
    },
    {
        "title": "Up to 58 Tets/Hex to untangle Hex meshes",
        "authors": [
            "Luca Schaller"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The request for high-quality solutions continually grows in a world where\nmore and more tasks are executed through computers. This also counts for fields\nsuch as engineering, computer graphics, etc., which use meshes to solve their\nproblems. A mesh is a combination of some elementary elements, for which\nhexahedral elements are a good choice thanks to their superior numerical\nfeatures. The solutions reached using these meshes depend on the quality of the\nelements making up the mesh. The problem is that these individual elements can\ntake on a shape which prevents accurate computations. Such elements are\nconsidered to be invalid. To allow users to get accurate results, the shape of\nthese elements must therefore be changed to be considered valid. In this work,\nwe combine the results of two papers to scan a mesh, identify possible invalid\nelements and then change the shape of these elements to make them valid. With\nthis combination, we end up with a working algorithm. But there is room for\nimprovement, which is why we introduce multiple improvements to speed up the\nalgorithm as well as make it more robust. We then test our algorithm and\ncompare it to another approach. This work, therefore, introduces a new\nefficient and robust approach to untangle invalid meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.05123v1"
    },
    {
        "title": "Insonification Angle-based Ultrasound Volume Reconstruction for Spine\n  Intervention",
        "authors": [
            "Baichuan Jiang",
            "Keshuai Xu",
            "Abhay Moghekar",
            "Peter Kazanzides",
            "Emad Boctor"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Ultrasound-guided spine interventions, such as lumbar-puncture procedures,\noften suffer from the reduced visibility of key anatomical features such as the\ninter-spinous process space, due to the complex shape of the self-shadowing\nvertebra. Therefore, we propose to design a wearable 3D ultrasound device\ncapable of imaging the vertebra from multiple insonification angles to improve\nthe 3D bone surface visualization for interventional guidance. In this work, we\naim to equip the imaging platform with a reconstruction algorithm taking\nadvantage of the redundant ultrasound beam angles. Specifically, we try to\nweight each beam's contribution for the same reconstructed voxel during the\nreconstruction process based on its incidence angle to the estimated bone\nsurface. To validate our approach, we acquired multi-angle ultrasound image\ndata on a spine phantom with a tracked phased array transducer. The results\nshow that with the proposed method the bone surface contrast can be\nsignificantly enhanced, providing clearer visual guidance for the clinician to\nperform spine intervention.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.05971v1"
    },
    {
        "title": "Optimizing Placements of 360-degree Panoramic Cameras in Indoor\n  Environments by Integer Programming",
        "authors": [
            "Syuan-Rong Syu",
            "Chi-Han Peng"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose a computational approach to find a minimal set of 360-degree\ncamera placements that together sufficiently cover an indoor environment for\nthe building documentation problem in the architecture, engineering, and\nconstruction (AEC) industries. Our approach, based on a simple integer\nprogramming (IP) problem formulation, solves very efficiently and globally\noptimally. We conducted a study of using panoramas to capture the appearances\nof a real-world indoor environment, in which we found that our computed\nsolutions are better than human solutions decided by both non-professional and\nprofessional users.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.07296v1"
    },
    {
        "title": "Floor Plan Exploration Framework Based on Similarity Distances",
        "authors": [
            "Chia-Ying Shih",
            "Chi-Han Peng"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Computational methods to compute similarities between floor plans can help\narchitects explore floor plans in large datasets to avoid duplication of\ndesigns and to search for existing plans that satisfy their needs. Recently,\nLayoutGMN delivered state-of-the-art performance for computing similarity\nscores between floor plans. However, the high computational costs of LayoutGMN\nmake it unsuitable for the aforementioned applications. In this paper, we\nsignificantly reduced the times needed to query results computed by LayoutGMN\nby projecting the floor plans into a common low-dimensional (e.g., three) data\nspace. The projection is done by optimizing for coordinates of floor plans with\nEuclidean distances mimicking their similarity scores originally calculated by\nLayoutGMN. Quantitative and qualitative evaluations show that our results match\nthe distributions of the original LayoutGMN similarity scores. User study shows\nthat our similarity results largely match human expectations.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.07331v1"
    },
    {
        "title": "Regression-based Monte Carlo Integration",
        "authors": [
            "Corentin Salaün",
            "Adrien Gruson",
            "Binh-Son Hua",
            "Toshiya Hachisuka",
            "Gurprit Singh"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Monte Carlo integration is typically interpreted as an estimator of the\nexpected value using stochastic samples. There exists an alternative\ninterpretation in calculus where Monte Carlo integration can be seen as\nestimating a \\emph{constant} function -- from the stochastic evaluations of the\nintegrand -- that integrates to the original integral. The integral mean value\ntheorem states that this \\emph{constant} function should be the mean (or\nexpectation) of the integrand. Since both interpretations result in the same\nestimator, little attention has been devoted to the calculus-oriented\ninterpretation. We show that the calculus-oriented interpretation actually\nimplies the possibility of using a more \\emph{complex} function than a\n\\emph{constant} one to construct a more efficient estimator for Monte Carlo\nintegration. We build a new estimator based on this interpretation and relate\nour estimator to control variates with least-squares regression on the\nstochastic samples of the integrand. Unlike prior work, our resulting estimator\nis \\emph{provably} better than or equal to the conventional Monte Carlo\nestimator. To demonstrate the strength of our approach, we introduce a\npractical estimator that can act as a simple drop-in replacement for\nconventional Monte Carlo integration. We experimentally validate our framework\non various light transport integrals. The code is available at\n\\url{https://github.com/iribis/regressionmc}.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.07422v1"
    },
    {
        "title": "Foveated Rendering: a State-of-the-Art Survey",
        "authors": [
            "Lili Wang",
            "Xuehuai Shi",
            "Yi Liu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Recently, virtual reality (VR) technology has been widely used in medical,\nmilitary, manufacturing, entertainment, and other fields.\n  These applications must simulate different complex material surfaces, various\ndynamic objects, and complex physical phenomena, increasing the complexity of\nVR scenes. Current computing devices cannot efficiently render these complex\nscenes in real time, and delayed rendering makes the content observed by the\nuser inconsistent with the user's interaction, causing discomfort.\n  Foveated rendering is a promising technique that can accelerate rendering. It\ntakes advantage of human eyes' inherent features and renders different regions\nwith different qualities without sacrificing perceived visual quality.\n  Foveated rendering research has a history of 31 years and is mainly focused\non solving the following three problems.\n  The first is to apply perceptual models of the human visual system into\nfoveated rendering. The second is to render the image with different qualities\naccording to foveation principles. The third is to integrate foveated rendering\ninto existing rendering paradigms to improve rendering performance.\n  In this survey, we review foveated rendering research from 1990 to 2021.\n  We first revisit the visual perceptual models related to foveated rendering.\n  Subsequently, we propose a new foveated rendering taxonomy and then classify\nand review the research on this basis. Finally, we discuss potential\nopportunities and open questions in the foveated rendering field.\n  We anticipate that this survey will provide new researchers with a high-level\noverview of the state of the art in this field, furnish experts with up-to-date\ninformation and offer ideas alongside a framework to VR display software and\nhardware designers and engineers.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.07969v1"
    },
    {
        "title": "Beyond ExaBricks: GPU Volume Path Tracing of AMR Data",
        "authors": [
            "Stefan Zellmann",
            "Qi Wu",
            "Alper Sahistan",
            "Kwan-Liu Ma",
            "Ingo Wald"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Adaptive Mesh Refinement (AMR) is becoming a prevalent data representation\nfor scientific visualization. Resulting from large fluid mechanics simulations,\nthe data is usually cell centric, imposing a number of challenges for high\nquality reconstruction at sample positions. While recent work has concentrated\non real-time volume and isosurface rendering on GPUs, the rendering methods\nused still focus on simple lighting models without scattering events and global\nillumination. As in other areas of rendering, key to real-time performance are\nacceleration data structures; in this work we analyze the major bottlenecks of\ndata structures that were originally optimized for camera/primary ray traversal\nwhen used with the incoherent ray tracing workload of a volumetric path tracer,\nand propose strategies to overcome the challenges coming with this.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.09997v2"
    },
    {
        "title": "Implicit frictional dynamics with soft constraints",
        "authors": [
            "Egor Larionov",
            "Andreas Longva",
            "Uri M. Ascher",
            "Jan Bender",
            "Dinesh K. Pai"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Dynamics simulation with frictional contacts is important for a wide range of\napplications, from cloth simulation to object manipulation. Recent methods\nusing smoothed lagged friction forces have enabled robust and differentiable\nsimulation of elastodynamics with friction. However, the resulting frictional\nbehavior can be inaccurate and may not converge to analytic solutions. Here we\nevaluate the accuracy of lagged friction models in comparison with implicit\nfrictional contact systems. We show that major inaccuracies near the stick-slip\nthreshold in such systems are caused by lagging of friction forces rather than\nby smoothing the Coulomb friction curve. Furthermore, we demonstrate how\nsystems involving implicit or lagged friction can be correctly used with\nhigher-order time integration and highlight limitations in earlier attempts. We\ndemonstrate how to exploit forward-mode automatic differentiation to simplify\nand, in some cases, improve the performance of the inexact Newton method.\nFinally, we show that other complex phenomena can also be simulated effectively\nwhile maintaining smoothness of the entire system. We extend our method to\nexhibit stick-slip frictional behavior and preserve volume on compressible and\nnearly-incompressible media using soft constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.10618v3"
    },
    {
        "title": "Fairing-PIA: Progressive iterative approximation for fairing curve and\n  surface generation",
        "authors": [
            "Yini Jiang",
            "Hongwei Lin"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The fairing curves and surfaces are used extensively in geometric design,\nmodeling, and industrial manufacturing. However, the majority of conventional\nfairing approaches, which lack sufficient parameters to improve fairness, are\nbased on energy minimization problems. In this study, we develop a novel\nprogressive-iterative approximation method for fairing curve and surface\ngeneration (fairing-PIA). Fairing-PIA is an iteration method that can generate\na series of curves (surfaces) by adjusting the control points of B-spline\ncurves (surfaces). In fairing-PIA, each control point is endowed with an\nindividual weight. Thus, the fairing-PIA has many parameters to optimize the\nshapes of curves and surfaces. Not only a fairing curve (surface) can be\ngenerated globally through fairing-PIA, but also the curve (surface) can be\nimproved locally. Moreover, we prove the convergence of the developed\nfairing-PIA and show that the conventional energy minimization fairing model is\na special case of fairing-PIA. Finally, numerical examples indicate that the\nproposed method is effective and efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.11416v1"
    },
    {
        "title": "Fragment-History Volumes",
        "authors": [
            "Francisco Inácio",
            "Jan P. Springer"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Hardware-based triangle rasterization is still the prevalent method for\ngenerating images at real-time interactive frame rates. With the availability\nof a programmable graphics pipeline a large variety of techniques are supported\nfor evaluating lighting and material properties of fragments. However, these\ntechniques are usually restricted to evaluating local lighting and material\neffects. In addition, view-point changes require the complete processing of\nscene data to generate appropriate images. Reusing already rendered data in the\nframe buffer for a given view point by warping for a new viewpoint increases\nnavigation fidelity at the expense of introducing artifacts for fragments\npreviously hidden from the viewer.\n  We present fragment-history volumes (FHV), a rendering technique based on a\nsparse, discretized representation of a 3d scene that emerges from recording\nall fragments that pass the rasterization stage in the graphics pipeline. These\nfragments are stored into per-pixel or per-octant lists for further processing;\nessentially creating an A-buffer. FHVs using per-octant fragment lists are view\nindependent and allow fast resampling for image generation as well as for using\nmore sophisticated approaches to evaluate material and lighting properties,\neventually enabling global-illumination evaluation in the standard graphics\npipeline available on current hardware.\n  We show how FHVs are stored on the GPU in several ways, how they are created,\nand how they can be used for image generation at high rates. We discuss results\nfor different usage scenarios, variations of the technique, and some\nlimitations.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.15460v1"
    },
    {
        "title": "RFEPS: Reconstructing Feature-line Equipped Polygonal Surface",
        "authors": [
            "Rui Xu",
            "Zixiong Wang",
            "Zhiyang Dou",
            "Chen Zong",
            "Shiqing Xin",
            "Mingyan Jiang",
            "Tao Ju",
            "Changhe Tu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Feature lines are important geometric cues in characterizing the structure of\na CAD model. Despite great progress in both explicit reconstruction and\nimplicit reconstruction, it remains a challenging task to reconstruct a\npolygonal surface equipped with feature lines, especially when the input point\ncloud is noisy and lacks faithful normal vectors. In this paper, we develop a\nmultistage algorithm, named RFEPS, to address this challenge. The key steps\ninclude (1)denoising the point cloud based on the assumption of local\nplanarity, (2)identifying the feature-line zone by optimization of discrete\noptimal transport, (3)augmenting the point set so that sufficiently many\nadditional points are generated on potential geometry edges, and (4) generating\na polygonal surface that interpolates the augmented point set based on\nrestricted power diagram. We demonstrate through extensive experiments that\nRFEPS, benefiting from the edge-point augmentation and the feature-preserving\nexplicit reconstruction, outperforms state-of-the-art methods in terms of the\nreconstruction quality, especially in terms of the ability to reconstruct\nmissing feature lines.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.03600v1"
    },
    {
        "title": "Online Heatmap Generation with Both High and Low Weights",
        "authors": [
            "Yan Y. Liu",
            "Melissa Allen-Dumas"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Heatmap is a common geovisualization method that interpolates and visualizes\na set of point observations on a map surface. Most of online web mapping\nlibraries implement a one-pass heatmap algorithm using HTML5 canvas or WebGL\nfor efficient heatmap generation. However, such implementation applies additive\noperations that accumulate the rendering of point weights on the map surface\ngrid, making it inappropriate for visualizations that require the highlighting\nof both low and high weights. We introduce \\textit{hilomap}, an online heatmap\nalgorithm that highlights surface areas where points with both low and high\ntrends are located. An HTML5 Canvas-based reference implementation on\nOpenLayers is presented and evaluated.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.07820v1"
    },
    {
        "title": "Estimating Cloth Elasticity Parameters Using Position-Based Simulation\n  of Compliant Constrained Dynamics",
        "authors": [
            "Egor Larionov",
            "Marie-Lena Eckert",
            "Katja Wolff",
            "Tuur Stuyck"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Clothing plays a vital role in real life and hence, is also important for\nvirtual realities and virtual applications, such as online retail, virtual\ntry-on, and real-time digital avatar interactions. However, choosing the\ncorrect parameters to generate realistic clothing requires expert knowledge and\nis often an arduous manual process. To alleviate this issue, we develop a\npipeline for automatically determining the static material parameters required\nto simulate clothing of a particular material based on easily captured\nreal-world fabrics. We use differentiable simulation to find an optimal set of\nparameters that minimizes the difference between simulated cloth and deformed\ntarget cloth. Our novel well-suited loss function is optimized through\nnon-linear least squares. We designed our objective function to capture\nmaterial-specific behavior, resulting in similar values for different wrinkle\nconfigurations of the same material. While existing methods carefully design\nexperiments to isolate stretch parameters from bending modes, we embrace that\nstretching fabrics causes wrinkling. We estimate bending first, given that\nmembrane stiffness has little effect on bending. Furthermore, our pipeline\ndecouples the capture method from the optimization by registering a template\nmesh to the scanned data. These choices simplify the capture system and allow\nfor wrinkles in scanned fabrics. We use a differentiable extended\nposition-based dynamics (XPBD) cloth simulator, which is capable of real-time\nsimulation. We demonstrate our method on captured data of three different\nreal-world fabrics and on three digital fabrics produced by a third-party\nsimulator.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.08790v1"
    },
    {
        "title": "Real-Time Rendering of Arbitrary Surface Geometries using Learnt\n  Transfer",
        "authors": [
            "Sirikonda Dhawal",
            "Aakash KT",
            "P. J. Narayanan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Precomputed Radiance Transfer (PRT) is widely used for real-time\nphotorealistic effects. PRT disentangles the rendering equation into transfer\nand lighting, enabling their precomputation. Transfer accounts for the\ncosine-weighted visibility of points in the scene while lighting for emitted\nradiance from the environment. Prior art stored precomputed transfer in a\ntabulated manner, either in vertex or texture space. These values are fetched\nwith interpolation at each point for shading. Vertex space methods require\ndensely tessellated mesh vertices for high quality images. Texture space\nmethods require non-overlapping and area-preserving UV mapping to be available.\nThey also require a high-resolution texture to avoid rendering artifacts. In\nthis paper, we propose a compact transfer representation that is learnt\ndirectly on scene geometry points. Specifically, we train a small multi-layer\nperceptron (MLP) to predict the transfer at sampled surface points. Our\napproach is most beneficial where inherent mesh storage structure and natural\nUV mapping are not available, such as Implicit Surfaces as it learns the\ntransfer values directly on the surface. We demonstrate real-time,\nphotorealistic renderings of diffuse and glossy materials on SDF geometries\nwith PRT using our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.09315v1"
    },
    {
        "title": "Analysis and Compilation of Normal Map Generation Techniques for Pixel\n  Art",
        "authors": [
            "Rodrigo D. Moreira",
            "Flávio Coutinho",
            "Luiz Chaimowicz"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Pixel art is a popular artistic style adopted in the gaming industry, and\nnowadays, it is often accompanied by modern rendering techniques. One example\nis dynamic lighting for the game sprites, for which normal mapping defines how\nthe light interacts with the material represented by each pixel. Although there\nare different methods to generate normal maps for 3D games, applying them for\npixel art may not yield correct results due to the style specificities.\nTherefore, this work compiles different normal map generation methods and study\ntheir applicability for pixel art, reducing the scarcity of existing material\non the techniques and contributing to a qualitative analysis of the behavior of\nthese methods in different case studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.09692v1"
    },
    {
        "title": "360° Stereo Image Composition with Depth Adaption",
        "authors": [
            "Kun Huang",
            "Fanglue Zhang",
            "Junhong Zhao",
            "Yiheng Li",
            "Neil Dodgson"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  360{\\deg} images and videos have become an economic and popular way to\nprovide VR experiences using real-world content. However, the manipulation of\nthe stereo panoramic content remains less explored. In this paper, we focus on\nthe 360{\\deg} image composition problem, and develop a solution that can take\nan object from a stereo image pair and insert it at a given 3D position in a\ntarget stereo panorama, with well-preserved geometry information. Our method\nuses recovered 3D point clouds to guide the composited image generation. More\nspecifically, we observe that using only a one-off operation to insert objects\ninto equirectangular images will never produce satisfactory depth perception\nand generate ghost artifacts when users are watching the result from different\nview directions. Therefore, we propose a novel per-view projection method that\nsegments the object in 3D spherical space with the stereo camera pair facing in\nthat direction. A deep depth densification network is proposed to generate\ndepth guidance for the stereo image generation of each view segment according\nto the desired position and pose of the inserted object. We finally combine the\nsynthesized view segments and blend the objects into the target stereo\n360{\\deg} scene. A user study demonstrates that our method can provide good\ndepth perception and removes ghost artifacts. The per-view solution is a\npotential paradigm for other content manipulation methods for 360{\\deg} images\nand videos.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.10062v2"
    },
    {
        "title": "Scaffolding Generation using a 3D Physarum Polycephalum Simulation",
        "authors": [
            "Drew Ehrlich",
            "Milad Hakimshafaei",
            "Oskar Elek"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this demo, we present a novel technique for approximating topologically\noptimal scaffoldings for 3D printed objects using a Monte Carlo algorithm based\non the foraging behavior of the Physarum polycephalum slime mold. As a case\nstudy, we have created a biologically inspired bicycle helmet using this\ntechnique that is designed to be effective in resisting impacts. We have\ncreated a prototype of this helmet and propose further studies that measure the\neffectiveness and validity of the design.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.11527v1"
    },
    {
        "title": "GeoCode: Interpretable Shape Programs",
        "authors": [
            "Ofek Pearl",
            "Itai Lang",
            "Yuhua Hu",
            "Raymond A. Yeh",
            "Rana Hanocka"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Mapping high-fidelity 3D geometry to a representation that allows for\nintuitive edits remains an elusive goal in computer vision and graphics. The\nkey challenge is the need to model both continuous and discrete shape\nvariations. Current approaches, such as implicit shape representation, lack\nstraightforward interpretable encoding, while others that employ procedural\nmethods output coarse geometry. We present GeoCode, a technique for 3D shape\nsynthesis using an intuitively editable parameter space. We build a novel\nprogram that enforces a complex set of rules and enables users to perform\nintuitive and controlled high-level edits that procedurally propagate at a low\nlevel to the entire shape. Our program produces high-quality mesh outputs by\nconstruction. We use a neural network to map a given point cloud or sketch to\nour interpretable parameter space. Once produced by our procedural program,\nshapes can be easily modified. Empirically, we show that GeoCode can infer and\nrecover 3D shapes more accurately compared to existing techniques and we\ndemonstrate its ability to perform controlled local and global shape\nmanipulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.11715v1"
    },
    {
        "title": "S-patch: Modification of the Hermite parametric patch",
        "authors": [
            "Vaclav Skala",
            "Vit Ondracka"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  A new modification of the Hermite cubic rectangular patch is proposed: the\nS-Patch, which is based on the requirement that diagonal curves must be of\ndegree 3 instead of degree 6 as it is in the case of the Hermite patch.\nTheoretical derivation of conditions is presented and some experimental results\nas well. The S-Patch is convenient for applications, where different\ntessellation of the u-v domain is needed, boundary and diagonal curves of\ndifferent degrees are not acceptable.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.11875v1"
    },
    {
        "title": "BS-Patch: Constrained Bezier Parametric Patch",
        "authors": [
            "Vaclav Skala",
            "Vit Ondracka"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Bezier parametric patches are used in engineering practice quite often,\nespecially in CAD/CAM systems oriented to mechanical design. In many cases\nquadrilateral meshes are used for tessellation of parameters domain. We propose\na new modification of the Bezier cubic rectangular patch, the BS-patch, which\nis based on the requirement that diagonal curves must be of degree 3 instead of\ndegree 6 as it is in the case of the Bezier patch. Theoretical derivation of\nconditions is presented and some experimental results as well. The BS-Patch is\nconvenient for applications where for different tessellation of the u-v domain\ndifferent degrees of diagonal curves are not acceptable.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.11986v1"
    },
    {
        "title": "Demo: New View on Plasma Fractals -- From the High Point of Array\n  Languages",
        "authors": [
            "Oleg Kiselyov",
            "Toshihiro Nakayama"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Plasma fractals is a technique to generate random and realistic clouds,\ntextures and terrains~-- traditionally using recursive subdivision. We\ndemonstrate a new approach, based on iterative expansion. It gives a family of\nalgorithms that includes the standard square-diamond algorithm and offers\nvarious interesting ways of extending it, and hence generating nicer pictures.\nThe approach came about from exploring plasma fractals from the point of view\nof an array language (which we implemented as an embedded DSL in OCaml)~-- that\nis, from the perspective of declaring whole image transformations rather than\nfiddling with individual pixels.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.12502v1"
    },
    {
        "title": "HS-Patch: A New Hermite Smart Bicubic Patch Modification",
        "authors": [
            "Vaclav Skala",
            "Michal Smolik",
            "Lukas Karlicek"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Bicubic four-sided patches are widely used in computer graphics, CAD/CAM\nsystems etc. Their flexibility is high and enables to compress a surface\ndescription before final rendering. However, computer graphics hardware\nsupports only triangular meshes. Therefore, four-sided bicubic patches are\napproximated by a triangular mesh. The border curves of a bicubic patch are of\ndegree 3, while diagonal and anti-diagonal curves are of degree 6. Therefore\nthe resulting shape and texturing depend on the actual mapping, i.e. how the\ntessellation of a bicubic patch is made. The proposed new modification of the\nHermite bicubic patch, the HS-patch, is a result of additional restriction put\non the Hermite bicubic patch formulation - the diagonal and anti-diagonal\ncurves are of degree 3. This requirement leads to a new Hermite based bicubic\nfour-sided patch with 12 control points and another 4 control points, i.e.\ntwist vectors, are computed from those 12 control points.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.12859v1"
    },
    {
        "title": "CIMS: Correction-Interpolation Method for Smoke Simulation",
        "authors": [
            "Yunjee Lee",
            "Dohae Lee",
            "Young Jin Oh",
            "In-Kwon Lee"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this paper, we propose CIMS: a novel correction-interpolation method for\nsmoke simulation. The basis of our method is to first generate a low frame rate\nsmoke simulation, then increase the frame rate using temporal interpolation.\nHowever, low frame rate smoke simulations are inaccurate as they require\nincreasing the time-step. A simulation with a larger time-step produces results\ndifferent from that of the original simulation with a small time-step.\nTherefore, the proposed method corrects the large time-step simulation results\ncloser to the corresponding small time-step simulation results using a\nU-Net-based DNN model. To obtain more precise results, we applied modeling\nconcepts used in the image domain, such as optical flow and perceptual loss. By\ncorrecting the large time-step simulation results and interpolating between\nthem, the proposed method can efficiently and accurately generate high frame\nrate smoke simulations. We conduct qualitative and quantitative analyses to\nconfirm the effectiveness of the proposed model. Our analyses show that our\nmethod reduces the mean squared error of large time-step simulation results by\nmore than 80% on average. Our method also produces results closer to the ground\ntruth than the previous DNN-based methods; it is on average 2.04 times more\naccurate than previous works. In addition, the computation time of the proposed\ncorrection method barely affects the overall computation time.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.14716v1"
    },
    {
        "title": "Neural Volumetric Blendshapes: Computationally Efficient Physics-Based\n  Facial Blendshapes",
        "authors": [
            "Nicolas Wagner",
            "Ulrich Schwanecke",
            "Mario Botsch"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Computationally weak systems and demanding graphical applications are still\nmostly dependent on linear blendshapes for facial animations. The accompanying\nartifacts such as self-intersections, loss of volume, or missing soft tissue\nelasticity can be avoided by using physics-based animation models. However,\nthese are cumbersome to implement and require immense computational effort. We\npropose neural volumetric blendshapes, an approach that combines the advantages\nof physics-based simulations with realtime runtimes even on consumer-grade\nCPUs. To this end, we present a neural network that efficiently approximates\nthe involved volumetric simulations and generalizes across human identities as\nwell as facial expressions. Our approach can be used on top of any linear\nblendshape system and, hence, can be deployed straightforwardly. Furthermore,\nit only requires a single neutral face mesh as input in the minimal setting.\nAlong with the design of the network, we introduce a pipeline for the\nchallenging creation of anatomically and physically plausible training data.\nPart of the pipeline is a novel hybrid regressor that densely positions a skull\nwithin a skin surface while avoiding intersections. The fidelity of all parts\nof the data generation pipeline as well as the accuracy and efficiency of the\nnetwork are evaluated in this work. Upon publication, the trained models and\nassociated code will be released.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.14784v2"
    },
    {
        "title": "LM-GAN: A Photorealistic All-Weather Parametric Sky Model",
        "authors": [
            "Lucas Valença",
            "Ian Maquignaz",
            "Hadi Moazen",
            "Rishikesh Madan",
            "Yannick Hold-Geoffroy",
            "Jean-François Lalonde"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present LM-GAN, an HDR sky model that generates photorealistic environment\nmaps with weathered skies. Our sky model retains the flexibility of traditional\nparametric models and enables the reproduction of photorealistic all-weather\nskies with visual diversity in cloud formations. This is achieved with flexible\nand intuitive user controls for parameters, including sun position, sky color,\nand atmospheric turbidity.\n  Our method is trained directly from inputs fitted to real HDR skies, learning\nboth to preserve the input's illumination and correlate it to the real\nreference's atmospheric components in an end-to-end manner. Our main\ncontributions are a generative model trained on both sky appearance and scene\nrendering losses, as well as a novel sky-parameter fitting algorithm. We\ndemonstrate that our fitting algorithm surpasses existing approaches in both\naccuracy and sky fidelity, and also provide quantitative and qualitative\nanalyses, demonstrating LM-GAN's ability to match parametric input to\nphotorealistic all-weather skies. The generated HDR environment maps are ready\nto use in 3D rendering engines and can be applied to a wide range of\nimage-based lighting applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.00087v1"
    },
    {
        "title": "Automatic inference of a anatomically meaningful solid wood texture from\n  a single photograph",
        "authors": [
            "Thomas K. Nindel",
            "Mohcen Hafidi",
            "Tomáš Iser",
            "Alexander Wilkie"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Wood is a volumetric material with a very large appearance gamut that is\nfurther enlarged by numerous finishing techniques. Computer graphics has made\nconsiderable progress in creating sophisticated and flexible appearance models\nthat allow convincing renderings of wooden materials.\n  However, these do not yet allow fully automatic appearance matching to a\nconcrete exemplar piece of wood, and have to be fine-tuned by hand. More\ngeneral appearance matching strategies are incapable of reconstructing\nanatomically meaningful volumetric information. This is essential for\napplications where the internal structure of wood is significant, such as\nnon-planar furniture parts machined from a solid block of wood, translucent\nappearance of thin wooden layers, or in the field of dendrochronology.\n  In this paper, we provide the two key ingredients for automatic matching of a\nprocedural wood appearance model to exemplar photographs: a good\ninitialization, built on detecting and modelling the ring structure, and a\nphase-based loss function that allows to accurately recover growth ring\ndeformations and gives anatomically meaningful results.\n  Our ring-detection technique is based on curved Gabor filters, and robustly\nworks for a considerable range of wood types.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.01820v1"
    },
    {
        "title": "Multiple-bounce Smith Microfacet BRDFs using the Invariance Principle",
        "authors": [
            "Yuang Cui",
            "Gaole Pan",
            "Jian Yang",
            "Lei Zhang",
            "Ling-qi Yan",
            "Beibei Wang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Smith microfacet models are widely used in computer graphics to represent\nmaterials. Traditional microfacet models do not consider the multiple bounces\non microgeometries, leading to visible energy missing, especially on rough\nsurfaces. Later, as the equivalence between the microfacets and volume has been\nrevealed, random walk solutions have been proposed to introduce multiple\nbounces, but at the cost of high variance. Recently, the position-free property\nhas been introduced into the multiple-bounce model, resulting in much less\nnoise, but also bias or a complex derivation. In this paper, we propose a\nsimple way to derive the multiple-bounce Smith microfacet bidirectional\nreflectance distribution functions (BRDFs) using the invariance principle. At\nthe core of our model is a shadowing-masking function for a path consisting of\ndirection collections, rather than separated bounces. Our model ensures\nunbiasedness and can produce less noise compared to the previous work with\nequal time, thanks to the simple formulation. Furthermore, we also propose a\nnovel probability density function (PDF) for BRDF multiple importance sampling,\nwhich has a better match with the multiple-bounce BRDFs, producing less noise\nthan previous naive approximations.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.03408v3"
    },
    {
        "title": "In-the-wild Material Appearance Editing using Perceptual Attributes",
        "authors": [
            "J. Daniel Subias",
            "Manuel Lagunas"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Intuitively editing the appearance of materials from a single image is a\nchallenging task given the complexity of the interactions between light and\nmatter, and the ambivalence of human perception. This problem has been\ntraditionally addressed by estimating additional factors of the scene like\ngeometry or illumination, thus solving an inverse rendering problem and\nsubduing the final quality of the results to the quality of these estimations.\nWe present a single-image appearance editing framework that allows us to\nintuitively modify the material appearance of an object by increasing or\ndecreasing high-level perceptual attributes describing such appearance (e.g.,\nglossy or metallic). Our framework takes as input an in-the-wild image of a\nsingle object, where geometry, material, and illumination are not controlled,\nand inverse rendering is not required. We rely on generative models and devise\na novel architecture with Selective Transfer Unit (STU) cells that allow to\npreserve the high-frequency details from the input image in the edited one. To\ntrain our framework we leverage a dataset with pairs of synthetic images\nrendered with physically-based algorithms, and the corresponding crowd-sourced\nratings of high-level perceptual attributes. We show that our material editing\nframework outperforms the state of the art, and showcase its applicability on\nsynthetic images, in-the-wild real-world photographs, and video sequences.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.03619v2"
    },
    {
        "title": "Accurate and Interpretable Solution of the Inverse Rig for Realistic\n  Blendshape Models with Quadratic Corrective Terms",
        "authors": [
            "Stevo Racković",
            "Cláudia Soares",
            "Dušan Jakovetić",
            "Zoranka Desnica"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a new model-based algorithm solving the inverse rig problem in\nfacial animation retargeting, exhibiting higher accuracy of the fit and\nsparser, more interpretable weight vector compared to SOTA. The proposed method\ntargets a specific subdomain of human face animation - highly-realistic\nblendshape models used in the production of movies and video games. In this\npaper, we formulate an optimization problem that takes into account all the\nrequirements of targeted models. Our objective goes beyond a linear blendshape\nmodel and employs the quadratic corrective terms necessary for correctly\nfitting fine details of the mesh. We show that the solution to the proposed\nproblem yields highly accurate mesh reconstruction even when general-purpose\nsolvers, like SQP, are used. The results obtained using SQP are highly accurate\nin the mesh space but do not exhibit favorable qualities in terms of weight\nsparsity and smoothness, and for this reason, we further propose a novel\nalgorithm relying on a MM technique. The algorithm is specifically suited for\nsolving the proposed objective, yielding a high-accuracy mesh fit while\nrespecting the constraints and producing a sparse and smooth set of weights\neasy to manipulate and interpret by artists. Our algorithm is benchmarked with\nSOTA approaches, and shows an overall superiority of the results, yielding a\nsmooth animation reconstruction with a relative improvement up to 45 percent in\nroot mean squared mesh error while keeping the cardinality comparable with\nbenchmark methods. This paper gives a comprehensive set of evaluation metrics\nthat cover different aspects of the solution, including mesh accuracy, sparsity\nof the weights, and smoothness of the animation curves, as well as the\nappearance of the produced animation, which human experts evaluated.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.04843v2"
    },
    {
        "title": "FastPoints: A State-of-the-Art Point Cloud Renderer for Unity",
        "authors": [
            "Elias Neuman-Donihue",
            "Michael Jarvis",
            "Yuhao Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we introduce FastPoints, a state-of-the-art point cloud\nrenderer for the Unity game development platform. Our program supports standard\nunprocessed point cloud formats with non-programmatic, drag-and-drop support,\nand creates an out-of-core data structure for large clouds without requiring an\nexplicit preprocessing step; instead, the software renders a decimated point\ncloud immediately and constructs a shallow octree online, during which time the\nUnity editor remains fully interactive.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.05002v1"
    },
    {
        "title": "Dynamic Simulation of Splashing Fluids",
        "authors": [
            "James F. O'Brien",
            "Jessica K. Hodgins"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper we describe a method for modeling the dynamic behavior of\nsplashing fluids. The model simulates the behavior of a fluid when objects\nimpact or float on its surface. The forces generated by the objects create\nwaves and splashes on the surface of the fluid. To demonstrate the realism and\nlimitations of the model, images from a computer-generated animation are\npresented and compared with video frames of actual splashes occurring under\nsimilar initial conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.06087v1"
    },
    {
        "title": "Animating Human Athletics",
        "authors": [
            "Jessica K. Hodgins",
            "Wayne L. Wooten",
            "David C. Brogan",
            "James F. O'Brien"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper describes algorithms for the animation of men and women performing\nthree dynamic athletic behaviors: running, bicycling, and vaulting. We animate\nthese behaviors using control algorithms that cause a physically realistic\nmodel to perform the desired maneuver. For example, control algorithms allow\nthe simulated humans to maintain balance while moving their arms, to run or\nbicycle at a variety of speeds, and to perform a handspring vault. Algorithms\nfor group behaviors allow a number of simulated bicyclists to ride as a group\nwhile avoiding simple patterns of obstacles. We add secondary motion to the\nanimations with spring-mass simulations of clothing driven by the rigid-body\nmotion of the simulated human. For each simulation, we compare the computed\nmotion to that of humans performing similar maneuvers both qualitatively\nthrough the comparison of real and simulated video images and quantitatively\nthrough the comparison of simulated and biomechanical data.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.06108v1"
    },
    {
        "title": "Fast Real-Time Shading for Polygonal Hair",
        "authors": [
            "Martin Gerard"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Though a lot of improvement has been made to hair rendering techniques in the\nrecent years, realistic rendering of hair remains a challenge, especially in\nreal time. In this paper, we propose a fast technique to approximate the\nshading of hair lighted by an environment map, direct lighting or a global\nillumination system, without having to render deep opacity maps or requiring\nadditional artistic work.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.06468v1"
    },
    {
        "title": "Perception of Human Motion with Different Geometric Models",
        "authors": [
            "Jessica K. Hodgins",
            "James F. O'Brien",
            "Jack Tumblin"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Human figures have been animated using a variety of geometric models\nincluding stick figures, polygonal models, and NURBS-based models with muscles,\nflexible skin, or clothing. This paper reports on experimental results\nindicating that a viewer's perception of motion characteristics is affected by\nthe geometric model used for rendering. Subjects were shown a series of paired\nmotion sequences and asked if the two motions in each pair were the same or\ndifferent. The motion sequences in each pair were rendered using the same\ngeometric model. For the three types of motion variation tested, sensitivity\nscores indicate that subjects were better able to observe changes with the\npolygonal model than they were with the stick figure model.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.07489v2"
    },
    {
        "title": "LiveHand: Real-time and Photorealistic Neural Hand Rendering",
        "authors": [
            "Akshay Mundra",
            "Mallikarjun B R",
            "Jiayi Wang",
            "Marc Habermann",
            "Christian Theobalt",
            "Mohamed Elgharib"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The human hand is the main medium through which we interact with our\nsurroundings, making its digitization an important problem. While there are\nseveral works modeling the geometry of hands, little attention has been paid to\ncapturing photo-realistic appearance. Moreover, for applications in extended\nreality and gaming, real-time rendering is critical. We present the first\nneural-implicit approach to photo-realistically render hands in real-time. This\nis a challenging problem as hands are textured and undergo strong articulations\nwith pose-dependent effects. However, we show that this aim is achievable\nthrough our carefully designed method. This includes training on a\nlow-resolution rendering of a neural radiance field, together with a\n3D-consistent super-resolution module and mesh-guided sampling and space\ncanonicalization. We demonstrate a novel application of perceptual loss on the\nimage space, which is critical for learning details accurately. We also show a\nlive demo where we photo-realistically render the human hand in real-time for\nthe first time, while also modeling pose- and view-dependent appearance\neffects. We ablate all our design choices and show that they optimize for\nrendering speed and quality. Video results and our code can be accessed from\nhttps://vcai.mpi-inf.mpg.de/projects/LiveHand/\n",
        "pdf_link": "http://arxiv.org/pdf/2302.07672v3"
    },
    {
        "title": "Project Elements: A computational entity-component-system in a\n  scene-graph pythonic framework, for a neural, geometric computer graphics\n  curriculum",
        "authors": [
            "George Papagiannakis",
            "Manos Kamarianakis",
            "Antonis Protopsaltis",
            "Dimitris Angelis",
            "Paul Zikas"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present the Elements project, a lightweight, open-source, computational\nscience and computer graphics (CG) framework, tailored for educational needs,\nthat offers, for the first time, the advantages of an Entity-Component-System\n(ECS) along with the rapid prototyping convenience of a Scenegraph-based\npythonic framework. This novelty allows advances in the teaching of CG: from\nheterogeneous directed acyclic graphs and depth-first traversals, to animation,\nskinning, geometric algebra and shader-based components rendered via unique\nsystems all the way to their representation as graph neural networks for 3D\nscientific visualization. Taking advantage of the unique ECS in a a Scenegraph\nunderlying system, this project aims to bridge CG curricula and modern game\nengines (MGEs), that are based on the same approach but often present these\nnotions in a black-box approach. It is designed to actively utilize software\ndesign patterns, under an extensible open-source approach. Although Elements\nprovides a modern (i.e., shader-based as opposed to fixed-function OpenGL),\nsimple to program approach with Jupyter notebooks and unit-tests, its CG\npipeline is not black-box, exposing for teaching for the first time unique\nchallenging scientific, visual and neural computing concepts.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.07691v3"
    },
    {
        "title": "LinSets.zip: Compressing Linear Set Diagrams",
        "authors": [
            "Markus Wallinger",
            "Alexander Dobler",
            "Martin Nöllenburg"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Linear diagrams are used to visualize set systems by depicting set\nmemberships as horizontal line segments in a matrix, where each set is\nrepresented as a row and each element as a column. Each such line segment of a\nset is shown in a contiguous horizontal range of cells of the matrix indicating\nthat the corresponding elements in the columns belong to the set. As each set\noccupies its own row in the matrix, the total height of the resulting\nvisualization is as large as the number of sets in the instance. Such a linear\ndiagram can be visually sparse and intersecting sets containing the same\nelement might be represented by distant rows. To alleviate such undesirable\neffects, we present LinSets.zip, a new approach that achieves a more\nspace-efficient representation of linear diagrams. First, we minimize the total\nnumber of gaps in the horizontal segments by reordering columns, a criterion\nthat has been shown to increase readability in linear diagrams. The main\ndifference of LinSets.zip to linear diagrams is that multiple non-intersecting\nsets can be positioned in the same row of the matrix. Furthermore, we present\nseveral different rendering variations for a matrix-based representation that\nutilize the proposed row compression. We implemented the different steps of our\napproach in a visualization pipeline using integer-linear programming, and\nsuitable heuristics aiming at sufficiently fast computations in practice. We\nconducted both a quantitative evaluation and a small-scale user experiment to\ncompare the effects of compressing linear diagrams.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.08401v1"
    },
    {
        "title": "Animating Sand, Mud, and Snow",
        "authors": [
            "Robert W. Sumner",
            "James F. O'Brien",
            "Jessica K. Hodgins"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Computer animations often lack the subtle environmental changes that should\noccur due to the actions of the characters. Squealing car tires usually leave\nno skid marks, airplanes rarely leave jet trails in the sky, and most runners\nleave no footprints. In this paper, we describe a simulation model of ground\nsurfaces that can be deformed by the impact of rigid body models of animated\ncharacters. To demonstrate the algorithms, we show footprints made by a runner\nin sand, mud, and snow as well as bicycle tire tracks, a bicycle crash, and a\nfalling runner. The shapes of the footprints in the three surfaces are quite\ndifferent, but the effects were controlled through only five essentially\nindependent parameters. To assess the realism of the resulting motion, we\ncompare the simulated footprints to human footprints in sand.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.08683v2"
    },
    {
        "title": "Creative NFT-Copyrighted AR Face Mask Authoring Using Unity3D Editor",
        "authors": [
            "Mohamed Al Hamzy",
            "Shijin Zhang",
            "Hong Huang",
            "Wanwan Li"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we extend well-designed 3D face masks into AR face masks and\ndemonstrate the possibility of transforming this into an NFT-copyrighted AR\nface mask that helps authenticate the ownership of the AR mask user so as to\nimprove creative control, brand identification, and ID protection. The output\nof this project will not only potentially validate the value of the NFT\ntechnology but also explore how to combine the NFT technology with AR\ntechnology so as to be applied to e-commerce and e-business aspects of the\nmultimedia industry.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.08685v1"
    },
    {
        "title": "Walk on Stars: A Grid-Free Monte Carlo Method for PDEs with Neumann\n  Boundary Conditions",
        "authors": [
            "Rohan Sawhney",
            "Bailey Miller",
            "Ioannis Gkioulekas",
            "Keenan Crane"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Grid-free Monte Carlo methods based on the walk on spheres (WoS) algorithm\nsolve fundamental partial differential equations (PDEs) like the Poisson\nequation without discretizing the problem domain or approximating functions in\na finite basis. Such methods hence avoid aliasing in the solution, and evade\nthe many challenges of mesh generation. Yet for problems with complex geometry,\npractical grid-free methods have been largely limited to basic Dirichlet\nboundary conditions. We introduce the walk on stars (WoSt) algorithm, which\nsolves linear elliptic PDEs with arbitrary mixed Neumann and Dirichlet boundary\nconditions. The key insight is that one can efficiently simulate reflecting\nBrownian motion (which models Neumann conditions) by replacing the balls used\nby WoS with star-shaped domains. We identify such domains via the closest point\non the visibility silhouette, by simply augmenting a standard bounding volume\nhierarchy with normal information. Overall, WoSt is an easy modification of\nWoS, and retains the many attractive features of grid-free Monte Carlo methods\nsuch as progressive and view-dependent evaluation, trivial parallelization, and\nsublinear scaling to increasing geometric detail.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.11815v3"
    },
    {
        "title": "Boundary Value Caching for Walk on Spheres",
        "authors": [
            "Bailey Miller",
            "Rohan Sawhney",
            "Keenan Crane",
            "Ioannis Gkioulekas"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Grid-free Monte Carlo methods such as walk on spheres can be used to solve\nelliptic partial differential equations without mesh generation or global\nsolves. However, such methods independently estimate the solution at every\npoint, and hence do not take advantage of the high spatial regularity of\nsolutions to elliptic problems. We propose a fast caching strategy which first\nestimates solution values and derivatives at randomly sampled points along the\nboundary of the domain (or a local region of interest). These cached values\nthen provide cheap, output-sensitive evaluation of the solution (or its\ngradient) at interior points, via a boundary integral formulation. Unlike\nclassic boundary integral methods, our caching scheme introduces zero\nstatistical bias and does not require a dense global solve. Moreover we can\nhandle imperfect geometry (e.g., with self-intersections) and detailed\nboundary/source terms without repairing or resampling the boundary\nrepresentation. Overall, our scheme is similar in spirit to virtual point light\nmethods from photorealistic rendering: it suppresses the typical\nsalt-and-pepper noise characteristic of independent Monte Carlo estimates,\nwhile still retaining the many advantages of Monte Carlo solvers: progressive\nevaluation, trivial parallelization, geometric robustness, etc. We validate our\napproach using test problems from visual and geometric computing.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.11825v3"
    },
    {
        "title": "Generator Matrices by Solving Integer Linear Programs",
        "authors": [
            "Loïs Paulin",
            "David Coeurjolly",
            "Nicolas Bonneel",
            "Jean-Claude Iehl",
            "Victor Ostromoukhov",
            "Alexander Keller"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In quasi-Monte Carlo methods, generating high-dimensional low discrepancy\nsequences by generator matrices is a popular and efficient approach.\nHistorically, constructing or finding such generator matrices has been a hard\nproblem. In particular, it is challenging to take advantage of the intrinsic\nstructure of a given numerical problem to design samplers of low discrepancy in\ncertain subsets of dimensions. To address this issue, we devise a greedy\nalgorithm allowing us to translate desired net properties into linear\nconstraints on the generator matrix entries. Solving the resulting integer\nlinear program yields generator matrices that satisfy the desired net\nproperties. We demonstrate that our method finds generator matrices in\nchallenging settings, offering low discrepancy sequences beyond the limitations\nof classic constructions.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.13943v3"
    },
    {
        "title": "A note on solid modeling: history, state of the art, future",
        "authors": [
            "Qiang Zou"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Solid modeling is a technique underlying CAD software as we see it today, and\nits theories and algorithms are among the most fundamental milestones in the\nhistorical development of CAD. Basically, it has answered the question of what\ngeometric information a computer should store and how to store/manipulate them\nin order for the computer to aid the processes of design and manufacturing.\nThis paper provides a brief review (in Chinese) on the historical development\nof solid modeling, its fundamental research problems, as well as their\nchallenges and state of the art. It then concludes with three prospective\ntrends of solid modeling, especially the promising paradigm shift from\n\"Computer-Aided Design\" to \"Computer-Automated Design\".\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14373v1"
    },
    {
        "title": "GPU-Accelerated LOD Generation for Point Clouds",
        "authors": [
            "Markus Schütz",
            "Bernhard Kerbl",
            "Philip Klaus",
            "Michael Wimmer"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  About: We introduce a GPU-accelerated LOD construction process that creates a\nhybrid voxel-point-based variation of the widely used layered point cloud (LPC)\nstructure for LOD rendering and streaming. The massive performance improvements\nprovided by the GPU allow us to improve the quality of lower LODs via color\nfiltering while still increasing construction speed compared to the\nnon-filtered, CPU-based state of the art.\n  Background: LOD structures are required to render hundreds of millions to\ntrillions of points, but constructing them takes time.\n  Results: LOD structures suitable for rendering and streaming are constructed\nat rates of about 1 billion points per second (with color filtering) to 4\nbillion points per second (sample-picking/random sampling, state of the art) on\nan RTX 3090 -- an improvement of a factor of 80 to 400 times over the CPU-based\nstate of the art (12 million points per second). Due to being in-core, model\nsizes are limited to about 500 million points per 24GB memory.\n  Discussion: Our method currently focuses on maximizing in-core construction\nspeed on the GPU. Issues such as out-of-core construction of arbitrarily large\ndata sets are not addressed, but we expect it to be suitable as a component of\nbottom-up out-of-core LOD construction schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14801v1"
    },
    {
        "title": "AR-Assisted Surgical Care via 5G networks for First Aid Responders",
        "authors": [
            "Manos Kamarianakis",
            "Antonis Protopsaltis",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Surgeons should play a central role in disaster planning and management due\nto the overwhelming number of bodily injuries that are typically involved\nduring most forms of disaster. In fact, various types of surgical procedures\nare performed by emergency medical teams after sudden-onset disasters, such as\nsoft tissue wounds, orthopaedic traumas, abdominal surgeries, etc. HMD-based\nAugmented Reality (AR), using state-of-the-art hardware such as the Magic Leap\nor the Microsoft HoloLens, have long been foreseen as a key enabler for\nclinicians in surgical use cases, especially for procedures performed outside\nof the operating room.\n  This paper describes the Use Case (UC) \"AR-assisted emergency surgical care\",\nidentified in the context of the 5G-EPICENTRE EU-funded project. Specifically,\nthe UC will experiment with holographic AR technology for emergency medical\nsurgery teams, by overlaying deformable medical models directly on top of the\npatient body parts, effectively enabling surgeons to see inside (visualizing\nbones, blood vessels, etc.) and perform surgical actions following step-by-step\ninstructions. The goal is to combine the computational and data-intensive\nnature of AR and Computer Vision algorithms with upcoming 5G network\narchitectures deployed for edge computing so as to satisfy real-time\ninteraction requirements and provide an efficient and powerful platform for the\npervasive promotion of such applications. By developing the necessary Virtual\nNetwork Functions (VNFs) to manage data-intensive services (e.g., prerendering,\ncaching, compression) and by exploiting available network resources and\nMulti-access Edge Computing (MEC) support, provided by the 5G-EPICENTRE\ninfrastructure, this UC aims to provide powerful AR-based tools, usable on\nsite, to first-aid responders.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.00458v1"
    },
    {
        "title": "Graphical Modeling and Animation of Brittle Fracture",
        "authors": [
            "James F. O'Brien",
            "Jessica K. Hodgins"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we augment existing techniques for simulating flexible objects\nto include models for crack initiation and propagation in three-dimensional\nvolumes. By analyzing the stress tensors computed over a finite element model,\nthe simulation determines where cracks should initiate and in what directions\nthey should propagate. We demonstrate our results with animations of breaking\nbowls, cracking walls, and objects that fracture when they collide. By varying\nthe shape of the objects, the material properties, and the initial conditions\nof the simulations, we can create strikingly different effects ranging from a\nwall that shatters when it is hit by a wrecking ball to a bowl that breaks in\ntwo when it is dropped on edge. This paper received the SIGGRAPH 99 Impact\nAward.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.02934v1"
    },
    {
        "title": "Shape Transformation Using Variational Implicit Functions",
        "authors": [
            "Greg Turk",
            "James F. O'Brien"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Traditionally, shape transformation using implicit functions is performed in\ntwo distinct steps: 1) creating two implicit functions, and 2) interpolating\nbetween these two functions. We present a new shape transformation method that\ncombines these two tasks into a single step. We create a transformation between\ntwo N-dimensional objects by casting this as a scattered data interpolation\nproblem in N + 1 dimensions. For the case of 2D shapes, we place all of our\ndata constraints within two planes, one for each shape. These planes are placed\nparallel to one another in 3D. Zero-valued constraints specify the locations of\nshape boundaries and positive-valued constraints are placed along the normal\ndirection in towards the center of the shape. We then invoke a variational\ninterpolation technique (the 3D generalization of thin-plate interpolation),\nand this yields a single implicit function in 3D. Intermediate shapes are\nsimply the zero-valued contours of 2D slices through this 3D function. Shape\ntransformation between 3D shapes can be performed similarly by solving a 4D\ninterpolation problem. To our knowledge, ours is the first shape transformation\nmethod to unify the tasks of implicit function creation and interpolation. The\ntransformations produced by this method appear smooth and natural, even between\nobjects of differing topologies. If desired, one or more additional shapes may\nbe introduced that influence the intermediate shapes in a sequence. Our method\ncan also reconstruct surfaces from multiple slices that are not restricted to\nbeing parallel to one another.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.02937v1"
    },
    {
        "title": "CAstelet in Virtual reality for shadOw AVatars (CAVOAV)",
        "authors": [
            "Georges Gagneré",
            "Anastasiia Ternova"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  After an overview of the use of digital shadows in computing science research\nprojects with cultural and social impacts and a focus on recent researches and\ninsights on virtual theaters, this paper introduces a research mixing the\nmanipulation of shadow avatars and the building of a virtual theater setup\ninspired by traditional shadow theater (or ``castelet'' in french) in a mixed\nreality environment. It describes the virtual 3D setup, the nature of the\nshadow avatars and the issues of directing believable interactions between\nvirtual avatars and physical performers on stage. Two modalities of shadow\navatars direction are exposed. Some results of the research are illustrated in\ntwo use cases: the development of theatrical creativity in mixed reality\nthrough pedagogical workshops; and an artistic achievement in ''The Shadow''\nperformance, after H. C. Andersen.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.06981v1"
    },
    {
        "title": "Experiencing avatar direction in low cost theatrical mixed reality setup",
        "authors": [
            "Georges Gagneré",
            "Cédric Plessiet"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce the setup and programming framework of AvatarStaging theatrical\nmixed reality experiment. We focus on a configuration addressing movement\nissues between physical and 3D digital spaces from performers and directors'\npoints of view. We propose 3 practical exercises.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.06984v1"
    },
    {
        "title": "Challenges of movement quality using motion capture in theatre",
        "authors": [
            "Georges Gagneré",
            "Andy Lavender",
            "Cédric Plessiet",
            "Tim White"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We describe1 two case studies of AvatarStaging theatrical mixed reality\nframework combining avatars and performers acting in an artistic context. We\noutline a qualitative approach toward the condition for stage presence for the\navatars. We describe the motion control solutions we experimented with from the\nperspective of building a protocol of avatar direction in a mixed reality\nappropriate to live performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.06987v1"
    },
    {
        "title": "Towards smoother surfaces by applying subdivision to voxel data",
        "authors": [
            "A. Michael Stock",
            "Sergio López-Ureña"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In computed tomography, the approximation quality of a scan of a physical\nobject is typically limited by the acquisition modalities, especially the\nhardware including X-ray detectors. To improve upon this, we experiment with a\nthree-dimensional subdivision scheme to increase the resolution of the\nreconstructed voxel data. Subdivision schemes are often used to refine\ntwo-dimensional manifolds (mostly meshes) leading to smoother surfaces. In this\nwork, we apply a refinement scheme to three-dimensional data first, and only\nthen, start the surface extraction process. Thus, the main subject of this work\nlies not on subdivision surfaces, but rather on subdivision volumes. In the\nvolumetric case, each subdivision iteration consumes eight times more storage\nspace than the previous one. Hence, we restrict ourselves to a single\nsubdivision iteration. We evaluate the quality of the produced subdivision\nvolumes using synthetic and industrial data. Furthermore, we consider\nmanufacturing errors in the original and in the subdivision volumes, extract\ntheir surfaces, and compare the resulting meshes in critical regions.\nObservations show that our specific choice of a subdivision scheme produces\nsmoothly interpolated data while also preserving edges.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.07075v1"
    },
    {
        "title": "Emergence and fragility of a research-creation (2000-2007)",
        "authors": [
            "Georges Gagneré"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  My research-creation process coincides with the encounter with the ''digital\nparadigm'' and the attempt to incorporate it into the foundation of my scenic\nwriting. I propose in this paper to give an account from a director point of\nview of how I realized my shows between 2000 and 2007 and which researches\ninfluenced the process. I will formulate some remarks on the fragilities that\ncan arise in a ''technological laboratory of staging''.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.09123v1"
    },
    {
        "title": "Automatic Joint Parameter Estimation from Magnetic Motion Capture Data",
        "authors": [
            "James F. O'Brien",
            "Robert E. Bodenheimer",
            "Gabriel J. Brostow",
            "Jessica K. Hodgins"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper describes a technique for using magnetic motion capture data to\ndetermine the joint parameters of an articulated hierarchy. This technique\nmakes it possible to determine limb lengths, joint locations, and sensor\nplacement for a human subject without external measurements. Instead, the joint\nparameters are inferred with high accuracy from the motion data acquired during\nthe capture session. The parameters are computed by performing a linear least\nsquares fit of a rotary joint model to the input data. A hierarchical structure\nfor the articulated model can also be determined in situations where the\ntopology of the model is not known. Once the system topology and joint\nparameters have been recovered, the resulting model can be used to perform\nforward and inverse kinematic procedures. We present the results of using the\nalgorithm on human motion capture data, as well as validation results obtained\nwith data from a simulation and a wooden linkage of known dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10532v1"
    },
    {
        "title": "Combining Active and Passive Simulations for Secondary Motion",
        "authors": [
            "James F. O'Brien",
            "Victor B. Zordan",
            "Jessica K. Hodgins"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Objects that move in response to the actions of a main character often make\nan important contribution to the visual richness of an animated scene. We use\nthe term \"secondary motion\" to refer to passive motions generated in response\nto the movements of characters and other objects or environmental forces.\nSecondary motions aren't normally the mail focus of an animated scene, yet\ntheir absence can distract or disturb the viewer, destroying the illusion of\nreality created by the scene. We describe how to generate secondary motion by\ncoupling physically based simulations of passive objects to actively controlled\ncharacters.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10551v1"
    },
    {
        "title": "Animating Fracture",
        "authors": [
            "James F. O'Brien",
            "Jessica K. Hodgins"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We have developed a simulation technique that uses non-linear finite element\nanalysis and elastic fracture mechanics to compute physically plausible motion\nfor three-dimensional, solid objects as they break, crack, or tear. When these\nobjects deform beyond their mechanical limits, the system automatically\ndetermines where fractures should begin and in what directions they should\npropagate. The system allows fractures to propagate in arbitrary directions by\ndynamically restructuring the elements of a tetrahedral mesh. Because cracks\nare not limited to the original element boundaries, the objects can form\nirregularly shaped shards and edges as they shatter. The result is realistic\nfracture patterns such as the ones shown in our examples. This paper presents\nan overview of the fracture algorithm, the details are presented in our ACM\nSIGGRAPH 1999 and 2002 papers.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10809v1"
    },
    {
        "title": "Fast Complementary Dynamics via Skinning Eigenmodes",
        "authors": [
            "Otman Benchekroun",
            "Jiayi Eris Zhang",
            "Siddhartha Chaudhuri",
            "Eitan Grinspun",
            "Yi Zhou",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a reduced-space elasto-dynamic solver that is well suited for\naugmenting rigged character animations with secondary motion. At the core of\nour method is a novel deformation subspace based on Linear Blend Skinning that\novercomes many of the shortcomings prior subspace methods face. Our skinning\nsubspace is parameterized entirely by a set of scalar weights, which we can\nobtain through a small, material-aware and rig-sensitive generalized eigenvalue\nproblem. The resulting subspace can easily capture rotational motion and\nguarantees that the resulting simulation is rotation equivariant. We further\npropose a simple local-global solver for linear co-rotational elasticity and\npropose a clustering method to aggregate per-tetrahedra non-linear energetic\nquantities. The result is a compact simulation that is fully decoupled from the\ncomplexity of the mesh.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.11886v2"
    },
    {
        "title": "Hardware Acceleration of Progressive Refinement Radiosity using Nvidia\n  RTX",
        "authors": [
            "Benjamin Kahl"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  A vital component of photo-realistic image synthesis is the simulation of\nindirect diffuse reflections, which still remain a quintessential hurdle that\nmodern rendering engines struggle to overcome. Real-time applications typically\npre-generate diffuse lighting information offline using radiosity to avoid\nperforming costly computations at run-time. In this thesis we present a variant\nof progressive refinement radiosity that utilizes Nvidia's novel RTX technology\nto accelerate the process of form-factor computation without compromising on\nvisual fidelity. Through a modern implementation built on DirectX 12 we\ndemonstrate that offloading radiosity's visibility component to RT cores\nsignificantly improves the lightmap generation process and potentially propels\nit into the domain of real-time.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.14831v1"
    },
    {
        "title": "Parallel Computation of Piecewise Linear Morse-Smale Segmentations",
        "authors": [
            "Robin G. C. Maack",
            "Jonas Lukasczyk",
            "Julien Tierny",
            "Hans Hagen",
            "Ross Maciejewski",
            "Christoph Garth"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper presents a well-scaling parallel algorithm for the computation of\nMorse-Smale (MS) segmentations, including the region separators and region\nboundaries. The segmentation of the domain into ascending and descending\nmanifolds, solely defined on the vertices, improves the computational time\nusing path compression and fully segments the border region. Region boundaries\nand region separators are generated using a multi-label marching tetrahedra\nalgorithm. This enables a fast and simple solution to find optimal parameter\nsettings in preliminary exploration steps by generating an MS complex preview.\nIt also poses a rapid option to generate a fast visual representation of the\nregion geometries for immediate utilization. Two experiments demonstrate the\nperformance of our approach with speedups of over an order of magnitude in\ncomparison to two publicly available implementations. The example section shows\nthe similarity to the MS complex, the useability of the approach, and the\nbenefits of this method with respect to the presented datasets. We provide our\nimplementation with the paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.15491v1"
    },
    {
        "title": "A Generalized Ray Formulation For Wave-Optics Rendering",
        "authors": [
            "Shlomi Steinberg",
            "Ravi Ramamoorthi",
            "Benedikt Bitterli",
            "Eugene d'Eon",
            "Ling-Qi Yan",
            "Matt Pharr"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Under ray-optical light transport, the classical ray serves as a linear and\nlocal \"point query\" of light's behaviour. Linearity and locality are crucial to\nthe formulation of sophisticated path tracing and sampling techniques, that\nenable efficient solutions to light transport problems in complex, real-world\nsettings and environments. However, such formulations are firmly confined to\nthe realm of ray optics, while many applications of interest -- in computer\ngraphics and computational optics -- demand a more precise understanding of\nlight: as waves. We rigorously formulate the generalized ray, which enables\nlinear and weakly-local queries of arbitrary wave-optical distributions of\nlight. Generalized rays arise from photodetection states, and therefore allow\nperforming backward (sensor-to-source) wave-optical light transport. Our\nformulations are accurate and highly general: they facilitate the application\nof modern path tracing techniques for wave-optical rendering, with light of any\nstate of coherence and any spectral properties. We improve upon the\nstate-of-the-art in terms of the generality and accuracy of the formalism, ease\nof application, as well as performance. As a consequence, we are able to render\nlarge, complex scenes, as in Fig. 1, and even do interactive wave-optical light\ntransport, none of which is possible with any existing method. We numerically\nvalidate our formalism, and make connection to partially-coherent light\ntransport.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.15762v2"
    },
    {
        "title": "On the derivatives of rational Bézier curves",
        "authors": [
            "Mao Shi"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  By studying the existing higher order derivation formulas of rational\nB\\'{e}zier curves, we find that they fail when the order of the derivative\nexceeds the degree of the curves. In this paper, we present a new derivation\nformula for rational B\\'{e}zier curves that overcomes this drawback and show\nthat the $k$th degree derivative of a $n$th degree rational B\\'{e}zier curve\ncan be written in terms of a $(2^kn)$th degree rational B\\'{e}zier curve.we\nalso consider the properties of the endpoints and the bounds of the\nderivatives.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.16156v3"
    },
    {
        "title": "DreamFace: Progressive Generation of Animatable 3D Faces under Text\n  Guidance",
        "authors": [
            "Longwen Zhang",
            "Qiwei Qiu",
            "Hongyang Lin",
            "Qixuan Zhang",
            "Cheng Shi",
            "Wei Yang",
            "Ye Shi",
            "Sibei Yang",
            "Lan Xu",
            "Jingyi Yu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Emerging Metaverse applications demand accessible, accurate, and easy-to-use\ntools for 3D digital human creations in order to depict different cultures and\nsocieties as if in the physical world. Recent large-scale vision-language\nadvances pave the way to for novices to conveniently customize 3D content.\nHowever, the generated CG-friendly assets still cannot represent the desired\nfacial traits for human characteristics. In this paper, we present DreamFace, a\nprogressive scheme to generate personalized 3D faces under text guidance. It\nenables layman users to naturally customize 3D facial assets that are\ncompatible with CG pipelines, with desired shapes, textures, and fine-grained\nanimation capabilities. From a text input to describe the facial traits, we\nfirst introduce a coarse-to-fine scheme to generate the neutral facial geometry\nwith a unified topology. We employ a selection strategy in the CLIP embedding\nspace, and subsequently optimize both the details displacements and normals\nusing Score Distillation Sampling from generic Latent Diffusion Model. Then,\nfor neutral appearance generation, we introduce a dual-path mechanism, which\ncombines the generic LDM with a novel texture LDM to ensure both the diversity\nand textural specification in the UV space. We also employ a two-stage\noptimization to perform SDS in both the latent and image spaces to\nsignificantly provides compact priors for fine-grained synthesis. Our generated\nneutral assets naturally support blendshapes-based facial animations. We\nfurther improve the animation ability with personalized deformation\ncharacteristics by learning the universal expression prior using the\ncross-identity hypernetwork. Notably, DreamFace can generate of realistic 3D\nfacial assets with physically-based rendering quality and rich animation\nability from video footage, even for fashion icons or exotic characters in\ncartoons and fiction movies.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.03117v1"
    },
    {
        "title": "Advances in Data-Driven Analysis and Synthesis of 3D Indoor Scenes",
        "authors": [
            "Akshay Gadi Patil",
            "Supriya Gadi Patil",
            "Manyi Li",
            "Matthew Fisher",
            "Manolis Savva",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This report surveys advances in deep learning-based modeling techniques that\naddress four different 3D indoor scene analysis tasks, as well as synthesis of\n3D indoor scenes. We describe different kinds of representations for indoor\nscenes, various indoor scene datasets available for research in the\naforementioned areas, and discuss notable works employing machine learning\nmodels for such scene modeling tasks based on these representations.\nSpecifically, we focus on the analysis and synthesis of 3D indoor scenes. With\nrespect to analysis, we focus on four basic scene understanding tasks -- 3D\nobject detection, 3D scene segmentation, 3D scene reconstruction and 3D scene\nsimilarity. And for synthesis, we mainly discuss neural scene synthesis works,\nthough also highlighting model-driven methods that allow for human-centric,\nprogressive scene synthesis. We identify the challenges involved in modeling\nscenes for these tasks and the kind of machinery that needs to be developed to\nadapt to the data representation, and the task setting in general. For each of\nthese tasks, we provide a comprehensive summary of the state-of-the-art works\nacross different axes such as the choice of data representation, backbone,\nevaluation metric, input, output, etc., providing an organized review of the\nliterature. Towards the end, we discuss some interesting research directions\nthat have the potential to make a direct impact on the way users interact and\nengage with these virtual scene models, making them an integral part of the\nmetaverse.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.03188v3"
    },
    {
        "title": "Importance Sampling BRDF Derivatives",
        "authors": [
            "Yash Belhe",
            "Bing Xu",
            "Sai Praveen Bangaru",
            "Ravi Ramamoorthi",
            "Tzu-Mao Li"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a set of techniques to efficiently importance sample the\nderivatives of several BRDF models. In differentiable rendering, BRDFs are\nreplaced by their differential BRDF counterparts which are real-valued and can\nhave negative values. This leads to a new source of variance arising from their\nchange in sign. Real-valued functions cannot be perfectly importance sampled by\na positive-valued PDF and the direct application of BRDF sampling leads to high\nvariance. Previous attempts at antithetic sampling only addressed the\nderivative with the roughness parameter of isotropic microfacet BRDFs. Our work\ngeneralizes BRDF derivative sampling to anisotropic microfacet models, mixture\nBRDFs, Oren-Nayar, Hanrahan-Krueger, among other analytic BRDFs.\n  Our method first decomposes the real-valued differential BRDF into a sum of\nsingle-signed functions, eliminating variance from a change in sign. Next, we\nimportance sample each of the resulting single-signed functions separately. The\nfirst decomposition, positivization, partitions the real-valued function based\non its sign, and is effective at variance reduction when applicable. However,\nit requires analytic knowledge of the roots of the differential BRDF, and for\nit to be analytically integrable too. Our key insight is that the single-signed\nfunctions can have overlapping support, which significantly broadens the ways\nwe can decompose a real-valued function. Our product and mixture decompositions\nexploit this property, and they allow us to support several BRDF derivatives\nthat positivization could not handle. For a wide variety of BRDF derivatives,\nour method significantly reduces the variance (up to 58x in some cases) at\nequal computation cost and enables better recovery of spatially varying\ntextures through gradient-descent-based inverse rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.04088v1"
    },
    {
        "title": "Feature-assisted interactive geometry reconstruction in 3D point clouds\n  using incremental region growing",
        "authors": [
            "Attila Szabo",
            "Georg Haaser",
            "Harald Steinlechner",
            "Andreas Walch",
            "Stefan Maierhofer",
            "Thomas Ortner",
            "Eduard Gröller"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Reconstructing geometric shapes from point clouds is a common task that is\noften accomplished by experts manually modeling geometries in CAD-capable\nsoftware. State-of-the-art workflows based on fully automatic geometry\nextraction are limited by point cloud density and memory constraints, and\nrequire pre- and post-processing by the user. In this work, we present a\nframework for interactive, user-driven, feature-assisted geometry\nreconstruction from arbitrarily sized point clouds. Based on seeded\nregion-growing point cloud segmentation, the user interactively extracts planar\npieces of geometry and utilizes contextual suggestions to point out plane\nsurfaces, normal and tangential directions, and edges and corners. We implement\na set of feature-assisted tools for high-precision modeling tasks in\narchitecture and urban surveying scenarios, enabling instant-feedback\ninteractive point cloud manipulation on large-scale data collected from\nreal-world building interiors and facades. We evaluate our results through\nsystematic measurement of the reconstruction accuracy, and interviews with\ndomain experts who deploy our framework in a commercial setting and give both\nstructured and subjective feedback.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.05109v1"
    },
    {
        "title": "A Survey of Developable Surfaces: From Shape Modeling to Manufacturing",
        "authors": [
            "Chao Yuan",
            "Nan Cao",
            "Yang Shi"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Developable surfaces are commonly observed in various applications such as\narchitecture, product design, manufacturing, mechanical materials, and data\nphysicalization as well as in the development of tangible interaction and\ndeformable robots, with the characteristics of easy-to-product, low-cost,\ntransport-friendly, and deformable. Transforming shapes into developable\nsurfaces is a complex and comprehensive task, which forms a variety of methods\nof segmentation, unfolding, and manufacturing for shapes with different\ngeometry and topology, resulting in the complexity of developable surfaces. In\nthis paper, we reviewed relevant methods and techniques for the study of\ndevelopable surfaces, characterize them with our proposed pipeline, and\ncategorize them based on digital modeling, physical modeling, interaction, and\napplication. Through the analysis to the relevant literature, we also discussed\nsome of the research challenges and future research opportunities.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09587v2"
    },
    {
        "title": "Synchronized-tracing of implicit surfaces",
        "authors": [
            "Cédric Zanni"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Implicit surfaces are known for their ability to represent smooth objects of\narbitrary topology thanks to hierarchical combinations of primitives using a\nstructure called a blobtree. We present a new tile-based rendering pipeline\nwell suited for modeling scenarios, i.e., no preprocessing is required when\nprimitive parameters are updated. When using approximate signed distance\nfields, we rely on compact, smooth CSG operators - extended from standard\nbounded operators - to compute a tight volume of interest for all primitives of\nthe blobtree. The pipeline relies on a low-resolution A-buffer storing the\nprimitives of interest of a given screen tile. The A-buffer is then used during\nray processing to synchronize threads within a subfrustum. This allows coherent\nfield evaluation within workgroups. We use a sparse bottom-up tree traversal to\nprune the blobtree on-the-fly which allows us to decorrelate field evaluation\ncomplexity from the full blobtree size. The ray processing itself is done using\nthe sphere-tracing algorithm. The pipeline scales well to surfaces consisting\nof thousands of primitives.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09673v1"
    },
    {
        "title": "Neurosymbolic Models for Computer Graphics",
        "authors": [
            "Daniel Ritchie",
            "Paul Guerrero",
            "R. Kenny Jones",
            "Niloy J. Mitra",
            "Adriana Schulz",
            "Karl D. D. Willis",
            "Jiajun Wu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Procedural models (i.e. symbolic programs that output visual data) are a\nhistorically-popular method for representing graphics content: vegetation,\nbuildings, textures, etc. They offer many advantages: interpretable design\nparameters, stochastic variations, high-quality outputs, compact\nrepresentation, and more. But they also have some limitations, such as the\ndifficulty of authoring a procedural model from scratch. More recently,\nAI-based methods, and especially neural networks, have become popular for\ncreating graphic content. These techniques allow users to directly specify\ndesired properties of the artifact they want to create (via examples,\nconstraints, or objectives), while a search, optimization, or learning\nalgorithm takes care of the details. However, this ease of use comes at a cost,\nas it's often hard to interpret or manipulate these representations. In this\nstate-of-the-art report, we summarize research on neurosymbolic models in\ncomputer graphics: methods that combine the strengths of both AI and symbolic\nprograms to represent, generate, and manipulate visual data. We survey recent\nwork applying these techniques to represent 2D shapes, 3D shapes, and materials\n& textures. Along the way, we situate each prior work in a unified design space\nfor neurosymbolic models, which helps reveal underexplored areas and\nopportunities for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.10320v1"
    },
    {
        "title": "Energy management system for biological 3D printing by the refinement of\n  manifold model morphing in flexible grasping space",
        "authors": [
            "Kang Wang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The use of 3D printing, or additive manufacturing, has gained significant\nattention in recent years due to its potential for revolutionizing traditional\nmanufacturing processes. One key challenge in 3D printing is managing energy\nconsumption, as it directly impacts the cost, efficiency, and sustainability of\nthe process. In this paper, we propose an energy management system that\nleverages the refinement of manifold model morphing in a flexible grasping\nspace, to reduce costs for biological 3D printing. The manifold model is a\nmathematical representation of the 3D object to be printed, and the refinement\nprocess involves optimizing the morphing parameters of the manifold model to\nachieve desired printing outcomes. To enable flexibility in the grasping space,\nwe incorporate data-driven approaches, such as machine learning and data\naugmentation techniques, to enhance the accuracy and robustness of the energy\nmanagement system. Our proposed system addresses the challenges of limited\nsample data and complex morphologies of manifold models in layered additive\nmanufacturing. Our method is more applicable for soft robotics and\nbiomechanisms. We evaluate the performance of our system through extensive\nexperiments and demonstrate its effectiveness in predicting and managing energy\nconsumption in 3D printing processes. The results highlight the importance of\nrefining manifold model morphing in the flexible grasping space for achieving\nenergy-efficient 3D printing, contributing to the advancement of green and\nsustainable manufacturing practices.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.10729v1"
    },
    {
        "title": "Globally Consistent Normal Orientation for Point Clouds by Regularizing\n  the Winding-Number Field",
        "authors": [
            "Rui Xu",
            "Zhiyang Dou",
            "Ningna Wang",
            "Shiqing Xin",
            "Shuangmin Chen",
            "Mingyan Jiang",
            "Xiaohu Guo",
            "Wenping Wang",
            "Changhe Tu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Estimating normals with globally consistent orientations for a raw point\ncloud has many downstream geometry processing applications. Despite tremendous\nefforts in the past decades, it remains challenging to deal with an unoriented\npoint cloud with various imperfections, particularly in the presence of data\nsparsity coupled with nearby gaps or thin-walled structures. In this paper, we\npropose a smooth objective function to characterize the requirements of an\nacceptable winding-number field, which allows one to find the globally\nconsistent normal orientations starting from a set of completely random\nnormals. By taking the vertices of the Voronoi diagram of the point cloud as\nexamination points, we consider the following three requirements: (1) the\nwinding number is either 0 or 1, (2) the occurrences of 1 and the occurrences\nof 0 are balanced around the point cloud, and (3) the normals align with the\noutside Voronoi poles as much as possible. Extensive experimental results show\nthat our method outperforms the existing approaches, especially in handling\nsparse and noisy point clouds, as well as shapes with complex\ngeometry/topology.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.11605v1"
    },
    {
        "title": "A Two-part Transformer Network for Controllable Motion Synthesis",
        "authors": [
            "Shuaiying Hou",
            "Hongyu Tao",
            "Hujun Bao",
            "Weiwei Xu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Although part-based motion synthesis networks have been investigated to\nreduce the complexity of modeling heterogeneous human motions, their\ncomputational cost remains prohibitive in interactive applications. To this\nend, we propose a novel two-part transformer network that aims to achieve\nhigh-quality, controllable motion synthesis results in real-time. Our network\nseparates the skeleton into the upper and lower body parts, reducing the\nexpensive cross-part fusion operations, and models the motions of each part\nseparately through two streams of auto-regressive modules formed by multi-head\nattention layers. However, such a design might not sufficiently capture the\ncorrelations between the parts. We thus intentionally let the two parts share\nthe features of the root joint and design a consistency loss to penalize the\ndifference in the estimated root features and motions by these two\nauto-regressive modules, significantly improving the quality of synthesized\nmotions. After training on our motion dataset, our network can synthesize a\nwide range of heterogeneous motions, like cartwheels and twists. Experimental\nand user study results demonstrate that our network is superior to\nstate-of-the-art human motion synthesis networks in the quality of generated\nmotions.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.12571v2"
    },
    {
        "title": "Generating Procedural Materials from Text or Image Prompts",
        "authors": [
            "Yiwei Hu",
            "Paul Guerrero",
            "Miloš Hašan",
            "Holly Rushmeier",
            "Valentin Deschaintre"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Node graph systems are used ubiquitously for material design in computer\ngraphics. They allow the use of visual programming to achieve desired effects\nwithout writing code. As high-level design tools they provide convenience and\nflexibility, but mastering the creation of node graphs usually requires\nprofessional training. We propose an algorithm capable of generating multiple\nnode graphs from different types of prompts, significantly lowering the bar for\nusers to explore a specific design space. Previous work was limited to\nunconditional generation of random node graphs, making the generation of an\nenvisioned material challenging. We propose a multi-modal node graph generation\nneural architecture for high-quality procedural material synthesis which can be\nconditioned on different inputs (text or image prompts), using a CLIP-based\nencoder. We also create a substantially augmented material graph dataset, key\nto improving the generation quality. Finally, we generate high-quality graph\nsamples using a regularized sampling process and improve the matching quality\nby differentiable optimization for top-ranked samples. We compare our methods\nto CLIP-based database search baselines (which are themselves novel) and\nachieve superior or similar performance without requiring massive data storage.\nWe further show that our model can produce a set of material graphs\nunconditionally, conditioned on images, text prompts or partial graphs, serving\nas a tool for automatic visual programming completion.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13172v1"
    },
    {
        "title": "Ultrasound Visualization using VTK",
        "authors": [
            "Bhavya Sehgal",
            " Gavin",
            " Gui",
            "Md Nahid sadik"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This project developed a web application using VTK for ultrasound\nvisualization. The images were enhanced using median and Gaussian filters, and\ntwo algorithms were utilized for data visualization: isosurface extraction and\nDelaunay triangulation. Results showed that both algorithms were effective at\nreducing Gaussian noise and high-frequency noise, such as speckle noise, which\nis common in ultrasound. The web application allows users to select MHA files,\nadjust the marching cubes threshold, and switch between the two algorithms in\nruntime. This project demonstrates the benefits of ultrasound visualization in\nmedical applications and the utility of using VTK to achieve high-quality\nvisualizations. The web application provides healthcare professionals with a\nuser-friendly platform to interpret ultrasound data, leading to better\ndiagnosis and treatment decisions\n",
        "pdf_link": "http://arxiv.org/pdf/2304.14592v1"
    },
    {
        "title": "Alternately denoising and reconstructing unoriented point sets",
        "authors": [
            "Dong Xiao",
            "Zuoqiang Shi",
            "Bin Wang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a new strategy to bridge point cloud denoising and surface\nreconstruction by alternately updating the denoised point clouds and the\nreconstructed surfaces. In Poisson surface reconstruction, the implicit\nfunction is generated by a set of smooth basis functions centered at the\noctnodes. When the octree depth is properly selected, the reconstructed surface\nis a good smooth approximation of the noisy point set. Our method projects the\nnoisy points onto the surface and alternately reconstructs and projects the\npoint set. We use the iterative Poisson surface reconstruction (iPSR) to\nsupport unoriented surface reconstruction. Our method iteratively performs iPSR\nand acts as an outer loop of iPSR. Considering that the octree depth\nsignificantly affects the reconstruction results, we propose an adaptive depth\nselection strategy to ensure an appropriate depth choice. To manage the\noversmoothing phenomenon near the sharp features, we propose a\n$\\lambda$-projection method, which means to project the noisy points onto the\nsurface with an individual control coefficient $\\lambda_{i}$ for each point.\nThe coefficients are determined through a Voronoi-based feature detection\nmethod. Experimental results show that our method achieves high performance in\npoint cloud denoising and unoriented surface reconstruction within different\nnoise scales, and exhibits well-rounded performance in various types of inputs.\nThe source code is available\nat~\\url{https://github.com/Submanifold/AlterUpdate}.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.00391v2"
    },
    {
        "title": "Contact Edit: Artist Tools for Intuitive Modeling of Hand-Object\n  Interactions",
        "authors": [
            "Arjun S. Lakshmipathy",
            "Nicole Feng",
            "Yu Xi Lee",
            "Moshe Mahler",
            "Nancy S. Pollard"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Posing high-contact interactions is challenging and time-consuming, with\nhand-object interactions being especially difficult due to the large number of\ndegrees of freedom (DOF) of the hand and the fact that humans are experts at\njudging hand poses. This paper addresses this challenge by elevating contact\nareas to first-class primitives. We provide \\textit{end-to-end art-directable}\n(EAD) tools to model interactions based on contact areas, directly manipulate\ncontact areas, and compute corresponding poses automatically. To make these\noperations intuitive and fast, we present a novel axis-based contact model that\nsupports real-time approximately isometry-preserving operations on triangulated\nsurfaces, permits movement between surfaces, and is both robust and scalable to\nlarge areas. We show that use of our contact model facilitates high quality\nposing even for unconstrained, high-DOF custom rigs intended for traditional\nkeyframe-based animation pipelines. We additionally evaluate our approach with\ncomparisons to prior art, ablation studies, user studies, qualitative\nassessments, and extensions to full-body interaction.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.02051v3"
    },
    {
        "title": "On procedural urban digital twin generation and visualization of large\n  scale data",
        "authors": [
            "Sanjay Somanath",
            "Vasilis Naserentin",
            "Orfeas Eleftheriou",
            "Daniel Sjölie",
            "Beata Stahre Wästberg",
            "Anders Logg"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The desired outcome for urban digital twins is an automatically generated\ndetailed 3D model of a building from aerial imagery, footprints, LiDAR, or a\nfusion of these. Such 3D models have applications in architecture, civil\nengineering, urban planning, construction, real estate, GIS, and many others.\nFurther, the visualization of large-scale data in conjunction with the\ngenerated 3D models is often a recurring and resource-intensive task. However,\na completely automated end-to-end workflow is complex, requiring many steps to\nachieve a high-quality visualization. Methods for building reconstruction\napproaches have come a long way from previously manual approaches to\nsemi-automatic or automatic approaches. The next step after reconstructing\nbuildings is visualizing the buildings and their context. Advances in real-time\nrendering using game engines have enabled the extension of building\nreconstruction methods to procedurally generated context generation. This paper\naims to complement existing methods of 3D building generation. First, we\npresent a literature review covering different options for procedurally\ngenerated context generation and visualization methods in-depth, focusing on\nworkflows and data pipelines. Next, we present a semi-automated workflow that\nextends the building reconstruction pipeline to include procedural context\ngeneration (terrain and vegetation) using Unreal Engine and, finally, the\nintegration of various types of large-scale urban analysis data for\nvisualization. We conclude with a series of challenges faced in achieving such\npipelines and the limitations of the current approach. The steps for a\ncomplete, end-to-end solution involve developing robust systems for building\ndetection, rooftop recognition, and geometry generation and importing and\nvisualizing data in the same 3D environment.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.02242v1"
    },
    {
        "title": "Real-Time Neural Appearance Models",
        "authors": [
            "Tizian Zeltner",
            "Fabrice Rousselle",
            "Andrea Weidlich",
            "Petrik Clarberg",
            "Jan Novák",
            "Benedikt Bitterli",
            "Alex Evans",
            "Tomáš Davidovič",
            "Simon Kallweit",
            "Aaron Lefohn"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a complete system for real-time rendering of scenes with complex\nappearance previously reserved for offline use. This is achieved with a\ncombination of algorithmic and system level innovations.\n  Our appearance model utilizes learned hierarchical textures that are\ninterpreted using neural decoders, which produce reflectance values and\nimportance-sampled directions. To best utilize the modeling capacity of the\ndecoders, we equip the decoders with two graphics priors. The first prior --\ntransformation of directions into learned shading frames -- facilitates\naccurate reconstruction of mesoscale effects. The second prior -- a microfacet\nsampling distribution -- allows the neural decoder to perform importance\nsampling efficiently. The resulting appearance model supports anisotropic\nsampling and level-of-detail rendering, and allows baking deeply layered\nmaterial graphs into a compact unified neural representation.\n  By exposing hardware accelerated tensor operations to ray tracing shaders, we\nshow that it is possible to inline and execute the neural decoders efficiently\ninside a real-time path tracer. We analyze scalability with increasing number\nof neural materials and propose to improve performance using code optimized for\ncoherent and divergent execution. Our neural material shaders can be over an\norder of magnitude faster than non-neural layered materials. This opens up the\ndoor for using film-quality visuals in real-time applications such as games and\nlive previews.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.02678v2"
    },
    {
        "title": "MPMNet: A Data-Driven MPM Framework for Dynamic Fluid-Solid Interaction",
        "authors": [
            "Jin Li",
            "Yang Gao",
            "Ju Dai",
            "Shuai Li",
            "Aimin Hao",
            "Hong Qin"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  High-accuracy, high-efficiency physics-based fluid-solid interaction is\nessential for reality modeling and computer animation in online games or\nreal-time Virtual Reality (VR) systems. However, the large-scale simulation of\nincompressible fluid and its interaction with the surrounding solid environment\nis either time-consuming or suffering from the reduced time/space resolution\ndue to the complicated iterative nature pertinent to numerical computations of\ninvolved Partial Differential Equations (PDEs). In recent years, we have\nwitnessed significant growth in exploring a different, alternative data-driven\napproach to addressing some of the existing technical challenges in\nconventional model-centric graphics and animation methods. This paper showcases\nsome of our exploratory efforts in this direction. One technical concern of our\nresearch is to address the central key challenge of how to best construct the\nnumerical solver effectively and how to best integrate\nspatiotemporal/dimensional neural networks with the available MPM's pressure\nsolvers.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03315v1"
    },
    {
        "title": "Iterative $α$-(de)Blending: a Minimalist Deterministic Diffusion\n  Model",
        "authors": [
            "Eric Heitz",
            "Laurent Belcour",
            "Thomas Chambon"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We derive a minimalist but powerful deterministic denoising-diffusion model.\nWhile denoising diffusion has shown great success in many domains, its\nunderlying theory remains largely inaccessible to non-expert users. Indeed, an\nunderstanding of graduate-level concepts such as Langevin dynamics or score\nmatching appears to be required to grasp how it works. We propose an\nalternative approach that requires no more than undergrad calculus and\nprobability. We consider two densities and observe what happens when random\nsamples from these densities are blended (linearly interpolated). We show that\niteratively blending and deblending samples produces random paths between the\ntwo densities that converge toward a deterministic mapping. This mapping can be\nevaluated with a neural network trained to deblend samples. We obtain a model\nthat behaves like deterministic denoising diffusion: it iteratively maps\nsamples from one density (e.g., Gaussian noise) to another (e.g., cat images).\nHowever, compared to the state-of-the-art alternative, our model is simpler to\nderive, simpler to implement, more numerically stable, achieves higher quality\nresults in our experiments, and has interesting connections to computer\ngraphics.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03486v1"
    },
    {
        "title": "HACK: Learning a Parametric Head and Neck Model for High-fidelity\n  Animation",
        "authors": [
            "Longwen Zhang",
            "Zijun Zhao",
            "Xinzhou Cong",
            "Qixuan Zhang",
            "Shuqi Gu",
            "Yuchong Gao",
            "Rui Zheng",
            "Wei Yang",
            "Lan Xu",
            "Jingyi Yu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Significant advancements have been made in developing parametric models for\ndigital humans, with various approaches concentrating on parts such as the\nhuman body, hand, or face. Nevertheless, connectors such as the neck have been\noverlooked in these models, with rich anatomical priors often unutilized. In\nthis paper, we introduce HACK (Head-And-neCK), a novel parametric model for\nconstructing the head and cervical region of digital humans. Our model seeks to\ndisentangle the full spectrum of neck and larynx motions, facial expressions,\nand appearance variations, providing personalized and anatomically consistent\ncontrols, particularly for the neck regions. To build our HACK model, we\nacquire a comprehensive multi-modal dataset of the head and neck under various\nfacial expressions. We employ a 3D ultrasound imaging scheme to extract the\ninner biomechanical structures, namely the precise 3D rotation information of\nthe seven vertebrae of the cervical spine. We then adopt a multi-view\nphotometric approach to capture the geometry and physically-based textures of\ndiverse subjects, who exhibit a diverse range of static expressions as well as\nsequential head-and-neck movements. Using the multi-modal dataset, we train the\nparametric HACK model by separating the 3D head and neck depiction into various\nshape, pose, expression, and larynx blendshapes from the neutral expression and\nthe rest skeletal pose. We adopt an anatomically-consistent skeletal design for\nthe cervical region, and the expression is linked to facial action units for\nartist-friendly controls. HACK addresses the head and neck as a unified entity,\noffering more accurate and expressive controls, with a new level of realism,\nparticularly for the neck regions. This approach has significant benefits for\nnumerous applications and enables inter-correlation analysis between head and\nneck for fine-grained motion synthesis and transfer.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04469v1"
    },
    {
        "title": "A Closest Point Method for PDEs on Manifolds with Interior Boundary\n  Conditions for Geometry Processing",
        "authors": [
            "Nathan King",
            "Haozhe Su",
            "Mridul Aanjaneya",
            "Steven Ruuth",
            "Christopher Batty"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Many geometry processing techniques require the solution of partial\ndifferential equations (PDEs) on manifolds embedded in $\\mathbb{R}^2$ or\n$\\mathbb{R}^3$, such as curves or surfaces. Such manifold PDEs often involve\nboundary conditions (e.g., Dirichlet or Neumann) prescribed at points or curves\non the manifold's interior or along the geometric (exterior) boundary of an\nopen manifold. However, input manifolds can take many forms (e.g., triangle\nmeshes, parametrizations, point clouds, implicit functions, etc.). Typically,\none must generate a mesh to apply finite element-type techniques or derive\nspecialized discretization procedures for each distinct manifold\nrepresentation. We propose instead to address such problems in a unified manner\nthrough a novel extension of the closest point method (CPM) to handle interior\nboundary conditions. CPM solves the manifold PDE by solving a volumetric PDE\ndefined over the Cartesian embedding space containing the manifold, and\nrequires only a closest point representation of the manifold. Hence, CPM\nsupports objects that are open or closed, orientable or not, and of any\ncodimension. To enable support for interior boundary conditions we derive a\nmethod that implicitly partitions the embedding space across interior\nboundaries. CPM's finite difference and interpolation stencils are adapted to\nrespect this partition while preserving second-order accuracy. Additionally, we\ndevelop an efficient sparse-grid implementation and numerical solver. We\ndemonstrate our method's convergence behaviour on selected model PDEs and\nexplore several geometry processing problems: diffusion curves on surfaces,\ngeodesic distance, tangent vector field design, harmonic map construction, and\nreaction-diffusion textures. Our proposed approach thus offers a powerful and\nflexible new tool for a range of geometry processing tasks on general manifold\nrepresentations.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04711v2"
    },
    {
        "title": "TauBench 1.1: A Dynamic Benchmark for Graphics Rendering",
        "authors": [
            "Erfan Momeni Yazdi",
            "Markku Mäkitalo",
            "Julius Ikkala",
            "Pekka Jääskeläinen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Many graphics rendering algorithms used in both real-time games and virtual\nreality applications can get performance boosts by temporally reusing previous\ncomputations. However, algorithms based on temporal reuse are typically\nmeasured using trivial benchmarks with very limited dynamic features. To this\nend, in [1] we presented TauBench 1.0, a benchmark designed to stress temporal\nreuse algorithms. Now, we release TauBench version 1.1, which improves the\nusability of the original benchmark. In particular, these improvements reduce\nthe size of the dataset significantly, resulting in faster loading and\nrendering times, and in better compatibility with 3D software that impose\nstrict size limits for the scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04804v1"
    },
    {
        "title": "Semi-sparsity on Piecewise Constant Function Spaces for Triangular Mesh\n  Denoising",
        "authors": [
            "Junqing Huang",
            "Haihui Wang",
            "Michael Ruzhansky"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a semi-sparsity model for 3D triangular mesh denoising, which is\nmotivated by the success of semi-sparsity regularization in image processing\napplications. We demonstrate that such a regularization model can be also\napplied for graphic processing and gives rise to similar simultaneous-fitting\nresults in preserving sharp features and piece-wise smoothing surfaces.\nSpecifically, we first describe the piecewise constant function spaces\nassociated with the differential operators on triangular meshes and then show\nhow to extend the semi-sparsity model to meshes denoising. To verify its\neffectiveness, we present an efficient iterative algorithm based on the\nalternating direction method of multipliers (ADMM) technique and show the\nexperimental results on synthetic and real scanning data against the\nstate-of-the-arts both visually and quantitatively.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04834v1"
    },
    {
        "title": "Anatomically Detailed Simulation of Human Torso",
        "authors": [
            "Seunghwan Lee",
            "Yifeng Jiang",
            "C. Karen Liu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Existing digital human models approximate the human skeletal system using\nrigid bodies connected by rotational joints. While the simplification is\nconsidered acceptable for legs and arms, it significantly lacks fidelity to\nmodel rich torso movements in common activities such as dancing, Yoga, and\nvarious sports. Research from biomechanics provides more detailed modeling for\nparts of the torso, but their models often operate in isolation and are not\nfast and robust enough to support computationally heavy applications and\nlarge-scale data generation for full-body digital humans. This paper proposes a\nnew torso model that aims to achieve high fidelity both in perception and in\nfunctionality, while being computationally feasible for simulation and optimal\ncontrol tasks. We build a detailed human torso model consisting of various\nanatomical components, including facets, ligaments, and intervertebral discs,\nby coupling efficient finite-element and rigid-body simulations. Given an\nexisting motion capture sequence without dense markers placed on the torso, our\nnew model is able to recover the underlying torso bone movements. Our method is\nremarkably robust that it can be used to automatically \"retrofit\" the entire\nMixamo motion database of highly diverse human motions without user\nintervention. We also show that our model is computationally efficient for\nsolving trajectory optimization of highly dynamic full-body movements, without\nrelying on any reference motion. Physiological validity of the model is\nvalidated against established literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04995v1"
    },
    {
        "title": "Stealth Shaper: Reflectivity Optimization as Surface Stylization",
        "authors": [
            "Kenji Tojo",
            "Ariel Shamir",
            "Bernd Bickel",
            "Nobuyuki Umetani"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a technique to optimize the reflectivity of a surface while\npreserving its overall shape. The naive optimization of the mesh vertices using\nthe gradients of reflectivity simulations results in undesirable distortion. In\ncontrast, our robust formulation optimizes the surface normal as an independent\nvariable that bridges the reflectivity term with differential rendering, and\nthe regularization term with as-rigid-as-possible elastic energy. We further\nadaptively subdivide the input mesh to improve the convergence. Consequently,\nour method can minimize the retroreflectivity of a wide range of input shapes,\nresulting in sharply creased shapes ubiquitous among stealth aircraft and\nSci-Fi vehicles. Furthermore, by changing the reward for the direction of the\noutgoing light directions, our method can be applied to other reflectivity\ndesign tasks, such as the optimization of architectural walls to concentrate\nlight in a specific region. We have tested the proposed method using\nlight-transport simulations and real-world 3D-printed objects.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.05944v1"
    },
    {
        "title": "View Correspondence Network for Implicit Light Field Representation",
        "authors": [
            "Süleyman Aslan",
            "Brandon Yushan Feng",
            "Amitabh Varshney"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a novel technique for implicit neural representation of light\nfields at continuously defined viewpoints with high quality and fidelity. Our\nimplicit neural representation maps 4D coordinates defining two-plane\nparameterization of the light fields to the corresponding color values. We\nleverage periodic activations to achieve high expressivity and accurate\nreconstruction for complex data manifolds while keeping low storage and\ninference time requirements. However, na\\\"ively trained non-3D structured\nnetworks do not adequately satisfy the multi-view consistency; instead, they\nperform alpha blending of nearby viewpoints. In contrast, our View\nCorrespondence Network, or VICON, leverages stereo matching, optimization by\nautomatic differentiation with respect to the input space, and multi-view pixel\ncorrespondence to provide a novel implicit representation of the light fields\nfaithful to the novel views that are unseen during the training. Experimental\nresults show VICON superior to the state-of-the-art non-3D implicit light field\nrepresentations both qualitatively and quantitatively. Moreover, our implicit\nrepresentation captures a larger field of view (FoV), surpassing the extent of\nthe observable scene by the cameras of the ground truth renderings.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.06233v1"
    },
    {
        "title": "Surface Simplification using Intrinsic Error Metrics",
        "authors": [
            "Hsueh-Ti Derek Liu",
            "Mark Gillespie",
            "Benjamin Chislett",
            "Nicholas Sharp",
            "Alec Jacobson",
            "Keenan Crane"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper describes a method for fast simplification of surface meshes.\nWhereas past methods focus on visual appearance, our goal is to solve equations\non the surface. Hence, rather than approximate the extrinsic geometry, we\nconstruct a coarse intrinsic triangulation of the input domain. In the spirit\nof the quadric error metric (QEM), we perform greedy decimation while\nagglomerating global information about approximation error. In lieu of\nextrinsic quadrics, however, we store intrinsic tangent vectors that track how\nfar curvature \"drifts\" during simplification. This process also yields a\nbijective map between the fine and coarse mesh, and prolongation operators for\nboth scalar- and vector-valued data. Moreover, we obtain hard guarantees on\nelement quality via intrinsic retriangulation - a feature unique to the\nintrinsic setting. The overall payoff is a \"black box\" approach to geometry\nprocessing, which decouples mesh resolution from the size of matrices used to\nsolve equations. We show how our method benefits several fundamental tasks,\nincluding geometric multigrid, all-pairs geodesic distance, mean curvature\nflow, geodesic Voronoi diagrams, and the discrete exponential map.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.06410v2"
    },
    {
        "title": "Geometric Modeling and Physics Simulation Framework for Building a\n  Digital Twin of Extrusion-based Additive Manufacturing",
        "authors": [
            "Dhruv Gamdha",
            "Kumar Saurabh",
            "Baskar Ganapathysubramanian",
            "Adarsh Krishnamurthy"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Accurate simulation of the printing process is essential for improving print\nquality, reducing waste, and optimizing the printing parameters of\nextrusion-based additive manufacturing. Traditional additive manufacturing\nsimulations are very compute-intensive and are not scalable to simulate even\nmoderately-sized geometries. In this paper, we propose a general framework for\ncreating a digital twin of the dynamic printing process by performing physics\nsimulations with the intermediate print geometries. Our framework takes a\ngeneral extrusion-based additive manufacturing G-code, generates an\nanalysis-suitable voxelized geometry representation from the print schedule,\nand performs physics-based (transient thermal and phase change) simulations of\nthe printing process. Our approach leverages parallel adaptive octree meshes\nfor both voxelated geometry representation as well as for fast simulations to\naddress real-time predictions. We demonstrate the effectiveness of our method\nby simulating the printing of complex geometries at high voxel resolutions with\nboth sparse and dense infills. Our results show that this approach scales to\nhigh voxel resolutions and can predict the transient heat distribution as the\nprint progresses. This work lays the computational and algorithmic foundations\nfor building real-time digital twins and performing rapid virtual print\nsequence exploration to improve print quality and further reduce material\nwaste.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.07120v1"
    },
    {
        "title": "Progressive Material Caching",
        "authors": [
            "Shin Fujieda",
            "Takahiro Harada"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The evaluation of material networks is a relatively resource-intensive\nprocess in the rendering pipeline. Modern production scenes can contain\nhundreds or thousands of complex materials with massive networks, so there is a\ngreat demand for an efficient way of handling material networks. In this paper,\nwe introduce an efficient method for progressively caching the material nodes\nwithout an overhead on the rendering performance. We evaluate the material\nnetworks as usual in the rendering process. Then, the output value of part of\nthe network is stored in a cache and can be used in the evaluation of the next\nmaterials. Using our method, we can render the scene with performance equal to\nor better than that of the method without caching, with a slight difference in\nthe images rendered with caching and without it.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.07238v1"
    },
    {
        "title": "Combining GPU Tracing Methods within a Single Ray Query",
        "authors": [
            "Pieterjan Bartels",
            "Takahiro Harada"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  A recent trend in real-time rendering is the utilization of the new hardware\nray tracing capabilities. Often, usage of a distance field representation is\nproposed as an alternative when hardware ray tracing is deemed too costly, and\nthe two are seen as competing approaches. In this work, we show that both\napproaches can work together effectively for a single ray query on modern\nhardware. We choose to use hardware ray tracing where precision is most\nimportant, while avoiding its heavy cost by using a distance field when\npossible. While a simple approach, in our experiments the resulting tracing\nalgorithm overcomes the associated overhead and allows a user-defined middle\nground between the performance of distance field traversal and the improved\nvisual quality of hardware ray tracing.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.07253v1"
    },
    {
        "title": "Street Layout Design via Conditional Adversarial Learning",
        "authors": [
            "Lehao Yang",
            "Long Li",
            "Qihao Chen",
            "Jiling Zhang",
            "Tian Feng",
            "Wei Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Designing high-quality urban street layouts has long been in high demand, but\nentangles notable challenges. Conventional methods based on deep generative\nmodels are yet to fill the gap on integrating both natural and socioeconomic\nfactors in the design loop. In this paper, we propose a novel urban street\nlayout design method based on conditional adversarial learning. Specifically, a\nconditional generative adversarial network trained on a real-world dataset\nsynthesizes street layout images from the feature map, into which an\nautoencoder fuses a set of natural and socioeconomic data for a region of\ninterest; The following extraction module generates high-quality street layout\ngraphs corresponding to the synthesized images. Experiments and evaluations\nsuggest that the proposed method outputs various urban street layouts that are\nvisually and structurally alike their real-world counterparts, which can be\nused to support the creation of high-quality urban virtual environments.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.08186v1"
    },
    {
        "title": "Subspace Culling for Ray-Box Intersection",
        "authors": [
            "Atsushi Yoshimura",
            "Takahiro Harada"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Ray tracing is an essential operation for realistic image synthesis. The\nacceleration of ray tracing has been studied for a long period of time because\nalgorithms such as light transport simulations require a large amount of ray\ntracing. One of the major approaches to accelerate the intersections is to use\nbounding volumes for early pruning for primitives in the volume. The\naxis-aligned bounding box is a popular bounding volume for ray tracing because\nof its simplicity and efficiency. However, the conservative bounding volume may\nproduce extra empty space in addition to its content. Especially, primitives\nthat are thin and diagonal to the axis give false-positive hits on the box\nvolume due to the extra space. Although more complex bounding volumes such as\noriented bounding boxes may reduce more false-positive hits, they are\ncomputationally expensive. In this paper, we propose a novel culling approach\nto reduce false-positive hits for the bounding box by embedding a binary voxel\ndata structure to the volume. As a ray is represented as a conservative voxel\nvolume as well in our approach, the ray--voxel intersection is cheaply done by\nbitwise AND operations. Our method is applicable to hierarchical data\nstructures such as bounding volume hierarchy (BVH). It reduces false-positive\nhits due to the ray--box test and reduces the number of intersections during\nthe traversal of BVH in ray tracing. We evaluate the reduction of intersections\nwith several scenes and show the possibility of performance improvement despite\nthe culling overhead. We also introduce a compression approach with a lookup\ntable for our voxel data. We show that our compressed voxel data achieves\nsignificant false-positive reductions with a small amount of memory.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.08343v1"
    },
    {
        "title": "Stochastic Porous Microstructures",
        "authors": [
            "Zhongren Wang",
            "Lihao Tian",
            "Xiaokang Liu",
            "Andrei Sharf",
            "Lin Lu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Stochastic porous structures are ubiquitous in natural phenomena and have\ngained considerable traction across diverse domains owing to their exceptional\nphysical properties. The recent surge in interest in microstructures can be\nattributed to their impressive attributes, such as a high strength-to-weight\nratio, isotropic elasticity, and bio-inspired design principles.\nNotwithstanding, extant stochastic structures are predominantly generated via\nprocedural modeling techniques, which present notable difficulties in\nrepresenting geometric microstructures with periodic boundaries, thereby\nleading to intricate simulations and computational overhead. In this\nmanuscript, we introduce an innovative method for designing stochastic\nmicrostructures that guarantees the periodicity of each microstructure unit to\nfacilitate homogenization. We conceptualize each pore and the interconnecting\ntunnel between proximate pores as Gaussian kernels and leverage a modified\nversion of the minimum spanning tree technique to assure pore connectivity. We\nharness the dart-throwing strategy to stochastically produce pore locations,\ntailoring the distribution law to enforce boundary periodicity. We subsequently\nemploy the level-set technique to extract the stochastic microstructures.\nConclusively, we adopt Wang tile rules to amplify the stochasticity at the\nboundary of the microstructure unit, concurrently preserving periodicity\nconstraints among units. Our methodology offers facile parametric control of\nthe designed stochastic microstructures. Experimental outcomes on 3D models\nmanifest the superior isotropy and energy absorption performance of the\nstochastic porous microstructures. We further corroborate the efficacy of our\nmodeling strategy through simulations of mechanical properties and empirical\nexperiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.09176v1"
    },
    {
        "title": "A Convex Optimization Framework for Regularized Geodesic Distances",
        "authors": [
            "Michal Edelstein",
            "Nestor Guillen",
            "Justin Solomon",
            "Mirela Ben-Chen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a general convex optimization problem for computing regularized\ngeodesic distances. We show that under mild conditions on the regularizer the\nproblem is well posed. We propose three different regularizers and provide\nanalytical solutions in special cases, as well as corresponding efficient\noptimization algorithms. Additionally, we show how to generalize the approach\nto the all pairs case by formulating the problem on the product manifold, which\nleads to symmetric distances. Our regularized distances compare favorably to\nexisting methods, in terms of robustness and ease of calibration.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.13101v1"
    },
    {
        "title": "Differentiable Stripe Patterns for Inverse Design of Structured Surfaces",
        "authors": [
            "Juan Montes Maestre",
            "Yinwei Du",
            "Ronan Hinchet",
            "Stelian Coros",
            "Bernhard Thomaszewski"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Stripe patterns are ubiquitous in nature and everyday life. While the\nsynthesis of these patterns has been thoroughly studied in the literature,\ntheir potential to control the mechanics of structured materials remains\nlargely unexplored. In this work, we introduce Differentiable Stripe Patterns\n-- a computational approach for automated design of physical surfaces\nstructured with stripe-shaped bi-material distributions. Our method builds on\nthe work by Knoppel and colleagues for generating globally-continuous and\nequally-spaced stripe patterns. To unlock the full potential of this design\nspace, we propose a gradient-based optimization tool to automatically compute\nstripe patterns that best approximate macromechanical performance goals.\nSpecifically, we propose a computational model that combines solid shell finite\nelements with XFEM for accurate and fully-differentiable modeling of elastic\nbi-material surfaces. To resolve non-uniqueness problems in the original\nmethod, we furthermore propose a robust formulation that yields unique and\ndifferentiable stripe patterns. %Finally, we introduce design space\nregularizers to avoid numerical singularities and improve stripe neatness We\ncombine these components with equilibrium state derivatives into an end-to-end\ndifferentiable pipeline that enables inverse design of mechanical stripe\npatterns. We demonstrate our method on a diverse set of examples that\nillustrate the potential of stripe patterns as a design space for structured\nmaterials. Our simulation results are experimentally validated on physical\nprototypes.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.13841v1"
    },
    {
        "title": "Faster Ray Tracing through Hierarchy Cut Code",
        "authors": [
            "WeiLai Xiang",
            "FengQi Liu",
            "Dan Li",
            "ZhaoNan Tan",
            "PengZhan Xu",
            "MeiZhi Liu",
            "QiLong Kou"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a novel ray reordering technique to accelerate the ray tracing\nprocess by encoding and sorting rays prior to traversal. Instead of spatial\ncoordinates, our method encodes rays according to the cuts of the hierarchical\nacceleration structure, which is called the hierarchy cut code. This approach\ncan better adapt to the acceleration structure and obtain a more reliable\nencoding result. We also propose a compression scheme to decrease the sorting\noverhead by a shorter sorting key. In addition, based on the phenomenon of\nboundary drift, we theoretically explain the reason why existing reordering\nmethods cannot achieve better performance by using longer sorting keys. The\nexperiment demonstrates that our method can accelerate secondary ray tracing by\nup to 1.81 times, outperforming the existing methods. Such result proves the\neffectiveness of hierarchy cut code, and indicate that the reordering technique\ncan achieve greater performance improvement, which worth further research.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.16652v2"
    },
    {
        "title": "Joint Optimization of Triangle Mesh, Material, and Light from Neural\n  Fields with Neural Radiance Cache",
        "authors": [
            "Jiakai Sun",
            "Zhanjie Zhang",
            "Tianyi Chu",
            "Guangyuan Li",
            "Lei Zhao",
            "Wei Xing"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Traditional inverse rendering techniques are based on textured meshes, which\nnaturally adapts to modern graphics pipelines, but costly differentiable\nmulti-bounce Monte Carlo (MC) ray tracing poses challenges for modeling global\nillumination. Recently, neural fields has demonstrated impressive\nreconstruction quality but falls short in modeling indirect illumination. In\nthis paper, we introduce a simple yet efficient inverse rendering framework\nthat combines the strengths of both methods. Specifically, given pre-trained\nneural field representing the scene, we can obtain an initial estimate of the\nsigned distance field (SDF) and create a Neural Radiance Cache (NRC), an\nenhancement over the traditional radiance cache used in real-time rendering. By\nusing the former to initialize differentiable marching tetrahedrons (DMTet) and\nthe latter to model indirect illumination, we can compute the global\nillumination via single-bounce differentiable MC ray tracing and jointly\noptimize the geometry, material, and light through back propagation.\nExperiments demonstrate that, compared to previous methods, our approach\neffectively prevents indirect illumination effects from being baked into\nmaterials, thus obtaining the high-quality reconstruction of triangle mesh,\nPhysically-Based (PBR) materials, and High Dynamic Range (HDR) light probe.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.16800v1"
    },
    {
        "title": "HuMoT: Human Motion Representation using Topology-Agnostic Transformers\n  for Character Animation Retargeting",
        "authors": [
            "Lucas Mourot",
            "Ludovic Hoyet",
            "François Le Clerc",
            "Pierre Hellier"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Motion retargeting is the long-standing problem in character animation that\nconsists in transferring and adapting the motion of a source character to\nanother target character. A typical application is the creation of motion\nsequences from off-the-shelf motions by transferring them onto new characters.\nMotion retargeting is also promising to increase interoperability of existing\nanimation systems and motion databases, as they often differ in the structure\nof the skeleton(s) considered. Moreover, since the goal of motion retargeting\nis to abstract and transfer motion dynamics, effective solutions might provide\nexpressive and powerful human motion models in which operations such as\ncleaning or editing are easier. In this article, we present a novel neural\nnetwork architecture for retargeting that extracts an abstract representation\nof human motion agnostic to skeleton topology and morphology. Based on\ntransformers, our model is able to encode and decode motion sequences with\nvariable morphology and topology -- extending the current scope of retargeting\n-- while supporting skeleton topologies not seen during the training phase.\nMore specifically, our model is structured as an autoencoder, and encoding and\ndecoding are separately conditioned on skeleton templates to extract and\ncontrol morphology and topology. Beyond motion retargeting, our model has many\napplications since our abstract representation is a convenient space to embed\nmotion data from different sources. It may potentially be benefical to a number\nof data-driven methods, allowing them to combine scarce specialised motion\ndatasets (e.g. with style or contact annotations) and larger general motion\ndatasets, for improved performance and generalisation ability. Moreover, we\nshow that our model can be useful for other applications beyond retargeting,\nincluding motion denoising and joint upsampling.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.18897v3"
    },
    {
        "title": "Towards Neural Path Tracing in SRAM",
        "authors": [
            "Mark Pupilli"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present an experimental neural path tracer designed to exploit the large\non-chip memory of Graphcore intelligence-processing-units (IPUs). This open\nsource renderer demonstrates how to map path tracing to the novel software and\nhardware architecture and is a useful tool for analysing in-cache\nneural-rendering scenarios. Such scenarios will be increasingly important if\nrasterisation is replaced by combinations of ray/path tracing, neural-radiance\ncaching, and AI denoising/up-scaling, for which small neural networks are\nalready routinely employed. A detailed description of the implementation also\nserves as a self-contained resource for more general software design on IPU.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.20061v1"
    },
    {
        "title": "Algebraic Smooth Occluding Contours",
        "authors": [
            "Ryan Capouellez",
            "Jiacheng Dai",
            "Aaron Hertzmann",
            "Denis Zorin"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Computing occluding contours is a key building block of non-photorealistic\nrendering, but producing contours with consistent visibility has been\nnotoriously challenging. This paper describes the first general-purpose smooth\nsurface construction for which the occluding contours can be computed in closed\nform. For a given input mesh and camera viewpoint, we produce a $G^1$\npiecewise-quadratic surface approximating the mesh. We show how the image-space\noccluding contours of this representation may then be described as piecewise\nrational curves. We show that this method produces smooth contours with\nconsistent visibility much more efficiently than the state-of-the-art.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.01973v1"
    },
    {
        "title": "B-rep Matching for Collaborating Across CAD Systems",
        "authors": [
            "Benjamin Jones",
            "James Noeckel",
            "Milin Kodnongbua",
            "Ilya Baran",
            "Adriana Schulz"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Large Computer-Aided Design (CAD) projects usually require collaboration\nacross many different CAD systems as well as applications that interoperate\nwith them for manufacturing, visualization, or simulation. A fundamental\nbarrier to such collaborations is the ability to refer to parts of the geometry\n(such as a specific face) robustly under geometric and/or topological changes\nto the model. Persistent referencing schemes are a fundamental aspect of most\nCAD tools, but models that are shared across systems cannot generally make use\nof these internal referencing mechanisms, creating a challenge for\ncollaboration. In this work, we address this issue by developing a novel\nlearning-based algorithm that can automatically find correspondences between\ntwo CAD models using the standard representation used for sharing models across\nCAD systems: the Boundary-Representation (B-rep). Because our method works\ndirectly on B-reps it can be generalized across different CAD applications\nenabling collaboration.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.03169v1"
    },
    {
        "title": "Zero-shot CAD Program Re-Parameterization for Interactive Manipulation",
        "authors": [
            "Milin Kodnongbua",
            "Benjamin T. Jones",
            "Maaz Bin Safeer Ahmad",
            "Vladimir G. Kim",
            "Adriana Schulz"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Parametric CAD models encode entire families of shapes that should, in\nprinciple, be easy for designers to explore. However, in practice, parametric\nCAD models can be difficult to manipulate due to implicit semantic constraints\namong parameter values. Finding and enforcing these semantic constraints solely\nfrom geometry or programmatic shape representations is not possible because\nthese constraints ultimately reflect design intent. They are informed by the\ndesigner's experience and semantics in the real world. To address this\nchallenge, we introduce a zero-shot pipeline that leverages pre-trained large\nlanguage and image model to infer meaningful space of variations for a shape.\nWe then re-parameterize a new constrained parametric CAD program that captures\nthese variations, enabling effortless exploration of the design space along\nmeaningful design axes.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.03217v1"
    },
    {
        "title": "GarmentCode: Programming Parametric Sewing Patterns",
        "authors": [
            "Maria Korosteleva",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Garment modeling is an essential task of the global apparel industry and a\ncore part of digital human modeling. Realistic representation of garments with\nvalid sewing patterns is key to their accurate digital simulation and eventual\nfabrication. However, little-to-no computational tools provide support for\nbridging the gap between high-level construction goals and low-level editing of\npattern geometry, e.g., combining or switching garment elements, semantic\nediting, or design exploration that maintains the validity of a sewing pattern.\nWe suggest the first DSL for garment modeling -- GarmentCode -- that applies\nprinciples of object-oriented programming to garment construction and allows\ndesigning sewing patterns in a hierarchical, component-oriented manner. The\nprogramming-based paradigm naturally provides unique advantages of component\nabstraction, algorithmic manipulation, and free-form design parametrization. We\nadditionally support the construction process by automating typical low-level\ntasks like placing a dart at a desired location. In our prototype garment\nconfigurator, users can manipulate meaningful design parameters and body\nmeasurements, while the construction of pattern geometry is handled by garment\nprograms implemented with GarmentCode. Our configurator enables the free\nexploration of rich design spaces and the creation of garments using\ninterchangeable, parameterized components. We showcase our approach by\nproducing a variety of garment designs and retargeting them to different body\nshapes using our configurator.\n  Project page: https://igl.ethz.ch/projects/garmentcode/\n",
        "pdf_link": "http://arxiv.org/pdf/2306.03642v2"
    },
    {
        "title": "Improved Mesh Processing using Distorted Pole Spherical Coordinates",
        "authors": [
            "Grzegorz Borowik",
            "Michał Balicki",
            "Michał Kasprzak",
            "Piotr Cukier"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The Cartesian coordinate system is the most commonly used system in computer\nvisualization. This is due to its ease of use and processing speed. However, it\nis not always suitable for a given problem. Angular measures often allow us to\noperate more efficiently on a three-dimensional model. When dealing with issues\nrelated to the processing of three-dimensional objects described using polygon\nmeshes, it often happens that these standard systems do not satisfy specific\nproperties that are crucial to us. The topic of the paper is to discuss a\nspecific transformation to spherical coordinates with distorted poles, which\nallows us to eliminate singular points from the determined subset of the mesh\nand bypass inconvenient seam lines in its two-dimensional projection, which can\nhinder further calculations.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.04625v1"
    },
    {
        "title": "Sampling Visible GGX Normals with Spherical Caps",
        "authors": [
            "Jonathan Dupuy",
            "Anis Benyoub"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Importance sampling the distribution of visible GGX normals requires sampling\nthose of a hemisphere. In this work, we introduce a novel method for sampling\nsuch visible normals. Our method builds upon the insight that a hemispherical\nmirror reflects parallel light rays uniformly within a solid angle shaped as a\nspherical cap. This spherical cap has the same apex as the hemispherical\nmirror, and its aperture given by the angle formed by the orientation of that\napex and the direction of incident light rays. Based on this insight, we sample\nGGX visible normals as halfway vectors between a given incident direction and\ndirections drawn from its associated spherical cap. Our resulting\nimplementation is even simpler than that of Heitz and leads to up to systematic\nspeed-ups in our benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.05044v2"
    },
    {
        "title": "Real-Time Rendering of Glinty Appearances using Distributed Binomial\n  Laws on Anisotropic Grids",
        "authors": [
            " Deliot",
            " Thomas",
            " Belcour",
            " Laurent"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this work, we render in real-time glittery materials caused by discrete\nflakes on the surface. To achieve this, one has to count the number of flakes\nreflecting the light towards the camera within every texel covered by a given\npixel footprint. To do so, we derive a counting method for arbitrary footprints\nthat, unlike previous work, outputs the correct statistics. We combine this\ncounting method with an anisotropic parameterization of the texture space that\nreduces the number of texels falling under a pixel footprint. This allows our\nmethod to run with both stable performance and 1.5X to 5X faster than the\nstate-of-the-art.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.05051v1"
    },
    {
        "title": "Transfer Function Optimization for Comparative Volume Rendering",
        "authors": [
            "Christoph Neuhauser",
            "Rüdiger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Direct volume rendering is often used to compare different 3D scalar fields.\nThe choice of the transfer function which maps scalar values to color and\nopacity plays a critical role in this task. We present a technique for the\nautomatic optimization of a transfer function so that rendered images of a\nsecond field match as good as possible images of a field that has been rendered\nwith some other transfer function. This enables users to see whether\ndifferences in the visualizations can be solely attributed to the choice of\ntransfer function or remain after optimization. We propose and compare two\ndifferent approaches to solve this problem, a voxel-based solution solving a\nleast squares problem, and an image-based solution using differentiable volume\nrendering for optimization. We further propose a residual-based visualization\nto emphasize the differences in information content.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.05885v1"
    },
    {
        "title": "Local Deformation for Interactive Shape Editing",
        "authors": [
            "Honglin Chen",
            "Changxi Zheng",
            "Kevin Wampler"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce a novel regularization for localizing an elastic-energy-driven\ndeformation to only those regions being manipulated by the user. Our local\ndeformation features a natural region of influence, which is automatically\nadaptive to the geometry of the shape, the size of the deformation and the\nelastic energy in use. We further propose a three-block ADMM-based optimization\nto efficiently minimize the energy and achieve interactive frame rates. Our\napproach avoids the artifacts of other alternative methods, is simple and easy\nto implement, does not require tedious control primitive setup and generalizes\nacross different dimensions and elastic energies. We demonstrates the\neffectiveness and efficiency of our localized deformation tool through a\nvariety of local editing scenarios, including 1D, 2D, 3D elasticity and cloth\ndeformation.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.06550v1"
    },
    {
        "title": "Neural Intersection Function",
        "authors": [
            "Shin Fujieda",
            "Chih-Chen Kao",
            "Takahiro Harada"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The ray casting operation in the Monte Carlo ray tracing algorithm usually\nadopts a bounding volume hierarchy (BVH) to accelerate the process of finding\nintersections to evaluate visibility. However, its characteristics are\nirregular, with divergence in memory access and branch execution, so it cannot\nachieve maximum efficiency on GPUs. This paper proposes a novel Neural\nIntersection Function based on a multilayer perceptron whose core operation\ncontains only dense matrix multiplication with predictable memory access. Our\nmethod is the first solution integrating the neural network-based approach and\nBVH-based ray tracing pipeline into one unified rendering framework. We can\nevaluate the visibility and occlusion of secondary rays without traversing the\nmost irregular and time-consuming part of the BVH and thus accelerate ray\ncasting. The experiments show the proposed method can reduce the secondary ray\ncasting time for direct illumination by up to 35% compared to a BVH-based\nimplementation and still preserve the image quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.07191v1"
    },
    {
        "title": "Constructing Printable Surfaces with View-Dependent Appearance",
        "authors": [
            "Maxine Perroni-Scharf",
            "Szymon Rusinkiewicz"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a method for the digital fabrication of surfaces whose appearance\nvaries based on viewing direction. The surfaces are constructed from a mesh of\nbars arranged in a self-occluding colored heightfield that creates the desired\nview-dependent effects. At the heart of our method is a novel and simple\ndifferentiable rendering algorithm specifically designed to render colored 3D\nheightfields and enable efficient calculation of the gradient of appearance\nwith respect to heights and colors. This algorithm forms the basis of a\ncoarse-to-fine ML-based optimization process that adjusts the heights and\ncolors of the strips to minimize the loss between the desired and real surface\nappearance from each viewpoint, deriving meshes that can then be fabricated\nusing a 3D printer. Using our method, we demonstrate both synthetic and\nreal-world fabricated results with view-dependent appearance.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.07449v1"
    },
    {
        "title": "Position-Based Nonlinear Gauss-Seidel for Quasistatic Hyperelasticity",
        "authors": [
            "Yizhou Chen",
            "Yushan Han",
            "Jingyu Chen",
            "Joseph Teran"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Position based dynamics is a powerful technique for simulating a variety of\nmaterials. Its primary strength is its robustness when run with limited\ncomputational budget. We develop a novel approach to address problems with PBD\nfor quasistatic hyperelastic materials. Even though PBD is based on the\nprojection of static constraints, PBD is best suited for dynamic simulations.\nThis is particularly relevant since the efficient creation of large data sets\nof plausible, but not necessarily accurate elastic equilibria is of increasing\nimportance with the emergence of quasistatic neural networks. Furthermore, PBD\nprojects one constraint at a time. We show that ignoring the effects of\nneighboring constraints limits its convergence and stability properties. Recent\nworks have shown that PBD can be related to the Gauss-Seidel approximation of a\nLagrange multiplier formulation of backward Euler time stepping, where each\nconstraint is solved/projected independently of the others in an iterative\nfashion. We show that a position-based, rather than constraint-based nonlinear\nGauss-Seidel approach solves these problems. Our approach retains the essential\nPBD feature of stable behavior with constrained computational budgets, but also\nallows for convergent behavior with expanded budgets. We demonstrate the\nefficacy of our method on a variety of representative hyperelastic problems and\nshow that both successive over relaxation (SOR) and Chebyshev acceleration can\nbe easily applied.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.09021v1"
    },
    {
        "title": "One-to-Many Spectral Upsampling of Reflectances and Transmittances",
        "authors": [
            "Laurent Belcour",
            "Pacal Barla",
            "Gael Guennebaud"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Spectral rendering is essential for the production of physically-plausible\nsynthetic images, but requires to introduce several changes in the content\ngeneration pipeline. In particular, the authoring of spectral material\nproperties (e.g., albedo maps, indices of refraction, transmittance\ncoefficients) raises new problems.While a large panel of computer graphics\nmethods exists to upsample a RGB color to a spectrum, they all provide a\none-to-one mapping. This limits the ability to control interesting color\nchanges such as the Usambara effect or metameric spectra. In this work, we\nintroduce a one-to-many mapping in which we show how we can explore the set of\nall spectra reproducing a given input color. We apply this method to different\ncolour changing effects such as vathochromism -- the change of color with\ndepth, and metamerism.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.11464v2"
    },
    {
        "title": "Decoupled Boundary Handling in SPH",
        "authors": [
            "Rustam Akhunov",
            "Andreas Kolb"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Particle-based boundary representations are frequently used in Smoothed\nParticle Hydrodynamics (SPH) due to their simple integration into fluid\nsolvers. Commonly, incompressible fluid solvers estimate the current density\nand corresponding forces in case the current density exceeds the rest density\nto push fluid particles apart. Close to the boundary, the calculation of the\nfluid particles' density involves both, neighboring fluid and neighboring\nboundary particles, yielding an overestimation of density, and, subsequently,\nwrong pressure forces and wrong velocities leading to the disturbed fluid\nparticles' behavior in the vicinity of the boundary. In this paper, we present\na detailed explanation of this disturbed fluid particle behavior, which is\nmainly due to the combined or coupled handling of the fluid-fluid particle and\nthe fluid-boundary particle interaction. We propose the decoupled handling of\nboth interaction types, leading to two densities for a given fluid particle,\ni.e., fluid-induced density and boundary-induced density. In our approach, we\nalternately apply the corresponding fluid-induced and boundary-induced forces\nduring pressure estimation. This separation avoids force overestimation and\nreduces unintended fluid dynamics near the boundary, as well as a consistent\nfluid-boundary distance across different fluid amounts and different\nparticle-based boundary handling methods. We compare our method with two\nregular state-of-the-art methods in different experiments and show how our\nmethod handles detailed boundary shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12277v1"
    },
    {
        "title": "BPM: Blended Piecewise Moebius Maps",
        "authors": [
            "Shir Rorberg",
            "Amir Vaxman",
            "Mirela Ben-Chen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a novel Moebius interpolator that takes as an input a discrete map\nbetween the vertices of two planar triangle meshes, and outputs a smooth map on\nthe input domain. The output map interpolates the discrete map, is continuous\nbetween triangles, and has low quasi-conformal distortion when the input map is\ndiscrete conformal. Our map leads to considerably smoother texture transfer\ncompared to the alternatives, even on very coarse triangulations. Furthermore,\nour approach has a closed-form expression, is local, applicable to any discrete\nmap, and leads to smooth results even for extreme deformations. Finally, by\nworking with local intrinsic coordinates, our approach is easily generalizable\nto discrete maps between a surface triangle mesh and a planar mesh, i.e., a\nplanar parameterization. We compare our method with existing approaches, and\ndemonstrate better texture transfer results, and lower quasi-conformal errors.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12792v1"
    },
    {
        "title": "Let's Resonate! How to Elicit Improvisation and Letting Go in\n  Interactive Digital Art",
        "authors": [
            "Jean-François Jégo",
            "Margherita Bergamo Meneghini"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Participatory art allows for the spectator to be a participant or a viewer\nable to engage actively with interactive art. Real-time technologies offer new\nways to create participative artworks. We hereby investigate how to engage\nparticipation through movement in interactive digital art, and what this\nengagement can awaken, focusing on the ways to elicit improvisation and letting\ngo. We analyze two Virtual Reality installations, ''InterACTE'' and ''Eve,\ndance is an unplaceable place,'' involving body movement, dance, creativity and\nthe presence of an observing audience. We evaluate the premises, the setup, and\nthe feedback of the spectators in the two installations. We propose a model\nfollowing three different perspectives of resonance: 1. Inter Resonance between\nSpectator and Artwork, which involves curiosity, imitation, playfulness and\nimprovisation. 2. Inner Resonance of Spectator him/herself, where embodiment\nand creativity contribute to the sense of being present and letting go. 3.\nCollective Resonance between Spectator/Artwork and Audience, which is\nstimulated by curiosity, and triggers motor contagion, engagement and\ngathering. The two analyzed examples seek to awaken open-minded communicative\npossibilities through the use of interactive digital artworks. Moreover, the\nneed to recognize and develop the idea of resonance becomes increasingly\nimportant in this time of urgency to communicate, understand and support\ncollectivity.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12832v1"
    },
    {
        "title": "Magenta Green Screen: Spectrally Multiplexed Alpha Matting with Deep\n  Colorization",
        "authors": [
            "Dmitriy Smirnov",
            "Chloe LeGendre",
            "Xueming Yu",
            "Paul Debevec"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce Magenta Green Screen, a novel machine learning--enabled matting\ntechnique for recording the color image of a foreground actor and a\nsimultaneous high-quality alpha channel without requiring a special camera or\nmanual keying techniques. We record the actor on a green background but light\nthem with only red and blue foreground lighting. In this configuration, the\ngreen channel shows the actor silhouetted against a bright, even background,\nwhich can be used directly as a holdout matte, the inverse of the actor's alpha\nchannel. We then restore the green channel of the foreground using a machine\nlearning colorization technique. We train the colorization model with an\nexample sequence of the actor lit by white lighting, yielding convincing and\ntemporally stable colorization results. We further show that time-multiplexing\nthe lighting between Magenta Green Screen and Green Magenta Screen allows the\ntechnique to be practiced under what appears to be mostly normal lighting. We\ndemonstrate that our technique yields high-quality compositing results when\nimplemented on a modern LED virtual production stage. The alpha channel data\nobtainable with our technique can provide significantly higher quality training\ndata for natural image matting algorithms to support future ML matting\nresearch.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.13702v1"
    },
    {
        "title": "ROAR: Robust Adaptive Reconstruction of Shapes Using Planar Projections",
        "authors": [
            "Amir Barda",
            "Yotam Erel",
            "Yoni Kasten",
            "Amit H. Bermano"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The majority of existing large 3D shape datasets contain meshes that lend\nthemselves extremely well to visual applications such as rendering, yet tend to\nbe topologically invalid (i.e, contain non-manifold edges and vertices,\ndisconnected components, self-intersections). Therefore, it is of no surprise\nthat state of the art studies in shape understanding do not explicitly use this\n3D information. In conjunction with this, triangular meshes remain the dominant\nshape representation for many downstream tasks, and their connectivity remain a\nrelatively untapped source of potential for more profound shape reasoning. In\nthis paper, we introduce ROAR, an iterative geometry/topology evolution\napproach to reconstruct 2-manifold triangular meshes from arbitrary 3D shape\nrepresentations, that is highly suitable for large existing in-the-wild\ndatasets. ROAR leverages the visual prior large datasets exhibit by evolving\nthe geometry of the mesh via a 2D render loss, and a novel 3D projection loss,\nthe Planar Projection. After each geometry iteration, our system performs\ntopological corrections. Self-intersections are reduced following a\ngeometrically motivated attenuation term, and resolution is added to required\nregions using a novel face scoring function. These steps alternate until\nconvergence is achieved, yielding a high-quality manifold mesh. We evaluate\nROAR on the notoriously messy yet popular dataset ShapeNet, and present\nShapeROAR - a topologically valid yet still geometrically accurate version of\nShapeNet. We compare our results to state-of-the-art reconstruction methods and\ndemonstrate superior shape faithfulness, topological correctness, and\ntriangulation quality. In addition, we demonstrate reconstructing a mesh from\nneural Signed Distance Functions (SDF), and achieve comparable Chamfer distance\nwith much fewer SDF sampling operations than the commonly used Marching Cubes\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.00690v1"
    },
    {
        "title": "Interpolation of Point Distributions for Digital Stippling",
        "authors": [
            "Germán Arroyo",
            "Domingo Martín",
            "Tobias Isenberg"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a new way to merge any two point distribution approaches using\ndistance fields. Our new process allows us to produce digital stippling that\nfills areas with stipple dots without visual artifacts as well as includes\nclear linear features without fussiness. Our merging thus benefits from past\nwork that can optimize for either goal individually, yet typically by\nsacrificing the other. The new possibility of combining any two distributions\nusing different distance field functions and their parameters also allows us to\nproduce a vast range of stippling styles, which we demonstrate as well.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.00938v1"
    },
    {
        "title": "Study on Virtual Gear Hobbing Simulation and Gear Tooth Surface Accuracy",
        "authors": [
            "Zhi Geng",
            "Gang Li"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper presents a digital simulation method for the hobbing process of\ncylindrical gears. Based on the gear generation principle, taking the\nprofessional software as the tool, the problem of virtual hobbing simulation on\ninvolute helical gears was studied, and the virtual hobbing simulation of\nhobbing on the whole gear was completed by using macros of CATIA V5. The\nvalidity of this method was validated by analyzing the tooth surface accuracy\nerror of the model which was below 0.001 mm between the virtual tooth surface\nand the theoretical tooth surface and the possible factors that affected the\ntooth surface accuracy during manufacturing were also carried on the\ndiscussion. It offers a fictitious three-D platform for studying the principle\nof manufacture errors of a gear-cutting machine as well as the finite element\nanalysis between the ideal tooth surface and the erroneous tooth surface.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.06270v1"
    },
    {
        "title": "Intrinsic Mesh Simplification",
        "authors": [
            "Randy Shoemaker",
            "Sam Sartor",
            "Pieter Peers"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper presents a novel simplification method for removing vertices from\nan intrinsic triangulation corresponding to extrinsic vertices lying on\nnear-developable (i.e., with limited Gaussian curvature) and general surfaces.\nWe greedily process all intrinsic vertices with an absolute Gaussian curvature\nbelow a user selected threshold. For each vertex, we repeatedly perform local\nintrinsic edge flips until the vertex reaches the desired valence (three for\ninternal vertices or two for boundary vertices) such that removal of the vertex\nand incident edges can be locally performed in the intrinsic triangulation.\nEach removed vertex's intrinsic location is tracked via (intrinsic) barycentric\ncoordinates that are updated to reflect changes in the intrinsic triangulation.\nWe demonstrate the robustness and effectiveness of our method on the Thingi10k\ndataset and analyze the effect of the curvature threshold on the solutions of\nPDEs.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.07115v1"
    },
    {
        "title": "Search Me Knot, Render Me Knot: Embedding Search and Differentiable\n  Rendering of Knots in 3D",
        "authors": [
            "Aalok Gangopadhyay",
            "Paras Gupta",
            "Tarun Sharma",
            "Prajwal Singh",
            "Shanmuganathan Raman"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce the problem of knot-based inverse perceptual art. Given multiple\ntarget images and their corresponding viewing configurations, the objective is\nto find a 3D knot-based tubular structure whose appearance resembles the target\nimages when viewed from the specified viewing configurations. To solve this\nproblem, we first design a differentiable rendering algorithm for rendering\ntubular knots embedded in 3D for arbitrary perspective camera configurations.\nUtilizing this differentiable rendering algorithm, we search over the space of\nknot configurations to find the ideal knot embedding. We represent the knot\nembeddings via homeomorphisms of the desired template knot, where the\nhomeomorphisms are parametrized by the weights of an invertible neural network.\nOur approach is fully differentiable, making it possible to find the ideal 3D\ntubular structure for the desired perceptual art using gradient-based\noptimization. We propose several loss functions that impose additional physical\nconstraints, enforcing that the tube is free of self-intersection, lies within\na predefined region in space, satisfies the physical bending limits of the tube\nmaterial and the material cost is within a specified budget. We demonstrate\nthrough results that our knot representation is highly expressive and gives\nimpressive results even for challenging target images in both single view as\nwell as multiple view constraints. Through extensive ablation study we show\nthat each of the proposed loss function is effective in ensuring physical\nrealizability. We construct a real world 3D-printed object to demonstrate the\npractical utility of our approach. To the best of our knowledge, we are the\nfirst to propose a fully differentiable optimization framework for knot-based\ninverse perceptual art.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.08652v4"
    },
    {
        "title": "Reducing Ambiguities in Line-based Density Plots by Image-space\n  Colorization",
        "authors": [
            "Yumeng Xue",
            "Patrick Paetzold",
            "Rebecca Kehlbeck",
            "Bin Chen",
            "Kin Chung Kwan",
            "Yunhai Wang",
            "Oliver Deussen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Line-based density plots are used to reduce visual clutter in line charts\nwith a multitude of individual lines. However, these traditional density plots\nare often perceived ambiguously, which obstructs the user's identification of\nunderlying trends in complex datasets. Thus, we propose a novel image space\ncoloring method for line-based density plots that enhances their\ninterpretability. Our method employs color not only to visually communicate\ndata density but also to highlight similar regions in the plot, allowing users\nto identify and distinguish trends easily. We achieve this by performing\nhierarchical clustering based on the lines passing through each region and\nmapping the identified clusters to the hue circle using circular MDS.\nAdditionally, we propose a heuristic approach to assign each line to the most\nprobable cluster, enabling users to analyze density and individual lines. We\nmotivate our method by conducting a small-scale user study, demonstrating the\neffectiveness of our method using synthetic and real-world datasets, and\nproviding an interactive online tool for generating colored line-based density\nplots.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.10447v2"
    },
    {
        "title": "Fabrication-Aware Strip-Decomposable Quadrilateral Meshes",
        "authors": [
            "Ioanna Mitropoulou",
            "Amir Vaxman",
            "Olga Diamanti",
            "Benjamin Dillenburger"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Strip-decomposable quadrilateral (SDQ) meshes, i.e., quad meshes that can be\ndecomposed into two transversal strip networks, are vital in numerous\nfabrication processes; examples include woven structures, surfaces from sheets,\ncustom rebar, or cable-net structures. However, their design is often\nchallenging and includes tedious manual work, and there is a lack of\nmethodologies for editing such meshes while preserving their strip\ndecomposability. We present an interactive methodology to generate and edit SDQ\nmeshes aligned to user-defined directions, while also incorporating desirable\nproperties to the strips for fabrication. Our technique is based on the\ncomputation of two coupled transversal tangent direction fields, integrated\ninto two overlapping networks of strips on the surface. As a case study, we\nconsider the fabrication scenario of robotic non-planar 3D printing of freefrom\nshell surfaces and apply the presented methodology to design and fabricate\nnon-planar print paths.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14020v1"
    },
    {
        "title": "Compact Phase Histograms for Guided Exploration of Periodicity",
        "authors": [
            "Max Franke",
            "Steffen Koch"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Periodically occurring accumulations of events or measured values are present\nin many time-dependent datasets and can be of interest for analyses. The\nfrequency of such periodic behavior is often not known in advance, making it\ndifficult to detect and tedious to explore. Automated analysis methods exist,\nbut can be too costly for smooth, interactive analysis. We propose a compact\nvisual representation that reveals periodicity by showing a phase histogram for\na given period length that can be used standalone or in combination with other\nlinked visualizations. Our approach supports guided, interactive analyses by\nsuggesting other period lengths to explore, which are ranked based on two\nquality measures. We further describe how the phase can be mapped to visual\nrepresentations in other views to reveal periodicity there.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15483v3"
    },
    {
        "title": "Quasi-Monte Carlo Algorithms (not only) for Graphics Software",
        "authors": [
            "Alexander Keller",
            "Carsten Wächter",
            "Nikolaus Binder"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Quasi-Monte Carlo methods have become the industry standard in computer\ngraphics. For that purpose, efficient algorithms for low discrepancy sequences\nare discussed. In addition, numerical pitfalls encountered in practice are\nrevealed. We then take a look at massively parallel quasi-Monte Carlo\nintegro-approximation for image synthesis by light transport simulation. Beyond\nsuperior uniformity, low discrepancy points may be optimized with respect to\nadditional criteria, such as noise characteristics at low sampling rates or the\nquality of low-dimensional projections.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15584v1"
    },
    {
        "title": "Simulating the Geometric Growth of the Marine Sponge Crella Incrustans",
        "authors": [
            "Joshua O'Hagan",
            "Andrew Chalmers",
            "Taehyun Rhee"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Simulating marine sponge growth helps marine biologists analyze, measure, and\npredict the effects that the marine environment has on marine sponges, and vice\nversa. This paper describes a way to simulate and grow geometric models of the\nmarine sponge Crella incrustans while considering environmental factors\nincluding fluid flow and nutrients. The simulation improves upon prior work by\nchanging the skeletal architecture of the sponge in the growth model to better\nsuit the structure of Crella incrustans. The change in skeletal architecture\nand other simulation parameters are then evaluated qualitatively against photos\nof a real-life Crella incrustans sponge. The results support the hypothesis\nthat changing the skeletal architecture from radiate accretive to Halichondrid\nproduces a sponge model which is closer in resemblance to Crella incrustans\nthan the prior work.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.00474v3"
    },
    {
        "title": "Using The Polynomial Particle-In-Cell Method For Liquid-Fabric\n  Interaction",
        "authors": [
            "Robert Dennison",
            "Steve Maddock"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Liquid-fabric interaction simulations using particle-in-cell (PIC) based\nmodels have been used to simulate a wide variety of phenomena and yield\nimpressive visual results. However, these models suffer from numerical damping\ndue to the data interpolation between the particles and grid. Our paper\naddresses this by using the polynomial PIC (PolyPIC) model instead of the\naffine PIC (APIC) model that is used in current state-of-the-art wet cloth\nmodels. The affine transfers of the APIC model are replaced by the higher order\npolynomials of PolyPIC, thus reducing numerical dissipation and improving\nresolution of vorticial details. This improved energy preservation enables more\ndynamic simulations to be generated although this is at an increased\ncomputational cost.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01060v2"
    },
    {
        "title": "Interactive High-Resolution Simulation of Granular Material",
        "authors": [
            "Alexander Sommer",
            "Ulrich Schwanecke",
            "Elmar Schömer"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce a particle-based simulation method for granular material in\ninteractive frame rates. We divide the simulation into two decoupled steps. In\nthe first step, a relatively small number of particles is accurately simulated\nwith a constraint-based method. Here, all collisions and the resulting friction\nbetween the particles are taken into account. In the second step, the small\nnumber of particles is significantly increased by an efficient sampling\nalgorithm without creating additional artifacts. The method is particularly\nrobust and allows relatively large time steps, which makes it well suited for\nreal-time applications. With our method, up to 500k particles can be computed\nin interactive frame rates on consumer CPUs without relying on GPU support for\nmassive parallel computing. This makes it well suited for applications where a\nlot of GPU power is already needed for render tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01629v1"
    },
    {
        "title": "LEAVEN -- Lightweight Surface and Volume Mesh Sampling Application for\n  Particle-based Simulations",
        "authors": [
            "Alexander Sommer",
            "Ulrich Schwanecke"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present an easy-to-use and lightweight surface and volume mesh sampling\nstandalone application tailored for the needs of particle-based simulation. We\ndescribe the surface and volume sampling algorithms used in LEAVEN in a\nbeginner-friendly fashion. Furthermore, we describe a novel method of\ngenerating random volume samples that satisfy blue noise criteria by modifying\na surface sampling algorithm. We aim to lower one entry barrier for starting\nwith particle-based simulations while still pose a benefit to advanced users.\nThe goal is to provide a useful tool to the community and lowering the need for\nheavyweight third-party applications, especially for starters.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01633v1"
    },
    {
        "title": "No Free Slide: Spurious Contact Forces in Incremental Potential Contact",
        "authors": [
            "Yinwei Du",
            "Yue Li",
            "Stelian Coros",
            "Bernhard Thomaszewski"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Modeling contact between deformable solids is a fundamental problem in\ncomputer animation, mechanical design, and robotics. Existing methods based on\n$C^0$-discretizations -- piece-wise linear or polynomial surfaces -- suffer\nfrom discontinuities and irregularities in tangential contact forces, which can\nsignificantly affect simulation outcomes and even prevent convergence. To\novercome this limitation, we employ smooth surface representations for both\ncontacting bodies. Through a series of test cases, we show that our approach\noffers advantages over existing methods in terms of accuracy and robustness for\nboth forward and inverse problems. The contributions of our work include\nidentifying the limitations of existing methods, examining the advantages of\nsmooth surface representation, and proposing forward and inverse problems to\nanalyze contact force irregularities.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01696v1"
    },
    {
        "title": "Merge Tree Geodesics and Barycenters with Path Mappings",
        "authors": [
            "Florian Wetzels",
            "Mathieu Pont",
            "Julien Tierny",
            "Christoph Garth"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Comparative visualization of scalar fields is often facilitated using\nsimilarity measures such as edit distances. In this paper, we describe a novel\napproach for similarity analysis of scalar fields that combines two recently\nintroduced techniques: Wasserstein geodesics/barycenters as well as path\nmappings, a branch decomposition-independent edit distance. Effectively, we are\nable to leverage the reduced susceptibility of path mappings to small\nperturbations in the data when compared with the original Wasserstein distance.\nOur approach therefore exhibits superior performance and quality in typical\ntasks such as ensemble summarization, ensemble clustering, and temporal\nreduction of time series, while retaining practically feasible runtimes. Beyond\nstudying theoretical properties of our approach and discussing implementation\naspects, we describe a number of case studies that provide empirical insights\ninto its utility for comparative visualization, and demonstrate the advantages\nof our method in both synthetic and real-world scenarios. We supply a C++\nimplementation that can be used to reproduce our results.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.03672v1"
    },
    {
        "title": "Real and complexified configuration spaces for spherical 4-bar linkages",
        "authors": [
            "Zeyuan He",
            "Kentaro Hayakawa",
            "Makoto Ohsaki"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This note is a complete library of symbolic parametrized expressions for both\nreal and complexified configuration spaces of a spherical 4-bar linkage.\nBuilding upon the previous work from Izmestiev, (2016, Section 2), this library\nexpands on the expressions by incorporating all four folding angles across all\npossible linkage length choices, along with the polynomial relation between\ndiagonals (spherical arcs). Furthermore, a complete MATLAB app script is\nincluded, enabling visualization and parametrization. The derivations are\npresented in a detailed manner, ensuring accessibility for researchers across\ndiverse disciplines.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.03765v2"
    },
    {
        "title": "Explicit Topology Optimization of Conforming Voronoi Foams",
        "authors": [
            "Ming Li",
            "Jingqiao Hu",
            "Wei Chen",
            "Weipeng Kong",
            "Jin Huang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Topology optimization is able to maximally leverage the high DOFs and\nmechanical potentiality of porous foams but faces three fundamental challenges:\nconforming to free-form outer shapes, maintaining geometric connectivity\nbetween adjacent cells, and achieving high simulation accuracy. To resolve the\nissues, borrowing the concept from Voronoi tessellation, we propose to use the\nsite (or seed) positions and radii of the beams as the DOFs for open-cell foam\ndesign. Such DOFs cover extensive design space and have clear geometrical\nmeaning, which makes it easy to provide explicit controls (e.g. granularity).\nDuring the gradient-based optimization, the foam topology can change freely,\nand some seeds may even be pushed out of the shape, which greatly alleviates\nthe challenges of prescribing a fixed underlying grid. The mechanical property\nof our foam is computed from its highly heterogeneous density field counterpart\ndiscretized on a background mesh, with a much improved accuracy via a new\nmaterial-aware numerical coarsening method. We also explore the\ndifferentiability of the open-cell Voronoi foams w.r.t. its seed locations, and\npropose a local finite difference method to estimate the derivatives\nefficiently. We do not only show the improved foam performance of our Voronoi\nfoam in comparison with classical topology optimization approaches, but also\ndemonstrate its advantages in various settings, especially when the target\nvolume fraction is extremely low.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.04001v1"
    },
    {
        "title": "Scalable Hypergraph Visualization",
        "authors": [
            "Peter Oliver",
            "Eugene Zhang",
            "Yue Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Hypergraph visualization has many applications in network data analysis.\nRecently, a polygon-based representation for hypergraphs has been proposed with\ndemonstrated benefits. However, the polygon-based layout often suffers from\nexcessive self-intersections when the input dataset is relatively large. In\nthis paper, we propose a framework in which the hypergraph is iteratively\nsimplified through a set of atomic operations. Then, the layout of the simplest\nhypergraph is optimized and used as the foundation for a reverse process that\nbrings the simplest hypergraph back to the original one, but with an improved\nlayout. At the core of our approach is the set of atomic simplification\noperations and an operation priority measure to guide the simplification\nprocess. In addition, we introduce necessary definitions and conditions for\nhypergraph planarity within the polygon representation. We extend our approach\nto handle simultaneous simplification and layout optimization for both the\nhypergraph and its dual. We demonstrate the utility of our approach with\ndatasets from a number of real-world applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.05043v1"
    },
    {
        "title": "Neural Super-Resolution for Real-time Rendering with Radiance\n  Demodulation",
        "authors": [
            "Jia Li",
            "Ziling Chen",
            "Xiaolong Wu",
            "Lu Wang",
            "Beibei Wang",
            "Lei Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  It is time-consuming to render high-resolution images in applications such as\nvideo games and virtual reality, and thus super-resolution technologies become\nincreasingly popular for real-time rendering. However, it is challenging to\npreserve sharp texture details, keep the temporal stability and avoid the\nghosting artifacts in real-time super-resolution rendering. To address this\nissue, we introduce radiance demodulation to separate the rendered image or\nradiance into a lighting component and a material component, considering the\nfact that the light component is smoother than the rendered image so that the\nhigh-resolution material component with detailed textures can be easily\nobtained. We perform the super-resolution on the lighting component only and\nre-modulate it with the high-resolution material component to obtain the final\nsuper-resolution image with more texture details. A reliable warping module is\nproposed by explicitly marking the occluded regions to avoid the ghosting\nartifacts. To further enhance the temporal stability, we design a\nframe-recurrent neural network and a temporal loss to aggregate the previous\nand current frames, which can better capture the spatial-temporal consistency\namong reconstructed frames. As a result, our method is able to produce\ntemporally stable results in real-time rendering with high-quality details,\neven in the challenging 4 $\\times$ 4 super-resolution scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.06699v2"
    },
    {
        "title": "Voxlines: Streamline Transparency through Voxelization and\n  View-Dependent Line Orders",
        "authors": [
            "Besm Osman",
            "Mestiez Pereira",
            "Huub van de Wetering",
            "Maxime Chamberland"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  As tractography datasets continue to grow in size, there is a need for\nimproved visualization methods that can capture structural patterns occurring\nin large tractography datasets. Transparency is an increasingly important\naspect of finding these patterns in large datasets but is inaccessible to\ntractography due to performance limitations. In this paper, we propose a\nrendering method that achieves performant rendering of transparent streamlines,\nallowing for exploration of deeper brain structures interactively. The method\nachieves this through a novel approximate order-independent transparency method\nthat utilizes voxelization and caching view-dependent line orders per voxel. We\ncompare our transparency method with existing tractography visualization\nsoftware in terms of performance and the ability to capture deeper structures\nin the dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.08436v1"
    },
    {
        "title": "Dr.Bokeh: DiffeRentiable Occlusion-aware Bokeh Rendering",
        "authors": [
            "Yichen Sheng",
            "Zixun Yu",
            "Lu Ling",
            "Zhiwen Cao",
            "Cecilia Zhang",
            "Xin Lu",
            "Ke Xian",
            "Haiting Lin",
            "Bedrich Benes"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Bokeh is widely used in photography to draw attention to the subject while\neffectively isolating distractions in the background. Computational methods\nsimulate bokeh effects without relying on a physical camera lens. However, in\nthe realm of digital bokeh synthesis, the two main challenges for bokeh\nsynthesis are color bleeding and partial occlusion at object boundaries. Our\nprimary goal is to overcome these two major challenges using physics principles\nthat define bokeh formation. To achieve this, we propose a novel and accurate\nfiltering-based bokeh rendering equation and a physically-based occlusion-aware\nbokeh renderer, dubbed Dr.Bokeh, which addresses the aforementioned challenges\nduring the rendering stage without the need of post-processing or data-driven\napproaches. Our rendering algorithm first preprocesses the input RGBD to obtain\na layered scene representation. Dr.Bokeh then takes the layered representation\nand user-defined lens parameters to render photo-realistic lens blur. By\nsoftening non-differentiable operations, we make Dr.Bokeh differentiable such\nthat it can be plugged into a machine-learning framework. We perform\nquantitative and qualitative evaluations on synthetic and real-world images to\nvalidate the effectiveness of the rendering quality and the differentiability\nof our method. We show Dr.Bokeh not only outperforms state-of-the-art bokeh\nrendering algorithms in terms of photo-realism but also improves the depth\nquality from depth-from-defocus.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.08843v1"
    },
    {
        "title": "Probabilistic Gradient-Based Extrema Tracking",
        "authors": [
            "Emma Nilsson",
            "Jonas Lukasczyk",
            "Talha Bin Masood",
            "Christoph Garth",
            "Ingrid Hotz"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Feature tracking is a common task in visualization applications, where\nmethods based on topological data analysis (TDA) have successfully been applied\nin the past for feature definition as well as tracking. In this work, we focus\non tracking extrema of temporal scalar fields. A family of TDA approaches\naddress this task by establishing one-to-one correspondences between extrema\nbased on discrete gradient vector fields. More specifically, two extrema of\nsubsequent time steps are matched if they fall into their respective ascending\nand descending manifolds. However, due to this one-to-one assignment, these\napproaches are prone to fail where, e.g., extrema are located in regions with\nlow gradient magnitude, or are located close to boundaries of the manifolds.\nTherefore, we propose a probabilistic matching that captures a larger set of\npossible correspondences via neighborhood sampling, or by computing the overlap\nof the manifolds. We illustrate the usefulness of the approach with two\napplication cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.08956v1"
    },
    {
        "title": "GIPC: Fast and stable Gauss-Newton optimization of IPC barrier energy",
        "authors": [
            "Kemeng Huang",
            "Floyd Chitalu",
            "Huancheng Lin",
            "Taku Komura"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Barrier functions are crucial for maintaining an intersection and inversion\nfree simulation trajectory but existing methods which directly use distance can\nrestrict implementation design and performance. We present an approach to\nrewriting the barrier function for arriving at an efficient and robust\napproximation of its Hessian. The key idea is to formulate a simplicial\ngeometric measure of contact using mesh boundary elements, from which analytic\neigensystems are derived and enhanced with filtering and stiffening terms that\nensure robustness with respect to the convergence of a Project-Newton solver. A\nfurther advantage of our rewriting of the barrier function is that it naturally\ncaters to the notorious case of nearly-parallel edge-edge contacts for which we\nalso present a novel analytic eigensystem. Our approach is thus well suited for\nstandard second order unconstrained optimization strategies for resolving\ncontacts, minimizing nonlinear nonconvex functions where the Hessian may be\nindefinite. The efficiency of our eigensystems alone yields a 3x speedup over\nthe standard IPC barrier formulation. We further apply our analytic proxy\neigensystems to produce an entirely GPU-based implementation of IPC with\nsignificant further acceleration.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.09400v4"
    },
    {
        "title": "Reach For the Spheres: Tangency-Aware Surface Reconstruction of SDFs",
        "authors": [
            "Silvia Sellán",
            "Christopher Batty",
            "Oded Stein"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Signed distance fields (SDFs) are a widely used implicit surface\nrepresentation, with broad applications in computer graphics, computer vision,\nand applied mathematics. To reconstruct an explicit triangle mesh surface\ncorresponding to an SDF, traditional isosurfacing methods, such as Marching\nCubes and and its variants, are typically used. However, these methods overlook\nfundamental properties of SDFs, resulting in reconstructions that exhibit\nsevere oversmoothing and feature loss. To address this shortcoming, we propose\na novel method based on a key insight: each SDF sample corresponds to a\nspherical region that must lie fully inside or outside the surface, depending\non its sign, and that must be tangent to the surface at some point. Leveraging\nthis understanding, we formulate an energy that gauges the degree of violation\nof tangency constraints by a proposed surface. We then employ a gradient flow\nthat minimizes our energy, starting from an initial triangle mesh that\nencapsulates the surface. This algorithm yields superior reconstructions to\nprevious methods, even with sparsely sampled SDFs. Our approach provides a more\nnuanced understanding of SDFs and offers significant improvements in surface\nreconstruction.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.09813v1"
    },
    {
        "title": "Variational Bonded Discrete Element Method with Manifold Optimization",
        "authors": [
            "Jia-Ming Lu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper proposes a novel approach that combines variational integration\nwith the bonded discrete element method (BDEM) to achieve faster and more\naccurate fracture simulations. The approach leverages the efficiency of\nimplicit integration and the accuracy of BDEM in modeling fracture phenomena.\nWe introduce a variational integrator and a manifold optimization approach\nutilizing a nullspace operator to speed up the solving of\nquaternion-constrained systems. Additionally, the paper presents an element\npacking and surface reconstruction method specifically designed for bonded\ndiscrete element methods. Results from the experiments prove that the proposed\nmethod offers 2.8 to 12 times faster state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.10459v1"
    },
    {
        "title": "Patternshop: Editing Point Patterns by Image Manipulation",
        "authors": [
            "Xingchang Huang",
            "Tobias Ritschel",
            "Hans-Peter Seidel",
            "Pooran Memari",
            "Gurprit Singh"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Point patterns are characterized by their density and correlation. While\nspatial variation of density is well-understood, analysis and synthesis of\nspatially-varying correlation is an open challenge. No tools are available to\nintuitively edit such point patterns, primarily due to the lack of a compact\nrepresentation for spatially varying correlation. We propose a low-dimensional\nperceptual embedding for point correlations. This embedding can map point\npatterns to common three-channel raster images, enabling manipulation with\noff-the-shelf image editing software. To synthesize back point patterns, we\npropose a novel edge-aware objective that carefully handles sharp variations in\ndensity and correlation. The resulting framework allows intuitive and\nbackward-compatible manipulation of point patterns, such as recoloring,\nrelighting to even texture synthesis that have not been available to 2D point\npattern design before. Effectiveness of our approach is tested in several user\nexperiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.10517v4"
    },
    {
        "title": "Patch-Grid: An Efficient and Feature-Preserving Neural Implicit Surface\n  Representation",
        "authors": [
            "Guying Lin",
            "Lei Yang",
            "Congyi Zhang",
            "Hao Pan",
            "Yuhan Ping",
            "Guodong Wei",
            "Taku Komura",
            "John Keyser",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Neural implicit representations are known to be more compact for depicting 3D\nshapes than traditional discrete representations. However, the neural\nrepresentations tend to round sharp corners or edges and struggle to represent\nsurfaces with open boundaries. Moreover, they are slow to train. We present a\nunified neural implicit representation, called Patch-Grid, that fits to complex\nshapes efficiently, preserves sharp features, and effectively models surfaces\nwith open boundaries and thin geometric features. Our superior efficiency comes\nfrom embedding each surface patch into a local latent volume and decoding it\nusing a shared MLP decoder, which is pretrained on various local surface\ngeometries. With this pretrained decoder fixed, fitting novel shapes and local\nshape updates can be done efficiently. The faithful preservation of sharp\nfeatures is enabled by adopting a novel merge grid to perform local\nconstructive solid geometry (CSG) combinations of surface patches in the cells\nof an adaptive Octree, yielding better robustness than using a global CSG\nconstruction as proposed in the literature. Experiments show that our\nPatch-Grid method faithfully captures shapes with complex sharp features, open\nboundaries and thin structures, and outperforms existing learning-based methods\nin both efficiency and quality for surface fitting and local shape updates.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.13934v1"
    },
    {
        "title": "P2M: A Fast Solver for Querying Distance from Point to Mesh Surface",
        "authors": [
            "Chen Zong",
            "Jiacheng Xu",
            "Jiantao Song",
            "Shuangmin Chen",
            "Shiqing Xin",
            "Wenping Wang",
            "Changhe Tu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Most of the existing point-to-mesh distance query solvers, such as Proximity\nQuery Package (PQP), Embree and Fast Closest Point Query (FCPW), are based on\nbounding volume hierarchy (BVH). The hierarchical organizational structure\nenables one to eliminate the vast majority of triangles that do not help find\nthe closest point. In this paper, we develop a totally different algorithmic\nparadigm, named P2M, to speed up point-to-mesh distance queries. Our original\nintention is to precompute a KD tree (KDT) of mesh vertices to approximately\nencode the geometry of a mesh surface containing vertices, edges and faces.\nHowever, it is very likely that the closest primitive to the query point is an\nedge e (resp., a face f), but the KDT reports a mesh vertex \\u{psion} instead.\nWe call \\u{psion} an interceptor of e (resp., f). The main contribution of this\npaper is to invent a simple yet effective interception inspection rule and an\nefficient flooding interception inspection algorithm for quickly finding out\nall the interception pairs. Once the KDT and the interception table are\nprecomputed, the query stage proceeds by first searching the KDT and then\nlooking up the interception table to retrieve the closest geometric primitive.\nStatistics show that our query algorithm runs many times faster than the\nstate-of-the-art solvers.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.16084v1"
    },
    {
        "title": "Fast Compressed Segmentation Volumes for Scientific Visualization",
        "authors": [
            "Max Piochowiak",
            "Carsten Dachsbacher"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Voxel-based segmentation volumes often store a large number of labels and\nvoxels, and the resulting amount of data can make storage, transfer, and\ninteractive visualization difficult. We present a lossless compression\ntechnique which addresses these challenges. It processes individual small\nbricks of a segmentation volume and compactly encodes the labelled regions and\ntheir boundaries by an iterative refinement scheme. The result for each brick\nis a list of labels, and a sequence of operations to reconstruct the brick\nwhich is further compressed using rANS-entropy coding. As the relative\nfrequencies of operations are very similar across bricks, the entropy coding\ncan use global frequency tables for an entire data set which enables efficient\nand effective parallel (de)compression. Our technique achieves high throughput\n(up to gigabytes per second both for compression and decompression) and strong\ncompression ratios of about 1% to 3% of the original data set size while being\napplicable to GPU-based rendering. We evaluate our method for various data sets\nfrom different fields and demonstrate GPU-based volume visualization with\non-the-fly decompression, level-of-detail rendering (with optional on-demand\nstreaming of detail coefficients to the GPU), and a caching strategy for\ndecompressed bricks for further performance improvement.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.16619v2"
    },
    {
        "title": "Visual-Guided Mesh Repair",
        "authors": [
            "Zhongtian Zheng",
            "Xifeng Gao",
            "Zherong Pan",
            "Wei Li",
            "Peng-Shuai Wang",
            "Guoping Wang",
            "Kui Wu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Mesh repair is a long-standing challenge in computer graphics and related\nfields. Converting defective meshes into watertight manifold meshes can greatly\nbenefit downstream applications such as geometric processing, simulation,\nfabrication, learning, and synthesis. In this work, we first introduce three\nvisual measures for visibility, orientation, and openness, based on\nray-tracing. We then present a novel mesh repair framework that incorporates\nvisual measures with several critical steps, i.e., open surface closing, face\nreorientation, and global optimization, to effectively repair defective meshes,\nincluding gaps, holes, self-intersections, degenerate elements, and\ninconsistent orientations. Our method reduces unnecessary mesh complexity\nwithout compromising geometric accuracy or visual quality while preserving\ninput attributes such as UV coordinates for rendering. We evaluate our approach\non hundreds of models randomly selected from ShapeNet and Thingi10K,\ndemonstrating its effectiveness and robustness compared to existing approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.00134v1"
    },
    {
        "title": "Technical Companion to Example-Based Procedural Modeling Using Graph\n  Grammars",
        "authors": [
            "Paul Merrell"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This is a companion piece to my paper on \"Example-Based Procedural Modeling\nUsing Graph Grammars.\" This paper examines some of the theoretical issues in\nmore detail. This paper discusses some more complex parts of the\nimplementation, why certain algorithmic decisions were made, proves the\nalgorithm can solve certain classes of problems, and examines other interesting\ntheoretical questions.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.00275v1"
    },
    {
        "title": "Interactive Design and Optics-Based Visualization of Arbitrary\n  Non-Euclidean Kaleidoscopic Orbifolds",
        "authors": [
            "Jinta Zheng",
            "Eugene Zhang",
            "Yue Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Orbifolds are a modern mathematical concept that arises in the research of\nhyperbolic geometry with applications in computer graphics and visualization.\nIn this paper, we make use of rooms with mirrors as the visual metaphor for\norbifolds. Given any arbitrary two-dimensional kaleidoscopic orbifold, we\nprovide an algorithm to construct a Euclidean, spherical, or hyperbolic polygon\nto match the orbifold. This polygon is then used to create a room for which the\npolygon serves as the floor and the ceiling. With our system that implements\nM\\\"obius transformations, the user can interactively edit the scene and see the\nreflections of the edited objects. To correctly visualize non-Euclidean\norbifolds, we adapt the rendering algorithms to account for the geodesics in\nthese spaces, which light rays follow. Our interactive orbifold design system\nallows the user to create arbitrary two-dimensional kaleidoscopic orbifolds. In\naddition, our mirror-based orbifold visualization approach has the potential of\nhelping our users gain insight on the orbifold, including its orbifold notation\nas well as its universal cover, which can also be the spherical space and the\nhyperbolic space.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.01853v1"
    },
    {
        "title": "Global Topology of 3D Symmetric Tensor Fields",
        "authors": [
            "Shih-Hsuan Hung",
            "Yue Zhang",
            "Eugene Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  There have been recent advances in the analysis and visualization of 3D\nsymmetric tensor fields, with a focus on the robust extraction of tensor field\ntopology. However, topological features such as degenerate curves and neutral\nsurfaces do not live in isolation. Instead, they intriguingly interact with\neach other. In this paper, we introduce the notion of {\\em topological graph}\nfor 3D symmetric tensor fields to facilitate global topological analysis of\nsuch fields. The nodes of the graph include degenerate curves and regions\nbounded by neutral surfaces in the domain. The edges in the graph denote the\nadjacency information between the regions and degenerate curves. In addition,\nwe observe that a degenerate curve can be a loop and even a knot and that two\ndegenerate curves (whether in the same region or not) can form a link. We\nprovide a definition and theoretical analysis of individual degenerate curves\nin order to help understand why knots and links may occur. Moreover, we\ndifferentiate between wedges and trisectors, thus making the analysis more\ndetailed about degenerate curves. We incorporate this information into the\ntopological graph. Such a graph can not only reveal the global structure in a\n3D symmetric tensor field but also allow two symmetric tensor fields to be\ncompared. We demonstrate our approach by applying it to solid mechanics and\nmaterial science data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.01863v1"
    },
    {
        "title": "Realistic Volume Rendering with Environment-Synced Illumination in Mixed\n  Reality",
        "authors": [
            "Haojie Cheng",
            "Chunxiao Xu",
            "Xujing Chen",
            "Zhenxin Chen",
            "Jiajun Wang",
            "Lingxiao Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Interactive volume visualization using a mixed reality (MR) system helps\nprovide users with an intuitive spatial perception of volumetric data. Due to\nsophisticated requirements of user interaction and vision when using MR\nhead-mounted display (HMD) devices, the conflict between the realisticness and\nefficiency of direct volume rendering (DVR) is yet to be resolved. In this\npaper, a new MR visualization framework that supports interactive realistic DVR\nis proposed. An efficient illumination estimation method is used to identify\nthe high dynamic range (HDR) environment illumination captured using a panorama\ncamera. To improve the visual quality of Monte Carlo-based DVR, a new\nspatio-temporal denoising algorithm is designed. Based on a reprojection\nstrategy, it makes full use of temporal coherence between adjacent frames and\nspatial coherence between the two screens of an HMD to optimize MR rendering\nquality. Several MR development modules are also developed for related devices\nto efficiently and stably display the DVR results in an MR HMD. Experimental\nresults demonstrate that our framework can better support immersive and\nintuitive user perception during MR viewing than existing MR solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.01916v1"
    },
    {
        "title": "Simplicial Approximation of Deforming 3D Spaces for Visualizing Fusion\n  Plasma Simulation Data",
        "authors": [
            "Congrong Ren",
            "Hanqi Guo"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce a fast and invertible approximation for data simulated as 2D\nplanar meshes with connectivities along the poloidal dimension in deforming 3D\ntoroidal (donut-like) spaces generated by fusion simulations. In fusion\nsimulations, scientific variables (e.g., density and temperature) are\ninterpolated following a complex magnetic-field-line-following scheme in the\ntoroidal space represented by a cylindrical coordinate system. This deformation\nin 3D space poses challenges for visualization tasks such as volume rendering\nand isosurfacing. To address these challenges, we propose a novel paradigm for\nvisualizing and analyzing such data based on a newly developed algorithm for\nconstructing a 3D simplicial mesh within the deforming 3D space. Our algorithm\nintroduces no new nodes and operates with reduced time complexity, generating a\nmesh that connects the 2D meshes using tetrahedra while adhering to the\nconstraints on node connectivities imposed by the magnetic field-line scheme.\nIn the algorithm, we first divide the space into smaller partitions to reduce\ncomplexity based on the input geometries and constraints on connectivities.\nThen we independently search for a feasible tetrahedralization of each\npartition taking nonconvexity into consideration. We demonstrate use cases of\nour method for visualizing XGC simulation data on ITER and W7-X.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.02677v2"
    },
    {
        "title": "Adaptive Sampling of 3D Spatial Correlations for Focus+Context\n  Visualization",
        "authors": [
            "Christoph Neuhauser",
            "Josef Stumpfegger",
            "Rüdiger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Visualizing spatial correlations in 3D ensembles is challenging due to the\nvast amounts of information that need to be conveyed. Memory and time\nconstraints make it unfeasible to pre-compute and store the correlations\nbetween all pairs of domain points. We propose the embedding of adaptive\ncorrelation sampling into chord diagrams with hierarchical edge bundling to\nalleviate these constraints. Entities representing spatial regions are arranged\nalong the circular chord layout via a space-filling curve, and Bayesian optimal\nsampling is used to efficiently estimate the maximum occurring correlation\nbetween any two points from different regions. Hierarchical edge bundling\nreduces visual clutter and emphasizes the major correlation structures. By\nselecting an edge, the user triggers a focus diagram in which only the two\nregions connected via this edge are refined and arranged in a specific way in a\nsecond chord layout. For visualizing correlations between two different\nvariables, which are not symmetric anymore, we switch to showing a full\ncorrelation matrix. This avoids drawing the same edges twice with different\ncorrelation values. We introduce GPU implementations of both linear and\nnon-linear correlation measures to further reduce the time that is required to\ngenerate the context and focus views, and to even enable the analysis of\ncorrelations in a 1000-member ensemble.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.03308v3"
    },
    {
        "title": "Residency Octree: A Hybrid Approach for Scalable Web-Based Multi-Volume\n  Rendering",
        "authors": [
            "Lukas Herzberger",
            "Markus Hadwiger",
            "Robert Krüger",
            "Peter Sorger",
            "Hanspeter Pfister",
            "Eduard Gröller",
            "Johanna Beyer"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a hybrid multi-volume rendering approach based on a novel\nResidency Octree that combines the advantages of out-of-core volume rendering\nusing page tables with those of standard octrees. Octree approaches work by\nperforming hierarchical tree traversal. However, in octree volume rendering,\ntree traversal and the selection of data resolution are intrinsically coupled.\nThis makes fine-grained empty-space skipping costly. Page tables, on the other\nhand, allow access to any cached brick from any resolution. However, they do\nnot offer a clear and efficient strategy for substituting missing\nhigh-resolution data with lower-resolution data. We enable flexible\nmixed-resolution out-of-core multi-volume rendering by decoupling the cache\nresidency of multi-resolution data from a resolution-independent spatial\nsubdivision determined by the tree. Instead of one-to-one node-to-brick\ncorrespondences, each residency octree node is mapped to a set of bricks from\ndifferent resolution levels. This makes it possible to efficiently and\nadaptively choose and mix resolutions, adapt sampling rates, and compensate for\ncache misses. At the same time, residency octrees support fine-grained\nempty-space skipping, independent of the data subdivision used for caching.\nFinally, to facilitate collaboration and outreach, and to eliminate local data\nstorage, our implementation is a web-based, pure client-side renderer using\nWebGPU and WebAssembly. Our method is faster than prior approaches and\nefficient for many data channels with a flexible and adaptive choice of data\nresolution.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.04393v1"
    },
    {
        "title": "GA-Sketching: Shape Modeling from Multi-View Sketching with\n  Geometry-Aligned Deep Implicit Functions",
        "authors": [
            "Jie Zhou",
            "Zhongjin Luo",
            "Qian Yu",
            "Xiaoguang Han",
            "Hongbo Fu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Sketch-based shape modeling aims to bridge the gap between 2D drawing and 3D\nmodeling by providing an intuitive and accessible approach to create 3D shapes\nfrom 2D sketches. However, existing methods still suffer from limitations in\nreconstruction quality and multi-view interaction friendliness, hindering their\npractical application. This paper proposes a faithful and user-friendly\niterative solution to tackle these limitations by learning geometry-aligned\ndeep implicit functions from one or multiple sketches. Our method lifts 2D\nsketches to volume-based feature tensors, which align strongly with the output\n3D shape, enabling accurate reconstruction and faithful editing. Such a\ngeometry-aligned feature encoding technique is well-suited to iterative\nmodeling since features from different viewpoints can be easily memorized or\naggregated. Based on these advantages, we design a unified interactive system\nfor sketch-based shape modeling. It enables users to generate the desired\ngeometry iteratively by drawing sketches from any number of viewpoints. In\naddition, it allows users to edit the generated surface by making a few local\nmodifications. We demonstrate the effectiveness and practicality of our method\nwith extensive experiments and user studies, where we found that our method\noutperformed existing methods in terms of accuracy, efficiency, and user\nsatisfaction. The source code of this project is available at\nhttps://github.com/LordLiang/GA-Sketching.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.05946v1"
    },
    {
        "title": "Digital 3D Smocking Design",
        "authors": [
            "Jing Ren",
            "Aviv Segall",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We develop an optimization-based method to model smocking, a surface\nembroidery technique that provides decorative geometric texturing while\nmaintaining stretch properties of the fabric. During smocking, multiple pairs\nof points on the fabric are stitched together, creating non-manifold geometric\nfeatures and visually pleasing textures. Designing smocking patterns is\nchallenging, because the outcome of stitching is unpredictable: the final\ntexture is often revealed only when the whole smocking process is completed,\nnecessitating painstaking physical fabrication and time consuming\ntrial-and-error experimentation. This motivates us to seek a digital smocking\ndesign method. Straightforward attempts to compute smocked fabric geometry\nusing surface deformation or cloth simulation methods fail to produce realistic\nresults, likely due to the intricate structure of the designs, the large number\nof contacts and high-curvature folds. We instead formulate smocking as a graph\nembedding and shape deformation problem. We extract a coarse graph representing\nthe fabric and the stitching constraints, and then derive the graph structure\nof the smocked result. We solve for the 3D embedding of this graph, which in\nturn reliably guides the deformation of the high-resolution fabric mesh. Our\noptimization based method is simple, efficient, and flexible, which allows us\nto build an interactive system for smocking pattern exploration. To demonstrate\nthe accuracy of our method, we compare our results to real fabrications on a\nlarge set of smocking patterns\n",
        "pdf_link": "http://arxiv.org/pdf/2309.07201v1"
    },
    {
        "title": "Surface Extraction from Neural Unsigned Distance Fields",
        "authors": [
            "Congyi Zhang",
            "Guying Lin",
            "Lei Yang",
            "Xin Li",
            "Taku Komura",
            "Scott Schaefer",
            "John Keyser",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a method, named DualMesh-UDF, to extract a surface from unsigned\ndistance functions (UDFs), encoded by neural networks, or neural UDFs. Neural\nUDFs are becoming increasingly popular for surface representation because of\ntheir versatility in presenting surfaces with arbitrary topologies, as opposed\nto the signed distance function that is limited to representing a closed\nsurface. However, the applications of neural UDFs are hindered by the notorious\ndifficulty in extracting the target surfaces they represent. Recent methods for\nsurface extraction from a neural UDF suffer from significant geometric errors\nor topological artifacts due to two main difficulties: (1) A UDF does not\nexhibit sign changes; and (2) A neural UDF typically has substantial\napproximation errors. DualMesh-UDF addresses these two difficulties.\nSpecifically, given a neural UDF encoding a target surface $\\bar{S}$ to be\nrecovered, we first estimate the tangent planes of $\\bar{S}$ at a set of sample\npoints close to $\\bar{S}$. Next, we organize these sample points into local\nclusters, and for each local cluster, solve a linear least squares problem to\ndetermine a final surface point. These surface points are then connected to\ncreate the output mesh surface, which approximates the target surface. The\nrobust estimation of the tangent planes of the target surface and the\nsubsequent minimization problem constitute our core strategy, which contributes\nto the favorable performance of DualMesh-UDF over other competing methods. To\nefficiently implement this strategy, we employ an adaptive Octree. Within this\nframework, we estimate the location of a surface point in each of the octree\ncells identified as containing part of the target surface. Extensive\nexperiments show that our method outperforms existing methods in terms of\nsurface reconstruction quality while maintaining comparable computational\nefficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.08878v1"
    },
    {
        "title": "Neural Parametric Surfaces for Shape Modeling",
        "authors": [
            "Lei Yang",
            "Yongqing Liang",
            "Xin Li",
            "Congyi Zhang",
            "Guying Lin",
            "Alla Sheffer",
            "Scott Schaefer",
            "John Keyser",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The recent surge of utilizing deep neural networks for geometric processing\nand shape modeling has opened up exciting avenues. However, there is a\nconspicuous lack of research efforts on using powerful neural representations\nto extend the capabilities of parametric surfaces, which are the prevalent\nsurface representations in product design, CAD/CAM, and computer animation. We\npresent Neural Parametric Surfaces, the first piecewise neural surface\nrepresentation that allows coarse patch layouts of arbitrary $n$-sided surface\npatches to model complex surface geometries with high precision, offering\ngreater flexibility over traditional parametric surfaces. By construction, this\nnew surface representation guarantees $G^0$ continuity between adjacent patches\nand empirically achieves $G^1$ continuity, which cannot be attained by existing\nneural patch-based methods. The key ingredient of our neural parametric surface\nis a learnable feature complex $\\mathcal{C}$ that is embedded in a\nhigh-dimensional space $\\mathbb{R}^D$ and topologically equivalent to the patch\nlayout of the surface; each face cell of the complex is defined by\ninterpolating feature vectors at its vertices. The learned feature complex is\nmapped by an MLP-encoded function $f:\\mathcal{C} \\rightarrow \\mathcal{S}$ to\nproduce the neural parametric surface $\\mathcal{S}$. We present a surface\nfitting algorithm that optimizes the feature complex $\\mathcal{C}$ and trains\nthe neural mapping $f$ to reconstruct given target shapes with high accuracy.\nWe further show that the proposed representation along with a compact-size\nneural net can learn a plausible shape space from a shape collection, which can\nbe used for shape interpolation or shape completion from noisy and incomplete\ninput data. Extensive experiments show that neural parametric surfaces offer\ngreater modeling capabilities than traditional parametric surfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.09911v1"
    },
    {
        "title": "OptCtrlPoints: Finding the Optimal Control Points for Biharmonic 3D\n  Shape Deformation",
        "authors": [
            "Kunho Kim",
            "Mikaela Angelina Uy",
            "Despoina Paschalidou",
            "Alec Jacobson",
            "Leonidas J. Guibas",
            "Minhyuk Sung"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose OptCtrlPoints, a data-driven framework designed to identify the\noptimal sparse set of control points for reproducing target shapes using\nbiharmonic 3D shape deformation. Control-point-based 3D deformation methods are\nwidely utilized for interactive shape editing, and their usability is enhanced\nwhen the control points are sparse yet strategically distributed across the\nshape. With this objective in mind, we introduce a data-driven approach that\ncan determine the most suitable set of control points, assuming that we have a\ngiven set of possible shape variations. The challenges associated with this\ntask primarily stem from the computationally demanding nature of the problem.\nTwo main factors contribute to this complexity: solving a large linear system\nfor the biharmonic weight computation and addressing the combinatorial problem\nof finding the optimal subset of mesh vertices. To overcome these challenges,\nwe propose a reformulation of the biharmonic computation that reduces the\nmatrix size, making it dependent on the number of control points rather than\nthe number of vertices. Additionally, we present an efficient search algorithm\nthat significantly reduces the time complexity while still delivering a nearly\noptimal solution. Experiments on SMPL, SMAL, and DeformingThings4D datasets\ndemonstrate the efficacy of our method. Our control points achieve better\ntemplate-to-target fit than FPS, random search, and neural-network-based\nprediction. We also highlight the significant reduction in computation time\nfrom days to approximately 3 minutes.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.12899v2"
    },
    {
        "title": "Editing Motion Graphics Video via Motion Vectorization and\n  Transformation",
        "authors": [
            "Sharon Zhang",
            "Jiaju Ma",
            "Jiajun Wu",
            "Daniel Ritchie",
            "Maneesh Agrawala"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Motion graphics videos are widely used in Web design, digital advertising,\nanimated logos and film title sequences, to capture a viewer's attention. But\nediting such video is challenging because the video provides a low-level\nsequence of pixels and frames rather than higher-level structure such as the\nobjects in the video with their corresponding motions and occlusions. We\npresent a motion vectorization pipeline for converting motion graphics video\ninto an SVG motion program that provides such structure. The resulting SVG\nprogram can be rendered using any SVG renderer(e.g. most Web browsers) and\nedited using any SVG editor. We also introduce a program transformation API\nthat facilitates editing of a SVG motion program to create variations that\nadjust the timing, motions and/or appearances of objects. We show how the API\ncan be used to create a variety of effects including retiming object motion to\nmatch a music beat, adding motion textures to objects, and collision preserving\nappearance changes.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.14642v3"
    },
    {
        "title": "Local Positional Encoding for Multi-Layer Perceptrons",
        "authors": [
            "Shin Fujieda",
            "Atsushi Yoshimura",
            "Takahiro Harada"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  A multi-layer perceptron (MLP) is a type of neural networks which has a long\nhistory of research and has been studied actively recently in computer vision\nand graphics fields. One of the well-known problems of an MLP is the capability\nof expressing high-frequency signals from low-dimensional inputs. There are\nseveral studies for input encodings to improve the reconstruction quality of an\nMLP by applying pre-processing against the input data. This paper proposes a\nnovel input encoding method, local positional encoding, which is an extension\nof positional and grid encodings. Our proposed method combines these two\nencoding techniques so that a small MLP learns high-frequency signals by using\npositional encoding with fewer frequencies under the lower resolution of the\ngrid to consider the local position and scale in each grid cell. We demonstrate\nthe effectiveness of our proposed method by applying it to common 2D and 3D\nregression tasks where it shows higher-quality results compared to positional\nand grid encodings, and comparable results to hierarchical variants of grid\nencoding such as multi-resolution grid encoding with equivalent memory\nfootprint.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.15101v2"
    },
    {
        "title": "Doppler Time-of-Flight Rendering",
        "authors": [
            "Juhyeon Kim",
            "Wojciech Jarosz",
            "Ioannis Gkioulekas",
            "Adithya Pediredla"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce Doppler time-of-flight (D-ToF) rendering, an extension of ToF\nrendering for dynamic scenes, with applications in simulating D-ToF cameras.\nD-ToF cameras use high-frequency modulation of illumination and exposure, and\nmeasure the Doppler frequency shift to compute the radial velocity of dynamic\nobjects. The time-varying scene geometry and high-frequency modulation\nfunctions used in such cameras make it challenging to accurately and\nefficiently simulate their measurements with existing ToF rendering algorithms.\nWe overcome these challenges in a twofold manner: To achieve accuracy, we\nderive path integral expressions for D-ToF measurements under global\nillumination and form unbiased Monte Carlo estimates of these integrals. To\nachieve efficiency, we develop a tailored time-path sampling technique that\ncombines antithetic time sampling with correlated path sampling. We show\nexperimentally that our sampling technique achieves up to two orders of\nmagnitude lower variance compared to naive time-path sampling. We provide an\nopen-source simulator that serves as a digital twin for D-ToF imaging systems,\nallowing imaging researchers, for the first time, to investigate the impact of\nmodulation functions, material properties, and global illumination on D-ToF\nimaging performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.16163v3"
    },
    {
        "title": "SnB Collaborative Visualization",
        "authors": [
            "Dave Pape",
            "Amin Ghadersohi",
            "Josephine Anstey",
            "Amit Makwana"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We describe a system for visualization and editing of data in a computational\nchemistry environment. The system is a collaborative tool allowing researchers\nusing virtual reality and/or desktop computer displays to work together on\nresults of the Shake-and-Bake structure determination application.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.01772v1"
    },
    {
        "title": "View-Independent Adjoint Light Tracing for Lighting Design Optimization",
        "authors": [
            "Lukas Lipp",
            "David Hahn",
            "Pierre Ecormier-Nocca",
            "Florian Rist",
            "Michael Wimmer"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Differentiable rendering methods promise the ability to optimize various\nparameters of 3d scenes to achieve a desired result. However, lighting design\nhas so far received little attention in this field. In this paper, we introduce\na method that enables continuous optimization of the arrangement of luminaires\nin a 3d scene via differentiable light tracing. Our experiments show two major\nissues when attempting to apply existing methods from differentiable path\ntracing to this problem: first, many rendering methods produce images, which\nrestricts the ability of a designer to define lighting objectives to image\nspace. Second, most previous methods are designed for scene geometry or\nmaterial optimization and have not been extensively tested for the case of\noptimizing light sources. Currently available differentiable ray-tracing\nmethods do not provide satisfactory performance, even on fairly basic test\ncases in our experience. In this paper, we propose a novel adjoint light\ntracing method that overcomes these challenges and enables gradient-based\nlighting design optimization in a view-independent (camera-free) way. Thus, we\nallow the user to paint illumination targets directly onto the 3d scene or use\nexisting baked illumination data (e.g., light maps). Using modern ray-tracing\nhardware, we achieve interactive performance. We find light tracing\nadvantageous over path tracing in this setting, as it naturally handles\nirregular geometry, resulting in less noise and improved optimization\nconvergence. We compare our adjoint gradients to state-of-the-art image-based\ndifferentiable rendering methods. We also demonstrate that our gradient data\nworks with various common optimization algorithms, providing good convergence\nbehaviour. Qualitative comparisons with real-world scenes underline the\npractical applicability of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02043v2"
    },
    {
        "title": "Immersive ExaBrick: Visualizing Large AMR Data in the CAVE",
        "authors": [
            "Zhaoyang Wang",
            "Stefan Wesner",
            "Stefan Zellmann"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Rendering large adaptive mesh refinement (AMR) data in real-time in virtual\nreality (VR) environments is a complex challenge that demands sophisticated\ntechniques and tools. The proposed solution harnesses the ExaBrick framework\nand integrates it as a plugin in COVISE, a robust visualization system equipped\nwith the VR-centric OpenCOVER render module. This setup enables direct\nnavigation and interaction within the rendered volume in a VR environment. The\nuser interface incorporates rendering options and functions, ensuring a smooth\nand interactive experience. We show that high-quality volume rendering of AMR\ndata in VR environments at interactive rates is possible using GPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02881v1"
    },
    {
        "title": "Perceptual error optimization for Monte Carlo animation rendering",
        "authors": [
            "Miša Korać",
            "Corentin Salaün",
            "Iliyan Georgiev",
            "Pascal Grittmann",
            "Philipp Slusallek",
            "Karol Myszkowski",
            "Gurprit Singh"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Independently estimating pixel values in Monte Carlo rendering results in a\nperceptually sub-optimal white-noise distribution of error in image space.\nRecent works have shown that perceptual fidelity can be improved significantly\nby distributing pixel error as blue noise instead. Most such works have focused\non static images, ignoring the temporal perceptual effects of animation\ndisplay. We extend prior formulations to simultaneously consider the spatial\nand temporal domains, and perform an analysis to motivate a perceptually better\nspatio-temporal error distribution. We then propose a practical error\noptimization algorithm for spatio-temporal rendering and demonstrate its\neffectiveness in various configurations.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02955v1"
    },
    {
        "title": "SimLOD: Simultaneous LOD Generation and Rendering",
        "authors": [
            "Markus Schütz",
            "Lukas Herzberger",
            "Michael Wimmer"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  About: We propose an incremental LOD generation approach for point clouds\nthat allows us to simultaneously load points from disk, update an octree-based\nlevel-of-detail representation, and render the intermediate results in real\ntime while additional points are still being loaded from disk. LOD construction\nand rendering are both implemented in CUDA and share the GPU's processing\npower, but each incremental update is lightweight enough to leave enough time\nto maintain real-time frame rates.\n  Background: LOD construction is typically implemented as a preprocessing step\nthat requires users to wait before they are able to view the results in real\ntime. This approach allows users to view intermediate results right away.\n  Results: Our approach is able to stream points from an SSD and update the\noctree on the GPU at rates of up to 580 million points per second (~9.3GB/s\nfrom a PCIe 5.0 SSD) on an RTX 4090. Depending on the data set, our approach\nspends an average of about 1 to 2 ms to incrementally insert 1 million points\ninto the octree, allowing us to insert several million points per frame into\nthe LOD structure and render the intermediate results within the same frame.\n  Discussion/Limitations: We aim to provide near-instant, real-time\nvisualization of large data sets without preprocessing. Out-of-core processing\nof arbitrarily large data sets and color-filtering for higher-quality LODs are\nsubject to future work.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.03567v1"
    },
    {
        "title": "GroundLink: A Dataset Unifying Human Body Movement and Ground Reaction\n  Dynamics",
        "authors": [
            "Xingjian Han",
            "Benjamin Senderling",
            "Stanley To",
            "Deepak Kumar",
            "Emily Whiting",
            "Jun Saito"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The physical plausibility of human motions is vital to various applications\nin fields including but not limited to graphics, animation, robotics, vision,\nbiomechanics, and sports science. While fully simulating human motions with\nphysics is an extreme challenge, we hypothesize that we can treat this\ncomplexity as a black box in a data-driven manner if we focus on the ground\ncontact, and have sufficient observations of physics and human activities in\nthe real world. To prove our hypothesis, we present GroundLink, a unified\ndataset comprised of captured ground reaction force (GRF) and center of\npressure (CoP) synchronized to standard kinematic motion captures. GRF and CoP\nof GroundLink are not simulated but captured at high temporal resolution using\nforce platforms embedded in the ground for uncompromising measurement accuracy.\nThis dataset contains 368 processed motion trials (~1.59M recorded frames) with\n19 different movements including locomotion and weight-shifting actions such as\ntennis swings to signify the importance of capturing physics paired with\nkinematics. GroundLinkNet, our benchmark neural network model trained with\nGroundLink, supports our hypothesis by predicting GRFs and CoPs accurately and\nplausibly on unseen motions from various sources. The dataset, code, and\nbenchmark models are made public for further research on various downstream\ntasks leveraging the rich physics information at\nhttps://csr.bu.edu/groundlink/.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.03930v1"
    },
    {
        "title": "Robust Average Networks for Monte Carlo Denoising",
        "authors": [
            "Javor Kalojanov",
            "Kimball Thurston"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a method for converting denoising neural networks from spatial\ninto spatio-temporal ones by modifying the network architecture and loss\nfunction. We insert Robust Average blocks at arbitrary depths in the network\ngraph. Each block performs latent space interpolation with trainable weights\nand works on the sequence of image representations from the preceding spatial\ncomponents of the network. The temporal connections are kept live during\ntraining by forcing the network to predict a denoised frame from subsets of the\ninput sequence. Using temporal coherence for denoising improves image quality\nand reduces temporal flickering independent of scene or image complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.04080v2"
    },
    {
        "title": "Motion Matching for Character Animation and Virtual Reality Avatars in\n  Unity",
        "authors": [
            "Jose Luis Ponton"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Real-time animation of virtual characters has traditionally been accomplished\nby playing short sequences of animations structured in the form of a graph.\nThese methods are time-consuming to set up and scale poorly with the number of\nmotions required in modern virtual environments. The ever-increasing need for\nhighly-realistic virtual characters in fields such as entertainment, virtual\nreality, or the metaverse has led to significant advances in the field of\ndata-driven character animation. Techniques like Motion Matching have provided\nenough versatility to conveniently animate virtual characters using a selection\nof features from an animation database. Data-driven methods retain the quality\nof the captured animations, thus delivering smoother and more natural-looking\nanimations. In this work, we researched and developed a Motion Matching\ntechnique for the Unity game engine. In this thesis, we present our findings on\nhow to implement an animation system based on Motion Matching. We also\nintroduce a novel method combining body orientation prediction with Motion\nMatching to animate avatars for consumer-grade virtual reality systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.05215v1"
    },
    {
        "title": "Generate Coherent Rays Directly",
        "authors": [
            "Fengqi Liu",
            "Zaonan Tan",
            "Weilai Xiang",
            "Chenhao Lu",
            "Dan Li",
            "Xu Gong",
            "Yulong Shi",
            "Songnan Shi",
            "Qilong Kou",
            "Bo Hu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The path tracing method generates incoherent rays by randomly sampling\ndirections. This randomness makes it unsuitable for modern processor\narchitectures that rely on coherence to achieve optimal performance. Many\nefforts have been made to address this issue by reordering rays based on their\norigin, end, or direction to enhance coherence. However, a drawback of\nreordering methods is the need to encode and sort rays before tracing,\nintroducing additional overhead. We propose a technique to generate coherent\nrays directly by reusing the direction. Additionally, we introduce an\ninterleaved reuse domain partition method to mitigate the impact of sampling\ncorrelation resulting from direction reuse. We demonstrate the effectiveness of\nour approach across various scenes, establishing its superiority over\nreordering methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.07182v1"
    },
    {
        "title": "Cut-Cell Microstructures for Two-scale Structural Optimization",
        "authors": [
            "Davi Colli Tozoni",
            "Zizhou Huang",
            "Daniele Panozzo",
            "Denis Zorin"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Two-scale topology optimization, combined with the design of microstructure\nfamilies with a broad range of effective material parameters, is increasingly\nwidely used in many fabrication applications to achieve a target deformation\nbehavior for a variety of objects. The main idea of this approach is to\noptimize the distribution of material properties in the object partitioned into\nrelatively coarse cells, and then replace each cell with microstructure\ngeometry that mimics these material properties. In this paper, we focus on\nadapting this approach to complex shapes in situations when preserving the\nshape's surface is important. Our approach extends any regular (i.e. defined on\na regular lattice grid) microstructure family to complex shapes, by enriching\nit with individually optimized cut-cell tiles adapted to the geometry of the\ncut-cell. We propose an automated and robust pipeline based on this approach,\nand we show that the performance of the regular microstructure family is only\nminimally affected by our extension while allowing its use on 2D and 3D shapes\nof high complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.07890v1"
    },
    {
        "title": "Circular Average Filtering and Circular Linear Interpolation in Complex\n  Color Spaces",
        "authors": [
            "Ergun Akleman",
            "Shubham Agarwall",
            "Donald H. House",
            "Tolga Talha Yildiz"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In color spaces where the chromatic term is given in polar coordinates, the\nshortest distance between colors of the same value is circular. By converting\nsuch a space into a complex polar form with a real-valued value axis, a color\nalgebra for combining colors is immediately available. In this work, we\nintroduce two complex space operations utilizing this observation: circular\naverage filtering and circular linear interpolation. These operations produce\nArchimedean Spirals, thus guaranteeing that they operate along the shortest\npaths. We demonstrate that these operations provide an intuitive way to work in\ncertain color spaces and that they are particularly useful for obtaining better\nfiltering and interpolation results. We present a set of examples based on the\nperceptually uniform color space CIELAB or L*a*b* with its polar form CIEHLC.\nWe conclude that representing colors in a complex space with circular\noperations can provide better visual results by exploitation of the strong\nalgebraic properties of complex space C.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.08646v1"
    },
    {
        "title": "Medial Skeletal Diagram: A Generalized Medial Axis Approach for 3D Shape\n  Representation",
        "authors": [
            "Minghao Guo",
            "Bohan Wang",
            "Wojciech Matusik"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose the Medial Skeletal Diagram, a novel skeletal representation that\ntackles the prevailing issues around skeleton sparsity and reconstruction\naccuracy in existing skeletal representations. Our approach augments the\ncontinuous elements in the medial axis representation to effectively shift the\ncomplexity away from the discrete elements. To that end, we introduce\ngeneralized enveloping primitives, an enhancement over the standard primitives\nin the medial axis, which ensure efficient coverage of intricate local features\nof the input shape and substantially reduce the number of discrete elements\nrequired. Moreover, we present a computational framework for constructing a\nmedial skeletal diagram from an arbitrary closed manifold mesh. Our\noptimization pipeline ensures that the resulting medial skeletal diagram\ncomprehensively covers the input shape with the fewest primitives.\nAdditionally, each optimized primitive undergoes a post-refinement process to\nguarantee an accurate match with the source mesh in both geometry and\ntessellation. We validate our approach on a comprehensive benchmark of 100\nshapes, demonstrating the sparsity of the discrete elements and superior\nreconstruction accuracy across a variety of cases. Finally, we exemplify the\nversatility of our representation in downstream applications such as shape\ngeneration, mesh decomposition, shape optimization, mesh alignment, mesh\ncompression, and user-interactive design.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.09395v3"
    },
    {
        "title": "B-rep Boolean Resulting Model Repair by Correcting Intersection Edges\n  Based on Inference Procedure",
        "authors": [
            "Haomian Huang",
            "Li Chen",
            "Enya Shen",
            "Jianmin Wang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  As the most essential part of CAD modeling operations, boolean operations on\nB-rep CAD models often suffer from errors. Errors caused by geometric precision\nor numerical uncertainty are hard to eliminate. They will reduce the\nreliability of boolean operations and damage the integrity of the resulting\nmodels. And it is difficult to repair false boolean resulting models damaged by\nerrors. In practice, we find that the illegal boolean resulting models stem\nfrom the false intersection edges caused by errors. Therefore, this paper\nproposes an automatic method based on set reasoning to repair flawed structures\nof the boolean resulting models by correcting their topological intersection\nedges. We provide a local adaptive tolerance estimation method for each\nintersection edge based on its geometric features as well as its origin. Then,\nwe propose a set of inference mechanisms based on set operations to infer\nwhether a repair is needed based on the tolerance value and how to correct the\ninaccurate intersection edge. Our inference strategies are strictly proven,\nensuring the reliability and robustness of the repair process. The inference\nprocess will transform the problem into a geometric equivalent form less\nsusceptible to errors to get a more accurate intersection edge. Since our\ninference procedure focuses on topological features, our method can repair the\nflawed boolean resulting models, no matter what source of errors causes the\nproblem.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.10351v1"
    },
    {
        "title": "Filter-adapted spatiotemporal sampling for real-time rendering",
        "authors": [
            "William Donnelly",
            "Alan Wolfe",
            "Judith Bütepage",
            "Jon Valdés"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Stochastic sampling techniques are ubiquitous in real-time rendering, where\nperformance constraints force the use of low sample counts, leading to noisy\nintermediate results. To remove this noise, the post-processing step of\ntemporal and spatial denoising is an integral part of the real-time graphics\npipeline. The main insight presented in this paper is that we can optimize the\nsamples used in stochastic sampling such that the post-processing error is\nminimized. The core of our method is an analytical loss function which measures\npost-filtering error for a class of integrands - multidimensional Heaviside\nfunctions. These integrands are an approximation of the discontinuous functions\ncommonly found in rendering. Our analysis applies to arbitrary spatial and\nspatiotemporal filters, scalar and vector sample values, and uniform and\nnon-uniform probability distributions. We show that the spectrum of Monte Carlo\nnoise resulting from our sampling method is adapted to the shape of the filter,\nresulting in less noisy final images. We demonstrate improvements over\nstate-of-the-art sampling methods in three representative rendering tasks:\nambient occlusion, volumetric ray-marching, and color image dithering. Common\nuse noise textures, and noise generation code is available at\nhttps://github.com/electronicarts/fastnoise.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.15364v1"
    },
    {
        "title": "LiCROM: Linear-Subspace Continuous Reduced Order Modeling with Neural\n  Fields",
        "authors": [
            "Yue Chang",
            "Peter Yichen Chen",
            "Zhecheng Wang",
            "Maurizio M. Chiaramonte",
            "Kevin Carlberg",
            "Eitan Grinspun"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Linear reduced-order modeling (ROM) simplifies complex simulations by\napproximating the behavior of a system using a simplified kinematic\nrepresentation. Typically, ROM is trained on input simulations created with a\nspecific spatial discretization, and then serves to accelerate simulations with\nthe same discretization. This discretization-dependence is restrictive.\n  Becoming independent of a specific discretization would provide flexibility\nto mix and match mesh resolutions, connectivity, and type (tetrahedral,\nhexahedral) in training data; to accelerate simulations with novel\ndiscretizations unseen during training; and to accelerate adaptive simulations\nthat temporally or parametrically change the discretization.\n  We present a flexible, discretization-independent approach to reduced-order\nmodeling. Like traditional ROM, we represent the configuration as a linear\ncombination of displacement fields. Unlike traditional ROM, our displacement\nfields are continuous maps from every point on the reference domain to a\ncorresponding displacement vector; these maps are represented as implicit\nneural fields.\n  With linear continuous ROM (LiCROM), our training set can include multiple\ngeometries undergoing multiple loading conditions, independent of their\ndiscretization. This opens the door to novel applications of reduced order\nmodeling. We can now accelerate simulations that modify the geometry at\nruntime, for instance via cutting, hole punching, and even swapping the entire\nmesh. We can also accelerate simulations of geometries unseen during training.\nWe demonstrate one-shot generalization, training on a single geometry and\nsubsequently simulating various unseen geometries.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.15907v1"
    },
    {
        "title": "FLSH -- Friendly Library for the Simulation of Humans",
        "authors": [
            "Pablo Ramón",
            "Cristian Romero",
            "Javier Tapia",
            "Miguel A. Otaduy"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Computer models of humans are ubiquitous throughout computer animation and\ncomputer vision. However, these models rarely represent the dynamics of human\nmotion, as this requires adding a complex layer that solves body motion in\nresponse to external interactions and according to the laws of physics. FLSH is\na library that facilitates this task for researchers and developers who are not\ninterested in the nuisances of physics simulation, but want to easily integrate\ndynamic humans in their applications. FLSH provides easy access to three\nflavors of body physics, with different features and computational complexity:\nskeletal dynamics, full soft-tissue dynamics, and reduced-order modeling of\nsoft-tissue dynamics. In all three cases, the simulation models are built on\ntop of the pseudo-standard SMPL parametric body model.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.18206v1"
    },
    {
        "title": "GI-1.0: A Fast and Scalable Two-level Radiance Caching Scheme for\n  Real-time Global Illumination",
        "authors": [
            "Guillaume Boissé",
            "Sylvain Meunier",
            "Heloise de Dinechin",
            "Pieterjan Bartels",
            "Alexander Veselov",
            "Kenta Eto",
            "Takahiro Harada"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Real-time global illumination is key to enabling more dynamic and physically\nrealistic worlds in performance-critical applications such as games or any\nother applications with real-time constraints.Hardware-accelerated ray tracing\nin modern GPUs allows arbitrary intersection queries against the geometry,\nmaking it possible to evaluate indirect lighting entirely at runtime. However,\nonly a small number of rays can be traced at each pixel to maintain high\nframerates at ever-increasing image resolutions. Existing solutions, such as\nprobe-based techniques, approximate the irradiance signal at the cost of a few\nrays per frame but suffer from a lack of details and slow response times to\nchanges in lighting. On the other hand, reservoir-based resampling techniques\ncapture much more details but typically suffer from poorer performance and\nincreased amounts of noise, making them impractical for the current generation\nof hardware and gaming consoles. To find a balance that achieves high lighting\nfidelity while maintaining a low runtime cost, we propose a solution that\ndynamically estimates global illumination without needing any content\npreprocessing, thus enabling easy integration into existing real-time rendering\npipelines.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.19855v1"
    },
    {
        "title": "GroomGen: A High-Quality Generative Hair Model Using Hierarchical Latent\n  Representations",
        "authors": [
            "Yuxiao Zhou",
            "Menglei Chai",
            "Alessandro Pepe",
            "Markus Gross",
            "Thabo Beeler"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Despite recent successes in hair acquisition that fits a high-dimensional\nhair model to a specific input subject, generative hair models, which establish\ngeneral embedding spaces for encoding, editing, and sampling diverse\nhairstyles, are way less explored. In this paper, we present GroomGen, the\nfirst generative model designed for hair geometry composed of highly-detailed\ndense strands. Our approach is motivated by two key ideas. First, we construct\nhair latent spaces covering both individual strands and hairstyles. The latent\nspaces are compact, expressive, and well-constrained for high-quality and\ndiverse sampling. Second, we adopt a hierarchical hair representation that\nparameterizes a complete hair model to three levels: single strands, sparse\nguide hairs, and complete dense hairs. This representation is critical to the\ncompactness of latent spaces, the robustness of training, and the efficiency of\ninference. Based on this hierarchical latent representation, our proposed\npipeline consists of a strand-VAE and a hairstyle-VAE that encode an individual\nstrand and a set of guide hairs to their respective latent spaces, and a hybrid\ndensification step that populates sparse guide hairs to a dense hair model.\nGroomGen not only enables novel hairstyle sampling and plausible hairstyle\ninterpolation, but also supports interactive editing of complex hairstyles, or\ncan serve as strong data-driven prior for hairstyle reconstruction from images.\nWe demonstrate the superiority of our approach with qualitative examples of\ndiverse sampled hairstyles and quantitative evaluation of generation quality\nregarding every single component and the entire pipeline.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.02062v2"
    },
    {
        "title": "Neural Appearance Model for Cloth Rendering",
        "authors": [
            "Guan Yu Soh",
            "Zahra Montazeri"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The realistic rendering of woven and knitted fabrics has posed significant\nchallenges throughout many years. Previously, fiber-based micro-appearance\nmodels have achieved considerable success in attaining high levels of realism.\nHowever, rendering such models remains complex due to the intricate internal\nscatterings of hundreds of fibers within a yarn, requiring vast amounts of\nmemory and time to render. In this paper, we introduce a new framework to\ncapture aggregated appearance by tracing many light paths through the\nunderlying fiber geometry. We then employ lightweight neural networks to\naccurately model the aggregated BSDF, which allows for the precise modeling of\na diverse array of materials while offering substantial improvements in speed\nand reductions in memory. Furthermore, we introduce a novel importance sampling\nscheme to further speed up the rate of convergence. We validate the efficacy\nand versatility of our framework through comparisons with preceding fiber-based\nshading models as well as the most recent yarn-based model.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.04061v2"
    },
    {
        "title": "A Practical Guide to Implementing Off-Axis Stereo Projection Using\n  Existing Ray Tracing Libraries",
        "authors": [
            "Stefan Zellmann",
            "Jeff Amstutz"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Virtual reality (VR) renderers driving CAVEs and similar immersive\nenvironments use the off-axis stereo camera model so that a tracked user can\nmove freely in front of the projection plane. Geometrically, off-axis\nprojection results in asymmetric viewing frusta and generalizes the ubiquitous\nperspective camera model to support positioning off the center of the\nprojection plane. VR renderers often integrate with larger visualization\nsystems that rely on libraries for position tracking and pose estimates, for\nray tracing-based rendering, and for user interaction. We demonstrate different\nstrategies to implement off-axis stereo projection within the constraints of\ngiven VR applications and ray tracing libraries. We aim for minimal to no\nadjustments required to the internal camera representation of such libraries.\nWe include host and shader code with the article that can be directly\nintegrated in custom applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.05887v2"
    },
    {
        "title": "Digitally reproducing the artistic style of XVI century artist Antonio\n  Campelo in Alegoria Prudencia",
        "authors": [
            "Joao Fradinho Oliveira",
            "Joao Madeiras Pereira"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this work, the artistic style of the sixteenth century Portuguese artist\nAnt\\'onio Campelo in Alegoria \\`a Prud\\^encia is analyzed in order to create a\ncomputational tool that allows one to transform any 3D digital sculpture model\ninto an image that resembles the modeled style. From this analysis the problem\nis divided into two parts: detection and drawing of contour lines and the\nshading of the scene. Several techniques from Non Photorealistic Rendering\n(NPR) and from Photorealistic Rendering that can resolve the problem are\npresented and, based on this study, a possible solution is presented. Each\nmodeled rendering component is then analyzed using image based methods against\nthe proposed artistic style and parameters are adjusted for a closer match. In\nthe final stage a group of people was asked to answer a questionnaire where the\nsimilarity between the renderings of different objects and the original style\nwas classified according to their personal opinion. One of our findings is that\nalthough the source 3D objects cannot be readily found for a direct comparison,\nnor can the paper medium with centuries old damage be the same, the comparison\nof sub -parts of both images of the same topology was still possible validating\nour method and discarding other styles from the comparison.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.09211v2"
    },
    {
        "title": "Survey of Rigid Body Simulation with Extended Position Based Dynamics",
        "authors": [
            "Miguel Luis Nunes Seabra",
            "Daniel Simões Lopes",
            "João Madeiras Pereira"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Interactive real-time rigid body simulation is a crucial tool in any modern\ngame engine or 3D authoring tool. The quest for fast, robust and accurate\nsimulations is ever evolving. PBRBD (Position Based Rigid Body Dynamics), a\nrecent expansion of PBD (Position Based Dynamics), is a novel approach for this\nissue. This work aims at providing a comprehensible state-of-the art comparison\nbetween Position Based methods and other real-time simulation methods used for\nrigid body dynamics using a custom 3D physics engine for benchmarking and\ncomparing PBRBD (Position Based Rigid Body Dynamics), and some variants, with\nstate-of-the-art simulators commonly used in the gaming industry, PhysX and\nHavok. Showing that PBRBD can produce simulations that are accurate and stable,\nexcelling at maintaining stable energy levels, and allowing for a variety of\nconstraints, but is limited in its handling of stable stacks of rigid bodies.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.09327v1"
    },
    {
        "title": "ART-Owen Scrambling",
        "authors": [
            "Abdalla G. M. Ahmed",
            "Matt Pharr",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a novel algorithm for implementing Owen-scrambling, combining the\ngeneration and distribution of the scrambling bits in a single self-contained\ncompact process. We employ a context-free grammar to build a binary tree of\nsymbols, and equip each symbol with a scrambling code that affects all\ndescendant nodes. We nominate the grammar of adaptive regular tiles (ART)\nderived from the repetition-avoiding Thue-Morse word, and we discuss its\npotential advantages and shortcomings. Our algorithm has many advantages,\nincluding random access to samples, fixed time complexity, GPU friendliness,\nand scalability to any memory budget. Further, it provides two unique features\nover known methods: it admits optimization, and it is invertible, enabling\nscreen-space scrambling of the high-dimensional Sobol sampler.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.11664v1"
    },
    {
        "title": "Automatic Digitization and Orientation of Scanned Mesh Data for Floor\n  Plan and 3D Model Generation",
        "authors": [
            "Ritesh Sharma",
            "Eric Bier",
            "Lester Nelson",
            "Mahabir Bhandari",
            "Niraj Kunwar"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper describes a novel approach for generating accurate floor plans and\n3D models of building interiors using scanned mesh data. Unlike previous\nmethods, which begin with a high resolution point cloud from a laser\nrange-finder, our approach begins with triangle mesh data, as from a Microsoft\nHoloLens. It generates two types of floor plans, a \"pen-and-ink\" style that\npreserves details and a drafting-style that reduces clutter. It processes the\n3D model for use in applications by aligning it with coordinate axes,\nannotating important objects, dividing it into stories, and removing the\nceiling. Its performance is evaluated on commercial and residential buildings,\nwith experiments to assess quality and dimensional accuracy. Our approach\ndemonstrates promising potential for automatic digitization and orientation of\nscanned mesh data, enabling floor plan and 3D model generation in various\napplications such as navigation, interior design, furniture placement,\nfacilities management, building construction, and HVAC design.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.11927v1"
    },
    {
        "title": "A Compact Dynamic 3D Gaussian Representation for Real-Time Dynamic View\n  Synthesis",
        "authors": [
            "Kai Katsumata",
            "Duc Minh Vo",
            "Hideki Nakayama"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  3D Gaussian Splatting (3DGS) has shown remarkable success in synthesizing\nnovel views given multiple views of a static scene. Yet, 3DGS faces challenges\nwhen applied to dynamic scenes because 3D Gaussian parameters need to be\nupdated per timestep, requiring a large amount of memory and at least a dozen\nobservations per timestep. To address these limitations, we present a compact\ndynamic 3D Gaussian representation that models positions and rotations as\nfunctions of time with a few parameter approximations while keeping other\nproperties of 3DGS including scale, color and opacity invariant. Our method can\ndramatically reduce memory usage and relax a strict multi-view assumption. In\nour experiments on monocular and multi-view scenarios, we show that our method\nnot only matches state-of-the-art methods, often linked with slower rendering\nspeeds, in terms of high rendering quality but also significantly surpasses\nthem by achieving a rendering speed of $118$ frames per second (FPS) at a\nresolution of 1,352$\\times$1,014 on a single GPU.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.12897v2"
    },
    {
        "title": "Pitfalls of Projection: A study of Newton-type solvers for incremental\n  potentials",
        "authors": [
            "Andreas Longva",
            "Fabian Löschner",
            "José Antonio Fernández-Fernández",
            "Egor Larionov",
            "Uri M. Ascher",
            "Jan Bender"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Nonlinear systems arising from time integrators like Backward Euler can\nsometimes be reformulated as optimization problems, known as incremental\npotentials. We show through a comprehensive experimental analysis that the\nwidely used Projected Newton method, which relies on unconditional semidefinite\nprojection of Hessian contributions, typically exhibits a reduced convergence\nrate compared to classical Newton's method. We demonstrate how factors like\nresolution, element order, projection method, material model and boundary\nhandling impact convergence of Projected Newton and Newton.\n  Drawing on these findings, we propose the hybrid method Project-on-Demand\nNewton, which projects only conditionally, and show that it enjoys both the\nrobustness of Projected Newton and convergence rate of Newton. We additionally\nintroduce Kinetic Newton, a regularization-based method that takes advantage of\nthe structure of incremental potentials and avoids projection altogether. We\ncompare the four solvers on hyperelasticity and contact problems.\n  We also present a nuanced discussion of convergence criteria, and propose a\nnew acceleration-based criterion that avoids problems associated with existing\nresidual norm criteria and is easier to interpret. We finally address a\nfundamental limitation of the Armijo backtracking line search that occasionally\nblocks convergence, especially for stiff problems. We propose a novel\nparameter-free, robust line search technique to eliminate this issue.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14526v1"
    },
    {
        "title": "Visualizing Plasma Physics Simulations in Immersive Environments",
        "authors": [
            "Nuno Verdelho Trindade",
            "Oscar Amaro",
            "David Bras",
            "Daniel Goncalves",
            "João Madeiras Pereira",
            "Alfredo Ferreira"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Plasma physics simulations create complex datasets for which researchers need\nstate-of-the-art visualization tools to gain insights. These datasets are 3D in\nnature but are commonly depicted and analyzed using 2D idioms displayed on 2D\nscreens. These offer limited understandability in a domain where spatial\nawareness is key. Virtual reality (VR) can be used as an alternative to\nconventional means for analyzing such datasets. VR has been known to improve\ndepth and spatial relationship perception, which are fundamental for obtaining\ninsights into 3D plasma morphology. Likewise, VR can potentially increase user\nengagement by offering more immersive and enjoyable experiences. Methods This\nstudy presents PlasmaVR, a proof-of-concept VR tool for visualizing datasets\nresulting from plasma physics simulations. It enables immersive\nmultidimensional data visualization of particles, scalar, and vector fields and\nuses a more natural interface. The study includes user evaluation with domain\nexperts where PlasmaVR was employed to assess the possible benefits of\nimmersive environments in plasma physics visualization. The experimental group\ncomprised five plasma physics researchers who were asked to perform tasks\ndesigned to represent their typical analysis workflow. To assess the\nsuitability of the prototype for the different types of tasks, a set of\nobjective metrics, such as completion time and number of errors, were measured.\nThe prototype's usability was also evaluated using a standard System Usability\nSurvey questionnaire.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14593v1"
    },
    {
        "title": "Hypernetworks for Generalizable BRDF Representation",
        "authors": [
            "Fazilet Gokbudak",
            "Alejandro Sztrajman",
            "Chenliang Zhou",
            "Fangcheng Zhong",
            "Rafal Mantiuk",
            "Cengiz Oztireli"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we introduce a technique to estimate measured BRDFs from a\nsparse set of samples. Our approach offers accurate BRDF reconstructions that\nare generalizable to new materials. This opens the door to BDRF reconstructions\nfrom a variety of data sources. The success of our approach relies on the\nability of hypernetworks to generate a robust representation of BRDFs and a set\nencoder that allows us to feed inputs of different sizes to the architecture.\nThe set encoder and the hypernetwork also enable the compression of densely\nsampled BRDFs. We evaluate our technique both qualitatively and quantitatively\non the well-known MERL dataset of 100 isotropic materials. Our approach\naccurately 1) estimates the BRDFs of unseen materials even for an extremely\nsparse sampling, 2) compresses the measured BRDFs into very small embeddings,\ne.g., 7D.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.15783v2"
    },
    {
        "title": "Revisiting Poisson-disk Subsampling for Massive Point Cloud Decimation",
        "authors": [
            "Marc Comino-Trinidad",
            "Antonio Chica",
            "Carlos andújar"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Scanning devices often produce point clouds exhibiting highly uneven\ndistributions of point samples across the surfaces being captured. Different\npoint cloud subsampling techniques have been proposed to generate more evenly\ndistributed samples. Poisson-disk sampling approaches assign each sample a cost\nvalue so that subsampling reduces to sorting the samples by cost and then\nremoving the desired ratio of samples with the highest cost. Unfortunately,\nthese approaches compute the sample cost using pairwise distances of the points\nwithin a constant search radius, which is very costly for massive point clouds\nwith uneven densities. In this paper, we revisit Poisson-disk sampling for\npoint clouds. Instead of optimizing for equal densities, we propose to maximize\nthe distance to the closest point, which is equivalent to estimating the local\npoint density as a value inversely proportional to this distance. This\nalgorithm can be efficiently implemented using k nearest-neighbors searches.\nBesides a kd-tree, our algorithm also uses a voxelization to speed up the\nsearches required to compute per-sample costs. We propose a new strategy to\nminimize cost updates that is amenable for out-of-core operation. We\ndemonstrate the benefits of our approach in terms of performance, scalability,\nand output quality. We also discuss extensions based on adding\norientation-based and color-based terms to the cost function.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.17604v1"
    },
    {
        "title": "A Unified Particle-Based Solver for Non-Newtonian Behaviors Simulation",
        "authors": [
            "Chunlei Li",
            "Yang Gao",
            "Jiayi He",
            "Tianwei Cheng",
            "Shuai Li",
            "Aimin Hao",
            "Hong Qin"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we present a unified framework to simulate non-Newtonian\nbehaviors. We combine viscous and elasto-plastic stress into a unified particle\nsolver to achieve various non-Newtonian behaviors ranging from fluid-like to\nsolid-like. Our constitutive model is based on a Generalized Maxwell model,\nwhich incorporates viscosity, elasticity and plasticity in one non-linear\nframework by a unified way. On the one hand, taking advantage of the viscous\nterm, we construct a series of strain-rate dependent models for classical\nnon-Newtonian behaviors such as shear-thickening, shear-thinning, Bingham\nplastic, etc. On the other hand, benefiting from the elasto-plastic model, we\nempower our framework with the ability to simulate solid-like non-Newtonian\nbehaviors, i.e., visco-elasticity/plasticity. In addition, we enrich our method\nwith a heat diffusion model to make our method flexible in simulating phase\nchange. Through sufficient experiments, we demonstrate a wide range of\nnon-Newtonian behaviors ranging from viscous fluid to deformable objects. We\nbelieve this non-Newtonian model will enhance the realism of physically-based\nanimation, which has great potential for computer graphics.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.04814v1"
    },
    {
        "title": "A Digital Compositing Approach to obtain Animated Chinese Still-life\n  Paintings with Global Effects",
        "authors": [
            "Sitong Deng",
            "Ergun Akleman"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this work, we present a method for turning Chinese still-life paintings\nwith global illumination effects into dynamic paintings with moving lights. Our\ngoal is to preserve the original look and feel of still-life paintings with\nmoving lights and objects. We have developed a deceptively simple method that\ncan be computed as a composite of two animated texture images using an animated\nrendering. The compositing process can be implemented directly in an animation\nsystem such as AfterEffect, which allows for the basic compositing operation\nover animations. It is also possible to control the colors by changing the\nmaterial colors in animated rendering. We have provided a proof-of-concept\nbased on an original digital Still-Life painting that is in realist Chinese\nstyle. This approach can be used to turn almost any still-life painting into a\ndynamic painting.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.05401v1"
    },
    {
        "title": "A Prediction-Traversal Approach for Compressing Scientific Data on\n  Unstructured Meshes with Bounded Error",
        "authors": [
            "Congrong Ren",
            "Xin Liang",
            "Hanqi Guo"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We explore an error-bounded lossy compression approach for reducing\nscientific data associated with 2D/3D unstructured meshes. While existing lossy\ncompressors offer a high compression ratio with bounded error for regular grid\ndata, methodologies tailored for unstructured mesh data are lacking; for\nexample, one can compress nodal data as 1D arrays, neglecting the spatial\ncoherency of the mesh nodes. Inspired by the SZ compressor, which predicts and\nquantizes values in a multidimensional array, we dynamically reorganize nodal\ndata into sequences. Each sequence starts with a seed cell; based on a\npredefined traversal order, the next cell is added to the sequence if the\ncurrent cell can predict and quantize the nodal data in the next cell with the\ngiven error bound. As a result, one can efficiently compress the quantized\nnodal data in each sequence until all mesh nodes are traversed. This paper also\nintroduces a suite of novel error metrics, namely continuous mean squared error\n(CMSE) and continuous peak signal-to-noise ratio (CPSNR), to assess compression\nresults for unstructured mesh data. The continuous error metrics are defined by\nintegrating the error function on all cells, providing objective statistics\nacross nonuniformly distributed nodes/cells in the mesh. We evaluate our\nmethods with several scientific simulations ranging from ocean-climate models\nand computational fluid dynamics simulations with both traditional and\ncontinuous error metrics. We demonstrated superior compression ratios and\nquality than existing lossy compressors.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.06080v2"
    },
    {
        "title": "Flow Symmetrization for Parameterized Constrained Diffeomorphisms",
        "authors": [
            "Aalok Gangopadhyay",
            "Dwip Dalal",
            "Progyan Das",
            "Shanmuganathan Raman"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Diffeomorphisms play a crucial role while searching for shapes with fixed\ntopological properties, allowing for smooth deformation of template shapes.\nSeveral approaches use diffeomorphism for shape search. However, these\napproaches employ only unconstrained diffeomorphisms. In this work, we develop\nFlow Symmetrization - a method to represent a parametric family of constrained\ndiffeomorphisms that contain additional symmetry constraints such as\nperiodicity, rotation equivariance, and transflection equivariance. Our\nrepresentation is differentiable in nature, making it suitable for\ngradient-based optimization approaches for shape search. As these symmetry\nconstraints naturally arise in tiling classes, our method is ideal for\nrepresenting tile shapes belonging to any tiling class. To demonstrate the\nefficacy of our method, we design two frameworks for addressing the challenging\nproblems of Escherization and Density Estimation. The first framework is\ndedicated to the Escherization problem, where we parameterize tile shapes\nbelonging to different isohedral classes. Given a target shape, the template\ntile is deformed using gradient-based optimization to resemble the target\nshape. The second framework focuses on density estimation in identification\nspaces. By leveraging the inherent link between tiling theory and\nidentification topology, we design constrained diffeomorphisms for the plane\nthat result in unconstrained diffeomorphisms on the identification spaces.\nSpecifically, we perform density estimation on identification spaces such as\ntorus, sphere, Klein bottle, and projective plane. Through results and\nexperiments, we demonstrate that our method obtains impressive results for\nEscherization on the Euclidean plane and density estimation on non-Euclidean\nidentification spaces.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.06317v1"
    },
    {
        "title": "Ray-Tracing With a Coherent Ray-Space Hierarchy",
        "authors": [
            "Nuno Reis",
            "Vasco Costa",
            "João M. Pereira"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present an algorithm for creating an n-level ray-space hierarchy (RSH) of\ncoherent rays that runs on the GPU. Our algorithm uses rasterization to process\nthe primary rays, then uses those results as the inputs for a RSH, that\nprocesses the secondary rays. The RSH algorithm generates bundles of rays;\nhashes them, according to their attributes; and sorts them. Thus we generate a\nray list with adjacent coherent rays. To improve the rendering performance of\nthe RSH vs a more classical approach. In addition the scenes geometry is\npartitioned into a set of bounding spheres, intersected with the RSH, to\nfurther decrease the amount of false ray bundle-primitive intersection tests.\nWe show that our technique notably reduces the amount of ray-primitive\nintersection tests, required to render an image. In particular, it performs up\nto 50% better in this metric than other algorithms in this class.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.06538v1"
    },
    {
        "title": "Exploring Crowd Dynamics: Simulating Structured Behaviors through Crowd\n  Simulation Models",
        "authors": [
            "Thiago Gomes Vidal de Mello",
            "Matheus Schreiner Homrich da Silva",
            "Gabriel Fonseca Silva",
            "Soraia Raupp Musse"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper proposes the simulation of structured behaviors in a crowd of\nvirtual agents by extending the BioCrowds simulation model.\n  Three behaviors were simulated and evaluated, a queue as a generic case and\ntwo specific behaviors observed at rock concerts. The extended model\nincorporates new parameters and modifications to replicate these behaviors\naccurately. Experiments were conducted to analyze the impact of parameters on\nsimulation results, and computational performance was considered.\n  The results demonstrate the model's effectiveness in simulating structured\nbehaviors and its potential for replicating complex social phenomena in diverse\nscenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.06549v1"
    },
    {
        "title": "Hybrid-Rendering Techniques in GPU",
        "authors": [
            "Pedro Granja",
            "João Pereira"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Ray tracing has long been the holy grail of real time rendering. This\ntechnique, commonly used for photo realism, simulates the physical behavior of\nlight, at the cost of being computationally heavy. With the introduction of\nNvidia RTX graphic card family, which provides hardware support for ray\ntracing, this technique started to look like a reality for real time. However,\nthe same problems that afflicted the usage of this technique remain, and even\nwith specialized hardware it is still extremely expensive. To account for these\ndrawbacks, researchers and developers pair this technique with rasterization\nand denoising. This results in a hybrid system that tries to join the best of\nboth worlds, having both photo realistic quality and real time performance. In\nthis work we intend on further exploring hybrid render systems, offering a\nreview of the state of the art with a special focus on real time ray tracing\nand our own hybrid implementation with photo realistic quality and real time\nperformance (>30 fps), implemented using the Vulkan API. In this project, we\nhighlight the detailed analysis of the impacts of History Rectification\n(Variance Color Clamping) on the temporal filter component of the denoising\nsystem and how to overcome the introduced artifacts. Additionally, we also\nhighlight the analysis of the introduction of a separable blur on the spatial\nfilter and the introduction of Reinhard Tone Mapping prior to denoising,\nconsequently improving this procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.06827v1"
    },
    {
        "title": "MuscleVAE: Model-Based Controllers of Muscle-Actuated Characters",
        "authors": [
            "Yusen Feng",
            "Xiyan Xu",
            "Libin Liu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we present a simulation and control framework for generating\nbiomechanically plausible motion for muscle-actuated characters. We incorporate\na fatigue dynamics model, the 3CC-r model, into the widely-adopted Hill-type\nmuscle model to simulate the development and recovery of fatigue in muscles,\nwhich creates a natural evolution of motion style caused by the accumulation of\nfatigue from prolonged activities. To address the challenging problem of\ncontrolling a musculoskeletal system with high degrees of freedom, we propose a\nnovel muscle-space control strategy based on PD control. Our simulation and\ncontrol framework facilitates the training of a generative model for\nmuscle-based motion control, which we refer to as MuscleVAE. By leveraging the\nvariational autoencoders (VAEs), MuscleVAE is capable of learning a rich and\nflexible latent representation of skills from a large unstructured motion\ndataset, encoding not only motion features but also muscle control and fatigue\nproperties. We demonstrate that the MuscleVAE model can be efficiently trained\nusing a model-based approach, resulting in the production of high-fidelity\nmotions and enabling a variety of downstream tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.07340v1"
    },
    {
        "title": "PerfectTailor: Scale-Preserving 2D Pattern Adjustment Driven by 3D\n  Garment Editing",
        "authors": [
            "Anran Qi",
            "Takeo Igarashi"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We address the problem of modifying a given well-designed 2D sewing pattern\nto accommodate garment edits in the 3D space. Existing methods usually adjust\nthe sewing pattern by applying uniform flattening to the 3D garment. The\nproblems are twofold: first, it ignores local scaling of the 2D sewing pattern\nsuch as shrinking ribs of cuffs; second, it does not respect the implicit\ndesign rules and conventions of the industry, such as the use of straight edges\nfor simplicity and precision in sewing. To address those problems, we present a\npattern adjustment method that considers the non-uniform local scaling of the\n2D sewing pattern by utilizing the intrinsic scale matrix. In addition, we\npreserve the original boundary shape by an as-similar-as-possible geometric\nconstraint when desirable. We build a prototype with a set of commonly used\nalteration operations and showcase the capability of our method via a number of\nalteration examples throughout the paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.08386v2"
    },
    {
        "title": "Sketch Vision: Artificial Intelligence with Sight for Imagination",
        "authors": [
            "Demircan Tas"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Visual design relies on seeing things in different ways, acting on them, and\nseeing results to act again. Parametric design tools are often not robust to\ndesign changes that result from sketching over the visualization of their\noutput. We propose a sketch to 3d workflow as an experiment medium for\nevaluating neural networks and their latent spaces as a representation that is\nrobust to overlay sketching.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.12270v1"
    },
    {
        "title": "Recursive Camera Painting: A Method for Real-Time Painterly Renderings\n  of 3D Scenes",
        "authors": [
            "Ergun Akleman",
            "Cassie Mullins",
            "Christopher Morrison",
            "David Oh"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this work, we present the recursive camera-painting approach to obtain\npainterly smudging in real-time rendering applications. We have implemented\nrecursive camera painting as both a GPU-based ray-tracing and in a Virtual\nReality game environment. Using this approach, we can obtain dynamic 3D\nPaintings in real-time. In a camera painting, each pixel has a separate\nassociated camera whose parameters are computed from a corresponding image of\nthe same size. In recursive camera painting, we use the rendered images to\ncompute new camera parameters. When we apply this process a few times, it\ncreates painterly images that can be viewed as real-time 3D dynamic paintings.\nThese visual results are not surprising since multi-view techniques help to\nobtain painterly effects.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.12392v1"
    },
    {
        "title": "Generating Forms via Informed Motion, a Flight Inspired Method Based on\n  Wind and Topography Data",
        "authors": [
            "Demircan Tas",
            "Osman Sumer"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Generative systems are becoming a crucial part of current design practice.\nThere exist gaps however, between the digital processes, field data and\ndesigner's input. To solve this problem, multiple processes were developed in\norder to generate emergent and self-organizing design solutions that combine\nthe designer's input with surface models acquired via photogrammetry and\ngenerative design tools. Different generative design methods were utilized for\ntrials, including surface scattering based on UV coordinates, animation\nsnapshots (similar to long exposure photography) and a particle swarm algorithm\non arbitrary data, interpolated within GIS software. A large volume of adaptive\nforms that are complex, yet responsive to changes in parameters, user input,\ntopography and/or various spatial data were acquired. Resulting outputs were\nrendered and projection mapped onto the original physical model and evaluated\nfor further iterations.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.13394v1"
    },
    {
        "title": "Design Systems for Closing Gaps with Rheotomic Surfaces and Allometry",
        "authors": [
            "Demircan Tas"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This study aims to present a material based, second order design method that\nmakes the rapid creation of bridging structures in order to connect two or more\nhorizontal planes which are currently separated and three design instances\ncreated via such method. The first element of the presented method is a\ngenerative system that creates circulation surfaces through the interpolation\nof the curves defined on the current surfaces, and also creates the structural\nvolumes via rheotomic (minimal) surfaces. The second constituent is the\ninstantiation of the exposed variable, connected to a displacement algorithm\ninspired by allometry based on user input, contextual data, or simulation\nresults. The method was created with applicability through additive\nmanufacturing in consideration. The design process - similar to manufacturing -\nproceeds in a vertical manner, in order to reduce the generation of support\ngeometry as much as possible. The system includes a raster data input viable\nfor simulation results, feeding the accumulation variable in order to modify\nmaterial amount or quality with the aim of improving structural performance\nwhere stress is greater. Through the application of the method, three physical\nmodels which connect different horizontal planes were obtained. The models can\nbe evaluated with digital of physical simulations, and results can be utilized\nin an iterative manner, improving the results by each recursion.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.13398v1"
    },
    {
        "title": "Simulating Parametric Thin Shells by Bicubic Hermite Elements",
        "authors": [
            "Xingyu Ni",
            "Xuwen Chen",
            "Cheng Yu",
            "Bin Wang",
            "Baoquan Chen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this study, we present the bicubic Hermite element method (BHEM), a new\ncomputational framework devised for the elastodynamic simulation of parametric\nthin-shell structures. The BHEM is constructed based on parametric\nquadrilateral Hermite patches, which serve as a unified representation for\nshell geometry, simulation, collision avoidance, as well as rendering. Compared\nwith the commonly utilized linear FEM, the BHEM offers higher-order solution\nspaces, enabling the capture of more intricate and smoother geometries while\nemploying significantly fewer finite elements. In comparison to other\nhigh-order methods, the BHEM achieves conforming $\\mathcal{C}^1$ continuity for\nKirchhoff-Love (KL) shells with minimal complexity. Furthermore, by leveraging\nthe subdivision and convex hull properties of Hermite patches, we develop an\nefficient algorithm for ray-patch intersections, facilitating collision\nhandling in simulations and ray tracing in rendering. This eliminates the need\nfor laborious remodeling of the pre-existing parametric surface as the\nconventional approaches do. We substantiate our claims with comprehensive\nexperiments, which demonstrate the high accuracy and versatility of the\nproposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.14839v1"
    },
    {
        "title": "Interactive Visualization of Time-Varying Flow Fields Using Particle\n  Tracing Neural Networks",
        "authors": [
            "Mengjiao Han",
            "Jixian Li",
            "Sudhanshu Sane",
            "Shubham Gupta",
            "Bei Wang",
            "Steve Petruzza",
            "Chris R. Johnson"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we present a comprehensive evaluation to establish a robust\nand efficient framework for Lagrangian-based particle tracing using deep neural\nnetworks (DNNs). Han et al. (2021) first proposed a DNN-based approach to learn\nLagrangian representations and demonstrated accurate particle tracing for an\nanalytic 2D flow field. In this paper, we extend and build upon this prior work\nin significant ways. First, we evaluate the performance of DNN models to\naccurately trace particles in various settings, including 2D and 3D\ntime-varying flow fields, flow fields from multiple applications, flow fields\nwith varying complexity, as well as structured and unstructured input data.\nSecond, we conduct an empirical study to inform best practices with respect to\nparticle tracing model architectures, activation functions, and training data\nstructures. Third, we conduct a comparative evaluation of prior techniques that\nemploy flow maps as input for exploratory flow visualization. Specifically, we\ncompare our extended model against its predecessor by Han et al. (2021), as\nwell as the conventional approach that uses triangulation and Barycentric\ncoordinate interpolation. Finally, we consider the integration and adaptation\nof our particle tracing model with different viewers. We provide an interactive\nweb-based visualization interface by leveraging the efficiencies of our\nframework, and perform high-fidelity interactive visualization by integrating\nit with an OSPRay-based viewer. Overall, our experiments demonstrate that using\na trained DNN model to predict new particle trajectories requires a low memory\nfootprint and results in rapid inference. Following best practices for large 3D\ndatasets, our deep learning approach using GPUs for inference is shown to\nrequire approximately 46 times less memory while being more than 400 times\nfaster than the conventional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.14973v3"
    },
    {
        "title": "Neural BSSRDF: Object Appearance Representation Including Heterogeneous\n  Subsurface Scattering",
        "authors": [
            "Thomson TG",
            "Jeppe Revall Frisvad",
            "Ravi Ramamoorthi",
            "Henrik Wann Jensen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Monte Carlo rendering of translucent objects with heterogeneous scattering\nproperties is often expensive both in terms of memory and computation. If we do\npath tracing and use a high dynamic range lighting environment, the rendering\nbecomes computationally heavy. We propose a compact and efficient neural method\nfor representing and rendering the appearance of heterogeneous translucent\nobjects. The neural representation function resembles a bidirectional\nscattering-surface reflectance distribution function (BSSRDF). However,\nconventional BSSRDF models assume a planar half-space medium and only surface\nvariation of the material, which is often not a good representation of the\nappearance of real-world objects. Our method represents the BSSRDF of a full\nobject taking its geometry and heterogeneities into account. This is similar to\na neural radiance field, but our representation works for an arbitrary distant\nlighting environment. In a sense, we present a version of neural precomputed\nradiance transfer that captures all-frequency relighting of heterogeneous\ntranslucent objects. We use a multi-layer perceptron (MLP) with skip\nconnections to represent the appearance of an object as a function of spatial\nposition, direction of observation, and direction of incidence. The latter is\nconsidered a directional light incident across the entire non-self-shadowed\npart of the object. We demonstrate the ability of our method to store highly\ncomplex materials while having high accuracy when comparing to reference images\nof the represented object in unseen lighting environments. As compared with\npath tracing of a heterogeneous light scattering volume behind a refractive\ninterface, our method more easily enables importance sampling of the directions\nof incidence and can be integrated into existing rendering frameworks while\nachieving interactive frame rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.15711v1"
    },
    {
        "title": "Representing and Modeling Inconsistent, Impossible, and Incoherent\n  Shapes and Scenes with 2D Non-Conservative Vector Fields mapped on\n  2-Complexes",
        "authors": [
            "Ergun Akleman",
            "Youyou Wang",
            "Ozgur Gonen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we present a framework to represent mock 3D objects and\nscenes, which are not 3D but appear 3D. In our framework, each mock-3D object\nis represented using 2D non-conservative vector fields and thickness\ninformation that are mapped on 2-complexes. Mock-3D scenes are simply scenes\nconsisting of more than one mock-3D object. We demonstrated that using this\nrepresentation, we can dynamically compute a 3D shape using rays emanating from\nany given point in 3D. These mock-3D objects are view-dependent since their\ncomputed shapes depend on the positions of ray centers. Using these dynamically\ncomputed shapes, we can compute shadows, reflections, and refractions in real\ntime. This representation is mainly useful for 2D artistic applications to\nmodel incoherent, inconsistent, and impossible objects. Using this\nrepresentation, it is possible to obtain expressive depictions with shadows and\nglobal illumination effects. The representation can also be used to convert\nexisting 2D artworks into a Mock-3D form that can be interactively re-rendered.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.17046v2"
    },
    {
        "title": "Free-form Shape Modeling in XR: A Systematic Review",
        "authors": [
            "Shounak Chatterjee"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Shape modeling research in Computer Graphics has been an active area for\ndecades. The ability to create and edit complex 3D shapes has been of key\nimportance in Computer-Aided Design, Animation, Architecture, and\nEntertainment. With the growing popularity of Virtual and Augmented Reality,\nnew applications and tools have been developed for artistic content creation;\nreal-time interactive shape modeling has become increasingly important for a\ncontinuum of virtual and augmented reality environments (eXtended Reality\n(XR)). Shape modeling in XR opens new possibilities for intuitive design and\nshape modeling in an accessible way. Artificial Intelligence (AI) approaches\ngenerating shape information from text prompts are set to change how artists\ncreate and edit 3D models. There has been a substantial body of research on\ninteractive 3D shape modeling. However, there is no recent extensive review of\nthe existing techniques and what AI shape generation means for shape modeling\nin interactive XR environments. In this state-of-the-art paper, we fill this\nresearch gap in the literature by surveying free-form shape modeling work in\nXR, with a focus on sculpting and 3D sketching, the most intuitive forms of\nfree-form shape modeling. We classify and discuss these works across five\ndimensions: contribution of the articles, domain setting, interaction tool,\nauto-completion, and collaborative designing. The paper concludes by discussing\nthe disconnect between interactive 3D sculpting and sketching and how this will\nlikely evolve with the prevalence of AI shape-generation tools in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.00924v1"
    },
    {
        "title": "Immersive Serious Games for Learning Physics Concepts: The Case of\n  Density",
        "authors": [
            "Iuliia Zhurakovskaia",
            "Jeanne Vézien",
            "Cécile de Hosson",
            "Patrick Bourdot"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Training students in basic concepts of physics, such as the ones related to\nmass, volume, or density, is much more complicated than just stating the\nunderlying definitions and laws. One of the reasons for this is that most\nstudents have deeply rooted delusions and misconceptions about the behavior of\nobjects, sometimes close to magical thinking. Many innovative and promising\ntechnologies, in particular Virtual Reality (VR), can be used to enhance\nstudent learning. We compared the effectiveness of a serious immersive game in\nteaching the concept of density in various conditions: a 2D version in an\nembedded web browser and a 3D immersive game in VR. We also developed a\nspecific questionnaire to assess students' knowledge improvement. Primary\nresults have shown an increase in learning efficiency using VR. Also, most\nstudents were able to see the shortcomings of their initial theories and revise\nthem, which means that they improved their understanding of this topic.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.01831v1"
    },
    {
        "title": "Compositing with 2D Vector Fields by using Shape Maps that can represent\n  Inconsistent, Impossible, and Incoherent Shapes",
        "authors": [
            "Ergun Akleman",
            "Youyou Wang",
            "Ozgur Gonen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this paper, we present a new compositing approach to obtain stylized\nreflections and refractions with a simple control. Our approach does not\nrequire any mask or separate 3D rendering. Moreover, only one additional image\nis sufficient to obtain a composited image with convincing qualitative\nreflection and refraction effects. We have also developed linearized methods\nthat are easy to compute. Although these methods do not directly correspond to\nthe underlying physical phenomena of reflection and refraction, they can\nprovide results that are visually similar to realistic 3D rendering. The main\nadvantage of this approach is the ability to treat images as ``mock-3D'' shapes\nthat can be inserted into any digital paint system without any significant\nstructural change. The core of our approach is the shape map, which encodes 2D\nshape and thickness information for all visible points of an image of a shape.\nThis information does not have to be complete or consistent to obtain\ninteresting composites. In particular, the shape maps allow us to represent\nimpossible and incoherent shapes with 2D non-conservative vector fields.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.02200v1"
    },
    {
        "title": "Particle-Wise Higher-Order SPH Field Approximation for DVR",
        "authors": [
            "Jonathan Fischer",
            "Martin Schulze",
            "Paul Rosenthal",
            "Lars Linsen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  When employing Direct Volume Rendering (DVR) for visualizing volumetric\nscalar fields, classification is generally performed on a piecewise constant or\npiecewise linear approximation of the field on a viewing ray. Smoothed Particle\nHydrodynamics (SPH) data sets define volumetric scalar fields as the sum of\nindividual particle contributions, at highly varying spatial resolution. We\npresent an approach for approximating SPH scalar fields along viewing rays with\npiece-wise polynomial functions of higher order. This is done by approximating\neach particle contribution individually and then efficiently summing the\nresults, thus generating a higher-order representation of the field with a\nresolution adapting to the data resolution in the volume.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.02896v1"
    },
    {
        "title": "AKN_Regie: bridging digital and performing arts",
        "authors": [
            "Georges Gagneré",
            "Anastasiia Ternova"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  AvatarStaging framework consists in directing avatars on a mixed theatrical\nstage, enabling a co-presence between the materiality of the physical actor and\nthe virtuality of avatars controlled in real time by motion capture or specific\nanimation players. It led to the implementation of the AKN_Regie authoring\ntool, programmed with the Blueprint visual language as a plugin for the Unreal\nEngine (UE) video game engine. The paper describes AKN_Regie main\nfunctionalities as a tool for non-programmer theatrical people. It gives\ninsights of its implementation in the Blueprint visual language specific to UE.\nIt details how the tool evolved along with its use in around ten theater\nproductions. A circulation process between a nonprogramming point of view on\nAKN_Regie called Plugin Perspective and a programming acculturation to its\ndevelopment called Blueprint Perspective is discussed. Finally, a C++\nPerspective is suggested to enhance the cultural appropriation of technological\nissues, bridging the gap between performing arts deeply involved in human\nmateriality and avatars inviting to discover new worlds.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.03761v1"
    },
    {
        "title": "Computational Smocking through Fabric-Thread Interaction",
        "authors": [
            "Ningfeng Zhou",
            "Jing Ren",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We formalize Italian smocking, an intricate embroidery technique that gathers\nflat fabric into pleats along meandering lines of stitches, resulting in pleats\nthat fold and gather where the stitching veers. In contrast to English\nsmocking, characterized by colorful stitches decorating uniformly shaped\npleats, and Canadian smocking, which uses localized knots to form voluminous\npleats, Italian smocking permits the fabric to move freely along the stitched\nthreads following curved paths, resulting in complex and unpredictable pleats\nwith highly diverse, irregular structures, achieved simply by pulling on the\nthreads. We introduce a novel method for digital previewing of Italian smocking\nresults, given the thread stitching path as input. Our method uses a\ncoarse-grained mass-spring system to simulate the interaction between the\nthreads and the fabric. This configuration guides the fine-level fabric\ndeformation through an adaptation of the state-of-the-art simulator, C-IPC. Our\nmethod models the general problem of fabric-thread interaction and can be\nreadily adapted to preview Canadian smocking as well. We compare our results to\nbaseline approaches and physical fabrications to demonstrate the accuracy of\nour method.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05533v2"
    },
    {
        "title": "Scalar Representation of 2D Steady Vector Fields",
        "authors": [
            "Holger Theisel",
            "Michael Motejat",
            "Janos Zimmermann",
            "Christian Rössl"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce a representation of a 2D steady vector field ${{\\mathbf v}}$ by\ntwo scalar fields $a$, $b$, such that the isolines of $a$ correspond to stream\nlines of ${{\\mathbf v}}$, and $b$ increases with constant speed under\nintegration of ${{\\mathbf v}}$. This way, we get a direct encoding of stream\nlines, i.e., a numerical integration of ${{\\mathbf v}}$ can be replaced by a\nlocal isoline extraction of $a$. To guarantee a solution in every case,\ngradient-preserving cuts are introduced such that the scalar fields are allowed\nto be discontinuous in the values but continuous in the gradient. Along with a\npiecewise linear discretization and a proper placement of the cuts, the fields\n$a$ and $b$ can be computed. We show several evaluations on non-trivial vector\nfields.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.06576v1"
    },
    {
        "title": "Lightweight Self-Driven Deformable Organ Animations",
        "authors": [
            "Benjamnin Kenwright",
            "Kanida Sinmai"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The subject of simulating internal organs is a valuable and important topic\nof research to multiple fields from medical analysis to education and training.\nThis paper presents a solution that utilizes a graphical technique in\ncombination with a Stochastic method for tuning an active physics-based model.\nWe generate responsive interactive organ animations with regional properties\n(i.e., areas of the model oscillating with different harmonic frequencies) to\nreproduce and capture real-world characteristics. Our method builds upon\nbiological and physical discoveries to procedurally generate internally\ncontrolled rhythmic motions but also enable the solution to be interactive and\nadaptive. We briefly review deformation models for medical simulations and\ninvestigate the impediments to combining 'computergraphics' representations\nwith biomechanical models. Finally, we present a lightweight solution that is\nscalable and able to procedurally generate large organ animations. In\nparticular, simplified geometric representations of deformable structures that\nuse periodic coupled forces to drive themselves.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.11614v1"
    },
    {
        "title": "A Multi-scale Yarn Appearance Model with Fiber Details",
        "authors": [
            "Apoorv Khattar",
            "Junqui Zhu",
            "Emiliano Padovani",
            "Jean-Marie Aurby",
            "Marc Droske",
            "Ling-Qi Yan",
            "Zahra Montazeri"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Rendering realistic cloth has always been a challenge due to its intricate\nstructure. Cloth is made up of fibers, plies, and yarns, and previous\ncurved-based models, while detailed, were computationally expensive and\ninflexible for large cloth. To address this, we propose a simplified approach.\n  We introduce a geometric aggregation technique that reduces ray-tracing\ncomputation by using fewer curves, focusing only on yarn curves. Our model\ngenerates ply and fiber shapes implicitly, compensating for the lack of\nexplicit geometry with a novel shadowing component. We also present a shading\nmodel that simplifies light interactions among fibers by categorizing them into\nfour components, accurately capturing specular and scattered light in both\nforward and backward directions.\n  To render large cloth efficiently, we propose a multi-scale solution based on\npixel coverage. Our yarn shading model outperforms previous methods, achieving\nrendering speeds 3-5 times faster with less memory in near-field views.\nAdditionally, our multi-scale solution offers a 20% speed boost for distant\ncloth observation.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.12724v1"
    },
    {
        "title": "Hyper-Realist Rendering: A Theoretical Framework",
        "authors": [
            "Ergun Akleman",
            "Murat Kurt",
            "Derya Akleman",
            "Gary Bruins",
            "Sitong Deng",
            "Meena Subramanian"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This is the first paper in a series on hyper-realist rendering. In this\npaper, we introduce the concept of hyper-realist rendering and present a\ntheoretical framework to obtain hyper-realist images. We are using the term\nHyper-realism as an umbrella word that captures all types of visual artifacts\nthat can evoke an impression of reality. The hyper-realist artifacts are visual\nrepresentations that are not necessarily created by following logical and\nphysical principles and can still be perceived as representations of reality.\nThis idea stems from the principles of representational arts, which attain\nvisually acceptable renderings of scenes without implementing strict physical\nlaws of optics and materials. The objective of this work is to demonstrate that\nit is possible to obtain visually acceptable illusions of reality by employing\nsuch artistic approaches. With representational art methods, we can even obtain\nan alternate illusion of reality that looks more real even when it is not real.\nThis paper demonstrates that it is common to create illusions of reality in\nvisual arts with examples of paintings by representational artists. We propose\nan approach to obtain expressive local and global illuminations to obtain these\nstylistic illusions with a set of well-defined and formal methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.12853v1"
    },
    {
        "title": "Evaluation of depth perception in crowded volumes",
        "authors": [
            "Žiga Lesar",
            "Ciril Bohak",
            "Matija Marolt"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Depth perception in volumetric visualization plays a crucial role in the\nunderstanding and interpretation of volumetric data. Numerous visualization\ntechniques, many of which rely on physically based optical effects, promise to\nimprove depth perception but often do so without considering camera movement or\nthe content of the volume. As a result, the findings from previous studies may\nnot be directly applicable to crowded volumes, where a large number of\ncontained structures disrupts spatial perception. Crowded volumes therefore\nrequire special analysis and visualization tools with sparsification\ncapabilities. Interactivity is an integral part of visualizing and exploring\ncrowded spaces, but has received little attention in previous studies. To\naddress this gap, we conducted a study to assess the impact of different\nrendering techniques on depth perception in crowded volumes, with a particular\nfocus on the effects of camera movement. The results show that depth perception\nconsidering camera motion depends much more on the content of the volume than\non the chosen visualization technique. Furthermore, we found that traditional\nrendering techniques, which have often performed poorly in previous studies,\nshowed comparable performance to physically based methods in our study.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.13341v1"
    },
    {
        "title": "Winding Clearness for Differentiable Point Cloud Optimization",
        "authors": [
            "Dong Xiao",
            "Yueji Ma",
            "Zuoqiang Shi",
            "Shiqing Xin",
            "Wenping Wang",
            "Bailin Deng",
            "Bin Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose to explore the properties of raw point clouds through the\n\\emph{winding clearness}, a concept we first introduce for assessing the\nclarity of the interior/exterior relationships represented by the winding\nnumber field of the point cloud. In geometric modeling, the winding number is a\npowerful tool for distinguishing the interior and exterior of a given surface\n$\\partial \\Omega$, and it has been previously used for point normal orientation\nand surface reconstruction. In this work, we introduce a novel approach to\nassess and optimize the quality of point clouds based on the winding clearness.\nWe observe that point clouds with reduced noise tend to exhibit improved\nwinding clearness. Accordingly, we propose an objective function that\nquantifies the error in winding clearness, solely utilizing the positions of\nthe point clouds. Moreover, we demonstrate that the winding clearness error is\ndifferentiable and can serve as a loss function in optimization-based and\nlearning-based point cloud processing. In the optimization-based method, the\nloss function is directly back-propagated to update the point positions,\nresulting in an overall improvement of the point cloud. In the learning-based\nmethod, we incorporate the winding clearness as a geometric constraint in the\ndiffusion-based 3D generative model. Experimental results demonstrate the\neffectiveness of optimizing the winding clearness in enhancing the quality of\nthe point clouds. Our method exhibits superior performance in handling noisy\npoint clouds with thin structures, highlighting the benefits of the global\nperspective enabled by the winding number.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.13639v1"
    },
    {
        "title": "Estimating Cloth Elasticity Parameters From Homogenized Yarn-Level\n  Models",
        "authors": [
            "Joy Xiaoji Zhang",
            "Gene Wei-Chin Lin",
            "Lukas Bode",
            "Hsiao-yu Chen",
            "Tuur Stuyck",
            "Egor Larionov"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Virtual garment simulation has become increasingly important with\napplications in garment design and virtual try-on. However, reproducing\ngarments faithfully remains a cumbersome process. We propose an end-to-end\nmethod for estimating parameters of shell material models corresponding to real\nfabrics with minimal priors. Our method determines yarn model properties from\ninformation directly obtained from real fabrics, unlike methods that require\nexpensive specialized capture systems. We use an extended homogenization method\nto match yarn-level and shell-level hyperelastic energies with respect to a\nrange of surface deformations represented by the first and second fundamental\nforms, including bending along the diagonal to warp and weft directions. We\noptimize the parameters of a shell deformation model involving uncoupled\nbending and membrane energies. This allows the simulated model to exhibit\nnonlinearity and anisotropy seen in real cloth. Finally, we validate our\nresults with quantitative and visual comparisons against real world fabrics\nthrough stretch tests and drape experiments. Our homogenized shell models not\nonly capture the characteristics of underlying yarn patterns, but also exhibit\ndistinct behaviors for different yarn materials.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15169v1"
    },
    {
        "title": "Performance-Based Biped Control using a Consumer Depth Camera",
        "authors": [
            "Yoonsang Lee",
            "Taesoo Kwon"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a technique for controlling physically simulated characters using\nuser inputs from an off-the-shelf depth camera. Our controller takes a\nreal-time stream of user poses as input, and simulates a stream of target poses\nof a biped based on it. The simulated biped mimics the user's actions while\nmoving forward at a modest speed and maintaining balance. The controller is\nparameterized over a set of modulated reference motions that aims to cover the\nrange of possible user actions. For real-time simulation, the best set of\ncontrol parameters for the current input pose is chosen from the parameterized\nsets of pre-computed control parameters via a regression method. By applying\nthe chosen parameters at each moment, the simulated biped can imitate a range\nof user actions while walking in various interactive scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15523v1"
    },
    {
        "title": "WetSpongeCake: a Surface Appearance Model Considering Porosity and\n  Saturation",
        "authors": [
            "Gaole Pan",
            "Yuang Cui",
            "Jian Yang",
            "Beibei Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Wet powdered materials, such as wet ground or moist walls, are common in the\nreal world. Despite their particle size being larger than the wavelength, they\nremain invisible from a macro view. Reproducing these appearances accurately is\ncrucial for various applications. Existing methods use different approaches,\nsuch as Monte Carlo path tracing on implicit shapes, which is accurate but\ncomputationally expensive. Another approach involves modeling powdered\nmaterials with a medium using the radiative transfer equation, but these\nmethods are computationally intensive and lack intuitive parameters. Some works\nrepresent porosity with cylinder-shaped holes on surfaces, but they have\nlimitations. In this paper, we propose a practical BSDF model called\nWetSpongeCake for wet powdered materials. This model includes controllable\nphysical parameters to faithfully reproduce real-world appearances while\nremaining computationally efficient. We reformulate Monte Carlo light transport\non implicit shapes into a medium, utilizing an equivalent phase function for\nlight transport within ellipsoid-shaped particles and a modified RTE for\nporosity and saturation effects. Our novel WetSpongeCake BSDF integrates this\nmedium into the SpongeCake framework, allowing representation of various wet\npowdered material appearances, including both reflection and transmission. We\nshowcase our model with examples, such as wet paper, sand saturated with\ndifferent liquids, and sculptures made of multiple particles.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15628v2"
    },
    {
        "title": "Learning Human-like Locomotion Based on Biological Actuation and Rewards",
        "authors": [
            "Minkwan Kim",
            "Yoonsang Lee"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a method of learning a policy for human-like locomotion via deep\nreinforcement learning based on a human anatomical model, muscle actuation, and\nbiologically inspired rewards, without any inherent control rules or reference\nmotions. Our main ideas involve providing a dense reward using metabolic energy\nconsumption at every step during the initial stages of learning and then\ntransitioning to a sparse reward as learning progresses, and adjusting the\ninitial posture of the human model to facilitate the exploration of locomotion.\nAdditionally, we compared and analyzed differences in learning outcomes across\nvarious settings other than the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15664v1"
    },
    {
        "title": "Refined Inverse Rigging: A Balanced Approach to High-fidelity Blendshape\n  Animation",
        "authors": [
            "Stevo Racković",
            "Cláudia Soares",
            "Dušan Jakovetić"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this paper, we present an advanced approach to solving the inverse rig\nproblem in blendshape animation, using high-quality corrective blendshapes. Our\nalgorithm introduces novel enhancements in three key areas: ensuring high data\nfidelity in reconstructed meshes, achieving greater sparsity in weight\ndistributions, and facilitating smoother frame-to-frame transitions. While the\nincorporation of corrective terms is a known practice, our method\ndifferentiates itself by employing a unique combination of $l_1$ norm\nregularization for sparsity and a temporal smoothness constraint through\nroughness penalty, focusing on the sum of second differences in consecutive\nframe weights. A significant innovation in our approach is the temporal\ndecoupling of blendshapes, which permits simultaneous optimization across\nentire animation sequences. This feature sets our work apart from existing\nmethods and contributes to a more efficient and effective solution. Our\nalgorithm exhibits a marked improvement in maintaining data fidelity and\nensuring smooth frame transitions when compared to prior approaches that either\nlack smoothness regularization or rely solely on linear blendshape models. In\naddition to superior mesh resemblance and smoothness, our method offers\npractical benefits, including reduced computational complexity and execution\ntime, achieved through a novel parallelization strategy using clustering\nmethods. Our results not only advance the state of the art in terms of\nfidelity, sparsity, and smoothness in inverse rigging but also introduce\nsignificant efficiency improvements. The source code will be made available\nupon acceptance of the paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.16496v1"
    },
    {
        "title": "Orientation-aware Incremental Potential Contact",
        "authors": [
            "Zizhou Huang",
            "Max Paik",
            "Zachary Ferguson",
            "Daniele Panozzo",
            "Denis Zorin"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The Incremental Potential Contact (IPC) method enables robust complex\nsimulations of deformable objects with contact and friction. The key to IPC's\nrobustness is its strict adherence to geometric constraints, avoiding\nintersections, which are a common cause of robustness issues in contact\nmechanics. A key element of the IPC approach to contact is a geometric barrier\nfunction, which is defined directly in the discrete setting. While IPC achieves\nits main goal of providing guarantees for contact constraints, its parameters\nneed to be chosen carefully to avoid significant simulation artifacts and\ninaccuracies. We present a systematic derivation of an IPC-like continuum\npotential defined for smooth and piecewise smooth surfaces, starting from\nidentifying a set of natural requirements for contact potentials, including the\nbarrier property, locality, differentiable dependence of shape, and absence of\nforces in rest configurations, based on the idea of candidate sets. Our\npotential is formulated in a way independent of surface discretization.\n  This new potential is suitable for piecewise-linear surfaces and its\nefficiency is similar to standard IPC. We demonstrate its behavior and compare\nit to IPC on a range of challenging contact examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00719v2"
    },
    {
        "title": "TensoSDF: Roughness-aware Tensorial Representation for Robust Geometry\n  and Material Reconstruction",
        "authors": [
            "Jia Li",
            "Lu Wang",
            "Lei Zhang",
            "Beibei Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Reconstructing objects with realistic materials from multi-view images is\nproblematic, since it is highly ill-posed. Although the neural reconstruction\napproaches have exhibited impressive reconstruction ability, they are designed\nfor objects with specific materials (e.g., diffuse or specular materials). To\nthis end, we propose a novel framework for robust geometry and material\nreconstruction, where the geometry is expressed with the implicit signed\ndistance field (SDF) encoded by a tensorial representation, namely TensoSDF. At\nthe core of our method is the roughness-aware incorporation of the radiance and\nreflectance fields, which enables a robust reconstruction of objects with\narbitrary reflective materials. Furthermore, the tensorial representation\nenhances geometry details in the reconstructed surface and reduces the training\ntime. Finally, we estimate the materials using an explicit mesh for efficient\nintersection computation and an implicit SDF for accurate representation.\nConsequently, our method can achieve more robust geometry reconstruction,\noutperform the previous works in terms of relighting quality, and reduce 50%\ntraining times and 70% inference time.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.02771v2"
    },
    {
        "title": "DARTS: Diffusion Approximated Residual Time Sampling for Time-of-flight\n  Rendering in Homogeneous Scattering Media",
        "authors": [
            "Qianyue He",
            "Dongyu Du",
            "Haitian Jiang",
            "Xin Jin"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Time-of-flight (ToF) devices have greatly propelled the advancement of\nvarious multi-modal perception applications. However, achieving accurate\nrendering of time-resolved information remains a challenge, particularly in\nscenes involving complex geometries, diverse materials and participating media.\nExisting ToF rendering works have demonstrated notable results, yet they\nstruggle with scenes involving scattering media and camera-warped settings.\nOther steady-state volumetric rendering methods exhibit significant bias or\nvariance when directly applied to ToF rendering tasks. To address these\nchallenges, we integrate transient diffusion theory into path construction and\npropose novel sampling methods for free-path distance and scattering direction,\nvia resampled importance sampling and offline tabulation. An elliptical\nsampling method is further adapted to provide controllable vertex connection\nsatisfying any required photon traversal time. In contrast to the existing\ntemporal uniform sampling strategy, our method is the first to consider the\ncontribution of transient radiance to importance-sample the full path, and thus\nenables improved temporal path construction under multiple scattering settings.\nThe proposed method can be integrated into both path tracing and photon-based\nframeworks, delivering significant improvements in quality and efficiency with\nat least a 5x MSE reduction versus SOTA methods in equal rendering time.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.03106v2"
    },
    {
        "title": "M2fNet: Multi-modal Forest Monitoring Network on Large-scale Virtual\n  Dataset",
        "authors": [
            "Yawen Lu",
            "Yunhan Huang",
            "Su Sun",
            "Tansi Zhang",
            "Xuewen Zhang",
            "Songlin Fei",
            "Yingjie Chen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Forest monitoring and education are key to forest protection, education and\nmanagement, which is an effective way to measure the progress of a country's\nforest and climate commitments. Due to the lack of a large-scale wild forest\nmonitoring benchmark, the common practice is to train the model on a common\noutdoor benchmark (e.g., KITTI) and evaluate it on real forest datasets (e.g.,\nCanaTree100). However, there is a large domain gap in this setting, which makes\nthe evaluation and deployment difficult. In this paper, we propose a new\nphotorealistic virtual forest dataset and a multimodal transformer-based\nalgorithm for tree detection and instance segmentation. To the best of our\nknowledge, it is the first time that a multimodal detection and segmentation\nalgorithm is applied to large-scale forest scenes. We believe that the proposed\ndataset and method will inspire the simulation, computer vision, education, and\nforestry communities towards a more comprehensive multi-modal understanding.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.04534v2"
    },
    {
        "title": "Sophia-in-Audition: Virtual Production with a Robot Performer",
        "authors": [
            "Taotao Zhou",
            "Teng Xu",
            "Dong Zhang",
            "Yuyang Jiao",
            "Peijun Xu",
            "Yaoyu He",
            "Lan Xu",
            "Jingyi Yu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present Sophia-in-Audition (SiA), a new frontier in virtual production, by\nemploying the humanoid robot Sophia within an UltraStage environment composed\nof a controllable lighting dome coupled with multiple cameras. We demonstrate\nSophia's capability to replicate iconic film segments, follow real performers,\nand perform a variety of motions and expressions, showcasing her versatility as\na virtual actor. Key to this process is the integration of facial motion\ntransfer algorithms and the UltraStage's controllable lighting and multi-camera\nsetup, enabling dynamic performances that align with the director's vision. Our\ncomprehensive user studies indicate positive audience reception towards\nSophia's performances, highlighting her potential to reduce the uncanny valley\neffect in virtual acting. Additionally, the immersive lighting in dynamic clips\nwas highly rated for its naturalness and its ability to mirror professional\nfilm standards. The paper presents a first-of-its-kind multi-view robot\nperformance video dataset with dynamic lighting, offering valuable insights for\nfuture enhancements in humanoid robotic performers and virtual production\ntechniques. This research contributes significantly to the field by presenting\na unique virtual production setup, developing tools for sophisticated\nperformance control, and providing a comprehensive dataset and user study\nanalysis for diverse applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.06978v1"
    },
    {
        "title": "Regional Adaptive Metropolis Light Transport",
        "authors": [
            "Hisanari Otsu",
            "Killian Herveau",
            "Johannes Hanika",
            "Derek Nowrouzezahrai",
            "Carsten Dachsbacher"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The design of the proposal distributions, and most notably the kernel\nparameters, are crucial for the performance of Markov chain Monte Carlo (MCMC)\nrendering. A poor selection of parameters can increase the correlation of the\nMarkov chain and result in bad rendering performance. We approach this problem\nby a novel path perturbation strategy for online-learning of state-dependent\nkernel parameters. We base our approach on the theoretical framework of\nregional adaptive MCMC which enables the adaptation of parameters depending on\nthe region of the state space which contains the current sample, and on\ninformation collected from previous samples. For this, we define a partitioning\nof the path space on a low-dimensional canonical space to capture the\ncharacteristics of paths, with a focus on path segments closer to the sensor.\nFast convergence is achieved by adaptive refinement of the partitions.\nExemplarily, we present two novel regional adaptive path perturbation\ntechniques akin to lens and multi-chain perturbations. Our approach can easily\nbe used on top of existing path space MLT methods to improve rendering\nefficiency, while being agnostic to the initial choice of kernel parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.08273v1"
    },
    {
        "title": "Watertightization of Trimmed Surfaces at Intersection Boundary",
        "authors": [
            "Hua Li",
            "Lu Zhang",
            "Ruoxi Guo",
            "Zushang Xiao",
            "Rui Guo"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper introduces a watertight technique to deal with the boundary\nrepresentation of surface-surface intersection in CAD.\n  Surfaces play an important role in today's geometric design. The mathematical\nmodel of non-uniform rational B-spline surfaces (NURBS) is the mainstream and\nISO standard. In the situation of surface-surface intersection, things are a\nlittle complicated, for some parts of surfaces may be cut-off, so called\ntrimmed surfaces occur, which is the central topic in the past decades in CAD\ncommunity of both academia and industry. The main problem is that the\nparametric domain of the trimmed surface generally is not the standard square\nor rectangle, and rather, typically, bounded by curves, based on point inverse\nof the intersection points and interpolated. The existence of gaps or overlaps\nat the intersection boundary makes hard the preprocessing of CAE and other\ndownstream applications. The NURBS are in this case hard to keep a closed form.\nIn common, a special data structure of intersection curves must be affiliated\nto support downstream applications, while the data structure of the whole CAD\nsystem is not unified, and the calculation is not efficient.\n  In terms of Bezier surface, a special case of NURBS, this paper designs a\nreparameterization or normalization to transform the trimmed surface into a\ngroup of Bezier surface patches in standard parametric domain [0,1]X[0,1]. And\nthen the boundary curve of normalized Bezier surface patch can be replaced by\nthe intersection curve to realize watertight along the boundary. In this way,\nthe trimmed surface is wiped out, the \"gap\" between CAD and CAE is closed.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.10216v1"
    },
    {
        "title": "Periodic Implicit Representation, Design and Optimization of Porous\n  Structures Using Periodic B-splines",
        "authors": [
            "Gao Depeng",
            "Gao Yang",
            "Lin Hongwei"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Porous structures are intricate solid materials with numerous small pores,\nextensively used in fields like medicine, chemical engineering, and aerospace.\nHowever, the design of such structures using computer-aided tools is a\ntime-consuming and tedious process.In this study, we propose a novel\nrepresentation method and design approach for porous units that can be\ninfinitely spliced to form a porous structure. We use periodic B-spline\nfunctions to represent periodic or symmetric porous units. Starting from a\nvoxel representation of a porous sample, the discrete distance field is\ncomputed. To fit the discrete distance field with a periodic B-spline, we\nintroduce the constrained least squares progressive-iterative approximation\nalgorithm, which results in an implicit porous unit. This unit can be subject\nto optimization to enhance connectivity and utilized for topology optimization,\nthereby improving the model's stiffness while maintaining periodicity or\nsymmetry. The experimental results demonstrate the potential of the designed\ncomplex porous units in enhancing the mechanical performance of the model.\nConsequently, this study has the potential to incorporate remarkable structures\nderived from artificial design or nature into the design of high-performing\nmodels, showing the promise for biomimetic applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.12076v1"
    },
    {
        "title": "Persistent Homology-Driven Optimization of Effective Relative Density\n  Range for Triply Periodic Minimal Surface",
        "authors": [
            "Gao Depeng",
            "Zhang Yuanzhi",
            "Lin Hongwei"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Triply periodic minimal surfaces (TPMSs) play a vital role in the design of\nporous structures, with applications in bone tissue engineering, chemical\nengineering, and the creation of lightweight models. However, fabrication of\nTPMSs via additive manufacturing is feasible only within a specific range of\nrelative densities, termed the effective relative density range (EDR), outside\nof which TPMSs exhibit unmanufacturable features. In this study, the persistent\nhomology is applied to theoretically calculate and extend the EDRs of TPMSs.\nThe TPMSs with extended EDRs are referred to as extended TPMSs. To achieve\nthis, TPMSs are converted into implicit B-spline representation through\nfitting. By analyzing the symmetry of TPMSs, a partial fitting method is\nutilized to preserve the symmetry and enhance fitting precision. A topological\nobjective function is modeled based on the understanding of topological\nfeatures, resulting in extended TPMSs that possess extended EDRs while\nmaintaining a high degree of similarity to the original TPMSs. Experimental\nvalidation confirms the effectiveness of the approach in extending the EDRs of\nTPMSs. Furthermore, the extended TPMSs demonstrate superior performance in\nporous model design and topology optimization compared to their original\ncounterparts. The extended TPMSs with increased EDRs hold promise for replacing\ntraditional TPMSs in applications that require porous structures with varying\ndensities.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.12109v1"
    },
    {
        "title": "Real-time High-resolution View Synthesis of Complex Scenes with Explicit\n  3D Visibility Reasoning",
        "authors": [
            "Tiansong Zhou",
            "Yebin Liu",
            "Xuangeng Chu",
            "Chengkun Cao",
            "Changyin Zhou",
            "Fei Yu",
            "Yu Li"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Rendering photo-realistic novel-view images of complex scenes has been a\nlong-standing challenge in computer graphics. In recent years, great research\nprogress has been made on enhancing rendering quality and accelerating\nrendering speed in the realm of view synthesis. However, when rendering complex\ndynamic scenes with sparse views, the rendering quality remains limited due to\nocclusion problems. Besides, for rendering high-resolution images on dynamic\nscenes, the rendering speed is still far from real-time. In this work, we\npropose a generalizable view synthesis method that can render high-resolution\nnovel-view images of complex static and dynamic scenes in real-time from sparse\nviews. To address the occlusion problems arising from the sparsity of input\nviews and the complexity of captured scenes, we introduce an explicit 3D\nvisibility reasoning approach that can efficiently estimate the visibility of\nsampled 3D points to the input views. The proposed visibility reasoning\napproach is fully differentiable and can gracefully fit inside the volume\nrendering pipeline, allowing us to train our networks with only multi-view\nimages as supervision while refining geometry and texture simultaneously.\nBesides, each module in our pipeline is carefully designed to bypass the\ntime-consuming MLP querying process and enhance the rendering quality of\nhigh-resolution images, enabling us to render high-resolution novel-view images\nin real-time.Experimental results show that our method outperforms previous\nview synthesis methods in both rendering quality and speed, particularly when\ndealing with complex dynamic scenes with sparse views.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.12886v1"
    },
    {
        "title": "Mochi: Fast \\& Exact Collision Detection",
        "authors": [
            "Durga Keerthi Mandarapu",
            "Nicholas James",
            "Milind Kulkarni"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Collision Detection (CD) has several applications across the domains such as\nrobotics, visual graphics, and fluid mechanics.\n  Finding exact collisions between the objects in the scene is quite\ncomputationally intensive.\n  To quickly filter the object pairs that do not result in a collision,\nbounding boxes are built on the objects, indexed using a Bounding Volume\nHierarchy(BVH), and tested for intersection before performing the expensive\nobject-object intersection tests.\n  In state-of-the-art CD libraries, accelerators such as GPUs are used to\naccelerate BVH traversal by building specialized data structures.\n  The recent addition of ray tracing architecture to GPU hardware is designed\nto do the same but in the context of implementing a Ray Tracing algorithm to\nrender a graphical scene in real-time.\n  We present Mochi, a fast and exact collision detection engine that\naccelerates both the broad and narrow phases by taking advantage of the\ncapabilities of Ray Tracing cores.\n  We introduce multiple new reductions to perform generic CD to support three\ntypes of objects for CD: simple spherical particles, objects describable by\nmathematical equations, and complex objects composed of a triangle mesh.\n  By implementing our reductions, Mochi achieves several orders of magnitude\nspeedups on synthetic datasets and 5x-28x speedups on real-world triangle mesh\ndatasets.\n  We further evaluate our reductions thoroughly and provide several\narchitectural insights on the ray tracing cores that are otherwise unknown due\nto their proprietorship.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.14801v1"
    },
    {
        "title": "CharacterMixer: Rig-Aware Interpolation of 3D Characters",
        "authors": [
            "Xiao Zhan",
            "Rao Fu",
            "Daniel Ritchie"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present CharacterMixer, a system for blending two rigged 3D characters\nwith different mesh and skeleton topologies while maintaining a rig throughout\ninterpolation. CharacterMixer also enables interpolation during motion for such\ncharacters, a novel feature. Interpolation is an important shape editing\noperation, but prior methods have limitations when applied to rigged\ncharacters: they either ignore the rig (making interpolated characters no\nlonger posable) or use a fixed rig and mesh topology. To handle different mesh\ntopologies, CharacterMixer uses a signed distance field (SDF) representation of\ncharacter shapes, with one SDF per bone. To handle different skeleton\ntopologies, it computes a hierarchical correspondence between source and target\ncharacter skeletons and interpolates the SDFs of corresponding bones. This\ncorrespondence also allows the creation of a single \"unified skeleton\" for\nposing and animating interpolated characters. We show that CharacterMixer\nproduces qualitatively better interpolation results than two state-of-the-art\nmethods while preserving a rig throughout interpolation.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.15580v1"
    },
    {
        "title": "Cinematographic Camera Diffusion Model",
        "authors": [
            "Hongda Jiang",
            "Xi Wang",
            "Marc Christie",
            "Libin Liu",
            "Baoquan Chen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Designing effective camera trajectories in virtual 3D environments is a\nchallenging task even for experienced animators. Despite an elaborate film\ngrammar, forged through years of experience, that enables the specification of\ncamera motions through cinematographic properties (framing, shots sizes,\nangles, motions), there are endless possibilities in deciding how to place and\nmove cameras with characters. Dealing with these possibilities is part of the\ncomplexity of the problem. While numerous techniques have been proposed in the\nliterature (optimization-based solving, encoding of empirical rules, learning\nfrom real examples,...), the results either lack variety or ease of control.\n  In this paper, we propose a cinematographic camera diffusion model using a\ntransformer-based architecture to handle temporality and exploit the\nstochasticity of diffusion models to generate diverse and qualitative\ntrajectories conditioned by high-level textual descriptions. We extend the work\nby integrating keyframing constraints and the ability to blend naturally\nbetween motions using latent interpolation, in a way to augment the degree of\ncontrol of the designers. We demonstrate the strengths of this text-to-camera\nmotion approach through qualitative and quantitative experiments and gather\nfeedback from professional artists. The code and data are available at\n\\URL{https://github.com/jianghd1996/Camera-control}.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.16143v1"
    },
    {
        "title": "2+2D Texture for Full Positive Parallax Effect",
        "authors": [
            "Alexandre Yip Gonçalves Dias",
            "Marcelo Knörich Zuffo"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The representation of parallax on virtual environment is still a problem to\nbe studied. Common algorithms, such as Bump Mapping, Parallax Mapping and\nDisplacement Mapping, treats this problem for small disparity between a real\nobject and a simplified model. This work will introduce a new texture structure\nand one possible render algorithm able to display parallax for large\ndisparities, it is an approach based on the four-dimensional representation of\nthe Light Field and was thought to positive parallax and to display the\nsurfaces on the inside of our simplified model. These conditions are imposed to\nallow the free movement of an observer, if its movement is restrict, these\nconditions may be loosen. It is a high storage low process approach possible to\nbe used in real time systems. As an example we will develop a scene with\nseveral objects and simplified them by a unique sphere that encloses them all,\nour system was able to run this scene with about 180fps.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.16815v1"
    },
    {
        "title": "Non-Euclidean Sliced Optimal Transport Sampling",
        "authors": [
            "Baptiste Genest",
            "Nicolas Courty",
            "David Coeurjolly"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In machine learning and computer graphics, a fundamental task is the\napproximation of a probability density function through a well-dispersed\ncollection of samples. Providing a formal metric for measuring the distance\nbetween probability measures on general spaces, Optimal Transport (OT) emerges\nas a pivotal theoretical framework within this context. However, the associated\ncomputational burden is prohibitive in most real-world scenarios. Leveraging\nthe simple structure of OT in 1D, Sliced Optimal Transport (SOT) has appeared\nas an efficient alternative to generate samples in Euclidean spaces. This paper\npushes the boundaries of SOT utilization in computational geometry problems by\nextending its application to sample densities residing on more diverse\nmathematical domains, including the spherical space Sd , the hyperbolic plane\nHd , and the real projective plane Pd . Moreover, it ensures the quality of\nthese samples by achieving a blue noise characteristic, regardless of the\ndimensionality involved. The robustness of our approach is highlighted through\nits application to various geometry processing tasks, such as the intrinsic\nblue noise sampling of meshes, as well as the sampling of directions and\nrotations. These applications collectively underscore the efficacy of our\nmethodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.16981v1"
    },
    {
        "title": "3D Gaussian Model for Animation and Texturing",
        "authors": [
            "Xiangzhi Eric Wang",
            "Zackary P. T. Sin"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  3D Gaussian Splatting has made a marked impact on neural rendering by\nachieving impressive fidelity and performance. Despite this achievement,\nhowever, it is not readily applicable to developing interactive applications.\nReal-time applications like XR apps and games require functions such as\nanimation, UV-mapping, and model editing simultaneously manipulated through the\nusage of a 3D model. We propose a modeling that is analogous to typical 3D\nmodels, which we call 3D Gaussian Model (3DGM); it provides a manipulatable\nproxy for novel animation and texture transfer. By binding the 3D Gaussians in\ntexture space and re-projecting them back to world space through implicit shell\nmapping, we show how our 3D modeling can serve as a valid rendering methodology\nfor interactive applications. It is further noted that recently, 3D mesh\nreconstruction works have been able to produce high-quality mesh for rendering.\nOur work, on the other hand, only requires an approximated geometry for\nrendering an object in high fidelity. Applicationwise, we will show that our\nproxy-based 3DGM is capable of driving novel animation without animated\ntraining data and texture transferring via UV mapping of the 3D Gaussians. We\nbelieve the result indicates the potential of our work for enabling interactive\napplications for 3D Gaussian Splatting.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.19441v1"
    },
    {
        "title": "Hybrid Base Complex: Extract and Visualize Structure of Hex-dominant\n  Meshes",
        "authors": [
            "Lei Si",
            "Haowei Cao",
            "Guoning Chen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Hex-dominant mesh generation has received significant attention in recent\nresearch due to its superior robustness compared to pure hex-mesh generation\ntechniques. In this work, we introduce the first structure for analyzing\nhex-dominant meshes. This structure builds on the base complex of pure\nhex-meshes but incorporates the non-hex elements for a more comprehensive and\ncomplete representation. We provide its definition and describe its\nconstruction steps. Based on this structure, we present an extraction and\ncategorization of sheets using advanced graph matching techniques to handle the\nnon-hex elements. This enables us to develop an enhanced visual analysis of the\nstructure for any hex-dominant meshes.We apply this structure-based visual\nanalysis to compare hex-dominant meshes generated by different methods to study\ntheir advantages and disadvantages. This complements the standard quality\nmetric based on the non-hex element percentage for hex-dominant meshes.\nMoreover, we propose a strategy to extract a cleaned (optimized) valence-based\nsingularity graph wireframe to analyze the structure for both mesh and sheets.\nOur results demonstrate that the proposed hybrid base complex provides a coarse\nrepresentation for mesh element, and the proposed valence singularity graph\nwireframe provides a better internal visualization of hex-dominant meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00300v1"
    },
    {
        "title": "VR Research at Fraunhofer IGD, Darmstadt, Germany",
        "authors": [
            "Wolfgang Felger",
            "Martin Göbel",
            "Dirk Reiners",
            "Gabriel Zachmann"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a historical outline of the research and developments of Virtual\nReality at the Fraunhofer Institute for Computer Graphics (IGD) in Darmstadt,\nGermany, from 1990 through 2000.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.01629v2"
    },
    {
        "title": "MagicClay: Sculpting Meshes With Generative Neural Fields",
        "authors": [
            "Amir Barda",
            "Vladimir G. Kim",
            "Noam Aigerman",
            "Amit H. Bermano",
            "Thibault Groueix"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The recent developments in neural fields have brought phenomenal capabilities\nto the field of shape generation, but they lack crucial properties, such as\nincremental control - a fundamental requirement for artistic work. Triangular\nmeshes, on the other hand, are the representation of choice for most geometry\nrelated tasks, offering efficiency and intuitive control, but do not lend\nthemselves to neural optimization. To support downstream tasks, previous art\ntypically proposes a two-step approach, where first a shape is generated using\nneural fields, and then a mesh is extracted for further processing. Instead, in\nthis paper we introduce a hybrid approach that maintains both a mesh and a\nSigned Distance Field (SDF) representations consistently. Using this\nrepresentation, we introduce MagicClay - an artist friendly tool for sculpting\nregions of a mesh according to textual prompts while keeping other regions\nuntouched. Our framework carefully and efficiently balances consistency between\nthe representations and regularizations in every step of the shape\noptimization; Relying on the mesh representation, we show how to render the SDF\nat higher resolutions and faster. In addition, we employ recent work in\ndifferentiable mesh reconstruction to adaptively allocate triangles in the mesh\nwhere required, as indicated by the SDF. Using an implemented prototype, we\ndemonstrate superior generated geometry compared to the state-of-the-art, and\nnovel consistent control, allowing sequential prompt-based edits to the same\nmesh for the first time.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.02460v4"
    },
    {
        "title": "Implicit-Explicit simulation of Mass-Spring-Charge Systems",
        "authors": [
            "Zhiyuan Zhang",
            "Zhaocheng Liu",
            "Stefanos Papanicolopulos",
            "Kartic Subr"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Point masses connected by springs, or mass-spring systems, are widely used in\ncomputer animation to approximate the behavior of deformable objects. One of\nthe restrictions imposed by these models is that points that are not\ntopologically constrained (linked by a spring) are unable to interact with each\nother explicitly. Such interactions would introduce a new dimension for\nartistic control and animation within the computer graphics community. Beyond\ngraphics, such a model could be an effective proxy to use for model-based\nlearning of complex physical systems such as molecular biology. We propose to\nimbue masses in a mass-spring system with electrostatic charge leading a system\nwith internal forces between all pairs of charged points -- regardless of\nwhether they are linked by a spring. We provide a practical and stable\nalgorithm to simulate charged mass-spring systems over long time horizons. We\ndemonstrate how these systems may be controlled via parameters such as guidance\nelectric fields or external charges, thus presenting fresh opportunities for\nartistic authoring. Our method is especially appropriate for computer graphics\napplications due to its robustness at larger simulation time steps.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.03005v1"
    },
    {
        "title": "Online Photon Guiding with 3D Gaussians for Caustics Rendering",
        "authors": [
            "Jiawei Huang",
            "Hajime Tanaka",
            "Taku Komura",
            "Yoshifumi Kitamura"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In production rendering systems, caustics are typically rendered via photon\nmapping and gathering, a process often hindered by insufficient photon density.\nIn this paper, we propose a novel photon guiding method to improve the photon\ndensity and overall quality for caustic rendering. The key insight of our\napproach is the application of a global 3D Gaussian mixture model, used in\nconjunction with an adaptive light sampler. This combination effectively guides\nphoton emission in expansive 3D scenes with multiple light sources. By\nemploying a global 3D Gaussian mixture, our method precisely models the\ndistribution of the points of interest. To sample emission directions from the\ndistribution at any observation point, we introduce a novel directional\ntransform of the 3D Gaussian, which ensures accurate photon emission guiding.\nFurthermore, our method integrates a global light cluster tree, which models\nthe contribution distribution of light sources to the image, facilitating\neffective light source selection. We conduct experiments demonstrating that our\napproach robustly outperforms existing photon guiding techniques across a\nvariety of scenarios, significantly advancing the quality of caustic rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.03641v2"
    },
    {
        "title": "Cyclic Polygon Plots",
        "authors": [
            "Maksim Schreck",
            "Peter Albers",
            "Filip Sadlo"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this paper, we introduce the cyclic polygon plot, a representation based\non a novel projection concept for multi-dimensional values. Cyclic polygon\nplots combine the typically competing requirements of quantitativeness,\nimage-space efficiency, and readability. Our approach is complemented with a\nplacement strategy based on its intrinsic features, resulting in a\ndimensionality reduction strategy that is consistent with our overall concept.\nAs a result, our approach combines advantages from dimensionality reduction\ntechniques and quantitative plots, supporting a wide range of tasks in\nmulti-dimensional data analysis. We examine and discuss the overall properties\nof our approach, and demonstrate its utility with a user study and selected\nexamples.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.05296v1"
    },
    {
        "title": "Vertex Block Descent",
        "authors": [
            "Anka He Chen",
            "Ziheng Liu",
            "Yin Yang",
            "Cem Yuksel"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce vertex block descent, a block coordinate descent solution for\nthe variational form of implicit Euler through vertex-level Gauss-Seidel\niterations. It operates with local vertex position updates that achieve\nreductions in global variational energy with maximized parallelism. This forms\na physics solver that can achieve numerical convergence with unconditional\nstability and exceptional computation performance. It can also fit in a given\ncomputation budget by simply limiting the iteration count while maintaining its\nstability and superior convergence rate.\n  We present and evaluate our method in the context of elastic body dynamics,\nproviding details of all essential components and showing that it outperforms\nalternative techniques. In addition, we discuss and show examples of how our\nmethod can be used for other simulation systems, including particle-based\nsimulations and rigid bodies.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.06321v4"
    },
    {
        "title": "Inverse Garment and Pattern Modeling with a Differentiable Simulator",
        "authors": [
            "Boyang Yu",
            "Frederic Cordier",
            "Hyewon Seo"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The capability to generate simulation-ready garment models from 3D shapes of\nclothed humans will significantly enhance the interpretability of captured\ngeometry of real garments, as well as their faithful reproduction in the\nvirtual world. This will have notable impact on fields like shape capture in\nsocial VR, and virtual try-on in the fashion industry. To align with the\ngarment modeling process standardized by the fashion industry as well as cloth\nsimulation softwares, it is required to recover 2D patterns. This involves an\ninverse garment design problem, which is the focus of our work here: Starting\nwith an arbitrary target garment geometry, our system estimates an animatable\ngarment model by automatically adjusting its corresponding 2D template pattern,\nalong with the material parameters of the physics-based simulation (PBS). Built\nupon a differentiable cloth simulator, the optimization process is directed\ntowards minimizing the deviation of the simulated garment shape from the target\ngeometry. Moreover, our produced patterns meet manufacturing requirements such\nas left-to-right-symmetry, making them suited for reverse garment fabrication.\nWe validate our approach on examples of different garment types, and show that\nour method faithfully reproduces both the draped garment shape and the sewing\npattern.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.06841v3"
    },
    {
        "title": "Surface-aware Mesh Texture Synthesis with Pre-trained 2D CNNs",
        "authors": [
            "Áron Samuel Kovács",
            "Pedro Hermosilla",
            "Renata G. Raidou"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Mesh texture synthesis is a key component in the automatic generation of 3D\ncontent. Existing learning-based methods have drawbacks -- either by\ndisregarding the shape manifold during texture generation or by requiring a\nlarge number of different views to mitigate occlusion-related inconsistencies.\nIn this paper, we present a novel surface-aware approach for mesh texture\nsynthesis that overcomes these drawbacks by leveraging the pre-trained weights\nof 2D Convolutional Neural Networks (CNNs) with the same architecture, but with\nconvolutions designed for 3D meshes. Our proposed network keeps track of the\noriented patches surrounding each texel, enabling seamless texture synthesis\nand retaining local similarity to classical 2D convolutions with square\nkernels. Our approach allows us to synthesize textures that account for the\ngeometric content of mesh surfaces, eliminating discontinuities and achieving\ncomparable quality to 2D image synthesis algorithms. We compare our approach\nwith state-of-the-art methods where, through qualitative and quantitative\nevaluations, we demonstrate that our approach is more effective for a variety\nof meshes and styles, while also producing visually appealing and consistent\ntextures on meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.06855v1"
    },
    {
        "title": "A New Split Algorithm for 3D Gaussian Splatting",
        "authors": [
            "Qiyuan Feng",
            "Gengchen Cao",
            "Haoxiang Chen",
            "Tai-Jiang Mu",
            "Ralph R. Martin",
            "Shi-Min Hu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  3D Gaussian splatting models, as a novel explicit 3D representation, have\nbeen applied in many domains recently, such as explicit geometric editing and\ngeometry generation. Progress has been rapid. However, due to their mixed\nscales and cluttered shapes, 3D Gaussian splatting models can produce a blurred\nor needle-like effect near the surface. At the same time, 3D Gaussian splatting\nmodels tend to flatten large untextured regions, yielding a very sparse point\ncloud. These problems are caused by the non-uniform nature of 3D Gaussian\nsplatting models, so in this paper, we propose a new 3D Gaussian splitting\nalgorithm, which can produce a more uniform and surface-bounded 3D Gaussian\nsplatting model. Our algorithm splits an $N$-dimensional Gaussian into two\nN-dimensional Gaussians. It ensures consistency of mathematical characteristics\nand similarity of appearance, allowing resulting 3D Gaussian splatting models\nto be more uniform and a better fit to the underlying surface, and thus more\nsuitable for explicit editing, point cloud extraction and other tasks.\nMeanwhile, our 3D Gaussian splitting approach has a very simple closed-form\nsolution, making it readily applicable to any 3D Gaussian model.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.09143v1"
    },
    {
        "title": "Building An Efficient Grid On GPU",
        "authors": [
            "Vasco Costa",
            "João M. Pereira",
            "Joaquim Jorge"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Grid space partitioning is a technique to speed up queries to graphics\ndatabases. We present a parallel grid construction algorithm which can\nefficiently construct a structured grid on GPU hardware. Our approach is\nsubstantially faster than existing uniform grid construction algorithms,\nespecially on non-homogeneous scenes. Indeed, it can populate a grid in\nreal-time (at rates over 25 Hz), for architectural scenes with 10 million\ntriangles.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.10647v1"
    },
    {
        "title": "The Simplex Projection: Lossless Visualization of 4D Compositional Data\n  on a 2D Canvas",
        "authors": [
            "Marvin Schmitt",
            "Yuga Hikida",
            "Stefan T Radev",
            "Filip Sadlo",
            "Paul-Christian Bürkner"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The simplex projection expands the capabilities of simplex plots (also known\nas ternary plots) to achieve a lossless visualization of 4D compositional data\non a 2D canvas. Previously, this was only possible for 3D compositional data.\nWe demonstrate how our approach can be applied to individual data points, point\nclouds, and continuous probability density functions on simplices. While we\nshowcase our visualization technique specifically for 4D compositional data, we\noffer rigorous proofs that support its extension to compositional data of any\n(finite) dimensionality.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.11141v1"
    },
    {
        "title": "LFS-Aware Surface Reconstruction from Unoriented 3D Point Clouds",
        "authors": [
            "Rao Fu",
            "Kai Hormann",
            "Pierre Alliez"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a novel approach for generating isotropic surface triangle meshes\ndirectly from unoriented 3D point clouds, with the mesh density adapting to the\nestimated local feature size (LFS). Popular reconstruction pipelines first\nreconstruct a dense mesh from the input point cloud and then apply remeshing to\nobtain an isotropic mesh. The sequential pipeline makes it hard to find a\nlower-density mesh while preserving more details. Instead, our approach\nreconstructs both an implicit function and an LFS-aware mesh sizing function\ndirectly from the input point cloud, which is then used to produce the final\nLFS-aware mesh without remeshing. We combine local curvature radius and shape\ndiameter to estimate the LFS directly from the input point clouds.\nAdditionally, we propose a new mesh solver to solve an implicit function whose\nzero level set delineates the surface without requiring normal orientation. The\nadded value of our approach is generating isotropic meshes directly from 3D\npoint clouds with an LFS-aware density, thus achieving a trade-off between\ngeometric detail and mesh complexity. Our experiments also demonstrate the\nrobustness of our method to noise, outliers, and missing data and can preserve\nsharp features for CAD point clouds.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.13924v3"
    },
    {
        "title": "Utilizing Motion Matching with Deep Reinforcement Learning for Target\n  Location Tasks",
        "authors": [
            "Jeongmin Lee",
            "Taesoo Kwon",
            "Hyunju Shin",
            "Yoonsang Lee"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present an approach using deep reinforcement learning (DRL) to directly\ngenerate motion matching queries for long-term tasks, particularly targeting\nthe reaching of specific locations. By integrating motion matching and DRL, our\nmethod demonstrates the rapid learning of policies for target location tasks\nwithin minutes on a standard desktop, employing a simple reward design.\nAdditionally, we propose a unique hit reward and obstacle curriculum scheme to\nenhance policy learning in environments with moving obstacles.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.15902v1"
    },
    {
        "title": "MATTopo: Topology-preserving Medial Axis Transform with Restricted Power\n  Diagram",
        "authors": [
            "Ningna Wang",
            "Hui Huang",
            "Shibo Song",
            "Bin Wang",
            "Wenping Wang",
            "Xiaohu Guo"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a novel topology-preserving 3D medial axis computation framework\nbased on volumetric restricted power diagram (RPD), while preserving the medial\nfeatures and geometric convergence simultaneously, for both 3D CAD and organic\nshapes. The volumetric RPD discretizes the input 3D volume into sub-regions\ngiven a set of medial spheres. With this intermediate structure, we convert the\nhomotopy equivalency between the generated medial mesh and the input 3D shape\ninto a localized contractibility checking for each restricted element (power\ncell, power face, power edge), by checking their connected components and Euler\ncharacteristics. We further propose a fractional Euler characteristic algorithm\nfor efficient GPU-based computation of Euler characteristic for each restricted\nelement on the fly while computing the volumetric RPD. Compared with existing\nvoxel-based or point-cloud-based methods, our approach is the first to\nadaptively and directly revise the medial mesh without globally modifying the\ndependent structure, such as voxel size or sampling density, while preserving\nits topology and medial features. In comparison with the feature preservation\nmethod MATFP, our method provides geometrically comparable results with fewer\nspheres and more robustly captures the topology of the input 3D shape.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.18761v4"
    },
    {
        "title": "Efficient GPU Cloth Simulation with Non-distance Barriers and Subspace\n  Reuse",
        "authors": [
            "Lei Lan",
            "Zixuan Lu",
            "Jingyi Long",
            "Chun Yuan",
            "Xuan Li",
            "Xiaowei He",
            "Huamin Wang",
            "Chenfanfu Jiang",
            "Yin Yang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper pushes the performance of cloth simulation, making the simulation\ninteractive even for high-resolution garment models while keeping every\ntriangle untangled. The penetration-free guarantee is inspired by the interior\npoint method, which converts the inequality constraints to barrier potentials.\nWe propose a major overhaul of this modality within the projective dynamics\nframework by leveraging an adaptive weighting mechanism inspired by barrier\nformulation. This approach does not depend on the distance between mesh\nprimitives, but on the virtual life span of a collision event and thus keeps\nall the vertices within feasible region. Such a non-distance barrier model\nallows a new way to integrate collision resolution into the simulation\npipeline. Another contributor to the performance boost comes from the subspace\nreuse strategy. This is based on the observation that low-frequency strain\npropagation is near orthogonal to the deformation induced by collisions or\nself-collisions, often of high frequency. Subspace reuse then takes care of\nlow-frequency residuals, while high-frequency residuals can also be effectively\nsmoothed by GPU-based iterative solvers. We show that our method outperforms\nexisting fast cloth simulators by at least one order while producing\nhigh-quality animations of high-resolution models.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.19272v4"
    },
    {
        "title": "CCWSIM: An Efficient and Fast Wavelet-Based CCSIM for Categorical\n  Characterization of Large-Scale",
        "authors": [
            "Mojtaba Bavandsavadkoohi",
            "Erwan Gloaguen",
            "Behzad Tokhmechi",
            "Alireza Arab-Amiri",
            "Bernard Giroux"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Over the last couple of decades, there has been a surge in various approaches\nto multiple-point statistics simulation, commonly referred to as MPS. These\nmethods have aimed to improve several critical aspects of realism in the\nresults, including spatial continuity, conditioning, stochasticity, and\ncomputational efficiency. Nevertheless, achieving a simultaneous enhancement of\nthese crucial factors has presented challenges to researchers. In the approach\nthat we propose, CCSIM is combined with the Discrete Wavelet Transform (DWT) to\naddress some of these concerns. The primary step in the method involves the\ncomputation of the DWT for both the Training Image (TI) and a region shared\nwith previously simulated grids at a specific level of wavelet decomposition.\nThen, the degree of similarity between the wavelet approximation coefficients\nis measured using a Cross-Correlation Function (CCF). These approximation\ncoefficients offer a compressed representation of the pattern while capturing\nits primary variations and essential characteristics, thereby expediting the\nsearch for the best-matched pattern. Once the best-matched pattern in the\nwavelet approximation coefficients is identified, the original pattern can be\nperfectly reconstructed by integrating the DWT detail coefficients through an\nInverse-DWT transformation. Experiments conducted across diverse categorical\nTIs demonstrate simulations comparable to multi-scale CCSIM (MS-CCSIM),\naccompanied by an enhancement in facies connectivity and pattern reproduction.\nThe source code implementations are available at\nhttps://github.com/MBS1984/CCWSIM.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.00441v1"
    },
    {
        "title": "Quand rechercher c'est faire des vagues : Dans et {à} partir des\n  images algorithmiques",
        "authors": [
            "Gaëtan Robillard"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In Search of the Wave is a computer-generated film made in 2013, highlighting\nthe computation of images through computer simulation, and through text and\nvoice. Originating from a screening of the film at the Gustave Eiffel\nUniversity, the article presents a reflection on research-creation in and from\nalgorithmic images. Fundamentally, what is it in this research-creation --\nespecially in research on algorithmic imagery -- that can be set in motion?\nWithout fully distinguishing between what would be research on one hand and\ncreation on the other, we focus on characterizing forms, aesthetics, or\ntheories that contribute to possible shifts. The inventory of these\npossibilities is precisely the challenge of the text: from mathematics to image\nand visualization, from the birth of generative aesthetics to the coding\nrelated to pioneering works (recoding), or from indexing new aesthetics to new\nforms of critical production.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.03923v1"
    },
    {
        "title": "Nanouniverse: Virtual Instancing of Structural Detail and Adaptive Shell\n  Mapping",
        "authors": [
            "Ruwayda Alharbi",
            "Ondřej Strnad",
            "Markus Hadwiger",
            "Ivan Viola"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Rendering huge biological scenes with atomistic detail presents a significant\nchallenge in molecular visualization due to the memory limitations inherent in\ntraditional rendering approaches. In this paper, we propose a novel method for\nthe interactive rendering of massive molecular scenes based on\nhardware-accelerated ray tracing. Our approach circumvents GPU memory\nconstraints by introducing virtual instantiation of full-detail scene elements.\nUsing instancing significantly reduces memory consumption while preserving the\nfull atomistic detail of scenes comprising trillions of atoms, with interactive\nrendering performance and completely free user exploration. We utilize coarse\nmeshes as proxy geometries to approximate the overall shape of biological\ncompartments, and access all atomistic detail dynamically during ray tracing.\nWe do this via a novel adaptive technique utilizing a volumetric shell layer of\nprisms extruded around proxy geometry triangles, and a virtual volume grid for\nthe interior of each compartment. Our algorithm scales to enormous molecular\nscenes with minimal memory consumption and the potential to accommodate even\nlarger scenes. Our method also supports advanced effects such as clipping\nplanes and animations. We demonstrate the efficiency and scalability of our\napproach by rendering tens of instances of Red Blood Cell and SARS-CoV-2 models\ntheoretically containing more than 20 trillion atoms.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.05116v1"
    },
    {
        "title": "Towards Practical Meshlet Compression",
        "authors": [
            "Bastian Kuth",
            "Max Oberberger",
            "Felix Kawala",
            "Sander Reitter",
            "Sebastian Michel",
            "Matthäus Chajdas",
            "Quirin Meyer"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a codec specifically designed for meshlet compression, optimized\nfor rapid data-parallel GPU decompression within a mesh shader. Our compression\nstrategy orders triangles in optimal generalized triangle strips (GTSs), which\nwe generate by formulating the creation as a mixed integer linear program\n(MILP). Our method achieves index buffer compression rates of 16:1 compared to\nthe vertex pipeline and crack-free vertex attribute quantization based on user\npreference. The 15.5 million triangles of our teaser image decompress and\nrender in 0.59 ms on an AMD Radeon RX 7900 XTX.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.06359v2"
    },
    {
        "title": "InverseVis: Revealing the Hidden with Curved Sphere Tracing",
        "authors": [
            "Kai Lawonn",
            "Monique Meuschke",
            "Tobias Günther"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Exploratory analysis of scalar fields on surface meshes presents significant\nchallenges in identifying and visualizing important regions, particularly on\nthe surface's backside. Previous visualization methods achieved only a limited\nvisibility of significant features, i.e., regions with high or low scalar\nvalues, during interactive exploration. In response to this, we propose a novel\ntechnique, InverseVis, which leverages curved sphere tracing and uses the\notherwise unused space to enhance visibility. Our approach combines direct and\nindirect rendering, allowing camera rays to wrap around the surface and reveal\ninformation from the backside. To achieve this, we formulate an energy term\nthat guides the image synthesis in previously unused space, highlighting the\nmost important regions of the backside. By quantifying the amount of visible\nimportant features, we optimize the camera position to maximize the visibility\nof the scalar field on both the front and backsides. InverseVis is benchmarked\nagainst state-of-the-art methods and a derived technique, showcasing its\neffectiveness in revealing essential features and outperforming existing\napproaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.09092v2"
    },
    {
        "title": "Transforming a Non-Differentiable Rasterizer into a Differentiable One\n  with Stochastic Gradient Estimation",
        "authors": [
            "Thomas Deliot",
            "Eric Heitz",
            "Laurent Belcour"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We show how to transform a non-differentiable rasterizer into a\ndifferentiable one with minimal engineering efforts and no external\ndependencies (no Pytorch/Tensorflow). We rely on Stochastic Gradient\nEstimation, a technique that consists of rasterizing after randomly perturbing\nthe scene's parameters such that their gradient can be stochastically estimated\nand descended. This method is simple and robust but does not scale in\ndimensionality (number of scene parameters). Our insight is that the number of\nparameters contributing to a given rasterized pixel is bounded. Estimating and\naveraging gradients on a per-pixel basis hence bounds the dimensionality of the\nunderlying optimization problem and makes the method scalable. Furthermore, it\nis simple to track per-pixel contributing parameters by rasterizing ID- and\nUV-buffers, which are trivial additions to a rasterization engine if not\nalready available. With these minor modifications, we obtain an in-engine\noptimizer for 3D assets with millions of geometry and texture parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.09758v2"
    },
    {
        "title": "Application of 3D Gaussian Splatting for Cinematic Anatomy on Consumer\n  Class Devices",
        "authors": [
            "Simon Niedermayr",
            "Christoph Neuhauser",
            "Kaloian Petkov",
            "Klaus Engel",
            "Rüdiger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Interactive photorealistic rendering of 3D anatomy is used in medical\neducation to explain the structure of the human body. It is currently\nrestricted to frontal teaching scenarios, where even with a powerful GPU and\nhigh-speed access to a large storage device where the data set is hosted,\ninteractive demonstrations can hardly be achieved. We present the use of novel\nview synthesis via compressed 3D Gaussian Splatting (3DGS) to overcome this\nrestriction, and to even enable students to perform cinematic anatomy on\nlightweight and mobile devices. Our proposed pipeline first finds a set of\ncamera poses that captures all potentially seen structures in the data.\nHigh-quality images are then generated with path tracing and converted into a\ncompact 3DGS representation, consuming < 70 MB even for data sets of multiple\nGBs. This allows for real-time photorealistic novel view synthesis that\nrecovers structures up to the voxel resolution and is almost indistinguishable\nfrom the path-traced images\n",
        "pdf_link": "http://arxiv.org/pdf/2404.11285v2"
    },
    {
        "title": "Holographic Parallax Improves 3D Perceptual Realism",
        "authors": [
            "Dongyeon Kim",
            "Seung-Woo Nam",
            "Suyeon Choi",
            "Jong-Mo Seo",
            "Gordon Wetzstein",
            "Yoonchan Jeong"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Holographic near-eye displays are a promising technology to solve\nlong-standing challenges in virtual and augmented reality display systems. Over\nthe last few years, many different computer-generated holography (CGH)\nalgorithms have been proposed that are supervised by different types of target\ncontent, such as 2.5D RGB-depth maps, 3D focal stacks, and 4D light fields. It\nis unclear, however, what the perceptual implications are of the choice of\nalgorithm and target content type. In this work, we build a perceptual testbed\nof a full-color, high-quality holographic near-eye display. Under natural\nviewing conditions, we examine the effects of various CGH supervision formats\nand conduct user studies to assess their perceptual impacts on 3D realism. Our\nresults indicate that CGH algorithms designed for specific viewpoints exhibit\nnoticeable deficiencies in achieving 3D realism. In contrast, holograms\nincorporating parallax cues consistently outperform other formats across\ndifferent viewing conditions, including the center of the eyebox. This finding\nis particularly interesting and suggests that the inclusion of parallax cues in\nCGH rendering plays a crucial role in enhancing the overall quality of the\nholographic experience. This work represents an initial stride towards\ndelivering a perceptually realistic 3D experience with holographic near-eye\ndisplays.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.11810v1"
    },
    {
        "title": "Rendering Participating Media Using Path Graphs",
        "authors": [
            "Becky Hu",
            "Xi Deng",
            "Fujun Luan",
            "Miloš Hašan",
            "Steve Marschner"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Rendering volumetric scattering media, including clouds, fog, smoke, and\nother complex materials, is crucial for realism in computer graphics.\nTraditional path tracing, while unbiased, requires many long path samples to\nconverge in scenes with scattering media, and a lot of work is wasted by paths\nthat make a negligible contribution to the image. Methods to make better use of\nthe information learned during path tracing range from photon mapping to\nradiance caching, but struggle to support the full range of heterogeneous\nscattering media. This paper introduces a new volumetric rendering algorithm\nthat extends and adapts the previous \\emph{path graph} surface rendering\nalgorithm. Our method leverages the information collected through\nmultiple-scattering transport paths to compute lower-noise estimates,\nincreasing computational efficiency by reducing the required sample count. Our\nkey contributions include an extended path graph for participating media and\nnew aggregation and propagation operators for efficient path reuse in volumes.\nCompared to previous methods, our approach significantly boosts convergence in\nscenes with challenging volumetric light transport, including heterogeneous\nmedia with high scattering albedos and dense, forward-scattering translucent\nmaterials, under complex lighting conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.11894v1"
    },
    {
        "title": "The Life and Legacy of Bui Tuong Phong",
        "authors": [
            "Yoehan Oh",
            "Jacinda Tran",
            "Theodore Kim"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We examine the life and legacy of pioneering Vietnamese computer scientist\nB\\`ui Tuong Phong, whose shading and lighting models turned 50 last year. We\ntrace the trajectory of his life through Vietnam, France, and the United\nStates, and its intersections with global conflicts. Crucially, we present\ndefinitive evidence that his name has been cited incorrectly over the last five\ndecades. His family name is B\\`ui Tuong, not Phong. By presenting these facts\nat SIGGRAPH, we hope to collect more information about his life, and ensure\nthat his name is remembered correctly in the future.\n  Correction: An earlier version of the article speculated his family name was\nB\\`ui. We have since received definitive confirmation that his family name was\nB\\`ui Tuong. We have amended the text accordingly.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.14376v2"
    },
    {
        "title": "Physical Non-inertial Poser (PNP): Modeling Non-inertial Effects in\n  Sparse-inertial Human Motion Capture",
        "authors": [
            "Xinyu Yi",
            "Yuxiao Zhou",
            "Feng Xu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Existing inertial motion capture techniques use the human root coordinate\nframe to estimate local poses and treat it as an inertial frame by default. We\nargue that when the root has linear acceleration or rotation, the root frame\nshould be considered non-inertial theoretically. In this paper, we model the\nfictitious forces that are non-neglectable in a non-inertial frame by an\nauto-regressive estimator delicately designed following physics. With the\nfictitious forces, the force-related IMU measurement (accelerations) can be\ncorrectly compensated in the non-inertial frame and thus Newton's laws of\nmotion are satisfied. In this case, the relationship between the accelerations\nand body motions is deterministic and learnable, and we train a neural network\nto model it for better motion capture. Furthermore, to train the neural network\nwith synthetic data, we develop an IMU synthesis by simulation strategy to\nbetter model the noise model of IMU hardware and allow parameter tuning to fit\ndifferent hardware. This strategy not only establishes the network training\nwith synthetic data but also enables calibration error modeling to handle bad\nmotion capture calibration, increasing the robustness of the system. Code is\navailable at https://xinyu-yi.github.io/PNP/.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.19619v1"
    },
    {
        "title": "GPU-friendly Stroke Expansion",
        "authors": [
            "Raph Levien",
            "Arman Uguray"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Vector graphics includes both filled and stroked paths as the main\nprimitives. While there are many techniques for rendering filled paths on GPU,\nstroked paths have proved more elusive. This paper presents a technique for\nperforming stroke expansion, namely the generation of the outline representing\nthe stroke of the given input path. Stroke expansion is a global problem, with\nchallenging constraints on continuity and correctness. Nonetheless, we\nimplement it using a fully parallel algorithm suitable for execution in a GPU\ncompute shader, with minimal preprocessing. The output of our method can be\neither line or circular arc segments, both of which are well suited to GPU\nrendering, and the number of segments is minimal. We introduce several novel\ntechniques, including an encoding of vector graphics primitives suitable for\nparallel processing, and an Euler spiral based method for computing\napproximations to parallel curves and evolutes.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.00127v2"
    },
    {
        "title": "Virtual Psychedelia",
        "authors": [
            "Jacob Yenney",
            "Weichen Liu",
            "Ying C. Wu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present an approach to designing 3D Iterated Function Systems (IFS) within\nthe Unity Editor and rendered to VR in real-time. Objects are modeled as a\nhierarchical tree of primitive shapes and operators, editable using a graphical\nuser interface allowing artists to develop psychedelic scenes with little to no\ncoding knowledge, and is easily extensible for more advanced users to add their\nown primitive shapes and operators.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.00938v1"
    },
    {
        "title": "Region-Aware Color Smudging",
        "authors": [
            "Ying Jiang",
            "Pengfei Xu",
            "Congyi Zhang",
            "Hongbo Fu",
            "Henry Lau",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Color smudge operations from digital painting software enable users to create\nnatural shading effects in high-fidelity paintings by interactively mixing\ncolors. To precisely control results in traditional painting software, users\ntend to organize flat-filled color regions in multiple layers and smudge them\nto generate different color gradients. However, the requirement to carefully\ndeal with regions makes the smudging process time-consuming and laborious,\nespecially for non-professional users. This motivates us to investigate how to\ninfer user-desired smudging effects when users smudge over regions in a single\nlayer. To investigate improving color smudge performance, we first conduct a\nformative study. Following the findings of this study, we design SmartSmudge, a\nnovel smudge tool that offers users dynamical smudge brushes and real-time\nregion selection for easily generating natural and efficient shading effects.\nWe demonstrate the efficiency and effectiveness of the proposed tool via a user\nstudy and quantitative analysis\n",
        "pdf_link": "http://arxiv.org/pdf/2405.02759v1"
    },
    {
        "title": "Lifting Directional Fields to Minimal Sections",
        "authors": [
            "David Palmer",
            "Albert Chern",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Directional fields, including unit vector, line, and cross fields, are\nessential tools in the geometry processing toolkit. The topology of directional\nfields is characterized by their singularities. While singularities play an\nimportant role in downstream applications such as meshing, existing methods for\ncomputing directional fields either require them to be specified in advance,\nignore them altogether, or treat them as zeros of a relaxed field. While fields\nare ill-defined at their singularities, the graphs of directional fields with\nsingularities are well-defined surfaces in a circle bundle. By lifting\noptimization of fields to optimization over their graphs, we can exploit a\nnatural convex relaxation to a minimal section problem over the space of\ncurrents in the bundle. This relaxation treats singularities as first-class\ncitizens, expressing the relationship between fields and singularities as an\nexplicit boundary condition. As curvature frustrates finite element\ndiscretization of the bundle, we devise a hybrid spectral method for\nrepresenting and optimizing minimal sections. Our method supports field\noptimization on both flat and curved domains and enables more precise control\nover singularity placement.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.03853v1"
    },
    {
        "title": "Modal Folding: Discovering Smooth Folding Patterns for Sheet Materials\n  using Strain-Space Modes",
        "authors": [
            "Pengbin Tang",
            "Ronan Hinchet",
            "Roi Poranne",
            "Bernhard Thomaszewski",
            "Stelian Coros"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Folding can transform mundane objects such as napkins into stunning works of\nart. However, finding new folding transformations for sheet materials is a\nchallenging problem that requires expertise and real-world experimentation. In\nthis paper, we present Modal Folding -- an automated approach for discovering\nenergetically optimal folding transformations, i.e., large deformations that\nrequire little mechanical work. For small deformations, minimizing internal\nenergy for fixed displacement magnitudes leads to the well-known elastic\neigenmodes. While linear modes provide promising directions for bending, they\ncannot capture the rotational motion required for folding. To overcome this\nlimitation, we introduce strain-space modes -- nonlinear analogues of elastic\neigenmodes that operate on per-element curvatures instead of vertices. Using\nstrain-space modes to determine target curvatures for bending elements, we can\ngenerate complex nonlinear folding motions by simply minimizing the sheet's\ninternal energy. Our modal folding approach offers a systematic and automated\nway to create complex designs. We demonstrate the effectiveness of our method\nwith simulation results for a range of shapes and materials, and validate our\ndesigns with physical prototypes.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.04280v1"
    },
    {
        "title": "Editing Mesh Sequences with Varying Connectivity",
        "authors": [
            "Filip Hácha",
            "Jan Dvořák",
            "Zuzana Káčereková",
            "Libor Váša"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Time-varying connectivity of triangle mesh sequences leads to substantial\ndifficulties in their processing. Unlike editing sequences with constant\nconnectivity, editing sequences with varying connectivity requires addressing\nthe problem of temporal correspondence between the frames of the sequence. We\npresent a method for time-consistent editing of triangle mesh sequences with\nvarying connectivity using sparse temporal correspondence, which can be\nobtained using existing methods. Our method includes a deformation model based\non the usage of the sparse temporal correspondence, which is suitable for the\ntemporal propagation of user-specified deformations of the edited surface with\nrespect to the shape and true topology of the surface while preserving the\nindividual connectivity of each frame. Since there is no other method capable\nof comparable types of editing on time-varying meshes, we compare our method\nand the proposed deformation model with a baseline approach and demonstrate the\nbenefits of our framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.04957v1"
    },
    {
        "title": "SketchDream: Sketch-based Text-to-3D Generation and Editing",
        "authors": [
            "Feng-Lin Liu",
            "Hongbo Fu",
            "Yu-Kun Lai",
            "Lin Gao"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Existing text-based 3D generation methods generate attractive results but\nlack detailed geometry control. Sketches, known for their conciseness and\nexpressiveness, have contributed to intuitive 3D modeling but are confined to\nproducing texture-less mesh models within predefined categories. Integrating\nsketch and text simultaneously for 3D generation promises enhanced control over\ngeometry and appearance but faces challenges from 2D-to-3D translation\nambiguity and multi-modal condition integration. Moreover, further editing of\n3D models in arbitrary views will give users more freedom to customize their\nmodels. However, it is difficult to achieve high generation quality, preserve\nunedited regions, and manage proper interactions between shape components. To\nsolve the above issues, we propose a text-driven 3D content generation and\nediting method, SketchDream, which supports NeRF generation from given\nhand-drawn sketches and achieves free-view sketch-based local editing. To\ntackle the 2D-to-3D ambiguity challenge, we introduce a sketch-based multi-view\nimage generation diffusion model, which leverages depth guidance to establish\nspatial correspondence. A 3D ControlNet with a 3D attention module is utilized\nto control multi-view images and ensure their 3D consistency. To support local\nediting, we further propose a coarse-to-fine editing approach: the coarse phase\nanalyzes component interactions and provides 3D masks to label edited regions,\nwhile the fine stage generates realistic results with refined details by local\nenhancement. Extensive experiments validate that our method generates\nhigher-quality results compared with a combination of 2D ControlNet and\nimage-to-3D generation techniques and achieves detailed control compared with\nexisting diffusion-based 3D editing approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.06461v2"
    },
    {
        "title": "Path Guiding for Wavefront Path Tracing: A Memory Efficient Approach for\n  GPU Path Tracers",
        "authors": [
            "Bora Yalçıner",
            "Ahmet Oğuz Akyüz"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a path-guiding algorithm to be incorporated into the wavefront\nstyle of path tracers (WFPTs). As WFPTs are primarily implemented on graphics\nprocessing units (GPUs), the proposed method aims to leverage the capabilities\nof the GPUs and reduce the hierarchical data structure and memory usage\ntypically required for such techniques. To achieve this, our algorithm only\nstores the radiant exitance on a single global sparse voxel octree (SVO) data\nstructure. Probability density functions required to guide the rays are\ngenerated on-the-fly using this data structure. The proposed approach reduces\nthe scene-related persistent memory requirements compared to other path-guiding\ntechniques while producing similar or better results depending on scene\ncharacteristics. To our knowledge, our algorithm is the first one that\nincorporates path guiding into a WFPT.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.06997v1"
    },
    {
        "title": "Vertex Shader Domain Warping with Automatic Differentiation",
        "authors": [
            "Dave Pagurek van Mossel"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Domain warping is a technique commonly used in creative coding to distort\ngraphics and add visual interest to a work. The approach has the potential to\nbe used in 3D art as mesh vertices can be efficiently warped using a vertex\nshader in a WebGL pipeline. However, 3D models packaged for the web typically\ncome with baked-in normal vectors, and these need to be updated when vertex\npositions change for lighting calculations to work. This is typically done via\nfinite differences, which requires parameter tuning to achieve optimal visual\nfidelity. We present a method for 3D domain warping that works with automatic\ndifferentiation, allowing exact normals to be used without any tuning while\nstill benefiting from hardware acceleration.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07124v1"
    },
    {
        "title": "Locality-Preserving Free-Form Deformation",
        "authors": [
            "Tsukasa Fukusato",
            "Akinobu Maejima",
            "Takeo Igarashi"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper proposes a method to estimate the locations of grid handles in\nfree-form deformation (FFD) while preserving the local shape characteristics of\nthe 2D/3D input model embedded into the grid, named locality-preserving FFD\n(lp-FFD). Users first specify some vertex locations in the input model and grid\nhandle locations. The system then optimizes all locations of grid handles by\nminimizing the distortion of the input model's mesh elements. The proposed\nmethod is fast and stable, allowing the user to directly and indirectly make\nthe deformed shape of mesh model and grid. This paper shows some examples of\ndeformation results to demonstrate the robustness of our lp-FFD. In addition,\nwe conducted a user study and confirm our lp-FFD's efficiency and effectiveness\nin shape deformation is higher than those of existing methods used in\ncommercial software.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07450v1"
    },
    {
        "title": "A Hessian-Based Field Deformer for Real-Time Topology-Aware Shape\n  Editing",
        "authors": [
            "Yunxiao Zhang",
            "Zixiong Wang",
            "Zihan Zhao",
            "Rui Xu",
            "Shuangmin Chen",
            "Shiqing Xin",
            "Wenping Wang",
            "Changhe Tu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Shape manipulation is a central research topic in computer graphics. Topology\nediting, such as breaking apart connections, joining disconnected ends, and\nfilling/opening a topological hole, is generally more challenging than geometry\nediting. In this paper, we observe that the saddle points of the signed\ndistance function (SDF) provide useful hints for altering surface topology\ndeliberately. Based on this key observation, we parameterize the SDF into a\ncubic trivariate tensor-product B-spline function $F$ whose saddle points\n$\\{\\boldsymbol{s}_i\\}$ can be quickly exhausted based on a subdivision-based\nroot-finding technique coupled with Newton's method. Users can select one of\nthe candidate points, say $\\boldsymbol{s}_i$, to edit the topology in real\ntime. In implementation, we add a compactly supported B-spline function rooted\nat $\\boldsymbol{s}_i$, which we call a \\textit{deformer} in this paper, to $F$,\nwith its local coordinate system aligning with the three eigenvectors of the\nHessian. Combined with ray marching technique, our interactive system operates\nat 30 FPS. Additionally, our system empowers users to create desired bulges or\nconcavities on the surface. An extensive user study indicates that our system\nis user-friendly and intuitive to operate. We demonstrate the effectiveness and\nusefulness of our system in a range of applications, including fixing surface\nreconstruction errors, artistic work design, 3D medical imaging and simulation,\nand antiquity restoration. Please refer to the attached video for a\ndemonstration.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07644v1"
    },
    {
        "title": "Unfolding via Progressive Mesh Approximation",
        "authors": [
            "Lars Zawallich",
            "Renato Pajarola"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  When folding a 3D object from a 2D material like paper, typically only an\napproximation of the original surface geometry is needed. Such an approximation\ncan effectively be created by a (progressive) mesh simplification approach,\ne.g. using an edge collapse technique. Moreover, when searching for an\nunfolding of the object, this approximation is assumed to be fixed. In this\nwork, we take a different route and allow the approximation to change while\nsearching for an unfolding. This way, we increase the chances to overcome\npossible ununfoldability issues. To join the two concepts of mesh approximation\nand unfolding, our work combines the edge collapsing mesh simplification\ntechnique with a Tabu Unfolder, a robust mesh unfolding approach. We\nempirically show that this strategy performs faster and that it is more\nreliable than prior state of the art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07922v1"
    },
    {
        "title": "Eulerian-Lagrangian Fluid Simulation on Particle Flow Maps",
        "authors": [
            "Junwei Zhou",
            "Duowen Chen",
            "Molin Deng",
            "Yitong Deng",
            "Yuchen Sun",
            "Sinan Wang",
            "Shiying Xiong",
            "Bo Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a novel Particle Flow Map (PFM) method to enable accurate\nlong-range advection for incompressible fluid simulation. The foundation of our\nmethod is the observation that a particle trajectory generated in a forward\nsimulation naturally embodies a perfect flow map. Centered on this concept, we\nhave developed an Eulerian-Lagrangian framework comprising four essential\ncomponents: Lagrangian particles for a natural and precise representation of\nbidirectional flow maps; a dual-scale map representation to accommodate the\nmapping of various flow quantities; a particle-to-grid interpolation scheme for\naccurate quantity transfer from particles to grid nodes; and a hybrid\nimpulse-based solver to enforce incompressibility on the grid. The efficacy of\nPFM has been demonstrated through various simulation scenarios, highlighting\nthe evolution of complex vortical structures and the details of turbulent\nflows. Notably, compared to NFM, PFM reduces computing time by up to 49 times\nand memory consumption by up to 41%, while enhancing vorticity preservation as\nevidenced in various tests like leapfrog, vortex tube, and turbulent flow.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.09672v1"
    },
    {
        "title": "Lagrangian Covector Fluid with Free Surface",
        "authors": [
            "Zhiqi Li",
            "Barnabás Börcsök",
            "Duowen Chen",
            "Yutong Sun",
            "Bo Zhu",
            "Greg Turk"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper introduces a novel Lagrangian fluid solver based on covector flow\nmaps. We aim to address the challenges of establishing a robust flow-map solver\nfor incompressible fluids under complex boundary conditions. Our key idea is to\nuse particle trajectories to establish precise flow maps and tailor path\nintegrals of physical quantities along these trajectories to reformulate the\nPoisson problem during the projection step. We devise a decoupling mechanism\nbased on path-integral identities from flow-map theory. This mechanism\nintegrates long-range flow maps for the main fluid body into a short-range\nprojection framework, ensuring a robust treatment of free boundaries. We show\nthat our method can effectively transform a long-range projection problem with\nintegral boundaries into a Poisson problem with standard boundary conditions --\nspecifically, zero Dirichlet on the free surface and zero Neumann on solid\nboundaries. This transformation significantly enhances robustness and accuracy,\nextending the applicability of flow-map methods to complex free-surface\nproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.09801v1"
    },
    {
        "title": "Real-time Level-of-Detail Strand-based Hair Rendering",
        "authors": [
            "Tao Huang",
            "Yang Zhou",
            "Daqi Lin",
            "Junqiu Zhu",
            "Ling-Qi Yan",
            "Kui Wu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Strand-based hair rendering has become increasingly popular in production for\nits realistic appearance. However, the prevailing level-of-detail solution\nemploying hair cards for distant hair models introduces a significant\ndiscontinuity in dynamics and appearance during the transition from strands to\ncards. We introduce an innovative real-time framework for strand-based hair\nrendering that ensures seamless transitions between different levels of detail\n(LOD) while maintaining a consistent hair appearance. Our method uses\nelliptical thick hairs that contain multiple hair strands at each LOD to\nmaintain the shapes of hair clusters. In addition to geometric fitting, we\nformulate an elliptical Bidirectional Curve Scattering Distribution Functions\n(BCSDF) model for a thick hair, accurately capturing single scattering and\nmultiple scattering within the hair cluster, accommodating a spectrum from\nsparse to dense hair distributions. Our framework, tested on various hairstyles\nwith dynamics as well as knits, shows that it can produce highly similar\nappearances to full hair geometries at different viewing distances with\nseamless LOD transitions, while achieving up to a 3x speedup.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.10565v2"
    },
    {
        "title": "Legible Label Layout for Data Visualization, Algorithm and Integration\n  into Vega-Lite",
        "authors": [
            "Chanwut Kittivorawong"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Legible labels should not overlap with other labels and other marks in a\nchart. When a chart contains a large number of data points, manually\npositioning these labels for each data point in the chart is a tedious task. A\nlabeling algorithm is necessary to automatically layout the labels for a chart\nwith a large number of data points. The state-of-the-art labeling algorithm\ndetects overlaps using a set of points to approximate each mark's shape. This\napproach is inefficient for large marks or many marks as it requires too many\npoints to detect overlaps. In response, we present a bitmap-based label\nplacement algorithm, which leverages an occupancy bitmap to accelerate overlap\ndetection. To create an occupancy bitmap, we rasterize marks onto a bitmap\nbased on the area they occupy in the chart. With the bitmap, we can efficiently\nplace labels without overlapping existing marks, regardless of the number and\ngeometric complexity of the marks. This bitmap-based algorithm offers\nsignificant performance improvements over the state-of-the-art approach while\nplacing a similar number of labels. We also integrate this algorithm into\nVega-Lite as one of its encoding channels, label encoding. Label encoding\nallows users to encode fields in each data point with a text label to annotate\nthe mark that represents the data point in a chart.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.10953v2"
    },
    {
        "title": "Generative AI for 2D Character Animation",
        "authors": [
            "Jaime Guajardo",
            "Ozgun Bursalioglu",
            "Dan B Goldman"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this pilot project, we teamed up with artists to develop new workflows for\n2D animation while producing a short educational cartoon. We identified several\nworkflows to streamline the animation process, bringing the artists' vision to\nthe screen more effectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.11098v2"
    },
    {
        "title": "XPBI: Position-Based Dynamics with Smoothing Kernels Handles Continuum\n  Inelasticity",
        "authors": [
            "Chang Yu",
            "Xuan Li",
            "Lei Lan",
            "Yin Yang",
            "Chenfanfu Jiang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  PBD and its extension, XPBD, have been predominantly applied to compliant\nconstrained elastodynamics, with their potential in finite strain (visco-)\nelastoplasticity remaining underexplored. XPBD is often perceived to stand in\ncontrast to other meshless methods, such as the MPM. MPM is based on\ndiscretizing the weak form of governing partial differential equations within a\ncontinuum domain, coupled with a hybrid Lagrangian-Eulerian method for tracking\ndeformation gradients. In contrast, XPBD formulates specific constraints,\nwhether hard or compliant, to positional degrees of freedom. We revisit this\nperception by investigating the potential of XPBD in handling inelastic\nmaterials that are described with classical continuum mechanics-based yield\nsurfaces and elastoplastic flow rules. Our inspiration is that a robust\nestimation of the velocity gradient is a sufficiently useful key to effectively\ntracking deformation gradients in XPBD simulations. By further incorporating\nimplicit inelastic constitutive relationships, we introduce a plasticity\nin-the-loop updated Lagrangian augmentation to XPBD. This enhancement enables\nthe simulation of elastoplastic, viscoplastic, and granular substances\nfollowing their standard constitutive laws. We demonstrate the effectiveness of\nour method through high-resolution and real-time simulations of diverse\nmaterials such as snow, sand, and plasticine, and its integration with standard\nXPBD simulations of cloth and water.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.11694v2"
    },
    {
        "title": "Volumetric Homogenization for Knitwear Simulation",
        "authors": [
            "Chun Yuan",
            "Haoyang Shi",
            "Lei Lan",
            "Yuxing Qiu",
            "Cem Yuksel",
            "Huamin Wang",
            "Chenfanfu Jiang",
            "Kui Wu",
            "Yin Yang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper presents volumetric homogenization, a spatially varying\nhomogenization scheme for knitwear simulation. We are motivated by the\nobservation that macro-scale fabric dynamics is strongly correlated with its\nunderlying knitting patterns. Therefore, homogenization towards a single\nmaterial is less effective when the knitting is complex and non-repetitive. Our\nmethod tackles this challenge by homogenizing the yarn-level material locally\nat volumetric elements. Assigning a virtual volume of a knitting structure\nenables us to model bending and twisting effects via a simple volume-preserving\npenalty and thus effectively alleviates the material nonlinearity. We employ an\nadjoint Gauss-Newton formulation to battle the dimensionality challenge of such\nper-element material optimization. This intuitive material model makes the\nforward simulation GPU-friendly. To this end, our pipeline also equips a novel\ndomain-decomposed subspace solver crafted for GPU projective dynamics, which\nmakes our simulator hundreds of times faster than the yarn-level simulator.\nExperiments validate the capability and effectiveness of volumetric\nhomogenization. Our method produces realistic animations of knitwear matching\nthe quality of full-scale yarn-level simulations. It is also orders of\nmagnitude faster than existing homogenization techniques in both the training\nand simulation stages.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12484v3"
    },
    {
        "title": "Differential Walk on Spheres",
        "authors": [
            "Bailey Miller",
            "Rohan Sawhney",
            "Keenan Crane",
            "Ioannis Gkioulekas"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce a Monte Carlo method for computing derivatives of the solution\nto a partial differential equation (PDE) with respect to problem parameters\n(such as domain geometry or boundary conditions). Derivatives can be evaluated\nat arbitrary points, without performing a global solve or constructing a\nvolumetric grid or mesh. The method is hence well suited to inverse problems\nwith complex geometry, such as PDE-constrained shape optimization. Like other\nwalk on spheres (WoS) algorithms, our method is trivial to parallelize, and is\nagnostic to boundary representation (meshes, splines, implicit surfaces, etc.),\nsupporting large topological changes. We focus in particular on screened\nPoisson equations, which model diverse problems from scientific and geometric\ncomputing. As in differentiable rendering, we jointly estimate derivatives with\nrespect to all parameters -- hence, cost does not grow significantly with\nparameter count. In practice, even noisy derivative estimates exhibit fast,\nstable convergence for stochastic gradient-based optimization, as we show\nthrough examples from thermal design, shape from diffusion, and computer\ngraphics.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12964v4"
    },
    {
        "title": "LucidRaster: GPU Software Rasterizer for Exact Order-Independent\n  Transparency",
        "authors": [
            "Krzysztof Jakubowski"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Transparency rendering is problematic and can be considered an open problem\nin real-time graphics. There are many different algorithms currently available,\nbut handling complex scenes and achieving accurate, glitch-free results is\nstill costly.\n  This paper describes LucidRaster: a software rasterizer running on a GPU\nwhich allows for efficient exact rendering of complex transparent scenes. It\nuses a new two-stage sorting technique and sample accumulation method. On\naverage it's faster than high-quality OIT approximations and only about 3x\nslower than hardware alpha blending. It can be very efficient especially when\nrendering scenes with high triangle density or high depth complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.13364v2"
    },
    {
        "title": "Specular Polynomials",
        "authors": [
            "Zhimin Fan",
            "Jie Guo",
            "Yiming Wang",
            "Tianyu Xiao",
            "Hao Zhang",
            "Chenxi Zhou",
            "Zhenyu Chen",
            "Pengpei Hong",
            "Yanwen Guo",
            "Ling-Qi Yan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Finding valid light paths that involve specular vertices in Monte Carlo\nrendering requires solving many non-linear, transcendental equations in\nhigh-dimensional space. Existing approaches heavily rely on Newton iterations\nin path space, which are limited to obtaining at most a single solution each\ntime and easily diverge when initialized with improper seeds.\n  We propose specular polynomials, a Newton iteration-free methodology for\nfinding a complete set of admissible specular paths connecting two arbitrary\nendpoints in a scene. The core is a reformulation of specular constraints into\npolynomial systems, which makes it possible to reduce the task to a univariate\nroot-finding problem. We first derive bivariate systems utilizing rational\ncoordinate mapping between the coordinates of consecutive vertices.\nSubsequently, we adopt the hidden variable resultant method for variable\nelimination, converting the problem into finding zeros of the determinant of\nunivariate matrix polynomials. This can be effectively solved through Laplacian\nexpansion for one bounce and a bisection solver for more bounces.\n  Our solution is generic, completely deterministic, accurate for the case of\none bounce, and GPU-friendly. We develop efficient CPU and GPU implementations\nand apply them to challenging glints and caustic rendering. Experiments on\nvarious scenarios demonstrate the superiority of specular polynomial-based\nsolutions compared to Newton iteration-based counterparts.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.13409v1"
    },
    {
        "title": "Subspace Mixed-FEM for Real-Time Heterogeneous Elastodynamics",
        "authors": [
            "Ty Trusty",
            "Otman Benchekroun",
            "Eitan Grinspun",
            "Danny M. Kaufman",
            "David I. W. Levin"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a reduced space mixed finite element method (MFEM) built on a\nSkinning Eigenmode subspace and material-aware cubature scheme. Our solver is\nwell-suited for simulating scenes with large material and geometric\nheterogeneities in real-time. This mammoth geometry is composed of 98,175\nvertices and 531,565 tetrahedral elements and with a heterogenous composition\nof widely varying materials of muscles ($E= 5\\times10^5$ Pa), joints\n($E=1\\times10^5$ Pa), and bone ($E=1\\times10^{10}$ Pa). The resulting\nsimulation runs at 120 frames per second (FPS).\n",
        "pdf_link": "http://arxiv.org/pdf/2405.13730v1"
    },
    {
        "title": "Diffusing Winding Gradients (DWG): A Parallel and Scalable Method for 3D\n  Reconstruction from Unoriented Point Clouds",
        "authors": [
            "Weizhou Liu",
            "Jiaze Li",
            "Xuhui Chen",
            "Fei Hou",
            "Shiqing Xin",
            "Xingce Wang",
            "Zhongke Wu",
            "Chen Qian",
            "Ying He"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper presents a new method, Diffusing Winding Gradients (DWG), for\nreconstructing watertight 3D surfaces from unoriented point clouds. Our method\nexploits the alignment between the gradients of the generalized winding number\n(GWN) field and globally consistent normals to orient points effectively.\nStarting with an unoriented point cloud, DWG initially assigns a random normal\nto each point. It computes the corresponding GWN field and extract a level set\nwhose iso-value is the average GWN values across all input points. The\ngradients of this level set are then utilized to update the point normals. This\ncycle of recomputing the GWN field and updating point normals is repeated until\nthe GWN level sets stabilize and their gradients cease to change. Unlike\nconventional methods, our method does not rely on solving linear systems or\noptimizing objective functions, which simplifies its implementation and\nenhances its suitability for efficient parallel execution. Experimental results\ndemonstrate that our method significantly outperforms existing methods in terms\nof runtime performance. For large-scale models with 10 to 20 million points,\nour CUDA implementation on an NVIDIA GTX 4090 GPU achieves speeds 30-120 times\nfaster than iPSR, the leading sequential method, tested on a high-end PC with\nan Intel i9 CPU. Additionally, by employing a screened variant of GWN, DWG\ndemonstrates enhanced robustness against noise and outliers, and proves\neffective for models with thin structures and real-world inputs with\noverlapping and misaligned scans. For source code and more details, visit our\nproject webpage: https://dwgtech.github.io/.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.13839v2"
    },
    {
        "title": "A Dynamic By-example BTF Synthesis Scheme",
        "authors": [
            "Zilin Xu",
            "Zahra Montazeri",
            "Beibei Wang",
            "Ling-Qi Yan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Measured Bidirectional Texture Function (BTF) can faithfully reproduce a\nrealistic appearance but is costly to acquire and store due to its 6D nature\n(2D spatial and 4D angular). Therefore, it is practical and necessary for\nrendering to synthesize BTFs from a small example patch. While previous methods\nmanaged to produce plausible results, we find that they seldomly take into\nconsideration the property of being dynamic, so a BTF must be synthesized\nbefore the rendering process, resulting in limited size, costly pre-generation\nand storage issues. In this paper, we propose a dynamic BTF synthesis scheme,\nwhere a BTF at any position only needs to be synthesized when being queried.\nOur insight is that, with the recent advances in neural dimension reduction\nmethods, a BTF can be decomposed into disjoint low-dimensional components. We\ncan perform dynamic synthesis only on the positional dimensions, and during\nrendering, recover the BTF by querying and combining these low-dimensional\nfunctions with the help of a lightweight Multilayer Perceptron (MLP).\nConsequently, we obtain a fully dynamic 6D BTF synthesis scheme that does not\nrequire any pre-generation, which enables efficient rendering of our infinitely\nlarge and non-repetitive BTFs on the fly. We demonstrate the effectiveness of\nour method through various types of BTFs taken from UBO2014.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.14025v1"
    },
    {
        "title": "LDM: Large Tensorial SDF Model for Textured Mesh Generation",
        "authors": [
            "Rengan Xie",
            "Wenting Zheng",
            "Kai Huang",
            "Yizheng Chen",
            "Qi Wang",
            "Qi Ye",
            "Wei Chen",
            "Yuchi Huo"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Previous efforts have managed to generate production-ready 3D assets from\ntext or images. However, these methods primarily employ NeRF or 3D Gaussian\nrepresentations, which are not adept at producing smooth, high-quality\ngeometries required by modern rendering pipelines. In this paper, we propose\nLDM, a novel feed-forward framework capable of generating high-fidelity,\nillumination-decoupled textured mesh from a single image or text prompts. We\nfirstly utilize a multi-view diffusion model to generate sparse multi-view\ninputs from single images or text prompts, and then a transformer-based model\nis trained to predict a tensorial SDF field from these sparse multi-view image\ninputs. Finally, we employ a gradient-based mesh optimization layer to refine\nthis model, enabling it to produce an SDF field from which high-quality\ntextured meshes can be extracted. Extensive experiments demonstrate that our\nmethod can generate diverse, high-quality 3D mesh assets with corresponding\ndecomposed RGB textures within seconds.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.14580v3"
    },
    {
        "title": "Elastic Locomotion with Mixed Second-order Differentiation",
        "authors": [
            "Siyuan Shen",
            "Tianjia Shao",
            "Kun Zhou",
            "Chenfanfu Jiang",
            "Sheldon Andrews",
            "Victor Zordan",
            "Yin Yang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a framework of elastic locomotion, which allows users to enliven\nan elastic body to produce interesting locomotion by prescribing its high-level\nkinematics. We formulate this problem as an inverse simulation problem and seek\nthe optimal muscle activations to drive the body to complete the desired\nactions. We employ the interior-point method to model wide-area contacts\nbetween the body and the environment with logarithmic barrier penalties. The\ncore of our framework is a mixed second-order differentiation algorithm. By\ncombining both analytic differentiation and numerical differentiation\nmodalities, a general-purpose second-order differentiation scheme is made\npossible. Specifically, we augment complex-step finite difference (CSFD) with\nreverse automatic differentiation (AD). We treat AD as a generic function,\nmapping a computing procedure to its derivative w.r.t. output loss, and promote\nCSFD along the AD computation. To this end, we carefully implement all the\narithmetics used in elastic locomotion, from elementary functions to linear\nalgebra and matrix operation for CSFD promotion. With this novel\ndifferentiation tool, elastic locomotion can directly exploit Newton's method\nand use its strong second-order convergence to find the needed activations at\nmuscle fibers. This is not possible with existing first-order inverse or\ndifferentiable simulation techniques. We showcase a wide range of interesting\nlocomotions of soft bodies and creatures to validate our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.14595v1"
    },
    {
        "title": "Challenges and Opportunities in 3D Content Generation",
        "authors": [
            "Ke Zhao",
            "Andreas Larsen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper explores the burgeoning field of 3D content generation within the\nlandscape of Artificial Intelligence Generated Content (AIGC) and large-scale\nmodels. It investigates innovative methods like Text-to-3D and Image-to-3D,\nwhich translate text or images into 3D objects, reshaping our understanding of\nvirtual and real-world simulations. Despite significant advancements in text\nand image generation, automatic 3D content generation remains nascent. This\npaper emphasizes the urgency for further research in this area. By leveraging\npre-trained diffusion models, which have demonstrated prowess in high-fidelity\nimage generation, this paper aims to summary 3D content creation, addressing\nchallenges such as data scarcity and computational resource limitations.\nAdditionally, this paper discusses the challenges and proposes solutions for\nusing pre-trained diffusion models in 3D content generation. By synthesizing\nrelevant research and outlining future directions, this study contributes to\nadvancing the field of 3D content generation amidst the proliferation of\nlarge-scale AIGC models.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.15335v1"
    },
    {
        "title": "Fast and Globally Consistent Normal Orientation based on the Winding\n  Number Normal Consistency",
        "authors": [
            "Siyou Lin",
            "Zuoqiang Shi",
            "Yebin Liu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Estimating consistently oriented normals for point clouds enables a number of\nimportant applications in computer graphics. While local normal estimation is\npossible with simple techniques like PCA, orienting them to be globally\nconsistent has been a notoriously difficult problem. Some recent methods\nexploit various properties of the winding number formula to achieve global\nconsistency. Despite their exciting progress, these algorithms either have high\nspace/time complexity, or do not produce accurate and consistently oriented\nnormals for imperfect data. In this paper, we propose a novel property from the\nwinding number formula, termed Winding Number Normal Consistency (WNNC), to\ntackle this problem. The derived property is based on the simple observation\nthat the normals (negative gradients) sampled from the winding number field\nshould be codirectional to the normals used to compute the winding number\nfield. Since the WNNC property itself does not resolve the inside/outside\norientation ambiguity, we further incorporate an objective function from\nParametric Gauss Reconstruction (PGR). We propose to iteratively update normals\nby alternating between WNNC-based normal updates and PGR-based gradient\ndescents, which leads to an embarrassingly simple yet effective iterative\nalgorithm that allows fast and high-quality convergence to globally consistent\nnormals. Furthermore, our proposed algorithm only involves repeatedly\nevaluating the winding number formula and its derivatives, which can be\naccelerated and parallelized using a treecode-based approximation algorithm.\nOur GPU (and even CPU) implementation can be significantly faster than the\nrecent state-of-the-art methods for normal orientation from raw points. Our\ncode is integrated with the popular PyTorch framework to facilitate further\nresearch into winding numbers, and is publicly available at\nhttps://jsnln.github.io/wnnc/index.html.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.16634v2"
    },
    {
        "title": "Anisotropic Gauss Reconstruction for Unoriented Point Clouds",
        "authors": [
            "Yueji Ma",
            "Dong Xiao",
            "Zuoqiang Shi",
            "Bin Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Unoriented surface reconstructions based on the Gauss formula have attracted\nmuch attention due to their elegant mathematical formulation and excellent\nperformance. However, the isotropic characteristics of the formulation limit\ntheir capacity to leverage the anisotropic information within the point cloud.\nIn this work, we propose a novel anisotropic formulation by introducing a\nconvection term in the original Laplace operator. By choosing different\nvelocity vectors, the anisotropic feature can be exploited to construct more\neffective linear equations. Moreover, an adaptive selection strategy is\nintroduced for the velocity vector to further enhance the orientation and\nreconstruction performance of thin structures. Extensive experiments\ndemonstrate that our method achieves state-of-the-art performance and manages\nvarious challenging situations, especially for models with thin structures or\nsmall holes. The source code will be released on GitHub.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17193v1"
    },
    {
        "title": "A Grid-Free Fluid Solver based on Gaussian Spatial Representation",
        "authors": [
            "Jingrui Xing",
            "Bin Wang",
            "Mengyu Chu",
            "Baoquan Chen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a grid-free fluid solver featuring a novel Gaussian\nrepresentation. Drawing inspiration from the expressive capabilities of 3D\nGaussian Splatting in multi-view image reconstruction, we model the continuous\nflow velocity as a weighted sum of multiple Gaussian functions. Leveraging this\nrepresentation, we derive differential operators for the field and implement a\ntime-dependent PDE solver using the traditional operator splitting method.\nCompared to implicit neural representations as another continuous spatial\nrepresentation with increasing attention, our method with flexible 3D Gaussians\npresents enhanced accuracy on vorticity preservation. Moreover, we apply\nphysics-driven strategies to accelerate the optimization-based time integration\nof Gaussian functions. This temporal evolution surpasses previous work based on\nimplicit neural representation with reduced computational time and memory.\nAlthough not surpassing the quality of state-of-the-art Eulerian methods in\nfluid simulation, experiments and ablation studies indicate the potential of\nour memory-efficient representation. With enriched spatial information, our\nmethod exhibits a distinctive perspective combining the advantages of Eulerian\nand Lagrangian approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.18133v1"
    },
    {
        "title": "NegGS: Negative Gaussian Splatting",
        "authors": [
            "Artur Kasymov",
            "Bartosz Czekaj",
            "Marcin Mazur",
            "Jacek Tabor",
            "Przemysław Spurek"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  One of the key advantages of 3D rendering is its ability to simulate\nintricate scenes accurately. One of the most widely used methods for this\npurpose is Gaussian Splatting, a novel approach that is known for its rapid\ntraining and inference capabilities. In essence, Gaussian Splatting involves\nincorporating data about the 3D objects of interest into a series of Gaussian\ndistributions, each of which can then be depicted in 3D in a manner analogous\nto traditional meshes. It is regrettable that the use of Gaussians in Gaussian\nSplatting is currently somewhat restrictive due to their perceived linear\nnature. In practice, 3D objects are often composed of complex curves and highly\nnonlinear structures. This issue can to some extent be alleviated by employing\na multitude of Gaussian components to reflect the complex, nonlinear structures\naccurately. However, this approach results in a considerable increase in time\ncomplexity. This paper introduces the concept of negative Gaussians, which are\ninterpreted as items with negative colors. The rationale behind this approach\nis based on the density distribution created by dividing the probability\ndensity functions (PDFs) of two Gaussians, which we refer to as Diff-Gaussian.\nSuch a distribution can be used to approximate structures such as donut and\nmoon-shaped datasets. Experimental findings indicate that the application of\nthese techniques enhances the modeling of high-frequency elements with rapid\ncolor transitions. Additionally, it improves the representation of shadows. To\nthe best of our knowledge, this is the first paper to extend the simple\nelipsoid shapes of Gaussian Splatting to more complex nonlinear structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.18163v2"
    },
    {
        "title": "Actuators À La Mode: Modal Actuations for Soft Body Locomotion",
        "authors": [
            "Otman Benchekroun",
            "Kaixiang Xie",
            "Hsueh-Ti Derek Liu",
            "Eitan Grinspun",
            "Sheldon Andrews",
            "Victor Zordan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Traditional character animation specializes in characters with a rigidly\narticulated skeleton and a bipedal/quadripedal morphology. This assumption\nsimplifies many aspects for designing physically based animations, like\nlocomotion, but comes with the price of excluding characters of arbitrary\ndeformable geometries. To remedy this, our framework makes use of a\nspatio-temporal actuation subspace built off of the natural vibration modes of\nthe character geometry. The resulting actuation is coupled to a reduced fast\nsoft body simulation, allowing us to formulate a locomotion optimization\nproblem that is tractable for a wide variety of high resolution deformable\ncharacters.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.18609v1"
    },
    {
        "title": "Neural Scene Baking for Permutation Invariant Transparency Rendering\n  with Real-time Global Illumination",
        "authors": [
            "Ziyang Zhang",
            "Edgar Simo-Serra"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Neural rendering provides a fundamentally new way to render photorealistic\nimages. Similar to traditional light-baking methods, neural rendering utilizes\nneural networks to bake representations of scenes, materials, and lights into\nlatent vectors learned from path-tracing ground truths. However, existing\nneural rendering algorithms typically use G-buffers to provide position,\nnormal, and texture information of scenes, which are prone to occlusion by\ntransparent surfaces, leading to distortions and loss of detail in the rendered\nimages. To address this limitation, we propose a novel neural rendering\npipeline that accurately renders the scene behind transparent surfaces with\nglobal illumination and variable scenes. Our method separates the G-buffers of\nopaque and transparent objects, retaining G-buffer information behind\ntransparent objects. Additionally, to render the transparent objects with\npermutation invariance, we designed a new permutation-invariant neural blending\nfunction. We integrate our algorithm into an efficient custom renderer to\nachieve real-time performance. Our results show that our method is capable of\nrendering photorealistic images with variable scenes and viewpoints, accurately\ncapturing complex transparent structures along with global illumination. Our\nrenderer can achieve real-time performance ($256\\times 256$ at 63 FPS and\n$512\\times 512$ at 32 FPS) on scenes with multiple variable transparent\nobjects.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.19056v1"
    },
    {
        "title": "Dress Anyone : Automatic Physically-Based Garment Pattern Refitting",
        "authors": [
            "Hsiao-yu Chen",
            "Egor Larionov",
            "Ladislav Kavan",
            "Gene Lin",
            "Doug Roble",
            "Olga Sorkine-Hornung",
            "Tuur Stuyck"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Well-fitted clothing is essential for both real and virtual garments to\nenable self-expression and accurate representation for a large variety of body\ntypes. Common practice in the industry is to provide a pre-made selection of\ndistinct garment sizes such as small, medium and large. While these may cater\nto certain groups of individuals that fall within this distribution, they often\nexclude large sections of the population. In contrast, individually tailored\nclothing offers a solution to obtain custom-fit garments that are tailored to\neach individual. However, manual tailoring is time-consuming and requires\nspecialized knowledge, prohibiting the approach from being applied to produce\nfitted clothing at scale. To address this challenge, we propose a novel method\nleveraging differentiable simulation for refitting and draping 3D garments and\ntheir corresponding 2D pattern panels onto a new body shape, enabling a\nworkflow where garments only need to be designed once, in a single size, and\nthey can be automatically refitted to support numerous body size and shape\nvariations. Our method enables downstream applications, where our optimized 3D\ndrape can be directly ingested into game engines or other applications. Our 2D\nsewing patterns allow for accurate physics-based simulations and enables\nmanufacturing clothing for the real world.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.19148v1"
    },
    {
        "title": "Data-Efficient Discovery of Hyperelastic TPMS Metamaterials with Extreme\n  Energy Dissipation",
        "authors": [
            "Maxine Perroni-Scharf",
            "Zachary Ferguson",
            "Thomas Butrille",
            "Carlos Portela",
            "Mina Konaković Luković"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Triply periodic minimal surfaces (TPMS) are a class of metamaterials with a\nvariety of applications and well-known primitives. We present a new method for\ndiscovering novel microscale TPMS structures with exceptional\nenergy-dissipation capabilities, achieving double the energy absorption of the\nbest existing TPMS primitive structure. Our approach employs a parametric\nrepresentation, allowing seamless interpolation between structures and\nrepresenting a rich TPMS design space. We show that simulations are intractable\nfor optimizing microscale hyperelastic structures, and instead propose a\nsample-efficient computational strategy for rapidly discovering structures with\nextreme energy dissipation using limited amounts of empirical data from\n3D-printed and tested microscale metamaterials. This strategy ensures\nhigh-fidelity results but involves time-consuming 3D printing and testing. To\naddress this, we leverage an uncertainty-aware Deep Ensembles model to predict\nmicrostructure behaviors and identify which structures to 3D-print and test\nnext. We iteratively refine our model through batch Bayesian optimization,\nselecting structures for fabrication that maximize exploration of the\nperformance space and exploitation of our energy-dissipation objective. Using\nour method, we produce the first open-source dataset of hyperelastic microscale\nTPMS structures, including a set of novel structures that demonstrate extreme\nenergy dissipation capabilities. We show several potential applications of\nthese structures in protective equipment and bone implants.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.19507v1"
    },
    {
        "title": "Creating Language-driven Spatial Variations of Icon Images",
        "authors": [
            "Xianghao Xu",
            "Aditya Ganeshan",
            "Karl D. D. Willis",
            "Yewen Pu",
            "Daniel Ritchie"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Editing 2D icon images can require significant manual effort from designers.\nIt involves manipulating multiple geometries while maintaining the logical or\nphysical coherence of the objects depicted in the image. Previous language\ndriven image editing methods can change the texture and geometry of objects in\nthe image but fail at producing spatial variations, i.e. modifying spatial\nrelations between objects while maintaining their identities. We present a\nlanguage driven editing method that can produce spatial variations of icon\nimages. Our method takes in an icon image along with a user's editing request\ntext prompt and outputs an edited icon image reflecting the user's editing\nrequest. Our method is designed based on two key observations: (1) A user's\nediting requests can be translated by a large language model (LLM), with help\nfrom a domain specific language (DSL) library, into to a set of geometrical\nconstraints defining the relationships between segments in an icon image. (2)\nOptimizing the affine transformations of the segments with respect to these\ngeometrical constraints can produce icon images that fulfill the editing\nrequest and preserve overall physical and logical coherence. Quantitative and\nqualitative results show that our system outperforms multiple baselines,\nenabling natural editing of icon images.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.19636v1"
    },
    {
        "title": "Topology-Aware Blending Method for Implicit Heterogeneous Porous Model\n  Design",
        "authors": [
            "Depeng Gao",
            "Yang Gao",
            "Yuanzhi Zhang",
            "Hongwei Lin"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Porous structures are materials consisting of minuscule pores, where the\nmicrostructure morphology significantly impacts their macroscopic properties.\n  Integrating different porous structures through a blending method is\nindispensable to cater to diverse functional regions in heterogeneous models.\n  Previous studies on blending methods for porous structures have mainly\nfocused on controlling the shape of blending regions, yet they have fallen\nshort in effectively addressing topological errors in blended structures.\n  This paper introduces a new blending method that successfully addresses this\nissue.\n  Initially, a novel initialization method is proposed, which includes distinct\nstrategies for blending regions of varying complexities.\n  Subsequently, we formulate the challenge of eliminating topological errors as\nan optimization problem based on persistent homology.\n  Through iterative updates of control coefficients, this optimization problem\nis solved to generate a blended porous structure.\n  Our approach not only avoids topological errors but also governs the shape\nand positioning of the blending region while remaining unchanged in the\nstructure outside blending region.\n  The experimental outcomes validate the effectiveness of our method in\nproducing high-quality blended porous structures.\n  Furthermore, these results highlight potential applications of our blending\nmethod in biomimetics and the design of high-stiffness mechanical heterogeneous\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.20580v1"
    },
    {
        "title": "Report on Methods and Applications for Crafting 3D Humans",
        "authors": [
            "Lei Liu",
            "Ke Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper presents an in-depth exploration of 3D human model and avatar\ngeneration technology, propelled by the rapid advancements in large-scale\nmodels and artificial intelligence. The paper reviews the comprehensive process\nof 3D human model generation, from scanning to rendering, and highlights the\npivotal role these models play in entertainment, VR, AR, healthcare, and\neducation. We underscore the significance of diffusion models in generating\nhigh-fidelity images and videos. It emphasizes the indispensable nature of 3D\nhuman models in enhancing user experiences and functionalities across various\nfields. Furthermore, this paper anticipates the potential of integrating\nlarge-scale models with deep learning to revolutionize 3D content generation,\noffering insights into the future prospects of the technology. It concludes by\nemphasizing the importance of continuous innovation in the field, suggesting\nthat ongoing advancements will significantly expand the capabilities and\napplications of 3D human models and avatars.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.01223v1"
    },
    {
        "title": "QuickCurve: revisiting slightly non-planar 3D printing",
        "authors": [
            "Emilio Ottonello",
            "Pierre-Alexandre Hugron",
            "Alberto Parmiggiani",
            "Sylvain Lefebvre"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Additive manufacturing builds physical objects by accumulating layers upon\nlayers of solidified material. This process is typically done with horizontal\nplanar layers. However, fused filament printers have the capability to extrude\nmaterial along 3D curves. The idea of depositing out-of-plane, also known as\nnon-planar printing, has spawned a trend of research towards algorithms that\ncould generate non-planar deposition paths automatically from a 3D object. In\nthis paper we introduce a novel algorithm for this purpose. Our method\noptimizes for a curved slicing surface. This surface is intersected with the\ninput model to extract non-planar layers, with the objective of accurately\nreproducing the model top surfaces while avoiding collisions. Our formulation\nleads to a simple and efficient approach that only requires solving for a\nsingle least-square problem. Notably, it does not require a tetrahedralization\nof the input or iterative solver passes, while being more general than simpler\napproaches. We further explore how to orient the paths to follow the principal\ncurvatures of the surfaces, how to filter spurious tiny features damaging the\nresults, and how to achieve a good compromise of mixing planar and non-planar\nstrategies within the same part. We present a complete formulation and its\nimplementation, and demonstrate our method on a variety of 3D printed models.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.03966v1"
    },
    {
        "title": "A Versatile Collage Visualization Technique",
        "authors": [
            "Zhenyu Wang",
            "Min Lu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Collage techniques are commonly used in visualization to organize a\ncollection of geometric shapes, facilitating the representation of visual\nfeatures holistically, as seen in word clouds or circular packing diagrams.\nTypically, packing methods rely on object-space optimization techniques, which\noften necessitate customizing the optimization process to suit the complexity\nof geometric primitives and the specific application requirements. In this\npaper, we introduce a versatile image-space collage technique designed to pack\ngeometric elements into a given shape. Leveraging a differential renderer and\nimage-space losses, our optimization process is highly efficient and can easily\naccommodate various loss functions. We demonstrate the diverse visual\nexpressiveness of our approach across various visualization applications. The\nevaluation confirmed the benefits of our method in terms of both visual quality\nand time performance. The project page is\nhttps://szuviz.github.io/pixel-space-collage-technique/.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.04008v2"
    },
    {
        "title": "DHR+S: Distributed Hybrid Rendering with Realistic Real-time Shadows for\n  Interactive Thin Client Metaverse and Game Applications",
        "authors": [
            "Yu Wei Tan",
            "Siang Ern Low",
            "Jonas Chow",
            "Javon Teo",
            "Anand Bhojan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Distributed hybrid rendering (DHR) is a real-time rendering approach that\nincorporates cloud-based ray tracing with locally rasterized graphics for\ninteractive thin client metaverse and game applications. With cloud assistance,\nDHR can generate high-fidelity ray-traced graphics contents remotely and\ndeliver them to thin clients with low graphics capability, including standalone\nextended reality devices and mobile phones, while maintaining interactive frame\nrates for users under adverse network conditions. DHR can already achieve the\neffect of ray-traced hard shadows that form with the occlusion of direct\nillumination. We enhance the realism of these shadows by softening their edges\nwith the direction of rays traced and approximating the occlusion of indirect\nillumination by reconstructing ray-traced ambient occlusion with a modified\nversion of spatiotemporal variance-guided filtering. Our technique uses only\n20-30% of the bandwidth of remote rendering and is also tolerant of delays of\nup to 200 ms with only slight distortion to the shadows along object edges.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.06963v1"
    },
    {
        "title": "Hybrid Rendering for Dynamic Scenes",
        "authors": [
            "Alexandr Kuznetsov",
            "Stavros Diolatzis",
            "Anton Sochenov",
            "Anton Kaplanyan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Despite significant advances in algorithms and hardware, global illumination\ncontinues to be a challenge in the real-time domain. Time constraints often\nforce developers to either compromise on the quality of global illumination or\ndisregard it altogether. We take advantage of a common setup in modern games:\nhaving a set of a level, which is a static scene with dynamic characters and\nlighting. We introduce a novel method for efficiently and accurately rendering\nglobal illumination in dynamic scenes. Our hybrid technique leverages\nprecomputation and neural networks to capture the light transport of a static\nscene. Then, we introduce a method to compute the difference between the\ncurrent scene and the static scene, which we already precomputed. By handling\nthe bulk of the light transport through precomputation, our method only\nrequires the rendering of a minimal difference, reducing the noise and\nincreasing the quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.07906v1"
    },
    {
        "title": "Towards Accelerating Real-Time Path Tracing with Foveated Framework",
        "authors": [
            "Bipul Mohanto",
            "Sven Kluge",
            "Oliver Staadt"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Path tracing is one of the most widespread rendering techniques for high-end\ngraphics fidelity. However, the slow convergence time and presence of intensive\nnoises make it infeasible for numerous real-time applications where physically\ncorrected photorealistic effects are salient. Additionally, the increased\ndemand for pixel density, geometric complexity, advanced material, and multiple\nlights hinder the algorithm from attaining an interactive frame rate for\nreal-time applications. To address these issues, we developed a framework to\naccelerate path tracing through foveated rendering, a robust technique that\nleverages human vision. Our dynamic foveated path-tracing framework integrates\nfixation data and selectively lowers the rendering resolution towards the\nperiphery. The framework is built on NVIDIA's OptiX 7.5 API with CUDA 12.1,\nserving as the base of future foveated path tracing research. Through\ncomprehensive experimentation, we demonstrated the effectiveness of our\nframework in this paper. Depending on the scene complexity, our solution can\nsignificantly enhance rendering performance up to a factor of 25 without any\nnotable visual differences. We further evaluated the framework using a\nstructured error map algorithm with variable sample numbers and foveated area\nsize.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.07981v2"
    },
    {
        "title": "FSH3D: 3D Representation via Fibonacci Spherical Harmonics",
        "authors": [
            "Zikuan Li",
            "Anyi Huang",
            "Wenru Jia",
            "Qiaoyun Wu",
            "Mingqiang Wei",
            "Jun Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Spherical harmonics are a favorable technique for 3D representation,\nemploying a frequency-based approach through the spherical harmonic transform\n(SHT). Typically, SHT is performed using equiangular sampling grids. However,\nthese grids are non-uniform on spherical surfaces and exhibit local anisotropy,\na common limitation in existing spherical harmonic decomposition methods. This\npaper proposes a 3D representation method using Fibonacci Spherical Harmonics\n(FSH3D). We introduce a spherical Fibonacci grid (SFG), which is more uniform\nthan equiangular grids for SHT in the frequency domain. Our method employs\nanalytical weights for SHT on SFG, effectively assigning sampling errors to\nspherical harmonic degrees higher than the recovered band-limited function.\nThis provides a novel solution for spherical harmonic transformation on\nnon-equiangular grids. The key advantages of our FSH3D method include: 1) With\nthe same number of sampling points, SFG captures more features without bias\ncompared to equiangular grids; 2) The root mean square error of 32-degree\nspherical harmonic coefficients is reduced by approximately 34.6% for SFG\ncompared to equiangular grids; and 3) FSH3D offers more stable frequency domain\nrepresentations, especially for rotating functions. FSH3D enhances the\nstability of frequency domain representations under rotational transformations.\nIts application in 3D shape reconstruction and 3D shape classification results\nin more accurate and robust representations. Our code is publicly available at\nhttps://github.com/Miraclelzk/Fibonacci-Spherical-Harmonics.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.08308v2"
    },
    {
        "title": "Optimized Dual-Volumes for Tetrahedral Meshes",
        "authors": [
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Constructing well-behaved Laplacian and mass matrices is essential for\ntetrahedral mesh processing. Unfortunately, the \\emph{de facto} standard linear\nfinite elements exhibit bias on tetrahedralized regular grids, motivating the\ndevelopment of finite-volume methods. In this paper, we place existing methods\ninto a common construction, showing how their differences amount to the choice\nof simplex centers. These choices lead to satisfaction or breakdown of\nimportant properties: continuity with respect to vertex positions, positive\nsemi-definiteness of the implied Dirichlet energy, positivity of the mass\nmatrix, and unbiased-ness on regular grids. Based on this analysis, we propose\na new method for constructing dual-volumes which explicitly satisfy all of\nthese properties via convex optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.08647v1"
    },
    {
        "title": "Learnable Fractal Flames",
        "authors": [
            "Jordan J. Bannister",
            "Derek Nowrouzezahrai"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This work presents a differentiable rendering approach that allows latent\nfractal flame parameters to be learned from image supervision using gradient\ndescent optimization. The approach extends the state-of-the-art in\ndifferentiable iterated function system fractal rendering through support for\ncolor images, non-linear generator functions, and multi-fractal compositions.\nWith this approach, artists can use reference images to quickly and intuitively\ncontrol the creation of fractals. We describe the approach and conduct a series\nof experiments exploring its use, culminating in the creation of complex and\ncolorful fractal artwork based on famous paintings.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.09328v2"
    },
    {
        "title": "Unified Gaussian Primitives for Scene Representation and Rendering",
        "authors": [
            "Yang Zhou",
            "Songyin Wu",
            "Ling-Qi Yan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Searching for a unified scene representation remains a research challenge in\ncomputer graphics. Traditional mesh-based representations are unsuitable for\ndense, fuzzy elements, and introduce additional complexity for filtering and\ndifferentiable rendering. Conversely, voxel-based representations struggle to\nmodel hard surfaces and suffer from intensive memory requirement. We propose a\ngeneral-purpose rendering primitive based on 3D Gaussian distribution for\nunified scene representation, featuring versatile appearance ranging from\nglossy surfaces to fuzzy elements, as well as physically based scattering to\nenable accurate global illumination. We formulate the rendering theory for the\nprimitive based on non-exponential transport and derive efficient rendering\noperations to be compatible with Monte Carlo path tracing. The new\nrepresentation can be converted from different sources, including meshes and 3D\nGaussian splatting, and further refined via transmittance optimization thanks\nto its differentiability. We demonstrate the versatility of our representation\nin various rendering applications such as global illumination and appearance\nediting, while supporting arbitrary lighting conditions by nature.\nAdditionally, we compare our representation to existing volumetric\nrepresentations, highlighting its efficiency to reproduce details.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.09733v2"
    },
    {
        "title": "Implementing a Machine Learning Deformer for CG Crowds: Our Journey",
        "authors": [
            "Bastien Arcelin",
            "Sebastien Maraux",
            "Nicolas Chaverou"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  CG crowds have become increasingly popular this last decade in the VFX and\nanimation industry: formerly reserved to only a few high end studios and\nblockbusters, they are now widely used in TV shows or commercials. Yet, there\nis still one major limitation: in order to be ingested properly in crowd\nsoftware, studio rigs have to comply with specific prerequisites, especially in\nterms of deformations. Usually only skinning, blend shapes and geometry caches\nare supported preventing close-up shots with facial performances on crowd\ncharacters. We envisioned two approaches to tackle this: either reverse\nengineer the hundreds of deformer nodes available in the major DCCs/plugins and\nincorporate them in our crowd package, or surf the machine learning wave to\ncompress the deformations of a rig using a neural network architecture.\nConsidering we could not commit 5+ man/years of development into this problem,\nand that we were excited to dip our toes in the machine learning pool, we went\nfor the latter.\n  From our first tests to a minimum viable product, we went through hopes and\ndisappointments: we hit multiple pitfalls, took false shortcuts and dead ends\nbefore reaching our destination. With this paper, we hope to provide a valuable\nfeedback by sharing the lessons we learnt from this experience.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.09783v1"
    },
    {
        "title": "Cascading upper bounds for triangle soup Pompeiu-Hausdorff distance",
        "authors": [
            "Leonardo Sacht",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a new method to accurately approximate the Pompeiu-Hausdorff\ndistance from a triangle soup A to another triangle soup B up to a given\ntolerance. Based on lower and upper bound computations, we discard triangles\nfrom A that do not contain the maximizer of the distance to B and subdivide the\nothers for further processing. In contrast to previous methods, we use four\nupper bounds instead of only one, three of which newly proposed by us. Many\ntriangles are discarded using the simpler bounds, while the most difficult\ncases are dealt with by the other bounds. Exhaustive testing determines the\nbest ordering of the four upper bounds. A collection of experiments shows that\nour method is faster than all previous accurate methods in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.10357v1"
    },
    {
        "title": "Two-point Equidistant Projection and Degree-of-interest Filtering for\n  Smooth Exploration of Geo-referenced Networks",
        "authors": [
            "Max Franke",
            "Samuel Beck",
            "Steffen Koch"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The visualization and interactive exploration of geo-referenced networks\nposes challenges if the network's nodes are not evenly distributed. Our\napproach proposes new ways of realizing animated transitions for exploring such\nnetworks from an ego-perspective. We aim to reduce the required screen estate\nwhile maintaining the viewers' mental map of distances and directions. A\npreliminary study provides first insights of the comprehensiveness of animated\ngeographic transitions regarding directional relationships between start and\nend point in different projections. Two use cases showcase how ego-perspective\ngraph exploration can be supported using less screen space than previous\napproaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.11493v1"
    },
    {
        "title": "GA-Unity: A Production-Ready Unity Package for Seamless Integration of\n  Geometric Algebra in Networked Collaborative Applications",
        "authors": [
            "Manos Kamarianakis",
            "Nick Lydatakis",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper introduces GA-Unity, the first Unity package specifically designed\nfor seamless integration of Geometric Algebra (GA) into collaborative networked\napplications. Indeed, in such contexts, it has been demonstrated that using\nmultivectors for interpolation between transmitted poses reduces runtime by 16%\nand bandwidth usage by an average of 50% compared to traditional representation\nforms (vectors and quaternions); we demonstrate that GA-Unity further enhances\nruntime performance. Tailored for 3D Conformal Geometric Algebra, GA-Unity also\noffers an intuitive interface within the Unity game engine, simplifying GA\nintegration for researchers and programmers. By eliminating the need for users\nto develop GA functionalities from scratch, GA-Unity expedites GA\nexperimentation and implementation processes. Its seamless integration enables\neasy representation of transformation properties using multivectors,\nfacilitating deformations and interpolations without necessitating\nmodifications to the rendering pipeline. Furthermore, its graphical interface\nestablishes a GA playground for developers within the familiar confines of a\nmodern game engine. In summary, GA-Unity represents a significant advancement\nin GA accessibility and usability, particularly in collaborative networked\nenvironments, empowering innovation and facilitating widespread adoption across\nvarious research and programming domains while upholding high-performance\nstandards.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.11560v2"
    },
    {
        "title": "Projecting Radiance Fields to Mesh Surfaces",
        "authors": [
            "Adrian Xuan Wei Lim",
            "Lynnette Hui Xian Ng",
            "Nicholas Kyger",
            "Tomo Michigami",
            "Faraz Baghernezhad"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Radiance fields produce high fidelity images with high rendering speed, but\nare difficult to manipulate. We effectively perform avatar texture transfer\nacross different appearances by combining benefits from radiance fields and\nmesh surfaces. We represent the source as a radiance field using 3D Gaussian\nSplatter, then project the Gaussians on the target mesh. Our pipeline consists\nof Source Preconditioning, Target Vectorization and Texture Projection. The\nprojection completes in 1.12s in a pure CPU compute, compared to baselines\ntechniques of Per Face Texture Projection and Ray Casting (31s, 4.1min). This\nmethod lowers the computational requirements, which makes it applicable to a\nbroader range of devices from low-end mobiles to high end computers.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.11570v1"
    },
    {
        "title": "Compressed Skinning for Facial Blendshapes",
        "authors": [
            "Ladislav Kavan",
            "John Doublestein",
            "Martin Prazak",
            "Matthew Cioffi",
            "Doug Roble"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a new method to bake classical facial animation blendshapes into a\nfast linear blend skinning representation. Previous work explored skinning\ndecomposition methods that approximate general animated meshes using a dense\nset of bone transformations; these optimizers typically alternate between\noptimizing for the bone transformations and the skinning weights.We depart from\nthis alternating scheme and propose a new approach based on proximal\nalgorithms, which effectively means adding a projection step to the popular\nAdam optimizer. This approach is very flexible and allows us to quickly\nexperiment with various additional constraints and/or loss functions.\nSpecifically, we depart from the classical skinning paradigms and restrict the\ntransformation coefficients to contain only about 10% non-zeros, while\nachieving similar accuracy and visual quality as the state-of-the-art. The\nsparse storage enables our method to deliver significant savings in terms of\nboth memory and run-time speed. We include a compact implementation of our new\nskinning decomposition method in PyTorch, which is easy to experiment with and\nmodify to related problems.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.11597v2"
    },
    {
        "title": "Hoop Diagrams: A Set Visualization Method",
        "authors": [
            "Peter Rodgers",
            "Peter Chapman",
            "Andrew Blake",
            "Martin Nöllenburg",
            "Markus Wallinger",
            "Alexander Dobler"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce Hoop Diagrams, a new visualization technique for set data. Hoop\nDiagrams are a circular visualization with hoops representing sets and sectors\nrepresenting set intersections. We present an interactive tool for drawing Hoop\nDiagrams and describe a user study comparing them with Linear Diagrams. The\nresults show only small differences, with users answering questions more\nquickly with Linear Diagrams, but answering some questions more accurately with\nHoop Diagrams. Interaction data indicates that those using set order and\nintersection highlighting were more successful at answering questions, but\nthose who used other interactions had a slower response. The similarity in\nusability suggests that the diagram type should be chosen based on the\npresentation method. Linear Diagrams increase in the horizontal direction with\nthe number of intersections, leading to difficulties fitting on a screen. Hoop\nDiagrams always have a square aspect ratio.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.13260v2"
    },
    {
        "title": "Text-based Transfer Function Design for Semantic Volume Rendering",
        "authors": [
            "Sangwon Jeong",
            "Jixian Li",
            "Christopher Johnson",
            "Shusen Liu",
            "Matthew Berger"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Transfer function design is crucial in volume rendering, as it directly\ninfluences the visual representation and interpretation of volumetric data.\nHowever, creating effective transfer functions that align with users' visual\nobjectives is often challenging due to the complex parameter space and the\nsemantic gap between transfer function values and features of interest within\nthe volume. In this work, we propose a novel approach that leverages recent\nadvancements in language-vision models to bridge this semantic gap. By\nemploying a fully differentiable rendering pipeline and an image-based loss\nfunction guided by language descriptions, our method generates transfer\nfunctions that yield volume-rendered images closely matching the user's intent.\nWe demonstrate the effectiveness of our approach in creating meaningful\ntransfer functions from simple descriptions, empowering users to intuitively\nexpress their desired visual outcomes with minimal effort. This advancement\nstreamlines the transfer function design process and makes volume rendering\nmore accessible to a wider range of users.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.15634v1"
    },
    {
        "title": "Residual path integrals for re-rendering",
        "authors": [
            "Bing Xu",
            "Tzu-Mao Li",
            "Iliyan Georgiev",
            "Trevor Hedstrom",
            "Ravi Ramamoorthi"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Conventional rendering techniques are primarily designed and optimized for\nsingle-frame rendering. In practical applications, such as scene editing and\nanimation rendering, users frequently encounter scenes where only a small\nportion is modified between consecutive frames. In this paper, we develop a\nnovel approach to incremental re-rendering of scenes with dynamic objects,\nwhere only a small part of a scene moves from one frame to the next. We\nformulate the difference (or residual) in the image between two frames as a\n(correlated) light-transport integral which we call the residual path integral.\nEfficient numerical solution of this integral then involves (1)~devising\nimportance sampling strategies to focus on paths with non-zero\nresidual-transport contributions and (2)~choosing appropriate mappings between\nthe native path spaces of the two frames. We introduce a set of path importance\nsampling strategies that trace from the moving object(s) which are the sources\nof residual energy. We explore path mapping strategies that generalize those\nfrom gradient-domain path tracing to our importance sampling techniques\nspecially for dynamic scenes. Additionally, our formulation can be applied to\nmaterial editing as a simpler special case. We demonstrate speed-ups over\nprevious correlated sampling of path differences and over rendering the new\nframe independently. Our formulation brings new insights into the re-rendering\nproblem and paves the way for devising new types of sampling techniques and\npath mappings with different trade-offs.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.16302v1"
    },
    {
        "title": "Non-Orthogonal Reduction for Rendering Fluorescent Materials in\n  Non-Spectral Engines",
        "authors": [
            "Alban Fichet",
            "Laurent Belcour",
            "Pascal Barla"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a method to accurately handle fluorescence in a non-spectral (\\eg,\ntristimulus) rendering engine, showcasing color-shifting and increased\nluminance effects. Core to our method is a principled reduction technique that\nencodes the re-radiation into a low-dimensional matrix working in the space of\nthe renderer's Color Matching Functions (CMFs). Our process is independent of a\nspecific CMF set and allows for the addition of a non-visible ultraviolet band\nduring light transport. Our representation visually matches full spectral light\ntransport for measured fluorescent materials even for challenging illuminants.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.17360v2"
    },
    {
        "title": "Standardized Data-Parallel Rendering Using ANARI",
        "authors": [
            "Ingo Wald",
            "Stefan Zellmann",
            "Jefferson Amstutz",
            "Qi Wu",
            "Kevin Griffin",
            "Milan Jaros",
            "Stefan Wesner"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose and discuss a paradigm that allows for expressing\n\\emph{data-parallel} rendering with the classically non-parallel ANARI API. We\npropose this as a new standard for data-parallel sci-vis rendering, describe\ntwo different implementations of this paradigm, and use multiple sample\nintegrations into existing apps to show how easy it is to adopt this paradigm,\nand what can be gained from doing so.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.00179v1"
    },
    {
        "title": "MetaSapiens: Real-Time Neural Rendering with Efficiency-Aware Pruning\n  and Accelerated Foveated Rendering",
        "authors": [
            "Weikai Lin",
            "Yu Feng",
            "Yuhao Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Point-Based Neural Rendering (PBNR) is emerging as a promising class of\nrendering techniques, which are permeating all aspects of society, driven by a\ngrowing demand for real-time, photorealistic rendering in AR/VR and digital\ntwins. Achieving real-time PBNR on mobile devices is challenging.\n  This paper proposes MetaSapiens, a PBNR system that for the first time\ndelivers real-time neural rendering on mobile devices while maintaining human\nvisual quality. MetaSapiens combines three techniques. First, we present an\nefficiency-aware pruning technique to optimize rendering speed. Second, we\nintroduce a Foveated Rendering (FR) method for PBNR, leveraging humans' low\nvisual acuity in peripheral regions to relax rendering quality and improve\nrendering speed. Finally, we propose an accelerator design for FR, addressing\nthe load imbalance issue in (FR-based) PBNR. Our evaluation shows that our\nsystem achieves an order of magnitude speedup over existing PBNR models without\nsacrificing subjective visual quality, as confirmed by a user study. The code\nand demo are available at: https://horizon-lab.org/metasapiens/.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.00435v3"
    },
    {
        "title": "Concurrent Binary Trees for Large-Scale Game Components",
        "authors": [
            "Anis Benyoub",
            "Jonathan Dupuy"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  A concurrent binary tree (CBT) is a GPU-friendly data-structure suitable for\nthe generation of bisection based terrain tessellations, i.e., adaptive\ntriangulations over square domains. In this paper, we expand the benefits of\nthis data-structure in two respects. First, we show how to bring bisection\nbased tessellations to arbitrary polygon meshes rather than just squares. Our\napproach consists of mapping a triangular subdivision primitive, which we refer\nto as a bisector, to each halfedge of the input mesh. These bisectors can then\nbe subdivided adaptively to produce conforming triangulations solely based on\nhalfedge operators. Second, we alleviate a limitation that restricted the\ntriangulations to low subdivision levels. We do so by using the CBT as a memory\npool manager rather than an implicit encoding of the triangulation as done\noriginally. By using a CBT in this way, we concurrently allocate and/or release\nbisectors during adaptive subdivision using shared GPU memory. We demonstrate\nthe benefits of our improvements by rendering planetary scale geometry out of\nvery coarse meshes. Performance-wise, our triangulation method evaluates in\nless than 0.2ms on console-level hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.02215v1"
    },
    {
        "title": "A Physics Oriented Mathematical Perspective for Creating Trochoids and\n  Co-Centered Ellipses Based on Controlled Combination of Rolling and Sliding\n  Motions",
        "authors": [
            "H. Arbab",
            "A. Arbab"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The mathematical perspective for creating trochoids [through a solid rule\nthat is based on the pure rolling a circle along a straight line or another\ncircle (centered trochoid)] is violated and changed it to a novel vision which\nis based on the combination of rolling and sliding motions of a circle along a\nstraight line or another circle! In this new vision we have not to define a\ntrochoid as a path that is swept by an attached point to a pure rolling circle\nalong a straight line or another circle. Instead, a trochoid can be defined as\na path that is swept by a definite point on the circumference of a rolling and\nsliding circle along a straight line or another circle! In this article we\npresent two different methods for implement a definite combination of sliding\nand rolling motions for a circle along another one in order to make a simple\nexperimental simulation to create centered trochoids and co-centered ellipses.\nIn our novel mathematical vision not only, the physical concepts are playing\nbasic role but also one can deduce the parametric equations of trochoids and\nellipses on the bases of rolling and sliding motions. In this perspective, an\nellipse can be visualized as a closed plane curve that can be generated through\na definite combination of rolling and sliding motions due to two co-polarized\nrotational motions with different commensurable angular frequencies! Centered\ntrochoids and ellipses can be implemented with the help of Virtual Rotating\nCircles Technique (VRCT) by an innovative device that we have named it\nMechanical Oscilloscope (MO) and other trochoids by a Virtual Sliding Simulator\n(VSS). Although the functions of our devices are independent from other\nelectronic equipment, but we can create trochoids on the bases of functional\noperation of (VSS) and (MO) with the help of computer.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.06966v9"
    },
    {
        "title": "A Novel Skiagraphic Method of Casting Shade of a Torus",
        "authors": [
            "Tanvir Morshed"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper introduces a novel skiagraphic method for shading toroidal forms\nin architectural illustrations, addressing the challenges of traditional\ntechniques. Skiagraphy projects 3D objects onto 2D surfaces to display\ngeometric properties. Traditional shading of tori involves extensive manual\ncalculations and multiple projections, leading to high complexity and\ninaccuracies. The proposed method simplifies this by focusing on the elevation\nview, eliminating the need for multiple projections and complex math. Utilizing\ndescriptive geometry, it reduces labor and complexity. Accuracy was validated\nthrough comparisons with SketchUp-generated shading and various torus\nconfigurations. This technique streamlines shading toroidal shapes while\nmaintaining the artistic value of traditional illustration. Additionally, it\nhas potential applications in 3D model generation from architectural shade\ncasts, contributing to the evolving field of architectural visualization and\nrepresentation.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.14557v1"
    },
    {
        "title": "EUFormer: Learning Driven 3D Spine Deformity Assessment with Orthogonal\n  Optical Images",
        "authors": [
            "Nan Meng",
            "Jason P. Y. Cheung",
            "Tao Huang",
            "Moxin Zhao",
            "Yue Zhang",
            "Chenxi Yu",
            "Chang Shi",
            "Teng Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In clinical settings, the screening, diagnosis, and monitoring of adolescent\nidiopathic scoliosis (AIS) typically involve physical or radiographic\nexaminations. However, physical examinations are subjective, while radiographic\nexaminations expose patients to harmful radiation. Consequently, we propose a\npipeline that can accurately determine scoliosis severity. This pipeline\nutilizes posteroanterior (PA) and lateral (LAT) RGB images as input to generate\nspine curve maps, which are then used to reconstruct the three-dimensional (3D)\nspine curve for AIS severity grading. To generate the 2D spine curves\naccurately and efficiently, we further propose an Efficient U-shape transFormer\n(EUFormer) as the generator. It can efficiently utilize the learned feature\nacross channels, therefore producing consecutive spine curves from both PA and\nLAT views. Experimental results demonstrate superior performance of EUFormer on\nspine curve generation against other classical U-shape models. This finding\ndemonstrates that the proposed method for grading the severity of AIS, based on\na 3D spine curve, is more accurate when compared to using a 2D spine curve.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.16942v1"
    },
    {
        "title": "The impact of differences in facial features between real speakers and\n  3D face models on synthesized lip motions",
        "authors": [
            "Rabab Algadhy",
            "Yoshihiko Gotoh",
            "Steve Maddock"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Lip motion accuracy is important for speech intelligibility, especially for\nusers who are hard of hearing or second language learners. A high level of\nrealism in lip movements is also required for the game and film production\nindustries. 3D morphable models (3DMMs) have been widely used for facial\nanalysis and animation. However, factors that could influence their use in\nfacial animation, such as the differences in facial features between recorded\nreal faces and animated synthetic faces, have not been given adequate\nattention. This paper investigates the mapping between real speakers and\nsimilar and non-similar 3DMMs and the impact on the resulting 3D lip motion.\nMouth height and mouth width are used to determine face similarity. The results\nshow that mapping 2D videos of real speakers with low mouth heights to 3D heads\nthat correspond to real speakers with high mouth heights, or vice versa,\ngenerates less good 3D lip motion. It is thus important that such a mismatch is\nconsidered when using a 2D recording of a real actor's lip movements to control\na 3D synthetic character.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.17253v1"
    },
    {
        "title": "Drawing ellipses and elliptical arcs with piecewise cubic Bézier curve\n  approximations",
        "authors": [
            "Jerry R. Van Aken"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This tutorial explains how to use piecewise cubic B\\'ezier curves to draw\narbitrarily oriented ellipses and elliptical arcs. The geometric principles\ndiscussed here result in strikingly simple interfaces for graphics functions\nthat can draw (approximate) circles, ellipses, and arcs of circles and\nellipses. C++ source code listings are included for these functions. Their code\nsize can be relatively small because they are designed to be used with a\ngraphics library or platform that draws B\\'ezier curves, and the library or\nplatform is tasked with the actual rendering of the curves.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.17675v2"
    },
    {
        "title": "Uncertainty Visualization of Critical Points of 2D Scalar Fields for\n  Parametric and Nonparametric Probabilistic Models",
        "authors": [
            "Tushar M. Athawale",
            "Zhe Wang",
            "David Pugmire",
            "Kenneth Moreland",
            "Qian Gong",
            "Scott Klasky",
            "Chris R. Johnson",
            "Paul Rosen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper presents a novel end-to-end framework for closed-form computation\nand visualization of critical point uncertainty in 2D uncertain scalar fields.\nCritical points are fundamental topological descriptors used in the\nvisualization and analysis of scalar fields. The uncertainty inherent in data\n(e.g., observational and experimental data, approximations in simulations, and\ncompression), however, creates uncertainty regarding critical point positions.\nUncertainty in critical point positions, therefore, cannot be ignored, given\ntheir impact on downstream data analysis tasks. In this work, we study\nuncertainty in critical points as a function of uncertainty in data modeled\nwith probability distributions. Although Monte Carlo (MC) sampling techniques\nhave been used in prior studies to quantify critical point uncertainty, they\nare often expensive and are infrequently used in production-quality\nvisualization software. We, therefore, propose a new end-to-end framework to\naddress these challenges that comprises a threefold contribution. First, we\nderive the critical point uncertainty in closed form, which is more accurate\nand efficient than the conventional MC sampling methods. Specifically, we\nprovide the closed-form and semianalytical (a mix of closed-form and MC\nmethods) solutions for parametric (e.g., uniform, Epanechnikov) and\nnonparametric models (e.g., histograms) with finite support. Second, we\naccelerate critical point probability computations using a parallel\nimplementation with the VTK-m library, which is platform portable. Finally, we\ndemonstrate the integration of our implementation with the ParaView software\nsystem to demonstrate near-real-time results for real datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.18015v1"
    },
    {
        "title": "WindPoly: Polygonal Mesh Reconstruction via Winding Numbers",
        "authors": [
            "Xin He",
            "Chenlei Lv",
            "Pengdi Huang",
            "Hui Huang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Polygonal mesh reconstruction of a raw point cloud is a valuable topic in the\nfield of computer graphics and 3D vision. Especially to 3D architectural\nmodels, polygonal mesh provides concise expressions for fundamental geometric\nstructures while effectively reducing data volume. However, there are some\nlimitations of traditional reconstruction methods: normal vector dependency,\nnoisy points and defective parts sensitivity, and internal geometric structure\nlost, which reduce the practicality in real scene. In this paper, we propose a\nrobust and efficient polygonal mesh reconstruction method to address the issues\nin architectural point cloud reconstruction task. It is an iterative adaptation\nprocess to detect planar shapes from scattered points. The initial structural\npolygonal mesh can be established in the constructed convex polyhedral space\nwithout assistance of normal vectors. Then, we develop an efficient\npolygon-based winding number strategy to orient polygonal mesh with global\nconsistency. The significant advantage of our method is to provide a structural\nreconstruction for architectural point clouds and avoid point-based normal\nvector analysis. It effectively improves the robustness to noisy points and\ndefective parts. More geometric details can be preserved in the reconstructed\npolygonal mesh. Experimental results show that our method can reconstruct\nconcise, oriented and faithfully polygonal mesh that are better than results of\nstate-of-the-art methods. More results and details can be found on\nhttps://vcc.tech/research/2024/WindPoly\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19208v1"
    },
    {
        "title": "FreeShell: A Context-Free 4D Printing Technique for Fabricating Complex\n  3D Triangle Mesh Shells",
        "authors": [
            "Chao Yuan",
            "Nan Cao",
            "Xuejiao Ma",
            "Shengqi Dang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Freeform thin-shell surfaces are critical in various fields, but their\nfabrication is complex and costly. Traditional methods are wasteful and require\ncustom molds, while 3D printing needs extensive support structures and\npost-processing. Thermoshrinkage actuated 4D printing is an effective method\nthrough flat structures fabricating 3D shell. However, existing research faces\nissues related to precise deformation and limited robustness. Addressing these\nissues is challenging due to three key factors: (1) Difficulty in finding a\nuniversal method to control deformation across different materials; (2)\nVariability in deformation influenced by factors such as printing speed, layer\nthickness, and heating temperature; (3) Environmental factors affecting the\ndeformation process. To overcome these challenges, we introduce FreeShell, a\nrobust 4D printing technique that uses thermoshrinkage to create precise 3D\nshells. This method prints triangular tiles connected by shrinkable connectors\nfrom a single material. Upon heating, the connectors shrink, moving the tiles\nto form the desired 3D shape, simplifying fabrication and reducing material and\nenvironment dependency. An optimized algorithm for flattening 3D meshes ensures\nprecision in printing. FreeShell demonstrates its effectiveness through various\nexamples and experiments, showcasing accuracy, robustness, and strength,\nrepresenting advancement in fabricating complex freeform surfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19533v1"
    },
    {
        "title": "Structure-Aware Simplification for Hypergraph Visualization",
        "authors": [
            "Peter Oliver",
            "Eugene Zhang",
            "Yue Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Hypergraphs provide a natural way to represent polyadic relationships in\nnetwork data. For large hypergraphs, it is often difficult to visually detect\nstructures within the data. Recently, a scalable polygon-based visualization\napproach was developed allowing hypergraphs with thousands of hyperedges to be\nsimplified and examined at different levels of detail. However, this approach\nis not guaranteed to eliminate all of the visual clutter caused by unavoidable\noverlaps. Furthermore, meaningful structures can be lost at simplified scales,\nmaking their interpretation unreliable. In this paper, we define hypergraph\nstructures using the bipartite graph representation, allowing us to decompose\nthe hypergraph into a union of structures including topological blocks,\nbridges, and branches, and to identify exactly where unavoidable overlaps must\noccur. We also introduce a set of topology preserving and topology altering\natomic operations, enabling the preservation of important structures while\nreducing unavoidable overlaps to improve visual clarity and interpretability in\nsimplified scales. We demonstrate our approach in several real-world\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19621v1"
    },
    {
        "title": "colorspace: A Python Toolbox for Manipulating and Assessing Colors and\n  Palettes",
        "authors": [
            "Reto Stauffer",
            "Achim Zeileis"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The Python colorspace package provides a toolbox for mapping between\ndifferent color spaces which can then be used to generate a wide range of\nperceptually-based color palettes for qualitative or quantitative (sequential\nor diverging) information. These palettes (as well as any other sets of colors)\ncan be visualized, assessed, and manipulated in various ways, e.g., by color\nswatches, emulating the effects of color vision deficiencies, or depicting the\nperceptual properties. Finally, the color palettes generated by the package can\nbe easily integrated into standard visualization workflows in Python, e.g.,\nusing matplotlib, seaborn, or plotly.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19921v2"
    },
    {
        "title": "Physically-based Path Tracer using WebGPU and OpenPBR",
        "authors": [
            "Simon Stucki",
            "Philipp Ackermann"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This work presents a web-based, open-source path tracer for rendering\nphysically-based 3D scenes using WebGPU and the OpenPBR surface shading model.\nWhile rasterization has been the dominant real-time rendering technique on the\nweb since WebGL's introduction in 2011, it struggles with global illumination.\nThis necessitates more complex techniques, often relying on pregenerated\nartifacts to attain the desired level of visual fidelity. Path tracing\ninherently addresses these limitations but at the cost of increased rendering\ntime. Our work focuses on industrial applications where highly customizable\nproducts are common and real-time performance is not critical. We leverage\nWebGPU to implement path tracing on the web, integrating the OpenPBR standard\nfor physically-based material representation. The result is a near real-time\npath tracer capable of rendering high-fidelity 3D scenes directly in web\nbrowsers, eliminating the need for pregenerated assets. Our implementation\ndemonstrates the potential of WebGPU for advanced rendering techniques and\nopens new possibilities for web-based 3D visualization in industrial\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19977v1"
    },
    {
        "title": "Seamless Parametrization in Penner Coordinates",
        "authors": [
            "Ryan Capouellez",
            "Denis Zorin"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce a conceptually simple and efficient algorithm for seamless\nparametrization, a key element in constructing quad layouts and texture charts\non surfaces. More specifically, we consider the construction of\nparametrizations with prescribed holonomy signatures i.e., a set of angles at\nsingularities, and rotations along homology loops, preserving which is\nessential for constructing parametrizations following an input field, as well\nas for user control of the parametrization structure. Our algorithm performs\nexceptionally well on a large dataset based on Thingi10k [Zhou and Jacobson\n2016], (16156 meshes) as well as on a challenging smaller dataset of [Myles et\nal. 2014], converging, on average, in 9 iterations. Although the algorithm\nlacks a formal mathematical guarantee, presented empirical evidence and the\nconnections between convex optimization and closely related algorithms, suggest\nthat a similar formulation can be found for this algorithm in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.21342v1"
    },
    {
        "title": "SceneMotifCoder: Example-driven Visual Program Learning for Generating\n  3D Object Arrangements",
        "authors": [
            "Hou In Ivan Tam",
            "Hou In Derek Pun",
            "Austin T. Wang",
            "Angel X. Chang",
            "Manolis Savva"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Despite advances in text-to-3D generation methods, generation of multi-object\narrangements remains challenging. Current methods exhibit failures in\ngenerating physically plausible arrangements that respect the provided text\ndescription. We present SceneMotifCoder (SMC), an example-driven framework for\ngenerating 3D object arrangements through visual program learning. SMC\nleverages large language models (LLMs) and program synthesis to overcome these\nchallenges by learning visual programs from example arrangements. These\nprograms are generalized into compact, editable meta-programs. When combined\nwith 3D object retrieval and geometry-aware optimization, they can be used to\ncreate object arrangements varying in arrangement structure and contained\nobjects. Our experiments show that SMC generates high-quality arrangements\nusing meta-programs learned from few examples. Evaluation results demonstrates\nthat object arrangements generated by SMC better conform to user-specified text\ndescriptions and are more physically plausible when compared with\nstate-of-the-art text-to-3D generation and layout methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.02211v1"
    },
    {
        "title": "Demystifying Spatial Dependence: Interactive Visualizations for\n  Interpreting Local Spatial Autocorrelation",
        "authors": [
            "Lee Mason",
            "Blanaid Hicks",
            "Jonas Almeida"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The Local Moran's I statistic is a valuable tool for identifying localized\npatterns of spatial autocorrelation. Understanding these patterns is crucial in\nspatial analysis, but interpreting the statistic can be difficult. To simplify\nthis process, we introduce three novel visualizations that enhance the\ninterpretation of Local Moran's I results. These visualizations can be\ninteractively linked to one another, and to established visualizations, to\noffer a more holistic exploration of the results. We provide a JavaScript\nlibrary with implementations of these new visual elements, along with a web\ndashboard that demonstrates their integrated use.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.02418v1"
    },
    {
        "title": "Automatic Skinning using the Mixed Finite Element Method",
        "authors": [
            "Hongcheng Song",
            "Dimitry Kachkovski",
            "Shaimaa Monem",
            "Abraham Kassauhun Negash",
            "David I. W. Levin"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this work, we show that exploiting additional variables in a mixed finite\nelement formulation of deformation leads to an efficient physics-based\ncharacter skinning algorithm. Taking as input, a user-defined rig, we show how\nto efficiently compute deformations of the character mesh which respect\nartist-supplied handle positions and orientations, but without requiring\ncomplicated constraints on the physics solver, which can cause poor\nperformance. Rather we demonstrate an efficient, user controllable skinning\npipeline that can generate compelling character deformations, using a variety\nof physics material models.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04066v2"
    },
    {
        "title": "One-Shot Method for Computing Generalized Winding Numbers",
        "authors": [
            "Cedric Martens",
            "Mikhail Bessmeltsev"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The generalized winding number is an essential part of the geometry\nprocessing toolkit, allowing to quantify how much a given point is inside a\nsurface, often represented by a mesh or a point cloud, even when the surface is\nopen, noisy, or non-manifold. Parameterized surfaces, which often contain\nintentional and unintentional gaps and imprecisions, would also benefit from a\ngeneralized winding number. Standard methods to compute it, however, rely on a\nsurface integral, challenging to compute without surface discretization,\nleading to loss of precision characteristic of parametric surfaces.\n  We propose an alternative method to compute a generalized winding number,\nbased only on the surface boundary and the intersections of a single ray with\nthe surface. For parametric surfaces, we show that all the necessary operations\ncan be done via a Sum-of-Squares (SOS) formulation, thus computing generalized\nwinding numbers without surface discretization with machine precision. We show\nthat by discretizing only the boundary of the surface, this becomes an\nefficient method.\n  We demonstrate an application of our method to the problem of computing a\ngeneralized winding number of a surface represented by a curve network, where\neach curve loop is surfaced via Laplace equation. We use the Boundary Element\nMethod to express the solution as a parametric surface, allowing us to apply\nour method without meshing the surfaces. As a bonus, we also demonstrate that\nfor meshes with many triangles and a simple boundary, our method is faster than\nthe hierarchical evaluation of the generalized winding number while still being\nprecise.\n  We validate our algorithms theoretically, numerically, and by demonstrating a\ngallery of results \\new{on a variety of parametric surfaces and meshes}, as\nwell uses in a variety of applications, including voxelizations and boolean\noperations.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04466v1"
    },
    {
        "title": "Mesh Simplification For Unfolding",
        "authors": [
            "Manas Bhargava",
            "Camille Schreck",
            "Marco Freire",
            "Pierre-Alexandre Hugron",
            "Sylvain Lefebvre",
            "Silvia Sellán",
            "Bernd Bickel"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a computational approach for unfolding 3D shapes isometrically\ninto the plane as a single patch without overlapping triangles. This is a hard,\nsometimes impossible, problem, which existing methods are forced to soften by\nallowing for map distortions or multiple patches. Instead, we propose a\ngeometric relaxation of the problem: we modify the input shape until it admits\nan overlap-free unfolding. We achieve this by locally displacing vertices and\ncollapsing edges, guided by the unfolding process. We validate our algorithm\nquantitatively and qualitatively on a large dataset of complex shapes and show\nits proficiency by fabricating real shapes from paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.06944v1"
    },
    {
        "title": "Exploring Uncertainty Visualization for Degenerate Tensors in 3D\n  Symmetric Second-Order Tensor Field Ensembles",
        "authors": [
            "Tadea Schmitz",
            "Tim Gerrits"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Symmetric second-order tensors are fundamental in various scientific and\nengineering domains, as they can represent properties such as material stresses\nor diffusion processes in brain tissue. In recent years, several approaches\nhave been introduced and improved to analyze these fields using topological\nfeatures, such as degenerate tensor locations, i.e., the tensor has repeated\neigenvalues, or normal surfaces. Traditionally, the identification of such\nfeatures has been limited to single tensor fields. However, it has become\ncommon to create ensembles to account for uncertainties and variability in\nsimulations and measurements. In this work, we explore novel methods for\ndescribing and visualizing degenerate tensor locations in 3D symmetric\nsecond-order tensor field ensembles. We base our considerations on the tensor\nmode and analyze its practicality in characterizing the uncertainty of\ndegenerate tensor locations before proposing a variety of visualization\nstrategies to effectively communicate degenerate tensor information. We\ndemonstrate our techniques for synthetic and simulation data sets. The results\nindicate that the interplay of different descriptions for uncertainty can\neffectively convey information on degenerate tensor locations.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.08099v2"
    },
    {
        "title": "Unified Smooth Vector Graphics: Modeling Gradient Meshes and Curve-based\n  Approaches Jointly as Poisson Problem",
        "authors": [
            "Xingze Tian",
            "Tobias Günther"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Research on smooth vector graphics is separated into two independent research\nthreads: one on interpolation-based gradient meshes and the other on\ndiffusion-based curve formulations. With this paper, we propose a mathematical\nformulation that unifies gradient meshes and curve-based approaches as solution\nto a Poisson problem. To combine these two well-known representations, we first\ngenerate a non-overlapping intermediate patch representation that specifies for\neach patch a target Laplacian and boundary conditions. Unifying the treatment\nof boundary conditions adds further artistic degrees of freedoms to the\nexisting formulations, such as Neumann conditions on diffusion curves. To\nsynthesize a raster image for a given output resolution, we then rasterize\nboundary conditions and Laplacians for the respective patches and compute the\nfinal image as solution to a Poisson problem. We evaluate the method on various\ntest scenes containing gradient meshes and curve-based primitives. Since our\nmathematical formulation works with established smooth vector graphics\nprimitives on the front-end, it is compatible with existing content creation\npipelines and with established editing tools. Rather than continuing two\nseparate research paths, we hope that a unification of the formulations will\nlead to new rasterization and vectorization tools in the future that utilize\nthe strengths of both approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09211v1"
    },
    {
        "title": "Double-Precision Floating-Point Data Visualizations Using Vulkan API",
        "authors": [
            "Nezihe Sozen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Proper representation of data in graphical visualizations becomes challenging\nwhen high accuracy in data types is required, especially in those situations\nwhere the difference between double-precision floating-point and\nsingle-precision floating-point values makes a significant difference. Some of\nthe limitations of using single-precision over double-precision include lesser\naccuracy, which accumulates errors over time, and poor modeling of large or\nsmall numbers. In such scenarios, emulated double precision is often used as a\nsolution. The proposed methodology uses a modern GPU pipeline and graphics\nlibrary API specifications to use native double precision. In this research,\nthe approach is implemented using the Vulkan API, C++, and GLSL. Experimental\nevaluation with a series of experiments on 2D and 3D point datasets is proposed\nto indicate the effectiveness of the approach. This evaluates performance\ncomparisons between native double-precision implementations against their\nemulated double-precision approaches with respect to rendering performance and\naccuracy. This study provides insight into the benefits of using native\ndouble-precision in graphical applications, denoting limitations and problems\nwith emulated double-precision usages. These results improve the general\nunderstanding of the precision involved in graphical visualizations and assist\ndevelopers in making decisions about which precision methods to use during\ntheir applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09699v1"
    },
    {
        "title": "Neural Representation of Shape-Dependent Laplacian Eigenfunctions",
        "authors": [
            "Yue Chang",
            "Otman Benchekroun",
            "Maurizio M. Chiaramonte",
            "Peter Yichen Chen",
            "Eitan Grinspun"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The eigenfunctions of the Laplace operator are essential in mathematical\nphysics, engineering, and geometry processing. Typically, these are computed by\ndiscretizing the domain and performing eigendecomposition, tying the results to\na specific mesh. However, this method is unsuitable for\ncontinuously-parameterized shapes.\n  We propose a novel representation for eigenfunctions in\ncontinuously-parameterized shape spaces, where eigenfunctions are spatial\nfields with continuous dependence on shape parameters, defined by minimal\nDirichlet energy, unit norm, and mutual orthogonality. We implement this with\nmultilayer perceptrons trained as neural fields, mapping shape parameters and\ndomain positions to eigenfunction values.\n  A unique challenge is enforcing mutual orthogonality with respect to\ncausality, where the causal ordering varies across the shape space. Our\ntraining method therefore requires three interwoven concepts: (1) learning $n$\neigenfunctions concurrently by minimizing Dirichlet energy with unit norm\nconstraints; (2) filtering gradients during backpropagation to enforce causal\northogonality, preventing earlier eigenfunctions from being influenced by later\nones; (3) dynamically sorting the causal ordering based on eigenvalues to track\neigenvalue curve crossovers.\n  We demonstrate our method on problems such as shape family analysis,\npredicting eigenfunctions for incomplete shapes, interactive shape\nmanipulation, and computing higher-dimensional eigenfunctions, on all of which\ntraditional methods fall short.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.10099v1"
    },
    {
        "title": "Bimodal Visualization of Industrial X-Ray and Neutron Computed\n  Tomography Data",
        "authors": [
            "Xuan Huang",
            "Haichao Miao",
            "Hyojin Kim",
            "Andrew Townsend",
            "Kyle Champley",
            "Joseph Tringe",
            "Valerio Pascucci",
            "Peer-Timo Bremer"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Advanced manufacturing creates increasingly complex objects with material\ncompositions that are often difficult to characterize by a single modality. Our\ncollaborating domain scientists are going beyond traditional methods by\nemploying both X-ray and neutron computed tomography to obtain complementary\nrepresentations expected to better resolve material boundaries. However, the\nuse of two modalities creates its own challenges for visualization, requiring\neither complex adjustments of bimodal transfer functions or the need for\nmultiple views. Together with experts in nondestructive evaluation, we designed\na novel interactive bimodal visualization approach to create a combined view of\nthe co-registered X-ray and neutron acquisitions of industrial objects. Using\nan automatic topological segmentation of the bivariate histogram of X-ray and\nneutron values as a starting point, the system provides a simple yet effective\ninterface to easily create, explore, and adjust a bimodal visualization. We\npropose a widget with simple brushing interactions that enables the user to\nquickly correct the segmented histogram results. Our semiautomated system\nenables domain experts to intuitively explore large bimodal datasets without\nthe need for either advanced segmentation algorithms or knowledge of\nvisualization techniques. We demonstrate our approach using synthetic examp\n",
        "pdf_link": "http://arxiv.org/pdf/2408.11957v1"
    },
    {
        "title": "Real-Time Rendering of Glints in the Presence of Area Lights",
        "authors": [
            "Tom Kneiphof",
            "Reinhard Klein"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Many real-world materials are characterized by a glittery appearance.\nReproducing this effect in physically based renderings is a challenging problem\ndue to its discrete nature, especially in real-time applications which require\na consistently low runtime. Recent work focuses on glittery appearance\nilluminated by infinitesimally small light sources only. For light sources like\nthe sun this approximation is a reasonable choice. In the real world however,\nall light sources are fundamentally area light sources. In this paper, we\nderive an efficient method for rendering glints illuminated by spatially\nconstant diffuse area lights in real time. To this end, we require an adequate\nestimate for the probability of a single microfacet to be correctly oriented\nfor reflection from the source to the observer. A good estimate is achieved\neither using linearly transformed cosines (LTC) for large light sources, or a\nlocally constant approximation of the normal distribution for small spherical\ncaps of light directions. To compute the resulting number of reflecting\nmicrofacets, we employ a counting model based on the binomial distribution. In\nthe evaluation, we demonstrate the visual accuracy of our approach, which is\neasily integrated into existing real-time rendering frameworks, especially if\nthey already implement shading for area lights using LTCs and a counting model\nfor glint shading under point and directional illumination. Besides the\noverhead of the preexisting constituents, our method adds little to no\nadditional overhead.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.13611v1"
    },
    {
        "title": "Evaluating and Comparing Crowd Simulations: Perspectives from a Crowd\n  Authoring Tool",
        "authors": [
            "Gabriel Fonseca Silva",
            "Paulo Ricardo Knob",
            "Rubens Halbig Montanha",
            "Soraia Raupp Musse"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Crowd simulation is a research area widely used in diverse fields, including\ngaming and security, assessing virtual agent movements through metrics like\ntime to reach their goals, speed, trajectories, and densities. This is relevant\nfor security applications, for instance, as different crowd configurations can\ndetermine the time people spend in environments trying to evacuate them. In\nthis work, we extend WebCrowds, an authoring tool for crowd simulation, to\nallow users to build scenarios and evaluate them through a set of metrics. The\naim is to provide a quantitative metric that can, based on simulation data,\nselect the best crowd configuration in a certain environment. We conduct\nexperiments to validate our proposed metric in multiple crowd simulation\nscenarios and perform a comparison with another metric found in the literature.\nThe results show that experts in the domain of crowd scenarios agree with our\nproposed quantitative metric.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.15762v1"
    },
    {
        "title": "Micro and macro facial expressions by driven animations in realistic\n  Virtual Humans",
        "authors": [
            "Rubens Halbig Montanha",
            "Giovana Nascimento Raupp",
            "Ana Carolina Policarpo Schmitt",
            "Victor Flávio de Andrade Araujo",
            "Soraia Raupp Musse"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Computer Graphics (CG) advancements have allowed the creation of more\nrealistic Virtual Humans (VH) through modern techniques for animating the VH\nbody and face, thereby affecting perception. From traditional methods,\nincluding blend shapes, to driven animations using facial and body tracking,\nthese advancements can potentially enhance the perception of comfort and\nrealism in relation to VHs. Previously, Psychology studied facial movements in\nhumans, with some works separating expressions into macro and micro\nexpressions. Also, some previous CG studies have analyzed how macro and micro\nexpressions are perceived, replicating psychology studies in VHs, encompassing\nstudies with realistic and cartoon VHs, and exploring different VH\ntechnologies. However, instead of using facial tracking animation methods,\nthese previous studies animated the VHs using blendshapes interpolation. To\nunderstand how the facial tracking technique alters the perception of VHs, this\npaper extends the study to macro and micro expressions, employing two datasets\nto transfer real facial expressions to VHs and analyze how their expressions\nare perceived. Our findings suggest that transferring facial expressions from\nreal actors to VHs significantly diminishes the accuracy of emotion perception\ncompared to VH facial animations created by artists.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.16110v1"
    },
    {
        "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
        "authors": [
            "Jianxin Sun",
            "David Lenz",
            "Hongfeng Yu",
            "Tom Peterka"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.00184v1"
    },
    {
        "title": "GroomCap: High-Fidelity Prior-Free Hair Capture",
        "authors": [
            "Yuxiao Zhou",
            "Menglei Chai",
            "Daoye Wang",
            "Sebastian Winberg",
            "Erroll Wood",
            "Kripasindhu Sarkar",
            "Markus Gross",
            "Thabo Beeler"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Despite recent advances in multi-view hair reconstruction, achieving\nstrand-level precision remains a significant challenge due to inherent\nlimitations in existing capture pipelines. We introduce GroomCap, a novel\nmulti-view hair capture method that reconstructs faithful and high-fidelity\nhair geometry without relying on external data priors. To address the\nlimitations of conventional reconstruction algorithms, we propose a neural\nimplicit representation for hair volume that encodes high-resolution 3D\norientation and occupancy from input views. This implicit hair volume is\ntrained with a new volumetric 3D orientation rendering algorithm, coupled with\n2D orientation distribution supervision, to effectively prevent the loss of\nstructural information caused by undesired orientation blending. We further\npropose a Gaussian-based hair optimization strategy to refine the traced hair\nstrands with a novel chained Gaussian representation, utilizing direct\nphotometric supervision from images. Our results demonstrate that GroomCap is\nable to capture high-quality hair geometries that are not only more precise and\ndetailed than existing methods but also versatile enough for a range of\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.00831v3"
    },
    {
        "title": "Efficient Analysis and Visualization of High-Resolution Computed\n  Tomography Data for the Exploration of Enclosed Cuneiform Tablets",
        "authors": [
            "Stephan Olbrich",
            "Andreas Beckert",
            "Cécile Michel",
            "Christian Schroer",
            "Samaneh Ehteram",
            "Andreas Schropp",
            "Philipp Paetzold"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Cuneiform is the earliest known system of writing, first developed for the\nSumerian language of southern Mesopotamia in the second half of the 4th\nmillennium BC. Cuneiform signs are obtained by impressing a stylus on fresh\nclay tablets. For certain purposes, e.g. authentication by seal imprint, some\ncuneiform tablets were enclosed in clay envelopes, which cannot be opened\nwithout destroying them. The aim of our interdisciplinary project is the\nnon-invasive study of clay tablets. A portable X-ray micro-CT scanner is\ndeveloped to acquire density data of such artifacts on a high-resolution,\nregular 3D grid at collection sites. The resulting volume data is processed\nthrough feature-preserving denoising, extraction of high-accuracy surfaces\nusing a manifold dual marching cubes algorithm and extraction of local features\nby enhanced curvature rendering and ambient occlusion. For the non-invasive\nstudy of cuneiform inscriptions, the tablet is virtually separated from its\nenvelope by curvature-based segmentation. The computational- and data-intensive\nalgorithms are optimized or near-real-time offline usage with limited resources\nat collection sites. To visualize the complexity-reduced and octree-based\ncompressed representation of surfaces, we develop and implement an interactive\napplication. To facilitate the analysis of such clay tablets, we implement\nshape-based feature extraction algorithms to enhance cuneiform recognition. Our\nworkflow supports innovative 3D display and interaction techniques such as\nautostereoscopic displays and gesture control.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.04236v1"
    },
    {
        "title": "Casper DPM: Cascaded Perceptual Dynamic Projection Mapping onto Hands",
        "authors": [
            "Yotam Erel",
            "Or Kozlovsky-Mordenfeld",
            "Daisuke Iwai",
            "Kosuke Sato",
            "Amit H. Bermano"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a technique for dynamically projecting 3D content onto human hands\nwith short perceived motion-to-photon latency. Computing the pose and shape of\nhuman hands accurately and quickly is a challenging task due to their\narticulated and deformable nature. We combine a slower 3D coarse estimation of\nthe hand pose with high speed 2D correction steps which improve the alignment\nof the projection to the hands, increase the projected surface area, and reduce\nperceived latency. Since our approach leverages a full 3D reconstruction of the\nhands, any arbitrary texture or reasonably performant effect can be applied,\nwhich was not possible before. We conducted two user studies to assess the\nbenefits of using our method. The results show subjects are less sensitive to\nlatency artifacts and perform faster and with more ease a given associated task\nover the naive approach of directly projecting rendered frames from the 3D pose\nestimation. We demonstrate several novel use cases and applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.04397v1"
    },
    {
        "title": "Weighted Squared Volume Minimization (WSVM) for Generating Uniform\n  Tetrahedral Meshes",
        "authors": [
            "Kaixin Yu",
            "Yifu Wang",
            "Peng Song",
            "Xiangqiao Meng",
            "Ying He",
            "Jianjun Chen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper presents a new algorithm, Weighted Squared Volume Minimization\n(WSVM), for generating high-quality tetrahedral meshes from closed triangle\nmeshes. Drawing inspiration from the principle of minimal surfaces that\nminimize squared surface area, WSVM employs a new energy function integrating\nweighted squared volumes for tetrahedral elements. When minimized with constant\nweights, this energy promotes uniform volumes among the tetrahedra. Adjusting\nthe weights to account for local geometry further achieves uniform dihedral\nangles within the mesh. The algorithm begins with an initial tetrahedral mesh\ngenerated via Delaunay tetrahedralization and proceeds by sequentially\nminimizing volume-oriented and then dihedral angle-oriented energies. At each\nstage, it alternates between optimizing vertex positions and refining mesh\nconnectivity through the iterative process. The algorithm operates fully\nautomatically and requires no parameter tuning. Evaluations on a variety of 3D\nmodels demonstrate that WSVM consistently produces tetrahedral meshes of higher\nquality, with fewer slivers and enhanced uniformity compared to existing\nmethods. Check out further details at the project webpage:\nhttps://kaixinyu-hub.github.io/WSVM.github.io.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.05525v1"
    },
    {
        "title": "Fiber-level Woven Fabric Capture from a Single Photo",
        "authors": [
            "Zixuan Li",
            "Pengfei Shen",
            "Hanxiao Sun",
            "Zibo Zhang",
            "Yu Guo",
            "Ligang Liu",
            "Ling-Qi Yan",
            "Steve Marschner",
            "Milos Hasan",
            "Beibei Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Accurately rendering the appearance of fabrics is challenging, due to their\ncomplex 3D microstructures and specialized optical properties. If we model the\ngeometry and optics of fabrics down to the fiber level, we can achieve\nunprecedented rendering realism, but this raises the difficulty of authoring or\ncapturing the fiber-level assets. Existing approaches can obtain fiber-level\ngeometry with special devices (e.g., CT) or complex hand-designed procedural\npipelines (manually tweaking a set of parameters). In this paper, we propose a\nunified framework to capture fiber-level geometry and appearance of woven\nfabrics using a single low-cost microscope image. We first use a simple neural\nnetwork to predict initial parameters of our geometric and appearance models.\nFrom this starting point, we further optimize the parameters of procedural\nfiber geometry and an approximated shading model via differentiable\nrasterization to match the microscope photo more accurately. Finally, we refine\nthe fiber appearance parameters via differentiable path tracing, converging to\naccurate fiber optical parameters, which are suitable for physically-based\nlight simulations to produce high-quality rendered results. We believe that our\nmethod is the first to utilize differentiable rendering at the microscopic\nlevel, supporting physically-based scattering from explicit fiber assemblies.\nOur fabric parameter estimation achieves high-quality re-rendering of measured\nwoven fabric samples in both distant and close-up views. These results can\nfurther be used for efficient rendering or converted to downstream\nrepresentations. We also propose a patch-space fiber geometry procedural\ngeneration and a two-scale path tracing framework for efficient rendering of\nfabric scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.06368v1"
    },
    {
        "title": "Instant Facial Gaussians Translator for Relightable and Interactable\n  Facial Rendering",
        "authors": [
            "Dafei Qin",
            "Hongyang Lin",
            "Qixuan Zhang",
            "Kaichun Qiao",
            "Longwen Zhang",
            "Zijun Zhao",
            "Jun Saito",
            "Jingyi Yu",
            "Lan Xu",
            "Taku Komura"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose GauFace, a novel Gaussian Splatting representation, tailored for\nefficient animation and rendering of physically-based facial assets. Leveraging\nstrong geometric priors and constrained optimization, GauFace ensures a neat\nand structured Gaussian representation, delivering high fidelity and real-time\nfacial interaction of 30fps@1440p on a Snapdragon 8 Gen 2 mobile platform.\n  Then, we introduce TransGS, a diffusion transformer that instantly translates\nphysically-based facial assets into the corresponding GauFace representations.\nSpecifically, we adopt a patch-based pipeline to handle the vast number of\nGaussians effectively. We also introduce a novel pixel-aligned sampling scheme\nwith UV positional encoding to ensure the throughput and rendering quality of\nGauFace assets generated by our TransGS. Once trained, TransGS can instantly\ntranslate facial assets with lighting conditions to GauFace representation,\nWith the rich conditioning modalities, it also enables editing and animation\ncapabilities reminiscent of traditional CG pipelines.\n  We conduct extensive evaluations and user studies, compared to traditional\noffline and online renderers, as well as recent neural rendering methods, which\ndemonstrate the superior performance of our approach for facial asset\nrendering. We also showcase diverse immersive applications of facial assets\nusing our TransGS approach and GauFace representation, across various platforms\nlike PCs, phones and even VR headsets.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.07441v2"
    },
    {
        "title": "DrawingSpinUp: 3D Animation from Single Character Drawings",
        "authors": [
            "Jie Zhou",
            "Chufeng Xiao",
            "Miu-Ling Lam",
            "Hongbo Fu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Animating various character drawings is an engaging visual content creation\ntask. Given a single character drawing, existing animation methods are limited\nto flat 2D motions and thus lack 3D effects. An alternative solution is to\nreconstruct a 3D model from a character drawing as a proxy and then retarget 3D\nmotion data onto it. However, the existing image-to-3D methods could not work\nwell for amateur character drawings in terms of appearance and geometry. We\nobserve the contour lines, commonly existing in character drawings, would\nintroduce significant ambiguity in texture synthesis due to their\nview-dependence. Additionally, thin regions represented by single-line contours\nare difficult to reconstruct (e.g., slim limbs of a stick figure) due to their\ndelicate structures. To address these issues, we propose a novel system,\nDrawingSpinUp, to produce plausible 3D animations and breathe life into\ncharacter drawings, allowing them to freely spin up, leap, and even perform a\nhip-hop dance. For appearance improvement, we adopt a removal-then-restoration\nstrategy to first remove the view-dependent contour lines and then render them\nback after retargeting the reconstructed character. For geometry refinement, we\ndevelop a skeleton-based thinning deformation algorithm to refine the slim\nstructures represented by the single-line contours. The experimental\nevaluations and a perceptual user study show that our proposed method\noutperforms the existing 2D and 3D animation methods and generates high-quality\n3D animations from a single character drawing. Please refer to our project page\n(https://lordliang.github.io/DrawingSpinUp) for the code and generated\nanimations.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08615v1"
    },
    {
        "title": "Architectural Co-LOD Generation",
        "authors": [
            "Runze Zhang",
            "Shanshan Pan",
            "Chenlei Lv",
            "Minglun Gong",
            "Hui Huang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Managing the level-of-detail (LOD) in architectural models is crucial yet\nchallenging, particularly for effective representation and visualization of\nbuildings. Traditional approaches often fail to deliver controllable detail\nalongside semantic consistency, especially when dealing with noisy and\ninconsistent inputs. We address these limitations with \\emph{Co-LOD}, a new\napproach specifically designed for effective LOD management in architectural\nmodeling. Co-LOD employs shape co-analysis to standardize geometric structures\nacross multiple buildings, facilitating the progressive and consistent\ngeneration of LODs. This method allows for precise detailing in both individual\nmodels and model collections, ensuring semantic integrity. Extensive\nexperiments demonstrate that Co-LOD effectively applies accurate LOD across a\nvariety of architectural inputs, consistently delivering superior detail and\nquality in LOD representations.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.12207v1"
    },
    {
        "title": "Rest Shape Optimization for Sag-Free Discrete Elastic Rods",
        "authors": [
            "Tetsuya Takahashi",
            "Christopher Batty"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a new rest shape optimization framework to achieve sag-free\nsimulations of discrete elastic rods. To optimize rest shape parameters, we\nformulate a minimization problem based on the kinetic energy with a regularizer\nwhile imposing box constraints on these parameters to ensure the system's\nstability. Our method solves the resulting constrained minimization problem via\nthe Gauss-Newton algorithm augmented with penalty methods. We demonstrate that\nthe optimized rest shape parameters enable discrete elastic rods to achieve\nstatic equilibrium for a wide range of strand geometries and material\nparameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.12362v1"
    },
    {
        "title": "CrossRT: A cross platform programming technology for\n  hardware-accelerated ray tracing in CG and CV applications",
        "authors": [
            "Vladimir Frolov",
            "Vadim Sanzharov",
            "Garifullin Albert",
            "Maxim Raenchuk",
            "Alexei Voloboy"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a programming technology that bridges cross-platform compatibility\nand hardware acceleration in ray tracing applications. Our methodology enables\ndevelopers to define algorithms while our translator manages implementation\nspecifics for different hardware or APIs. Features include: generating\nhardware-accelerated code from hardware-agnostic, object-oriented C++ algorithm\ndescriptions; enabling users to define software fallbacks for\nnon-hardware-accelerated CPUs and GPUs; producing GPU programming API-based\nalgorithm implementations resembling manually ported C++ versions. The\ngenerated code is editable and readable, allowing for additional hardware\nacceleration. Our translator supports single megakernel and multiple kernel\npath tracing implementations without altering the programming model or input\nsource code. Wavefront mode is crucial for NeRF and SDF, ensuring efficient\nevaluation with multiple kernels. Validation on tasks such as BVH tree\nbuild/traversal, ray-surface intersection for SDF, ray-volume intersection for\n3D Gaussian Splatting, and complex Path Tracing models showed comparable\nperformance levels to expert-written implementations for GPUs. Our technology\noutperformed existing Path Tracing implementations.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.12617v1"
    },
    {
        "title": "Data-driven Viscosity Solver for Fluid Simulation",
        "authors": [
            "Wonjung Park",
            "Hyunsoo Kim",
            "Jinah Park"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a data-driven viscosity solver based on U-shaped convolutional\nneural network to predict velocity changes due to viscosity. Our solver takes\nvelocity derivatives, fluid volume, and solid indicator quantities as input.\nThe traditional marker-and-cell (MAC) grid stores velocities at the edges of\nthe grid, causing the dimensions of the velocity field vary from axis to axis.\nIn our work, we suggest a symmetric MAC grid that maintains consistent\ndimensions across axes without interpolation or symmetry breaking. The proposed\ngrid effectively transfers spatial fluid quantities such as partial derivatives\nof velocity, enabling networks to generate accurate predictions. Additionally,\nwe introduce a physics-based loss inspired by the variational formulation of\nviscosity to enhance the network's generalization for a wide range of viscosity\ncoefficients. We demonstrate various fluid simulation results, including 2D and\n3D fluid-rigid body scenes and a scene exhibiting the buckling effect. Our code\nis available at \\url{https://github.com/SSTDV-Project/python-fluid-simulation.}\n",
        "pdf_link": "http://arxiv.org/pdf/2409.14653v1"
    },
    {
        "title": "Hybrid Drawing Solutions in AR Bitmap-to-Vector Techniques on 3D\n  Surfaces",
        "authors": [
            "Pengcheng Ding",
            "Yedian Cheng",
            "Mirjana Prpa"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Recent advancements in augmented reality and virtual reality have\nsignificantly enhanced workflows for drawing 3D objects. Despite these\ntechnological strides, existing AR tools often lack the necessary precision and\nstruggle to maintain quality when scaled, posing challenges for larger-scale\ndrawing tasks. This paper introduces a novel AR tool that uniquely integrates\nbitmap drawing and vectorization techniques. This integration allows engineers\nto perform rapid, real-time drawings directly on 3D models, with the capability\nto vectorize the data for scalable accuracy and editable points, ensuring no\nloss in fidelity when modifying or resizing the drawings. We conducted user\nstudies involving professional engineers, designers, and contractors to\nevaluate the tool's integration into existing workflows, its usability, and its\nimpact on project outcomes. The results demonstrate that our enhancements\nsignificantly improve the efficiency of drawing processes. Specifically, the\nability to perform quick, editable, and scalable drawings directly on 3D models\nnot only enhances productivity but also ensures adaptability across various\nproject sizes and complexities.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.15171v1"
    },
    {
        "title": "Simplifying Triangle Meshes in the Wild",
        "authors": [
            "Hsueh-Ti Derek Liu",
            "Xiaoting Zhang",
            "Cem Yuksel"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper introduces a fast and robust method for simplifying surface\ntriangle meshes in the wild while maintaining high visual quality. While\nprevious methods achieve excellent results on manifold meshes by using the\nquadric error metric, they suffer from producing high-quality outputs for\nuser-created meshes, which often contain non-manifold elements and multiple\nconnected components. In this work, we begin by outlining the pitfalls of\nexisting mesh simplification techniques and highlighting the discrepancy in\ntheir formulations with existing mesh data. We then propose a method for\nsimplifying these (non-manifold) triangle meshes, while maintaining quality\ncomparable to the existing methods for manifold inputs. Our key idea is to\nreformulate mesh simplification as a problem of decimating simplicial\n2-complexes. This involves a novel construction to turn a triangle soup into a\nsimplicial 2-complex, followed by iteratively collapsing 1-simplices (vertex\npairs) with our modified quadric error metric tailored for topology changes.\nBesides, we also tackle textured mesh simplification. Instead of following\nexisting strategies to preserve mesh UVs, we propose a novel perspective that\nonly focuses on preserving texture colors defined on the surface, regardless of\nthe layout in the texture UV space. This leads to a more robust method for\ntextured mesh simplification that is free from the texture bleeding artifact.\nOur mesh simplification enables level-of-detail algorithms to operate on\narbitrary triangle meshes in the wild. We demonstrate improvements over prior\ntechniques through extensive qualitative and quantitative evaluations, along\nwith user studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.15458v1"
    },
    {
        "title": "A Differentiable Material Point Method Framework for Shape Morphing",
        "authors": [
            "Michael Xu",
            "Chang-Yong Song",
            "David I. W. Levin",
            "David Hyde"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a novel, physically-based morphing technique for elastic shapes,\nleveraging the differentiable material point method (MPM) with space-time\ncontrol through per-particle deformation gradients to accommodate complex\ntopology changes. This approach, grounded in MPM's natural handling of dynamic\ntopologies, is enhanced by a chained iterative optimization technique, allowing\nfor the creation of both succinct and extended morphing sequences that maintain\ncoherence over time. Demonstrated across various challenging scenarios, our\nmethod is able to produce detailed elastic deformation and topology\ntransitions, all grounded within our physics-based simulation framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.15746v1"
    },
    {
        "title": "Synchronize Dual Hands for Physics-Based Dexterous Guitar Playing",
        "authors": [
            "Pei Xu",
            "Ruocheng Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a novel approach to synthesize dexterous motions for physically\nsimulated hands in tasks that require coordination between the control of two\nhands with high temporal precision. Instead of directly learning a joint policy\nto control two hands, our approach performs bimanual control through\ncooperative learning where each hand is treated as an individual agent. The\nindividual policies for each hand are first trained separately, and then\nsynchronized through latent space manipulation in a centralized environment to\nserve as a joint policy for two-hand control. By doing so, we avoid directly\nperforming policy learning in the joint state-action space of two hands with\nhigher dimensions, greatly improving the overall training efficiency. We\ndemonstrate the effectiveness of our proposed approach in the challenging\nguitar-playing task. The virtual guitarist trained by our approach can\nsynthesize motions from unstructured reference data of general guitar-playing\npractice motions, and accurately play diverse rhythms with complex chord\npressing and string picking patterns based on the input guitar tabs that do not\nexist in the references. Along with this paper, we provide the motion capture\ndata that we collected as the reference for policy training. Code is available\nat: https://pei-xu.github.io/guitar.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.16629v1"
    },
    {
        "title": "pyGANDALF -- An open-source, Geometric, ANimation, Directed,\n  Algorithmic, Learning Framework for Computer Graphics",
        "authors": [
            "John Petropoulos",
            "Manos Kamarianakis",
            "Antonis Protopsaltis",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In computer graphics (CG) education, the challenge of finding modern,\nversatile tools is significant, particularly when integrating both legacy and\nadvanced technologies. Traditional frameworks, often reliant on solid, yet\noutdated APIs like OpenGL, limit the exploration of cutting-edge graphics\ntechniques. To address this, we introduce pyGANDALF, a unique, lightweight,\nopen-source CG framework built on three pillars: Entity-Component-System (ECS)\narchitecture, Python programming, and WebGPU integration. This combination sets\npyGANDALF apart by providing a streamlined ECS design with an editor layer,\ncompatibility with WebGPU for state-of-the-art features like compute and ray\ntracing pipelines, and a programmer-friendly Python environment. The framework\nsupports modern features, such as Physically Based Rendering (PBR) capabilities\nand integration with Universal Scene Description (USD) formats, making it\nsuitable for both educational demonstrations and real-world applications.\nEvaluations by expert users confirmed that pyGANDALF effectively balances ease\nof use with advanced functionality, preparing students for contemporary CG\ndevelopment challenges.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.16724v1"
    },
    {
        "title": "Multi-Tier Preservation of Discrete Morse Smale Complexes in\n  Error-Bounded Lossy Compression",
        "authors": [
            "Yuxiao Li",
            "Xin Liang",
            "Bei Wang",
            "Hanqi Guo"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a multi-tier paradigm to preserve various components of\nMorse-Smale complexes in lossy compressed scalar fields, including extrema,\nsaddles, separatrices, and persistence diagrams. Existing error-bounded lossy\ncompressors rarely consider preserving topological structures such as discrete\nMorse-Smale complexes, leading to significant inaccuracies in data\ninterpretation and potentially resulting in incorrect scientific conclusions.\nThis paper mainly focuses on preserving the Morse-Smale complexes in 2D or 3D\ndiscrete scalar fields by precisely preserving critical simplices and the\nseparatrices that connect them. Our approach generates a series of edits during\ncompression time, which are applied to the decompressed data to accurately\nreconstruct the complexes while maintaining the error within prescribed bounds.\nWe design a workflow that iteratively fixes critical simplices and separatrices\nin alternating steps until convergence within finite iterations. Our approach\naddresses diverse application needs by offering users flexible options to\nbalance compression efficiency and feature preservation. To enable effective\nintegration with lossy compressors, we use GPU parallelism to enhance the\nperformance of each workflow component. We conduct experiments on various\ndatasets to demonstrate the effectiveness of our method in accurately\npreserving Morse-Smale complexes.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.17346v1"
    },
    {
        "title": "SShaDe: scalable shape deformation via local representations",
        "authors": [
            "Filippo Maggioli",
            "Daniele Baieri",
            "Zorah Lähner",
            "Simone Melzi"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  With the increase in computational power for the available hardware, the\ndemand for high-resolution data in computer graphics applications increases.\nConsequently, classical geometry processing techniques based on linear algebra\nsolutions are starting to become obsolete. In this setting, we propose a novel\napproach for tackling mesh deformation tasks on high-resolution meshes. By\nreducing the input size with a fast remeshing technique and preserving a\nconsistent representation of the original mesh with local reference frames, we\nprovide a solution that is both scalable and robust in multiple applications,\nsuch as as-rigid-as-possible deformations, non-rigid isometric transformations,\nand pose transfer tasks. We extensively test our technique and compare it\nagainst state-of-the-art methods, proving that our approach can handle meshes\nwith hundreds of thousands of vertices in tens of seconds while still achieving\nresults comparable with the other solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.17961v2"
    },
    {
        "title": "Local Surface Parameterizations via Geodesic Splines",
        "authors": [
            "Abhishek Madan",
            "David I. W. Levin"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a general method for computing local parameterizations rooted at a\npoint on a surface, where the surface is described only through a signed\nimplicit function and a corresponding projection function. Using a two-stage\nprocess, we compute several points radially emanating from the map origin, and\ninterpolate between them with a spline surface. The narrow interface of our\nmethod allows it to support several kinds of geometry such as signed distance\nfunctions, general analytic implicit functions, triangle meshes, neural\nimplicits, and point clouds. We demonstrate the high quality of our generated\nparameterizations on a variety of examples, and show applications in local\ntexturing and surface curve drawing.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.06330v1"
    },
    {
        "title": "Deformation Recovery: Localized Learning for Detail-Preserving\n  Deformations",
        "authors": [
            "Ramana Sundararaman",
            "Nicolas Donati",
            "Simone Melzi",
            "Etienne Corman",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce a novel data-driven approach aimed at designing high-quality\nshape deformations based on a coarse localized input signal. Unlike previous\ndata-driven methods that require a global shape encoding, we observe that\ndetail-preserving deformations can be estimated reliably without any global\ncontext in certain scenarios. Building on this intuition, we leverage Jacobians\ndefined in a one-ring neighborhood as a coarse representation of the\ndeformation. Using this as the input to our neural network, we apply a series\nof MLPs combined with feature smoothing to learn the Jacobian corresponding to\nthe detail-preserving deformation, from which the embedding is recovered by the\nstandard Poisson solve. Crucially, by removing the dependence on a global\nencoding, every \\textit{point} becomes a training example, making the\nsupervision particularly lightweight. Moreover, when trained on a class of\nshapes, our approach demonstrates remarkable generalization across different\nobject categories. Equipped with this novel network, we explore three main\ntasks: refining an approximate shape correspondence, unsupervised deformation\nand mapping, and shape editing. Our code is made available at\nhttps://github.com/sentient07/LJN\n",
        "pdf_link": "http://arxiv.org/pdf/2410.08225v1"
    },
    {
        "title": "NePHIM: A Neural Physics-Based Head-Hand Interaction Model",
        "authors": [
            "Nicolas Wagner",
            "Mario Botsch",
            "Ulrich Schwanecke"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Due to the increasing use of virtual avatars, the animation of head-hand\ninteractions has recently gained attention. To this end, we present a novel\nvolumetric and physics-based interaction simulation. In contrast to previous\nwork, our simulation incorporates temporal effects such as collision paths,\nrespects anatomical constraints, and can detect and simulate skin pulling. As a\nresult, we can achieve more natural-looking interaction animations and take a\nstep towards greater realism. However, like most complex and computationally\nexpensive simulations, ours is not real-time capable even on high-end machines.\nTherefore, we train small and efficient neural networks as accurate\napproximations that achieve about 200 FPS on consumer GPUs, about 50 FPS on\nCPUs, and are learned in less than four hours for one person. In general, our\nfocus is not to generalize the approximation networks to low-resolution head\nmodels but to adapt them to more detailed personalized avatars. Nevertheless,\nwe show that these networks can learn to approximate our head-hand interaction\nmodel for multiple identities while maintaining computational efficiency.\n  Since the quality of the simulations can only be judged subjectively, we\nconducted a comprehensive user study which confirms the improved realism of our\napproach. In addition, we provide extensive visual results and inspect the\nneural approximations quantitatively. All data used in this work has been\nrecorded with a multi--view camera rig and will be made available upon\npublication. We will also publish relevant implementations.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.13503v1"
    },
    {
        "title": "Hybrid Voxel Formats for Efficient Ray Tracing",
        "authors": [
            "Russel Arbore",
            "Jeffrey Liu",
            "Aidan Wefel",
            "Steven Gao",
            "Eric Shaffer"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Voxels are a geometric representation used for rendering volumes,\nmulti-resolution models, and indirect lighting effects. Since the memory\nconsumption of uncompressed voxel volumes scales cubically with resolution,\npast works have introduced data structures for exploiting spatial sparsity and\nhomogeneity to compress volumes and accelerate ray tracing. However, these\nworks don't systematically evaluate the trade-off between compression and ray\nintersection performance for a variety of storage formats. We show that a\nhierarchical combination of voxel formats can achieve Pareto optimal trade-offs\nbetween memory consumption and rendering speed. We present a formulation of\n\"hybrid\" voxel formats, where each level of a hierarchical format can have a\ndifferent structure. For evaluation, we implement a metaprogramming system to\nautomatically generate construction and ray intersection code for arbitrary\nhybrid formats. We also identify transformations on these formats that can\nimprove compression and rendering performance. We evaluate this system with\nseveral models and hybrid formats, demonstrating that compared to standalone\nbase formats, hybrid formats achieve a new Pareto frontier in ray intersection\nperformance and storage cost.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.14128v1"
    },
    {
        "title": "Navigating the Digital Chain in Concrete 3D Printing",
        "authors": [
            "Ali El Hage",
            "Elodie Paquet",
            "Thibault Neu",
            "Philippe Poullain",
            "Ali-Nordine Leklou"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The advancement of concrete 3D printing (C3DP) technology has revolutionized\nthe construction industry, offering unique opportunities for innovation and\nefficiency. At the heart of this process lies a comprehensive digital chain\nthat integrates various stages, from initial design to post-processing. This\narticle provides an overview of this digital chain, explaining each crucial\nstep. The chain begins with design, utilizing Design for Additive Manufacturing\n(DFAM) concept and parametric modeling to create optimized structures. Path\ngeneration follows, determining the precise toolpath for extruding concrete\nlayers. Simulations, both numerical and analytical, ensure the design's\nintegrity and feasibility. Several articles have addressed parametric modeling,\nprocess and numerical simulation, and the post-processing phase. However, none\nhas proposed an updated methodology for the workflow. This study aims to\npropose a robust digital chain for C3DP technology, using one platform\n(3Dexperience) and seamless data transfer between applications. These steps\nprovide insights into the structural performance of printed components,\nenabling necessary adjustments and optimizations. In essence, the digital chain\ncoordinates a seamless workflow that transforms digital designs into concrete\nstructures, unlocking the full potential of C3DP and paving the way for\ninnovative and efficient construction.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16319v1"
    },
    {
        "title": "Recent consumer OLED monitors can be suitable for vision science",
        "authors": [
            "Tarek Abu Haila",
            "Korbinian Kunst",
            "Tran Quoc Khanh",
            "Thomas S. A. Wallis"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Vision science imposes rigorous requirements for the design and execution of\npsychophysical studies and experiments. These requirements ensure precise\ncontrol over variables, accurate measurement of perceptual responses, and\nreproducibility of results, which are essential for investigating visual\nperception and its underlying mechanisms. Since different experiments have\ndifferent requirements, not all aspects of a display system are critical for a\ngiven setting. Therefore, some display systems may be suitable for certain\ntypes of experiments but unsuitable for others. An additional challenge is that\nthe performance of consumer systems is often highly dependent on specific\nmonitor settings and firmware behavior. Here, we evaluate the performance of\nfour display systems: a consumer LCD gaming monitor, a consumer OLED gaming\nmonitor, a consumer OLED TV, and a VPixx PROPixx projector system. To allow the\nreader to assess the suitability of these systems for different experiments, we\npresent a range of different metrics: luminance behavior, luminance uniformity\nacross display surface, estimated gamma values and linearity, channel\nadditivity, channel dependency, color gamut, pixel response time, and pixel\nwaveform. In addition, we exhaustively report the monitor firmware settings\nused. Our analyses show that current consumer-level OLED display systems are\npromising, and adequate to fulfill the requirements of some critical vision\nscience experiments, allowing laboratories to run their experiments even\nwithout investing in high-quality professional display systems. For example,\nthe tested Asus OLED gaming monitor shows excellent response time, a sharp\nsquare waveform even at 240 Hz, a color gamut that covers 94% of DCI-P3 color\nspace, and the best luminance uniformity among all four tested systems, making\nit a favorable option on price-to-performance ratio.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.17019v1"
    },
    {
        "title": "EON: A practical energy-preserving rough diffuse BRDF",
        "authors": [
            "Jamie Portsmouth",
            "Peter Kutz",
            "Stephen Hill"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce the \"Energy-preserving Oren--Nayar\" (EON) model for reflection\nfrom rough surfaces. Unlike the popular qualitative Oren--Nayar model (QON) and\nits variants, our model is energy-preserving via analytical energy\ncompensation. We include self-contained GLSL source code for efficient\nevaluation of the new model and importance sampling based on a novel technique\nwe term \"Clipped Linearly Transformed Cosine\" (CLTC) sampling.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.18026v2"
    },
    {
        "title": "Path Guiding for Monte Carlo PDE Solvers",
        "authors": [
            "Tianyu Huang",
            "Jingwang Ling",
            "Shuang Zhao",
            "Feng Xu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In recent years, Monte Carlo PDE solvers have garnered increasing attention\nin computer graphics, demonstrating value across a wide range of applications.\nDespite offering clear advantages over traditional methods-such as avoiding\ndiscretization and enabling local evaluations-Monte Carlo PDE solvers face\nchallenges due to their stochastic nature, including high variance and slow\nconvergence rates. To mitigate the variance issue, we draw inspiration from\nMonte Carlo path tracing and apply the path guiding technique to the Walk on\nStars estimator. Specifically, we examine the target sampling distribution at\neach step of the Walk on Stars estimator, parameterize it, and introduce neural\nimplicit representations to model the spatially-varying guiding distribution.\nThis path guiding approach is implemented in a wavefront-style PDE solver, and\nexperimental results demonstrate that it effectively reduces variance in Monte\nCarlo PDE solvers.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.18944v1"
    },
    {
        "title": "MARS: Multi-sample Allocation through Russian roulette and Splitting",
        "authors": [
            "Joshua Meyer",
            "Alexander Rath",
            "Ömercan Yazici",
            "Philipp Slusallek"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Multiple importance sampling (MIS) is an indispensable tool in rendering that\nconstructs robust sampling strategies by combining the respective strengths of\nindividual distributions. Its efficiency can be greatly improved by carefully\nselecting the number of samples drawn from each distribution, but automating\nthis process remains a challenging problem. Existing works are mostly limited\nto mixture sampling, in which only a single sample is drawn in total, and the\nworks that do investigate multi-sample MIS only optimize the sample counts at a\nper-pixel level, which cannot account for variations beyond the first bounce.\nRecent work on Russian roulette and splitting has demonstrated how fixed-point\nschemes can be used to spatially vary sample counts to optimize image\nefficiency but is limited to choosing the same number of samples across all\nsampling strategies. Our work proposes a highly flexible sample allocation\nstrategy that bridges the gap between these areas of work. We show how to\niteratively optimize the sample counts to maximize the efficiency of the\nrendered image using a lightweight data structure, which allows us to make\nlocal and individual decisions per technique. We demonstrate the benefits of\nour approach in two applications, path guiding and bidirectional path tracing,\nin both of which we achieve consistent and substantial speedups over the\nrespective previous state-of-the-art.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.20429v1"
    },
    {
        "title": "LoDAvatar: Hierarchical Embedding and Adaptive Levels of Detail with\n  Gaussian Splatting for Enhanced Human Avatars",
        "authors": [
            "Xiaonuo Dongye",
            "Hanzhi Guo",
            "Le Luo",
            "Haiyan Jiang",
            "Yihua Bao",
            "Zeyu Tian",
            "Dongdong Weng"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  With the advancement of virtual reality, the demand for 3D human avatars is\nincreasing. The emergence of Gaussian Splatting technology has enabled the\nrendering of Gaussian avatars with superior visual quality and reduced\ncomputational costs. Despite numerous methods researchers propose for\nimplementing drivable Gaussian avatars, limited attention has been given to\nbalancing visual quality and computational costs. In this paper, we introduce\nLoDAvatar, a method that introduces levels of detail into Gaussian avatars\nthrough hierarchical embedding and selective detail enhancement methods. The\nkey steps of LoDAvatar encompass data preparation, Gaussian embedding, Gaussian\noptimization, and selective detail enhancement. We conducted experiments\ninvolving Gaussian avatars at various levels of detail, employing both\nobjective assessments and subjective evaluations. The outcomes indicate that\nincorporating levels of detail into Gaussian avatars can decrease computational\ncosts during rendering while upholding commendable visual quality, thereby\nenhancing runtime frame rates. We advocate adopting LoDAvatar to render\nmultiple dynamic Gaussian avatars or extensive Gaussian scenes to balance\nvisual quality and computational costs.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.20789v1"
    },
    {
        "title": "A Practical Style Transfer Pipeline for 3D Animation: Insights from\n  Production R&D",
        "authors": [
            "Hideki Todo",
            "Yuki Koyama",
            "Kunihiro Sakai",
            "Akihiro Komiya",
            "Jun Kato"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Our animation studio has developed a practical style transfer pipeline for\ncreating stylized 3D animation, which is suitable for complex real-world\nproduction. This paper presents the insights from our development process,\nwhere we explored various options to balance quality, artist control, and\nworkload, leading to several key decisions. For example, we chose patch-based\ntexture synthesis over machine learning for better control and to avoid\ntraining data issues. We also addressed specifying style exemplars, managing\nmultiple colors within a scene, controlling outlines and shadows, and reducing\ntemporal noise. These insights were used to further refine our pipeline,\nultimately enabling us to produce an experimental short film showcasing various\nstyles.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.24123v1"
    },
    {
        "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
        "authors": [
            "John Whitington"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.00131v1"
    },
    {
        "title": "ITS: Implicit Thin Shell for Polygonal Meshes",
        "authors": [
            "Huibiao Wen",
            "Lei Wang",
            "Yunxiao Zhang",
            "Shuangmin Chen",
            "Shiqing Xin",
            "Chongyang Deng",
            "Ying He",
            "Wenping Wang",
            "Changhe Tu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In computer graphics, simplifying a polygonal mesh surface~$\\mathcal{M}$ into\na geometric proxy that maintains close conformity to~$\\mathcal{M}$ is crucial,\nas it can significantly reduce computational demands in various applications.\nIn this paper, we introduce the Implicit Thin Shell~(ITS), a concept designed\nto implicitly represent the sandwich-walled space surrounding~$\\mathcal{M}$,\ndefined as~$\\{\\textbf{x}\\in\\mathbb{R}^3|\\epsilon_1\\leq f(\\textbf{x}) \\leq\n\\epsilon_2, \\epsilon_1< 0, \\epsilon_2>0\\}$. Here, $f$ is an approximation of\nthe signed distance function~(SDF) of~$\\mathcal{M}$, and we aim to minimize the\nthickness~$\\epsilon_2-\\epsilon_1$. To achieve a balance between mathematical\nsimplicity and expressive capability in~$f$, we employ a tri-variate\ntensor-product B-spline to represent~$f$. This representation is coupled with\nadaptive knot grids that adapt to the inherent shape variations\nof~$\\mathcal{M}$, while restricting~$f$'s basis functions to the first degree.\nIn this manner, the analytical form of~$f$ can be rapidly determined by solving\na sparse linear system. Moreover, the process of identifying the extreme values\nof~$f$ among the infinitely many points on~$\\mathcal{M}$ can be simplified to\nseeking extremes among a finite set of candidate points. By exhausting the\ncandidate points, we find the extreme values~$\\epsilon_1<0$ and $\\epsilon_2>0$\nthat minimize the thickness. The constructed ITS is guaranteed to\nwrap~$\\mathcal{M}$ rigorously, without any intersections between the bounding\nsurfaces and~$\\mathcal{M}$. ITS offers numerous potential applications thanks\nto its rigorousness, tightness, expressiveness, and computational efficiency.\nWe demonstrate the efficacy of ITS in rapid inside-outside tests and in mesh\nsimplification through the control of global error.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.01488v1"
    },
    {
        "title": "Learning Uniformly Distributed Embedding Clusters of Stylistic Skills\n  for Physically Simulated Characters",
        "authors": [
            "Nian Liu",
            "Libin Liu",
            "Zilong Zhang",
            "Zi Wang",
            "Hongzhao Xie",
            "Tengyu Liu",
            "Xinyi Tong",
            "Yaodong Yang",
            "Zhaofeng He"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Learning natural and diverse behaviors from human motion datasets remains\nchallenging in physics-based character control. Existing conditional\nadversarial models often suffer from tight and biased embedding distributions\nwhere embeddings from the same motion are closely grouped in a small area and\nshorter motions occupy even less space. Our empirical observations indicate\nthis limits the representational capacity and diversity under each skill. An\nideal latent space should be maximally packed by all motion's embedding\nclusters. In this paper, we propose a skill-conditioned controller that learns\ndiverse skills with expressive variations. Our approach leverages the Neural\nCollapse phenomenon, a natural outcome of the classification-based encoder, to\nuniformly distributed cluster centers. We additionally propose a novel\nEmbedding Expansion technique to form stylistic embedding clusters for diverse\nskills that are uniformly distributed on a hypersphere, maximizing the\nrepresentational area occupied by each skill and minimizing unmapped regions.\nThis maximally packed and uniformly distributed embedding space ensures that\nembeddings within the same cluster generate behaviors conforming to the\ncharacteristics of the corresponding motion clips, yet exhibiting noticeable\nvariations within each cluster. Compared to existing methods, our controller\nnot only generates high-quality, diverse motions covering the entire dataset\nbut also achieves superior controllability, motion coverage, and diversity\nunder each skill. Both qualitative and quantitative results confirm these\ntraits, enabling our controller to be applied to a wide range of downstream\ntasks and serving as a cornerstone for diverse applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.06459v1"
    },
    {
        "title": "Towards Voronoi Diagrams of Surface Patches",
        "authors": [
            "Pengfei Wang",
            "Jiantao Song",
            "Lei Wang",
            "Shiqing Xin",
            "Dongming Yan",
            "Shuangmin Chen",
            "Changhe Tu",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Extraction of a high-fidelity 3D medial axis is a crucial operation in CAD.\nWhen dealing with a polygonal model as input, ensuring accuracy and tidiness\nbecomes challenging due to discretization errors inherent in the mesh surface.\nCommonly, existing approaches yield medial-axis surfaces with various\nartifacts, including zigzag boundaries, bumpy surfaces, unwanted spikes, and\nnon-smooth stitching curves. Considering that the surface of a CAD model can be\neasily decomposed into a collection of surface patches, its 3D medial axis can\nbe extracted by computing the Voronoi diagram of these surface patches, where\neach surface patch serves as a generator. However, no solver currently exists\nfor accurately computing such an extended Voronoi diagram. Under the assumption\nthat each generator defines a linear distance field over a sufficiently small\nrange, our approach operates by tetrahedralizing the region of interest and\ncomputing the medial axis within each tetrahedral element. Just as\nSurfaceVoronoi computes surface-based Voronoi diagrams by cutting a 3D prism\nwith 3D planes (each plane encodes a linear field in a triangle), the key\noperation in this paper is to conduct the hyperplane cutting process in 4D,\nwhere each hyperplane encodes a linear field in a tetrahedron. In comparison\nwith the state-of-the-art, our algorithm produces better outcomes. Furthermore,\nit can also be used to compute the offset surface.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.06471v1"
    },
    {
        "title": "ScribGen: Generating Scribble Art Through Metaheuristics",
        "authors": [
            "Soumyaratna Debnath",
            "Ashish Tiwari",
            "Shanmuganathan Raman"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Art has long been a medium for individuals to engage with the world. Scribble\nart, a form of abstract visual expression, features spontaneous, gestural\nstrokes made with pens or brushes. These dynamic and expressive compositions,\ncreated quickly and impulsively, reveal intricate patterns and hidden meanings\nupon closer inspection. While scribble art is often associated with spontaneous\nexpression and experimentation, it can also be planned and intentional. Some\nartists use scribble techniques as a starting point for their creative process,\nexploring the possibilities of line, shape, and texture before refining their\nwork into more polished compositions. From ancient cave paintings to modern\nabstract sketches and doodles, scribble art has evolved with civilizations,\nreflecting diverse artistic movements and cultural influences. This evolution\nhighlights its universal appeal, transcending language and cultural barriers\nand connecting people through the shared experience of creating art.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.08673v1"
    },
    {
        "title": "On integer sequences for rendering limit sets of Kleinian groups",
        "authors": [
            "Alessandro Rosa"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a technique for rendering limit sets for kleinian groups, based\nupon the base transformation of integers and which aims at saving memory\nresources and being faster than the traditional dictionary based approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.08818v1"
    },
    {
        "title": "NeuMaDiff: Neural Material Synthesis via Hyperdiffusion",
        "authors": [
            "Chenliang Zhou",
            "Zheyuan Hu",
            "Alejandro Sztrajman",
            "Yancheng Cai",
            "Yaru Liu",
            "Cengiz Oztireli"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  High-quality material synthesis is essential for replicating complex surface\nproperties to create realistic digital scenes. However, existing methods often\nsuffer from inefficiencies in time and memory, require domain expertise, or\ndemand extensive training data, with high-dimensional material data further\nconstraining performance. Additionally, most approaches lack multi-modal\nguidance capabilities and standardized evaluation metrics, limiting control and\ncomparability in synthesis tasks. To address these limitations, we propose\nNeuMaDiff, a novel neural material synthesis framework utilizing\nhyperdiffusion. Our method employs neural fields as a low-dimensional\nrepresentation and incorporates a multi-modal conditional hyperdiffusion model\nto learn the distribution over material weights. This enables flexible guidance\nthrough inputs such as material type, text descriptions, or reference images,\nproviding greater control over synthesis. To support future research, we\ncontribute two new material datasets and introduce two BRDF distributional\nmetrics for more rigorous evaluation. We demonstrate the effectiveness of\nNeuMaDiff through extensive experiments, including a novel statistics-based\nconstrained synthesis approach, which enables the generation of materials of\ndesired categories.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.12015v1"
    },
    {
        "title": "Elastic Shape Registration of Surfaces in 3D Space with Gradient Descent\n  and Dynamic Programming",
        "authors": [
            "Javier Bernal",
            "Jim Lawrence"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Algorithms based on gradient descent for computing the elastic shape\nregistration of two simple surfaces in 3-dimensional space and therefore the\nelastic shape distance between them have been proposed by Kurtek, Jermyn, et\nal., and more recently by Riseth. Their algorithms are designed to minimize a\ndistance function between the surfaces by rotating and reparametrizing one of\nthe surfaces, the minimization for reparametrizing based on a gradient descent\napproach that may terminate at a local solution. On the other hand, Bernal and\nLawrence have proposed a similar algorithm, the minimization for\nreparametrizing based on dynamic programming thus producing a partial not\nnecessarily optimal elastic shape registration of the surfaces. Accordingly,\nBernal and Lawrence have proposed to use the rotation and reparametrization\ncomputed with their algorithm as the initial solution to any algorithm based on\na gradient descent approach for reparametrizing. Here we present results from\ndoing exactly that. We also describe and justify the gradient descent approach\nthat is used for reparametrizing one of the surfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.12743v1"
    },
    {
        "title": "Single Edge Collapse Quad-Dominant Mesh Reduction",
        "authors": [
            "Julian Knodt"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Mesh reduction using quadric error metrics is the industry standard for\nproducing level-of-detail (LOD) geometry for meshes. Although industry tools\nproduce visually excellent LODs, mesh topology is often ruined during\ndecimation. This is because tools focus on triangle simplification and\npreserving rendered appearance, whereas artists often produce quad dominant\nmeshes with clean edge topology. Artist created manual LODs preserve both\nappearance and quad topology. Furthermore, most existing tools for quad\ndecimation only accept pure quad meshes and cannot handle any triangles. The\ngap between quad and triangular mesh decimation is because they are built on\nfundamentally different operations, triangle simplification uses single edge\ncollapses, whereas quad decimation requires that entire sets of edges be\ncollapsed atomically. In this work, we demonstrate that single edge collapse\ncan be used to preserve most input quads without degrading geometric quality.\nSingle edge collapse quad preservation is made possible by introducing\ndihedral-angle weighted quadrics for every edges, allowing optimization to\nevenly space edges while preserving features. It is further enabled by\nexplicitly ordering edge collapses with nearly equivalent quadric error that\npreserves quad topology. In addition to quad preservation, we demonstrate that\nby introducing weights for quadrics on certain edges, our framework can be used\nto preserve symmetry and joint influences. To demonstrate our approach is\nsuitable for skinned mesh decimation (a key use case of quad meshes), we show\nthat QEM with attributes can preserve joint influences better than prior work.\nOn both static and animated meshes, our approach consistently outperforms prior\nwork with lower Chamfer and Hausdorff distance, while preserving more quad\ntopology.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.16874v1"
    },
    {
        "title": "Higher-order Differentiable Rendering",
        "authors": [
            "Zican Wang",
            "Michael Fischer",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We derive methods to compute higher order differentials (Hessians and\nHessian-vector products) of the rendering operator. Our approach is based on\nimportance sampling of a convolution that represents the differentials of\nrendering parameters and shows to be applicable to both rasterization and path\ntracing. We further suggest an aggregate sampling strategy to importance-sample\nmultiple dimensions of one convolution kernel simultaneously. We demonstrate\nthat this information improves convergence when used in higher-order optimizers\nsuch as Newton or Conjugate Gradient relative to a gradient descent baseline in\nseveral inverse rendering tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.03489v2"
    },
    {
        "title": "Optimizing Parameters for Static Equilibrium of Discrete Elastic Rods\n  with Active-Set Cholesky",
        "authors": [
            "Tetsuya Takahashi",
            "Christopher Batty"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a parameter optimization method for achieving static equilibrium\nof discrete elastic rods. Our method simultaneously optimizes material\nstiffness and rest shape parameters under box constraints to exactly enforce\nzero net force while avoiding stability issues and violations of physical laws.\nFor efficiency, we split our constrained optimization problem into primal and\ndual subproblems via the augmented Lagrangian method, while handling the dual\nsubproblem via simple vector updates. To efficiently solve the box-constrained\nprimal subproblem, we propose a new active-set Cholesky preconditioner. Our\nmethod surpasses prior work in generality, robustness, and speed.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.16461v1"
    },
    {
        "title": "Augmented Mass-Spring model for Real-Time Dense Hair Simulation",
        "authors": [
            "Jorge Alejandro Amador Herrera",
            "Yi Zhou",
            "Xin Sun",
            "Zhixin Shu",
            "Chengan He",
            "Sören Pirk",
            "Dominik L. Michels"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a novel Augmented Mass-Spring (AMS) model for real-time simulation\nof dense hair at strand level. Our approach considers the traditional edge,\nbending, and torsional degrees of freedom in mass-spring systems, but\nincorporates an additional one-way biphasic coupling with a ghost rest-shape\nconfiguration. Trough multiple evaluation experiments with varied dynamical\nsettings, we show that AMS improves the stability of the simulation in\ncomparison to mass-spring discretizations, preserves global features, and\nenables the simulation of non-Hookean effects. Using an heptadiagonal\ndecomposition of the resulting matrix, our approach provides the efficiency\nadvantages of mass-spring systems over more complex constitutive hair models,\nwhile enabling a more robust simulation of multiple strand configurations.\nFinally, our results demonstrate that our framework enables the generation,\ncomplex interactivity, and editing of simulation-ready dense hair assets in\nreal-time. More details can be found on our project page:\nhttps://agrosamad.github.io/AMS/.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.17144v2"
    },
    {
        "title": "Codimensional MultiMeshing: Synchronizing the Evolution of Multiple\n  Embedded Geometries",
        "authors": [
            "Michael Tao",
            "Jiacheng Dai",
            "Denis Zorin",
            "Teseo Schneider",
            "Daniele Panozzo"
        ],
        "category": "cs.GR",
        "published_year": "2025",
        "summary": "  Complex geometric tasks such as geometric modeling, physical simulation, and\ntexture parametrization often involve the embedding of many complex sub-domains\nwith potentially different dimensions. These tasks often require evolving the\ngeometry and topology of the discretizations of these sub-domains, and\nguaranteeing a \\emph{consistent} overall embedding for the multiplicity of\nsub-domains is required to define boundary conditions. We propose a data\nstructure and algorithmic framework for hierarchically encoding a collection of\nmeshes, enabling topological and geometric changes to be automatically\npropagated with coherent correspondences between them. We demonstrate the\neffectiveness of our approach in surface mesh decimation while preserving UV\nseams, periodic 2D/3D meshing, and extending the TetWild algorithm to ensure\ntopology preservation of the embedded structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.01362v1"
    },
    {
        "title": "BundleFit: Display and See-Through Models for Augmented Reality\n  Head-Mounted Displays",
        "authors": [
            "Yufeng Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The head-mounted display is a vital component of augmented reality,\nincorporating optics with complex display and see-through optical behavior.\nComputationally modeling these optical behaviors requires meeting three key\ncriteria: accuracy, efficiency, and accessibility. In recent years, various\napproaches have been proposed to model display and see-through optics, which\ncan broadly be classified into black-box and white-box models. However, both\ncategories face significant limitations that hinder their adoption in\ncommercial applications. To overcome these challenges, we leveraged prior\nknowledge of ray bundle properties outside the optical hardware and proposed a\nnovel bundle-fit-based model. In this approach, the ray paths within the optics\nare treated as a black box, while a lightweight optimization problem is solved\nto fit the ray bundle outside the optics. This method effectively addresses the\naccuracy issues of black-box models and the accessibility challenges of\nwhite-box models. Although our model involves runtime optimization, this is\ntypically not a concern, as it can use the solution from a previous query to\ninitialize the optimization for the current query. We evaluated the performance\nof our proposed method through both simulations and experiments on real\nhardware, demonstrating its effectiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.01382v1"
    },
    {
        "title": "Data Parallel Visualization and Rendering on the RAMSES Supercomputer\n  with ANARI",
        "authors": [
            "Stefan Zellmann"
        ],
        "category": "cs.GR",
        "published_year": "2025",
        "summary": "  3D visualization and rendering in HPC are very heterogenous applications,\nthough fundamentally the tasks involved are well-defined and do not differ much\nfrom application to application. The Khronos Group's ANARI standard seeks to\nconsolidate 3D rendering across sci-vis applications. This paper makes an\neffort to convey challenges of 3D rendering and visualization with ANARI in the\ncontext of HPC, where the data does not fit within a single node or GPU but\nmust be distributed. It also provides a gentle introduction to parallel\nrendering concepts and challenges to practitioners from the field of HPC in\ngeneral. Finally, we present a case study showcasing data parallel rendering on\nthe new supercomputer RAMSES at the University of Cologne.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.01628v1"
    },
    {
        "title": "Exact computation of the color function for triangular element\n  interfaces",
        "authors": [
            "Jieyun Pan",
            "Désir-André Koffi Bi",
            "Ahmed Basil Kottilingal",
            "Serena Costanzo",
            "Jiacai Lu",
            "Yue Ling",
            "Ruben Scardovelli",
            "Grétar Tryggvason",
            "Stéphane Zaleski"
        ],
        "category": "cs.GR",
        "published_year": "2025",
        "summary": "  The calculation of the volume enclosed by curved surfaces discretized into\ntriangular elements, and a cube is of great importance in different domains,\nsuch as computer graphics and multiphase flow simulations. We propose a robust\nalgorithm, the Front2VOF (F2V) algorithm, to address this problem. The F2V\nalgorithm consists of two main steps. First, it identifies the polygons within\nthe cube by segmenting the triangular elements on the surface, retaining only\nthe portions inside the cube boundaries. Second, it computes the volume\nenclosed by these polygons in combination with the cube faces. To validate the\nalgorithm's accuracy and robustness, we tested it using a range of synthetic\nconfigurations with known analytical solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.04744v1"
    },
    {
        "title": "User software for the next generation",
        "authors": [
            "T. G. Worlton",
            "A. Chatterjee",
            "J. P. Hammonds",
            "P. F. Peterson",
            "D. J. Mikkelson",
            "R. L. Mikkelson"
        ],
        "category": "cs.GR",
        "published_year": "2002",
        "summary": "  New generations of neutron scattering sources and instrumentation are\nproviding challenges in data handling for user software. Time-of-Flight\ninstruments used at pulsed sources typically produce hundreds or thousands of\nchannels of data for each detector segment. New instruments are being designed\nwith thousands to hundreds of thousands of detector segments. High intensity\nneutron sources make possible parametric studies and texture studies which\nfurther increase data handling requirements. The Integrated Spectral Analysis\nWorkbench (ISAW) software developed at Argonne handles large numbers of spectra\nsimultaneously while providing operations to reduce, sort, combine and export\nthe data. It includes viewers to inspect the data in detail in real time. ISAW\nuses existing software components and packages where feasible and takes\nadvantage of the excellent support for user interface design and network\ncommunication in Java. The included scripting language simplifies repetitive\noperations for analyzing many files related to a given experiment. Recent\nadditions to ISAW include a contour view, a time-slice table view, routines for\nfinding and fitting peaks in data, and support for data from other facilities\nusing the NeXus format. In this paper, I give an overview of features and\nplanned improvements of ISAW. Details of some of the improvements are covered\nin other presentations at this conference.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0210018v1"
    },
    {
        "title": "Computing Conformal Structure of Surfaces",
        "authors": [
            "Xianfeng Gu",
            "Shing-Tung Yau"
        ],
        "category": "cs.GR",
        "published_year": "2002",
        "summary": "  This paper solves the problem of computing conformal structures of general\n2-manifolds represented as triangle meshes. We compute conformal structures in\nthe following way: first compute homology bases from simplicial complex\nstructures, then construct dual cohomology bases and diffuse them to harmonic\n1-forms. Next, we construct bases of holomorphic differentials. We then obtain\nperiod matrices by integrating holomorphic differentials along homology bases.\nWe also study the global conformal mapping between genus zero surfaces and\nspheres, and between general meshes and planes. Our method of computing\nconformal structures can be applied to tackle fundamental problems in computer\naid design and computer graphics, such as geometry classification and\nidentification, and surface global parametrization.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0212043v1"
    },
    {
        "title": "Practical and Robust Stenciled Shadow Volumes for Hardware-Accelerated\n  Rendering",
        "authors": [
            "Cass Everitt",
            "Mark J. Kilgard"
        ],
        "category": "cs.GR",
        "published_year": "2003",
        "summary": "  Twenty-five years ago, Crow published the shadow volume approach for\ndetermining shadowed regions in a scene. A decade ago, Heidmann described a\nhardware-accelerated stencil buffer-based shadow volume algorithm.\n  Unfortunately hardware-accelerated stenciled shadow volume techniques have\nnot been widely adopted by 3D games and applications due in large part to the\nlack of robustness of described techniques. This situation persists despite\nwidely available hardware support. Specifically what has been lacking is a\ntechnique that robustly handles various \"hard\" situations created by near or\nfar plane clipping of shadow volumes.\n  We describe a robust, artifact-free technique for hardware-accelerated\nrendering of stenciled shadow volumes. Assuming existing hardware, we resolve\nthe issues otherwise caused by shadow volume near and far plane clipping\nthrough a combination of (1) placing the conventional far clip plane \"at\ninfinity\", (2) rasterization with infinite shadow volume polygons via\nhomogeneous coordinates, and (3) adopting a zfail stencil-testing scheme. Depth\nclamping, a new rasterization feature provided by NVIDIA's GeForce3, preserves\nexisting depth precision by not requiring the far plane to be placed at\ninfinity. We also propose two-sided stencil testing to improve the efficiency\nof rendering stenciled shadow volumes.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0301002v1"
    },
    {
        "title": "Cg in Two Pages",
        "authors": [
            "Mark J. Kilgard"
        ],
        "category": "cs.GR",
        "published_year": "2003",
        "summary": "  Cg is a language for programming GPUs. This paper describes Cg briefly.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0302013v1"
    },
    {
        "title": "The Graphics Card as a Streaming Computer",
        "authors": [
            "Suresh Venkatasubramanian"
        ],
        "category": "cs.GR",
        "published_year": "2003",
        "summary": "  Massive data sets have radically changed our understanding of how to design\nefficient algorithms; the streaming paradigm, whether it in terms of number of\npasses of an external memory algorithm, or the single pass and limited memory\nof a stream algorithm, appears to be the dominant method for coping with large\ndata.\n  A very different kind of massive computation has had the same effect at the\nlevel of the CPU. The most prominent example is that of the computations\nperformed by a graphics card. The operations themselves are very simple, and\nrequire very little memory, but require the ability to perform many\ncomputations extremely fast and in parallel to whatever degree possible. What\nhas resulted is a stream processor that is highly optimized for stream\ncomputations. An intriguing side effect of this is the growing use of a\ngraphics card as a general purpose stream processing engine. In an\never-increasing array of applications, researchers are discovering that\nperforming a computation on a graphics card is far faster than performing it on\na CPU, and so are using a GPU as a stream co-processor.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0310002v2"
    },
    {
        "title": "Benchmarking and Implementation of Probability-Based Simulations on\n  Programmable Graphics Cards",
        "authors": [
            "S. Tomov",
            "M. McGuigan",
            "R. Bennett",
            "G. Smith",
            "J. Spiletic"
        ],
        "category": "cs.GR",
        "published_year": "2003",
        "summary": "  The latest Graphics Processing Units (GPUs) are reported to reach up to\n  200 billion floating point operations per second (200 Gflops) and to have\nprice performance of 0.1 cents per M flop. These facts raise great interest in\nthe plausibility of extending the GPUs' use to non-graphics applications, in\nparticular numerical simulations on structured grids (lattice).\n  We review previous work on using GPUs for non-graphics applications,\nimplement probability-based simulations on the GPU, namely the\n  Ising and percolation models, implement vector operation benchmarks for the\nGPU, and finally compare the CPU's and GPU's performance.\n  A general conclusion from the results obtained is that moving computations\nfrom the CPU to the GPU is feasible, yielding good time and price performance,\nfor certain lattice computations.\n  Preliminary results also show that it is feasible to use them in parallel\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0312006v1"
    },
    {
        "title": "Estimacao Temporal da Deformacao entre Objectos utilizando uma\n  Metodologia Fisica",
        "authors": [
            "Joao Manuel R. S. Tavares",
            "Raquel R. Pinho"
        ],
        "category": "cs.GR",
        "published_year": "2005",
        "summary": "  In this paper, it is presented a methodology to estimate the deformation\ninvolved between two objects attending to its physical properties. This\nmethodology can be used, for example, in Computational Vision or Computer\nGraphics applications, and consists in physically modeling the objects, by\nmeans of the Finite Elements Method, establishing correspondences between some\nof its data points, by using Modal Matching, and finally, determining the\ndisplacement field, that is the intermediate shapes, through the resolution of\nthe Lagrange Dynamic Equilibrium Equation. As in many of the possible\napplications of the methodology to present, it is necessary to quantify the\nexisting deformation, as well as to estimate only the non rigid component of\nthe involved global deformation. The solutions adopted to satisfy such\nintentions will be also presented.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0505043v2"
    },
    {
        "title": "Spatiotemporal sensistivity and visual attention for efficient rendering\n  of dynamic environments",
        "authors": [
            "Yang Li Hector Yee"
        ],
        "category": "cs.GR",
        "published_year": "2005",
        "summary": "  We present a method to accelerate global illumination computation in dynamic\nenvironments by taking advantage of limitations of the human visual system. A\nmodel of visual attention is used to locate regions of interest in a scene and\nto modulate spatiotemporal sensitivity. The method is applied in the form of a\nspatiotemporal error tolerance map. Perceptual acceleration combined with good\nsampling protocols provide a global illumination solution feasible for use in\nanimation. Results indicate an order of magnitude improvement in computational\nspeed. The method is adaptable and can also be used in image-based rendering,\ngeometry level of detail selection, realistic image synthesis, video telephony\nand video compression.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0511032v1"
    },
    {
        "title": "Mathematical models of the complex surfaces in simulation and\n  visualization systems",
        "authors": [
            "Dmitry P. Paukov"
        ],
        "category": "cs.GR",
        "published_year": "2005",
        "summary": "  Modeling, simulation and visualization of three-dimension complex bodies\nwidely use mathematical model of curves and surfaces. The most important curves\nand surfaces for these purposes are curves and surfaces in Hermite and Bezier\nforms, splines and NURBS. Article is devoted to survey this way to use\ngeometrical data in various computer graphics systems and adjacent fields.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0512098v1"
    },
    {
        "title": "Outlier Robust ICP for Minimizing Fractional RMSD",
        "authors": [
            "Jeff M. Phillips",
            "Ran Liu",
            "Carlo Tomasi"
        ],
        "category": "cs.GR",
        "published_year": "2006",
        "summary": "  We describe a variation of the iterative closest point (ICP) algorithm for\naligning two point sets under a set of transformations. Our algorithm is\nsuperior to previous algorithms because (1) in determining the optimal\nalignment, it identifies and discards likely outliers in a statistically robust\nmanner, and (2) it is guaranteed to converge to a locally optimal solution. To\nthis end, we formalize a new distance measure, fractional root mean squared\ndistance (frmsd), which incorporates the fraction of inliers into the distance\nfunction. We lay out a specific implementation, but our framework can easily\nincorporate most techniques and heuristics from modern registration algorithms.\nWe experimentally validate our algorithm against previous techniques on 2 and 3\ndimensional data exposed to a variety of outlier types.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0606098v1"
    },
    {
        "title": "User driven applications - new design paradigm",
        "authors": [
            "Sergey Andreyev"
        ],
        "category": "cs.GR",
        "published_year": "2007",
        "summary": "  Programs for complicated engineering and scientific tasks always have to deal\nwith a problem of showing numerous graphical results. The limits of the screen\nspace and often opposite requirements from different users are the cause of the\ninfinite discussions between designers and users, but the source of this\nongoing conflict is not in the level of interface design, but in the basic\nprinciple of current graphical output: user may change some views and details,\nbut in general the output view is absolutely defined and fixed by the\ndeveloper. Author was working for several years on the algorithm that will\nallow eliminating this problem thus allowing stepping from designer-driven\napplications to user-driven. Such type of applications in which user is\ndeciding what, when and how to show on the screen, is the dream of scientists\nand engineers working on the analysis of the most complicated tasks. The new\nparadigm is based on movable and resizable graphics, and such type of graphics\ncan be widely used not only for scientific and engineering applications.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.4224v1"
    },
    {
        "title": "Design of moveable and resizable graphics",
        "authors": [
            "Sergey Andreyev"
        ],
        "category": "cs.GR",
        "published_year": "2007",
        "summary": "  We are communicating with computers on two different levels. On upper level\nwe have a very flexible system of windows: we can move them, resize, overlap or\nput side by side. At any moment we decide what would be the best view and\nreorganize the whole view easily. Then we start any application, go to the\ninner level, and everything changes. Here we are stripped of all the\nflexibility and can work only inside the scenario, developed by the designer of\nthe program. Interface will allow us to change some tiny details, but in\ngeneral everything is fixed: graphics is neither moveable, nor resizable, and\nthe same with controls. Author designed an extremely powerful mechanism of\nturning graphical objects and controls into moveable and resizable. This can\nnot only significantly improve the existing applications, but this will bring\nthe applications to another level. (To estimate the possible difference, try to\nimagine the Windows system without its flexibility and compare it with the\ncurrent one.) This article explains in details the construction and use of\nmoveable and resizable graphical objects.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.3553v1"
    },
    {
        "title": "Size matters: performance declines if your pixels are too big or too\n  small",
        "authors": [
            "Vassilis Kostakos",
            "Eamonn O'Neill"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  We present a conceptual model that describes the effect of pixel size on\ntarget acquisition. We demonstrate the use of our conceptual model by applying\nit to predict and explain the results of an experiment to evaluate users'\nperformance in a target acquisition task involving three distinct display\nsizes: standard desktop, small and large displays. The results indicate that\nusers are fastest on standard desktop displays, undershoots are the most common\nerror on small displays and overshoots are the most common error on large\ndisplays. We propose heuristics to maintain usability when changing displays.\nFinally, we contribute to the growing body of evidence that amplitude does\naffect performance in a display-based pointing task.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.3103v1"
    },
    {
        "title": "Perspective Drawing of Surfaces with Line Hidden Line Elimination,\n  Dibujando Superficies En Perspectiva Con Eliminacion De Lineas Ocultas",
        "authors": [
            "Ignacio Vega-Paez",
            "Jose Angel Ortega",
            "Georgina G. Pulido"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  An efficient computer algorithm is described for the perspective drawing of a\nwide class of surfaces. The class includes surfaces corresponding lo\nsingle-valued, continuous functions which are defined over rectangular domains.\nThe algorithm automatically computes and eliminates hidden lines. The number of\ncomputations in the algorithm grows linearly with the number of sample points\non the surface to be drawn. An analysis of the algorithm is presented, and\nextensions lo certain multi-valued functions are indicated. The algorithm is\nimplemented and tested on .Net 2.0 platform that left interactive use. Running\ntimes are found lo be exceedingly efficient for visualization, where\ninteraction on-line and view-point control, enables effective and rapid\nexamination of a surfaces from many perspectives.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.4093v2"
    },
    {
        "title": "Visualization Optimization : Application to the RoboCup Rescue Domain",
        "authors": [
            "Pedro Miguel Moreira",
            "Luís Paulo Reis",
            "António Augusto de Sousa"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  In this paper we demonstrate the use of intelligent optimization\nmethodologies on the visualization optimization of virtual / simulated\nenvironments. The problem of automatic selection of an optimized set of views,\nwhich better describes an on-going simulation over a virtual environment is\naddressed in the context of the RoboCup Rescue Simulation domain. A generic\narchitecture for optimization is proposed and described. We outline the\npossible extensions of this architecture and argue on how several problems\nwithin the fields of Interactive Rendering and Visualization can benefit from\nit.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.2021v1"
    },
    {
        "title": "GPU-Based Interactive Visualization of Billion Point Cosmological\n  Simulations",
        "authors": [
            "Tamas Szalay",
            "Volker Springel",
            "Gerard Lemson"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  Despite the recent advances in graphics hardware capabilities, a brute force\napproach is incapable of interactively displaying terabytes of data. We have\nimplemented a system that uses hierarchical level-of-detailing for the results\nof cosmological simulations, in order to display visually accurate results\nwithout loading in the full dataset (containing over 10 billion points). The\nguiding principle of the program is that the user should not be able to\ndistinguish what they are seeing from a full rendering of the original data.\nFurthermore, by using a tree-based system for levels of detail, the size of the\nunderlying data is limited only by the capacity of the IO system containing it.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.2055v2"
    },
    {
        "title": "String Art: Circle Drawing Using Straight Lines",
        "authors": [
            "Sankar K",
            "Sarad AV"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  An algorithm to generate the locus of a circle using the intersection points\nof straight lines is proposed. The pixels on the circle are plotted independent\nof one another and the operations involved in finding the locus of the circle\nfrom the intersection of straight lines are parallelizable. Integer only\narithmetic and algorithmic optimizations are used for speedup. The proposed\nalgorithm makes use of an envelope to form a parabolic arc which is consequent\ntransformed into a circle. The use of parabolic arcs for the transformation\nresults in higher pixel errors as the radius of the circle to be drawn\nincreases. At its current state, the algorithm presented may be suitable only\nfor generating circles for string art.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.4121v1"
    },
    {
        "title": "The Good, the Bad, and the Ugly: three different approaches to break\n  their watermarking system",
        "authors": [
            "Gaëtan Le Guelvouit",
            "Teddy Furon",
            "François Cayre"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  The Good is Blondie, a wandering gunman with a strong personal sense of\nhonor. The Bad is Angel Eyes, a sadistic hitman who always hits his mark. The\nUgly is Tuco, a Mexican bandit who's always only looking out for himself.\nAgainst the backdrop of the BOWS contest, they search for a watermark in gold\nburied in three images. Each knows only a portion of the gold's exact location,\nso for the moment they're dependent on each other. However, none are\nparticularly inclined to share...\n",
        "pdf_link": "http://arxiv.org/pdf/0811.4681v1"
    },
    {
        "title": "An analysis of a random algorithm for estimating all the matchings",
        "authors": [
            "Jinshan Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  Counting the number of all the matchings on a bipartite graph has been\ntransformed into calculating the permanent of a matrix obtained from the\nextended bipartite graph by Yan Huo, and Rasmussen presents a simple approach\n(RM) to approximate the permanent, which just yields a critical ratio\nO($n\\omega(n)$) for almost all the 0-1 matrices, provided it's a simple\npromising practical way to compute this #P-complete problem. In this paper, the\nperformance of this method will be shown when it's applied to compute all the\nmatchings based on that transformation. The critical ratio will be proved to be\nvery large with a certain probability, owning an increasing factor larger than\nany polynomial of $n$ even in the sense for almost all the 0-1 matrices. Hence,\nRM fails to work well when counting all the matchings via computing the\npermanent of the matrix. In other words, we must carefully utilize the known\nmethods of estimating the permanent to count all the matchings through that\ntransformation.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.1119v1"
    },
    {
        "title": "On the Complexity of Smooth Spline Surfaces from Quad Meshes",
        "authors": [
            "Jorg Peters",
            "Jianhua Fan"
        ],
        "category": "cs.GR",
        "published_year": "2009",
        "summary": "  This paper derives strong relations that boundary curves of a smooth complex\nof patches have to obey when the patches are computed by local averaging. These\nrelations restrict the choice of reparameterizations for geometric continuity.\nIn particular, when one bicubic tensor-product B-spline patch is associated\nwith each facet of a quadrilateral mesh with n-valent vertices and we do not\nwant segments of the boundary curves forced to be linear, then the relations\ndictate the minimal number and multiplicity of knots: For general data, the\ntensor-product spline patches must have at least two internal double knots per\nedge to be able to model a G^1-conneced complex of C^1 splines. This lower\nbound on the complexity of any construction is proven to be sharp by suitably\ninterpreting an existing surface construction. That is, we have a tight bound\non the complexity of smoothing quad meshes with bicubic tensor-product B-spline\npatches.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.1226v1"
    },
    {
        "title": "A Neural Network Classifier of Volume Datasets",
        "authors": [
            "Dženan Zukić",
            "Christof Rezk-Salama",
            "Andreas Kolb"
        ],
        "category": "cs.GR",
        "published_year": "2009",
        "summary": "  Many state-of-the art visualization techniques must be tailored to the\nspecific type of dataset, its modality (CT, MRI, etc.), the recorded object or\nanatomical region (head, spine, abdomen, etc.) and other parameters related to\nthe data acquisition process. While parts of the information (imaging modality\nand acquisition sequence) may be obtained from the meta-data stored with the\nvolume scan, there is important information which is not stored explicitly\n(anatomical region, tracing compound). Also, meta-data might be incomplete,\ninappropriate or simply missing.\n  This paper presents a novel and simple method of determining the type of\ndataset from previously defined categories. 2D histograms based on intensity\nand gradient magnitude of datasets are used as input to a neural network, which\nclassifies it into one of several categories it was trained with. The proposed\nmethod is an important building block for visualization systems to be used\nautonomously by non-experts. The method has been tested on 80 datasets, divided\ninto 3 classes and a \"rest\" class.\n  A significant result is the ability of the system to classify datasets into a\nspecific class after being trained with only one dataset of that class. Other\nadvantages of the method are its easy implementation and its high computational\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.2274v1"
    },
    {
        "title": "Analyzing Midpoint Subdivision",
        "authors": [
            "Hartmut Prautzsch",
            "Qi Chen"
        ],
        "category": "cs.GR",
        "published_year": "2009",
        "summary": "  Midpoint subdivision generalizes the Lane-Riesenfeld algorithm for uniform\ntensor product splines and can also be applied to non regular meshes. For\nexample, midpoint subdivision of degree 2 is a specific Doo-Sabin algorithm and\nmidpoint subdivision of degree 3 is a specific Catmull-Clark algorithm. In\n2001, Zorin and Schroeder were able to prove C1-continuity for midpoint\nsubdivision surfaces analytically up to degree 9. Here, we develop general\nanalysis tools to show that the limiting surfaces under midpoint subdivision of\nany degree >= 2 are C1-continuous at their extraordinary points.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.5157v3"
    },
    {
        "title": "Computing Principal Components Dynamically",
        "authors": [
            "Darko Dimitrov",
            "Mathias Holst",
            "Christian Knauer",
            "Klaus Kriegel"
        ],
        "category": "cs.GR",
        "published_year": "2009",
        "summary": "  In this paper we present closed-form solutions for efficiently updating the\nprincipal components of a set of $n$ points, when $m$ points are added or\ndeleted from the point set. For both operations performed on a discrete point\nset in $\\mathbb{R}^d$, we can compute the new principal components in $O(m)$\ntime for fixed $d$. This is a significant improvement over the commonly used\napproach of recomputing the principal components from scratch, which takes\n$O(n+m)$ time. An important application of the above result is the dynamical\ncomputation of bounding boxes based on principal component analysis. PCA\nbounding boxes are very often used in many fields, among others in computer\ngraphics for collision detection and fast rendering. We have implemented and\nevaluated few algorithms for computing dynamically PCA bounding boxes in\n$\\mathbb{R}^3$. In addition, we present closed-form solutions for computing\ndynamically principal components of continuous point sets in $\\mathbb{R}^2$ and\n$\\mathbb{R}^3$. In both cases, discrete and continuous, to compute the new\nprincipal components, no additional data structures or storage are needed.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.5380v1"
    },
    {
        "title": "Teaching Physical Based Animation via OpenGL Slides",
        "authors": [
            "Miao Song",
            "Serguei A. Mokhov",
            "Peter Grogono"
        ],
        "category": "cs.GR",
        "published_year": "2009",
        "summary": "  This work expands further our earlier poster presentation and integration of\nthe OpenGL Slides Framework (OGLSF) - to make presentations with real-time\nanimated graphics where each slide is a scene with tidgets - and physical based\nanimation of elastic two-, three-layer softbody objects. The whole project is\nvery interactive, and serves dual purpose - delivering the teaching material in\na classroom setting with real running animated examples as well as releasing\nthe source code to the students to show how the actual working things are made.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.5494v1"
    },
    {
        "title": "Modelacion y Visualizacion Tridimensional Interactiva de Variables\n  Electricas en Celdas de Electro-Obtencion con Electrodos Bipolares",
        "authors": [
            "César Mena Labraña",
            "Ricardo Sánchez Schulz",
            "Lautaro Salazar Silva"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  The use of floating bipolar electrodes in electrowinning cells of copper\nconstitutes a nonconventional technology that promises economic and operational\nimpacts. This paper presents a computational tool for the simulation and\nanalysis of such electrochemical cells. A new model is developed for floating\nelectrodes and a method of finite difference is used to obtain the\nthreedimensional distribution of the potential and the field of current density\ninside the cell. The analysis of the results is based on a technique for the\ninteractive visualization of three-dimensional vectorial fields as lines of\nflow.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.3974v2"
    },
    {
        "title": "Aplicacion Grafica para el estudio de un Modelo de Celda Electrolitica\n  usando Tecnicas de Visualizacion de Campos Vectoriales",
        "authors": [
            "César Mena Labraña"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  The use of floating bipolar electrodes in electrowinning cells of copper\nconstitutes a nonconventional technology that promises economic and operational\nimpacts. This thesis presents a computational tool for the simulation and\nanalysis of such electrochemical cells. A new model is developed for floating\nelectrodes and a method of finite difference is used to obtain the\nthreedimensional distribution of the potential and the field of current density\ninside the cell. The analysis of the results is based on a technique for the\ninteractive visualization of three-dimensional vectorial fields as lines of\nflow.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.4002v1"
    },
    {
        "title": "From granular avalanches to fluid turbulences through oozing pastes. A\n  mesoscopic physically-based particle model",
        "authors": [
            "Annie Luciani"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  In this paper, we describe how we can precisely produce complex and various\ndynamic morphological features such as structured and chaotic features which\noccur in sand pilings (piles, avalanches, internal collapses, arches) , in\nflowing fluids (laminar flowing, Kelvin-Helmholtz and Von Karmann eddies), and\nin cohesive pastes (twist-and-turn oozing and packing) using only a single\nunified model, called \"mesoscopic model\". This model is a physically-based\nparticle model whose behavior depends on only four simple, but easy to\nunderstand, physically-based parameters : elasticity, viscosity and their local\nareas of influence. It is fast to compute and easy to understand by\nnon-physicist users.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.3190v1"
    },
    {
        "title": "A physically-based particle model of emergent crowd behaviors",
        "authors": [
            "Laure Heïgeas",
            "Annie Luciani",
            "Joëlle Thollot",
            "Nicolas Castagné"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  This paper presents a modeling process in order to produce a realistic\nsimulation of crowds in the ancient Greek agora of Argos. This place was a\nsocial theater in which two kinds of collective phenomena took place:\ninterpersonal interactions (small group discussion and negotiation, etc.) and\nglobal collective phenomena, such as flowing and jamming. In this paper, we\nfocus on the second type of collective human phenomena, called non-deliberative\nemergent crowd phenomena. This is a typical case of collective emergent\nself-organization. When a great number of individuals move within a confined\nenvironment and under a common fate, collective structures appear\nspontaneously: jamming with inner collapses, organized flowing with queues,\ncurls, and vortices, propagation effects, etc. These are particularly relevant\nfeatures to enhance the realism - more precisely the \"truthfulness\" - of models\nof this kind of collective phenomena. We assume that this truthfulness is\nstrongly associated with the concept of emergence: evolutions are not\npredetermined by the individual characters, but emerge from the interaction of\nnumerous characters. The evolutions are not repetitive, and evolve on the basis\nof small changes. This paper demonstrates that the physically-based interacting\nparticles system is an adequate candidate to model emergent crowd effects: it\nassociates a large number of elementary dynamic actors via elementary\nnon-linear dynamic interactions. Our model of the scene is regulated as a\nlarge, dynamically coupled network of second order differential automata. We\ntake advantage of symbolic non-photorealistic and efficient visualization to\nrender the style of the person, rather than the person itself. As an artistic\nrepresentation, NPR reinforces the symbolic acceptance of the scene by the\nobserver, triggering an immediate and intuitive recognition of the scene as a\nplausible scene from ancient Greece.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.4405v1"
    },
    {
        "title": "Physically-based particle simulation and visualization of pastes and\n  gels",
        "authors": [
            "Claire Guilbaud",
            "Annie Luciani",
            "Nicolas Castagné"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  This paper is focused on the question of simulation and visualiza- tion of 3D\ngel and paste dynamic effects. In a first part, we introduce a 3D physically\nbased particle (or mass-interaction) model, with a small number of masses and\nfew powerful interaction parameters, which is able to generate the dynamic\nfeatures of both gels and pastes. This model proves that the 3D\nmass-interaction method is relevant for the simulation of such phenomena,\nwithout an explicit knowledge of their underly- ing physics. In a second part,\nwe expose an original rendering process, the Flow Structuring Method that\nenhances the dynamic properties of the simulation and offers a realistic\nvisualization. This process ignores all the properties of the underlying\nphysical model. It leads to a reconstruction of the spatial structure of the\ngel (or paste) flow only through an analysis of the output of the simula- tion\nwhich is a set of unorganized points moving in a 3D space. Finally, the paper\npresents realistic renderings obtained by using implicit surfaces and\nray-tracing techniques on the Structured Flow previously obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.4563v1"
    },
    {
        "title": "Toric degenerations of Bezier patches",
        "authors": [
            "Luis David Garcia-Puente",
            "Frank Sottile",
            "Chungang Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  The control polygon of a Bezier curve is well-defined and has geometric\nsignificance---there is a sequence of weights under which the limiting position\nof the curve is the control polygon. For a Bezier surface patch, there are many\npossible polyhedral control structures, and none are canonical. We propose a\nnot necessarily polyhedral control structure for surface patches, regular\ncontrol surfaces, which are certain C^0 spline surfaces. While not unique,\nregular control surfaces are exactly the possible limiting positions of a\nBezier patch when the weights are allowed to vary.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.4903v2"
    },
    {
        "title": "Parametric polynomial minimal surfaces of arbitrary degree",
        "authors": [
            "Gang Xu",
            "Guozhao Wang"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  Weierstrass representation is a classical parameterization of minimal\nsurfaces. However, two functions should be specified to construct the\nparametric form in Weierestrass representation. In this paper, we propose an\nexplicit parametric form for a class of parametric polynomial minimal surfaces\nof arbitrary degree. It includes the classical Enneper surface for cubic case.\nThe proposed minimal surfaces also have some interesting properties such as\nsymmetry, containing straight lines and self-intersections. According to the\nshape properties, the proposed minimal surface can be classified into four\ncategories with respect to $n=4k-1$ $n=4k+1$, $n=4k$ and $n=4k+2$. The explicit\nparametric form of corresponding conjugate minimal surfaces is given and the\nisometric deformation is also implemented.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.0208v2"
    },
    {
        "title": "Surface Curvature Effects on Reflectance from Translucent Materials",
        "authors": [
            "Konstantin Kolchin"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  Most of the physically based techniques for rendering translucent objects use\nthe diffusion theory of light scattering in turbid media. The widely used\ndipole diffusion model (Jensen et al. 2001) applies the diffusion-theory\nformula derived for a planar interface to objects of arbitrary shapes. This\npaper presents first results of our investigation of how surface curvature\naffects the diffuse reflectance from translucent materials.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.2623v2"
    },
    {
        "title": "Video Stippling",
        "authors": [
            "Thomas Houit",
            "Frank Nielsen"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  In this paper, we consider rendering color videos using a non-photo-realistic\nart form technique commonly called stippling. Stippling is the art of rendering\nimages using point sets, possibly with various attributes like sizes,\nelementary shapes, and colors. Producing nice stippling is attractive not only\nfor the sake of image depiction but also because it yields a compact vectorial\nformat for storing the semantic information of media. Moreover, stippling is by\nconstruction easily tunable to various device resolutions without suffering\nfrom bitmap sampling artifacts when resizing. The underlying core technique for\nstippling images is to compute a centroidal Voronoi tessellation on a\nwell-designed underlying density. This density relates to the image content,\nand is used to compute a weighted Voronoi diagram. By considering videos as\nimage sequences and initializing properly the stippling of one image by the\nresult of its predecessor, one avoids undesirable point flickering artifacts\nand can produce stippled videos that nevertheless still exhibit noticeable\nartifacts. To overcome this, our method improves over the naive scheme by\nconsidering dynamic point creation and deletion according to the current scene\nsemantic complexity, and show how to effectively vectorize video while\nadjusting for both color and contrast characteristics. Furthermore, we explain\nhow to produce high quality stippled ``videos'' (eg., fully dynamic\nspatio-temporal point sets) for media containing various fading effects, like\nquick motions of objects or progressive shot changes. We report on practical\nperformances of our implementation, and present several stippled video results\nrendered on-the-fly using our viewer that allows both spatio-temporal dynamic\nrescaling (eg., upscale vectorially frame rate).\n",
        "pdf_link": "http://arxiv.org/pdf/1011.6049v1"
    },
    {
        "title": "The Role of Computer Graphics in Documentary Film Production",
        "authors": [
            "Miao Song"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  We discuss a topic on the role of computer graphics in the production of\ndocumentaries, which is often ignored in favor of other topics. Typically,\nexcept for some rare occasions, documentary producers and computer scientists\nor digital artists that do computer graphics are relatively far apart in their\ndomains and rarely intercommunicate to have a joint production; yet it happens,\nand perhaps more so in the present and the future.\n  We attempt to classify the documentaries on the amount and techniques of\ncomputer graphics used for documentaries. We come up with the initial\ncategories such as \"plain\" (no graphics), \"in-between\", \"all-out\" -- nearly\n100% of the documentary consisting of computer-generated imagery. Computer\ngraphics can be used to enhance the scenery, fill in the gaps in the missing\nstoryline pieces, or animate between scenes. It can incorporate stereoscopic\neffects for higher viewer impression as well as interactivity aspects. It can\nalso be used simply in old archived image and film restoration.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.0663v1"
    },
    {
        "title": "Chameleon: A Color-Adaptive Web Browser for Mobile OLED Displays",
        "authors": [
            "Mian Dong",
            "Lin Zhong"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  Displays based on organic light-emitting diode (OLED) technology are\nappearing on many mobile devices. Unlike liquid crystal displays (LCD), OLED\ndisplays consume dramatically different power for showing different colors. In\nparticular, OLED displays are inefficient for showing bright colors. This has\nmade them undesirable for mobile devices because much of the web content is of\nbright colors.\n  To tackle this problem, we present the motivational studies, design, and\nrealization of Chameleon, a color adaptive web browser that renders web pages\nwith power-optimized color schemes under user-supplied constraints. Driven by\nthe findings from our motivational studies, Chameleon provides end users with\nimportant options, offloads tasks that are not absolutely needed in real-time,\nand accomplishes real-time tasks by carefully enhancing the codebase of a\nbrowser engine. According to measure-ments with OLED smartphones, Chameleon is\nable to re-duce average system power consumption for web browsing by 41% and\nreduce display power consumption by 64% without introducing any noticeable\ndelay.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.1240v1"
    },
    {
        "title": "An Efficient and Integrated Algorithm for Video Enhancement in\n  Challenging Lighting Conditions",
        "authors": [
            "Xuan Dong",
            " Jiangtao",
            " Wen",
            "Weixin Li",
            " Yi",
            " Pang",
            "Guan Wang",
            "Yao Lu",
            "Wei Meng"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  We describe a novel integrated algorithm for real-time enhancement of video\nacquired under challenging lighting conditions. Such conditions include low\nlighting, haze, and high dynamic range situations. The algorithm automatically\ndetects the dominate source of impairment, then depending on whether it is low\nlighting, haze or others, a corresponding pre-processing is applied to the\ninput video, followed by the core enhancement algorithm. Temporal and spatial\nredundancies in the video input are utilized to facilitate real-time processing\nand to improve temporal and spatial consistency of the output. The proposed\nalgorithm can be used as an independent module, or be integrated in either a\nvideo encoder or a video decoder for further optimizations.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.3328v1"
    },
    {
        "title": "Rendering of 3D Dynamic Virtual Environments",
        "authors": [
            "Salvatore Catanese",
            "Emilio Ferrara",
            "Giacomo Fiumara",
            "Francesco Pagano"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  In this paper we present a framework for the rendering of dynamic 3D virtual\nenvironments which can be integrated in the development of videogames. It\nincludes methods to manage sounds and particle effects, paged static\ngeometries, the support of a physics engine and various input systems. It has\nbeen designed with a modular structure to allow future expansions. We exploited\nsome open-source state-of-the-art components such as OGRE, PhysX,\nParticleUniverse, etc.; all of them have been properly integrated to obtain\npeculiar physical and environmental effects. The stand-alone version of the\napplication is fully compatible with Direct3D and OpenGL APIs and adopts OpenAL\nAPIs to manage audio cards. Concluding, we devised a showcase demo which\nreproduces a dynamic 3D environment, including some particular effects: the\nalternation of day and night infuencing the lighting of the scene, the\nrendering of terrain, water and vegetation, the reproduction of sounds and\natmospheric agents.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.4271v2"
    },
    {
        "title": "Injectivity of 2D Toric Bézier Patches",
        "authors": [
            "Frank Sottile",
            "Chungang Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  Rational B\\'{e}zier functions are widely used as mapping functions in surface\nreparameterization, finite element analysis, image warping and morphing. The\ninjectivity (one-to-one property) of a mapping function is typically necessary\nfor these applications. Toric B\\'{e}zier patches are generalizations of\nclassical patches (triangular, tensor product) which are defined on the convex\nhull of a set of integer lattice points. We give a geometric condition on the\ncontrol points that we show is equivalent to the injectivity of every 2D toric\nB\\'{e}zier patch with those control points for all possible choices of weights.\nThis condition refines that of Craciun, et al., which only implied injectivity\non the interior of a patch.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.2877v1"
    },
    {
        "title": "A Framework for Designing 3D Virtual Environments",
        "authors": [
            "Salvatore Catanese",
            "Emilio Ferrara",
            "Giacomo Fiumara",
            "Francesco Pagano"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  The process of design and development of virtual environments can be\nsupported by tools and frameworks, to save time in technical aspects and\nfocusing on the content. In this paper we present an academic framework which\nprovides several levels of abstraction to ease this work. It includes\nstate-of-the-art components we devised or integrated adopting open-source\nsolutions in order to face specific problems. Its architecture is modular and\ncustomizable, the code is open-source.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.0690v1"
    },
    {
        "title": "3-Phase Recognition Approach to Pseudo 3D Building Generation from 2D\n  Floor Plan",
        "authors": [
            "Raj Kishen Moloo",
            "Muhammad Ajmal Sheik Dawood",
            "Abu Salmaan Auleear"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  Nowadays three dimension (3D) architectural visualisation has become a\npowerful tool in the conceptualisation, design and presentation of\narchitectural products in the construction industry, providing realistic\ninteraction and walkthrough on engineering products. Traditional ways of\nimplementing 3D models involves the use of specialised 3D authoring tools along\nwith skilled 3D designers with blueprints of the model and this is a slow and\nlaborious process. The aim of this paper is to automate this process by simply\nanalyzing the blueprint document and generating the 3D scene automatically. For\nthis purpose we have devised a 3-Phase recognition approach to pseudo 3D\nbuilding generation from 2D floor plan and developed a software accordingly.\nOur 3-phased 3D building system has been implemented using C, C++ and OpenCV\nlibrary [24] for the Image Processing module; The Save Module generated an XML\nfile for storing the processed floor plan objects attributes; while the\nIrrlitch [14] game engine was used to implement the Interactive 3D module.\nThough still at its infancy, our proposed system gave commendable results. We\ntested our system on 6 floor plans with complexities ranging from low to high\nand the results seems to be very promising with an average processing time of\naround 3s and a 3D generation in 4s. In addition the system provides an\ninteractive walk-though and allows users to modify components.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.3680v1"
    },
    {
        "title": "GPU-based Image Analysis on Mobile Devices",
        "authors": [
            "Andrew Ensor",
            "Seth Hall"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  With the rapid advances in mobile technology many mobile devices are capable\nof capturing high quality images and video with their embedded camera. This\npaper investigates techniques for real-time processing of the resulting images,\nparticularly on-device utilizing a graphical processing unit. Issues and\nlimitations of image processing on mobile devices are discussed, and the\nperformance of graphical processing units on a range of devices measured\nthrough a programmable shader implementation of Canny edge detection.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.3110v1"
    },
    {
        "title": "Visual definition of procedures for automatic virtual scene generation",
        "authors": [
            "Drazen Lucanin"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  With more and more digital media, especially in the field of virtual reality\nwhere detailed and convincing scenes are much required, procedural scene\ngeneration is a big helping tool for artists. A problem is that defining scene\ndescriptions through these procedures usually requires a knowledge in formal\nlanguage grammars, programming theory and manually editing textual files using\na strict syntax, making it less intuitive to use. Luckily, graphical user\ninterfaces has made a lot of tasks on computers easier to perform and out of\nthe belief that creating computer programs can also be one of them, visual\nprogramming languages (VPLs) have emerged. The goal in VPLs is to shift more\nwork from the programmer to the integrated development environment (IDE),\nmaking programming an user-friendlier task.\n  In this thesis, an approach of using a VPL for defining procedures that\nautomatically generate virtual scenes is presented. The methods required to\nbuild a VPL are presented, including a novel method of generating readable code\nin a structured programming language. Also, the methods for achieving basic\nprinciples of VPLs will be shown -- suitable visual presentation of information\nand guiding the programmer in the right direction using constraints. On the\nother hand, procedural generation methods are presented in the context of\nvisual programming -- adapting the application programming interface (API) of\nthese methods to better serve the user. The main focus will be on the methods\nfor urban modeling, such as building, city layout and details generation with\nrandom number generation used to create non-deterministic scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.2868v1"
    },
    {
        "title": "From individual to population: Challenges in Medical Visualization",
        "authors": [
            "Charl P. Botha",
            "Bernhard Preim",
            "Arie Kaufman",
            "Shigeo Takahashi",
            "Anders Ynnerman"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  In this paper, we first give a high-level overview of medical visualization\ndevelopment over the past 30 years, focusing on key developments and the trends\nthat they represent. During this discussion, we will refer to a number of key\npapers that we have also arranged on the medical visualization research\ntimeline. Based on the overview and our observations of the field, we then\nidentify and discuss the medical visualization research challenges that we\nforesee for the coming decade.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.1148v2"
    },
    {
        "title": "Visualization in Connectomics",
        "authors": [
            "Hanspeter Pfister",
            "Verena Kaynig",
            "Charl P. Botha",
            "Stefan Bruckner",
            "Vincent J. Dercksen",
            "Hans-Christian Hege",
            "Jos B. T. M. Roerdink"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Connectomics is a field of neuroscience that analyzes neuronal connections. A\nconnectome is a complete map of a neuronal system, comprising all neuronal\nconnections between its structures. The term \"connectome\" is close to the word\n\"genome\" and implies completeness of all neuronal connections, in the same way\nas a genome is a complete listing of all nucleotide sequences. The goal of\nconnectomics is to create a complete representation of the brain's wiring. Such\na representation is believed to increase our understanding of how functional\nbrain states emerge from their underlying anatomical structure. Furthermore, it\ncan provide important information for the cure of neuronal dysfunctions like\nschizophrenia or autism. In this paper, we review the current state-of-the-art\nof visualization and image processing techniques in the field of connectomics\nand describe some remaining challenges.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.1428v2"
    },
    {
        "title": "The Ultrasound Visualization Pipeline - A Survey",
        "authors": [
            "Åsmund Birkeland",
            "Veronika Solteszova",
            "Dieter Hönigmann",
            "Odd Helge Gilja",
            "Svein Brekke",
            "Timo Ropinski",
            "Ivan Viola"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Ultrasound is one of the most frequently used imaging modality in medicine.\nThe high spatial resolution, its interactive nature and non-invasiveness makes\nit the first choice in many examinations. Image interpretation is one of\nultrasound's main challenges. Much training is required to obtain a confident\nskill level in ultrasound-based diagnostics. State-of-the-art graphics\ntechniques is needed to provide meaningful visualizations of ultrasound in\nreal-time. In this paper we present the process-pipeline for ultrasound\nvisualization, including an overview of the tasks performed in the specific\nsteps. To provide an insight into the trends of ultrasound visualization\nresearch, we have selected a set of significant publications and divided them\ninto a technique-based taxonomy covering the topics pre-processing,\nsegmentation, registration, rendering and augmented reality. For the different\ntechnique types we discuss the difference between ultrasound-based techniques\nand techniques for other modalities.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.3975v1"
    },
    {
        "title": "Improved visualisation of brain arteriovenous malformations using color\n  intensity projections with hue cycling",
        "authors": [
            "Keith S. Cover"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Color intensity projections (CIP) have been shown to improve the\nvisualisation of greyscale angiography images by combining greyscale images\ninto a single color image. A key property of the combined CIP image is the\nencoding of the arrival time information from greyscale images into the hue of\nthe color in the CIP image. A few minor improvements to the calculation of the\nCIP image are introduced that substantially improve the quality of the\nvisualisation. One improvement is interpolating of the greyscale images in time\nbefore calculation of the CIP image. A second is the use of hue cycling - where\nthe hue of the color is cycled through more than once in an image. The hue\ncycling allows the variation of the hue to be concentrated in structures of\ninterest. If there is a zero time point hue cycling can be applied after zero\ntime and before zero time can be indicated by greyscale. If there is an end\ntime point hue cycling can be applied before the end time and pixels can be set\nto black after the end time. An angiogram of a brain is used to demonstrate the\nsubstantial improvements hue cycling brings to CIP images. A third improvement\nis the use of maximum intensity projection for 2D rendering of a 3D CIP image\nvolume. A fourth improvement allowing interpreters to interactively adjust the\nphase of the hue via standard contrast - brightness controls using lookup\ntables. Other potential applications of CIP are also mentioned.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.6049v5"
    },
    {
        "title": "Combining Brain-Computer Interfaces and Haptics: Detecting Mental\n  Workload to Adapt Haptic Assistance",
        "authors": [
            "Laurent George",
            "Maud Marchal",
            "Loeïz Glondu",
            "Anatole Lécuyer"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  In this paper we introduce the combined use of Brain-Computer Interfaces\n(BCI) and Haptic interfaces. We propose to adapt haptic guides based on the\nmental activity measured by a BCI system. This novel approach is illustrated\nwithin a proof-of-concept system: haptic guides are toggled during a\npath-following task thanks to a mental workload index provided by a BCI. The\naim of this system is to provide haptic assistance only when the user's brain\nactivity reflects a high mental workload. A user study conducted with 8\nparticipants shows that our proof-of-concept is operational and exploitable.\nResults show that activation of haptic guides occurs in the most difficult part\nof the path-following task. Moreover it allows to increase task performance by\n53% by activating assistance only 59% of the time. Taken together, these\nresults suggest that BCI could be used to determine when the user needs\nassistance during haptic interaction and to enable haptic guides accordingly.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.3351v1"
    },
    {
        "title": "PlotXY: a high quality plotting system for the Herschel Interactive\n  Processing Environment (HIPE), and the astronomical community",
        "authors": [
            "Pasquale Panuzzo",
            "Jinjing Li",
            "Emmanuel Caux"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  The Herschel Interactive Processing Environment (HIPE) was developed by the\nEuropean Space Agency (ESA) in collaboration with NASA and the Herschel\nInstrument Control Centres to provide the astronomical community a complete\nenvironment to process and analyze the data gathered by the Herschel Space\nObservatory. One of the most important components of HIPE is the plotting\nsystem (named PlotXY) that we present here. With PlotXY it is possible to\nproduce easily high quality publication ready 2D plots. It provides a long list\nof features, with fully configurable components, and interactive zooming. The\nentire code of HIPE is written in Java and is open source released under the\nGNU Lesser General Public License version 3. A new version of PlotXY is being\ndeveloped to be independent from the HIPE code base; it is available to the\nsoftware development community for the inclusion in other projects at the URL\nhttp://code.google.com/p/jplot2d/.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.3921v1"
    },
    {
        "title": "General Midpoint Subdivision",
        "authors": [
            "Qi Chen",
            "Hartmut Prautzsch"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  In this paper, we introduce two generalizations of midpoint subdivision and\nanalyze the smoothness of the resulting subdivision surfaces at regular and\nextraordinary points.\n  The smoothing operators used in midpoint and mid-edge subdivision connect the\nmidpoints of adjacent faces or of adjacent edges, respectively. An arbitrary\ncombination of these two operators and the refinement operator that splits each\nface with m vertices into m quadrilateral subfaces forms a general midpoint\nsubdivision operator. We analyze the smoothness of the resulting subdivision\nsurfaces by estimating the norm of a special second order difference scheme and\nby using established methods for analyzing midpoint subdivision. The surfaces\nare smooth at their regular points and they are also smooth at extraordinary\npoints for a certain subclass of general midpoint subdivision schemes.\n  Generalizing the smoothing rules of non general midpoint subdivision schemes\naround extraordinary and regular vertices or faces results in a class of\nsubdivision schemes, which includes the Catmull-Clark algorithm with restricted\nparameters. We call these subdivision schemes generalized Catmull-Clark schemes\nand we analyze their smoothness properties.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.3794v1"
    },
    {
        "title": "Reduction of Blocking Artifacts In JPEG Compressed Image",
        "authors": [
            "Sukhpal Singh"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  In JPEG (DCT based) compresses image data by representing the original image\nwith a small number of transform coefficients. It exploits the fact that for\ntypical images a large amount of signal energy is concentrated in a small\nnumber of coefficients. The goal of DCT transform coding is to minimize the\nnumber of retained transform coefficients while keeping distortion at an\nacceptable level.In JPEG, it is done in 8X8 non overlapping blocks. It divides\nan image into blocks of equal size and processes each block independently.\nBlock processing allows the coder to adapt to the local image statistics,\nexploit the correlation present among neighboring image pixels, and to reduce\ncomputational and storage requirements. One of the most degradation of the\nblock transform coding is the blocking artifact. These artifacts appear as a\nregular pattern of visible block boundaries. This degradation is a direct\nresult of the coarse quantization of the coefficients and the independent\nprocessing of the blocks which does not take into account the existing\ncorrelations among adjacent block pixels. In this paper attempt is being made\nto reduce the blocking artifact introduced by the Block DCT Transform in JPEG.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.1192v3"
    },
    {
        "title": "Sketch-to-Design: Context-based Part Assembly",
        "authors": [
            "Xiaohua Xie",
            "Kai Xu",
            "Niloy J. Mitra",
            "Daniel Cohen-Or",
            "Baoquan Chen"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Designing 3D objects from scratch is difficult, especially when the user\nintent is fuzzy without a clear target form. In the spirit of\nmodeling-by-example, we facilitate design by providing reference and\ninspiration from existing model contexts. We rethink model design as navigating\nthrough different possible combinations of part assemblies based on a large\ncollection of pre-segmented 3D models. We propose an interactive\nsketch-to-design system, where the user sketches prominent features of parts to\ncombine. The sketched strokes are analyzed individually and in context with the\nother parts to generate relevant shape suggestions via a design gallery\ninterface. As the session progresses and more parts get selected, contextual\ncues becomes increasingly dominant and the system quickly converges to a final\ndesign. As a key enabler, we use pre-learned part-based contextual information\nto allow the user to quickly explore different combinations of parts. Our\nexperiments demonstrate the effectiveness of our approach for efficiently\ndesigning new variations from existing shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.4490v1"
    },
    {
        "title": "Discrete Surface Modeling Based on Google Earth: A Case Study",
        "authors": [
            "Gang Mei",
            "John C. Tipper",
            "Nengxiong Xu"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Google Earth (GE) has become a powerful tool for geological, geophysical and\ngeographical modeling; yet GE can be accepted to acquire elevation data of\nterrain. In this paper, we present a real study case of building the discrete\nsurface model (DSM) at Haut-Barr Castle in France based on the elevation data\nof terrain points extracted from GE using the COM API. We first locate the\nposition of Haut-Barr Castle and determine the region of the study area, then\nextract elevation data of terrain at Haut-Barr, and thirdly create a planar\ntriangular mesh that covers the study area and finally generate the desired DSM\nby calculating the elevation of vertices in the planar mesh via interpolating\nwith Universal Kriging (UK) and Inverse Distance Weighting (IDW). The generated\nDSM can reflect the features of the ground surface at Haut-Barr well, and can\nbe used for constructingthe Sealed Engineering Geological Model (SEGM) in\nfurther step.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.6048v1"
    },
    {
        "title": "The Geant4 Visualisation System - a multi-driver graphics system",
        "authors": [
            "John Allison",
            "Laurent Garnier",
            "Akinori Kimura",
            "Joseph Perl"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  From the beginning the Geant4 Visualisation System was designed to support\nseveral simultaneous graphics systems written to common abstract interfaces.\nToday it has matured into a powerful diagnostic and presentational tool. It\ncomes with a library of models that may be added to the current scene and which\ninclude the representation of the Geant4 geometry hierarchy, simulated\ntrajectories and user-written hits and digitisations. The workhorse is the\nOpenGL suite of drivers for X, Xm, Qt and Win32. There is an Open Inventor\ndriver. Scenes can be exported in special graphics formats for offline viewing\nin the DAWN, VRML, HepRApp and gMocren browsers. PostScript can be generated\nthrough OpenGL, Open Inventor, DAWN and HepRApp. Geant4's own tracking\nalgorithms are used by the Ray Tracer. Not all drivers support all features but\nall drivers bring added functionality of some sort. This paper describes the\ninterfaces and details the individual drivers.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.6923v1"
    },
    {
        "title": "Reconstructing Self Organizing Maps as Spider Graphs for better visual\n  interpretation of large unstructured datasets",
        "authors": [
            "Aaditya Prakash"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Self-Organizing Maps (SOM) are popular unsupervised artificial neural network\nused to reduce dimensions and visualize data. Visual interpretation from\nSelf-Organizing Maps (SOM) has been limited due to grid approach of data\nrepresentation, which makes inter-scenario analysis impossible. The paper\nproposes a new way to structure SOM. This model reconstructs SOM to show\nstrength between variables as the threads of a cobweb and illuminate\ninter-scenario analysis. While Radar Graphs are very crude representation of\nspider web, this model uses more lively and realistic cobweb representation to\ntake into account the difference in strength and length of threads. This model\nallows for visualization of highly unstructured dataset with large number of\ndimensions, common in Bigdata sources.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.0289v1"
    },
    {
        "title": "3D Geological Modeling and Visualization of Rock Masses Based on Google\n  Earth: A Case Study",
        "authors": [
            "Gang Mei",
            "John C. Tipper",
            "Nengxiong Xu"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Google Earth (GE) has become a powerful tool for geological modeling and\nvisualization. An interesting and useful feature of GE, Google Street View, can\nallow the GE users to view geological structure such as layers of rock masses\nat a field site. In this paper, we introduce a practical solution for building\n3D geological models for rock masses based on the data acquired by use with GE.\nA real study case at Haut-Barr, France is presented to demonstrate our\nsolution. We first locate the position of Haut-Barr in GE, and then determine\nthe shape and scale of the rock masses in the study area, and thirdly acquire\nthe layout of layers of rock masses in the Google Street View, and finally\ncreate the approximate 3D geological models by extruding and intersecting. The\ngenerated 3D geological models can simply reflect the basic structure of the\nrock masses at Haut-Barr, and can be used for visualizing the rock bodies\ninteractively.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.3455v1"
    },
    {
        "title": "Immersive VR Visualizations by VFIVE. Part 1: Development",
        "authors": [
            "Akira Kageyama",
            "Nobuaki Ohno"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We have been developing a visualization application for CAVE-type virtual\nreality (VR) systems for more than a decade. This application, VFIVE, is\ncurrently used in several CAVE systems in Japan for routine visualizations. It\nis also used as a base system of further developments of advanced\nvisualizations. The development of VFIVE is summarized.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.6007v1"
    },
    {
        "title": "Immersive VR Visualizations by VFIVE. Part 2: Applications",
        "authors": [
            "Akira Kageyama",
            "Nobuaki Ohno",
            "Shintaro Kawahara",
            "Kazuo Kashiyama",
            "Hiroaki Ohtani"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  VFIVE is a scientific visualization application for CAVE-type immersive\nvirtual reality systems. The source codes are freely available. VFIVE is used\nas a research tool in various VR systems. It also lays the groundwork for\ndevelopments of new visualization software for CAVEs. In this paper, we pick up\nfive CAVE systems in four different institutions in Japan. Applications of\nVFIVE in each CAVE system are summarized. Special emphases will be placed on\nscientific and technical achievements made possible by VFIVE.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.6008v1"
    },
    {
        "title": "User Interface for Volume Rendering in Virtual Reality Environments",
        "authors": [
            "Jonathan Klein",
            "Dennis Reuling",
            "Jan Grimm",
            "Andreas Pfau",
            "Damien Lefloch",
            "Martin Lambers",
            "Andreas Kolb"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Volume Rendering applications require sophisticated user interaction for the\ndefinition and refinement of transfer functions. Traditional 2D desktop user\ninterface elements have been developed to solve this task, but such concepts do\nnot map well to the interaction devices available in Virtual Reality\nenvironments.\n  In this paper, we propose an intuitive user interface for Volume Rendering\nspecifically designed for Virtual Reality environments. The proposed interface\nallows transfer function design and refinement based on intuitive two-handed\noperation of Wand-like controllers. Additional interaction modes such as\nnavigation and clip plane manipulation are supported as well.\n  The system is implemented using the Sony PlayStation Move controller system.\nThis choice is based on controller device capabilities as well as application\nand environment constraints.\n  Initial results document the potential of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.2024v1"
    },
    {
        "title": "Fourth-order flows in surface modelling",
        "authors": [
            "Ty Kang"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  This short article is a brief account of the usage of fourth-order curvature\nflow in surface modelling.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.2824v1"
    },
    {
        "title": "Fast exact digital differential analyzer for circle generation",
        "authors": [
            "Jan L. Cieśliński",
            "Leonid V. Moroz"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  In the first part of the paper we present a short review of applications of\ndigital differential analyzers (DDA) to generation of circles showing that they\ncan be treated as one-step numerical schemes. In the second part we present and\ndiscuss a novel fast algorithm based on a two-step numerical scheme (explicit\nmidpoint rule). Although our algorithm is as cheap as the simplest one-step DDA\nalgoritm (and can be represented in terms of shifts and additions), it\ngenerates circles with maximal accuracy, i.e., it is exact up to round-off\nerrors.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.4974v1"
    },
    {
        "title": "Mobile augmented reality survey: a bottom-up approach",
        "authors": [
            "Zhanpeng Huang",
            "Pan Hui",
            "Christoph Peylo",
            "Dimitris Chatzopoulos"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Augmented Reality (AR) is becoming mobile. Mobile devices have many\nconstraints but also rich new features that traditional desktop computers do\nnot have. There are several survey papers on AR, but none is dedicated to\nMobile Augmented Reality (MAR). Our work serves the purpose of closing this\ngap. The contents are organized with a bottom-up approach. We first present the\nstate-of-the-art in system components including hardware platforms, software\nframeworks and display devices, follows with enabling technologies such as\ntracking and data management. We then survey the latest technologies and\nmethods to improve run-time performance and energy efficiency for practical\nimplementation. On top of these, we further introduce the application fields\nand several typical MAR applications. Finally we conclude the survey with\nseveral challenge problems, which are under exploration and require great\nresearch efforts in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.4413v2"
    },
    {
        "title": "ImageSpirit: Verbal Guided Image Parsing",
        "authors": [
            "Ming-Ming Cheng",
            "Shuai Zheng",
            "Wen-Yan Lin",
            "Jonathan Warrell",
            "Vibhav Vineet",
            "Paul Sturgess",
            "Nigel Crook",
            "Niloy Mitra",
            "Philip Torr"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Humans describe images in terms of nouns and adjectives while algorithms\noperate on images represented as sets of pixels. Bridging this gap between how\nhumans would like to access images versus their typical representation is the\ngoal of image parsing, which involves assigning object and attribute labels to\npixel. In this paper we propose treating nouns as object labels and adjectives\nas visual attribute labels. This allows us to formulate the image parsing\nproblem as one of jointly estimating per-pixel object and attribute labels from\na set of training images. We propose an efficient (interactive time) solution.\nUsing the extracted labels as handles, our system empowers a user to verbally\nrefine the results. This enables hands-free parsing of an image into pixel-wise\nobject/attribute labels that correspond to human semantics. Verbally selecting\nobjects of interests enables a novel and natural interaction modality that can\npossibly be used to interact with new generation devices (e.g. smart phones,\nGoogle Glass, living room devices). We demonstrate our system on a large number\nof real-world images with varying complexity. To help understand the tradeoffs\ncompared to traditional mouse based interactions, results are reported for both\na large scale quantitative evaluation and a user study.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.4389v2"
    },
    {
        "title": "Real-time High Resolution Fusion of Depth Maps on GPU",
        "authors": [
            "Dmitry Trifonov"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  A system for live high quality surface reconstruction using a single moving\ndepth camera on a commodity hardware is presented. High accuracy and real-time\nframe rate is achieved by utilizing graphics hardware computing capabilities\nvia OpenCL and by using sparse data structure for volumetric surface\nrepresentation. Depth sensor pose is estimated by combining serial texture\nregistration algorithm with iterative closest points algorithm (ICP) aligning\nobtained depth map to the estimated scene model. Aligned surface is then fused\ninto the scene. Kalman filter is used to improve fusion quality. Truncated\nsigned distance function (TSDF) stored as block-based sparse buffer is used to\nrepresent surface. Use of sparse data structure greatly increases accuracy of\nscanned surfaces and maximum scanning area. Traditional GPU implementation of\nvolumetric rendering and fusion algorithms were modified to exploit sparsity to\nachieve desired performance. Incorporation of texture registration for sensor\npose estimation and Kalman filter for measurement integration improved accuracy\nand robustness of scanning process.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.7194v1"
    },
    {
        "title": "Introduction to computer animation and its possible educational\n  applications",
        "authors": [
            "Sajid Musa",
            "Rushan Ziatdinov",
            "Carol Griffiths"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Animation, which is basically a form of pictorial presentation, has become\nthe most prominent feature of technology-based learning environments. It refers\nto simulated motion pictures showing movement of drawn objects. Recently,\neducational computer animation has turned out to be one of the most elegant\ntools for presenting multimedia materials for learners, and its significance in\nhelping to understand and remember information has greatly increased since the\nadvent of powerful graphics-oriented computers. In this book chapter we\nintroduce and discuss the history of computer animation, its well-known\nfundamental principles and some educational applications. It is however still\ndebatable if truly educational computer animations help in learning, as the\nresearch on whether animation aids learners' understanding of dynamic phenomena\nhas come up with positive, negative and neutral results. We have tried to\nprovide as much detailed information on computer animation as we could, and we\nhope that this book chapter will be useful for students who study computer\nscience, computer-assisted education or some other courses connected with\ncontemporary education, as well as researchers who conduct their research in\nthe field of computer animation.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.1824v1"
    },
    {
        "title": "A Topologically-informed Hyperstreamline Seeding Method for Alignment\n  Tensor Fields",
        "authors": [
            "Fred Fu",
            "Nasser Mohieddin Abukhdeir"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  A topologically-informed method is presented for seeding of hyperstreamlines\nfor visualization of alignment tensor fields. The method is inspired by and\napplied to visualization of nematic liquid crystal (LC) reorientation dynamics\nsimulations. The method distributes hyperstreamlines along domain boundaries\nand edges of a nearest-neighbor graph whose vertices are degenerate regions of\nthe alignment tensor field, which correspond to orientational defects in a\nnematic LC domain. This is accomplished without iteration while conforming to a\nuser-specified spacing between hyperstreamlines and avoids possible failure\nmodes associated with hyperstreamline integration in the vicinity of\ndegeneracies of alignment (orientational defects). It is shown that the\npresented seeding method enables automated hyperstreamline-based visualization\nof a broad range of alignment tensor fields which enhances the ability of\nresearchers to interpret these fields and provides an alternative to using\nglyph-based techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.7034v2"
    },
    {
        "title": "Animation of 3D Human Model Using Markerless Motion Capture Applied To\n  Sports",
        "authors": [
            "Ashish Shingade",
            "Archana Ghotkar"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Markerless motion capture is an active research in 3D virtualization. In\nproposed work we presented a system for markerless motion capture for 3D human\ncharacter animation, paper presents a survey on motion and skeleton tracking\ntechniques which are developed or are under development. The paper proposed a\nmethod to transform the motion of a performer to a 3D human character (model),\nthe 3D human character performs similar movements as that of a performer in\nreal time. In the proposed work, human model data will be captured by Kinect\ncamera, processed data will be applied on 3D human model for animation. 3D\nhuman model is created using open source software (MakeHuman). Anticipated\ndataset for sport activity is considered as input which can be applied to any\nHCI application.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.2363v1"
    },
    {
        "title": "An Extension Of Weiler-Atherton Algorithm To Cope With The\n  Self-intersecting Polygon",
        "authors": [
            "Anurag Chakraborty"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  In this paper a new algorithm has been proposed which can fix the problem of\nWeiler Atherton algorithm. The problem of Weiler Atherton algorithm lies in\nclipping self intersecting polygon. Clipping self intersecting polygon is not\nconsidered in Weiler Atherton algorithm and hence it is also a main\ndisadvantage of this algorithm. In our new algorithm a self intersecting\npolygon has been divided into non self intersecting contours and then perform\nthe Weiler Atherton clipping algorithm on those sub polygons. For holes we have\nto store the edges that is not own boundary of hole contour from recently\nclipped polygon. Thus if both contour is hole then we have to store all the\nedges of the recently clipped polygon. Finally the resultant polygon has been\nproduced by eliminating all the stored edges.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.0917v1"
    },
    {
        "title": "Image Retargeting by Content-Aware Synthesis",
        "authors": [
            "Weiming Dong",
            "Fuzhang Wu",
            "Yan Kong",
            "Xing Mei",
            "Tong-Yee Lee",
            "Xiaopeng Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Real-world images usually contain vivid contents and rich textural details,\nwhich will complicate the manipulation on them. In this paper, we design a new\nframework based on content-aware synthesis to enhance content-aware image\nretargeting. By detecting the textural regions in an image, the textural image\ncontent can be synthesized rather than simply distorted or cropped. This method\nenables the manipulation of textural & non-textural regions with different\nstrategy since they have different natures. We propose to retarget the textural\nregions by content-aware synthesis and non-textural regions by fast\nmulti-operators. To achieve practical retargeting applications for general\nimages, we develop an automatic and fast texture detection method that can\ndetect multiple disjoint textural regions. We adjust the saliency of the image\naccording to the features of the textural regions. To validate the proposed\nmethod, comparisons with state-of-the-art image targeting techniques and a user\nstudy were conducted. Convincing visual results are shown to demonstrate the\neffectiveness of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.6566v2"
    },
    {
        "title": "Voronoi Grid-Shell Structures",
        "authors": [
            "Nico Pietroni",
            "Davide Tonelli",
            "Enrico Puppo",
            "Maurizio Froli",
            "Roberto Scopigno",
            "Paolo Cignoni"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  We introduce a framework for the generation of grid-shell structures that is\nbased on Voronoi diagrams and allows us to design tessellations that achieve\nexcellent static performances. We start from an analysis of stress on the input\nsurface and we use the resulting tensor field to induce an anisotropic\nnon-Euclidean metric over it. Then we compute a Centroidal Voronoi Tessellation\nunder the same metric. The resulting mesh is hex-dominant and made of cells\nwith a variable density, which depends on the amount of stress, and anisotropic\nshape, which depends on the direction of maximum stress. This mesh is further\noptimized taking into account symmetry and regularity of cells to improve\naesthetics. We demonstrate that our grid-shells achieve better static\nperformances with respect to quad-based grid shells, while offering an\ninnovative and aesthetically pleasing look.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.6591v1"
    },
    {
        "title": "A Canonical Representation of Data-Linear Visualization Algorithms",
        "authors": [
            "Thomas Baudel"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  We introduce linear-state dataflows, a canonical model for a large set of\nvisualization algorithms that we call data-linear visualizations. Our model\ndefines a fixed dataflow architecture: partitioning and subpartitioning of\ninput data, ordering, graphic primitives, and graphic attributes generation.\nLocal variables and accumulators are specific concepts that extend the\nexpressiveness of the dataflow to support features of visualization algorithms\nthat require state handling. We first show the flexibility of our model: it\nenables the declarative construction of many common algorithms with just a few\nmappings. Furthermore, the model enables easy mixing of visual mappings, such\nas creating treemaps of histograms and 2D plots, plots of histograms...\nFinally, we introduce our model in a more formal way and present some of its\nimportant properties. We have implemented this model in a visualization\nframework built around the concept of linear-state dataflows.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.4246v1"
    },
    {
        "title": "Interactive 3D Face Stylization Using Sculptural Abstraction",
        "authors": [
            "Jan Jachnik",
            "Dan B Goldman",
            "Linjie Luo",
            "Andrew J. Davison"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Sculptors often deviate from geometric accuracy in order to enhance the\nappearance of their sculpture. These subtle stylizations may emphasize anatomy,\ndraw the viewer's focus to characteristic features of the subject, or symbolize\ntextures that might not be accurately reproduced in a particular sculptural\nmedium, while still retaining fidelity to the unique proportions of an\nindividual. In this work we demonstrate an interactive system for enhancing\nface geometry using a class of stylizations based on visual decomposition into\nabstract semantic regions, which we call sculptural abstraction. We propose an\ninteractive two-scale optimization framework for stylization based on\nsculptural abstraction, allowing real-time adjustment of both global and local\nparameters. We demonstrate this system's effectiveness in enhancing physical 3D\nprints of scans from various sources.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.01954v1"
    },
    {
        "title": "Facial Expression Cloning with Elastic and Muscle Models",
        "authors": [
            "Yihao Zhang",
            "Weiyao Lin",
            "Bing Zhou",
            "Zhenzhong Chen",
            "Bin Sheng",
            "Jianxin Wu",
            "Wenjun Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Expression cloning plays an important role in facial expression synthesis. In\nthis paper, a novel algorithm is proposed for facial expression cloning. The\nproposed algorithm first introduces a new elastic model to balance the global\nand local warping effects, such that the impacts from facial feature diversity\namong people can be minimized, and thus more effective geometric warping\nresults can be achieved. Furthermore, a muscle-distribution-based (MD) model is\nproposed, which utilizes the muscle distribution of the human face and results\nin more accurate facial illumination details. In addition, we also propose a\nnew distance-based metric to automatically select the optimal parameters such\nthat the global and local warping effects in the elastic model can be suitably\nbalanced. Experimental results show that our proposed algorithm outperforms the\nexisting methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00088v1"
    },
    {
        "title": "Interpolation of a spline developable surface between a curve and two\n  rulings",
        "authors": [
            "A. Cantón",
            "L. Fernández-Jambrina"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  In this paper we address the problem of interpolating a spline developable\npatch bounded by a given spline curve and the first and the last rulings of the\ndevelopable surface. In order to complete the boundary of the patch a second\nspline curve is to be given. Up to now this interpolation problem could be\nsolved, but without the possibility of choosing both endpoints for the rulings.\nWe circumvent such difficulty here by resorting to degree elevation of the\ndevelopable surface. This is useful not only to solve this problem, but also\nother problems dealing with triangular developable patches.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.06995v1"
    },
    {
        "title": "Conformal Surface Morphing with Applications on Facial Expressions",
        "authors": [
            "Mei-Heng Yueh",
            "Xianfeng David Gu",
            "Wen-Wei Lin",
            "Chin-Tien Wu",
            "Shing-Tung Yau"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Morphing is the process of changing one figure into another. Some numerical\nmethods of 3D surface morphing by deformable modeling and conformal mapping are\nshown in this study. It is well known that there exists a unique Riemann\nconformal mapping from a simply connected surface into a unit disk by the\nRiemann mapping theorem. The dilation and relative orientations of the 3D\nsurfaces can be linked through the M\\\"obius transformation due to the conformal\ncharacteristic of the Riemann mapping. On the other hand, a 3D surface\ndeformable model can be built via various approaches such as mutual\nparameterization from direct interpolation or surface matching using landmarks.\nIn this paper, we take the advantage of the unique representation of 3D\nsurfaces by the mean curvatures and the conformal factors associated with the\nRiemann mapping. By registering the landmarks on the conformal parametric\ndomains, the correspondence of the mean curvatures and the conformal factors\nfor each surfaces can be obtained. As a result, we can construct the 3D\ndeformation field from the surface reconstruction algorithm proposed by Gu and\nYau. Furthermore, by composition of the M\\\"obius transformation and the 3D\ndeformation field, the morphing sequence can be generated from the mean\ncurvatures and the conformal factors on a unified mesh structure by using the\ncubic spline homotopy. Several numerical experiments of the face morphing are\npresented to demonstrate the robustness of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.00097v1"
    },
    {
        "title": "Preprint Big City 3D Visual Analysis",
        "authors": [
            "Zhihan Lv",
            "Xiaoming Li",
            "Baoyun Zhang",
            "Weixi Wang",
            "Shengzhong Feng",
            "Jinxing Hu"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  This is the preprint version of our paper on EUROGRAPHICS 2015. A big city\nvisual analysis platform based on Web Virtual Reality Geographical Information\nSystem (WEBVRGIS) is presented. Extensive model editing functions and spatial\nanalysis functions are available, including terrain analysis, spatial analysis,\nsunlight analysis, traffic analysis, population analysis and community\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.01379v2"
    },
    {
        "title": "Light-field Microscopy with a Consumer Light-field Camera",
        "authors": [
            "Lois Mignard-Debise",
            "Ivo Ihrke"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We explore the use of inexpensive consumer light- field camera technology for\nthe purpose of light-field mi- croscopy. Our experiments are based on the Lytro\n(first gen- eration) camera. Unfortunately, the optical systems of the Lytro\nand those of microscopes are not compatible, lead- ing to a loss of light-field\ninformation due to angular and spatial vignetting when directly recording\nmicroscopic pic- tures. We therefore consider an adaptation of the Lytro op-\ntical system. We demonstrate that using the Lytro directly as an oc- ular\nreplacement, leads to unacceptable spatial vignetting. However, we also found a\nsetting that allows the use of the Lytro camera in a virtual imaging mode which\nprevents the information loss to a large extent. We analyze the new vir- tual\nimaging mode and use it in two different setups for im- plementing light-field\nmicroscopy using a Lytro camera. As a practical result, we show that the camera\ncan be used for low magnification work, as e.g. common in quality control,\nsurface characterization, etc. We achieve a maximum spa- tial resolution of\nabout 6.25{\\mu}m, albeit at a limited SNR for the side views.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.03590v2"
    },
    {
        "title": "3D-Computer Animation for a Yoruba Native Folktale",
        "authors": [
            "Ibrahim Adeyanju",
            "Comfort Babalola",
            "Kareemat Salaudeen",
            "Biola Oyediran"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Computer graphics has wide range of applications which are implemented into\ncomputer animation, computer modeling among others. Since the invention of\ncomputer graphics researchers have not paid much of attentions toward the\npossibility of converting oral tales otherwise known as folktales into possible\ncartoon animated videos. This paper is based on how to develop cartoons of\nlocal folktales that will be of huge benefits to Nigerians. The activities were\ndivided into 5 stages; analysis, design, development, implementation and\nevaluation which involved various processes and use of various specialized\nsoftware and hardware. After the implementation of this project, the video\ncharacteristics were evaluated using likert scale. Analysis of 30 user\nresponses indicated that 17 users (56.7 percent) rated the image quality as\nexcellent, the video and image synchronization was rated as excellent by 9\nusers (30 percent), the Background noise was rated excellent by 18 users (60\npercent), the Character Impression was rated Excellent by 11 users (36.67\npercent), the general assessment of the storyline was rated excellent by 17\nusers (56.7 percent), the video Impression was rated excellent by 11 users\n(36.67 percent) and the voice quality was rated by 10 users (33.33 percent) as\nexcellent.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.03840v1"
    },
    {
        "title": "Light Efficient Flutter Shutter",
        "authors": [
            "Moshe Ben-Ezra"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Flutter shutter is a technique in which the exposure is chopped into segments\nand light is only integrated part of the time. By carefully selecting the\nchopping sequence it is possible to better condition the data for\nreconstruction problems such as motion deblurring, focal sweeping, and\ncompressed sensing. The partial exposure trades better conditioning for less\nenergy. In problems such as motion deblurring the available energy is what\ncaused the problem in the first place (as strong illumination allows short\nexposure thus eliminates motion blur). It is still beneficial because the\nbenefit from the better conditioning outweighs the cost in energy.\n  This documents is focused on light efficient flutter shutter that provides\nbetter conditioning and better energy utilization than conventional flutter\nshutter.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01220v1"
    },
    {
        "title": "Deformation Lamps: A Projection Technique to Make a Static Object\n  Dynamic",
        "authors": [
            "Takahiro Kawabe",
            "Taiki Fukiage",
            "Masataka Sawayama",
            "Shin'ya Nishida"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Light projection is a powerful technique to edit appearances of objects in\nthe real world. Based on pixel-wise modification of light transport, previous\ntechniques have successfully modified static surface properties such as surface\ncolor, dynamic range, gloss and shading. Here, we propose an alternative light\nprojection technique that adds a variety of illusory, yet realistic distortions\nto a wide range of static 2D and 3D projection targets. The key idea of our\ntechnique, named Deformation Lamps, is to project only dynamic luminance\ninformation, which effectively activates the motion (and shape) processing in\nthe visual system, while preserving the color and texture of the original\nobject. Although the projected dynamic luminance information is spatially\ninconsistent with the color and texture of the target object, the observer's\nbrain automatically com- bines these sensory signals in such a way as to\ncorrect the inconsistency across visual attributes. We conducted a\npsychophysical experiment to investigate the characteristics of the\ninconsistency correction, and found that the correction was dependent\ncritically on the retinal magnitude of inconsistency. Another experiment showed\nthat perceived magnitude of image deformation by our techniques was\nunderestimated. The results ruled out the possibility that the effect by our\ntechnique stemmed simply from the physical change of object appearance by light\nprojection. Finally, we discuss how our techniques can make the observers\nperceive a vivid and natural movement, deformation, or oscillation of a variety\nof static objects, including drawn pictures, printed photographs, sculptures\nwith 3D shading, objects with natural textures including human bodies.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.08037v1"
    },
    {
        "title": "Visualization techniques for the developing chicken heart",
        "authors": [
            "Ly Phan",
            "Sandra Rugonyi",
            "Cindy Grimm"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We present a geometric surface parameterization algorithm and several\nvisualization techniques adapted to the problem of understanding the 4D\nperistaltic-like motion of the outflow tract (OFT) in an embryonic chick heart.\nWe illustrated the techniques using data from hearts under normal conditions\n(four embryos), and hearts in which blood flow conditions are altered through\nOFT banding (four embryos). The overall goal is to create quantitative measures\nof the temporal heart-shape change both within a single subject and between\nmultiple subjects. These measures will help elucidate how altering hemodynamic\nconditions changes the shape and motion of the OFT walls, which in turn\ninfluence the stresses and strains on the developing heart, causing it to\ndevelop differently. We take advantage of the tubular shape and periodic motion\nof the OFT to produce successively lower dimensional visualizations of the\ncardiac motion (e.g. curvature, volume, and cross-section) over time, and\nquantifications of such visualizations.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.08834v1"
    },
    {
        "title": "RAID: A Relation-Augmented Image Descriptor",
        "authors": [
            "Paul Guerrero",
            "Niloy J. Mitra",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  As humans, we regularly interpret images based on the relations between image\nregions. For example, a person riding object X, or a plank bridging two\nobjects. Current methods provide limited support to search for images based on\nsuch relations. We present RAID, a relation-augmented image descriptor that\nsupports queries based on inter-region relations. The key idea of our\ndescriptor is to capture the spatial distribution of simple point-to-region\nrelationships to describe more complex relationships between two image regions.\nWe evaluate the proposed descriptor by querying into a large subset of the\nMicrosoft COCO database and successfully extract nontrivial images\ndemonstrating complex inter-region relations, which are easily missed or\nerroneously classified by existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.01113v2"
    },
    {
        "title": "Simulating the Dynamic Behavior of Shear Thickening Fluids",
        "authors": [
            "Oktar Ozgen",
            "Marcelo Kallmann",
            "Eric Brown"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  While significant research has been dedicated to the simulation of fluids,\nnot much attention has been given to exploring new interesting behavior that\ncan be generated with the different types of non-Newtonian fluids with\nnon-constant viscosity. Going in this direction, this paper introduces a\ncomputational model for simulating the interesting phenomena observed in\nnon-Newtonian shear thickening fluids, which are fluids where the viscosity\nincreases with increased stress. These fluids have unique and unconventional\nbehavior, and they often appear in real world scenarios such as when sinking in\nquicksand or when experimenting with popular cornstarch and water mixtures.\nWhile interesting behavior of shear thickening fluids can be easily observed in\nthe real world, the most interesting phenomena of these fluids have not been\nsimulated before in computer graphics. The fluid exhibits unique phase changes\nbetween solid and liquid states, great impact resistance in its solid state and\nstrong hysteresis effects. Our proposed approach builds on existing\nnon-Newtonian fluid models in computer graphics and introduces an efficient\nhistory-based stiffness term that is essential to produce the most interesting\nshear thickening phenomena. The history-based stiffness is formulated through\nthe use of fractional derivatives, leveraging the fractional calculus ability\nto depict both the viscoelastic behavior and the history effects of\nhistory-dependent systems. Simulations produced by our method are compared\nagainst real experiments and the results demonstrate that the proposed model\nsuccessfully captures key phenomena observed in shear thickening fluids.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.09069v1"
    },
    {
        "title": "Design of false color palettes for grayscale reproduction",
        "authors": [
            "Filip A. Sala"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Design of false color palette is quite easy but some effort has to be done to\nachieve good dynamic range, contrast and overall appearance of the palette.\nSuch palettes, for instance, are commonly used in scientific papers for\npresenting the data. However, to lower the cost of the paper most scientists\ndecide to let the data to be printed in grayscale. The same applies to e-book\nreaders based on e-ink where most of them are still grayscale. For majority of\nfalse color palettes reproducing them in grayscale results in ambiguous mapping\nof the colors and may be misleading for the reader. In this article design of\nfalse color palettes suitable for grayscale reproduction is described. Due to\nthe monotonic change of luminance of these palettes grayscale representation is\nvery similar to the data directly presented with a grayscale palette. Some\nsuggestions and examples how to design such palettes are provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.03206v2"
    },
    {
        "title": "On a recursive construction of circular paths and the search for $π$\n  on the integer lattice $\\mathbb{Z}^2$",
        "authors": [
            "Michelle Rudolph-Lilith"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Digital circles not only play an important role in various technological\nsettings, but also provide a lively playground for more fundamental\nnumber-theoretical questions. In this paper, we present a new recursive\nalgorithm for the construction of digital circles on the integer lattice\n$\\mathbb{Z}^2$, which makes sole use of the signum function. By briefly\nelaborating on the nature of discretization of circular paths, we then find\nthat this algorithm recovers, in a space endowed with $\\ell^1$-norm, the\ndefining constant $\\pi$ of a circle in $\\mathbb{R}^2$.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.06239v1"
    },
    {
        "title": "Creating Simplified 3D Models with High Quality Textures",
        "authors": [
            "Song Liu",
            "Wanqing Li",
            "Philip Ogunbona",
            "Yang-Wai Chow"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  This paper presents an extension to the KinectFusion algorithm which allows\ncreating simplified 3D models with high quality RGB textures. This is achieved\nthrough (i) creating model textures using images from an HD RGB camera that is\ncalibrated with Kinect depth camera, (ii) using a modified scheme to update\nmodel textures in an asymmetrical colour volume that contains a higher number\nof voxels than that of the geometry volume, (iii) simplifying dense polygon\nmesh model using quadric-based mesh decimation algorithm, and (iv) creating and\nmapping 2D textures to every polygon in the output 3D model. The proposed\nmethod is implemented in real-time by means of GPU parallel processing.\nVisualization via ray casting of both geometry and colour volumes provides\nusers with a real-time feedback of the currently scanned 3D model. Experimental\nresults show that the proposed method is capable of keeping the model texture\nquality even for a heavily decimated model and that, when reconstructing small\nobjects, photorealistic RGB textures can still be reconstructed.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.06645v1"
    },
    {
        "title": "BundleFusion: Real-time Globally Consistent 3D Reconstruction using\n  On-the-fly Surface Re-integration",
        "authors": [
            "Angela Dai",
            "Matthias Nießner",
            "Michael Zollhöfer",
            "Shahram Izadi",
            "Christian Theobalt"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed\nreality and robotic applications. However, scalability brings challenges of\ndrift in pose estimation, introducing significant errors in the accumulated\nmodel. Approaches often require hours of offline processing to globally correct\nmodel errors. Recent online methods demonstrate compelling results, but suffer\nfrom: (1) needing minutes to perform online correction preventing true\nreal-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation\nresulting in many tracking failures; or (3) supporting only unstructured\npoint-based representations, which limit scan quality and applicability. We\nsystematically address these issues with a novel, real-time, end-to-end\nreconstruction framework. At its core is a robust pose estimation strategy,\noptimizing per frame for a global set of camera poses by considering the\ncomplete history of RGB-D input with an efficient hierarchical approach. We\nremove the heavy reliance on temporal tracking, and continually localize to the\nglobally optimized frames instead. We contribute a parallelizable optimization\nframework, which employs correspondences based on sparse features and dense\ngeometric and photometric matching. Our approach estimates globally optimized\n(i.e., bundle adjusted) poses in real-time, supports robust tracking with\nrecovery from gross tracking failures (i.e., relocalization), and re-estimates\nthe 3D model in real-time to ensure global consistency; all within a single\nframework. Our approach outperforms state-of-the-art online systems with\nquality on par to offline methods, but with unprecedented speed and scan\ncompleteness. Our framework leads to a comprehensive online scanning solution\nfor large indoor environments, enabling ease of use and high-quality results.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01093v3"
    },
    {
        "title": "Keyboard Based Control of Four Dimensional Rotations",
        "authors": [
            "Akira Kageyama"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Aiming at applications to the scientific visualization of three dimensional\nsimulations with time evolution, a keyboard based control method to specify\nrotations in four dimensions is proposed. It is known that four dimensional\nrotations are generally so-called double rotations, and a double rotation is a\ncombination of simultaneously applied two simple rotations. The proposed method\ncan specify both the simple and double rotations by single key typings of the\nkeyboard. The method is tested in visualizations of a regular pentachoron in\nfour dimensional space by a hyperplane slicing.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.02013v1"
    },
    {
        "title": "How2Sketch: Generating Easy-To-Follow Tutorials for Sketching 3D Objects",
        "authors": [
            "James W. Hennessey",
            "Han Liu",
            "Holger Winnemöller",
            "Mira Dontcheva",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Accurately drawing 3D objects is difficult for untrained individuals, as it\nrequires an understanding of perspective and its effects on geometry and\nproportions. Step-by-step tutorials break the complex task of sketching an\nentire object down into easy-to-follow steps that even a novice can follow.\nHowever, creating such tutorials requires expert knowledge and is a\ntime-consuming task. As a result, the availability of tutorials for a given\nobject or viewpoint is limited. How2Sketch addresses this problem by\nautomatically generating easy-to-follow tutorials for arbitrary 3D objects.\nGiven a segmented 3D model and a camera viewpoint,it computes a sequence of\nsteps for constructing a drawing scaffold comprised of geometric primitives,\nwhich helps the user draw the final contours in correct perspective and\nproportion. To make the drawing scaffold easy to construct, the algorithm\nsolves for an ordering among the scaffolding primitives and explicitly makes\nsmall geometric modifications to the size and location of the object parts to\nsimplify relative positioning. Technically, we formulate this scaffold\nconstruction as a single selection problem that simultaneously solves for the\nordering and geometric changes of the primitives. We demonstrate our algorithm\nfor generating tutorials on a variety of man-made objects and evaluate how\neasily the tutorials can be followed with a user study.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.07980v1"
    },
    {
        "title": "Registration of Volumetric Prostate Scans using Curvature Flow",
        "authors": [
            "Saad Nadeem",
            "Rui Shi",
            "Joseph Marino",
            "Wei Zeng",
            "Xianfeng Gu",
            "Arie Kaufman"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Radiological imaging of the prostate is becoming more popular among\nresearchers and clinicians in searching for diseases, primarily cancer. Scans\nmight be acquired with different equipment or at different times for prognosis\nmonitoring, with patient movement between scans, resulting in multiple datasets\nthat need to be registered. For these cases, we introduce a method for\nvolumetric registration using curvature flow. Multiple prostate datasets are\nmapped to canonical solid spheres, which are in turn aligned and registered\nthrough the use of identified landmarks on or within the gland. Theoretical\nproof and experimental results show that our method produces homeomorphisms\nwith feature constraints. We provide thorough validation of our method by\nregistering prostate scans of the same patient in different orientations, from\ndifferent days and using different modes of MRI. Our method also provides the\nfoundation for a general group-wise registration using a standard reference,\ndefined on the complex plane, for any input. In the present context, this can\nbe used for registering as many scans as needed for a single patient or\ndifferent patients on the basis of age, weight or even malignant and\nnon-malignant attributes to study the differences in general population. Though\nwe present this technique with a specific application to the prostate, it is\ngenerally applicable for volumetric registration problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.00921v1"
    },
    {
        "title": "Multimodal Brain Visualization",
        "authors": [
            "Saad Nadeem",
            "Arie Kaufman"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Current connectivity diagrams of human brain image data are either overly\ncomplex or overly simplistic. In this work we introduce simple yet accurate\ninteractive visual representations of multiple brain image structures and the\nconnectivity among them. We map cortical surfaces extracted from human brain\nmagnetic resonance imaging (MRI) data onto 2D surfaces that preserve shape\n(angle), extent (area), and spatial (neighborhood) information for 2D (circular\ndisk) and 3D (spherical) mapping, split these surfaces into separate patches,\nand cluster functional and diffusion tractography MRI connections between pairs\nof these patches. The resulting visualizations are easier to compute on and\nmore visually intuitive to interact with than the original data, and facilitate\nsimultaneous exploration of multiple data sets, modalities, and statistical\nmaps.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.00936v4"
    },
    {
        "title": "Curvature transformation",
        "authors": [
            "Dimitris Vartziotis"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  A transformation based on mean curvature is introduced which morphs\ntriangulated surfaces into round spheres.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.03898v1"
    },
    {
        "title": "A Perceptual Aesthetics Measure for 3D Shapes",
        "authors": [
            "Kapil Dev",
            "Manfred Lau",
            "Ligang Liu"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  While the problem of image aesthetics has been well explored, the study of 3D\nshape aesthetics has focused on specific manually defined features. In this\npaper, we learn an aesthetics measure for 3D shapes autonomously from raw voxel\ndata and without manually-crafted features by leveraging the strength of deep\nlearning. We collect data from humans on their aesthetics preferences for\nvarious 3D shape classes. We take a deep convolutional 3D shape ranking\napproach to compute a measure that gives an aesthetics score for a 3D shape. We\ndemonstrate our approach with various types of shapes and for applications such\nas aesthetics-based visualization, search, and scene composition.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04953v1"
    },
    {
        "title": "Segmenting a Surface Mesh into Pants Using Morse Theory",
        "authors": [
            "Mustafa Hajij",
            "Tamal Dey",
            "Xin Li"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  A pair of pants is a genus zero orientable surface with three boundary\ncomponents. A pants decomposition of a surface is a finite collection of\nunordered pairwise disjoint simple closed curves embedded in the surface that\ndecompose the surface into pants. In this paper we present two Morse theory\nbased algorithms for pants decomposition of a surface mesh. Both algorithms\noperates on a choice of an appropriate Morse function on the surface. The first\nalgorithm uses this Morse function to identify handles that are glued\nsystematically to obtain a pant decomposition. The second algorithm uses the\nReeb graph of the Morse function to obtain a pant decomposition. Both\nalgorithms work for surfaces with or without boundaries. Our preliminary\nimplementation of the two algorithms shows that both algorithms run in much\nless time than an existing state-of-the-art method, and the Reeb graph based\nalgorithm achieves the best time efficiency. Finally, we demonstrate the\nrobustness of our algorithms against noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.06368v3"
    },
    {
        "title": "Hermite interpolation by piecewise polynomial surfaces with polynomial\n  area element",
        "authors": [
            "Michal Bizzarri",
            "Miroslav Lávička",
            "Zbyňek Šír",
            "Jan Vršek"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  This paper is devoted to the construction of polynomial 2-surfaces which\npossess a polynomial area element. In particular we study these surfaces in the\nEuclidean space $\\mathbb R^3$ (where they are equivalent to the PN surfaces)\nand in the Minkowski space $\\mathbb R^{3,1}$ (where they provide the MOS\nsurfaces). We show generally in real vector spaces of any dimension and any\nmetric that the Gram determinant of a parametric set of subspaces is a perfect\nsquare if and only if the Gram determinant of its orthogonal complement is a\nperfect square. Consequently the polynomial surfaces of a given degree with\npolynomial area element can be constructed from the prescribed normal fields\nsolving a system of linear equations. The degree of the constructed surface\ndepending on the degree and the quality of the prescribed normal field is\ninvestigated and discussed. We use the presented approach to interpolate a\nnetwork of points and associated normals with piecewise polynomial surfaces\nwith polynomial area element and demonstrate our method on a number of examples\n(constructions of quadrilateral as well as triangular patches\n",
        "pdf_link": "http://arxiv.org/pdf/1609.05328v1"
    },
    {
        "title": "Customized Facial Constant Positive Air Pressure (CPAP) Masks",
        "authors": [
            "Matan Sela",
            "Nadav Toledo",
            "Yaron Honen",
            "Ron Kimmel"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Sleep apnea is a syndrome that is characterized by sudden breathing halts\nwhile sleeping. One of the common treatments involves wearing a mask that\ndelivers continuous air flow into the nostrils so as to maintain a steady air\npressure. These masks are designed for an average facial model and are often\ndifficult to adjust due to poor fit to the actual patient. The incompatibility\nis characterized by gaps between the mask and the face, which deteriorates the\nimpermeability of the mask and leads to air leakage. We suggest a fully\nautomatic approach for designing a personalized nasal mask interface using a\nfacial depth scan. The interfaces generated by the proposed method accurately\nfit the geometry of the scanned face, and are easy to manufacture. The proposed\nmethod utilizes cheap commodity depth sensors and 3D printing technologies to\nefficiently design and manufacture customized masks for patients suffering from\nsleep apnea.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.07049v1"
    },
    {
        "title": "Fast Blended Transformations for Partial Shape Registration",
        "authors": [
            "Alon Shtern",
            "Matan Sela",
            "Ron Kimmel"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Automatic estimation of skinning transformations is a popular way to deform a\nsingle reference shape into a new pose by providing a small number of control\nparameters. We generalize this approach by efficiently enabling the use of\nmultiple exemplar shapes. Using a small set of representative natural poses, we\npropose to express an unseen appearance by a low-dimensional linear subspace,\nspecified by a redundant dictionary of weighted vertex positions. Minimizing a\nnonlinear functional that regulates the example manifold, the suggested\napproach supports local-rigid deformations of articulated objects, as well as\nnearly isometric embeddings of smooth shapes. A real-time non-rigid deformation\nsystem is demonstrated, and a shape completion and partial registration\nframework is introduced. These applications can recover a target pose and\nimplicit inverse kinematics from a small number of examples and just a few\nvertex positions. The result reconstruction is more accurate compared to\nstate-of-the-art reduced deformable models.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.07738v1"
    },
    {
        "title": "Towards a Drone Cinematographer: Guiding Quadrotor Cameras using Visual\n  Composition Principles",
        "authors": [
            "Niels Joubert",
            "Jane L. E",
            "Dan B Goldman",
            "Floraine Berthouzoz",
            "Mike Roberts",
            "James A. Landay",
            "Pat Hanrahan"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We present a system to capture video footage of human subjects in the real\nworld. Our system leverages a quadrotor camera to automatically capture\nwell-composed video of two subjects. Subjects are tracked in a large-scale\noutdoor environment using RTK GPS and IMU sensors. Then, given the tracked\nstate of our subjects, our system automatically computes static shots based on\nwell-established visual composition principles and canonical shots from\ncinematography literature. To transition between these static shots, we\ncalculate feasible, safe, and visually pleasing transitions using a novel\nreal-time trajectory planning algorithm. We evaluate the performance of our\ntracking system, and experimentally show that RTK GPS significantly outperforms\nconventional GPS in capturing a variety of canonical shots. Lastly, we\ndemonstrate our system guiding a consumer quadrotor camera autonomously\ncapturing footage of two subjects in a variety of use cases. This is the first\nend-to-end system that enables people to leverage the mobility of quadrotors,\nas well as the knowledge of expert filmmakers, to autonomously capture\nhigh-quality footage of people in the real world.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.01691v1"
    },
    {
        "title": "Augmented Reality with Hololens: Experiential Architectures Embedded in\n  the Real World",
        "authors": [
            "Paul Hockett",
            "Tim Ingleby"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Early hands-on experiences with the Microsoft Hololens augmented/mixed\nreality device are reported and discussed, with a general aim of exploring\nbasic 3D visualization. A range of usage cases are tested, including data\nvisualization and immersive data spaces, in-situ visualization of 3D models and\nfull scale architectural form visualization. Ultimately, the Hololens is found\nto provide a remarkable tool for moving from traditional visualization of 3D\nobjects on a 2D screen, to fully experiential 3D visualizations embedded in the\nreal world.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.04281v1"
    },
    {
        "title": "Partial Procedural Geometric Model Fitting for Point Clouds",
        "authors": [
            "Zongliang Zhang",
            "Jonathan Li",
            "Yulan Guo",
            "Yangbin Lin",
            "Ming Cheng",
            "Cheng Wang"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Geometric model fitting is a fundamental task in computer graphics and\ncomputer vision. However, most geometric model fitting methods are unable to\nfit an arbitrary geometric model (e.g. a surface with holes) to incomplete\ndata, due to that the similarity metrics used in these methods are unable to\nmeasure the rigid partial similarity between arbitrary models. This paper hence\nproposes a novel rigid geometric similarity metric, which is able to measure\nboth the full similarity and the partial similarity between arbitrary geometric\nmodels. The proposed metric enables us to perform partial procedural geometric\nmodel fitting (PPGMF). The task of PPGMF is to search a procedural geometric\nmodel space for the model rigidly similar to a query of non-complete point set.\nModels in the procedural model space are generated according to a set of\nparametric modeling rules. A typical query is a point cloud. PPGMF is very\nuseful as it can be used to fit arbitrary geometric models to non-complete\n(incomplete, over-complete or hybrid-complete) point cloud data. For example,\nmost laser scanning data is non-complete due to occlusion. Our PPGMF method\nuses Markov chain Monte Carlo technique to optimize the proposed similarity\nmetric over the model space. To accelerate the optimization process, the method\nalso employs a novel coarse-to-fine model dividing strategy to reject\ndissimilar models in advance. Our method has been demonstrated on a variety of\ngeometric models and non-complete data. Experimental results show that the\nPPGMF method based on the proposed metric is able to fit non-complete data,\nwhile the method based on other metrics is unable. It is also shown that our\nmethod can be accelerated by several times via early rejection.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.04936v1"
    },
    {
        "title": "Deconfliction and Surface Generation from Bathymetry Data Using LR\n  B-splines",
        "authors": [
            "Vibeke Skytt",
            "Quillon Harpham",
            "Tor Dokken",
            "Heidi E. I. Dahl"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  A set of bathymetry point clouds acquired by different measurement techniques\nat different times, having different accuracy and varying patterns of points,\nare approximated by an LR B-spline surface. The aim is to represent the sea\nbottom with good accuracy and at the same time reduce the data size\nconsiderably. In this process the point clouds must be cleaned by selecting the\n\"best\" points for surface generation. This cleaning process is called\ndeconfliction, and we use a rough approximation of the combined point clouds as\na reference surface to select a consistent set of points. The reference surface\nis updated with the selected points to create an accurate approximation. LR\nB-splines is the selected surface format due to its suitability for adaptive\nrefinement and approximation, and its ability to represent local detail without\na global increase in the data size of the surface\n",
        "pdf_link": "http://arxiv.org/pdf/1610.09992v1"
    },
    {
        "title": "SceneSuggest: Context-driven 3D Scene Design",
        "authors": [
            "Manolis Savva",
            "Angel X. Chang",
            "Maneesh Agrawala"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present SceneSuggest: an interactive 3D scene design system providing\ncontext-driven suggestions for 3D model retrieval and placement. Using a\npoint-and-click metaphor we specify regions in a scene in which to\nautomatically place and orient relevant 3D models. Candidate models are ranked\nusing a set of static support, position, and orientation priors learned from 3D\nscenes. We show that our suggestions enable rapid assembly of indoor scenes. We\nperform a user study comparing suggestions to manual search and selection, as\nwell as to suggestions with no automatic orientation. We find that suggestions\nreduce total modeling time by 32%, that orientation priors reduce time spent\nre-orienting objects by 27%, and that context-driven suggestions reduce the\nnumber of text queries by 50%.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00061v1"
    },
    {
        "title": "The Signals and Systems Approach to Animation",
        "authors": [
            "Andrew McCaleb Reach",
            "Chris North"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Animation is ubiquitous in visualization systems, and a common technique for\ncreating these animations is the transition. In the transition approach,\nanimations are created by smoothly interpolating a visual attribute between a\nstart and end value, reaching the end value after a specified duration. This\napproach works well when each transition for an attribute is allowed to finish\nbefore the next is triggered, but performs poorly when a new transition is\ntriggered before the current transition has finished. In particular,\ninterruptions introduce velocity discontinuities, and frequent interruptions\ncan slow down the resulting animation. To solve these problems, we model the\nproblem of animation as a signal processing problem. In our technique,\nanimations are produced by transformations of signals, or functions over time.\nIn particular, an animation is produced by transforming an input signal, a\nfunction from time to target attribute value, into an output signal, a function\nfrom time to displayed attribute value. We show that well-known\nsignal-processing techniques can be applied to produce animations that are free\nfrom velocity discontinuities even when interrupted.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00521v1"
    },
    {
        "title": "HTC Vive MeVisLab integration via OpenVR for medical applications",
        "authors": [
            "Jan Egger",
            "Markus Gall",
            "Jürgen Wallner",
            "Pedro Boechat",
            "Alexander Hann",
            "Xing Li",
            "Xiaojun Chen",
            "Dieter Schmalstieg"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Virtual Reality, an immersive technology that replicates an environment via\ncomputer-simulated reality, gets a lot of attention in the entertainment\nindustry. However, VR has also great potential in other areas, like the medical\ndomain, Examples are intervention planning, training and simulation. This is\nespecially of use in medical operations, where an aesthetic outcome is\nimportant, like for facial surgeries. Alas, importing medical data into Virtual\nReality devices is not necessarily trivial, in particular, when a direct\nconnection to a proprietary application is desired. Moreover, most researcher\ndo not build their medical applications from scratch, but rather leverage\nplatforms like MeVisLab, MITK, OsiriX or 3D Slicer. These platforms have in\ncommon that they use libraries like ITK and VTK, and provide a convenient\ngraphical interface. However, ITK and VTK do not support Virtual Reality\ndirectly. In this study, the usage of a Virtual Reality device for medical data\nunder the MeVisLab platform is presented. The OpenVR library is integrated into\nthe MeVisLab platform, allowing a direct and uncomplicated usage of the head\nmounted display HTC Vive inside the MeVisLab platform. Medical data coming from\nother MeVisLab modules can directly be connected per drag-and-drop to the\nVirtual Reality module, rendering the data inside the HTC Vive for immersive\nvirtual reality inspection.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.07575v1"
    },
    {
        "title": "Graffinity: Visualizing Connectivity In Large Graphs",
        "authors": [
            "Ethan Kerzner",
            "Alexander Lex",
            "Crystal Lynn Sigulinsky",
            "Timothy Urness",
            "Bryan William Jones",
            "Robert E. Marc",
            "Miriah Meyer"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Multivariate graphs are prolific across many fields, including transportation\nand neuroscience. A key task in graph analysis is the exploration of\nconnectivity, to, for example, analyze how signals flow through neurons, or to\nexplore how well different cities are connected by flights. While standard\nnode-link diagrams are helpful in judging connectivity, they do not scale to\nlarge networks. Adjacency matrices also do not scale to large networks and are\nonly suitable to judge connectivity of adjacent nodes. A key approach to\nrealize scalable graph visualization are queries: instead of displaying the\nwhole network, only a relevant subset is shown. Query-based techniques for\nanalyzing connectivity in graphs, however, can also easily suffer from\ncluttering if the query result is big enough. To remedy this, we introduce\ntechniques that provide an overview of the connectivity and reveal details on\ndemand. We have two main contributions: (1) two novel visualization techniques\nthat work in concert for summarizing graph connectivity; and (2) Graffinity, an\nopen-source implementation of these visualizations supplemented by detail views\nto enable a complete analysis workflow. Graffinity was designed in a close\ncollaboration with neuroscientists and is optimized for connectomics data\nanalysis, yet the technique is applicable across domains. We validate the\nconnectivity overview and our open-source tool with illustrative examples using\nflight and connectomics data.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.07729v1"
    },
    {
        "title": "Autocomplete 3D Sculpting",
        "authors": [
            "Mengqi Peng",
            "Jun Xing",
            "Li-Yi Wei"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Digital sculpting is a popular means to create 3D models but remains a\nchallenging task for many users. This can be alleviated by recent advances in\ndata-driven and procedural modeling, albeit bounded by the underlying data and\nprocedures. We propose a 3D sculpting system that assists users in freely\ncreating models without predefined scope. With a brushing interface similar to\ncommon sculpting tools, our system silently records and analyzes users'\nworkflows, and predicts what they might or should do in the future to reduce\ninput labor or enhance output quality. Users can accept, ignore, or modify the\nsuggestions and thus maintain full control and individual style. They can also\nexplicitly select and clone past workflows over output model regions. Our key\nidea is to consider how a model is authored via dynamic workflows in addition\nto what it is shaped in static geometry, for more accurate analysis of user\nintentions and more general synthesis of shape structures. The workflows\ncontain potential repetitions for analysis and synthesis, including user inputs\n(e.g. pen strokes on a pressure sensing tablet), model outputs (e.g. extrusions\non an object surface), and camera viewpoints. We evaluate our method via user\nfeedbacks and authored models.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.10405v1"
    },
    {
        "title": "Liquid Splash Modeling with Neural Networks",
        "authors": [
            "Kiwon Um",
            "Xiangyu Hu",
            "Nils Thuerey"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This paper proposes a new data-driven approach to model detailed splashes for\nliquid simulations with neural networks. Our model learns to generate\nsmall-scale splash detail for the fluid-implicit-particle method using training\ndata acquired from physically parametrized, high resolution simulations. We use\nneural networks to model the regression of splash formation using a classifier\ntogether with a velocity modifier. For the velocity modification, we employ a\nheteroscedastic model. We evaluate our method for different spatial scales,\nsimulation setups, and solvers. Our simulation results demonstrate that our\nmodel significantly improves visual fidelity with a large amount of realistic\ndroplet formation and yields splash detail much more efficiently than finer\ndiscretizations.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.04456v2"
    },
    {
        "title": "A learning-based approach for automatic image and video colorization",
        "authors": [
            "Raj Kumar Gupta",
            "Alex Yong-Sang Chia",
            "Deepu Rajan",
            "Huang Zhiyong"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In this paper, we present a color transfer algorithm to colorize a broad\nrange of gray images without any user intervention. The algorithm uses a\nmachine learning-based approach to automatically colorize grayscale images. The\nalgorithm uses the superpixel representation of the reference color images to\nlearn the relationship between different image features and their corresponding\ncolor values. We use this learned information to predict the color value of\neach grayscale image superpixel. As compared to processing individual image\npixels, our use of superpixels helps us to achieve a much higher degree of\nspatial consistency as well as speeds up the colorization process. The\npredicted color values of the gray-scale image superpixels are used to provide\na 'micro-scribble' at the centroid of the superpixels. These color scribbles\nare refined by using a voting based approach. To generate the final\ncolorization result, we use an optimization-based approach to smoothly spread\nthe color scribble across all pixels within a superpixel. Experimental results\non a broad range of images and the comparison with existing state-of-the-art\ncolorization methods demonstrate the greater effectiveness of the proposed\nalgorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.04610v1"
    },
    {
        "title": "Generating Liquid Simulations with Deformation-aware Neural Networks",
        "authors": [
            "Lukas Prantl",
            "Boris Bonev",
            "Nils Thuerey"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We propose a novel approach for deformation-aware neural networks that learn\nthe weighting and synthesis of dense volumetric deformation fields. Our method\nspecifically targets the space-time representation of physical surfaces from\nliquid simulations. Liquids exhibit highly complex, non-linear behavior under\nchanging simulation conditions such as different initial conditions. Our\nalgorithm captures these complex phenomena in two stages: a first neural\nnetwork computes a weighting function for a set of pre-computed deformations,\nwhile a second network directly generates a deformation field for refining the\nsurface. Key for successful training runs in this setting is a suitable loss\nfunction that encodes the effect of the deformations, and a robust calculation\nof the corresponding gradients. To demonstrate the effectiveness of our\napproach, we showcase our method with several complex examples of flowing\nliquids with topology changes. Our representation makes it possible to rapidly\ngenerate the desired implicit surfaces. We have implemented a mobile\napplication to demonstrate that real-time interactions with complex liquid\neffects are possible with our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.07854v4"
    },
    {
        "title": "Data-Driven Synthesis of Smoke Flows with CNN-based Feature Descriptors",
        "authors": [
            "Mengyu Chu",
            "Nils Thuerey"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a novel data-driven algorithm to synthesize high-resolution flow\nsimulations with reusable repositories of space-time flow data. In our work, we\nemploy a descriptor learning approach to encode the similarity between fluid\nregions with differences in resolution and numerical viscosity. We use\nconvolutional neural networks to generate the descriptors from fluid data such\nas smoke density and flow velocity. At the same time, we present a deformation\nlimiting patch advection method which allows us to robustly track deformable\nfluid regions. With the help of this patch advection, we generate stable\nspace-time data sets from detailed fluids for our repositories. We can then use\nour learned descriptors to quickly localize a suitable data set when running a\nnew simulation. This makes our approach very efficient, and resolution\nindependent. We will demonstrate with several examples that our method yields\nvolumes with very high effective resolutions, and non-dissipative small scale\ndetails that naturally integrate into the motions of the underlying flow.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01425v2"
    },
    {
        "title": "GRASS: Generative Recursive Autoencoders for Shape Structures",
        "authors": [
            "Jun Li",
            "Kai Xu",
            "Siddhartha Chaudhuri",
            "Ersin Yumer",
            "Hao Zhang",
            "Leonidas Guibas"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We introduce a novel neural network architecture for encoding and synthesis\nof 3D shapes, particularly their structures. Our key insight is that 3D shapes\nare effectively characterized by their hierarchical organization of parts,\nwhich reflects fundamental intra-shape relationships such as adjacency and\nsymmetry. We develop a recursive neural net (RvNN) based autoencoder to map a\nflat, unlabeled, arbitrary part layout to a compact code. The code effectively\ncaptures hierarchical structures of man-made 3D objects of varying structural\ncomplexities despite being fixed-dimensional: an associated decoder maps a code\nback to a full hierarchy. The learned bidirectional mapping is further tuned\nusing an adversarial setup to yield a generative model of plausible structures,\nfrom which novel structures can be sampled. Finally, our structure synthesis\nframework is augmented by a second trained module that produces fine-grained\npart geometry, conditioned on global and local structural context, leading to a\nfull generative pipeline for 3D shapes. We demonstrate that without\nsupervision, our network learns meaningful structural hierarchies adhering to\nperceptual grouping principles, produces compact codes which enable\napplications such as shape classification and partial matching, and supports\nshape synthesis and interpolation with significant variations in topology and\ngeometry.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.02090v2"
    },
    {
        "title": "Automated Body Structure Extraction from Arbitrary 3D Mesh",
        "authors": [
            "Yong Khoo",
            "Sang Chung"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This paper presents an automated method for 3D character skeleton extraction\nthat can be applied for generic 3D shapes. Our work is motivated by the\nskeleton-based prior work on automatic rigging focused on skeleton extraction\nand can automatically aligns the extracted structure to fit the 3D shape of the\ngiven 3D mesh. The body mesh can be subsequently skinned based on the extracted\nskeleton and thus enables rigging process. In the experiment, we apply public\ndataset to drive the estimated skeleton from different body shapes, as well as\nthe real data obtained from 3D scanning systems. Satisfactory results are\nobtained compared to the existing approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.05508v1"
    },
    {
        "title": "Visualizing Time-Varying Particle Flows with Diffusion Geometry",
        "authors": [
            "Matthew Berger",
            "Joshua A. Levine"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  The tasks of identifying separation structures and clusters in flow data are\nfundamental to flow visualization. Significant work has been devoted to these\ntasks in flow represented by vector fields, but there are unique challenges in\naddressing these tasks for time-varying particle data. The unstructured nature\nof particle data, nonuniform and sparse sampling, and the inability to access\narbitrary particles in space-time make it difficult to define separation and\nclustering for particle data. We observe that weaker notions of separation and\nclustering through continuous measures of these structures are meaningful when\ncoupled with user exploration. We achieve this goal by defining a measure of\nparticle similarity between pairs of particles. More specifically, separation\noccurs when spatially-localized particles are dissimilar, while clustering is\ncharacterized by sets of particles that are similar to one another. To be\nrobust to imperfections in sampling we use diffusion geometry to compute\nparticle similarity. Diffusion geometry is parameterized by a scale that allows\na user to explore separation and clustering in a continuous manner. We\nillustrate the benefits of our technique on a variety of 2D and 3D flow\ndatasets, from particles integrated in fluid simulations based on time-varying\nvector fields, to particle-based simulations in astrophysics.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.03686v1"
    },
    {
        "title": "Calipso: Physics-based Image and Video Editing through CAD Model Proxies",
        "authors": [
            "Nazim Haouchine",
            "Frederick Roy",
            "Hadrien Courtecuisse",
            "Matthias Nießner",
            "Stephane Cotin"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present Calipso, an interactive method for editing images and videos in a\nphysically-coherent manner. Our main idea is to realize physics-based\nmanipulations by running a full physics simulation on proxy geometries given by\nnon-rigidly aligned CAD models. Running these simulations allows us to apply\nnew, unseen forces to move or deform selected objects, change physical\nparameters such as mass or elasticity, or even add entire new objects that\ninteract with the rest of the underlying scene. In Calipso, the user makes\nedits directly in 3D; these edits are processed by the simulation and then\ntransfered to the target 2D content using shape-to-image correspondences in a\nphoto-realistic rendering process. To align the CAD models, we introduce an\nefficient CAD-to-image alignment procedure that jointly minimizes for rigid and\nnon-rigid alignment while preserving the high-level structure of the input\nshape. Moreover, the user can choose to exploit image flow to estimate scene\nmotion, producing coherent physical behavior with ambient dynamics. We\ndemonstrate Calipso's physics-based editing on a wide range of examples\nproducing myriad physical behavior while preserving geometric and visual\nconsistency.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.03748v1"
    },
    {
        "title": "Fast, large-scale hologram calculation in wavelet domain",
        "authors": [
            "Tomoyoshi Shimobaba",
            "Kyoji Matsushima",
            "Takayuki Takahashi",
            "Yuki Nagahama",
            "Satoki Hasegawa",
            "Marie Sano",
            "Ryuji Hirayama",
            "Takashi Kakue",
            "Tomoyoshi Ito"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We propose a large-scale hologram calculation using WAvelet ShrinkAge-Based\nsuperpositIon (WASABI), a wavelet transform-based algorithm. An image-type\nhologram calculated using the WASABI method is printed on a glass substrate\nwith the resolution of $65,536 \\times 65,536$ pixels and a pixel pitch of $1\n\\mu$m. The hologram calculation time amounts to approximately 354 s on a\ncommercial CPU, which is approximately 30 times faster than conventional\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04233v1"
    },
    {
        "title": "Light in Power: A General and Parameter-free Algorithm for Caustic\n  Design",
        "authors": [
            "Jocelyn Meyron",
            "Quentin Mérigot",
            "Boris Thibert"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present in this paper a generic and parameter-free algorithm to\nefficiently build a wide variety of optical components, such as mirrors or\nlenses, that satisfy some light energy constraints. In all of our problems, one\nis given a collimated or point light source and a desired illumination after\nreflection or refraction and the goal is to design the geometry of a mirror or\nlens which transports exactly the light emitted by the source onto the target.\nWe first propose a general framework and show that eight different optical\ncomponent design problems amount to solving a light energy conservation\nequation that involves the computation of visibility diagrams. We then show\nthat these diagrams all have the same structure and can be obtained by\nintersecting a 3D Power diagram with a planar or spherical domain. This allows\nus to propose an efficient and fully generic algorithm capable to solve these\neight optical component design problems. The support of the prescribed target\nillumination can be a set of directions or a set of points located at a finite\ndistance. Our solutions satisfy design constraints such as convexity or\nconcavity. We show the effectiveness of our algorithm on simulated and\nfabricated examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04820v2"
    },
    {
        "title": "A Novel Stretch Energy Minimization Algorithm for Equiareal\n  Parameterizations",
        "authors": [
            "Mei-Heng Yueh",
            "Wen-Wei Lin",
            "Chin-Tien Wu",
            "Shing-Tung Yau"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Surface parameterizations have been widely applied to computer graphics and\ndigital geometry processing. In this paper, we propose a novel stretch energy\nminimization (SEM) algorithm for the computation of equiareal parameterizations\nof simply connected open surfaces with a very small area distortion and a\nhighly improved computational efficiency. In addition, the existence of\nnontrivial limit points of the SEM algorithm is guaranteed under some mild\nassumptions of the mesh quality. Numerical experiments indicate that the\nefficiency, accuracy, and robustness of the proposed SEM algorithm outperform\nother state-of-the-art algorithms. Applications of the SEM on surface remeshing\nand surface registration for simply connected open surfaces are demonstrated\nthereafter. Thanks to the SEM algorithm, the computations for these\napplications can be carried out efficiently and robustly.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07391v1"
    },
    {
        "title": "Exploring the Design Space of Immersive Urban Analytics",
        "authors": [
            "Chen Zhu-Tian",
            "Yifang Wang",
            "Tianchen Sun",
            "Xiang Gao",
            "Wei Chen",
            "Zhigeng Pan",
            "Huamin Qu",
            "Yingcai Wu"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Recent years have witnessed the rapid development and wide adoption of\nimmersive head-mounted devices, such as HTC VIVE, Oculus Rift, and Microsoft\nHoloLens. These immersive devices have the potential to significantly extend\nthe methodology of urban visual analytics by providing critical 3D context\ninformation and creating a sense of presence. In this paper, we propose an\ntheoretical model to characterize the visualizations in immersive urban\nanalytics. Further more, based on our comprehensive and concise model, we\ncontribute a typology of combination methods of 2D and 3D visualizations that\ndistinguish between linked views, embedded views, and mixed views. We also\npropose a supporting guideline to assist users in selecting a proper view under\ncertain circumstances by considering visual geometry and spatial distribution\nof the 2D and 3D visualizations. Finally, based on existing works, possible\nfuture research opportunities are explored and discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08774v2"
    },
    {
        "title": "Exposure: A White-Box Photo Post-Processing Framework",
        "authors": [
            "Yuanming Hu",
            "Hao He",
            "Chenxi Xu",
            "Baoyuan Wang",
            "Stephen Lin"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Retouching can significantly elevate the visual appeal of photos, but many\ncasual photographers lack the expertise to do this well. To address this\nproblem, previous works have proposed automatic retouching systems based on\nsupervised learning from paired training images acquired before and after\nmanual editing. As it is difficult for users to acquire paired images that\nreflect their retouching preferences, we present in this paper a deep learning\napproach that is instead trained on unpaired data, namely a set of photographs\nthat exhibits a retouching style the user likes, which is much easier to\ncollect. Our system is formulated using deep convolutional neural networks that\nlearn to apply different retouching operations on an input image. Network\ntraining with respect to various types of edits is enabled by modeling these\nretouching operations in a unified manner as resolution-independent\ndifferentiable filters. To apply the filters in a proper sequence and with\nsuitable parameters, we employ a deep reinforcement learning approach that\nlearns to make decisions on what action to take next, given the current state\nof the image. In contrast to many deep learning systems, ours provides users\nwith an understandable solution in the form of conventional retouching edits,\nrather than just a \"black-box\" result. Through quantitative comparisons and\nuser studies, we show that this technique generates retouching results\nconsistent with the provided photo set.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.09602v2"
    },
    {
        "title": "An Application of Mosaic Diagrams to the Visualization of Set\n  Relationships",
        "authors": [
            "Saturnino Luz",
            "Masood Masoodian"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present an application of mosaic diagrams to the visualisation of set\nrelations. Venn and Euler diagrams are the best known visual representations of\nsets and their relationships (intersections, containment or subsets, exclusion\nor disjointness). In recent years, alternative forms of visualisation have been\nproposed. Among them, linear diagrams have been shown to compare favourably to\nVenn and Euler diagrams, in supporting non-interactive assessment of set\nrelationships. Recent studies that compared several variants of linear diagrams\nhave demonstrated that users perform best at tasks involving identification of\nintersections, disjointness and subsets when using a horizontally drawn linear\ndiagram with thin lines representing sets, and employing vertical lines as\nguide lines. The essential visual task the user needs to perform in order to\ninterpret this kind of diagram is vertical alignment of parallel lines and\ndetection of overlaps. Space-filling mosaic diagrams which support this same\nvisual task have been used in other applications, such as the visualization of\nschedules of activities, where they have been shown to be superior to linear\nGantt charts. In this paper, we present an application of mosaic diagrams for\nvisualization of set relationships, and compare it to linear diagrams in terms\nof accuracy, time-to-answer, and subjective ratings of perceived task\ndifficulty. The study participants exhibited similar performance on both\nvisualisations, suggesting that mosaic diagrams are a good alternative to Venn\nand Euler diagrams, and that the choice between linear diagrams and mosaics may\nbe solely guided by visual design considerations.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.03065v1"
    },
    {
        "title": "Solving Poisson's Equation on the Microsoft HoloLens",
        "authors": [
            "Anders Logg",
            "Carl Lundholm",
            "Magne Nordaas"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a mixed reality application (HoloFEM) for the Microsoft HoloLens.\nThe application lets a user define and solve a physical problem governed by\nPoisson's equation with the surrounding real world geometry as input data.\nHolograms are used to visualise both the problem and the solution. The finite\nelement method is used to solve Poisson's equation. Solving and visualising\npartial differential equations in mixed reality could have potential usage in\nareas such as building planning and safety engineering.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.07790v1"
    },
    {
        "title": "Photometric Stereo by UV-Induced Fluorescence to Detect Protrusions on\n  Georgia O'Keeffe's Paintings",
        "authors": [
            "Johanna Salvant",
            "Marc Walton",
            "Dale Kronkright",
            "Chia-Kai Yeh",
            "Fengqiang Li",
            "Oliver Cossairt",
            "Aggelos K. Katsaggelos"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  A significant number of oil paintings produced by Georgia O'Keeffe\n(1887-1986) show surface protrusions of varying width, up to several hundreds\nof microns. These protrusions are similar to those described in the art\nconservation literature as metallic soaps. Since the presence of these\nprotrusions raises questions about the state of conservation and long-term\nprospects for deterioration of these artworks, a 3D-imaging technique,\nphotometric stereo using ultraviolet illumination, was developed for the\nlong-term monitoring of the surface-shape of the protrusions and the\nsurrounding paint. Because the UV fluorescence response of painting materials\nis isotropic, errors typically caused by non-Lambertian (anisotropic)\nspecularities when using visible reflected light can be avoided providing a\nmore accurate estimation of shape. As an added benefit, fluorescence provides\nadditional contrast information contributing to materials characterization. The\ndeveloped methodology aims to detect, characterize, and quantify the\ndistribution of micro-protrusions and their development over the surface of\nentire artworks. Combined with a set of analytical in-situ techniques, and\ncomputational tools, this approach constitutes a novel methodology to\ninvestigate the selective distribution of protrusions in correlation with the\ncomposition of painting materials at the macro-scale. While focused on\nO'Keeffe's paintings as a case study, we expect the proposed approach to have\nbroader significance by providing a non-invasive protocol to the conservation\ncommunity to probe topological changes for any relatively flat painted surface\nof an artwork, and more specifically to monitor the dynamic formation of\nprotrusions, in relation to paint composition and modifications of\nenvironmental conditions, loans, exhibitions and storage over the long-term.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08103v1"
    },
    {
        "title": "A Novel Image-centric Approach Towards Direct Volume Rendering",
        "authors": [
            "Naimul Khan",
            "Riadh Ksantini",
            "Ling Guan"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Transfer Function (TF) generation is a fundamental problem in Direct Volume\nRendering (DVR). A TF maps voxels to color and opacity values to reveal inner\nstructures. Existing TF tools are complex and unintuitive for the users who are\nmore likely to be medical professionals than computer scientists. In this\npaper, we propose a novel image-centric method for TF generation where instead\nof complex tools, the user directly manipulates volume data to generate DVR.\nThe user's work is further simplified by presenting only the most informative\nvolume slices for selection. Based on the selected parts, the voxels are\nclassified using our novel Sparse Nonparametric Support Vector Machine\nclassifier, which combines both local and near-global distributional\ninformation of the training data. The voxel classes are mapped to aesthetically\npleasing and distinguishable color and opacity values using harmonic colors.\nExperimental results on several benchmark datasets and a detailed user survey\nshow the effectiveness of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.11123v1"
    },
    {
        "title": "A Deep Recurrent Framework for Cleaning Motion Capture Data",
        "authors": [
            "Utkarsh Mall",
            "G. Roshan Lal",
            "Siddhartha Chaudhuri",
            "Parag Chaudhuri"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a deep, bidirectional, recurrent framework for cleaning noisy and\nincomplete motion capture data. It exploits temporal coherence and joint\ncorrelations to infer adaptive filters for each joint in each frame. A single\nmodel can be trained to denoise a heterogeneous mix of action types, under\nsubstantial amounts of noise. A signal that has both noise and gaps is\npreprocessed with a second bidirectional network that synthesizes missing\nframes from surrounding context. The approach handles a wide variety of noise\ntypes and long gaps, does not rely on knowledge of the noise distribution, and\noperates in a streaming setting. We validate our approach through extensive\nevaluations on noise both in joint angles and in joint positions, and show that\nit improves upon various alternatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.03380v1"
    },
    {
        "title": "Persistent Homology Guided Force-Directed Graph Layouts",
        "authors": [
            "Ashley Suh",
            "Mustafa Hajij",
            "Bei Wang",
            "Carlos Scheidegger",
            "Paul Rosen"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Graphs are commonly used to encode relationships among entities, yet their\nabstractness makes them difficult to analyze. Node-link diagrams are popular\nfor drawing graphs, and force-directed layouts provide a flexible method for\nnode arrangements that use local relationships in an attempt to reveal the\nglobal shape of the graph. However, clutter and overlap of unrelated structures\ncan lead to confusing graph visualizations. This paper leverages the persistent\nhomology features of an undirected graph as derived information for interactive\nmanipulation of force-directed layouts. We first discuss how to efficiently\nextract 0-dimensional persistent homology features from both weighted and\nunweighted undirected graphs. We then introduce the interactive persistence\nbarcode used to manipulate the force-directed graph layout. In particular, the\nuser adds and removes contracting and repulsing forces generated by the\npersistent homology features, eventually selecting the set of persistent\nhomology features that most improve the layout. Finally, we demonstrate the\nutility of our approach across a variety of synthetic and real datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.05548v4"
    },
    {
        "title": "graphTPP: A multivariate based method for interactive graph layout and\n  analysis",
        "authors": [
            "Helen Gibson",
            "Paul Vickers"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Graph layout is the process of creating a visual representation of a graph\nthrough a node-link diagram. Node-attribute graphs have additional data stored\non the nodes which describe certain properties of the nodes called attributes.\nTypical force-directed representations often produce hairball-like structures\nthat neither aid in understanding the graph's topology nor the relationship to\nits attributes. The aim of this research was to investigate the use of\nnode-attributes for graph layout in order to improve the analysis process and\nto give further insight into the graph over purely topological layouts. In this\narticle we present graphTPP, a graph based extension to targeted projection\npursuit (TPP) --- an interactive, linear, dimension reduction technique --- as\na method for graph layout and subsequent further analysis. TPP allows users to\ncontrol the projection and is optimised for clustering. Three case studies were\nconducted in the areas of influence graphs, network security, and citation\nnetworks. In each case graphTPP was shown to outperform standard force-directed\ntechniques and even other dimension reduction methods in terms of clarity of\nclustered structure in the layout, the association between the structure and\nthe attributes and the insights elicited in each domain area.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.05644v1"
    },
    {
        "title": "Be-Educated: Multimedia Learning through 3D Animation",
        "authors": [
            "Zeeshan Bhatti",
            "Ahsan Abro",
            "Abdul Rehman Gillal",
            "Mostafa Karbasi"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Multimedia learning tools and techniques are placing its importance with\nlarge scale in education sector. With the help of multimedia learning, various\ncomplex phenomenon and theories can be explained and taught easily and\nconveniently. This project aims to teach and spread the importance of education\nand respecting the tools of education: pen, paper, pencil, rubber. To achieve\nthis cognitive learning, a 3D animated movie has been developed using\nprinciples of multimedia learning with 3D cartoon characters resembling the\nactual educational objects, where the buildings have also been modelled to\nresemble real books and diaries. For modelling and animation of these\ncharacters, polygon mesh tools are used in 3D Studio Max. Additionally, the\nfinal composition of video and audio is performed in adobe premiere. This 3D\nanimated video aims to highlight a message of importance for education and\nstationary. The Moral of movie is that do not waste your stationary material,\nuse your Pen and Paper for the purpose they are made for. To be a good citizen\nyou have to Be-Educated yourself and for that you need to give value to Pen.\nThe final rendered and composited 3D animated video reflects this moral and\nportrays the intended message with very vibrant visuals\n",
        "pdf_link": "http://arxiv.org/pdf/1802.06852v1"
    },
    {
        "title": "Sensor-topology based simplicial complex reconstruction",
        "authors": [
            "Stephane Guinard",
            "Bruno Vallet"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We propose a new method for the reconstruction of simplicial complexes\n(combining points, edges and triangles) from 3D point clouds from Mobile Laser\nScanning (MLS). Our main goal is to produce a reconstruction of a scene that is\nadapted to the local geometry of objects. Our method uses the inherent topology\nof the MLS sensor to define a spatial adjacency relationship between points. We\nthen investigate each possible connexion between adjacent points and filter\nthem by searching collinear structures in the scene, or structures\nperpendicular to the laser beams. Next, we create triangles for each triplet of\nself-connected edges. Last, we improve this method with a regularization based\non the co-planarity of triangles and collinearity of remaining edges. We\ncompare our results to a naive simplicial complexes reconstruction based on\nedge length.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.07487v2"
    },
    {
        "title": "Least Square Error Method Robustness of Computation: What is not usually\n  considered and taught",
        "authors": [
            "Vaclav Skala"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  There are many practical applications based on the Least Square Error (LSE)\napproximation. It is based on a square error minimization 'on a vertical' axis.\nThe LSE method is simple and easy also for analytical purposes. However, if\ndata span is large over several magnitudes or non-linear LSE is used, severe\nnumerical instability can be expected. The presented contribution describes a\nsimple method for large span of data LSE computation. It is especially\nconvenient if large span of data are to be processed, when the 'standard'\npseudoinverse matrix is ill conditioned. It is actually based on a LSE solution\nusing orthogonal basis vectors instead of orthonormal basis vectors. The\npresented approach has been used for a linear regression as well as for\napproximation using radial basis functions.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.07591v1"
    },
    {
        "title": "Edge-based LBP description of surfaces with colorimetric patterns",
        "authors": [
            "Elia Moscoso Thompson",
            "Silvia Biasotti"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In this paper we target the problem of the retrieval of colour patterns over\nsurfaces. We generalize to surface tessellations the well known Local Binary\nPattern (LBP) descriptor for images. The key concept of the LBP is to code the\nvariability of the colour values around each pixel. In the case of a surface\ntessellation we adopt rings around vertices that are obtained with a\nsphere-mesh intersection driven by the edges of the mesh; for this reason, we\nname our method edgeLBP. Experimental results are provided to show how this\ndescription performs well for pattern retrieval, also when patterns come from\ndegraded and corrupted archaeological fragments.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03977v1"
    },
    {
        "title": "Experimental similarity assessment for a collection of fragmented\n  artifacts",
        "authors": [
            "Silvia Biasotti",
            "Elia Moscoso Thompson",
            "Michela Spagnuolo"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In the Visual Heritage domain, search engines are expected to support\narchaeologists and curators to address cross-correlation and searching across\nmultiple collections. Archaeological excavations return artifacts that often\nare damaged with parts that are fragmented in more pieces or totally missing.\nThe notion of similarity among fragments cannot simply base on the geometric\nshape but style, material, color, decorations, etc. are all important factors\nthat concur to this concept. In this work, we discuss to which extent the\nexisting techniques for 3D similarity matching are able to approach fragment\nsimilarity, what is missing and what is necessary to be further developed.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03979v1"
    },
    {
        "title": "Semi-Supervised Co-Analysis of 3D Shape Styles from Projected Lines",
        "authors": [
            "Fenggen Yu",
            "Yan Zhang",
            "Kai Xu",
            "Ali Mahdavi-Amiri",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a semi-supervised co-analysis method for learning 3D shape styles\nfrom projected feature lines, achieving style patch localization with only weak\nsupervision. Given a collection of 3D shapes spanning multiple object\ncategories and styles, we perform style co-analysis over projected feature\nlines of each 3D shape and then backproject the learned style features onto the\n3D shapes. Our core analysis pipeline starts with mid-level patch sampling and\npre-selection of candidate style patches. Projective features are then encoded\nvia patch convolution. Multi-view feature integration and style clustering are\ncarried out under the framework of partially shared latent factor (PSLF)\nlearning, a multi-view feature learning scheme. PSLF achieves effective\nmulti-view feature fusion by distilling and exploiting consistent and\ncomplementary feature information from multiple views, while also selecting\nstyle patches from the candidates. Our style analysis approach supports both\nunsupervised and semi-supervised analysis. For the latter, our method accepts\nboth user-specified shape labels and style-ranked triplets as clustering\nconstraints.We demonstrate results from 3D shape style analysis and patch\nlocalization as well as improvements over state-of-the-art methods. We also\npresent several applications enabled by our style analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06579v2"
    },
    {
        "title": "syGlass: Interactive Exploration of Multidimensional Images Using\n  Virtual Reality Head-mounted Displays",
        "authors": [
            "Stanislav Pidhorskyi",
            "Michael Morehead",
            "Quinn Jones",
            "George Spirou",
            "Gianfranco Doretto"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The quest for deeper understanding of biological systems has driven the\nacquisition of increasingly larger multidimensional image datasets. Inspecting\nand manipulating data of this complexity is very challenging in traditional\nvisualization systems. We developed syGlass, a software package capable of\nvisualizing large scale volumetric data with inexpensive virtual reality\nhead-mounted display technology. This allows leveraging stereoscopic vision to\nsignificantly improve perception of complex 3D structures, and provides\nimmersive interaction with data directly in 3D. We accomplished this by\ndeveloping highly optimized data flow and volume rendering pipelines, tested on\ndatasets up to 16TB in size, as well as tools available in a virtual reality\nGUI to support advanced data exploration, annotation, and cataloguing.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.08197v4"
    },
    {
        "title": "Layered Fields for Natural Tessellations on Surfaces",
        "authors": [
            "Rhaleb Zayer",
            "Daniel Mlakar",
            "Markus Steinberger",
            "Hans-Peter Seidel"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Mimicking natural tessellation patterns is a fascinating multi-disciplinary\nproblem. Geometric methods aiming at reproducing such partitions on surface\nmeshes are commonly based on the Voronoi model and its variants, and are often\nfaced with challenging issues such as metric estimation, geometric, topological\ncomplications, and most critically parallelization. In this paper, we introduce\nan alternate model which may be of value for resolving these issues. We drop\nthe assumption that regions need to be separated by lines. Instead, we regard\nregion boundaries as narrow bands and we model the partition as a set of smooth\nfunctions layered over the surface. Given an initial set of seeds or regions,\nthe partition emerges as the solution of a time dependent set of partial\ndifferential equations describing concurrently evolving fronts on the surface.\nOur solution does not require geodesic estimation, elaborate numerical solvers,\nor complicated bookkeeping data structures. The cost per time-iteration is\ndominated by the multiplication and addition of two sparse matrices. Extension\nof our approach in a Lloyd's algorithm fashion can be easily achieved and the\nextraction of the dual mesh can be conveniently preformed in parallel through\nmatrix algebra. As our approach relies mainly on basic linear algebra kernels,\nit lends itself to efficient implementation on modern graphics hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.09152v1"
    },
    {
        "title": "GeneVis - An interactive visualization tool for combining\n  cross-discipline datasets within genetics",
        "authors": [
            "Casper van Leeuwen"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  GeneVis is a web-based tool to visualize complementary data sets of different\ndisciplines within the field of genetics. It overlays gene-cluster information,\ngene-interaction data and gene-disease association data by means of web-based\ninteractive graph visualizations. This allows an intuitive and quick assessment\nof possible relations between the different datasets. By starting from a\nhigh-level graph abstraction based on gene clusters, which can be selected for\ndetailed inspection at the gene-interaction level in a separate window, GeneVis\ncircumvents the common visual clutter problem when using gene datasets with a\nhigh number of gene entries.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.02493v1"
    },
    {
        "title": "Full 3D Reconstruction of Transparent Objects",
        "authors": [
            "Bojian Wu",
            "Yang Zhou",
            "Yiming Qian",
            "Minglun Gong",
            "Hui Huang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Numerous techniques have been proposed for reconstructing 3D models for\nopaque objects in past decades. However, none of them can be directly applied\nto transparent objects. This paper presents a fully automatic approach for\nreconstructing complete 3D shapes of transparent objects. Through positioning\nan object on a turntable, its silhouettes and light refraction paths under\ndifferent viewing directions are captured. Then, starting from an initial rough\nmodel generated from space carving, our algorithm progressively optimizes the\nmodel under three constraints: surface and refraction normal consistency,\nsurface projection and silhouette consistency, and surface smoothness.\nExperimental results on both synthetic and real objects demonstrate that our\nmethod can successfully recover the complex shapes of transparent objects and\nfaithfully reproduce their light refraction properties.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03482v2"
    },
    {
        "title": "Non-Stationary Texture Synthesis by Adversarial Expansion",
        "authors": [
            "Yang Zhou",
            "Zhen Zhu",
            "Xiang Bai",
            "Dani Lischinski",
            "Daniel Cohen-Or",
            "Hui Huang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The real world exhibits an abundance of non-stationary textures. Examples\ninclude textures with large-scale structures, as well as spatially variant and\ninhomogeneous textures. While existing example-based texture synthesis methods\ncan cope well with stationary textures, non-stationary textures still pose a\nconsiderable challenge, which remains unresolved. In this paper, we propose a\nnew approach for example-based non-stationary texture synthesis. Our approach\nuses a generative adversarial network (GAN), trained to double the spatial\nextent of texture blocks extracted from a specific texture exemplar. Once\ntrained, the fully convolutional generator is able to expand the size of the\nentire exemplar, as well as of any of its sub-blocks. We demonstrate that this\nconceptually simple approach is highly effective for capturing large-scale\nstructures, as well as other non-stationary attributes of the input exemplar.\nAs a result, it can cope with challenging textures, which, to our knowledge, no\nother existing method can handle.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.04487v1"
    },
    {
        "title": "Anderson Acceleration for Geometry Optimization and Physics Simulation",
        "authors": [
            "Yue Peng",
            "Bailin Deng",
            "Juyong Zhang",
            "Fanyu Geng",
            "Wenjie Qin",
            "Ligang Liu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Many computer graphics problems require computing geometric shapes subject to\ncertain constraints. This often results in non-linear and non-convex\noptimization problems with globally coupled variables, which pose great\nchallenge for interactive applications. Local-global solvers developed in\nrecent years can quickly compute an approximate solution to such problems,\nmaking them an attractive choice for applications that prioritize efficiency\nover accuracy. However, these solvers suffer from lower convergence rate, and\nmay take a long time to compute an accurate result. In this paper, we propose a\nsimple and effective technique to accelerate the convergence of such solvers.\nBy treating each local-global step as a fixed-point iteration, we apply\nAnderson acceleration, a well-established technique for fixed-point solvers, to\nspeed up the convergence of a local-global solver. To address the stability\nissue of classical Anderson acceleration, we propose a simple strategy to\nguarantee the decrease of target energy and ensure its global convergence. In\naddition, we analyze the connection between Anderson acceleration and\nquasi-Newton methods, and show that the canonical choice of its mixing\nparameter is suitable for accelerating local-global solvers. Moreover, our\ntechnique is effective beyond classical local-global solvers, and can be\napplied to iterative methods with a common structure. We evaluate the\nperformance of our technique on a variety of geometry optimization and physics\nsimulation problems. Our approach significantly reduces the number of\niterations required to compute an accurate result, with only a slight increase\nof computational cost per iteration. Its simplicity and effectiveness makes it\na promising tool for accelerating existing algorithms as well as designing\nefficient new algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.05715v1"
    },
    {
        "title": "Object-Aware Guidance for Autonomous Scene Reconstruction",
        "authors": [
            "Ligang Liu",
            "Xi Xia",
            "Han Sun",
            "Qi Shen",
            "Juzhan Xu",
            "Bin Chen",
            "Hui Huang",
            "Kai Xu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  To carry out autonomous 3D scanning and online reconstruction of unknown\nindoor scenes, one has to find a balance between global exploration of the\nentire scene and local scanning of the objects within it. In this work, we\npropose a novel approach, which provides object-aware guidance for\nautoscanning, for exploring, reconstructing, and understanding an unknown scene\nwithin one navigation pass. Our approach interleaves between object analysis to\nidentify the next best object (NBO) for global exploration, and object-aware\ninformation gain analysis to plan the next best view (NBV) for local scanning.\nFirst, an objectness-based segmentation method is introduced to extract\nsemantic objects from the current scene surface via a multi-class graph cuts\nminimization. Then, an object of interest (OOI) is identified as the NBO which\nthe robot aims to visit and scan. The robot then conducts fine scanning on the\nOOI with views determined by the NBV strategy. When the OOI is recognized as a\nfull object, it can be replaced by its most similar 3D model in a shape\ndatabase. The algorithm iterates until all of the objects are recognized and\nreconstructed in the scene. Various experiments and comparisons have shown the\nfeasibility of our proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.07794v1"
    },
    {
        "title": "Analyzing Interfaces and Workflows for Light Field Editing",
        "authors": [
            "Marta Ortin",
            "Adrian Jarabo",
            "Belen Masia",
            "Diego Gutierrez"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  With the increasing number of available consumer light field cameras, such as\nLytro TM, Raytrix TM, or Pelican Imaging TM, this new form of photography is\nprogressively becoming more common. However, there are still very few tools for\nlight field editing, and the interfaces to create those edits remain largely\nunexplored. Given the extended dimensionality of light field data, it is not\nclear what the most intuitive interfaces and optimal workflows are, in contrast\nwith well-studied 2D image manipulation software. In this work we provide a\ndetailed description of subjects' performance and preferences for a number of\nsimple editing tasks, which form the basis for more complex operations. We\nperform a detailed state sequence analysis and hidden Markov chain analysis\nbased on the sequence of tools and interaction paradigms users employ while\nediting light fields. These insights can aid researchers and designers in\ncreating new light field editing tools and interfaces, thus helping to close\nthe gap between 4D and 2D image editing.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.08729v1"
    },
    {
        "title": "On-the-fly Vertex Reuse for Massively-Parallel Software Geometry\n  Processing",
        "authors": [
            "Michael Kenzel",
            "Bernhard Kerbl",
            "Wolfgang Tatzgern",
            "Elena Ivanchenko",
            "Dieter Schmalstieg",
            "Markus Steinberger"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Compute-mode rendering is becoming more and more attractive for non-standard\nrendering applications, due to the high flexibility of compute-mode execution.\nThese newly designed pipelines often include streaming vertex and geometry\nprocessing stages. In typical triangle meshes, the same transformed vertex is\non average required six times during rendering. To avoid redundant computation,\na post-transform cache is traditionally suggested to enable reuse of vertex\nprocessing results. However, traditional caching neither scales well as the\nhardware becomes more parallel, nor can be efficiently implemented in a\nsoftware design. We investigate alternative strategies to reusing vertex\nshading results on-the-fly for massively parallel software geometry processing.\nForming static and dynamic batching on the data input stream, we analyze the\neffectiveness of identifying potential local reuse based on sorting, hashing,\nand efficient intra-thread-group communication. Altogether, we present four\nvertex reuse strategies, tailored to modern parallel architectures. Our\nsimulations showcase that our batch-based strategies significantly outperform\nparallel caches in terms of reuse. On actual GPU hardware, our evaluation shows\nthat our strategies not only lead to good reuse of processing results, but also\nboost performance by $2-3\\times$ compared to na\\\"ively ignoring reuse in a\nvariety of practical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.08893v1"
    },
    {
        "title": "Data-Driven Modeling of Group Entitativity in Virtual Environments",
        "authors": [
            "Aniket Bera",
            "Tanmay Randhavane",
            "Emily Kubin",
            "Husam Shaik",
            "Kurt Gray",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a data-driven algorithm to model and predict the socio-emotional\nimpact of groups on observers. Psychological research finds that highly\nentitative i.e. cohesive and uniform groups induce threat and unease in\nobservers. Our algorithm models realistic trajectory-level behaviors to\nclassify and map the motion-based entitativity of crowds. This mapping is based\non a statistical scheme that dynamically learns pedestrian behavior and\ncomputes the resultant entitativity induced emotion through group motion\ncharacteristics. We also present a novel interactive multi-agent simulation\nalgorithm to model entitative groups and conduct a VR user study to validate\nthe socio-emotional predictive power of our algorithm. We further show that\nmodel-generated high-entitativity groups do induce more negative emotions than\nlow-entitative groups.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.00028v1"
    },
    {
        "title": "Superimposition-guided Facial Reconstruction from Skull",
        "authors": [
            "Celong Liu",
            "Xin Li"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We develop a new algorithm to perform facial reconstruction from a given\nskull. This technique has forensic application in helping the identification of\nskeletal remains when other information is unavailable. Unlike most existing\nstrategies that directly reconstruct the face from the skull, we utilize a\ndatabase of portrait photos to create many face candidates, then perform a\nsuperimposition to get a well matched face, and then revise it according to the\nsuperimposition. To support this pipeline, we build an effective autoencoder\nfor image-based facial reconstruction, and a generative model for constrained\nface inpainting. Our experiments have demonstrated that the proposed pipeline\nis stable and accurate.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.00107v1"
    },
    {
        "title": "Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse\n  Inertial Measurements in Real Time",
        "authors": [
            "Yinghao Huang",
            "Manuel Kaufmann",
            "Emre Aksan",
            "Michael J. Black",
            "Otmar Hilliges",
            "Gerard Pons-Moll"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We demonstrate a novel deep neural network capable of reconstructing human\nfull body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on\nthe user's body. In doing so, we address several difficult challenges. First,\nthe problem is severely under-constrained as multiple pose parameters produce\nthe same IMU orientations. Second, capturing IMU data in conjunction with\nground-truth poses is expensive and difficult to do in many target application\nscenarios (e.g., outdoors). Third, modeling temporal dependencies through\nnon-linear optimization has proven effective in prior work but makes real-time\nprediction infeasible. To address this important limitation, we learn the\ntemporal pose priors using deep learning. To learn from sufficient data, we\nsynthesize IMU data from motion capture datasets. A bi-directional RNN\narchitecture leverages past and future information that is available at\ntraining time. At test time, we deploy the network in a sliding window fashion,\nretaining real time capabilities. To evaluate our method, we recorded DIP-IMU,\na dataset consisting of $10$ subjects wearing 17 IMUs for validation in $64$\nsequences with $330\\,000$ time instants; this constitutes the largest IMU\ndataset publicly available. We quantitatively evaluate our approach on multiple\ndatasets and show results from a real-time implementation. DIP-IMU and the code\nare available for research purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04703v1"
    },
    {
        "title": "Immercity: a curation content application in Virtual and Augmented\n  reality",
        "authors": [
            "Jean-Daniel Taupiac",
            "Nancy Rodriguez",
            "Olivier Strauss"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  When working with emergent and appealing technologies as Virtual Reality,\nMixed Reality and Augmented Reality, the issue of definitions appear very\noften. Indeed, our experience with various publics allows us to notice that\ntechnology definitions pose ambiguity and representation problems for informed\nas well as novice users. In this paper we present Immercity, a content curation\nsystem designed in the context of a collaboration between the University of\nMontpellier and CapGemi-ni, to deliver a technology watch. It is also used as a\ntestbed for our experiences with Virtual, Mixed and Augmented reality to\nexplore new interaction techniques and devices, artificial intelligence\nintegration, visual affordances, performance , etc. But another, very\ninteresting goal appeared: use Immercity to communicate about Virtual, Mixed\nand Augmented Reality by using them as a support.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.10206v1"
    },
    {
        "title": "Unsupervised Deep Learning for Structured Shape Matching",
        "authors": [
            "Jean-Michel Roufosse",
            "Abhishek Sharma",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a novel method for computing correspondences across 3D shapes\nusing unsupervised learning. Our method computes a non-linear transformation of\ngiven descriptor functions, while optimizing for global structural properties\nof the resulting maps, such as their bijectivity or approximate isometry. To\nthis end, we use the functional maps framework, and build upon the recent FMNet\narchitecture for descriptor learning. Unlike that approach, however, we show\nthat learning can be done in a purely \\emph{unsupervised setting}, without\nhaving access to any ground truth correspondences. This results in a very\ngeneral shape matching method that we call SURFMNet for Spectral Unsupervised\nFMNet, and which can be used to establish correspondences within 3D shape\ncollections without any prior information. We demonstrate on a wide range of\nchallenging benchmarks, that our approach leads to state-of-the-art results\ncompared to the existing unsupervised methods and achieves results that are\ncomparable even to the supervised learning techniques. Moreover, our framework\nis an order of magnitude faster, and does not rely on geodesic distance\ncomputation or expensive post-processing.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.03794v3"
    },
    {
        "title": "Eyes on the Prize: Improved Biological Surface Registration via Forward\n  Propagation",
        "authors": [
            "Robert J. Ravier"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Many algorithms for surface registration risk producing significant errors if\nsurfaces are significantly nonisometric. Manifold learning has been shown to be\neffective at improving registration quality, using information from an entire\ncollection of surfaces to correct issues present in pairwise registrations.\nThese methods, however, are not robust to changes in the collection of\nsurfaces, or do not produce accurate registrations at a resolution high enough\nfor subsequent downstream analysis. We propose a novel algorithm for\nefficiently registering such collections given initial correspondences with\nvarying degrees of accuracy. By combining the initial information with recent\ndevelopments in manifold learning, we employ a simple metric condition to\nconstruct a measure on the space of correspondences between any pair of shapes\nin our collection, which we then use to distill soft correspondences. We\ndemonstrate that this measure can improve correspondence accuracy between\nfeature points compared to currently employed, less robust methods on a diverse\ndataset of surfaces from evolutionary biology. We then show how our methods can\nbe used, in combination with recent sampling and interpolation methods, to\ncompute accurate and consistent homeomorphisms between surfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.10592v2"
    },
    {
        "title": "Liver Pathology Simulation: Algorithm for Haptic Rendering and Force\n  Maps for Palpation Assessment",
        "authors": [
            "Felix G. Hamza-Lup",
            "Adrian Seitan",
            "Dorin M. Popovici",
            "Crenguta M. Bogdan"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Preoperative gestures include tactile sampling of the mechanical properties\nof biological tissue for both histological and pathological considerations.\nTactile properties used in conjunction with visual cues can provide useful\nfeedback to the surgeon. Development of novel cost effective haptic-based\nsimulators and their introduction in the minimally invasive surgery learning\ncycle can absorb the learning curve for your residents. Receiving pre-training\nin a core set of surgical skills can reduce skill acquisition time and risks.\nWe present the integration of a real-time surface stiffness adjustment\nalgorithm and a novel paradigm -- force maps -- in a visuo-haptic simulator\nmodule designed to train internal organs disease diagnostics through palpation.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01249v1"
    },
    {
        "title": "NormalNet: Learning-based Normal Filtering for Mesh Denoising",
        "authors": [
            "Wenbo Zhao",
            "Xianming Liu",
            "Yongsen Zhao",
            "Xiaopeng Fan",
            "Debin Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Mesh denoising is a critical technology in geometry processing that aims to\nrecover high-fidelity 3D mesh models of objects from their noise-corrupted\nversions. In this work, we propose a learning-based normal filtering scheme for\nmesh denoising called NormalNet, which maps the guided normal filtering (GNF)\ninto a deep network. The scheme follows the iterative framework of\nfiltering-based mesh denoising. During each iteration, first, the voxelization\nstrategy is applied on each face in a mesh to transform the irregular local\nstructure into the regular volumetric representation, therefore, both the\nstructure and face normal information are preserved and the convolution\noperations in CNN(Convolutional Neural Network) can be easily performed.\nSecond, instead of the guidance normal generation and the guided filtering in\nGNF, a deep CNN is designed, which takes the volumetric representation as\ninput, and outputs the learned filtered normals. At last, the vertex positions\nare updated according to the filtered normals. Specifically, the iterative\ntraining framework is proposed, in which the generation of training data and\nthe network training are alternately performed, whereas the ground truth\nnormals are taken as the guidance normals in GNF to get the target normals.\nCompared to state-of-the-art works, NormalNet can effectively remove noise\nwhile preserving the original features and avoiding pseudo-features.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.04015v2"
    },
    {
        "title": "Smart, Deep Copy-Paste",
        "authors": [
            "Tiziano Portenier",
            "Qiyang Hu",
            "Paolo Favaro",
            "Matthias Zwicker"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this work, we propose a novel system for smart copy-paste, enabling the\nsynthesis of high-quality results given a masked source image content and a\ntarget image context as input. Our system naturally resolves both shading and\ngeometric inconsistencies between source and target image, resulting in a\nmerged result image that features the content from the pasted source image,\nseamlessly pasted into the target context. Our framework is based on a novel\ntraining image transformation procedure that allows to train a deep\nconvolutional neural network end-to-end to automatically learn a representation\nthat is suitable for copy-pasting. Our training procedure works with any image\ndataset without additional information such as labels, and we demonstrate the\neffectiveness of our system on two popular datasets, high-resolution face\nimages and the more complex Cityscapes dataset. Our technique outperforms the\ncurrent state of the art on face images, and we show promising results on the\nCityscapes dataset, demonstrating that our system generalizes to much higher\nresolution than the training data.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.06763v1"
    },
    {
        "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams",
        "authors": [
            "Jiacheng Pan",
            "Wei Chen",
            "Xiaodong Zhao",
            "Shuyue Zhou",
            "Wei Zeng",
            "Minfeng Zhu",
            "Jian Chen",
            "Siwei Fu",
            "Yingcai Wu"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We design and evaluate a novel layout fine-tuning technique for node-link\ndiagrams that facilitates exemplar-based adjustment of a group of substructures\nin batching mode. The key idea is to transfer user modifications on a local\nsubstructure to other substructures in the whole graph that are topologically\nsimilar to the exemplar. We first precompute a canonical representation for\neach substructure with node embedding techniques and then use it for on-the-fly\nsubstructure retrieval. We design and develop a light-weight interactive system\nto enable intuitive adjustment, modification transfer, and visual graph\nexploration. We also report some results of quantitative comparisons, three\ncase studies, and a within-participant user study.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.00666v6"
    },
    {
        "title": "Real-Time Cleaning and Refinement of Facial Animation Signals",
        "authors": [
            "Eloïse Berson",
            "Catherine Soladié",
            "Nicolas Stoiber"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  With the increasing demand for real-time animated 3D content in the\nentertainment industry and beyond, performance-based animation has garnered\ninterest among both academic and industrial communities. While recent solutions\nfor motion-capture animation have achieved impressive results, handmade\npost-processing is often needed, as the generated animations often contain\nartifacts. Existing real-time motion capture solutions have opted for standard\nsignal processing methods to strengthen temporal coherence of the resulting\nanimations and remove inaccuracies. While these methods produce smooth results,\nthey inherently filter-out part of the dynamics of facial motion, such as high\nfrequency transient movements. In this work, we propose a real-time animation\nrefining system that preserves -- or even restores -- the natural dynamics of\nfacial motions. To do so, we leverage an off-the-shelf recurrent neural network\narchitecture that learns proper facial dynamics patterns on clean animation\ndata. We parametrize our system using the temporal derivatives of the signal,\nenabling our network to process animations at any framerate. Qualitative\nresults show that our system is able to retrieve natural motion signals from\nnoisy or degraded input animation.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.01332v1"
    },
    {
        "title": "Accelerating Evolutionary Construction Tree Extraction via Graph\n  Partitioning",
        "authors": [
            "Markus Friedrich",
            "Sebastian Feld",
            "Thomy Phan",
            "Pierre-Alain Fayolle"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Extracting a Construction Tree from potentially noisy point clouds is an\nimportant aspect of Reverse Engineering tasks in Computer Aided Design.\nSolutions based on algorithmic geometry impose constraints on usable model\nrepresentations (e.g. quadric surfaces only) and noise robustness.\nRe-formulating the problem as a combinatorial optimization problem and solving\nit with an Evolutionary Algorithm can mitigate some of these constraints at the\ncost of increased computational complexity. This paper proposes a graph-based\nsearch space partitioning scheme that is able to accelerate Evolutionary\nConstruction Tree extraction while exploiting parallelization capabilities of\nmodern CPUs. The evaluation indicates a speed-up up to a factor of $46.6$\ncompared to the baseline approach while resulting tree sizes increased by\n$25.2\\%$ to $88.6\\%$.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.03669v1"
    },
    {
        "title": "The Topology of Shapes Made with Points",
        "authors": [
            "Alexandros Haridis"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In architecture, city planning, visual arts, and other design areas, shapes\nare often made with points, or with structural representations based on\npoint-sets. Shapes made with points can be understood more generally as finite\narrangements formed with elements (i.e. points) of the algebra of shapes $U_i$,\nfor $i = 0$. This paper examines the kind of topology that is applicable to\nsuch shapes. From a mathematical standpoint, any \"shape made with points\" is\nequivalent to a finite space, so that topology on a shape made with points is\nno different than topology on a finite space: the study of topological\nstructure naturally coincides with the study of preorder relations on the\npoints of the shape. After establishing this fact, some connections between the\ntopology of shapes made with points and the topology of \"point-free\" pictorial\nshapes (when $i > 0$) are discussed and the main differences between the two\nare summarized.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.05262v1"
    },
    {
        "title": "DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape\n  Generation",
        "authors": [
            "Jie Yang",
            "Kaichun Mo",
            "Yu-Kun Lai",
            "Leonidas J. Guibas",
            "Lin Gao"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  D shape generation is a fundamental operation in computer graphics. While\nsignificant progress has been made, especially with recent deep generative\nmodels, it remains a challenge to synthesize high-quality shapes with rich\ngeometric details and complex structure, in a controllable manner. To tackle\nthis, we introduce DSG-Net, a deep neural network that learns a disentangled\nstructured and geometric mesh representation for 3D shapes, where two key\naspects of shapes, geometry, and structure, are encoded in a synergistic manner\nto ensure plausibility of the generated shapes, while also being disentangled\nas much as possible. This supports a range of novel shape generation\napplications with disentangled control, such as interpolation of structure\n(geometry) while keeping geometry (structure) unchanged. To achieve this, we\nsimultaneously learn structure and geometry through variational autoencoders\n(VAEs) in a hierarchical manner for both, with bijective mappings at each\nlevel. In this manner, we effectively encode geometry and structure in separate\nlatent spaces, while ensuring their compatibility: the structure is used to\nguide the geometry and vice versa. At the leaf level, the part geometry is\nrepresented using a conditional part VAE, to encode high-quality geometric\ndetails, guided by the structure context as the condition. Our method not only\nsupports controllable generation applications but also produces high-quality\nsynthesized shapes, outperforming state-of-the-art methods. The code has been\nreleased at https://github.com/IGLICT/DSG-Net.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.05440v4"
    },
    {
        "title": "MetroSets: Visualizing Sets as Metro Maps",
        "authors": [
            "Ben Jacobsen",
            "Markus Wallinger",
            "Stephen Kobourov",
            "Martin Nöllenburg"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose MetroSets, a new, flexible online tool for visualizing set systems\nusing the metro map metaphor. We model a given set system as a hypergraph\n$\\mathcal{H} = (V, \\mathcal{S})$, consisting of a set $V$ of vertices and a set\n$\\mathcal{S}$, which contains subsets of $V$ called hyperedges. Our system then\ncomputes a metro map representation of $\\mathcal{H}$, where each hyperedge $E$\nin $\\mathcal{S}$ corresponds to a metro line and each vertex corresponds to a\nmetro station. Vertices that appear in two or more hyperedges are drawn as\ninterchanges in the metro map, connecting the different sets. MetroSets is\nbased on a modular 4-step pipeline which constructs and optimizes a path-based\nhypergraph support, which is then drawn and schematized using metro map layout\nalgorithms. We propose and implement multiple algorithms for each step of the\nMetroSet pipeline and provide a functional prototype with easy-to-use preset\nconfigurations. Furthermore, using several real-world datasets, we perform an\nextensive quantitative evaluation of the impact of different pipeline stages on\ndesirable properties of the generated maps, such as octolinearity,\nmonotonicity, and edge uniformity.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.09367v2"
    },
    {
        "title": "A Square Equal-area Map Projection with Low Angular Distortion, Minimal\n  Cusps, and Closed-form Solutions",
        "authors": [
            "Matthew A. Petroff"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  A novel square equal-area map projection is proposed. The projection combines\nclosed-form forward and inverse solutions with relatively low angular\ndistortion and minimal cusps, a combination of properties not manifested by any\npreviously published square equal-area projection. Thus, the new projection has\nlower angular distortion than any previously published square equal-area\nprojection with a closed-form solution. Utilizing a quincuncial arrangement,\nthe new projection places the north pole at the center of the square and\ndivides the south pole between its four corners; the projection can be\nseamlessly tiled. The existence of closed-form solutions makes the projection\nsuitable for real-time visualization applications, both in cartography and in\nother areas, such as for the display of panoramic images.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.13670v3"
    },
    {
        "title": "Comparative Study of Geometric and Image Based Modelling and Rendering\n  Techniques",
        "authors": [
            "Agrima Seth",
            "Deepak Mishra"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  This is a comparative study of the traditional 3D computer graphics technique\nof geometric modelling and image-based rendering techniques that were surveyed\nand implemented.We have discussed the classifications and representative\nmethods of both the techniques. The study has shown that there is a strong\ncontinuum between both the techniques and a hybrid of the two is most suitable\nfor further implementations.This hybridisation study is underway to create\nmodels of real life situations and provide disaster management training.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.5024v1"
    },
    {
        "title": "Reactive Programming for Interactive Graphics",
        "authors": [
            "Yihui Xie",
            "Heike Hofmann",
            "Xiaoyue Cheng"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  One of the big challenges of developing interactive statistical applications\nis the management of the data pipeline, which controls transformations from\ndata to plot. The user's interactions needs to be propagated through these\nmodules and reflected in the output representation at a fast pace. Each\nindividual module may be easy to develop and manage, but the dependency\nstructure can be quite challenging. The MVC (Model/View/Controller) pattern is\nan attempt to solve the problem by separating the user's interaction from the\nrepresentation of the data. In this paper we discuss the paradigm of reactive\nprogramming in the framework of the MVC architecture and show its applicability\nto interactive graphics. Under this paradigm, developers benefit from the\nseparation of user interaction from the graphical representation, which makes\nit easier for users and developers to extend interactive applications. We show\nthe central role of reactive data objects in an interactive graphics system,\nimplemented as the R package cranvas, which is freely available on GitHub and\nthe main developers include the authors of this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.7256v1"
    },
    {
        "title": "Using 3D Printing to Visualize Social Media Big Data",
        "authors": [
            "Zachary Weber",
            "Vijay Gadepally"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Big data volume continues to grow at unprecedented rates. One of the key\nfeatures that makes big data valuable is the promise to find unknown patterns\nor correlations that may be able to improve the quality of processes or\nsystems. Unfortunately, with the exponential growth in data, users often have\ndifficulty in visualizing the often-unstructured, non-homogeneous data coming\nfrom a variety of sources. The recent growth in popularity of 3D printing has\nushered in a revolutionary way to interact with big data. Using a 3D printed\nmockup up a physical or notional environment, one can display data on the\nmockup to show real-time data patterns. In this poster and demonstration, we\ndescribe the process of 3D printing and demonstrate an application of\ndisplaying Twitter data on a 3D mockup of the Massachusetts Institute of\nTechnology (MIT) campus, known as LuminoCity.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.7724v1"
    },
    {
        "title": "Deep Bilateral Learning for Real-Time Image Enhancement",
        "authors": [
            "Michaël Gharbi",
            "Jiawen Chen",
            "Jonathan T. Barron",
            "Samuel W. Hasinoff",
            "Frédo Durand"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Performance is a critical challenge in mobile image processing. Given a\nreference imaging pipeline, or even human-adjusted pairs of images, we seek to\nreproduce the enhancements and enable real-time evaluation. For this, we\nintroduce a new neural network architecture inspired by bilateral grid\nprocessing and local affine color transforms. Using pairs of input/output\nimages, we train a convolutional neural network to predict the coefficients of\na locally-affine model in bilateral space. Our architecture learns to make\nlocal, global, and content-dependent decisions to approximate the desired image\ntransformation. At runtime, the neural network consumes a low-resolution\nversion of the input image, produces a set of affine transformations in\nbilateral space, upsamples those transformations in an edge-preserving fashion\nusing a new slicing node, and then applies those upsampled transformations to\nthe full-resolution image. Our algorithm processes high-resolution images on a\nsmartphone in milliseconds, provides a real-time viewfinder at 1080p\nresolution, and matches the quality of state-of-the-art approximation\ntechniques on a large class of image operators. Unlike previous work, our model\nis trained off-line from data and therefore does not require access to the\noriginal operator at runtime. This allows our model to learn complex,\nscene-dependent transformations for which no reference implementation is\navailable, such as the photographic edits of a human retoucher.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02880v2"
    },
    {
        "title": "Visual Detection of Structural Changes in Time-Varying Graphs Using\n  Persistent Homology",
        "authors": [
            "Mustafa Hajij",
            "Bei Wang",
            "Carlos Scheidegger",
            "Paul Rosen"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Topological data analysis is an emerging area in exploratory data analysis\nand data mining. Its main tool, persistent homology, has become a popular\ntechnique to study the structure of complex, high-dimensional data. In this\npaper, we propose a novel method using persistent homology to quantify\nstructural changes in time-varying graphs. Specifically, we transform each\ninstance of the time-varying graph into metric spaces, extract topological\nfeatures using persistent homology, and compare those features over time. We\nprovide a visualization that assists in time-varying graph exploration and\nhelps to identify patterns of behavior within the data. To validate our\napproach, we conduct several case studies on real world data sets and show how\nour method can find cyclic patterns, deviations from those patterns, and\none-time events in time-varying graphs. We also examine whether\npersistence-based similarity measure as a graph metric satisfies a set of\nwell-established, desirable properties for graph metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.06683v2"
    },
    {
        "title": "Notes on optimal approximations for importance sampling",
        "authors": [
            "Jacopo Pantaleoni",
            "Eric Heitz"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In this manuscript, we derive optimal conditions for building function\napproximations that minimize variance when used as importance sampling\nestimators for Monte Carlo integration problems. Particularly, we study the\nproblem of finding the optimal projection $g$ of an integrand $f$ onto certain\nclasses of piecewise constant functions, in order to minimize the variance of\nthe unbiased importance sampling estimator $E_g[f/g]$, as well as the related\nproblem of finding optimal mixture weights to approximate and importance sample\na target mixture distribution $f = \\sum_i \\alpha_i f_i$ with components $f_i$\nin a family $\\mathcal{F}$, through a corresponding mixture of importance\nsampling densities $g_i$ that are only approximately proportional to $f_i$. We\nfurther show that in both cases the optimal projection is different from the\ncommonly used $\\ell_1$ projection, and provide an intuitive explanation for the\ndifference.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.08358v2"
    },
    {
        "title": "3D Sketching using Multi-View Deep Volumetric Prediction",
        "authors": [
            "Johanna Delanoy",
            "Mathieu Aubry",
            "Phillip Isola",
            "Alexei A. Efros",
            "Adrien Bousseau"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Sketch-based modeling strives to bring the ease and immediacy of drawing to\nthe 3D world. However, while drawings are easy for humans to create, they are\nvery challenging for computers to interpret due to their sparsity and\nambiguity. We propose a data-driven approach that tackles this challenge by\nlearning to reconstruct 3D shapes from one or more drawings. At the core of our\napproach is a deep convolutional neural network (CNN) that predicts occupancy\nof a voxel grid from a line drawing. This CNN provides us with an initial 3D\nreconstruction as soon as the user completes a single drawing of the desired\nshape. We complement this single-view network with an updater CNN that refines\nan existing prediction given a new drawing of the shape created from a novel\nviewpoint. A key advantage of our approach is that we can apply the updater\niteratively to fuse information from an arbitrary number of viewpoints, without\nrequiring explicit stroke correspondences between the drawings. We train both\nCNNs by rendering synthetic contour drawings from hand-modeled shape\ncollections as well as from procedurally-generated abstract shapes. Finally, we\nintegrate our CNNs in a minimal modeling interface that allows users to\nseamlessly draw an object, rotate it to see its 3D reconstruction, and refine\nit by re-drawing from another vantage point using the 3D reconstruction as\nguidance. The main strengths of our approach are its robustness to freehand\nbitmap drawings, its ability to adapt to different object categories, and the\ncontinuum it offers between single-view and multi-view sketch-based modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.08390v4"
    },
    {
        "title": "Superposition de calques monochromes d'opacités variables",
        "authors": [
            "Alexandre Bali"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  For a monochrome layer $x$ of opacity $0\\le o_x\\le1 $ placed on another\nmonochrome layer of opacity 1, the result given by the standard formula is\n$$\\small\\Pi\\left({\\bf\nC}_\\varphi\\right)=1+\\sum_{n=1}^2\\left(2-n-(-1)^no_{\\chi(\\varphi+1)}\\right)\\left(\\chi(n+\\varphi-1)-o_{\\chi(n+\\varphi-1)}\\right),$$\nthe formula being of course explained in detail in this paper. We will\neventually deduce a very simple theorem, generalize it and then see its\nvalidity with alternative formulas to this standard containing the same main\nproperties here exposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09839v4"
    },
    {
        "title": "AnimePose: Multi-person 3D pose estimation and animation",
        "authors": [
            "Laxman Kumarapu",
            "Prerana Mukherjee"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  3D animation of humans in action is quite challenging as it involves using a\nhuge setup with several motion trackers all over the person's body to track the\nmovements of every limb. This is time-consuming and may cause the person\ndiscomfort in wearing exoskeleton body suits with motion sensors. In this work,\nwe present a trivial yet effective solution to generate 3D animation of\nmultiple persons from a 2D video using deep learning. Although significant\nimprovement has been achieved recently in 3D human pose estimation, most of the\nprior works work well in case of single person pose estimation and multi-person\npose estimation is still a challenging problem. In this work, we firstly\npropose a supervised multi-person 3D pose estimation and animation framework\nnamely AnimePose for a given input RGB video sequence. The pipeline of the\nproposed system consists of various modules: i) Person detection and\nsegmentation, ii) Depth Map estimation, iii) Lifting 2D to 3D information for\nperson localization iv) Person trajectory prediction and human pose tracking.\nOur proposed system produces comparable results on previous state-of-the-art 3D\nmulti-person pose estimation methods on publicly available datasets MuCo-3DHP\nand MuPoTS-3D datasets and it also outperforms previous state-of-the-art human\npose tracking methods by a significant margin of 11.7% performance gain on MOTA\nscore on Posetrack 2018 dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.02792v1"
    },
    {
        "title": "Visualizing modular forms",
        "authors": [
            "David Lowry-Duda"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We examine several currently used techniques for visualizing complex-valued\nfunctions applied to modular forms. We plot several examples and study the\nbenefits and limitations of each technique. We then introduce a method of\nvisualization that can take advantage of colormaps in Python's matplotlib\nlibrary, describe an implementation, and give more examples. Much of this\ndiscussion applies to general visualizations of complex-valued functions in the\nplane.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.05234v2"
    },
    {
        "title": "A Large-Scale Evaluation of Shape-Aware Neighborhood Weights and\n  Neighborhood Sizes",
        "authors": [
            "Martin Skrodzki",
            "Eric Zimmermann"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we define and evaluate a weighting scheme for neighborhoods in\npoint sets. Our weighting takes the shape of the geometry, i.e., the normal\ninformation, into account. This causes the obtained neighborhoods to be more\nreliable in the sense that connectivity also depends on the orientation of the\npoint set. We utilize a sigmoid to define the weights based on the normal\nvariation. For an evaluation of the weighting scheme, we turn to a Shannon\nentropy model for feature classification that can be proven to be\nnon-degenerate for our family of weights. Based on this model, we evaluate our\nweighting terms on a large scale of both clean and real-world models. This\nevaluation provides results regarding the choice of optimal parameters within\nour weighting scheme. Furthermore, the large-scale evaluation also reveals that\nneighborhood sizes should not be fixed globally when processing models.\nFinally, we highlight the applicability of our weighting scheme withing the\napplication context of denoising.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.06827v3"
    },
    {
        "title": "Jittering Samples using a kd-Tree Stratification",
        "authors": [
            "Alexandros D. Keros",
            "Divakaran Divakaran",
            "Kartic Subr"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Monte Carlo sampling techniques are used to estimate high-dimensional\nintegrals that model the physics of light transport in virtual scenes for\ncomputer graphics applications. These methods rely on the law of large numbers\nto estimate expectations via simulation, typically resulting in slow\nconvergence. Their errors usually manifest as undesirable grain in the pictures\ngenerated by image synthesis algorithms. It is well known that these errors\ndiminish when the samples are chosen appropriately. A well known technique for\nreducing error operates by subdividing the integration domain, estimating\nintegrals in each \\emph{stratum} and aggregating these values into a stratified\nsampling estimate. Na\\\"{i}ve methods for stratification, based on a lattice\n(grid) are known to improve the convergence rate of Monte Carlo, but require\nsamples that grow exponentially with the dimensionality of the domain.\n  We propose a simple stratification scheme for $d$ dimensional hypercubes\nusing the kd-tree data structure. Our scheme enables the generation of an\narbitrary number of equal volume partitions of the rectangular domain, and $n$\nsamples can be generated in $O(n)$ time. Since we do not always need to\nexplicitly build a kd-tree, we provide a simple procedure that allows the\nsample set to be drawn fully in parallel without any precomputation or storage,\nspeeding up sampling to $O(\\log n)$ time per sample when executed on $n$ cores.\nIf the tree is implicitly precomputed ($O(n)$ storage) the parallelised run\ntime reduces to $O(1)$ on $n$ cores. In addition to these benefits, we provide\nan upper bound on the worst case star-discrepancy for $n$ samples matching that\nof lattice-based sampling strategies, which occur as a special case of our\nproposed method. We use a number of quantitative and qualitative tests to\ncompare our method against state of the art samplers for image synthesis.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.07002v1"
    },
    {
        "title": "Real-Time Visualization in Non-Isotropic Geometries",
        "authors": [
            "Eryk Kopczyński",
            "Dorota Celińska-Kopczyńska"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Non-isotropic geometries are of interest to low-dimensional topologists,\nphysicists and cosmologists. However, they are challenging to comprehend and\nvisualize. We present novel methods of computing real-time native geodesic\nrendering of non-isotropic geometries. Our methods can be applied not only to\nvisualization, but also are essential for potential applications in machine\nlearning and video games.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.09533v2"
    },
    {
        "title": "Image Stylization: From Predefined to Personalized",
        "authors": [
            "Ignacio Garcia-Dorado",
            "Pascal Getreuer",
            "Bartlomiej Wronski",
            "Peyman Milanfar"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a framework for interactive design of new image stylizations using\na wide range of predefined filter blocks. Both novel and off-the-shelf image\nfiltering and rendering techniques are extended and combined to allow the user\nto unleash their creativity to intuitively invent, modify, and tune new styles\nfrom a given set of filters. In parallel to this manual design, we propose a\nnovel procedural approach that automatically assembles sequences of filters,\nleading to unique and novel styles. An important aim of our framework is to\nallow for interactive exploration and design, as well as to enable videos and\ncamera streams to be stylized on the fly. In order to achieve this real-time\nperformance, we use the \\textit{Best Linear Adaptive Enhancement} (BLADE)\nframework -- an interpretable shallow machine learning method that simulates\ncomplex filter blocks in real time. Our representative results include over a\ndozen styles designed using our interactive tool, a set of styles created\nprocedurally, and new filters trained with our BLADE approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.10945v1"
    },
    {
        "title": "$G^1$ hole filling with S-patches made easy",
        "authors": [
            "Péter Salvi"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  S-patches have been around for 30 years, but they are seldom used, and are\nconsidered more of a mathematical curiosity than a practical surface\nrepresentation. In this article a method is presented for automatically\ncreating S-patches of any degree or any number of sides, suitable for inclusion\nin a curve network with tangential continuity to the adjacent surfaces. The\npresentation aims at making the implementation straightforward; a few examples\nconclude the paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.11109v1"
    },
    {
        "title": "On the CAD-compatible conversion of S-patches",
        "authors": [
            "Péter Salvi"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  S-patches have many nice mathematical properties. It is known since their\nfirst appearance, that any regular S-patch can be exactly converted into a\ntrimmed rational B\\'ezier surface. This is a big advantage compared to other\nmulti-sided surface representations that have to be approximated for exporting\nthem into CAD/CAM systems. The actual conversion process, however, remained at\na theoretical level, with bits and pieces scattered in multiple publications.\nIn this paper we review the entirety of the algorithm, and investigate it from\na practical aspect.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.11111v1"
    },
    {
        "title": "Computationally efficient transfinite patches with fullness control",
        "authors": [
            "Péter Salvi",
            "István Kovács",
            "Tamás Várady"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Transfinite patches provide a simple and elegant solution to the problem of\nrepresenting non-four-sided continuous surfaces, which are useful in a variety\nof applications, such as curve network based design. Real-time responsiveness\nis essential in this context, and thus reducing the computation cost is an\nimportant concern. The Midpoint Coons (MC) patch presented in this paper is a\nfusion of two previous transfinite schemes, combining the speed of one with the\nsuperior control mechanism of the other. This is achieved using a new\nconstrained parameterization based on generalized barycentric coordinates and\ntransfinite blending functions.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.11212v1"
    },
    {
        "title": "A multi-sided generalization of the $C^0$ Coons patch",
        "authors": [
            "Péter Salvi"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Most multi-sided transfinite surfaces require cross-derivatives at the\nboundaries. Here we show a general $n$-sided patch that interpolates all\nboundaries based on only positional information. The surface is a weighted sum\nof $n$ Coons patches, using a parameterization based on Wachspress coordinates.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.11347v1"
    },
    {
        "title": "Complex Eigenvalues for Binary Subdivision Schemes",
        "authors": [
            "Christian Kuehn"
        ],
        "category": "cs.GR",
        "published_year": "2008",
        "summary": "  Convergence properties of binary stationary subdivision schemes for curves\nhave been analyzed using the techniques of z-transforms and eigenanalysis.\nEigenanalysis provides a way to determine derivative continuity at specific\npoints based on the eigenvalues of a finite matrix. None of the well-known\nsubdivision schemes for curves have complex eigenvalues. We prove when a\nconvergent scheme with palindromic mask can have complex eigenvalues and that a\nlower limit for the size of the mask exists in this case. We find a scheme with\ncomplex eigenvalues achieving this lower bound. Furthermore we investigate this\nscheme numerically and explain from a geometric viewpoint why such a scheme has\nnot yet been used in computer-aided geometric design.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.3249v1"
    },
    {
        "title": "Electronic Visualisation in Chemistry: From Alchemy to Art",
        "authors": [
            "Karl Harrison",
            "Jonathan P. Bowen",
            "Alice M. Bowen"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Chemists now routinely use software as part of their work. For example,\nvirtual chemistry allows chemical reactions to be simulated. In particular, a\nselection of software is available for the visualisation of complex\n3-dimensional molecular structures. Many of these are very beautiful in their\nown right. As well as being included as illustrations in academic papers, such\nvisualisations are often used on the covers of chemistry journals as\nartistically decorative and attractive motifs. Chemical images have also been\nused as the basis of artworks in exhibitions. This paper explores the\ndevelopment of the relationship of chemistry, art, and IT. It covers some of\nthe increasingly sophisticated software used to generate these projections\n(e.g., UCSF Chimera) and their progressive use as a visual art form.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.6360v1"
    },
    {
        "title": "Real-time Decolorization using Dominant Colors",
        "authors": [
            "Wei Hu",
            "Wei Li",
            "Fan Zhang",
            "Qian Du"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Decolorization is the process to convert a color image or video to its\ngrayscale version, and it has received great attention in recent years. An\nideal decolorization algorithm should preserve the original color contrast as\nmuch as possible. Meanwhile, it should provide the final decolorized result as\nfast as possible. However, most of the current methods are suffering from\neither unsatisfied color information preservation or high computational cost,\nlimiting their application value. In this paper, a simple but effective\ntechnique is proposed for real-time decolorization. Based on the typical\nrgb2gray() color conversion model, which produces a grayscale image by linearly\ncombining R, G, and B channels, we propose a dominant color hypothesis and a\ncorresponding distance measurement metric to evaluate the quality of grayscale\nconversion. The local optimum scheme provides several \"good\" candidates in a\nconfidence interval, from which the \"best\" result can be extracted.\nExperimental results demonstrate that remarkable simplicity of the proposed\nmethod facilitates the process of high resolution images and videos in\nreal-time using a common CPU.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.2728v2"
    },
    {
        "title": "Geometric elements and classification of quadrics in rational Bézier\n  form",
        "authors": [
            "A. Cantón",
            "L. Fernández-Jambrina",
            "M. E. Rosado María",
            "M. J. Vázquez-Gallo"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  In this paper we classify and derive closed formulas for geometric elements\nof quadrics in rational B\\'ezier triangular form (such as the center, the conic\nat infinity, the vertex and the axis of paraboloids and the principal planes),\nusing just the control vertices and the weights for the quadric patch. The\nresults are extended also to quadric tensor product patches. Our results rely\non using techniques from projective algebraic geometry to find suitable\nbilinear forms for the quadric in a coordinate-free fashion, considering a\npencil of quadrics that are tangent to the given quadric along a conic. Most of\nthe information about the quadric is encoded in one coefficient, involving the\nweights of the patch, which allows us to tell apart oval from ruled quadrics.\nThis coefficient is also relevant to determine the affine type of the quadric.\nSpheres and quadrics of revolution are characterised within this framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.00967v2"
    },
    {
        "title": "Lens Factory: Automatic Lens Generation Using Off-the-shelf Components",
        "authors": [
            "Libin Sun",
            "Brian Guenter",
            "Neel Joshi",
            "Patrick Therien",
            "James Hays"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Custom optics is a necessity for many imaging applications. Unfortunately,\ncustom lens design is costly (thousands to tens of thousands of dollars), time\nconsuming (10-12 weeks typical lead time), and requires specialized optics\ndesign expertise. By using only inexpensive, off-the-shelf lens components the\nLens Factory automatic design system greatly reduces cost and time. Design,\nordering of parts, delivery, and assembly can be completed in a few days, at a\ncost in the low hundreds of dollars. Lens design constraints, such as focal\nlength and field of view, are specified in terms familiar to the graphics\ncommunity so no optics expertise is necessary. Unlike conventional lens design\nsystems, which only use continuous optimization methods, Lens Factory adds a\ndiscrete optimization stage. This stage searches the combinatorial space of\npossible combinations of lens elements to find novel designs, evolving simple\ncanonical lens designs into more complex, better designs. Intelligent pruning\nrules make the combinatorial search feasible. We have designed and built\nseveral high performance optical systems which demonstrate the practicality of\nthe system.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.08956v2"
    },
    {
        "title": "Automatic Knot Adjustment Using Dolphin Echolocation Algorithm for\n  B-Spline Curve Approximation",
        "authors": [
            "Hasan Ali Akyürek",
            "Erkan Ülker",
            "Barış Koçer"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In this paper, a new approach to solve the cubic B-spline curve fitting\nproblem is presented based on a meta-heuristic algorithm called \" dolphin\necholocation \". The method minimizes the proximity error value of the selected\nnodes that measured using the least squares method and the Euclidean distance\nmethod of the new curve generated by the reverse engineering. The results of\nthe proposed method are compared with the genetic algorithm. As a result, this\nnew method seems to be successful.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.04383v1"
    },
    {
        "title": "Bezier developable surfaces",
        "authors": [
            "L. Fernández-Jambrina"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In this paper we address the issue of designing developable surfaces with\nBezier patches. We show that developable surfaces with a polynomial edge of\nregression are the set of developable surfaces which can be constructed with\nAumann's algorithm. We also obtain the set of polynomial developable surfaces\nwhich can be constructed using general polynomial curves. The conclusions can\nbe extended to spline surfaces as well.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.02878v1"
    },
    {
        "title": "A Data-driven Approach for Furniture and Indoor Scene Colorization",
        "authors": [
            "Jie Zhu",
            "Yanwen Guo",
            "Han Ma"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a data-driven approach that colorizes 3D furniture models and\nindoor scenes by leveraging indoor images on the internet. Our approach is able\nto colorize the furniture automatically according to an example image. The core\nis to learn image-guided mesh segmentation to segment the model into different\nparts according to the image object. Given an indoor scene, the system supports\ncolorization-by-example, and has the ability to recommend the colorization\nscheme that is consistent with a user-desired color theme. The latter is\nrealized by formulating the problem as a Markov random field model that imposes\nuser input as an additional constraint. We contribute to the community a\nhierarchically organized image-model database with correspondences between each\nimage and the corresponding model at the part-level. Our experiments and a user\nstudy show that our system produces perceptually convincing results comparable\nto those generated by interior designers.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.08680v1"
    },
    {
        "title": "Exploration of Heterogeneous Data Using Robust Similarity",
        "authors": [
            "Mahsa Mirzargar",
            "Ross T. Whitaker",
            "Robert M. Kirby"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Heterogeneous data pose serious challenges to data analysis tasks, including\nexploration and visualization. Current techniques often utilize dimensionality\nreductions, aggregation, or conversion to numerical values to analyze\nheterogeneous data. However, the effectiveness of such techniques to find\nsubtle structures such as the presence of multiple modes or detection of\noutliers is hindered by the challenge to find the proper subspaces or prior\nknowledge to reveal the structures. In this paper, we propose a generic\nsimilarity-based exploration technique that is applicable to a wide variety of\ndatatypes and their combinations, including heterogeneous ensembles. The\nproposed concept of similarity has a close connection to statistical analysis\nand can be deployed for summarization, revealing fine structures such as the\npresence of multiple modes, and detection of anomalies or outliers. We then\npropose a visual encoding framework that enables the exploration of a\nheterogeneous dataset in different levels of detail and provides insightful\ninformation about both global and local structures. We demonstrate the utility\nof the proposed technique using various real datasets, including ensemble data.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.02862v1"
    },
    {
        "title": "Embedded Spectral Descriptors: Learning the point-wise correspondence\n  metric via Siamese neural networks",
        "authors": [
            "Zhiyu Sun",
            "Yusen He",
            "Andrey Gritsenko",
            "Amaury Lendasse",
            "Stephen Baek"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  A robust and informative local shape descriptor plays an important role in\nmesh registration. In this regard, spectral descriptors that are based on the\nspectrum of the Laplace-Beltrami operator have been a popular subject of\nresearch for the last decade due to their advantageous properties, such as\nisometry invariance. Despite such, however, spectral descriptors often fail to\ngive a correct similarity measure for non-isometric cases where the metric\ndistortion between the models is large. Hence, they are not reliable for\ncorrespondence matching problems when the models are not isometric. In this\npaper, it is proposed a method to improve the similarity metric of spectral\ndescriptors for correspondence matching problems. We embed a spectral shape\ndescriptor into a different metric space where the Euclidean distance between\nthe elements directly indicates the geometric dissimilarity. We design and\ntrain a Siamese neural network to find such an embedding, where the embedded\ndescriptors are promoted to rearrange based on the geometric similarity. We\ndemonstrate our approach can significantly enhance the performance of the\nconventional spectral descriptors by the simple augmentation achieved via the\nSiamese neural network in comparison to other state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.06368v3"
    },
    {
        "title": "Photo-Guided Exploration of Volume Data Features",
        "authors": [
            "Mohammad Raji",
            "Alok Hota",
            "Robert Sisneros",
            "Peter Messmer",
            "Jian Huang"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In this work, we pose the question of whether, by considering qualitative\ninformation such as a sample target image as input, one can produce a rendered\nimage of scientific data that is similar to the target. The algorithm resulting\nfrom our research allows one to ask the question of whether features like those\nin the target image exists in a given dataset. In that way, our method is one\nof imagery query or reverse engineering, as opposed to manual parameter\ntweaking of the full visualization pipeline. For target images, we can use\nreal-world photographs of physical phenomena. Our method leverages deep neural\nnetworks and evolutionary optimization. Using a trained similarity function\nthat measures the difference between renderings of a phenomenon and real-world\nphotographs, our method optimizes rendering parameters. We demonstrate the\nefficacy of our method using a superstorm simulation dataset and images found\nonline. We also discuss a parallel implementation of our method, which was run\non NCSA's Blue Waters.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.06815v1"
    },
    {
        "title": "A Generative Model for Volume Rendering",
        "authors": [
            "Matthew Berger",
            "Jixian Li",
            "Joshua A. Levine"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We present a technique to synthesize and analyze volume-rendered images using\ngenerative models. We use the Generative Adversarial Network (GAN) framework to\ncompute a model from a large collection of volume renderings, conditioned on\n(1) viewpoint and (2) transfer functions for opacity and color. Our approach\nfacilitates tasks for volume analysis that are challenging to achieve using\nexisting rendering techniques such as ray casting or texture-based methods. We\nshow how to guide the user in transfer function editing by quantifying expected\nchange in the output image. Additionally, the generative model transforms\ntransfer functions into a view-invariant latent space specifically designed to\nsynthesize volume-rendered images. We use this space directly for rendering,\nenabling the user to explore the space of volume-rendered images. As our model\nis independent of the choice of volume rendering process, we show how to\nanalyze volume-rendered images produced by direct and global illumination\nlighting, for a variety of volume datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.09545v2"
    },
    {
        "title": "Learning Patterns in Sample Distributions for Monte Carlo Variance\n  Reduction",
        "authors": [
            "Oskar Elek",
            "Manu M. Thomas",
            "Angus Forbes"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper investigates a novel a-posteriori variance reduction approach in\nMonte Carlo image synthesis. Unlike most established methods based on lateral\nfiltering in the image space, our proposition is to produce the best possible\nestimate for each pixel separately, from all the samples drawn for it. To\nenable this, we systematically study the per-pixel sample distributions for\ndiverse scene configurations. Noting that these are too complex to be\ncharacterized by standard statistical distributions (e.g. Gaussians), we\nidentify patterns recurring in them and exploit those for training a\nvariance-reduction model based on neural nets. In result, we obtain numerically\nbetter estimates compared to simple averaging of samples. This method is\ncompatible with existing image-space denoising methods, as the improved\nestimates of our model can be used for further processing. We conclude by\ndiscussing how the proposed model could in future be extended for fully\nprogressive rendering with constant memory footprint and scene-sensitive\noutput.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.00124v1"
    },
    {
        "title": "3D Magic Mirror: Automatic Video to 3D Caricature Translation",
        "authors": [
            "Yudong Guo",
            "Luo Jiang",
            "Lin Cai",
            "Juyong Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Caricature is an abstraction of a real person which distorts or exaggerates\ncertain features, but still retains a likeness. While most existing works focus\non 3D caricature reconstruction from 2D caricatures or translating 2D photos to\n2D caricatures, this paper presents a real-time and automatic algorithm for\ncreating expressive 3D caricatures with caricature style texture map from 2D\nphotos or videos. To solve this challenging ill-posed reconstruction problem\nand cross-domain translation problem, we first reconstruct the 3D face shape\nfor each frame, and then translate 3D face shape from normal style to\ncaricature style by a novel identity and expression preserving VAE-CycleGAN.\nBased on a labeling formulation, the caricature texture map is constructed from\na set of multi-view caricature images generated by CariGANs. The effectiveness\nand efficiency of our method are demonstrated by comparison with baseline\nimplementations. The perceptual study shows that the 3D caricatures generated\nby our method meet people's expectations of 3D caricature style.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.00544v1"
    },
    {
        "title": "Sea of Genes: Combining Animation and Narrative Strategies to Visualize\n  Metagenomic Data for Museums",
        "authors": [
            "Keshav Dasu",
            "Kwan-Liu Ma",
            "Joyce Ma",
            "Jennifer Frazier"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We examine the application of narrative strategies to present a complex and\nunfamiliar metagenomics dataset to the public in a science museum. Our dataset\ncontains information about microbial gene expressions that scientists use to\ninfer the behavior of microbes. This exhibit had three goals: to inform (the)\npublic about microbes' behavior, cycles, and patterns; to link their behavior\nto the concept of gene expression; and to highlight scientists' use of gene\nexpression data to understand the role of microbes. To address these three\ngoals, we created a visualization with three narrative layers, each layer\ncorresponding to a goal. This study presented us with an opportunity to assess\nexisting frameworks for narrative visualization in a naturalistic setting. We\npresent three successive rounds of design and evaluation of our attempts to\nengage visitors with complex data through narrative visualization. We highlight\nour design choices and their underlying rationale based on extant theories. We\nconclude that a central animation based on a curated dataset could successfully\nachieve our first goal, i.e., to communicate the aggregate behavior and\ninteractions of microbes. We failed to achieve our second goal and had limited\nsuccess with the third goal. Overall, this study highlights the challenges of\ntelling multi-layered stories and the need for new frameworks for communicating\nlayered stories in public settings.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.01071v2"
    },
    {
        "title": "Coherent Point Drift Networks: Unsupervised Learning of Non-Rigid Point\n  Set Registration",
        "authors": [
            "Lingjing Wang",
            "Xiang Li",
            "Jianchun Chen",
            "Yi Fang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Given new pairs of source and target point sets, standard point set\nregistration methods often repeatedly conduct the independent iterative search\nof desired geometric transformation to align the source point set with the\ntarget one. This limits their use in applications to handle the real-time point\nset registration with large volume dataset. This paper presents a novel method,\nnamed coherent point drift networks (CPD-Net), for the unsupervised learning of\ngeometric transformation towards real-time non-rigid point set registration. In\ncontrast to previous efforts (e.g. coherent point drift), CPD-Net can learn\ndisplacement field function to estimate geometric transformation from a\ntraining dataset, consequently, to predict the desired geometric transformation\nfor the alignment of previously unseen pairs without any additional iterative\noptimization process. Furthermore, CPD-Net leverages the power of deep neural\nnetworks to fit an arbitrary function, that adaptively accommodates different\nlevels of complexity of the desired geometric transformation. Particularly,\nCPD-Net is proved with a theoretical guarantee to learn a continuous\ndisplacement vector function that could further avoid imposing additional\nparametric smoothness constraint as in previous works. Our experiments verify\nthe impressive performance of CPD-Net for non-rigid point set registration on\nvarious 2D/3D datasets, even in the presence of significant displacement noise,\noutliers, and missing points. Our code will be available at\nhttps://github.com/nyummvc/CPD-Net.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.03039v5"
    },
    {
        "title": "Differentiable Surface Splatting for Point-based Geometry Processing",
        "authors": [
            "Wang Yifan",
            "Felice Serena",
            "Shihao Wu",
            "Cengiz Öztireli",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose Differentiable Surface Splatting (DSS), a high-fidelity\ndifferentiable renderer for point clouds. Gradients for point locations and\nnormals are carefully designed to handle discontinuities of the rendering\nfunction. Regularization terms are introduced to ensure uniform distribution of\nthe points on the underlying surface. We demonstrate applications of DSS to\ninverse rendering for geometry synthesis and denoising, where large scale\ntopological changes, as well as small scale detail modifications, are\naccurately and robustly handled without requiring explicit connectivity,\noutperforming state-of-the-art techniques. The data and code are at\nhttps://github.com/yifita/DSS.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.04173v3"
    },
    {
        "title": "RodSteward: A Design-to-Assembly System for Fabrication using 3D-Printed\n  Joints and Precision-Cut Rods",
        "authors": [
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present RodSteward, a design-to-assembly system for creating\nfurniture-scale structures composed of 3D printed joints and precision-cut\nrods. The RodSteward systems consists of: RSDesigner, a fabrication-aware\ndesign interface that visualizes accurate geometries during edits and\nidentifies infeasible designs; physical fabrication of parts via novel fully\nautomatic construction of solid 3D-printable joint geometries and automatically\ngenerated cutting plans for rods; and RSAssembler, a guided-assembly interface\nthat prompts the user to place parts in order while showing a focus+context\nvisualization of the assembly in progress. We demonstrate the effectiveness of\nour tools with a number of example constructions of varying complexity, style\nand parameter choices.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05710v1"
    },
    {
        "title": "Symmetric Algorithmic Components for Shape Analysis with Diffeomorphisms",
        "authors": [
            "N. Guigui",
            "Shuman Jia",
            "Maxime Sermesant",
            "Xavier Pennec"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In computational anatomy, the statistical analysis of temporal deformations\nand inter-subject variability relies on shape registration. However, the\nnumerical integration and optimization required in diffeomorphic registration\noften lead to important numerical errors. In many cases, it is well known that\nthe error can be drastically reduced in the presence of a symmetry. In this\nwork, the leading idea is to approximate the space of deformations and images\nwith a possibly non-metric symmetric space structure using an involution, with\nthe aim to perform parallel transport. Through basic properties of symmetries,\nwe investigate how the implementations of a midpoint and the involution compare\nwith the ones of the Riemannian exponential and logarithm on diffeomorphisms\nand propose a modification of these maps using registration errors. This leads\nus to identify transvections, the composition of two symmetries, as a mean to\nmeasure how far from symmetric the underlying structure is. We test our method\non a set of 138 cardiac shapes and demonstrate improved numerical consistency\nin the Pole Ladder scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05921v1"
    },
    {
        "title": "Volumetric Isosurface Rendering with Deep Learning-Based\n  Super-Resolution",
        "authors": [
            "Sebastian Weiss",
            "Mengyu Chu",
            "Nils Thuerey",
            "Rüdiger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Rendering an accurate image of an isosurface in a volumetric field typically\nrequires large numbers of data samples. Reducing the number of required samples\nlies at the core of research in volume rendering. With the advent of deep\nlearning networks, a number of architectures have been proposed recently to\ninfer missing samples in multi-dimensional fields, for applications such as\nimage super-resolution and scan completion. In this paper, we investigate the\nuse of such architectures for learning the upscaling of a low-resolution\nsampling of an isosurface to a higher resolution, with high fidelity\nreconstruction of spatial detail and shading. We introduce a fully\nconvolutional neural network, to learn a latent representation generating a\nsmooth, edge-aware normal field and ambient occlusions from a low-resolution\nnormal and depth field. By adding a frame-to-frame motion loss into the\nlearning stage, the upscaling can consider temporal variations and achieves\nimproved frame-to-frame coherence. We demonstrate the quality of the network\nfor isosurfaces which were never seen during training, and discuss remote and\nin-situ visualization as well as focus+context visualization as potential\napplications\n",
        "pdf_link": "http://arxiv.org/pdf/1906.06520v1"
    },
    {
        "title": "Pi-surfaces: products of implicit surfaces towards constructive\n  composition of 3D objects",
        "authors": [
            "Adriano N. Raposo",
            "Abel J. P. Gomes"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Implicit functions provide a fundamental basis to model 3D objects, no matter\nthey are rigid or deformable, in computer graphics and geometric modeling. This\npaper introduces a new constructive scheme of implicitly-defined 3D objects\nbased on products of implicit functions. This scheme is in contrast with\npopular approaches like blobbies, meta balls and soft objects, which rely on\nthe sum of specific implicit functions to fit a 3D object to a set of spheres.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.06751v1"
    },
    {
        "title": "Active Scene Understanding via Online Semantic Reconstruction",
        "authors": [
            "Lintao Zheng",
            "Chenyang Zhu",
            "Jiazhao Zhang",
            "Hang Zhao",
            "Hui Huang",
            "Matthias Niessner",
            "Kai Xu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a novel approach to robot-operated active understanding of unknown\nindoor scenes, based on online RGBD reconstruction with semantic segmentation.\nIn our method, the exploratory robot scanning is both driven by and targeting\nat the recognition and segmentation of semantic objects from the scene. Our\nalgorithm is built on top of the volumetric depth fusion framework (e.g.,\nKinectFusion) and performs real-time voxel-based semantic labeling over the\nonline reconstructed volume. The robot is guided by an online estimated\ndiscrete viewing score field (VSF) parameterized over the 3D space of 2D\nlocation and azimuth rotation. VSF stores for each grid the score of the\ncorresponding view, which measures how much it reduces the uncertainty\n(entropy) of both geometric reconstruction and semantic labeling. Based on VSF,\nwe select the next best views (NBV) as the target for each time step. We then\njointly optimize the traverse path and camera trajectory between two adjacent\nNBVs, through maximizing the integral viewing score (information gain) along\npath and trajectory. Through extensive evaluation, we show that our method\nachieves efficient and accurate online scene parsing during exploratory\nscanning.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.07409v2"
    },
    {
        "title": "Neural Volumes: Learning Dynamic Renderable Volumes from Images",
        "authors": [
            "Stephen Lombardi",
            "Tomas Simon",
            "Jason Saragih",
            "Gabriel Schwartz",
            "Andreas Lehrmann",
            "Yaser Sheikh"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Modeling and rendering of dynamic scenes is challenging, as natural scenes\noften contain complex phenomena such as thin structures, evolving topology,\ntranslucency, scattering, occlusion, and biological motion. Mesh-based\nreconstruction and tracking often fail in these cases, and other approaches\n(e.g., light field video) typically rely on constrained viewing conditions,\nwhich limit interactivity. We circumvent these difficulties by presenting a\nlearning-based approach to representing dynamic objects inspired by the\nintegral projection model used in tomographic imaging. The approach is\nsupervised directly from 2D images in a multi-view capture setting and does not\nrequire explicit reconstruction or tracking of the object. Our method has two\nprimary components: an encoder-decoder network that transforms input images\ninto a 3D volume representation, and a differentiable ray-marching operation\nthat enables end-to-end training. By virtue of its 3D representation, our\nconstruction extrapolates better to novel viewpoints compared to screen-space\nrendering techniques. The encoder-decoder architecture learns a latent\nrepresentation of a dynamic scene that enables us to produce novel content\nsequences not seen during training. To overcome memory limitations of\nvoxel-based representations, we learn a dynamic irregular grid structure\nimplemented with a warp field during ray-marching. This structure greatly\nimproves the apparent resolution and reduces grid-like artifacts and jagged\nmotion. Finally, we demonstrate how to incorporate surface-based\nrepresentations into our volumetric-learning framework for applications where\nthe highest resolution is required, using facial performance capture as a case\nin point.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.07751v1"
    },
    {
        "title": "Gaze-Contingent Ocular Parallax Rendering for Virtual Reality",
        "authors": [
            "Robert Konrad",
            "Anastasios Angelopoulos",
            "Gordon Wetzstein"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Immersive computer graphics systems strive to generate perceptually realistic\nuser experiences. Current-generation virtual reality (VR) displays are\nsuccessful in accurately rendering many perceptually important effects,\nincluding perspective, disparity, motion parallax, and other depth cues. In\nthis article, we introduce ocular parallax rendering, a technology that\naccurately renders small amounts of gaze-contingent parallax capable of\nimproving depth perception and realism in VR. Ocular parallax describes the\nsmall amounts of depth-dependent image shifts on the retina that are created as\nthe eye rotates. The effect occurs because the centers of rotation and\nprojection of the eye are not the same. We study the perceptual implications of\nocular parallax rendering by designing and conducting a series of user\nexperiments. Specifically, we estimate perceptual detection and discrimination\nthresholds for this effect and demonstrate that it is clearly visible in most\nVR applications. Additionally, we show that ocular parallax rendering provides\nan effective ordinal depth cue and it improves the impression of realistic\ndepth in VR.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.09740v2"
    },
    {
        "title": "Efficient Spatial Anti-Aliasing Rendering for Line Joins on Vector Maps",
        "authors": [
            "Chaoyang He",
            "Ming Li"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The spatial anti-aliasing technique for line joins (intersections of the road\nsegments) on vector maps is exclusively crucial to visual experience and system\nperformance. Due to limitations of OpenGL API, one common practice to achieve\nthe anti-aliased effect is splicing multiple triangles at varying scale levels\nto approximate the fan-shaped line joins. However, this approximation\ninevitably produces some unreality, and the system rendering performance is not\noptimal. To circumvent these drawbacks, in this paper, we propose a simple but\nefficient algorithm which uses only two triangles to substitute the multiple\ntriangles approximation and then renders a realistic fan-shaped curve with\nalpha operation based on geometrical relation computing. Our experiment shows\nit has advantages of a realistic anti-aliasing effect, less memory cost, higher\nframe rate, and drawing line joins without overlapping rendering. Our proposed\nspatial anti-aliasing technique has been widely used in Internet Maps such as\nTencent Mobile Maps and Tencent Automotive Maps.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.11999v1"
    },
    {
        "title": "TAP-Net: Transport-and-Pack using Reinforcement Learning",
        "authors": [
            "Ruizhen Hu",
            "Juzhan Xu",
            "Bin Chen",
            "Minglun Gong",
            "Hao Zhang",
            "Hui Huang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce the transport-and-pack(TAP) problem, a frequently encountered\ninstance of real-world packing, and develop a neural optimization solution\nbased on reinforcement learning. Given an initial spatial configuration of\nboxes, we seek an efficient method to iteratively transport and pack the boxes\ncompactly into a target container. Due to obstruction and accessibility\nconstraints, our problem has to add a new search dimension, i.e., finding an\noptimal transport sequence, to the already immense search space for packing\nalone. Using a learning-based approach, a trained network can learn and encode\nsolution patterns to guide the solution of new problem instances instead of\nexecuting an expensive online search. In our work, we represent the transport\nconstraints using a precedence graph and train a neural network, coined\nTAP-Net, using reinforcement learning to reward efficient and stable packing.\nThe network is built on an encoder-decoder architecture, where the encoder\nemploys convolution layers to encode the box geometry and precedence graph and\nthe decoder is a recurrent neural network (RNN) which inputs the current\nencoder output, as well as the current box packing state of the target\ncontainer, and outputs the next box to pack, as well as its orientation. We\ntrain our network on randomly generated initial box configurations, without\nsupervision, via policy gradients to learn optimal TAP policies to maximize\npacking efficiency and stability. We demonstrate the performance of TAP-Net on\na variety of examples, evaluating the network through ablation studies and\ncomparisons to baselines and alternative network designs. We also show that our\nnetwork generalizes well to larger problem instances, when trained on\nsmall-sized inputs.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.01469v1"
    },
    {
        "title": "Trimmed Spline Surfaces with Accurate Boundary Control",
        "authors": [
            "Florian Martin",
            "Ulrich Reif"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce trimmed NURBS surfaces with accurate boundary control, briefly\ncalled ABC-surfaces, as a solution to the notorious problem of constructing\nwatertight or smooth ($G^1$ and $G^2)$ multi-patch surfaces within the function\nrange of standard CAD/CAM systems and the associated file exchange formats. Our\nconstruction is based on the appropriate blend of a base surface, which traces\nout the intended global shape, and a series of reparametrized ribbons, which\ndominate the shape near the boundary.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02480v2"
    },
    {
        "title": "Length-optimal tool path planning for freeform surfaces with preferred\n  feed directions",
        "authors": [
            "Qiang Zou",
            "Charlie C. L. Wang",
            "Hsi-Yung Feng"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper presents a new method to generate tool paths for machining\nfreeform surfaces represented either as parametric surfaces or as triangular\nmeshes. This method allows for the optimal tradeoff between the preferred feed\ndirection field and the constant scallop height, and yields a minimized overall\npath length. The optimality is achieved by formulating tool path planning as a\nPoisson problem that minimizes a simple, quadratic energy. This Poisson\nformulation considers all tool paths at once, without resorting to any\nheuristic sampling or initial tool path choosing as in existing methods, and is\nthus a globally optimal solution. Finding the optimal tool paths amounts to\nsolving a well-conditioned sparse linear system, which is computationally\nconvenient and efficient. Tool paths are represented with an implicit scheme\nthat can completely avoid the challenging topological issues of path\nsingularities and self-intersections seen in previous methods. The presented\nmethod has been validated with a series of examples and comparisons.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02660v1"
    },
    {
        "title": "Interactive Visualization of Terascale Data in the Browser: Fact or\n  Fiction?",
        "authors": [
            "Will Usher",
            "Valerio Pascucci"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Information visualization applications have become ubiquitous, in no small\npart thanks to the ease of wide distribution and deployment to users enabled by\nthe web browser. Scientific visualization applications, relying on native code\nlibraries and parallel processing, have been less suited to such widespread\ndistribution, as browsers do not provide the required libraries or compute\ncapabilities. In this paper, we revisit this gap in visualization technologies\nand explore how new web technologies, WebAssembly and WebGPU, can be used to\ndeploy powerful visualization solutions for large-scale scientific data in the\nbrowser. In particular, we evaluate the programming effort required to bring\nscientific visualization applications to the browser through these technologies\nand assess their competitiveness against classic native solutions. As a main\nexample, we present a new GPU-driven isosurface extraction method for\nblock-compressed data sets, that is suitable for interactive isosurface\ncomputation on large volumes in resource-constrained environments, such as the\nbrowser. We conclude that web browsers are on the verge of becoming a\ncompetitive platform for even the most demanding scientific visualization\ntasks, such as interactive visualization of isosurfaces from a 1TB DNS\nsimulation. We call on researchers and developers to consider investing in a\ncommunity software stack to ease use of these upcoming browser features to\nbring accessible scientific visualization to the browser.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03254v1"
    },
    {
        "title": "Responsive Matrix Cells: A Focus+Context Approach for Exploring and\n  Editing Multivariate Graphs",
        "authors": [
            "Tom Horak",
            "Philip Berger",
            "Heidrun Schumann",
            "Raimund Dachselt",
            "Christian Tominski"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Matrix visualizations are a useful tool to provide a general overview of a\ngraph's structure. For multivariate graphs, a remaining challenge is to cope\nwith the attributes that are associated with nodes and edges. Addressing this\nchallenge, we propose responsive matrix cells as a focus+context approach for\nembedding additional interactive views into a matrix. Responsive matrix cells\nare local zoomable regions of interest that provide auxiliary data exploration\nand editing facilities for multivariate graphs. They behave responsively by\nadapting their visual contents to the cell location, the available display\nspace, and the user task. Responsive matrix cells enable users to reveal\ndetails about the graph, compare node and edge attributes, and edit data values\ndirectly in a matrix without resorting to external views or tools. We report\nthe general design considerations for responsive matrix cells covering the\nvisual and interactive means necessary to support a seamless data exploration\nand editing. Responsive matrix cells have been implemented in a web-based\nprototype based on which we demonstrate the utility of our approach. We\ndescribe a walk-through for the use case of analyzing a graph of soccer players\nand report on insights from a preliminary user feedback session.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03385v1"
    },
    {
        "title": "Computational Design of Cold Bent Glass Façades",
        "authors": [
            "Konstantinos Gavriil",
            "Ruslan Guseinov",
            "Jesús Pérez",
            "Davide Pellis",
            "Paul Henderson",
            "Florian Rist",
            "Helmut Pottmann",
            "Bernd Bickel"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Cold bent glass is a promising and cost-efficient method for realizing doubly\ncurved glass fa\\c{c}ades. They are produced by attaching planar glass sheets to\ncurved frames and require keeping the occurring stress within safe limits.\nHowever, it is very challenging to navigate the design space of cold bent glass\npanels due to the fragility of the material, which impedes the form-finding for\npractically feasible and aesthetically pleasing cold bent glass fa\\c{c}ades. We\npropose an interactive, data-driven approach for designing cold bent glass\nfa\\c{c}ades that can be seamlessly integrated into a typical architectural\ndesign pipeline. Our method allows non-expert users to interactively edit a\nparametric surface while providing real-time feedback on the deformed shape and\nmaximum stress of cold bent glass panels. Designs are automatically refined to\nminimize several fairness criteria while maximal stresses are kept within glass\nlimits. We achieve interactive frame rates by using a differentiable Mixture\nDensity Network trained from more than a million simulations. Given a curved\nboundary, our regression model is capable of handling multistable\nconfigurations and accurately predicting the equilibrium shape of the panel and\nits corresponding maximal stress. We show predictions are highly accurate and\nvalidate our results with a physical realization of a cold bent glass surface.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03667v1"
    },
    {
        "title": "Sketch2CAD: Sequential CAD Modeling by Sketching in Context",
        "authors": [
            "Changjian Li",
            "Hao Pan",
            "Adrien Bousseau",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a sketch-based CAD modeling system, where users create objects\nincrementally by sketching the desired shape edits, which our system\nautomatically translates to CAD operations. Our approach is motivated by the\nclose similarities between the steps industrial designers follow to draw 3D\nshapes, and the operations CAD modeling systems offer to create similar shapes.\nTo overcome the strong ambiguity with parsing 2D sketches, we observe that in a\nsketching sequence, each step makes sense and can be interpreted in the\n\\emph{context} of what has been drawn before. In our system, this context\ncorresponds to a partial CAD model, inferred in the previous steps, which we\nfeed along with the input sketch to a deep neural network in charge of\ninterpreting how the model should be modified by that sketch. Our deep network\narchitecture then recognizes the intended CAD operation and segments the sketch\naccordingly, such that a subsequent optimization estimates the parameters of\nthe operation that best fit the segmented sketch strokes. Since there exists no\ndatasets of paired sketching and CAD modeling sequences, we train our system by\ngenerating synthetic sequences of CAD operations that we render as line\ndrawings. We present a proof of concept realization of our algorithm supporting\nfour frequently used CAD operations. Using our system, participants are able to\nquickly model a large and diverse set of objects, demonstrating Sketch2CAD to\nbe an alternate way of interacting with current CAD modeling systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.04927v1"
    },
    {
        "title": "Interactive Focus+Context Rendering for Hexahedral Mesh Inspection",
        "authors": [
            "Christoph Neuhauser",
            "Junpeng Wang",
            "Rüdiger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The visual inspection of a hexahedral mesh with respect to element quality is\ndifficult due to clutter and occlusions that are produced when rendering all\nelement faces or their edges simultaneously. Current approaches overcome this\nproblem by using focus on specific elements that are then rendered opaque, and\ncarving away all elements occluding their view. In this work, we make use of\nadvanced GPU shader functionality to generate a focus+context rendering that\nhighlights the elements in a selected region and simultaneously conveys the\nglobal mesh structure in the surrounding. To achieve this, we propose a gradual\ntransition from edge-based focus rendering to volumetric context rendering, by\ncombining fragment shader-based edge and face rendering with per-pixel fragment\nlists. A fragment shader smoothly transitions between wireframe and face-based\nrendering, including focus-dependent rendering style and depth-dependent edge\nthickness and halos, and per-pixel fragment lists are used to blend fragments\nin correct visibility order. To maintain the global mesh structure in the\ncontext regions, we propose a new method to construct a sheet-based\nlevel-of-detail hierarchy and smoothly blend it with volumetric information.\nThe user guides the exploration process by moving a lens-like hotspot. Since\nall operations are performed on the GPU, interactive frame rates are achieved\neven for large meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06574v1"
    },
    {
        "title": "Mid-Air Drawing of Curves on 3D Surfaces in Virtual Reality",
        "authors": [
            "Rahul Arora",
            "Karan Singh"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Complex 3D curves can be created by directly drawing mid-air in immersive\nenvironments (Augmented and Virtual Realities). Drawing mid-air strokes\nprecisely on the surface of a 3D virtual object, however, is difficult;\nnecessitating a projection of the mid-air stroke onto the user \"intended\"\nsurface curve. We present the first detailed investigation of the fundamental\nproblem of 3D stroke projection in VR. An assessment of the design requirements\nof real-time drawing of curves on 3D objects in VR is followed by the\ndefinition and classification of multiple techniques for 3D stroke projection.\nWe analyze the advantages and shortcomings of these approaches both\ntheoretically and via practical pilot testing. We then formally evaluate the\ntwo most promising techniques spraycan and mimicry with 20 users in VR. The\nstudy shows a strong qualitative and quantitative user preference for our novel\nstroke mimicry projection algorithm. We further illustrate the effectiveness\nand utility of stroke mimicry, to draw complex 3D curves on surfaces for\nvarious artistic and functional design applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09029v2"
    },
    {
        "title": "SceneGen: Generative Contextual Scene Augmentation using Scene Graph\n  Priors",
        "authors": [
            "Mohammad Keshavarzi",
            "Aakash Parikh",
            "Xiyu Zhai",
            "Melody Mao",
            "Luisa Caldas",
            "Allen Y. Yang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Spatial computing experiences are constrained by the real-world surroundings\nof the user. In such experiences, augmenting virtual objects to existing scenes\nrequire a contextual approach, where geometrical conflicts are avoided, and\nfunctional and plausible relationships to other objects are maintained in the\ntarget environment. Yet, due to the complexity and diversity of user\nenvironments, automatically calculating ideal positions of virtual content that\nis adaptive to the context of the scene is considered a challenging task.\nMotivated by this problem, in this paper we introduce SceneGen, a generative\ncontextual augmentation framework that predicts virtual object positions and\norientations within existing scenes. SceneGen takes a semantically segmented\nscene as input, and outputs positional and orientational probability maps for\nplacing virtual content. We formulate a novel spatial Scene Graph\nrepresentation, which encapsulates explicit topological properties between\nobjects, object groups, and the room. We believe providing explicit and\nintuitive features plays an important role in informative content creation and\nuser interaction of spatial computing settings, a quality that is not captured\nin implicit models. We use kernel density estimation (KDE) to build a\nmultivariate conditional knowledge model trained using prior spatial Scene\nGraphs extracted from real-world 3D scanned data. To further capture\norientational properties, we develop a fast pose annotation tool to extend\ncurrent real-world datasets with orientational labels. Finally, to demonstrate\nour system in action, we develop an Augmented Reality application, in which\nobjects can be contextually augmented in real-time.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.12395v2"
    },
    {
        "title": "A grammar of graphics framework for generalized parallel coordinate\n  plots",
        "authors": [
            "Yawei Ge",
            "Heike Hofmann"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Parallel coordinate plots (PCP) are a useful tool in exploratory data\nanalysis of high-dimensional numerical data. The use of PCPs is limited when\nworking with categorical variables or a mix of categorical and continuous\nvariables. In this paper, we propose generalized parallel coordinate plots\n(GPCP) to extend the ability of PCPs from just numeric variables to dealing\nseamlessly with a mix of categorical and numeric variables in a single plot. In\nthis process we find that existing solutions for categorical values only, such\nas hammock plots or parsets become edge cases in the new framework. By focusing\non individual observation rather a marginal frequency we gain additional\nflexibility. The resulting approach is implemented in the R package ggpcp.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.12933v1"
    },
    {
        "title": "Hamiltonian operator for spectral shape analysis",
        "authors": [
            "Yoni Choukroun",
            "Alon Shtern",
            "Alex Bronstein",
            "Ron Kimmel"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Many shape analysis methods treat the geometry of an object as a metric space\nthat can be captured by the Laplace-Beltrami operator. In this paper, we\npropose to adapt the classical Hamiltonian operator from quantum mechanics to\nthe field of shape analysis. To this end we study the addition of a potential\nfunction to the Laplacian as a generator for dual spaces in which shape\nprocessing is performed. We present a general optimization approach for solving\nvariational problems involving the basis defined by the Hamiltonian using\nperturbation theory for its eigenvectors. The suggested operator is shown to\nproduce better functional spaces to operate with, as demonstrated on different\nshape analysis tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.01990v2"
    },
    {
        "title": "Error-Bounded and Feature Preserving Surface Remeshing with Minimal\n  Angle Improvement",
        "authors": [
            "Kaimo Hu",
            "Dong-Ming Yan",
            "David Bommes",
            "Pierre Alliez",
            "Bedrich Benes"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  The typical goal of surface remeshing consists in finding a mesh that is (1)\ngeometrically faithful to the original geometry, (2) as coarse as possible to\nobtain a low-complexity representation and (3) free of bad elements that would\nhamper the desired application. In this paper, we design an algorithm to\naddress all three optimization goals simultaneously. The user specifies desired\nbounds on approximation error {\\delta}, minimal interior angle {\\theta} and\nmaximum mesh complexity N (number of vertices). Since such a desired mesh might\nnot even exist, our optimization framework treats only the approximation error\nbound {\\delta} as a hard constraint and the other two criteria as optimization\ngoals. More specifically, we iteratively perform carefully prioritized local\noperators, whenever they do not violate the approximation error bound and\nimprove the mesh otherwise. In this way our optimization framework greedily\nsearches for the coarsest mesh with minimal interior angle above {\\theta} and\napproximation error bounded by {\\delta}. Fast runtime is enabled by a local\napproximation error estimation, while implicit feature preservation is obtained\nby specifically designed vertex relocation operators. Experiments show that our\napproach delivers high-quality meshes with implicitly preserved features and\nbetter balances between geometric fidelity, mesh complexity and element quality\nthan the state-of-the-art.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.02147v1"
    },
    {
        "title": "Oriented bounding boxes using multiresolution contours for fast\n  interference detection of arbitrary geometry objects",
        "authors": [
            "L. A. Rivera",
            "Vania V. Estrela",
            "P. C. P. Carvalho"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Interference detection of arbitrary geometric objects is not a trivial task\ndue to the heavy computational load imposed by implementation issues. The\nhierarchically structured bounding boxes help us to quickly isolate the contour\nof segments in interference. In this paper, a new approach is introduced to\ntreat the interference detection problem involving the representation of\narbitrary shaped objects. Our proposed method relies upon searching for the\nbest possible way to represent contours by means of hierarchically structured\nrectangular oriented bounding boxes. This technique handles 2D objects\nboundaries defined by closed B-spline curves with roughness details. Each\noriented box is adapted and fitted to the segments of the contour using second\norder statistical indicators from some elements of the segments of the object\ncontour in a multiresolution framework. Our method is efficient and robust when\nit comes to 2D animations in real time. It can deal with smooth curves and\npolygonal approximations as well results are present to illustrate the\nperformance of the new method.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.03666v1"
    },
    {
        "title": "Porous Structure Design in Tissue Engineering Using Anisotropic Radial\n  Basis Function",
        "authors": [
            "Ke Liu",
            "Ye Guo",
            "Zeyun Yu"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Development of additive manufacturing in last decade greatly improves tissue\nengineering. During the manufacturing of porous scaffold, simplified but\nfunctionally equivalent models are getting focused for practically reasons.\nScaffolds can be classified into regular porous scaffolds and irregular porous\nscaffolds. Several methodologies are developed to design these scaffolds. A\nnovel method is proposed in this paper using anisotropic radial basis function\n(ARBF) interpolation. This is method uses geometric models such as volumetric\nmeshes as input and proves to be flexible because geometric models are able to\ncapture the characteristics of complex tissues easily. Moreover, this method is\nstraightforward and easy to implement.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.01944v2"
    },
    {
        "title": "Coupled Fluid Density and Motion from Single Views",
        "authors": [
            "Marie-Lena Eckert",
            "Wolfgang Heidrich",
            "Nils Thuerey"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present a novel method to reconstruct a fluid's 3D density and motion\nbased on just a single sequence of images. This is rendered possible by using\npowerful physical priors for this strongly under-determined problem. More\nspecifically, we propose a novel strategy to infer density updates strongly\ncoupled to previous and current estimates of the flow motion. Additionally, we\nemploy an accurate discretization and depth-based regularizers to compute\nstable solutions. Using only one view for the reconstruction reduces the\ncomplexity of the capturing setup drastically and could even allow for online\nvideo databases or smart-phone videos as inputs. The reconstructed 3D velocity\ncan then be flexibly utilized, e.g., for re-simulation, domain modification or\nguiding purposes. We will demonstrate the capacity of our method with a series\nof synthetic test cases and the reconstruction of real smoke plumes captured\nwith a Raspberry Pi camera.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.06613v1"
    },
    {
        "title": "iMapper: Interaction-guided Joint Scene and Human Motion Mapping from\n  Monocular Videos",
        "authors": [
            "Aron Monszpart",
            "Paul Guerrero",
            "Duygu Ceylan",
            "Ersin Yumer",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  A long-standing challenge in scene analysis is the recovery of scene\narrangements under moderate to heavy occlusion, directly from monocular video.\nWhile the problem remains a subject of active research, concurrent advances\nhave been made in the context of human pose reconstruction from monocular\nvideo, including image-space feature point detection and 3D pose recovery.\nThese methods, however, start to fail under moderate to heavy occlusion as the\nproblem becomes severely under-constrained. We approach the problems\ndifferently. We observe that people interact similarly in similar scenes.\nHence, we exploit the correlation between scene object arrangement and motions\nperformed in that scene in both directions: first, typical motions performed\nwhen interacting with objects inform us about possible object arrangements; and\nsecond, object arrangements, in turn, constrain the possible motions.\n  We present iMapper, a data-driven method that focuses on identifying\nhuman-object interactions, and jointly reasons about objects and human movement\nover space-time to recover both a plausible scene arrangement and consistent\nhuman interactions. We first introduce the notion of characteristic\ninteractions as regions in space-time when an informative human-object\ninteraction happens. This is followed by a novel occlusion-aware matching\nprocedure that searches and aligns such characteristic snapshots from an\ninteraction database to best explain the input monocular video. Through\nextensive evaluations, both quantitative and qualitative, we demonstrate that\niMapper significantly improves performance over both dedicated state-of-the-art\nscene analysis and 3D human pose recovery approaches, especially under medium\nto heavy occlusion.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07889v1"
    },
    {
        "title": "Shape-from-Mask: A Deep Learning Based Human Body Shape Reconstruction\n  from Binary Mask Images",
        "authors": [
            "Zhongping Ji",
            "Xiao Qi",
            "Yigang Wang",
            "Gang Xu",
            "Peng Du",
            "Qing Wu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  3D content creation is referred to as one of the most fundamental tasks of\ncomputer graphics. And many 3D modeling algorithms from 2D images or curves\nhave been developed over the past several decades. Designers are allowed to\nalign some conceptual images or sketch some suggestive curves, from front,\nside, and top views, and then use them as references in constructing a 3D model\nautomatically or manually. However, to the best of our knowledge, no studies\nhave investigated on 3D human body reconstruction in a similar manner. In this\npaper, we propose a deep learning based reconstruction of 3D human body shape\nfrom 2D orthographic views. A novel CNN-based regression network, with two\nbranches corresponding to frontal and lateral views respectively, is designed\nfor estimating 3D human body shape from 2D mask images. We train our networks\nseparately to decouple the feature descriptors which encode the body parameters\nfrom different views, and fuse them to estimate an accurate human body shape.\nIn addition, to overcome the shortage of training data required for this\npurpose, we propose some significantly data augmentation schemes for 3D human\nbody shapes, which can be used to promote further research on this topic.\nExtensive experimen- tal results demonstrate that visually realistic and\naccurate reconstructions can be achieved effectively using our algorithm.\nRequiring only binary mask images, our method can help users create their own\ndigital avatars quickly, and also make it easy to create digital human body for\n3D game, virtual reality, online fashion shopping.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.08485v1"
    },
    {
        "title": "Diffeomorphic Medial Modeling",
        "authors": [
            "Paul A. Yushkevich",
            "Ahmed Aly",
            "Jiancong Wang",
            "Long Xie",
            "Robert C. Gorman",
            "Laurent Younes",
            "Alison Pouch"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Deformable shape modeling approaches that describe objects in terms of their\nmedial axis geometry (e.g., m-reps [Pizer et al., 2003]) yield rich geometrical\nfeatures that can be useful for analyzing the shape of sheet-like biological\nstructures, such as the myocardium. We present a novel shape analysis approach\nthat combines the benefits of medial shape modeling and diffeomorphometry. Our\nalgorithm is formulated as a problem of matching shapes using diffeomorphic\nflows under constraints that approximately preserve medial axis geometry during\ndeformation. As the result, correspondence between the medial axes of similar\nshapes is maintained. The approach is evaluated in the context of modeling the\nshape of the left ventricular wall from 3D echocardiography images.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.02371v2"
    },
    {
        "title": "Puppet Dubbing",
        "authors": [
            "Ohad Fried",
            "Maneesh Agrawala"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Dubbing puppet videos to make the characters (e.g. Kermit the Frog)\nconvincingly speak a new speech track is a popular activity with many examples\nof well-known puppets speaking lines from films or singing rap songs. But\nmanually aligning puppet mouth movements to match a new speech track is tedious\nas each syllable of the speech must match a closed-open-closed segment of mouth\nmovement for the dub to be convincing. In this work, we present two methods to\nalign a new speech track with puppet video, one semi-automatic appearance-based\nand the other fully-automatic audio-based. The methods offer complementary\nadvantages and disadvantages. Our appearance-based approach directly identifies\nclosed-open-closed segments in the puppet video and is robust to low-quality\naudio as well as misalignments between the mouth movements and speech in the\noriginal performance, but requires some manual annotation. Our audio-based\napproach assumes the original performance matches a closed-open-closed mouth\nsegment to each syllable of the original speech. It is fully automatic, robust\nto visual occlusions and fast puppet movements, but does not handle\nmisalignments in the original performance. We compare the methods and show that\nboth improve the credibility of the resulting video over simple baseline\ntechniques, via quantitative evaluation and user ratings.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.04285v1"
    },
    {
        "title": "Parallel Rendering and Large Data Visualization",
        "authors": [
            "Stefan Eilemann"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We are living in the big data age: An ever increasing amount of data is being\nproduced through data acquisition and computer simulations. While large scale\nanalysis and simulations have received significant attention for cloud and\nhigh-performance computing, software to efficiently visualise large data sets\nis struggling to keep up.\n  Visualization has proven to be an efficient tool for understanding data, in\nparticular visual analysis is a powerful tool to gain intuitive insight into\nthe spatial structure and relations of 3D data sets. Large-scale visualization\nsetups are becoming ever more affordable, and high-resolution tiled display\nwalls are in reach even for small institutions. Virtual reality has arrived in\nthe consumer space, making it accessible to a large audience.\n  This thesis addresses these developments by advancing the field of parallel\nrendering. We formalise the design of system software for large data\nvisualization through parallel rendering, provide a reference implementation of\na parallel rendering framework, introduce novel algorithms to accelerate the\nrendering of large amounts of data, and validate this research and development\nwith new applications for large data visualization. Applications built using\nour framework enable domain scientists and large data engineers to better\nextract meaning from their data, making it feasible to explore more data and\nenabling the use of high-fidelity visualization installations to see more\ndetail of the data.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.08755v1"
    },
    {
        "title": "VoroCrust: Voronoi Meshing Without Clipping",
        "authors": [
            "Ahmed Abdelkader",
            "Chandrajit L. Bajaj",
            "Mohamed S. Ebeida",
            "Ahmed H. Mahmoud",
            "Scott A. Mitchell",
            "John D. Owens",
            "Ahmad A. Rushdi"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Polyhedral meshes are increasingly becoming an attractive option with\nparticular advantages over traditional meshes for certain applications. What\nhas been missing is a robust polyhedral meshing algorithm that can handle broad\nclasses of domains exhibiting arbitrarily curved boundaries and sharp features.\nIn addition, the power of primal-dual mesh pairs, exemplified by\nVoronoi-Delaunay meshes, has been recognized as an important ingredient in\nnumerous formulations. The VoroCrust algorithm is the first provably-correct\nalgorithm for conforming polyhedral Voronoi meshing for non-convex and\nnon-manifold domains with guarantees on the quality of both surface and volume\nelements. A robust refinement process estimates a suitable sizing field that\nenables the careful placement of Voronoi seeds across the surface circumventing\nthe need for clipping and avoiding its many drawbacks. The algorithm has the\nflexibility of filling the interior by either structured or random samples,\nwhile preserving all sharp features in the output mesh. We demonstrate the\ncapabilities of the algorithm on a variety of models and compare against\nstate-of-the-art polyhedral meshing methods based on clipped Voronoi cells\nestablishing the clear advantage of VoroCrust output.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.08767v3"
    },
    {
        "title": "Unsupervised Detection of Distinctive Regions on 3D Shapes",
        "authors": [
            "Xianzhi Li",
            "Lequan Yu",
            "Chi-Wing Fu",
            "Daniel Cohen-Or",
            "Pheng-Ann Heng"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper presents a novel approach to learn and detect distinctive regions\non 3D shapes. Unlike previous works, which require labeled data, our method is\nunsupervised. We conduct the analysis on point sets sampled from 3D shapes,\nthen formulate and train a deep neural network for an unsupervised shape\nclustering task to learn local and global features for distinguishing shapes\nwith respect to a given shape set. To drive the network to learn in an\nunsupervised manner, we design a clustering-based nonparametric softmax\nclassifier with an iterative re-clustering of shapes, and an adapted\ncontrastive loss for enhancing the feature embedding quality and stabilizing\nthe learning process. By then, we encourage the network to learn the point\ndistinctiveness on the input shapes. We extensively evaluate various aspects of\nour approach and present its applications for distinctiveness-guided shape\nretrieval, sampling, and view selection in 3D scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.01684v2"
    },
    {
        "title": "Transport-Based Neural Style Transfer for Smoke Simulations",
        "authors": [
            "Byungsoo Kim",
            "Vinicius C. Azevedo",
            "Markus Gross",
            "Barbara Solenthaler"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Artistically controlling fluids has always been a challenging task.\nOptimization techniques rely on approximating simulation states towards target\nvelocity or density field configurations, which are often handcrafted by\nartists to indirectly control smoke dynamics. Patch synthesis techniques\ntransfer image textures or simulation features to a target flow field. However,\nthese are either limited to adding structural patterns or augmenting coarse\nflows with turbulent structures, and hence cannot capture the full spectrum of\ndifferent styles and semantically complex structures. In this paper, we propose\nthe first Transport-based Neural Style Transfer (TNST) algorithm for volumetric\nsmoke data. Our method is able to transfer features from natural images to\nsmoke simulations, enabling general content-aware manipulations ranging from\nsimple patterns to intricate motifs. The proposed algorithm is physically\ninspired, since it computes the density transport from a source input smoke to\na desired target configuration. Our transport-based approach allows direct\ncontrol over the divergence of the stylization velocity field by optimizing\nincompressible and irrotational potentials that transport smoke towards\nstylization. Temporal consistency is ensured by transporting and aligning\nsubsequent stylized velocities, and 3D reconstructions are computed by\nseamlessly merging stylizations from different camera viewpoints.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.07442v2"
    },
    {
        "title": "BrainPainter: A software for the visualisation of brain structures,\n  biomarkers and associated pathological processes",
        "authors": [
            "Razvan V. Marinescu",
            "Arman Eshaghi",
            "Daniel C. Alexander",
            "Polina Golland"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present BrainPainter, a software that automatically generates images of\nhighlighted brain structures given a list of numbers corresponding to the\noutput colours of each region. Compared to existing visualisation software\n(i.e. Freesurfer, SPM, 3D Slicer), BrainPainter has three key advantages: (1)\nit does not require the input data to be in a specialised format, allowing\nBrainPainter to be used in combination with any neuroimaging analysis tools,\n(2) it can visualise both cortical and subcortical structures and (3) it can be\nused to generate movies showing dynamic processes, e.g. propagation of\npathology on the brain. We highlight three use cases where BrainPainter was\nused in existing neuroimaging studies: (1) visualisation of the degree of\natrophy through interpolation along a user-defined gradient of colours, (2)\nvisualisation of the progression of pathology in Alzheimer's disease as well as\n(3) visualisation of pathology in subcortical regions in Huntington's disease.\nMoreover, through the design of BrainPainter we demonstrate the possibility of\nusing a powerful 3D computer graphics engine such as Blender to generate brain\nvisualisations for the neuroscience community. Blender's capabilities, e.g.\nparticle simulations, motion graphics, UV unwrapping, raster graphics editing,\nraytracing and illumination effects, open a wealth of possibilities for brain\nvisualisation not available in current neuroimaging software. BrainPainter is\ncustomisable, easy to use, and can run straight from the web browser:\nhttps://brainpainter.csail.mit.edu , as well as from source-code packaged in a\ndocker container: https://github.com/mrazvan22/brain-coloring . It can be used\nto visualise biomarker data from any brain imaging modality, or simply to\nhighlight a particular brain structure for e.g. anatomy courses.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08627v2"
    },
    {
        "title": "Data-Driven Crowd Simulation with Generative Adversarial Networks",
        "authors": [
            "Javad Amirian",
            "Wouter van Toll",
            "Jean-Bernard Hayet",
            "Julien Pettré"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper presents a novel data-driven crowd simulation method that can\nmimic the observed traffic of pedestrians in a given environment. Given a set\nof observed trajectories, we use a recent form of neural networks, Generative\nAdversarial Networks (GANs), to learn the properties of this set and generate\nnew trajectories with similar properties. We define a way for simulated\npedestrians (agents) to follow such a trajectory while handling local collision\navoidance. As such, the system can generate a crowd that behaves similarly to\nobservations, while still enabling real-time interactions between agents. Via\nexperiments with real-world data, we show that our simulated trajectories\npreserve the statistical properties of their input. Our method simulates crowds\nin real time that resemble existing crowds, while also allowing insertion of\nextra agents, combination with other simulation methods, and user interaction.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.09661v1"
    },
    {
        "title": "Continuous Histograms for Anisotropy of 2D Symmetric Piece-wise Linear\n  Tensor Fields",
        "authors": [
            "Talha Bin Masood",
            "Ingrid Hotz"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The analysis of contours of scalar fields plays an important role in\nvisualization. For example the contour tree and contour statistics can be used\nas a means for interaction and filtering or as signatures. In the context of\ntensor field analysis, such methods are also interesting for the analysis of\nderived scalar invariants. While there are standard algorithms to compute and\nanalyze contours, they are not directly applicable to tensor invariants when\nusing component-wise tensor interpolation. In this chapter we present an\naccurate derivation of the contour spectrum for invariants with quadratic\nbehavior computed from two-dimensional piece-wise linear tensor fields. For\nthis work, we are mostly motivated by a consistent treatment of the anisotropy\nfield, which plays an important role as stability measure for tensor field\ntopology. We show that it is possible to derive an analytical expression for\nthe distribution of the invariant values in this setting, which is exemplary\ngiven for the anisotropy in all details. Our derivation is based on a\ntopological sub-division of the mesh in triangles that exhibit a monotonic\nbehavior. This triangulation can also directly be used to compute the accurate\ncontour tree with standard algorithms. We compare the results to a na\\\"ive\napproach based on linear interpolation on the original mesh or the subdivision.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.00739v1"
    },
    {
        "title": "A Bayesian Inference Framework for Procedural Material Parameter\n  Estimation",
        "authors": [
            "Yu Guo",
            "Milos Hasan",
            "Lingqi Yan",
            "Shuang Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Procedural material models have been gaining traction in many applications\nthanks to their flexibility, compactness, and easy editability. We explore the\ninverse rendering problem of procedural material parameter estimation from\nphotographs, presenting a unified view of the problem in a Bayesian framework.\nIn addition to computing point estimates of the parameters by optimization, our\nframework uses a Markov Chain Monte Carlo approach to sample the space of\nplausible material parameters, providing a collection of plausible matches that\na user can choose from, and efficiently handling both discrete and continuous\nmodel parameters. To demonstrate the effectiveness of our framework, we fit\nprocedural models of a range of materials---wall plaster, leather, wood,\nanisotropic brushed metals and layered metallic paints---to both synthetic and\nreal target images.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.01067v5"
    },
    {
        "title": "Multiple Approaches to Frame Field Correction for CAD Models",
        "authors": [
            "Maxence Reberol",
            "Alexandre Chemin",
            "Jean-Francois Remacle"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Three-dimensional frame fields computed on CAD models often contain singular\ncurves that are not compatible with hexahedral meshing. In this paper, we show\nhow CAD feature curves can induce non meshable 3-5 singular curves and we study\nfour different approaches that aims at correcting the frame field topology. All\napproaches consist in modifying the frame field computation, the two first ones\nconsisting in applying internal constraints and the two last ones consisting in\nmodifying the boundary conditions. Approaches based on internal constraints are\nshown not to be very reliable because of their interactions with other\nsingularities. On the other hand, boundary condition modifications are more\npromising as their impact is very localized. We eventually recommend the 3-5\nsingular curve boundary snapping strategy, which is simple to implement and\nallows to generate topologically correct frame fields.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.01248v2"
    },
    {
        "title": "Towards Sustainable Architecture: 3D Convolutional Neural Networks for\n  Computational Fluid Dynamics Simulation and Reverse DesignWorkflow",
        "authors": [
            "Josef Musil",
            "Jakub Knir",
            "Athanasios Vitsas",
            "Irene Gallou"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a general and flexible approximation model for near real-time\nprediction of steady turbulent flow in a 3D domain based on residual\nConvolutional Neural Networks (CNNs). This approach can provide immediate\nfeedback for real-time iterations at the early stage of architectural design.\nThis work-flow is then reversed and offers a designer a tool that generates\nbuilding volumes based on target wind flow.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.02125v1"
    },
    {
        "title": "Spectral domain decomposition method for physically-based rendering of\n  Royaumont abbey",
        "authors": [
            "Guillaume Gbikpi-Benissan",
            "Patrick Callet",
            "Frederic Magoules"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In the context of a virtual reconstitution of the destroyed Royaumont abbey\nchurch, this paper investigates computer sciences issues intrinsic to the\nphysically-based image rendering. First, a virtual model was designed from\nhistorical sources and archaeological descriptions. Then some materials\nphysical properties were measured on remains of the church and on pieces from\nsimilar ancient churches. We specify the properties of our lighting source\nwhich is a representation of the sun, and present the rendering algorithm\nimplemented in our software Virtuelium. In order to accelerate the computation\nof the interactions between light-rays and objects, this ray-tracing algorithm\nis parallelized by means of domain decomposition techniques. Numerical\nexperiments show that the computational time saved by a classic parallelization\nis much less significant than that gained with our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.04000v1"
    },
    {
        "title": "Interactive 3D fluid simulation: steering the simulation in progress\n  using Lattice Boltzmann Method",
        "authors": [
            "Mengchen Wang",
            "Nicolas Ferey",
            "Patrick Bourdot",
            "Frederic Magoules"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper describes a work in progress about software and hardware\narchitecture to steer and control an ongoing fluid simulation in a context of a\nserious game application. We propose to use the Lattice Boltzmann Method as the\nsimulation approach considering that it can provide fully parallel algorithms\nto reach interactive time and because it is easier to change parameters while\nthe simulation is in progress remaining physically relevant than more classical\nsimulation approaches. We describe which parameters we can modify and how we\nsolve technical issues of interactive steering and we finally show an\napplication of our interactive fluid simulation approach of water dam\nphenomena.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.04356v1"
    },
    {
        "title": "Modelling curvature of a bent paper leaf",
        "authors": [
            "Sasikanth Raghava Goteti"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this article, we briefly describe various tools and approaches that\nalgebraic geometry has to offer to straighten bent objects. Throughout this\narticle we will consider a specific example of a bent or curved piece of paper\nwhich in our case acts very much like an elastica curve. We conclude this\narticle with a suggestion to algebraic geometry as a viable and fast\nperformance alternative of neural networks in vision and machine learning. The\npurpose of this article is not to build a full blown framework but to show\npossibility of using algebraic geometry as an alternative to neural networks\nfor recognizing or extracting features on manifolds.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.04898v1"
    },
    {
        "title": "Uncertainty Visualization of 2D Morse Complex Ensembles Using\n  Statistical Summary Maps",
        "authors": [
            "Tushar Athawale",
            "Dan Maljovec",
            "Chris R. Johnson",
            "Valerio Pascucci",
            "Bei Wang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Morse complexes are gradient-based topological descriptors with close\nconnections to Morse theory. They are widely applicable in scientific\nvisualization as they serve as important abstractions for gaining insights into\nthe topology of scalar fields. Noise inherent to scalar field data due to\nacquisitions and processing, however, limits our understanding of the Morse\ncomplexes as structural abstractions. We, therefore, explore uncertainty\nvisualization of an ensemble of 2D Morse complexes that arise from scalar\nfields coupled with data uncertainty. We propose statistical summary maps as\nnew entities for capturing structural variations and visualizing positional\nuncertainties of Morse complexes in ensembles. Specifically, we introduce two\ntypes of statistical summary maps -- the Probabilistic Map and the Survival Map\n-- to characterize the uncertain behaviors of local extrema and local gradient\nflows, respectively. We demonstrate the utility of our proposed approach using\nsynthetic and real-world datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06341v1"
    },
    {
        "title": "Spectral domain decomposition method for physically-based rendering of\n  photochromic/electrochromic glass windows",
        "authors": [
            "Guillaume Gbikpi-Benissan",
            "Patrick Callet",
            "Frederic Magoules"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper covers the time consuming issues intrinsic to physically-based\nimage rendering algorithms. First, glass materials optical properties were\nmeasured on samples of real glasses and other objects materials inside an hotel\nroom were characterized by deducing spectral data from multiple trichromatic\nimages. We then present the rendering model and ray-tracing algorithm\nimplemented in Virtuelium, an open source software. In order to accelerate the\ncomputation of the interactions between light rays and objects, the ray-tracing\nalgorithm is parallelized by means of domain decomposition method techniques.\nNumerical experiments show that the speedups obtained with classical\nparallelization techniques are significantly less significant than those\nachieved with parallel domain decomposition methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06474v1"
    },
    {
        "title": "Multi-display Visual Analysis: Model, Interface, and Layout Computation",
        "authors": [
            "Christian Eichner",
            "Heidrun Schumann",
            "Christian Tominski"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Modern display environments offer great potential for involving multiple\nusers in presentations, discussions, and data analysis sessions. By showing\nmultiple views on multiple displays, information exchange can be improved,\nseveral perspectives on the data can be combined, and different analysis\nstrategies can be pursued.\n  In this report, we describe concepts to support display composition,\ninformation distribution, and analysis coordination for visual data analysis in\nmulti-display environments. In particular, a basic model for layout modeling is\nintroduced, a graphical interface for interactive generation of the model is\npresented, and a layout mechanism is described that arranges multiple views on\nmultiple displays automatically. Furthermore, approaches to meta-analysis will\nbe discussed. The developed approaches are demonstrated in a use case that\nfocuses on parameter space analysis for the segmentation of time series data.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.08558v1"
    },
    {
        "title": "GrabAR: Occlusion-aware Grabbing Virtual Objects in AR",
        "authors": [
            "Xiao Tang",
            "Xiaowei Hu",
            "Chi-Wing Fu",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Existing augmented reality (AR) applications often ignore occlusion between\nreal hands and virtual objects when incorporating virtual objects in our views.\nThe challenges come from the lack of accurate depth and mismatch between real\nand virtual depth. This paper presents GrabAR, a new approach that directly\npredicts the real-and-virtual occlusion, and bypasses the depth acquisition and\ninference. Our goal is to enhance AR applications with interactions between\nhand (real) and grabbable objects (virtual). With paired images of hand and\nobject as inputs, we formulate a neural network that learns to generate the\nocclusion mask. To train the network, we compile a synthetic dataset to\npre-train it and a real dataset to fine-tune it, thus reducing the burden of\nmanual labels and addressing the domain difference. Then, we embed the trained\nnetwork in a prototyping AR system that supports hand grabbing of various\nvirtual objects, demonstrate the system performance, both quantitatively and\nqualitatively, and showcase interaction scenarios, in which we can use bare\nhand to grab virtual objects and directly manipulate them.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10637v3"
    },
    {
        "title": "Blind Recovery of Spatially Varying Reflectance from a Single Image",
        "authors": [
            "Kevin Karsch",
            "David Forsyth"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a new technique for estimating spatially varying parametric\nmaterials from a single image of an object with unknown shape in unknown\nillumination. Our method uses a low-order parametric reflectance model, and\nincorporates strong assumptions about lighting and shape. We develop new priors\nabout how materials mix over space, and jointly infer all of these properties\nfrom a single image. This produces a decomposition of an image which\ncorresponds, in one sense, to microscopic features (material reflectance) and\nmacroscopic features (weights defining the mixing properties of materials over\nspace). We have built a large dataset of real objects rendered with different\nmaterial models under different illumination fields for training and ground\ntruth evaluation. Extensive experiments on both our synthetic dataset images as\nwell as real images show that (a) our method recovers parameters with\nreasonable accuracy; (b) material parameters recovered by our method give\naccurate predictions of new renderings of the object; and (c) our low-order\nreflectance model still provides a good fit to many real-world reflectances.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11568v1"
    },
    {
        "title": "Automatic Scene Inference for 3D Object Compositing",
        "authors": [
            "Kevin Karsch",
            "Kalyan Sunkavalli",
            "Sunil Hadap",
            "Nathan Carr",
            "Hailin Jin",
            "Rafael Fonte",
            "Michael Sittig"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a user-friendly image editing system that supports a drag-and-drop\nobject insertion (where the user merely drags objects into the image, and the\nsystem automatically places them in 3D and relights them appropriately),\npost-process illumination editing, and depth-of-field manipulation. Underlying\nour system is a fully automatic technique for recovering a comprehensive 3D\nscene model (geometry, illumination, diffuse albedo and camera parameters) from\na single, low dynamic range photograph. This is made possible by two novel\ncontributions: an illumination inference algorithm that recovers a full\nlighting model of the scene (including light sources that are not directly\nvisible in the photograph), and a depth estimation algorithm that combines\ndata-driven depth transfer with geometric reasoning about the scene layout. A\nuser study shows that our system produces perceptually convincing results, and\nachieves the same level of realism as techniques that require significant user\ninteraction.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12297v1"
    },
    {
        "title": "Animals in Virtual Environments",
        "authors": [
            "Hemal Naik",
            "Renaud Bastien",
            "Nassir Navab",
            "Iain Couzin"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The core idea in an XR (VR/MR/AR) application is to digitally stimulate one\nor more sensory systems (e.g. visual, auditory, olfactory) of the human user in\nan interactive way to achieve an immersive experience. Since the early 2000s\nbiologists have been using Virtual Environments (VE) to investigate the\nmechanisms of behavior in non-human animals including insect, fish, and\nmammals. VEs have become reliable tools for studying vision, cognition, and\nsensory-motor control in animals. In turn, the knowledge gained from studying\nsuch behaviors can be harnessed by researchers designing biologically inspired\nrobots, smart sensors, and multi-agent artificial intelligence. VE for animals\nis becoming a widely used application of XR technology but such applications\nhave not previously been reported in the technical literature related to XR.\nBiologists and computer scientists can benefit greatly from deepening\ninterdisciplinary research in this emerging field and together we can develop\nnew methods for conducting fundamental research in behavioral sciences and\nengineering. To support our argument we present this review which provides an\noverview of animal behavior experiments conducted in virtual environments.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12763v2"
    },
    {
        "title": "Bas-relief Generation from Point Clouds Based on Normal Space\n  Compression with Real-time Adjustment on CPU",
        "authors": [
            "Jianhui Nie",
            "Wenkai Shi",
            "Ye Liu",
            "Hao Gao",
            "Feng Xu",
            "Zhaochen Zhang",
            "Guoping Jiang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Bas-relief generation based on 3d models is a hot topic in computer graphics.\nState-of-the-art algorithms take a mesh surface as input, but real-time\ninteraction via CPU cannot be realized. In this paper, a bas-relief generation\nalgorithm that takes a scattered point cloud as input is proposed. The\nalgorithm takes normal vectors as the operation object and the variation of the\nlocal surface as the compression criterion. By constructing and solving linear\nequations of bas-relief vertices, the closed-form solution can be obtained.\nSince there is no need to compute discrete gradients on a point cloud lacking\ntopology information, it is easier to implement and more intuitive than\ngradient domain methods. The algorithm provides parameters to adjust the\nbas-relief height, saturation and detail richness. At the same time, through\nthe solution strategy based on the subspace, it realizes the real-time\nadjustment of the bas-relief effect based on the computing power of a consumer\nCPU. In addition, an iterative solution to generate a bas-relief model of a\nspecified height is presented to meet specific application requirements.\nExperiments show that our algorithm provides a unified solution for various\ntypes of bas-relief creation and can generate bas-reliefs with good saturation\nand rich details.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.13140v1"
    },
    {
        "title": "Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings",
        "authors": [
            "Amy Zhao",
            "Guha Balakrishnan",
            "Kathleen M. Lewis",
            "Frédo Durand",
            "John V. Guttag",
            "Adrian V. Dalca"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce a new video synthesis task: synthesizing time lapse videos\ndepicting how a given painting might have been created. Artists paint using\nunique combinations of brushes, strokes, and colors. There are often many\npossible ways to create a given painting. Our goal is to learn to capture this\nrich range of possibilities.\n  Creating distributions of long-term videos is a challenge for learning-based\nvideo synthesis methods. We present a probabilistic model that, given a single\nimage of a completed painting, recurrently synthesizes steps of the painting\nprocess. We implement this model as a convolutional neural network, and\nintroduce a novel training scheme to enable learning from a limited dataset of\npainting time lapses. We demonstrate that this model can be used to sample many\ntime steps, enabling long-term stochastic video synthesis. We evaluate our\nmethod on digital and watercolor paintings collected from video websites, and\nshow that human raters find our synthetic videos to be similar to time lapse\nvideos produced by real artists. Our code is available at\nhttps://xamyzhao.github.io/timecraft.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.01026v2"
    },
    {
        "title": "Unsupervised multi-modal Styled Content Generation",
        "authors": [
            "Omry Sendik",
            "Dani Lischinski",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The emergence of deep generative models has recently enabled the automatic\ngeneration of massive amounts of graphical content, both in 2D and in 3D.\nGenerative Adversarial Networks (GANs) and style control mechanisms, such as\nAdaptive Instance Normalization (AdaIN), have proved particularly effective in\nthis context, culminating in the state-of-the-art StyleGAN architecture. While\nsuch models are able to learn diverse distributions, provided a sufficiently\nlarge training set, they are not well-suited for scenarios where the\ndistribution of the training data exhibits a multi-modal behavior. In such\ncases, reshaping a uniform or normal distribution over the latent space into a\ncomplex multi-modal distribution in the data domain is challenging, and the\ngenerator might fail to sample the target distribution well. Furthermore,\nexisting unsupervised generative models are not able to control the mode of the\ngenerated samples independently of the other visual attributes, despite the\nfact that they are typically disentangled in the training data.\n  In this paper, we introduce UMMGAN, a novel architecture designed to better\nmodel multi-modal distributions, in an unsupervised fashion. Building upon the\nStyleGAN architecture, our network learns multiple modes, in a completely\nunsupervised manner, and combines them using a set of learned weights. We\ndemonstrate that this approach is capable of effectively approximating a\ncomplex distribution as a superposition of multiple simple ones. We further\nshow that UMMGAN effectively disentangles between modes and style, thereby\nproviding an independent degree of control over the generated content.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.03640v2"
    },
    {
        "title": "Neural Human Video Rendering by Learning Dynamic Textures and\n  Rendering-to-Video Translation",
        "authors": [
            "Lingjie Liu",
            "Weipeng Xu",
            "Marc Habermann",
            "Michael Zollhoefer",
            "Florian Bernard",
            "Hyeongwoo Kim",
            "Wenping Wang",
            "Christian Theobalt"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Synthesizing realistic videos of humans using neural networks has been a\npopular alternative to the conventional graphics-based rendering pipeline due\nto its high efficiency. Existing works typically formulate this as an\nimage-to-image translation problem in 2D screen space, which leads to artifacts\nsuch as over-smoothing, missing body parts, and temporal instability of\nfine-scale detail, such as pose-dependent wrinkles in the clothing. In this\npaper, we propose a novel human video synthesis method that approaches these\nlimiting factors by explicitly disentangling the learning of time-coherent\nfine-scale details from the embedding of the human in 2D screen space. More\nspecifically, our method relies on the combination of two convolutional neural\nnetworks (CNNs). Given the pose information, the first CNN predicts a dynamic\ntexture map that contains time-coherent high-frequency details, and the second\nCNN conditions the generation of the final video on the temporally coherent\noutput of the first CNN. We demonstrate several applications of our approach,\nsuch as human reenactment and novel view synthesis from monocular video, where\nwe show significant improvement over the state of the art both qualitatively\nand quantitatively.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.04947v3"
    },
    {
        "title": "A Variational Staggered Particle Framework for Incompressible\n  Free-Surface Flows",
        "authors": [
            "Xiaowei He",
            "Huamin Wang",
            "Guoping Wang",
            "Hongan Wang",
            "Enhua Wu"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Smoothed particle hydrodynamics (SPH) has been extensively studied in\ncomputer graphics to animate fluids with versatile effects. However, SPH still\nsuffers from two numerical difficulties: the particle deficiency problem, which\nwill deteriorate the simulation accuracy, and the particle clumping problem,\nwhich usually leads to poor stability of particle simulations. We propose to\nsolve these two problems by developing an approximate projection method for\nincompressible free-surface flows under a variational staggered particle\nframework. After particle discretization, we first categorize all fluid\nparticles into four subsets. Then according to the classification, we propose\nto solve the particle deficiency problem by analytically imposing free surface\nboundary conditions on both the Laplacian operator and the source term. To\naddress the particle clumping problem, we propose to extend the Taylor-series\nconsistent pressure gradient model with kernel function correction and\nsemi-analytical boundary conditions. Compared to previous approximate\nprojection method [1], our incompressibility solver is stable under both\ncompressive and tensile stress states, no pressure clumping or iterative\ndensity correction (e.g., a density constrained pressure approach) is necessary\nto stabilize the solver anymore. Motivated by the Helmholtz free energy\nfunctional, we additionally introduce an iterative particle shifting algorithm\nto improve the accuracy. It significantly reduces particle splashes near the\nfree surface. Therefore, high-fidelity simulations of the formation and\nfragmentation of liquid jets and sheets are obtained for both the two-jets and\nmilk-crown examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.09421v1"
    },
    {
        "title": "MGCN: Descriptor Learning using Multiscale GCNs",
        "authors": [
            "Yiqun Wang",
            "Jing Ren",
            "Dong-Ming Yan",
            "Jianwei Guo",
            "Xiaopeng Zhang",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose a novel framework for computing descriptors for characterizing\npoints on three-dimensional surfaces. First, we present a new non-learned\nfeature that uses graph wavelets to decompose the Dirichlet energy on a\nsurface. We call this new feature wavelet energy decomposition signature\n(WEDS). Second, we propose a new multiscale graph convolutional network (MGCN)\nto transform a non-learned feature to a more discriminative descriptor. Our\nresults show that the new descriptor WEDS is more discriminative than the\ncurrent state-of-the-art non-learned descriptors and that the combination of\nWEDS and MGCN is better than the state-of-the-art learned descriptors. An\nimportant design criterion for our descriptor is the robustness to different\nsurface discretizations including triangulations with varying numbers of\nvertices. Our results demonstrate that previous graph convolutional networks\nsignificantly overfit to a particular resolution or even a particular\ntriangulation, but MGCN generalizes well to different surface discretizations.\nIn addition, MGCN is compatible with previous descriptors and it can also be\nused to improve the performance of other descriptors, such as the heat kernel\nsignature, the wave kernel signature, or the local point signature.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10472v3"
    },
    {
        "title": "An Automated Approach for the Discovery of Interoperability",
        "authors": [
            "Duygu Sap",
            "Daniel P. Szabo"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this article, we present an automated approach that would test for and\ndiscover the interoperability of CAD systems based on the\napproximately-invariant shape properties of their models. We further show that\nexchanging models in standard format does not guarantee the preservation of\nshape properties. Our analysis is based on utilizing queries in deriving the\nshape properties and constructing the proxy models of the given CAD models [1].\nWe generate template files to accommodate the information necessary for the\nproperty computations and proxy model constructions, and implement an\ninteroperability discovery program called DTest to execute the interoperability\ntesting. We posit that our method could be extended to interoperability testing\non CAD-to-CAE and/or CAD-to-CAM interactions by modifying the set of property\nchecks and providing the additional requirements that may emerge in CAE or CAM\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10585v1"
    },
    {
        "title": "RigNet: Neural Rigging for Articulated Characters",
        "authors": [
            "Zhan Xu",
            "Yang Zhou",
            "Evangelos Kalogerakis",
            "Chris Landreth",
            "Karan Singh"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present RigNet, an end-to-end automated method for producing animation\nrigs from input character models. Given an input 3D model representing an\narticulated character, RigNet predicts a skeleton that matches the animator\nexpectations in joint placement and topology. It also estimates surface skin\nweights based on the predicted skeleton. Our method is based on a deep\narchitecture that directly operates on the mesh representation without making\nassumptions on shape class and structure. The architecture is trained on a\nlarge and diverse collection of rigged models, including their mesh, skeletons\nand corresponding skin weights. Our evaluation is three-fold: we show better\nresults than prior art when quantitatively compared to animator rigs;\nqualitatively we show that our rigs can be expressively posed and animated at\nmultiple levels of detail; and finally, we evaluate the impact of various\nalgorithm choices on our output rigs.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.00559v2"
    },
    {
        "title": "Lagrangian Neural Style Transfer for Fluids",
        "authors": [
            "Byungsoo Kim",
            "Vinicius C. Azevedo",
            "Markus Gross",
            "Barbara Solenthaler"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Artistically controlling the shape, motion and appearance of fluid\nsimulations pose major challenges in visual effects production. In this paper,\nwe present a neural style transfer approach from images to 3D fluids formulated\nin a Lagrangian viewpoint. Using particles for style transfer has unique\nbenefits compared to grid-based techniques. Attributes are stored on the\nparticles and hence are trivially transported by the particle motion. This\nintrinsically ensures temporal consistency of the optimized stylized structure\nand notably improves the resulting quality. Simultaneously, the expensive,\nrecursive alignment of stylization velocity fields of grid approaches is\nunnecessary, reducing the computation time to less than an hour and rendering\nneural flow stylization practical in production settings. Moreover, the\nLagrangian representation improves artistic control as it allows for\nmulti-fluid stylization and consistent color transfer from images, and the\ngenerality of the method enables stylization of smoke and liquids likewise.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.00803v1"
    },
    {
        "title": "Variational Shape Approximation of Point Set Surfaces",
        "authors": [
            "Martin Skrodzki",
            "Eric Zimmermann",
            "Konrad Polthier"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this work, we present a translation of the complete pipeline for\nvariational shape approximation (VSA) to the setting of point sets. First, we\ndescribe an explicit example for the theoretically known non-convergence of the\ncurrently available VSA approaches. The example motivates us to introduce an\nalternate version of VSA based on a switch operation for which we prove\nconvergence. Second, we discuss how two operations - split and merge - can be\nincluded in a fully automatic pipeline that is in turn independent of the\nplacement and number of initial seeds. Third and finally, we present two\napproaches how to obtain a simplified mesh from the output of the VSA\nprocedure. This simplification is either based on simple plane intersection or\nbased on a variational optimization problem. Several qualitative and\nquantitative results prove the relevance of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01003v2"
    },
    {
        "title": "Neural Subdivision",
        "authors": [
            "Hsueh-Ti Derek Liu",
            "Vladimir G. Kim",
            "Siddhartha Chaudhuri",
            "Noam Aigerman",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper introduces Neural Subdivision, a novel framework for data-driven\ncoarse-to-fine geometry modeling. During inference, our method takes a coarse\ntriangle mesh as input and recursively subdivides it to a finer geometry by\napplying the fixed topological updates of Loop Subdivision, but predicting\nvertex positions using a neural network conditioned on the local geometry of a\npatch. This approach enables us to learn complex non-linear subdivision\nschemes, beyond simple linear averaging used in classical techniques. One of\nour key contributions is a novel self-supervised training setup that only\nrequires a set of high-resolution meshes for learning network weights. For any\ntraining shape, we stochastically generate diverse low-resolution\ndiscretizations of coarse counterparts, while maintaining a bijective mapping\nthat prescribes the exact target position of every new vertex during the\nsubdivision process. This leads to a very efficient and accurate loss function\nfor conditional mesh generation, and enables us to train a method that\ngeneralizes across discretizations and favors preserving the manifold structure\nof the output. During training we optimize for the same set of network weights\nacross all local mesh patches, thus providing an architecture that is not\nconstrained to a specific input mesh, fixed genus, or category. Our network\nencodes patch geometry in a local frame in a rotation- and\ntranslation-invariant manner. Jointly, these design choices enable our method\nto generalize well, and we demonstrate that even when trained on a single\nhigh-resolution mesh our method generates reasonable subdivisions for novel\nshapes.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01819v1"
    },
    {
        "title": "Optimally Fast Soft Shadows on Curved Terrain with Dynamic Programming\n  and Maximum Mipmaps",
        "authors": [
            "Dawoon Jung",
            "Fridger Schrempp",
            "Seunghee Son"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a simple, novel method of efficiently rendering ray cast soft\nshadows on curved terrain by using dynamic programming and maximum mipmaps to\nrapidly find a global minimum shadow cost in constant runtime complexity.\nAdditionally, we apply a new method of reducing view ray computation times that\npre-displaces the terrain mesh to bootstrap ray starting positions. Combining\nthese two methods, our ray casting engine runs in real-time with more than 200%\nspeed up over uniform ray stepping with comparable image quality and without\nhardware ray tracing acceleration. To add support for accurate planetary\nephemerides and interactive features, we integrated the engine into\ncelestia.Sci, a general space simulation software. We demonstrate the ability\nof our engine to accurately handle a large range of distance scales by using it\nto generate videos of lunar landing trajectories. The numerical error when\ncompared with real lunar mission imagery is small, demonstrating the accuracy\nand efficiency of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.06671v1"
    },
    {
        "title": "Plane-Activated Mapped Microstructure",
        "authors": [
            "Jeremy Youngquist",
            "Jörg Peters",
            "Meera Sitharam"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Querying and interacting with models of massive material micro-structure\nrequires localized on-demand generation of the micro-structure since the\nfull-scale storing and retrieving is cost prohibitive. When the micro-structure\nis efficiently represented as the image of a canonical structure under a\nnon-linear space deformation to allow it to conform to curved shape, the\nadditional challenge is to relate the query of the mapped micro-structure back\nto its canonical structure. This paper presents an efficient algorithm to pull\nback a mapped micro-structure to a partition of the canonical domain structure\ninto boxes and only activates boxes whose image is likely intersected by a\nplane. The active boxes are organized into a forest whose trees are traversed\ndepth first to generate mapped micro-structure only of the active boxes. The\ntraversal supports, for example, 3D print slice generation in additive\nmanufacturing.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.06998v1"
    },
    {
        "title": "MeshODE: A Robust and Scalable Framework for Mesh Deformation",
        "authors": [
            "Jingwei Huang",
            "Chiyu Max Jiang",
            "Baiqiang Leng",
            "Bin Wang",
            "Leonidas Guibas"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present MeshODE, a scalable and robust framework for pairwise CAD model\ndeformation without prespecified correspondences. Given a pair of shapes, our\nframework provides a novel shape feature-preserving mapping function that\ncontinuously deforms one model to the other by minimizing fitting and rigidity\nlosses based on the non-rigid iterative-closest-point (ICP) algorithm. We\naddress two challenges in this problem, namely the design of a powerful\ndeformation function and obtaining a feature-preserving CAD deformation. While\ntraditional deformation directly optimizes for the coordinates of the mesh\nvertices or the vertices of a control cage, we introduce a deep bijective\nmapping that utilizes a flow model parameterized as a neural network. Our\nfunction has the capacity to handle complex deformations, produces deformations\nthat are guaranteed free of self-intersections, and requires low rigidity\nconstraining for geometry preservation, which leads to a better fitting quality\ncompared with existing methods. It additionally enables continuous deformation\nbetween two arbitrary shapes without supervision for intermediate shapes.\nFurthermore, we propose a robust preprocessing pipeline for raw CAD meshes\nusing feature-aware subdivision and a uniform graph template representation to\naddress artifacts in raw CAD models including self-intersections, irregular\ntriangles, topologically disconnected components, non-manifold edges, and\nnonuniformly distributed vertices. This facilitates a fast deformation\noptimization process that preserves global and local details. Our code is\npublicly available.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.11617v1"
    },
    {
        "title": "ManifoldPlus: A Robust and Scalable Watertight Manifold Surface\n  Generation Method for Triangle Soups",
        "authors": [
            "Jingwei Huang",
            "Yichao Zhou",
            "Leonidas Guibas"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present ManifoldPlus, a method for robust and scalable conversion of\ntriangle soups to watertight manifolds. While many algorithms in computer\ngraphics require the input mesh to be a watertight manifold, in practice many\nmeshes designed by artists are often for visualization purposes, and thus have\nnon-manifold structures such as incorrect connectivity, ambiguous face\norientation, double surfaces, open boundaries, self-intersections, etc.\nExisting methods suffer from problems in the inputs with face orientation and\nzero-volume structures. Additionally most methods do not scale to meshes of\nhigh complexity. In this paper, we propose a method that extracts exterior\nfaces between occupied voxels and empty voxels, and uses a projection-based\noptimization method to accurately recover a watertight manifold that resembles\nthe reference mesh. Compared to previous methods, our methodology is simpler.\nIt does not rely on face normals of the input triangle soups and can accurately\nrecover zero-volume structures. Our algorithm is scalable, because it employs\nan adaptive Gauss-Seidel method for shape optimization, in which each step is\nan easy-to-solve convex problem. We test ManifoldPlus on ModelNet10 and\nAccuCity datasets to verify that our methods can generate watertight meshes\nranging from object-level shapes to city-level models. Furthermore, through our\nexperimental evaluations, we show that our method is more robust, efficient and\naccurate than the state-of-the-art. Our implementation is publicly available.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.11621v1"
    },
    {
        "title": "Haptic Rendering of Thin, Deformable Objects with Spatially Varying\n  Stiffness",
        "authors": [
            "Priyadarshini Kumari",
            "Subhasis Chaudhuri"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In the real world, we often come across soft objects having spatially varying\nstiffness, such as human palm or a wart on the skin. In this paper, we propose\na novel approach to render thin, deformable objects having spatially varying\nstiffness (inhomogeneous material). We use the classical Kirchhoff thin plate\ntheory to compute the deformation. In general, the physics-based rendering of\nan arbitrary 3D surface is complex and time-consuming. Therefore, we\napproximate the 3D surface locally by a 2D plane using an area-preserving\nmapping technique - Gall-Peters mapping. Once the deformation is computed by\nsolving a fourth-order partial differential equation, we project the points\nback onto the original object for proper haptic rendering. The method was\nvalidated through user experiments and was found to be realistic.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.11799v2"
    },
    {
        "title": "HiVision: Rapid Visualization of Large-Scale Spatial Vector Data",
        "authors": [
            "Mengyu Ma",
            "Ye Wu",
            "Xue Ouyang",
            "Luo Chen",
            "Jun Li",
            "Ning Jing"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Rapid visualization of large-scale spatial vector data is a long-standing\nchallenge in Geographic Information Science. In existing methods, the\ncomputation overheads grow rapidly with data volumes, leading to the\nincapability of providing real-time visualization for large-scale spatial\nvector data, even with parallel acceleration technologies. To fill the gap, we\npresent HiVision, a display-driven visualization model for large-scale spatial\nvector data. Different from traditional data-driven methods, the computing\nunits in HiVision are pixels rather than spatial objects to achieve real-time\nperformance, and efficient spatial-index-based strategies are introduced to\nestimate the topological relationships between pixels and spatial objects.\nHiVision can maintain exceedingly good performance regardless of the data\nvolume due to the stable pixel number for display. In addition, an optimized\nparallel computing architecture is proposed in HiVision to ensure the ability\nof real-time visualization. Experiments show that our approach outperforms\ntraditional methods in rendering speed and visual effects while dealing with\nlarge-scale spatial vector data, and can provide interactive visualization of\ndatasets with billion-scale points/segments/edges in real-time with flexible\nrendering styles. The HiVision code is open-sourced at\nhttps://github.com/MemoryMmy/HiVision with an online demonstration.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.12489v2"
    },
    {
        "title": "Survey: Machine Learning in Production Rendering",
        "authors": [
            "Shilin Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In the past few years, machine learning-based approaches have had some great\nsuccess for rendering animated feature films. This survey summarizes several of\nthe most dramatic improvements in using deep neural networks over traditional\nrendering methods, such as better image quality and lower computational\noverhead. More specifically, this survey covers the fundamental principles of\nmachine learning and its applications, such as denoising, path guiding,\nrendering participating media, and other notoriously difficult light transport\nsituations. Some of these techniques have already been used in the latest\nreleased animations while others are still in the continuing development by\nresearchers in both academia and movie studios. Although learning-based\nrendering methods still have some open issues, they have already demonstrated\npromising performance in multiple parts of the rendering pipeline, and people\nare continuously making new attempts.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.12518v1"
    },
    {
        "title": "Visual Exploration of Simulated and Measured Blood Flow",
        "authors": [
            "Anna Vilanova",
            "Bernhard Preim",
            "Roy van Pelt",
            "Rocco Gasteiger",
            "Mathias Neugebauer",
            "Thomas Wischgoll"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Morphology of cardiovascular tissue is influenced by the unsteady behavior of\nthe blood flow and vice versa. Therefore, the pathogenesis of several\ncardiovascular diseases is directly affected by the blood-flow dynamics.\nUnderstanding flow behavior is of vital importance to understand the\ncardiovascular system and potentially harbors a considerable value for both\ndiagnosis and risk assessment. The analysis of hemodynamic characteristics\ninvolves qualitative and quantitative inspection of the blood-flow field.\nVisualization plays an important role in the qualitative exploration, as well\nas the definition of relevant quantitative measures and its validation. There\nare two main approaches to obtain information about the blood flow: simulation\nby computational fluid dynamics, and in-vivo measurements. Although research on\nblood flow simulation has been performed for decades, many open problems remain\nconcerning accuracy and patient-specific solutions. Possibilities for real\nmeasurement of blood flow have recently increased considerably by new\ndevelopments in magnetic resonance imaging which enable the acquisition of 3D\nquantitative measurements of blood-flow velocity fields. This chapter presents\nthe visualization challenges for both simulation and real measurements of\nunsteady blood-flow fields.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.0999v1"
    },
    {
        "title": "Effect of Eye Dominance on the Perception of Stereoscopic 3D Video",
        "authors": [
            "Amin Banitalebi-Dehkordi",
            "Mahsa T. Pourazad",
            "Panos Nasiopoulos"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Asymmetric schemes have widespread applications in the 3D video transmission\npipeline. The significance of eye dominance becomes a concern when designing\nsuch schemes. In this paper, in order to investigate the effect of eye\ndominance on the perceptual 3D video quality, a database of representative\nasymmetric stereoscopic sequences is prepared and the overall 3D quality of\nthese sequences is evaluated through subjective experiments. Experiment results\nshowed that viewers find an asymmetric video more pleasant when the view with\nhigher quality is projected to their dominant eye. Moreover, the eye dominance\nchanges the mean opinion quality score by 16 % at most, a result caused by\nslight asymmetric video compression. For all other representative types of\nasymmetry, the statistical difference is much lower and in some cases even\nnegligible.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04656v1"
    },
    {
        "title": "P2P-NET: Bidirectional Point Displacement Net for Shape Transform",
        "authors": [
            "Kangxue Yin",
            "Hui Huang",
            "Daniel Cohen-Or",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We introduce P2P-NET, a general-purpose deep neural network which learns\ngeometric transformations between point-based shape representations from two\ndomains, e.g., meso-skeletons and surfaces, partial and complete scans, etc.\nThe architecture of the P2P-NET is that of a bi-directional point displacement\nnetwork, which transforms a source point set to a target point set with the\nsame cardinality, and vice versa, by applying point-wise displacement vectors\nlearned from data. P2P-NET is trained on paired shapes from the source and\ntarget domains, but without relying on point-to-point correspondences between\nthe source and target point sets. The training loss combines two\nuni-directional geometric losses, each enforcing a shape-wise similarity\nbetween the predicted and the target point sets, and a cross-regularization\nterm to encourage consistency between displacement vectors going in opposite\ndirections. We develop and present several different applications enabled by\nour general-purpose bidirectional P2P-NET to highlight the effectiveness,\nversatility, and potential of our network in solving a variety of point-based\nshape transformation problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09263v4"
    },
    {
        "title": "Evolution of natural patterns from random fields",
        "authors": [
            "Lovrenc Švegl",
            "Igor Grabec"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In the article a transition from pattern evolution equation of\nreaction-diffusion type to a cellular automaton (CA) is described. The\napplicability of CA is demonstrated by generating patterns of complex irregular\nstructure on a hexagonal and quadratic lattice. With this aim a random initial\nfield is transformed by a sequence of CA actions into a new pattern. On the\nhexagonal lattice this pattern resembles a lizard skin. The properties of CA\nare specified by the most simple majority rule that adapts selected cell state\nto the most frequent state of cells in its surrounding. The method could be of\ninterest for manufacturing of textiles as well as for modeling of patterns on\nskin of various animals.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.02451v1"
    },
    {
        "title": "Guided Proceduralization: Optimizing Geometry Processing and Grammar\n  Extraction for Architectural Models",
        "authors": [
            "Ilke Demir",
            "Daniel G. Aliaga"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We describe a guided proceduralization framework that optimizes geometry\nprocessing on architectural input models to extract target grammars. We aim to\nprovide efficient artistic workflows by creating procedural representations\nfrom existing 3D models, where the procedural expressiveness is controlled by\nthe user. Architectural reconstruction and modeling tasks have been handled as\neither time consuming manual processes or procedural generation with difficult\ncontrol and artistic influence. We bridge the gap between creation and\ngeneration by converting existing manually modeled architecture to procedurally\neditable parametrized models, and carrying the guidance to procedural domain by\nletting the user define the target procedural representation. Additionally, we\npropose various applications of such procedural representations, including\nguided completion of point cloud models, controllable 3D city modeling, and\nother benefits of procedural modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.02578v1"
    },
    {
        "title": "Non-Rigid Point Set Registration Networks",
        "authors": [
            "Lingjing Wang",
            "Jianchun Chen",
            "Xiang Li",
            "Yi Fang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Point set registration is defined as a process to determine the spatial\ntransformation from the source point set to the target one. Existing methods\noften iteratively search for the optimal geometric transformation to register a\ngiven pair of point sets, driven by minimizing a predefined alignment loss\nfunction. In contrast, the proposed point registration neural network (PR-Net)\nactively learns the registration pattern as a parametric function from a\ntraining dataset, consequently predict the desired geometric transformation to\nalign a pair of point sets. PR-Net can transfer the learned knowledge (i.e.\nregistration pattern) from registering training pairs to testing ones without\nadditional iterative optimization. Specifically, in this paper, we develop\nnovel techniques to learn shape descriptors from point sets that help formulate\na clear correlation between source and target point sets. With the defined\ncorrelation, PR-Net tends to predict the transformation so that the source and\ntarget point sets can be statistically aligned, which in turn leads to an\noptimal spatial geometric registration. PR-Net achieves robust and superior\nperformance for non-rigid registration of point sets, even in presence of\nGaussian noise, outliers, and missing points, but requires much less time for\nregistering large number of pairs. More importantly, for a new pair of point\nsets, PR-Net is able to directly predict the desired transformation using the\nlearned model without repetitive iterative optimization routine. Our code is\navailable at https://github.com/Lingjing324/PR-Net.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.01428v1"
    },
    {
        "title": "Developable surface patches bounded by NURBS curves",
        "authors": [
            "Leonardo Fernandez-Jambrina",
            "Francisco Perez-Arribas"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper we construct developable surface patches which are bounded by\ntwo rational or NURBS curves, though the resulting patch is not a rational or\nNURBS surface in general. This is accomplished by reparameterizing one of the\nboundary curves. The reparameterization function is the solution of an\nalgebraic equation. For the relevant case of cubic or cubic spline curves, this\nequation is quartic at most, quadratic if the curves are Bezier or splines and\nlie on parallel planes, and hence it may be solved either by standard\nanalytical or numerical methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.04603v1"
    },
    {
        "title": "Unwind: Interactive Fish Straightening",
        "authors": [
            "Francis Williams",
            "Alexander Bock",
            "Harish Doraiswamy",
            "Cassandra Donatelli",
            "Kayla Hall",
            "Adam Summers",
            "Daniele Panozzo",
            "Cláudio T. Silva"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The ScanAllFish project is a large-scale effort to scan all the world's\n33,100 known species of fishes. It has already generated thousands of\nvolumetric CT scans of fish species which are available on open access\nplatforms such as the Open Science Framework. To achieve a scanning rate\nrequired for a project of this magnitude, many specimens are grouped together\ninto a single tube and scanned all at once. The resulting data contain many\nfish which are often bent and twisted to fit into the scanner. Our system,\nUnwind, is a novel interactive visualization and processing tool which\nextracts, unbends, and untwists volumetric images of fish with minimal user\ninteraction. Our approach enables scientists to interactively unwarp these\nvolumes to remove the undesired torque and bending using a piecewise-linear\nskeleton extracted by averaging isosurfaces of a harmonic function connecting\nthe head and tail of each fish. The result is a volumetric dataset of a\nindividual, straight fish in a canonical pose defined by the marine biologist\nexpert user. We have developed Unwind in collaboration with a team of marine\nbiologists: Our system has been deployed in their labs, and is presently being\nused for dataset construction, biomechanical analysis, and the generation of\nfigures for scientific publication.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.04890v2"
    },
    {
        "title": "Deep Parametric Shape Predictions using Distance Fields",
        "authors": [
            "Dmitriy Smirnov",
            "Matthew Fisher",
            "Vladimir G. Kim",
            "Richard Zhang",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Many tasks in graphics and vision demand machinery for converting shapes into\nconsistent representations with sparse sets of parameters; these\nrepresentations facilitate rendering, editing, and storage. When the source\ndata is noisy or ambiguous, however, artists and engineers often manually\nconstruct such representations, a tedious and potentially time-consuming\nprocess. While advances in deep learning have been successfully applied to\nnoisy geometric data, the task of generating parametric shapes has so far been\ndifficult for these methods. Hence, we propose a new framework for predicting\nparametric shape primitives using deep learning. We use distance fields to\ntransition between shape parameters like control points and input data on a\npixel grid. We demonstrate efficacy on 2D and 3D tasks, including font\nvectorization and surface abstraction.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.08921v2"
    },
    {
        "title": "OperatorNet: Recovering 3D Shapes From Difference Operators",
        "authors": [
            "Ruqi Huang",
            "Marie-Julie Rakotosaona",
            "Panos Achlioptas",
            "Leonidas Guibas",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper proposes a learning-based framework for reconstructing 3D shapes\nfrom functional operators, compactly encoded as small-sized matrices. To this\nend we introduce a novel neural architecture, called OperatorNet, which takes\nas input a set of linear operators representing a shape and produces its 3D\nembedding. We demonstrate that this approach significantly outperforms previous\npurely geometric methods for the same problem. Furthermore, we introduce a\nnovel functional operator, which encodes the extrinsic or pose-dependent shape\ninformation, and thus complements purely intrinsic pose-oblivious operators,\nsuch as the classical Laplacian. Coupled with this novel operator, our\nreconstruction network achieves very high reconstruction accuracy, even in the\npresence of incomplete information about a shape, given a soft or functional\nmap expressed in a reduced basis. Finally, we demonstrate that the\nmultiplicative functional algebra enjoyed by these operators can be used to\nsynthesize entirely new unseen shapes, in the context of shape interpolation\nand shape analogy applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.10754v2"
    },
    {
        "title": "3D Dynamic Point Cloud Inpainting via Temporal Consistency on Graphs",
        "authors": [
            "Zeqing Fu",
            "Wei Hu",
            "Zongming Guo"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  With the development of 3D laser scanning techniques and depth sensors, 3D\ndynamic point clouds have attracted increasing attention as a representation of\n3D objects in motion, enabling various applications such as 3D immersive\ntele-presence, gaming and navigation. However, dynamic point clouds usually\nexhibit holes of missing data, mainly due to the fast motion, the limitation of\nacquisition and complicated structure. Leveraging on graph signal processing\ntools, we represent irregular point clouds on graphs and propose a novel\ninpainting method exploiting both intra-frame self-similarity and inter-frame\nconsistency in 3D dynamic point clouds. Specifically, for each missing region\nin every frame of the point cloud sequence, we search for its self-similar\nregions in the current frame and corresponding ones in adjacent frames as\nreferences. Then we formulate dynamic point cloud inpainting as an optimization\nproblem based on the two types of references, which is regularized by a\ngraph-signal smoothness prior. Experimental results show the proposed approach\noutperforms three competing methods significantly, both in objective and\nsubjective quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.10795v2"
    },
    {
        "title": "How much do you perceive this? An analysis on perceptions of geometric\n  features, personalities and emotions in virtual humans (Extended Version)",
        "authors": [
            "Victor Araujo",
            "Rodolfo Migon Favaretto",
            "Paulo Knob",
            "Soraia Raupp Musse",
            "Felipe Vilanova",
            "Angelo Brandelli Costa"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This work aims to evaluate people's perception regarding geometric features,\npersonalities and emotions characteristics in virtual humans. For this, we use\nas a basis, a dataset containing the tracking files of pedestrians captured\nfrom spontaneous videos and visualized them as identical virtual humans. The\ngoal is to focus on their behavior and not being distracted by other features.\nIn addition to tracking files containing their positions, the dataset also\ncontains pedestrian emotions and personalities detected using Computer Vision\nand Pattern Recognition techniques. We proceed with our analysis in order to\nanswer the question if subjects can perceive geometric features as\ndistances/speeds as well as emotions and personalities in video sequences when\npedestrians are represented by virtual humans. Regarding the participants, an\namount of 73 people volunteered for the experiment. The analysis was divided in\ntwo parts: i) evaluation on perception of geometric characteristics, such as\ndensity, angular variation, distances and speeds, and ii) evaluation on\npersonality and emotion perceptions. Results indicate that, even without\nexplaining to the participants the concepts of each personality or emotion and\nhow they were calculated (considering geometric characteristics), in most of\nthe cases, participants perceived the personality and emotion expressed by the\nvirtual agents, in accordance with the available ground truth.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11084v1"
    },
    {
        "title": "XR: Enabling training mode in the human brain XR: Enabling training mode\n  in the human brain",
        "authors": [
            "Philippe Lépinard",
            "Sébastien Lozé"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The face of simulation-based training has greatly evolved, with the most\nrecent tools giving the ability to create virtual environments that rival\nrealism. At first glance, it might appear that what the training sector needs\nis the most realistic simulators possible, but traditional simulators are not\nnecessarily the most efficient or practical training tools. With all that these\nnew technologies have to offer; the challenge is to go back to the core of\ntraining needs and identify the right vector of sensory cues that will most\neffectively enable training mode in the human brain. Bigger and Pricier doesn't\nnecessarily mean better. Simulation with cross-reality content (XR), which by\ndefinition encompasses virtual reality (VR), mixed reality (MR), and augmented\nreality (AR), is the most practical solution for deploying any kind of\nsimulation-based training. The authors of this paper (a teacher and a\ntechnology expert) share their experiences and expose XR-specific best\npractices to maximize learning transfer. ABOUT THE AUTHORS Sebastien Loze :\nStarting his career in the modeling and simulation community more than 15 years\nago, S{\\'e}bastien has focused on learning about the latest simulation\ninnovations and sharing information on how experts have solved their\nchallenges. He worked on the COTS integration at CAE and the Presagis focusing\non Simulation and Visualization products. More recently, Sebastien put together\nsimulation and training teams and strategies for emerging companies like CM\nLabs and D-BOX. He is now the Simulations Industry Manager at Epic Games,\nfocusing on helping companies develop real-time solutions for simulation-based\ntraining. Philippe Lepinard: Former military helicopter pilot and simulation\nofficer, Philippe L{\\'e}pinard is now an associate professor at the University\nof Paris-Est Cr{\\'e}teil (UPEC). His research is focusing on playful learning\nand training through simulation. He is one of the founding members of the\nFrench simulation association.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11704v1"
    },
    {
        "title": "Differentiable Visual Computing",
        "authors": [
            "Tzu-Mao Li"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Derivatives of computer graphics, image processing, and deep learning\nalgorithms have tremendous use in guiding parameter space searches, or solving\ninverse problems. As the algorithms become more sophisticated, we no longer\nonly need to differentiate simple mathematical functions, but have to deal with\ngeneral programs which encode complex transformations of data. This\ndissertation introduces three tools for addressing the challenges that arise\nwhen obtaining and applying the derivatives for complex graphics algorithms.\n  Traditionally, practitioners have been constrained to composing programs with\na limited set of operators, or hand-deriving derivatives. We extend the image\nprocessing language Halide with reverse-mode automatic differentiation, and the\nability to automatically optimize the gradient computations. This enables\nautomatic generation of the gradients of arbitrary Halide programs, at high\nperformance, with little programmer effort.\n  In 3D rendering, the gradient is required with respect to variables such as\ncamera parameters, geometry, and appearance. However, computing the gradient is\nchallenging because the rendering integral includes visibility terms that are\nnot differentiable. We introduce, to our knowledge, the first general-purpose\ndifferentiable ray tracer that solves the full rendering equation, while\ncorrectly taking the geometric discontinuities into account.\n  Finally, we demonstrate that the derivatives of light path throughput can\nalso be useful for guiding sampling in forward rendering. Simulating light\ntransport in the presence of multi-bounce glossy effects and motion in 3D\nrendering is challenging due to the hard-to-sample high-contribution areas. We\npresent a Markov Chain Monte Carlo rendering algorithm that extends Metropolis\nLight Transport by automatically and explicitly adapting to the local\nintegrand, thereby increasing sampling efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.12228v2"
    },
    {
        "title": "Structurally optimized shells",
        "authors": [
            "Francisca Gil-Ureta",
            "Nico Pietroni",
            "Denis Zorin"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Shells, i.e., objects made of a thin layer of material following a surface,\nare among the most common structures in use. They are highly efficient, in\nterms of material required to maintain strength, but also prone to deformation\nand failure. We introduce an efficient method for reinforcing shells, that is,\nadding material to the shell to increase its resilience to external loads. Our\ngoal is to produce a reinforcement structure of minimal weight. It has been\ndemonstrated that optimal reinforcement structures may be qualitatively\ndifferent, depending on external loads and surface shape. In some cases, it\nnaturally consists of discrete protruding ribs; in other cases, a smooth shell\nthickness variation allows to save more material.\n  Most previously proposed solutions, starting from classical Michell trusses,\nare not able to handle a full range of shells (e.g., are restricted to\nself-supporting structures) or are unable to reproduce this range of behaviors,\nresulting in suboptimal structures.\n  We propose a new method that works for any input surface with any load\nconfigurations, taking into account both in-plane (tensile/compression) and\nout-of-plane (bending) forces. By using a more precise volume model, we are\ncapable of producing optimized structures with the full range of qualitative\nbehaviors. Our method includes new algorithms for determining the layout of\nreinforcement structure elements, and an efficient algorithm to optimize their\nshape, minimizing a non-linear non-convex functional at a fraction of the cost\nand with better optimality compared to standard solvers.\n  We demonstrate the optimization results for a variety of shapes, and the\nimprovements it yields in the strength of 3D-printed objects.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.12240v1"
    },
    {
        "title": "Software-Enhanced Teaching and Visualization Capabilities of an\n  Ultra-High-Resolution Video Wall",
        "authors": [
            "Ramses van Zon",
            "Marcelo Ponce"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper presents a modular approach to enhance the capabilities and\nfeatures of a visualization and teaching room using software. This approach was\napplied to a room with a large, high resolution (7680$\\times$4320 pixels),\ntiled screen of 13 $\\times$ 7.5 feet as its main display, and with a variety of\naudio and video inputs, connected over a network. Many of the techniques\ndescribed are possible because of a software-enhanced setup, utilizing existing\nhardware and a collection of mostly open-source tools, allowing to perform\ncollaborative, high-resolution visualizations as well as broadcasting and\nrecording workshops and lectures. The software approach is flexible and allows\none to add functionality without changing the hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.00056v1"
    },
    {
        "title": "Design by Immersion: A Transdisciplinary Approach to Problem-Driven\n  Visualizations",
        "authors": [
            "Kyle Wm. Hall",
            "Adam J. Bradley",
            "Uta Hinrichs",
            "Samuel Huron",
            "Jo Wood",
            "Christopher Collins",
            "Sheelagh Carpendale"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  While previous work exists on how to conduct and disseminate insights from\nproblem-driven visualization projects and design studies, the literature does\nnot address how to accomplish these goals in transdisciplinary teams in ways\nthat advance all disciplines involved. In this paper we introduce and define a\nnew methodological paradigm we call design by immersion, which provides an\nalternative perspective on problem-driven visualization work. Design by\nimmersion embeds transdisciplinary experiences at the center of the\nvisualization process by having visualization researchers participate in the\nwork of the target domain (or domain experts participate in visualization\nresearch). Based on our own combined experiences of working on\ncross-disciplinary, problem-driven visualization projects, we present six case\nstudies that expose the opportunities that design by immersion enables,\nincluding (1) exploring new domain-inspired visualization design spaces, (2)\nenriching domain understanding through personal experiences, and (3) building\nstrong transdisciplinary relationships. Furthermore, we illustrate how the\nprocess of design by immersion opens up a diverse set of design activities that\ncan be combined in different ways depending on the type of collaboration,\nproject, and goals. Finally, we discuss the challenges and potential pitfalls\nof design by immersion.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.00559v2"
    },
    {
        "title": "Evaluating Ordering Strategies of Star Glyph Axes",
        "authors": [
            "Matthias Miller",
            "Xuan Zhang",
            "Johannes Fuchs",
            "Michael Blumenschein"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Star glyphs are a well-researched visualization technique to represent\nmulti-dimensional data. They are often used in small multiple settings for a\nvisual comparison of many data points. However, their overall visual appearance\nis strongly influenced by the ordering of dimensions. To this end, two\northogonal categories of layout strategies are proposed in the literature:\norder dimensions by similarity to get homogeneously shaped glyphs vs. order by\ndissimilarity to emphasize spikes and salient shapes. While there is evidence\nthat salient shapes support clustering tasks, evaluation, and direct comparison\nof data-driven ordering strategies has not received much research attention. We\ncontribute an empirical user study to evaluate the efficiency, effectiveness,\nand user confidence in visual clustering tasks using star glyphs. In comparison\nto similarity-based ordering, our results indicate that dissimilarity-based\nstar glyph layouts support users better in clustering tasks, especially when\nclutter is present.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.00576v1"
    },
    {
        "title": "Another Simple but Faster Method for 2D Line Clipping",
        "authors": [
            "Dimitrios Matthes",
            "Vasileios Drakopoulos"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The majority of methods for line clipping make a rather large number of\ncomparisons and involve a lot of calculations compared to modern ones. Most of\nthe times, they are not so efficient as well as not so simple and applicable to\nthe majority of cases. Besides the most popular ones, namely, Cohen-Sutherland,\nLiang-Barsky, Cyrus-Beck and Nicholl-Lee-Nicholl, other line-clipping methods\nhave been presented over the years, each one having its own advantages and\ndisadvantages. In this paper a new computation method for 2D line clipping\nagainst a rectangular window is introduced. The proposed method has been\ncompared with the afore-mentioned ones as well as with two others; namely,\nSkala and Kodituwakku-Wijeweera-Chamikara, with respect to the number of\noperations performed and the computation time. The performance of the proposed\nmethod has been found to be better than all of the above-mentioned methods and\nit is found to be very fast, simple and can be implemented easily in any\nprogramming language or integrated development environment.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.01350v1"
    },
    {
        "title": "Rendering Non-Euclidean Geometry in Real-Time Using Spherical and\n  Hyperbolic Trigonometry",
        "authors": [
            "Daniil Osudin",
            "Christopher Child",
            "Yang-Hui He"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper introduces a method of calculating and rendering shapes in a\nnon-Euclidean 2D space. In order to achieve this, we developed a physics and\ngraphics engine that uses hyperbolic trigonometry to calculate and subsequently\nrender the shapes in a 2D space of constant negative or positive curvature in\nreal-time. We have chosen to use polar coordinates to record the parameters of\nthe objects as well as an azimuthal equidistant projection to render the space\nonto the screen because of the multiple useful properties they have. For\nexample, polar coordinate system works well with trigonometric calculations,\ndue to the distance from the reference point (analogous to origin in Cartesian\ncoordinates) being one of the coordinates by definition. Azimuthal equidistant\nprojection is not a typical projection, used for neither spherical nor\nhyperbolic space, however one of the main features of our engine relies on it:\nchanging the curvature of the world in real-time without stopping the execution\nof the application in order to re-calculate the world. This is due to the\nprojection properties that work identically for both spherical and hyperbolic\nspace, as can be seen in the Figure 1 above. We will also be looking at the\ncomplexity analysis of this method as well as renderings that the engine\nproduces. Finally we will be discussing the limitations and possible\napplications of the created engine as well as potential improvements of the\ndescribed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.01742v1"
    },
    {
        "title": "Point Cloud Super Resolution with Adversarial Residual Graph Networks",
        "authors": [
            "Huikai Wu",
            "Junge Zhang",
            "Kaiqi Huang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Point cloud super-resolution is a fundamental problem for 3D reconstruction\nand 3D data understanding. It takes a low-resolution (LR) point cloud as input\nand generates a high-resolution (HR) point cloud with rich details. In this\npaper, we present a data-driven method for point cloud super-resolution based\non graph networks and adversarial losses. The key idea of the proposed network\nis to exploit the local similarity of point cloud and the analogy between LR\ninput and HR output. For the former, we design a deep network with graph\nconvolution. For the latter, we propose to add residual connections into graph\nconvolution and introduce a skip connection between input and output. The\nproposed network is trained with a novel loss function, which combines Chamfer\nDistance (CD) and graph adversarial loss. Such a loss function captures the\ncharacteristics of HR point cloud automatically without manual design. We\nconduct a series of experiments to evaluate our method and validate the\nsuperiority over other methods. Results show that the proposed method achieves\nthe state-of-the-art performance and have a good generalization ability to\nunseen data.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02111v1"
    },
    {
        "title": "Relighting Humans: Occlusion-Aware Inverse Rendering for Full-Body Human\n  Images",
        "authors": [
            "Yoshihiro Kanamori",
            "Yuki Endo"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Relighting of human images has various applications in image synthesis. For\nrelighting, we must infer albedo, shape, and illumination from a human\nportrait. Previous techniques rely on human faces for this inference, based on\nspherical harmonics (SH) lighting. However, because they often ignore light\nocclusion, inferred shapes are biased and relit images are unnaturally bright\nparticularly at hollowed regions such as armpits, crotches, or garment\nwrinkles. This paper introduces the first attempt to infer light occlusion in\nthe SH formulation directly. Based on supervised learning using convolutional\nneural networks (CNNs), we infer not only an albedo map, illumination but also\na light transport map that encodes occlusion as nine SH coefficients per pixel.\nThe main difficulty in this inference is the lack of training datasets compared\nto unlimited variations of human portraits. Surprisingly, geometric information\nincluding occlusion can be inferred plausibly even with a small dataset of\nsynthesized human figures, by carefully preparing the dataset so that the CNNs\ncan exploit the data coherency. Our method accomplishes more realistic\nrelighting than the occlusion-ignored formulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02714v1"
    },
    {
        "title": "SDM-NET: Deep Generative Network for Structured Deformable Mesh",
        "authors": [
            "Lin Gao",
            "Jie Yang",
            "Tong Wu",
            "Yu-Jie Yuan",
            "Hongbo Fu",
            "Yu-Kun Lai",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We introduce SDM-NET, a deep generative neural network which produces\nstructured deformable meshes. Specifically, the network is trained to generate\na spatial arrangement of closed, deformable mesh parts, which respect the\nglobal part structure of a shape collection, e.g., chairs, airplanes, etc. Our\nkey observation is that while the overall structure of a 3D shape can be\ncomplex, the shape can usually be decomposed into a set of parts, each\nhomeomorphic to a box, and the finer-scale geometry of the part can be\nrecovered by deforming the box. The architecture of SDM-NET is that of a\ntwo-level variational autoencoder (VAE). At the part level, a PartVAE learns a\ndeformable model of part geometries. At the structural level, we train a\nStructured Parts VAE (SP-VAE), which jointly learns the part structure of a\nshape collection and the part geometries, ensuring a coherence between global\nshape structure and surface details. Through extensive experiments and\ncomparisons with the state-of-the-art deep generative models of shapes, we\ndemonstrate the superiority of SDM-NET in generating meshes with visual\nquality, flexible topology, and meaningful structures, which benefit shape\ninterpolation and other subsequently modeling tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.04520v2"
    },
    {
        "title": "Spatio-temporal Manifold Learning for Human Motions via Long-horizon\n  Modeling",
        "authors": [
            "He Wang",
            "Edmond S. L. Ho",
            "Hubert P. H. Shum",
            "Zhanxing Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Data-driven modeling of human motions is ubiquitous in computer graphics and\ncomputer vision applications, such as synthesizing realistic motions or\nrecognizing actions. Recent research has shown that such problems can be\napproached by learning a natural motion manifold using deep learning to address\nthe shortcomings of traditional data-driven approaches. However, previous\nmethods can be sub-optimal for two reasons. First, the skeletal information has\nnot been fully utilized for feature extraction. Unlike images, it is difficult\nto define spatial proximity in skeletal motions in the way that deep networks\ncan be applied. Second, motion is time-series data with strong multi-modal\ntemporal correlations. A frame could be followed by several candidate frames\nleading to different motions; long-range dependencies exist where a number of\nframes in the beginning correlate to a number of frames later. Ineffective\nmodeling would either under-estimate the multi-modality and variance, resulting\nin featureless mean motion or over-estimate them resulting in jittery motions.\nIn this paper, we propose a new deep network to tackle these challenges by\ncreating a natural motion manifold that is versatile for many applications. The\nnetwork has a new spatial component for feature extraction. It is also equipped\nwith a new batch prediction model that predicts a large number of frames at\nonce, such that long-term temporally-based objective functions can be employed\nto correctly learn the motion multi-modality and variances. With our system,\nlong-duration motions can be predicted/synthesized using an open-loop setup\nwhere the motion retains the dynamics accurately. It can also be used for\ndenoising corrupted motions and synthesizing new motions with given control\nsignals. We demonstrate that our system can create superior results comparing\nto existing work in multiple applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.07214v1"
    },
    {
        "title": "Design, Assembly, Calibration, and Measurement of an Augmented Reality\n  Haploscope",
        "authors": [
            "Nate Phillips",
            "Kristen Massey",
            "Mohammed Safayet Arefin",
            "J. Edward Swan II"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  A haploscope is an optical system which produces a carefully controlled\nvirtual image. Since the development of Wheatstone's original stereoscope in\n1838, haploscopes have been used to measure perceptual properties of human\nstereoscopic vision. This paper presents an augmented reality (AR) haploscope,\nwhich allows the viewing of virtual objects superimposed against the real\nworld. Our lab has used generations of this device to make a careful series of\nperceptual measurements of AR phenomena, which have been described in\npublications over the previous 8 years. This paper systematically describes the\ndesign, assembly, calibration, and measurement of our AR haploscope. These\nmethods have been developed and improved in our lab over the past 10 years.\nDespite the fact that 180 years have elapsed since the original report of\nWheatstone's stereoscope, we have not previously found a paper that describes\nthese kinds of details.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.08532v1"
    },
    {
        "title": "Subdivision of point-normal pairs with application to smoothing feasible\n  robot path",
        "authors": [
            "Evgeny Lipovetsky"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In a previous paper [11] we introduced a weighted binary average of two 2D\npoint-normal pairs, termed circle average, and investigated subdivision schemes\nbased on it. These schemes refine point-normal pairs in 2D, and converge to\nlimit curves and limit normals. Such a scheme has the disadvantage that the\nlimit normals are not the normals of the limit curve. In this paper we solve\nthis problem by proposing a new averaging method, and obtaining a new family of\nalgorithms based on it. We demonstrate their new editing capabilities and apply\nthis subdivision technique to smooth a precomputed feasible polygonal point\nrobot path.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.09913v3"
    },
    {
        "title": "Animated Stickies: Fast Video Projection Mapping onto a Markerless Plane\n  through a Direct Closed-Loop Alignment",
        "authors": [
            "Shingo Kagami",
            "Koichi Hashimoto"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This paper presents a fast projection mapping method for moving image content\nprojected onto a markerless planar surface using a low-latency Digital\nMicromirror Device (DMD) projector. By adopting a closed-loop alignment\napproach, in which not only the surface texture but also the projected image is\ntracked by a camera, the proposed method is free from a calibration or position\nadjustment between the camera and projector. We designed fiducial patterns to\nbe inserted into a fast flapping sequence of binary frames of the DMD\nprojector, which allows the simultaneous tracking of the surface texture and a\nfiducial geometry separate from a single image captured by the camera. The\nproposed method implemented on a CPU runs at 400 fps and enables arbitrary\nvideo contents to be \"stuck\" onto a variety of textured surfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.00032v1"
    },
    {
        "title": "Scenior: An Immersive Visual Scripting system based on VR Software\n  Design Patterns for Experiential Training",
        "authors": [
            "Paul Zikas",
            "George Papagiannakis",
            "Nick Lydatakis",
            "Steve Kateros",
            "Stavroula Ntoa",
            "Ilia Adami",
            "Constantine Stephanidis"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Virtual reality (VR) has re-emerged as a low-cost, highly accessible consumer\nproduct, and training on simulators is rapidly becoming standard in many\nindustrial sectors. However, the available systems are either focusing on\ngaming context, featuring limited capabilities or they support only content\ncreation of virtual environments without any rapid prototyping and\nmodification. In this project, we propose a code-free, visual scripting\nplatform to replicate gamified training scenarios through rapid prototyping and\nVR software design patterns. We implemented and compared two authoring tools:\na) visual scripting and b) VR editor for the rapid reconstruction of VR\ntraining scenarios. Our visual scripting module is capable to generate training\napplications utilizing a node-based scripting system whereas the VR editor\ngives user/developer the ability to customize and populate new VR training\nscenarios directly from the virtual environment. We also introduce action\nprototypes, a new software design pattern suitable to replicate behavioral\ntasks for VR experiences. In addition, we present the training scenegraph\narchitecture as the main model to represent training scenarios on a modular,\ndynamic and highly adaptive acyclic graph based on a structured educational\ncurriculum. Finally, a user-based evaluation of the proposed solution indicated\nthat users - regardless of their programming expertise - can effectively use\nthe tools to create and modify training scenarios in VR.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.05719v2"
    },
    {
        "title": "A True AR Authoring Tool for Interactive Virtual Museums",
        "authors": [
            "Efstratios Geronikolakis",
            "Paul Zikas",
            "Steve Kateros",
            "Nick Lydatakis",
            "Stelios Georgiou",
            "Mike Kentros",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this work, a new and innovative way of spatial computing that appeared\nrecently in the bibliography called True Augmented Reality (AR), is employed in\ncultural heritage preservation. This innovation could be adapted by the Virtual\nMuseums of the future to enhance the quality of experience. It emphasises, the\nfact that a visitor will not be able to tell, at a first glance, if the\nartefact that he/she is looking at is real or not and it is expected to draw\nthe visitors' interest. True AR is not limited to artefacts but extends even to\nbuildings or life-sized character simulations of statues. It provides the best\nvisual quality possible so that the users will not be able to tell the real\nobjects from the augmented ones. Such applications can be beneficial for future\nmuseums, as with True AR, 3D models of various exhibits, monuments, statues,\ncharacters and buildings can be reconstructed and presented to the visitors in\na realistic and innovative way. We also propose our Virtual Reality Sample\napplication, a True AR playground featuring basic components and tools for\ngenerating interactive Virtual Museum applications, alongside a 3D\nreconstructed character (the priest of Asinou church) facilitating the\nstoryteller of the augmented experience.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.09429v4"
    },
    {
        "title": "Manufacturability Oriented Model Correction and Build Direction\n  Optimization for Additive Manufacturing",
        "authors": [
            "Erva Ulu",
            "Nurcan Gecer Ulu",
            "Walter Hsiao",
            "Saigopal Nelaturi"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We introduce a method to analyze and modify a shape to make it manufacturable\nfor a given additive manufacturing (AM) process. Different AM technologies,\nprocess parameters or materials introduce geometric constraints on what is\nmanufacturable or not. Given an input 3D model and minimum printable feature\nsize dictated by the manufacturing process characteristics and parameters, our\nalgorithm generates a corrected geometry that is printable with the intended AM\nprocess. A key issue in model correction for manufacturability is the\nidentification of critical features that are affected by the printing process.\nTo address this challenge, we propose a topology aware approach to construct\nthe allowable space for a print head to traverse during the 3D printing\nprocess. Combined with our build orientation optimization algorithm, the amount\nof modifications performed on the shape is kept at minimum while providing an\naccurate approximation of the as-manufactured part. We demonstrate our method\non a variety of 3D models and validate it by 3D printing the results.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.11620v1"
    },
    {
        "title": "Photorealistic Material Editing Through Direct Image Manipulation",
        "authors": [
            "Károly Zsolnai-Fehér",
            "Peter Wonka",
            "Michael Wimmer"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Creating photorealistic materials for light transport algorithms requires\ncarefully fine-tuning a set of material properties to achieve a desired\nartistic effect. This is typically a lengthy process that involves a trained\nartist with specialized knowledge. In this work, we present a technique that\naims to empower novice and intermediate-level users to synthesize high-quality\nphotorealistic materials by only requiring basic image processing knowledge. In\nthe proposed workflow, the user starts with an input image and applies a few\nintuitive transforms (e.g., colorization, image inpainting) within a 2D image\neditor of their choice, and in the next step, our technique produces a\nphotorealistic result that approximates this target image. Our method combines\nthe advantages of a neural network-augmented optimizer and an encoder neural\nnetwork to produce high-quality output results within 30 seconds. We also\ndemonstrate that it is resilient against poorly-edited target images and\npropose a simple extension to predict image sequences with a strict time budget\nof 1-2 seconds per image.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.11622v1"
    },
    {
        "title": "Color continuity along the journey from ideas to objects",
        "authors": [
            "Jan Morovic"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Human endeavor has involved making choices about color and looking for ways\nto color objects since the dawn of civilization. While it has been the\nexclusive domain of artists and craftspeople for millennia, the last century\nhas seen the introduction of a scientific basis to color communication. The\nultimate goal of this development is for color communication to happen\nseamlessly and in a transparent way. There are however two categories of\nchallenges here: first, understanding and quantifying color needs and\nexpectation and second, developing control mechanisms that deliver the desired\ncolor. In this paper a review will be presented of the color needs in\nend-to-end color journeys, from initial concept to final colored object and an\noverview of recent developments in color printing will follow. Topics like\nimaging pipelines (including the recently-introduced HP Pixel Control), the\nease of use of color workflows (including HP Smart Color Tools), the handling\nof brand or corporate identity colors (via HP Professional PANTONE Emulation)\nand the measurement of color difference under specific viewing arrangements\n(i.e., the dENS metric for viewing samples without separation) will be\naddressed. Finally, a series of challenges for the future will be set out, so\nthat their solution can be approached by both academic and industrial\ncommunities.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.12583v1"
    },
    {
        "title": "Learning-based Real-time Detection of Intrinsic Reflectional Symmetry",
        "authors": [
            "Yi-Ling Qiao",
            "Lin Gao",
            "Shu-Zhi Liu",
            "Ligang Liu",
            "Yu-Kun Lai",
            "Xilin Chen"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Reflectional symmetry is ubiquitous in nature. While extrinsic reflectional\nsymmetry can be easily parametrized and detected, intrinsic symmetry is much\nharder due to the high solution space. Previous works usually solve this\nproblem by voting or sampling, which suffer from high computational cost and\nrandomness. In this paper, we propose \\YL{a} learning-based approach to\nintrinsic reflectional symmetry detection. Instead of directly finding\nsymmetric point pairs, we parametrize this self-isometry using a functional map\nmatrix, which can be easily computed given the signs of Laplacian\neigenfunctions under the symmetric mapping. Therefore, we train a novel deep\nneural network to predict the sign of each eigenfunction under symmetry, which\nin addition takes the first few eigenfunctions as intrinsic features to\ncharacterize the mesh while avoiding coping with the connectivity explicitly.\nOur network aims at learning the global property of functions, and consequently\nconverts the problem defined on the manifold to the functional domain. By\ndisentangling the prediction of the matrix into separated basis, our method\ngeneralizes well to new shapes and is invariant under perturbation of\neigenfunctions. Through extensive experiments, we demonstrate the robustness of\nour method in challenging cases, including different topology and incomplete\nshapes with holes. By avoiding random sampling, our learning-based algorithm is\nover 100 times faster than state-of-the-art methods, and meanwhile, is more\nrobust, achieving higher correspondence accuracy in commonly used metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00189v1"
    },
    {
        "title": "Personality-Aware Probabilistic Map for Trajectory Prediction of\n  Pedestrians",
        "authors": [
            "Chaochao Li",
            "Pei Lv",
            "Mingliang Xu",
            "Xinyu Wang",
            "Dinesh Manocha",
            "Bing Zhou",
            "Meng Wang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a novel trajectory prediction algorithm for pedestrians based on a\npersonality-aware probabilistic feature map. This map is computed using a\nspatial query structure and each value represents the probability of the\npredicted pedestrian passing through various positions in the crowd space. We\nupdate this map dynamically based on the agents in the environment and prior\ntrajectory of a pedestrian. Furthermore, we estimate the personality\ncharacteristics of each pedestrian and use them to improve the prediction by\nestimating the shortest path in this map. Our approach is general and works\nwell on crowd videos with low and high pedestrian density. We evaluate our\nmodel on standard human-trajectory datasets. In practice, our prediction\nalgorithm improves the accuracy by 5-9% over prior algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00193v1"
    },
    {
        "title": "Enhancing User Experience in Virtual Reality with Radial Basis Function\n  Interpolation Based Stereoscopic Camera Control",
        "authors": [
            "Emre Avan",
            "Ufuk Celikcan",
            "Tolga K. Capin",
            "Hasmet Gurcay"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Providing a depth-rich Virtual Reality (VR) experience to users without\ncausing discomfort remains to be a challenge with today's commercially\navailable head-mounted displays (HMDs), which enforce strict measures on\nstereoscopic camera parameters for the sake of keeping visual discomfort to a\nminimum. However, these measures often lead to an unimpressive VR experience\nwith shallow depth feeling. We propose the first method ready to be used with\nexisting consumer HMDs for automated stereoscopic camera control in virtual\nenvironments (VEs). Using radial basis function interpolation and projection\nmatrix manipulations, our method makes it possible to significantly enhance\nuser experience in terms of overall perceived depth while maintaining visual\ndiscomfort on a par with the default arrangement. In our implementation, we\nalso introduce the first immersive interface for authoring a unique 3D\nstereoscopic cinematography for any VE to be experienced with consumer HMDs. We\nconducted a user study that demonstrates the benefits of our approach in terms\nof superior picture quality and perceived depth. We also investigated the\neffects of using depth of field (DoF) in combination with our approach and\nobserved that the addition of our DoF implementation was seen as a degraded\nexperience, if not similar.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.04446v1"
    },
    {
        "title": "DR-KFS: A Differentiable Visual Similarity Metric for 3D Shape\n  Reconstruction",
        "authors": [
            "Jiongchao Jin",
            "Akshay Gadi Patil",
            "Zhang Xiong",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We introduce a differential visual similarity metric to train deep neural\nnetworks for 3D reconstruction, aimed at improving reconstruction quality. The\nmetric compares two 3D shapes by measuring distances between multi-view images\ndifferentiably rendered from the shapes. Importantly, the image-space distance\nis also differentiable and measures visual similarity, rather than pixel-wise\ndistortion. Specifically, the similarity is defined by mean-squared errors over\nHardNet features computed from probabilistic keypoint maps of the compared\nimages. Our differential visual shape similarity metric can be easily plugged\ninto various 3D reconstruction networks, replacing their distortion-based\nlosses, such as Chamfer or Earth Mover distances, so as to optimize the network\nweights to produce reconstructions with better structural fidelity and visual\nquality. We demonstrate this both objectively, using well-known shape metrics\nfor retrieval and classification tasks that are independent from our new\nmetric, and subjectively through a perceptual study.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.09204v4"
    },
    {
        "title": "Virtual Lenses as Embodied Tools for Immersive Analytics",
        "authors": [
            "Sven Kluge",
            "Stefan Gladisch",
            "Uwe Freiherr von Lukas",
            "Oliver Staadt",
            "Christian Tominski"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Interactive lenses are useful tools for supporting the analysis of data in\ndifferent ways. Most existing lenses are designed for 2D visualization and are\noperated using standard mouse and keyboard interaction. On the other hand,\nresearch on virtual lenses for novel 3D immersive visualization environments is\nscarce. Our work aims to narrow this gap in the literature. We focus\nparticularly on the interaction with lenses. Inspired by natural interaction\nwith magnifying glasses in the real world, our lenses are designed as graspable\ntools that can be created and removed as needed, manipulated and parameterized\ndepending on the task, and even combined to flexibly create new views on the\ndata. We implemented our ideas in a system for the visual analysis of 3D sonar\ndata. Informal user feedback from more than a hundred people suggests that the\ndesigned lens interaction is easy to use for the task of finding a hidden wreck\nin sonar data.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.10044v1"
    },
    {
        "title": "Deep Generation of Face Images from Sketches",
        "authors": [
            "Shu-Yu Chen",
            "Wanchao Su",
            "Lin Gao",
            "Shihong Xia",
            "Hongbo Fu"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Recent deep image-to-image translation techniques allow fast generation of\nface images from freehand sketches. However, existing solutions tend to overfit\nto sketches, thus requiring professional sketches or even edge maps as input.\nTo address this issue, our key idea is to implicitly model the shape space of\nplausible face images and synthesize a face image in this space to approximate\nan input sketch. We take a local-to-global approach. We first learn feature\nembeddings of key face components, and push corresponding parts of input\nsketches towards underlying component manifolds defined by the feature vectors\nof face component samples. We also propose another deep neural network to learn\nthe mapping from the embedded component features to realistic images with\nmulti-channel feature maps as intermediate results to improve the information\nflow. Our method essentially uses input sketches as soft constraints and is\nthus able to produce high-quality face images even from rough and/or incomplete\nsketches. Our tool is easy to use even for non-artists, while still supporting\nfine-grained control of shape details. Both qualitative and quantitative\nevaluations show the superior generation ability of our system to existing and\nalternative solutions. The usability and expressiveness of our system are\nconfirmed by a user study.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.01047v2"
    },
    {
        "title": "Accurate Face Rig Approximation with Deep Differential Subspace\n  Reconstruction",
        "authors": [
            "Steven L. Song",
            "Weiqi Shi",
            "Michael Reed"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  To be suitable for film-quality animation, rigs for character deformation\nmust fulfill a broad set of requirements. They must be able to create highly\nstylized deformation, allow a wide variety of controls to permit artistic\nfreedom, and accurately reflect the design intent. Facial deformation is\nespecially challenging due to its nonlinearity with respect to the animation\ncontrols and its additional precision requirements, which often leads to highly\ncomplex face rigs that are not generalizable to other characters. This lack of\ngenerality creates a need for approximation methods that encode the deformation\nin simpler structures. We propose a rig approximation method that addresses\nthese issues by learning localized shape information in differential\ncoordinates and, separately, a subspace for mesh reconstruction. The use of\ndifferential coordinates produces a smooth distribution of errors in the\nresulting deformed surface, while the learned subspace provides constraints\nthat reduce the low frequency error in the reconstruction. Our method can\nreconstruct both face and body deformations with high fidelity and does not\nrequire a set of well-posed animation examples, as we demonstrate with a\nvariety of production characters.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.01746v2"
    },
    {
        "title": "RBF Solver for Quaternions Interpolation",
        "authors": [
            "Rinaldi Fabio",
            "Dolci Daniele"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper we adapt the RBF Solver to work with quaternions by taking\nadvantage of their Lie Algebra and exponential map. This will allow to work\nwith quaternions as if they were normal vectors in R^3 and blend them in a very\nefficient way.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.04686v1"
    },
    {
        "title": "Computational Design and Evaluation Methods for Empowering Non-Experts\n  in Digital Fabrication",
        "authors": [
            "Nurcan Gecer Ulu"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Despite the increasing availability of personal fabrication hardware and\nservices, the true potential of digital fabrication remains unrealized due to\nlack of computational techniques that can support 3D shape design by\nnon-experts. This work develops computational methods that address two key\naspects of content creation:(1) Function-driven design synthesis, (2) Design\nassessment.\n  For design synthesis, a generative shape modeling algorithm that facilitates\nautomatic geometry synthesis and user-driven modification for non-experts is\nintroduced. A critical observation that arises from this study is that the most\ngeometrical specifications are dictated by functional requirements. To support\ndesign by high-level functional prescriptions, a physics based shape\noptimization method for compliant coupling behavior design has been developed.\nIn line with this idea, producing complex 3D surfaces from flat 2D sheets by\nexploiting the concept of buckling beams has also been explored. Effective\ndesign assessment, the second key aspect, becomes critical for problems in\nwhich computational solutions do not exist. For these problems, this work\nproposes crowdsourcing as a way to empower non-experts in esoteric design\ndomains that traditionally require expertise and specialized knowledge.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.05921v1"
    },
    {
        "title": "Structure and Design of HoloGen",
        "authors": [
            "Peter J. Christopher",
            "Timothy D. Wilkinson"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Increasing popularity of augmented and mixed reality systems has seen a\nsimilar increase of interest in 2D and 3D computer generated holography (CGH).\nUnlike stereoscopic approaches, CGH can fully represent a light field including\ndepth of focus, accommodation and vergence. Along with existing\ntelecommunications, imaging, projection, lithography, beam shaping and optical\ntweezing applications, CGH is an exciting technique applicable to a wide array\nof photonic problems including full 3D representation. Traditionally, the\nprimary roadblock to acceptance has been the significant numerical processing\nrequired to generate holograms requiring both significant expertise and\nsignificant computational power. This article discusses the structure and\ndesign of HoloGen. HoloGen is an MIT licensed application that may be used to\ngenerate holograms using a wide array of algorithms without expert guidance.\nHoloGen uses a Cuda C and C++ backend with a C# and Windows Presentation\nFramework graphical user interface. The article begins by introducing HoloGen\nbefore providing an in-depth discussion of its design and structure. Particular\nfocus is given to the communication, data transfer and algorithmic aspects.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.10509v1"
    },
    {
        "title": "OMiCroN -- Oblique Multipass Hierarchy Creation while Navigating",
        "authors": [
            "Vinícius da Silva",
            "Claudio Esperança",
            "Ricardo Marroquim"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Rendering large point clouds ordinarily requires building a hierarchical data\nstructure for accessing the points that best represent the object for a given\nviewing frustum and level-of-detail. The building of such data structures\nfrequently represents a large portion of the cost of the rendering pipeline\nboth in terms of time and space complexity, especially when rendering is done\nfor inspection purposes only. This problem has been addressed in the past by\nincremental construction approaches, but these either result in low quality\nhierarchies or in longer construction times. In this work we present OMiCroN --\nOblique Multipass Hierarchy Creation while Navigating -- which is the first\nalgorithm capable of immediately displaying partial renders of the geometry,\nprovided the cloud is made available sorted in Morton order. OMiCroN is fast,\nbeing capable of building the entire data structure in memory spending an\namount of time that is comparable to that of just reading the cloud from disk.\nThus, there is no need for storing an expensive hierarchy, nor for delaying the\nrendering until the whole hierarchy is read from disk. In fact, a pipeline\ncoupling OMiCroN with an incremental sorting algorithm running in parallel can\nstart rendering as soon as the first sorted prefix is produced, making this\nsetup very convenient for streamed viewing.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.13266v1"
    },
    {
        "title": "Multiscale Mesh Deformation Component Analysis with Attention-based\n  Autoencoders",
        "authors": [
            "Jie Yang",
            "Lin Gao",
            "Qingyang Tan",
            "Yihua Huang",
            "Shihong Xia",
            "Yu-Kun Lai"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Deformation component analysis is a fundamental problem in geometry\nprocessing and shape understanding. Existing approaches mainly extract\ndeformation components in local regions at a similar scale while deformations\nof real-world objects are usually distributed in a multi-scale manner. In this\npaper, we propose a novel method to exact multiscale deformation components\nautomatically with a stacked attention-based autoencoder. The attention\nmechanism is designed to learn to softly weight multi-scale deformation\ncomponents in active deformation regions, and the stacked attention-based\nautoencoder is learned to represent the deformation components at different\nscales. Quantitative and qualitative evaluations show that our method\noutperforms state-of-the-art methods. Furthermore, with the multiscale\ndeformation components extracted by our method, the user can edit shapes in a\ncoarse-to-fine fashion which facilitates effective modeling of new shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.02459v1"
    },
    {
        "title": "On spherical harmonics possessing octahedral symmetry",
        "authors": [
            "Yuri Nesterenko"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we present the implicit equations for one special class of\nreal-valued spherical harmonics with octahedral symmetry. Based on this\nrepresentation, we construct the rotationally invariant measure of deviation\nfrom the specified symmetry. The spherical harmonics we consider have some\napplications in the area of directional fields design due to their ability to\nrepresent mutually orthogonal axes in 3D space, not relative to their order and\norientation.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.12614v3"
    },
    {
        "title": "Real-time rendering of complex fractals",
        "authors": [
            "Vinícius da Silva",
            "Tiago Novello",
            "Hélio Lopes",
            "Luiz Velho"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This chapter describes how to use intersection and closest-hit shaders to\nimplement real-time visualizations of complex fractals using distance\nfunctions. The Mandelbulb and Julia Sets are used as examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.01747v1"
    },
    {
        "title": "Curvy: An Interactive Design Tool for Varying Density Support Structures",
        "authors": [
            "Erva Ulu",
            "Nurcan Gecer Ulu",
            "Jiahao Li",
            "Walter Hsiao"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We introduce Curvy-an interactive design tool to generate varying density\nsupport structures for 3D printing. Support structures are essential for\nprinting models with extreme overhangs. Yet, they often cause defects on\ncontact areas, resulting in poor surface quality. Low-level design of support\nstructures may alleviate such negative effects. However, it is tedious and\nunintuitive for novice users as it is hard to predict the impact of changes to\nthe support structure on the final printed part. Curvy allows users to define\ntheir high-level preferences on the surface quality directly on the target\nobject rather than explicitly designing the supports. These preferences are\nthen automatically translated into low-level design parameters to generate the\nsupport structure. Underlying novel curvy zigzag toolpathing algorithm uses\nthese instructions to generate varying density supports by altering the spacing\nbetween individual paths in order to achieve prescribed quality. Combined with\nthe build orientation optimization, Curvy provides a practical solution to the\ndesign of support structures with minimal perceptual or functional impact on\nthe target part to be printed.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10013v1"
    },
    {
        "title": "Rendering Discrete Participating Media with Geometrical Optics\n  Approximation",
        "authors": [
            "Jie Guo",
            "Bingyang Hu",
            "Yanjun Chen",
            "Yuanqi Li",
            "Yanwen Guo",
            "Ling-Qi Yan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We consider the scattering of light in participating media composed of\nsparsely and randomly distributed discrete particles. The particle size is\nexpected to range from the scale of the wavelength to the scale several orders\nof magnitude greater than the wavelength, and the appearance shows distinct\ngraininess as opposed to the smooth appearance of continuous media. One\nfundamental issue in physically-based synthesizing this appearance is to\ndetermine necessary optical properties in every local region. Since these\noptical properties vary spatially, we resort to geometrical optics\napproximation (GOA), a highly efficient alternative to rigorous Lorenz-Mie\ntheory, to quantitatively represent the scattering of a single particle. This\nenables us to quickly compute bulk optical properties according to any particle\nsize distribution. Then, we propose a practical Monte Carlo rendering solution\nto solve the transfer of energy in discrete participating media. Results show\nthat for the first time our proposed framework can simulate a wide range of\ndiscrete participating media with different levels of graininess and converges\nto continuous media as the particle concentration increases.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12285v1"
    },
    {
        "title": "Aximorphic Perspective Projection Model for Immersive Imagery",
        "authors": [
            "Jakub Maksymilian Fober"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  A wide choice of cinematic lenses enables motion-picture creators to adapt\nimage visual-appearance to their creative vision. Such choice does not exist in\nthe realm of real-time computer graphics, where only one type of perspective\nprojection is widely used, a linear perspective. This paper presents an\nextended perspective imaging model, which can represent distortion and FoV\nparameters of entire variety of film and photographic lenses (e.g., wide-angle,\nfisheye, anamorphic), while preserving parametrization in an artistically\nconvincing manner. Self-experimentation with the model revealed that each\nprojection type provides accurate perception of a different aspect of depicted\nspace (e.g., speed, distance, shape). Presented model, enables combination of\nmultiple projections, each on a different axis of the image, to achieve optimal\nperception for a given scenario. This new projection, named aximorphic, was\nmade available here, under an open license (CC BY-SA 3.0), for a wide and easy\nadoption.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12682v6"
    },
    {
        "title": "Mixture of Volumetric Primitives for Efficient Neural Rendering",
        "authors": [
            "Stephen Lombardi",
            "Tomas Simon",
            "Gabriel Schwartz",
            "Michael Zollhoefer",
            "Yaser Sheikh",
            "Jason Saragih"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Real-time rendering and animation of humans is a core function in games,\nmovies, and telepresence applications. Existing methods have a number of\ndrawbacks we aim to address with our work. Triangle meshes have difficulty\nmodeling thin structures like hair, volumetric representations like Neural\nVolumes are too low-resolution given a reasonable memory budget, and\nhigh-resolution implicit representations like Neural Radiance Fields are too\nslow for use in real-time applications. We present Mixture of Volumetric\nPrimitives (MVP), a representation for rendering dynamic 3D content that\ncombines the completeness of volumetric representations with the efficiency of\nprimitive-based rendering, e.g., point-based or mesh-based methods. Our\napproach achieves this by leveraging spatially shared computation with a\ndeconvolutional architecture and by minimizing computation in empty regions of\nspace with volumetric primitives that can move to cover only occupied regions.\nOur parameterization supports the integration of correspondence and tracking\nconstraints, while being robust to areas where classical tracking fails, such\nas around thin or translucent structures and areas with large topological\nvariability. MVP is a hybrid that generalizes both volumetric and\nprimitive-based representations. Through a series of extensive experiments we\ndemonstrate that it inherits the strengths of each, while avoiding many of\ntheir limitations. We also compare our approach to several state-of-the-art\nmethods and demonstrate that MVP produces superior results in terms of quality\nand runtime performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01954v2"
    },
    {
        "title": "Learning to Manipulate Amorphous Materials",
        "authors": [
            "Yunbo Zhang",
            "Wenhao Yu",
            "C. Karen Liu",
            "Charles C. Kemp",
            "Greg Turk"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a method of training character manipulation of amorphous materials\nsuch as those often used in cooking. Common examples of amorphous materials\ninclude granular materials (salt, uncooked rice), fluids (honey), and\nvisco-plastic materials (sticky rice, softened butter). A typical task is to\nspread a given material out across a flat surface using a tool such as a\nscraper or knife. We use reinforcement learning to train our controllers to\nmanipulate materials in various ways. The training is performed in a physics\nsimulator that uses position-based dynamics of particles to simulate the\nmaterials to be manipulated. The neural network control policy is given\nobservations of the material (e.g. a low-resolution density map), and the\npolicy outputs actions such as rotating and translating the knife. We\ndemonstrate policies that have been successfully trained to carry out the\nfollowing tasks: spreading, gathering, and flipping. We produce a final\nanimation by using inverse kinematics to guide a character's arm and hand to\nmatch the motion of the manipulation tool such as a knife or a frying pan.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.02533v1"
    },
    {
        "title": "HexDom: Polycube-Based Hexahedral-Dominant Mesh Generation",
        "authors": [
            "Yuxuan Yu",
            "Jialei Ginny Liu",
            "Yongjie Jessica Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this paper, we extend our earlier polycube-based all-hexahedral mesh\ngeneration method to hexahedral-dominant mesh generation, and present the\nHexDom software package. Given the boundary representation of a solid model,\nHexDom creates a hex-dominant mesh by using a semi-automated polycube-based\nmesh generation method. The resulting hexahedral dominant mesh includes\nhexahedra, tetrahedra, and triangular prisms. By adding non-hexahedral\nelements, we are able to generate better quality hexahedral elements than in\nall-hexahedral meshes. We explain the underlying algorithms in four modules\nincluding segmentation, polycube construction, hex-dominant mesh generation and\nquality improvement, and use a rockerarm model to explain how to run the\nsoftware. We also apply our software to a number of other complex models to\ntest their robustness. The software package and all tested models are availabe\nin github (https://github.com/CMU-CBML/HexDom).\n",
        "pdf_link": "http://arxiv.org/pdf/2103.04183v2"
    },
    {
        "title": "Impacts of the Numbers of Colors and Shapes on Outlier Detection: from\n  Automated to User Evaluation",
        "authors": [
            "Loann Giovannangeli",
            "Romain Giot",
            "David Auber",
            "Romain Bourqui"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The design of efficient representations is well established as a fruitful way\nto explore and analyze complex or large data. In these representations, data\nare encoded with various visual attributes depending on the needs of the\nrepresentation itself. To make coherent design choices about visual attributes,\nthe visual search field proposes guidelines based on the human brain perception\nof features. However, information visualization representations frequently need\nto depict more data than the amount these guidelines have been validated on.\nSince, the information visualization community has extended these guidelines to\na wider parameter space.\n  This paper contributes to this theme by extending visual search theories to\nan information visualization context. We consider a visual search task where\nsubjects are asked to find an unknown outlier in a grid of randomly laid out\ndistractor. Stimuli are defined by color and shape features for the purpose of\nvisually encoding categorical data. The experimental protocol is made of a\nparameters space reduction step (i.e., sub-sampling) based on a machine\nlearning model, and a user evaluation to measure capacity limits and validate\nhypotheses. The results show that the major difficulty factor is the number of\nvisual attributes that are used to encode the outlier. When redundantly\nencoded, the display heterogeneity has no effect on the task. When encoded with\none attribute, the difficulty depends on that attribute heterogeneity until its\ncapacity limit (7 for color, 5 for shape) is reached. Finally, when encoded\nwith two attributes simultaneously, performances drop drastically even with\nminor heterogeneity.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.06084v1"
    },
    {
        "title": "Volumetric Procedural Models for Shape Representation",
        "authors": [
            "Andrew Willis",
            "Prashant Ganesh",
            "Kyle Volle",
            "Jincheng Zhang",
            "Kevin Brink"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This article describes a volumetric approach for procedural shape modeling\nand a new Procedural Shape Modeling Language (PSML) that facilitates the\nspecification of these models. PSML provides programmers the ability to\ndescribe shapes in terms of their 3D elements where each element may be a\nsemantic group of 3D objects, e.g., a brick wall, or an indivisible object,\ne.g., an individual brick. Modeling shapes in this manner facilitates the\ncreation of models that more closely approximate the organization and structure\nof their real-world counterparts. As such, users may query these models for\nvolumetric information such as the number, position, orientation and volume of\n3D elements which cannot be provided using surface based model-building\ntechniques. PSML also provides a number of new language-specific capabilities\nthat allow for a rich variety of context-sensitive behaviors and\npost-processing functions. These capabilities include an object-oriented\napproach for model design, methods for querying the model for component-based\ninformation and the ability to access model elements and components to perform\nBoolean operations on the model parts. PSML is open-source and includes freely\navailable tutorial videos, demonstration code and an integrated development\nenvironment to support writing PSML programs.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.11930v1"
    },
    {
        "title": "FoV-NeRF: Foveated Neural Radiance Fields for Virtual Reality",
        "authors": [
            "Nianchen Deng",
            "Zhenyi He",
            "Jiannan Ye",
            "Budmonde Duinkharjav",
            "Praneeth Chakravarthula",
            "Xubo Yang",
            "Qi Sun"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Virtual Reality (VR) is becoming ubiquitous with the rise of consumer\ndisplays and commercial VR platforms. Such displays require low latency and\nhigh quality rendering of synthetic imagery with reduced compute overheads.\nRecent advances in neural rendering showed promise of unlocking new\npossibilities in 3D computer graphics via image-based representations of\nvirtual or physical environments. Specifically, the neural radiance fields\n(NeRF) demonstrated that photo-realistic quality and continuous view changes of\n3D scenes can be achieved without loss of view-dependent effects. While NeRF\ncan significantly benefit rendering for VR applications, it faces unique\nchallenges posed by high field-of-view, high resolution, and\nstereoscopic/egocentric viewing, typically causing low quality and high latency\nof the rendered images. In VR, this not only harms the interaction experience\nbut may also cause sickness. To tackle these problems toward\nsix-degrees-of-freedom, egocentric, and stereo NeRF in VR, we present the first\ngaze-contingent 3D neural representation and view synthesis method. We\nincorporate the human psychophysics of visual- and stereo-acuity into an\negocentric neural representation of 3D scenery. We then jointly optimize the\nlatency/performance and visual quality while mutually bridging human perception\nand neural scene synthesis to achieve perceptually high-quality immersive\ninteraction. We conducted both objective analysis and subjective studies to\nevaluate the effectiveness of our approach. We find that our method\nsignificantly reduces latency (up to 99% time reduction compared with NeRF)\nwithout loss of high-fidelity rendering (perceptually identical to\nfull-resolution ground truth). The presented approach may serve as the first\nstep toward future VR/AR systems that capture, teleport, and visualize remote\nenvironments in real-time.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.16365v2"
    },
    {
        "title": "Interactive Control over Temporal Consistency while Stylizing Video\n  Streams",
        "authors": [
            "Sumit Shekhar",
            "Max Reimann",
            "Moritz Hilscher",
            "Amir Semmo",
            "Jürgen Döllner",
            "Matthias Trapp"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Image stylization has seen significant advancement and widespread interest\nover the years, leading to the development of a multitude of techniques.\nExtending these stylization techniques, such as Neural Style Transfer (NST), to\nvideos is often achieved by applying them on a per-frame basis. However,\nper-frame stylization usually lacks temporal consistency, expressed by\nundesirable flickering artifacts. Most of the existing approaches for enforcing\ntemporal consistency suffer from one or more of the following drawbacks: They\n(1) are only suitable for a limited range of techniques, (2) do not support\nonline processing as they require the complete video as input, (3) cannot\nprovide consistency for the task of stylization, or (4) do not provide\ninteractive consistency control. Domain-agnostic techniques for temporal\nconsistency aim to eradicate flickering completely but typically disregard\naesthetic aspects. For stylization tasks, however, consistency control is an\nessential requirement as a certain amount of flickering adds to the artistic\nlook and feel. Moreover, making this control interactive is paramount from a\nusability perspective. To achieve the above requirements, we propose an\napproach that stylizes video streams in real-time at full HD resolutions while\nproviding interactive consistency control. We develop a lite optical-flow\nnetwork that operates at 80 FPS on desktop systems with sufficient accuracy.\nFurther, we employ an adaptive combination of local and global consistency\nfeatures and enable interactive selection between them. Objective and\nsubjective evaluations demonstrate that our method is superior to\nstate-of-the-art video consistency approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.00750v2"
    },
    {
        "title": "Neural Point Catacaustics for Novel-View Synthesis of Reflections",
        "authors": [
            "Georgios Kopanas",
            "Thomas Leimkühler",
            "Gilles Rainer",
            "Clément Jambon",
            "George Drettakis"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  View-dependent effects such as reflections pose a substantial challenge for\nimage-based and neural rendering algorithms. Above all, curved reflectors are\nparticularly hard, as they lead to highly non-linear reflection flows as the\ncamera moves. We introduce a new point-based representation to compute Neural\nPoint Catacaustics allowing novel-view synthesis of scenes with curved\nreflectors, from a set of casually-captured input photos. At the core of our\nmethod is a neural warp field that models catacaustic trajectories of\nreflections, so complex specular effects can be rendered using efficient point\nsplatting in conjunction with a neural renderer. One of our key contributions\nis the explicit representation of reflections with a reflection point cloud\nwhich is displaced by the neural warp field, and a primary point cloud which is\noptimized to represent the rest of the scene. After a short manual annotation\nstep, our approach allows interactive high-quality renderings of novel views\nwith accurate reflection flow. Additionally, the explicit representation of\nreflection flow supports several forms of scene manipulation in captured\nscenes, such as reflection editing, cloning of specular objects, reflection\ntracking across views, and comfortable stereo viewing. We provide the source\ncode and other supplemental material on https://repo-sam.inria.fr/\nfungraph/neural_catacaustics/\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01087v1"
    },
    {
        "title": "Scene Synthesis from Human Motion",
        "authors": [
            "Sifan Ye",
            "Yixing Wang",
            "Jiaman Li",
            "Dennis Park",
            "C. Karen Liu",
            "Huazhe Xu",
            "Jiajun Wu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Large-scale capture of human motion with diverse, complex scenes, while\nimmensely useful, is often considered prohibitively costly. Meanwhile, human\nmotion alone contains rich information about the scene they reside in and\ninteract with. For example, a sitting human suggests the existence of a chair,\nand their leg position further implies the chair's pose. In this paper, we\npropose to synthesize diverse, semantically reasonable, and physically\nplausible scenes based on human motion. Our framework, Scene Synthesis from\nHUMan MotiON (SUMMON), includes two steps. It first uses ContactFormer, our\nnewly introduced contact predictor, to obtain temporally consistent contact\nlabels from human motion. Based on these predictions, SUMMON then chooses\ninteracting objects and optimizes physical plausibility losses; it further\npopulates the scene with objects that do not interact with humans. Experimental\nresults demonstrate that SUMMON synthesizes feasible, plausible, and diverse\nscenes and has the potential to generate extensive human-scene interaction data\nfor the community.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01424v1"
    },
    {
        "title": "Freeform Islamic Geometric Patterns",
        "authors": [
            "Rebecca Lin",
            "Craig S. Kaplan"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Islamic geometric patterns are a rich and venerable ornamental tradition.\nMany classic designs feature periodic arrangements of rosettes: star shapes\nsurrounded by rings of hexagonal petals. We present a new technique for\ngenerating 'freeform' compositions of rosettes: finite designs that freely mix\nrosettes of unusual sizes while retaining the aesthetics of traditional\npatterns. We use a circle packing as a scaffolding for developing a patch of\npolygons and fill each polygon with a motif based on established constructions\nfrom Islamic art.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01471v1"
    },
    {
        "title": "Character Simulation Using Imitation Learning With Game Engine Physics",
        "authors": [
            "João Rodrigues",
            "Rui Nóbrega"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Creating visual 3D sensing characters that interact with AI peers and virtual\nenvironments can be a difficult task for those with less experience in using\nlearning algorithms or creating visual environments to execute an agent-based\nsimulation. In this paper, the use of game engines as a tool to create and\nexecute graphic simulations with 3D sensing characters is being explored with\nplugins such as ML-Agents for the Unity3D game engine. This allows the\nsimulation of agents using off-the-shelf algorithms and using the game engine's\nmotor for the visualizations of these agents. We explore the use of these tools\nto create visual bots for games, and teach them how to play the game until they\nreach a level where they can serve as adversaries for real-life players in\ninteractive games.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.02123v1"
    },
    {
        "title": "Variational Direct Modeling: A framework towards integration of\n  parametric modeling and direct modeling in CAD",
        "authors": [
            "Qiang Zou",
            "Hsi-Yung Feng",
            "Shuming Gao"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Feature-based parametric modeling is the de facto standard in CAD. Boundary\nrepresentation-based direct modeling is another CAD paradigm developed\nrecently. They have complementary advantages and limitations, thereby offering\nhuge potential for improvement towards an integrated CAD modeling scheme. Most\nexisting integration methods are developed by industry and typically treat\ndirect edits as pseudo-features, where little can be said about seamless\nintegration. This paper presents an alternative method for seamless\nparametric/direct integration, which allows parametric and direct edits to work\nin a unified way. The fundamental issues and challenges of parametric/direct\nintegration are first explained. A framework is then proposed to handle those\ninformation inconsistencies, based on a detection-then-resolution strategy.\nAlgorithms that can systematically detect and resolve all possible types of\ninformation inconsistencies are also given to implement the framework. With\nthem, model validity can be maintained during the whole model editing process,\nand then the discrepancy between direct edits and parametric edits can be\nresolved. The effectiveness of the proposed approach has been shown with a\nseries of case studies and comparisons, based on a preliminary prototype.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.02999v1"
    },
    {
        "title": "Recognising geometric primitives in 3D point clouds of mechanical CAD\n  objects",
        "authors": [
            "Chiara Romanengo",
            "Andrea Raffo",
            "Silvia Biasotti",
            "Bianca Falcidieno"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The problem faced in this paper concerns the recognition of simple and\ncomplex geometric primitives in point clouds resulting from scans of mechanical\nCAD objects. A large number of points, the presence of noise, outliers, missing\nor redundant parts and uneven distribution are the main problems to be\naddressed to meet this need. In this article we propose a solution, based on\nthe Hough transform, that can recognize simple and complex geometric primitives\nand is robust to noise, outliers, and missing parts. Additionally, we can\nextract a series of geometric descriptors that uniquely characterize a\nprimitive and, based on them, aggregate the output into maximal or compound\nprimitives, thus reducing oversegmentation. The results presented in the paper\ndemonstrate the robustness of the method and its competitiveness with respect\nto other solutions proposed in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04371v1"
    },
    {
        "title": "Learning Audio-Driven Viseme Dynamics for 3D Face Animation",
        "authors": [
            "Linchao Bao",
            "Haoxian Zhang",
            "Yue Qian",
            "Tangli Xue",
            "Changhai Chen",
            "Xuefei Zhe",
            "Di Kang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a novel audio-driven facial animation approach that can generate\nrealistic lip-synchronized 3D facial animations from the input audio. Our\napproach learns viseme dynamics from speech videos, produces animator-friendly\nviseme curves, and supports multilingual speech inputs. The core of our\napproach is a novel parametric viseme fitting algorithm that utilizes phoneme\npriors to extract viseme parameters from speech videos. With the guidance of\nphonemes, the extracted viseme curves can better correlate with phonemes, thus\nmore controllable and friendly to animators. To support multilingual speech\ninputs and generalizability to unseen voices, we take advantage of deep audio\nfeature models pretrained on multiple languages to learn the mapping from audio\nto viseme curves. Our audio-to-curves mapping achieves state-of-the-art\nperformance even when the input audio suffers from distortions of volume,\npitch, speed, or noise. Lastly, a viseme scanning approach for acquiring\nhigh-fidelity viseme assets is presented for efficient speech animation\nproduction. We show that the predicted viseme curves can be applied to\ndifferent viseme-rigged characters to yield various personalized animations\nwith realistic and natural facial motions. Our approach is artist-friendly and\ncan be easily integrated into typical animation production workflows including\nblendshape or bone based animation.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.06059v1"
    },
    {
        "title": "Velocity-Based LOD Reduction in Virtual Reality: A Psychometric Approach",
        "authors": [
            "David Petrescu",
            "Paul A. Warren",
            "Zahra Montazeri",
            "Stephen Pettifer"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Virtual Reality headsets enable users to explore the environment by\nperforming self-induced movements. The retinal velocity produced by such motion\nreduces the visual system's ability to resolve fine detail. We measured the\nimpact of self-induced head rotations on the ability to detect quality changes\nof a realistic 3D model in an immersive virtual reality environment. We varied\nthe Level-of-Detail (LOD) as a function of rotational head velocity with\ndifferent degrees of severity. Using a psychophysical method, we asked 17\nparticipants to identify which of the two presented intervals contained the\nhigher quality model under two different maximum velocity conditions. After\nfitting psychometric functions to data relating the percentage of correct\nresponses to the aggressiveness of LOD manipulations, we identified the\nthreshold severity for which participants could reliably (75\\%) detect the\nlower LOD model. Participants accepted an approximately four-fold LOD reduction\neven in the low maximum velocity condition without a significant impact on\nperceived quality, which suggests that there is considerable potential for\noptimisation when users are moving (increased range of perceptual uncertainty).\nMoreover, LOD could be degraded significantly more in the maximum head velocity\ncondition, suggesting these effects are indeed speed dependent.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.09394v1"
    },
    {
        "title": "PhysGraph: Physics-Based Integration Using Graph Neural Networks",
        "authors": [
            "Oshri Halimi",
            "Egor Larionov",
            "Zohar Barzelay",
            "Philipp Herholz",
            "Tuur Stuyck"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Physics-based simulation of mesh based domains remains a challenging task.\nState-of-the-art techniques can produce realistic results but require expert\nknowledge. A major bottleneck in many approaches is the step of integrating a\npotential energy in order to compute velocities or displacements. Recently,\nlearning based method for physics-based simulation have sparked interest with\ngraph based approaches being a promising research direction. One of the\nchallenges for these methods is to generate models that are mesh independent\nand generalize to different material properties. Moreover, the model should\nalso be able to react to unforeseen external forces like ubiquitous collisions.\nOur contribution is based on a simple observation: evaluating forces is\ncomputationally relatively cheap for traditional simulation methods and can be\ncomputed in parallel in contrast to their integration. If we learn how a system\nreacts to forces in general, irrespective of their origin, we can learn an\nintegrator that can predict state changes due to the total forces with high\ngeneralization power. We effectively factor out the physical model behind\nresulting forces by relying on an opaque force module. We demonstrate that this\nidea leads to a learnable module that can be trained on basic internal forces\nof small mesh patches and generalizes to different mesh typologies,\nresolutions, material parameters and unseen forces like collisions at inference\ntime. Our proposed paradigm is general and can be used to model a variety of\nphysical phenomena. We focus our exposition on the detail enhancement of coarse\nclothing geometry which has many applications including computer games, virtual\nreality and virtual try-on.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.11841v2"
    },
    {
        "title": "Interactive Character Posing by Sparse Coding",
        "authors": [
            "Ranch Y. Q. Lai",
            "Pong C. Yuen",
            "K. W. Lee",
            "J. H. Lai"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Character posing is of interest in computer animation. It is difficult due to\nits dependence on inverse kinematics (IK) techniques and articulate property of\nhuman characters . To solve the IK problem, classical methods that rely on\nnumerical solutions often suffer from the under-determination problem and can\nnot guarantee naturalness. Existing data-driven methods address this problem by\nlearning from motion capture data. When facing a large variety of poses\nhowever, these methods may not be able to capture the pose styles or be\napplicable in real-time environment. Inspired from the low-rank motion\nde-noising and completion model in \\cite{lai2011motion}, we propose a novel\nmodel for character posing based on sparse coding. Unlike conventional\napproaches, our model directly captures the pose styles in Euclidean space to\nprovide intuitive training error measurements and facilitate pose synthesis. A\npose dictionary is learned in training stage and based on it natural poses are\nsynthesized to satisfy users' constraints . We compare our model with existing\nmodels for tasks of pose de-noising and completion. Experiments show our model\nobtains lower de-noising and completion error. We also provide User\nInterface(UI) examples illustrating that our model is effective for interactive\ncharacter posing.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.1409v1"
    },
    {
        "title": "Proofs of two Theorems concerning Sparse Spacetime Constraints",
        "authors": [
            "Christian Schulz",
            "Christoph von Tycowicz",
            "Hans-Peter Seidel",
            "Klaus Hildebrandt"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  In the SIGGRAPH 2014 paper [SvTSH14] an approach for animating deformable\nobjects using sparse spacetime constraints is introduced. This report contains\nthe proofs of two theorems presented in the paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.1902v1"
    },
    {
        "title": "Addressing the unmet need for visualizing Conditional Random Fields in\n  Biological Data",
        "authors": [
            "William C. Ray",
            "Samuel L. Wolock",
            "Nicholas W Callahan",
            "Min Dong",
            "Q. Quinn Li",
            "Chun Liang",
            "Thomas J Magliery",
            "Christopher W. Bartlett"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Background: The biological world is replete with phenomena that appear to be\nideally modeled and analyzed by one archetypal statistical framework - the\nGraphical Probabilistic Model (GPM). The structure of GPMs is a uniquely good\nmatch for biological problems that range from aligning sequences to modeling\nthe genome-to-phenome relationship. The fundamental questions that GPMs address\ninvolve making decisions based on a complex web of interacting factors.\nUnfortunately, while GPMs ideally fit many questions in biology, they are not\nan easy solution to apply. Building a GPM is not a simple task for an end user.\nMoreover, applying GPMs is also impeded by the insidious fact that the complex\nweb of interacting factors inherent to a problem might be easy to define and\nalso intractable to compute upon. Discussion: We propose that the visualization\nsciences can contribute to many domains of the bio-sciences, by developing\ntools to address archetypal representation and user interaction issues in GPMs,\nand in particular a variety of GPM called a Conditional Random Field(CRF). CRFs\nbring additional power, and additional complexity, because the CRF dependency\nnetwork can be conditioned on the query data. Conclusions: In this manuscript\nwe examine the shared features of several biological problems that are amenable\nto modeling with CRFs, highlight the challenges that existing visualization and\nvisual analytics paradigms induce for these data, and document an experimental\nsolution called StickWRLD which, while leaving room for improvement, has been\nsuccessfully applied in several biological research projects.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.2110v1"
    },
    {
        "title": "Image compression overview",
        "authors": [
            "Martin Prantl"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Compression plays a significant role in a data storage and a transmission. If\nwe speak about a generall data compression, it has to be a lossless one. It\nmeans, we are able to recover the original data 1:1 from the compressed file.\nMultimedia data (images, video, sound...), are a special case. In this area, we\ncan use something called a lossy compression. Our main goal is not to recover\ndata 1:1, but only keep them visually similar. This article is about an image\ncompression, so we will be interested only in image compression. For a human\neye, it is not a huge difference, if we recover RGB color with values\n[150,140,138] instead of original [151,140,137]. The magnitude of a difference\ndetermines the loss rate of the compression. The bigger difference usually\nmeans a smaller file, but also worse image quality and noticable differences\nfrom the original image. We want to cover compression techniques mainly from\nthe last decade. Many of them are variations of existing ones, only some of\nthem uses new principes.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.2259v1"
    },
    {
        "title": "Shape Animation with Combined Captured and Simulated Dynamics",
        "authors": [
            "Benjamin Allain",
            "Li Wang",
            "Jean-Sebastien Franco",
            "Franck Hetroy",
            "Edmond Boyer"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We present a novel volumetric animation generation framework to create new\ntypes of animations from raw 3D surface or point cloud sequence of captured\nreal performances. The framework considers as input time incoherent 3D\nobservations of a moving shape, and is thus particularly suitable for the\noutput of performance capture platforms. In our system, a suitable virtual\nrepresentation of the actor is built from real captures that allows seamless\ncombination and simulation with virtual external forces and objects, in which\nthe original captured actor can be reshaped, disassembled or reassembled from\nuser-specified virtual physics. Instead of using the dominant surface-based\ngeometric representation of the capture, which is less suitable for volumetric\neffects, our pipeline exploits Centroidal Voronoi tessellation decompositions\nas unified volumetric representation of the real captured actor, which we show\ncan be used seamlessly as a building block for all processing stages, from\ncapture and tracking to virtual physic simulation. The representation makes no\nhuman specific assumption and can be used to capture and re-simulate the actor\nwith props or other moving scenery elements. We demonstrate the potential of\nthis pipeline for virtual reanimation of a real captured event with various\nunprecedented volumetric visual effects, such as volumetric distortion,\nerosion, morphing, gravity pull, or collisions.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.01232v1"
    },
    {
        "title": "Anti-commutative Dual Complex Numbers and 2D Rigid Transformation",
        "authors": [
            "Genki Matsuda",
            "Shizuo Kaji",
            "Hiroyuki Ochiai"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We introduce a new presentation of the two dimensional rigid transformation\nwhich is more concise and efficient than the standard matrix presentation. By\nmodifying the ordinary dual number construction for the complex numbers, we\ndefine the ring of the anti-commutative dual complex numbers, which\nparametrizes two dimensional rotation and translation all together. With this\npresentation, one can easily interpolate or blend two or more rigid\ntransformations at a low computational cost. We developed a library for C++\nwith the MIT-licensed source code and demonstrate its facility by an\ninteractive deformation tool developed for iPad.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.01754v1"
    },
    {
        "title": "Implicit equations of non-degenerate rational Bezier quadric triangles",
        "authors": [
            "A. Canton",
            "L. Fernandez-Jambrina",
            "E. Rosado Maria",
            "M. J. Vazquez-Gallo"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  In this paper we review the derivation of implicit equations for\nnon-degenerate quadric patches in rational Bezier triangular form. These are\nthe case of Steiner surfaces of degree two. We derive the bilinear forms for\nsuch quadrics in a coordinate-free fashion in terms of their control net and\ntheir list of weights in a suitable form. Our construction relies on projective\ngeometry and is grounded on the pencil of quadrics circumscribed to a\ntetrahedron formed by vertices of the control net and an additional point which\nis required for the Steiner surface to be a non-degenerate quadric.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.03224v1"
    },
    {
        "title": "Tetrisation of triangular meshes and its application in shape blending",
        "authors": [
            "Shizuo Kaji"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  The As-Rigid-As-Possible (ARAP) shape deformation framework is a versatile\ntechnique for morphing, surface modelling, and mesh editing. We discuss an\nimprovement of the ARAP framework in a few aspects: 1. Given a triangular mesh\nin 3D space, we introduce a method to associate a tetrahedral structure, which\nencodes the geometry of the original mesh. 2. We use a Lie algebra based method\nto interpolate local transformation, which provides better handling of rotation\nwith large angle. 3. We propose a new error function to compile local\ntransformations into a global piecewise linear map, which is rotation invariant\nand easy to minimise. We implemented a shape blender based on our algorithm and\nits MIT licensed source code is available online.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04816v1"
    },
    {
        "title": "Deep Shading: Convolutional Neural Networks for Screen-Space Shading",
        "authors": [
            "Oliver Nalbach",
            "Elena Arabadzhiyska",
            "Dushyant Mehta",
            "Hans-Peter Seidel",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  In computer vision, convolutional neural networks (CNNs) have recently\nachieved new levels of performance for several inverse problems where RGB pixel\nappearance is mapped to attributes such as positions, normals or reflectance.\nIn computer graphics, screen-space shading has recently increased the visual\nquality in interactive image synthesis, where per-pixel attributes such as\npositions, normals or reflectance of a virtual 3D scene are converted into RGB\npixel appearance, enabling effects like ambient occlusion, indirect light,\nscattering, depth-of-field, motion blur, or anti-aliasing. In this paper we\nconsider the diagonal problem: synthesizing appearance from given per-pixel\nattributes using a CNN. The resulting Deep Shading simulates various\nscreen-space effects at competitive quality and speed while not being\nprogrammed by human experts but learned from example images.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06078v2"
    },
    {
        "title": "Neurally-Guided Procedural Models: Amortized Inference for Procedural\n  Graphics Programs using Neural Networks",
        "authors": [
            "Daniel Ritchie",
            "Anna Thomas",
            "Pat Hanrahan",
            "Noah D. Goodman"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Probabilistic inference algorithms such as Sequential Monte Carlo (SMC)\nprovide powerful tools for constraining procedural models in computer graphics,\nbut they require many samples to produce desirable results. In this paper, we\nshow how to create procedural models which learn how to satisfy constraints. We\naugment procedural models with neural networks which control how the model\nmakes random choices based on the output it has generated thus far. We call\nsuch models neurally-guided procedural models. As a pre-computation, we train\nthese models to maximize the likelihood of example outputs generated via SMC.\nThey are then used as efficient SMC importance samplers, generating\nhigh-quality results with very few samples. We evaluate our method on\nL-system-like models with image-based constraints. Given a desired quality\nthreshold, neurally-guided models can generate satisfactory results up to 10x\nfaster than unguided models.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06143v2"
    },
    {
        "title": "SMASH: Physics-guided Reconstruction of Collisions from Videos",
        "authors": [
            "Aron Monszpart",
            "Nils Thuerey",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Collision sequences are commonly used in games and entertainment to add drama\nand excitement. Authoring even two body collisions in the real world can be\ndifficult, as one has to get timing and the object trajectories to be correctly\nsynchronized. After tedious trial-and-error iterations, when objects can\nactually be made to collide, then they are difficult to capture in 3D. In\ncontrast, synthetically generating plausible collisions is difficult as it\nrequires adjusting different collision parameters (e.g., object mass ratio,\ncoefficient of restitution, etc.) and appropriate initial parameters. We\npresent SMASH to directly read off appropriate collision parameters directly\nfrom raw input video recordings. Technically we enable this by utilizing laws\nof rigid body collision to regularize the problem of lifting 2D trajectories to\na physically valid 3D reconstruction of the collision. The reconstructed\nsequences can then be modified and combined to easily author novel and\nplausible collisions. We evaluate our system on a range of synthetic scenes and\ndemonstrate the effectiveness of our method by accurately reconstructing\nseveral complex real world collision events.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.08984v2"
    },
    {
        "title": "Squares that Look Round: Transforming Spherical Images",
        "authors": [
            "Saul Schleimer",
            "Henry Segerman"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  We propose M\\\"obius transformations as the natural rotation and scaling tools\nfor editing spherical images. As an application we produce spherical Droste\nimages. We obtain other self-similar visual effects using rational functions,\nelliptic functions, and Schwarz-Christoffel maps.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.01396v1"
    },
    {
        "title": "Sufficient Conditions for Tuza's Conjecture on Packing and Covering\n  Triangles",
        "authors": [
            "Xujin Chen",
            "Zhuo Diao",
            "Xiaodong Hu",
            "Zhongzheng Tang"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Given a simple graph $G=(V,E)$, a subset of $E$ is called a triangle cover if\nit intersects each triangle of $G$. Let $\\nu_t(G)$ and $\\tau_t(G)$ denote the\nmaximum number of pairwise edge-disjoint triangles in $G$ and the minimum\ncardinality of a triangle cover of $G$, respectively. Tuza conjectured in 1981\nthat $\\tau_t(G)/\\nu_t(G)\\le2$ holds for every graph $G$. In this paper, using a\nhypergraph approach, we design polynomial-time combinatorial algorithms for\nfinding small triangle covers. These algorithms imply new sufficient conditions\nfor Tuza's conjecture on covering and packing triangles. More precisely,\nsuppose that the set $\\mathscr T_G$ of triangles covers all edges in $G$. We\nshow that a triangle cover of $G$ with cardinality at most $2\\nu_t(G)$ can be\nfound in polynomial time if one of the following conditions is satisfied: (i)\n$\\nu_t(G)/|\\mathscr T_G|\\ge\\frac13$, (ii) $\\nu_t(G)/|E|\\ge\\frac14$, (iii)\n$|E|/|\\mathscr T_G|\\ge2$.\n  Keywords: Triangle cover, Triangle packing, Linear 3-uniform hypergraphs,\nCombinatorial algorithms\n",
        "pdf_link": "http://arxiv.org/pdf/1605.01816v3"
    },
    {
        "title": "As-exact-as-possible repair of unprintable STL files",
        "authors": [
            "Marco Attene"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Purpose: The class of models that can be represented by STL files is larger\nthan the class of models that can be printed using additive manufacturing\ntechnologies. Stated differently, there exist well-formed STL files that cannot\nbe printed. In this paper such a gap is formalized and a fully automatic\nprocedure is described to turn any such file into a printable model.\n  Approach: Based on well-established concepts from combinatorial topology, we\nprovide an unambiguous description of all the mathematical entities involved in\nthe modeling-printing pipeline. Specifically, we formally define the conditions\nthat an STL file must satisfy to be printable and, based on these, we design an\nas-exact-as-possible repairing algorithm.\n  Findings: We have found that, in order to cope with all the possible triangle\nconfigurations, the algorithm must distinguish between triangles that bound\nsolid parts and triangles that constitute zero-thickness sheets. Only the\nformer set can be fixed without distortion.\n  Originality: Previous methods that are guaranteed to fix all the possible\nconfigurations provide only approximate solutions with an unnecessary\ndistortion. Conversely, our procedure is as exact as possible, meaning that no\nvisible distortion is introduced unless it is strictly imposed by limitations\nof the printing device. Thanks to such an unprecedented flexibility and\naccuracy, this algorithm is expected to significantly simplify the\nmodeling-printing process, in particular within the continuously emerging\nnon-professional \"maker\" communities.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.07829v3"
    },
    {
        "title": "Quantitative Analysis of Saliency Models",
        "authors": [
            "Flora Ponjou Tasse",
            "Jiří Kosinka",
            "Neil Anthony Dodgson"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Previous saliency detection research required the reader to evaluate\nperformance qualitatively, based on renderings of saliency maps on a few\nshapes. This qualitative approach meant it was unclear which saliency models\nwere better, or how well they compared to human perception. This paper provides\na quantitative evaluation framework that addresses this issue. In the first\nquantitative analysis of 3D computational saliency models, we evaluate four\ncomputational saliency models and two baseline models against ground-truth\nsaliency collected in previous work.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.09451v1"
    },
    {
        "title": "Where and Who? Automatic Semantic-Aware Person Composition",
        "authors": [
            "Fuwen Tan",
            "Crispin Bernier",
            "Benjamin Cohen",
            "Vicente Ordonez",
            "Connelly Barnes"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Image compositing is a method used to generate realistic yet fake imagery by\ninserting contents from one image to another. Previous work in compositing has\nfocused on improving appearance compatibility of a user selected foreground\nsegment and a background image (i.e. color and illumination consistency). In\nthis work, we instead develop a fully automated compositing model that\nadditionally learns to select and transform compatible foreground segments from\na large collection given only an input image background. To simplify the task,\nwe restrict our problem by focusing on human instance composition, because\nhuman segments exhibit strong correlations with their background and because of\nthe availability of large annotated data. We develop a novel branching\nConvolutional Neural Network (CNN) that jointly predicts candidate person\nlocations given a background image. We then use pre-trained deep feature\nrepresentations to retrieve person instances from a large segment database.\nExperimental results show that our model can generate composite images that\nlook visually convincing. We also develop a user interface to demonstrate the\npotential application of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01021v2"
    },
    {
        "title": "DeepSketch2Face: A Deep Learning Based Sketching System for 3D Face and\n  Caricature Modeling",
        "authors": [
            "Xiaoguang Han",
            "Chang Gao",
            "Yizhou Yu"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Face modeling has been paid much attention in the field of visual computing.\nThere exist many scenarios, including cartoon characters, avatars for social\nmedia, 3D face caricatures as well as face-related art and design, where\nlow-cost interactive face modeling is a popular approach especially among\namateur users. In this paper, we propose a deep learning based sketching system\nfor 3D face and caricature modeling. This system has a labor-efficient\nsketching interface, that allows the user to draw freehand imprecise yet\nexpressive 2D lines representing the contours of facial features. A novel CNN\nbased deep regression network is designed for inferring 3D face models from 2D\nsketches. Our network fuses both CNN and shape based features of the input\nsketch, and has two independent branches of fully connected layers generating\nindependent subsets of coefficients for a bilinear face representation. Our\nsystem also supports gesture based interactions for users to further manipulate\ninitial face models. Both user studies and numerical results indicate that our\nsketching system can help users create face models quickly and effectively. A\nsignificantly expanded face database with diverse identities, expressions and\nlevels of exaggeration is constructed to promote further research and\nevaluation of face modeling techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.02042v1"
    },
    {
        "title": "cGAN-based Manga Colorization Using a Single Training Image",
        "authors": [
            "Paulina Hensman",
            "Kiyoharu Aizawa"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  The Japanese comic format known as Manga is popular all over the world. It is\ntraditionally produced in black and white, and colorization is time consuming\nand costly. Automatic colorization methods generally rely on greyscale values,\nwhich are not present in manga. Furthermore, due to copyright protection,\ncolorized manga available for training is scarce. We propose a manga\ncolorization method based on conditional Generative Adversarial Networks\n(cGAN). Unlike previous cGAN approaches that use many hundreds or thousands of\ntraining images, our method requires only a single colorized reference image\nfor training, avoiding the need of a large dataset. Colorizing manga using\ncGANs can produce blurry results with artifacts, and the resolution is limited.\nWe therefore also propose a method of segmentation and color-correction to\nmitigate these issues. The final results are sharp, clear, and in high\nresolution, and stay true to the character's original color scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.06918v1"
    },
    {
        "title": "From Big Data to Big Displays: High-Performance Visualization at Blue\n  Brain",
        "authors": [
            "Stefan Eilemann",
            "Marwan Abdellah",
            "Nicolas Antille",
            "Ahmet Bilgili",
            "Grigory Chevtchenko",
            "Raphael Dumusc",
            "Cyrille Favreau",
            "Juan Hernando",
            "Daniel Nachbaur",
            "Pawel Podhajski",
            "Jafet Villafranca",
            "Felix Schürmann"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Blue Brain has pushed high-performance visualization (HPV) to complement its\nHPC strategy since its inception in 2007. In 2011, this strategy has been\naccelerated to develop innovative visualization solutions through increased\nfunding and strategic partnerships with other research institutions.\n  We present the key elements of this HPV ecosystem, which integrates C++\nvisualization applications with novel collaborative display systems. We\nmotivate how our strategy of transforming visualization engines into services\nenables a variety of use cases, not only for the integration with high-fidelity\ndisplays, but also to build service oriented architectures, to link into web\napplications and to provide remote services to Python applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.10098v1"
    },
    {
        "title": "Deep Appearance Models for Face Rendering",
        "authors": [
            "Stephen Lombardi",
            "Jason Saragih",
            "Tomas Simon",
            "Yaser Sheikh"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We introduce a deep appearance model for rendering the human face. Inspired\nby Active Appearance Models, we develop a data-driven rendering pipeline that\nlearns a joint representation of facial geometry and appearance from a\nmultiview capture setup. Vertex positions and view-specific textures are\nmodeled using a deep variational autoencoder that captures complex nonlinear\neffects while producing a smooth and compact latent representation.\nView-specific texture enables the modeling of view-dependent effects such as\nspecularity. In addition, it can also correct for imperfect geometry stemming\nfrom biased or low resolution estimates. This is a significant departure from\nthe traditional graphics pipeline, which requires highly accurate geometry as\nwell as all elements of the shading model to achieve realism through\nphysically-inspired light transport. Acquiring such a high level of accuracy is\ndifficult in practice, especially for complex and intricate parts of the face,\nsuch as eyelashes and the oral cavity. These are handled naturally by our\napproach, which does not rely on precise estimates of geometry. Instead, the\nshading model accommodates deficiencies in geometry though the flexibility\nafforded by the neural network employed. At inference time, we condition the\ndecoding network on the viewpoint of the camera in order to generate the\nappropriate texture for rendering. The resulting system can be implemented\nsimply using existing rendering engines through dynamic textures with flat\nlighting. This representation, together with a novel unsupervised technique for\nmapping images to facial states, results in a system that is naturally suited\nto real-time interactive settings such as Virtual Reality (VR).\n",
        "pdf_link": "http://arxiv.org/pdf/1808.00362v1"
    },
    {
        "title": "Everybody Dance Now",
        "authors": [
            "Caroline Chan",
            "Shiry Ginosar",
            "Tinghui Zhou",
            "Alexei A. Efros"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This paper presents a simple method for \"do as I do\" motion transfer: given a\nsource video of a person dancing, we can transfer that performance to a novel\n(amateur) target after only a few minutes of the target subject performing\nstandard moves. We approach this problem as video-to-video translation using\npose as an intermediate representation. To transfer the motion, we extract\nposes from the source subject and apply the learned pose-to-appearance mapping\nto generate the target subject. We predict two consecutive frames for\ntemporally coherent video results and introduce a separate pipeline for\nrealistic face synthesis. Although our method is quite simple, it produces\nsurprisingly compelling results (see video). This motivates us to also provide\na forensics tool for reliable synthetic content detection, which is able to\ndistinguish videos synthesized by our system from real data. In addition, we\nrelease a first-of-its-kind open-source dataset of videos that can be legally\nused for training and motion transfer.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.07371v2"
    },
    {
        "title": "Optimizing B-spline surfaces for developability and paneling\n  architectural freeform surfaces",
        "authors": [
            "Konstantinos Gavriil",
            "Alexander Schiftner",
            "Helmut Pottmann"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Motivated by applications in architecture and design, we present a novel\nmethod for increasing the developability of a B-spline surface. We use the\nproperty that the Gauss image of a developable surface is 1-dimensional and can\nbe locally well approximated by circles. This is cast into an algorithm for\nthinning the Gauss image by increasing the planarity of the Gauss images of\nappropriate neighborhoods. A variation of the main method allows us to tackle\nthe problem of paneling a freeform architectural surface with developable\npanels, in particular enforcing rotational cylindrical, rotational conical and\nplanar panels, which are the main preferred types of developable panels in\narchitecture due to the reduced cost of manufacturing.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.07560v3"
    },
    {
        "title": "Novel Approach to Measure Motion-To-Photon and Mouth-To-Ear Latency in\n  Distributed Virtual Reality Systems",
        "authors": [
            "Armin Becher",
            "Jens Angerer",
            "Thomas Grauschopf"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Distributed Virtual Reality systems enable globally dispersed users to\ninteract with each other in a shared virtual environment. In such systems,\ndifferent types of latencies occur. For a good VR experience, they need to be\ncontrolled. The time delay between the user's head motion and the corresponding\ndisplay output of the VR system might lead to adverse effects such as a reduced\nsense of presence or motion sickness. Additionally, high network latency among\nworldwide locations makes collaboration between users more difficult and leads\nto misunderstandings. To evaluate the performance and optimize dispersed VR\nsolutions it is therefore important to measure those delays. In this work, a\nnovel, easy to set up, and inexpensive method to measure local and remote\nsystem latency will be described. The measuring setup consists of a\nmicrocontroller, a microphone, a piezo buzzer, a photosensor, and a\npotentiometer. With these components, it is possible to measure\nmotion-to-photon and mouth-to-ear latency of various VR systems. By using\nGPS-receivers for timecode-synchronization it is also possible to obtain the\nend-to-end delays between different worldwide locations. The described system\nwas used to measure local and remote latencies of two HMD based distributed VR\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.06320v1"
    },
    {
        "title": "Light Field Neural Network",
        "authors": [
            "Yuchi Huo",
            "Sung-Eui Yoon"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We introduce an optical neural network system made by off-the-shelf\ncomponents. In order to test the evaluate the physical property of the proposed\nsystem, we are making a prototype. After further discussions with our\ncooperators, we are agreed that the prototype implementation may take longer\ntime than we expected earlier. Therefore we reach a consensus on withdrawing\nthe paper until the physical data is available.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.07009v2"
    },
    {
        "title": "Non-Line-of-Sight Reconstruction using Efficient Transient Rendering",
        "authors": [
            "Julian Iseringhausen",
            "Matthias B. Hullin"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Being able to see beyond the direct line of sight is an intriguing\nprospective and could benefit a wide variety of important applications. Recent\nwork has demonstrated that time-resolved measurements of indirect diffuse light\ncontain valuable information for reconstructing shape and reflectance\nproperties of objects located around a corner. In this paper, we introduce a\nnovel reconstruction scheme that, by design, produces solutions that are\nconsistent with state-of-the-art physically-based rendering. Our method\ncombines an efficient forward model (a custom renderer for time-resolved\nthree-bounce indirect light transport) with an optimization framework to\nreconstruct object geometry in an analysis-by-synthesis sense. We evaluate our\nalgorithm on a variety of synthetic and experimental input data, and show that\nit gracefully handles uncooperative scenes with high levels of noise or\nnon-diffuse material reflectance.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.08044v2"
    },
    {
        "title": "PhotoShape: Photorealistic Materials for Large-Scale Shape Collections",
        "authors": [
            "Keunhong Park",
            "Konstantinos Rematas",
            "Ali Farhadi",
            "Steven M. Seitz"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Existing online 3D shape repositories contain thousands of 3D models but lack\nphotorealistic appearance. We present an approach to automatically assign\nhigh-quality, realistic appearance models to large scale 3D shape collections.\nThe key idea is to jointly leverage three types of online data -- shape\ncollections, material collections, and photo collections, using the photos as\nreference to guide assignment of materials to shapes. By generating a large\nnumber of synthetic renderings, we train a convolutional neural network to\nclassify materials in real photos, and employ 3D-2D alignment techniques to\ntransfer materials to different parts of each shape model. Our system produces\nphotorealistic, relightable, 3D shapes (PhotoShapes).\n",
        "pdf_link": "http://arxiv.org/pdf/1809.09761v1"
    },
    {
        "title": "Deep-learning the Latent Space of Light Transport",
        "authors": [
            "Pedro Hermosilla",
            "Sebastian Maisch",
            "Tobias Ritschel",
            "Timo Ropinski"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We suggest a method to directly deep-learn light transport, i. e., the\nmapping from a 3D geometry-illumination-material configuration to a shaded 2D\nimage. While many previous learning methods have employed 2D convolutional\nneural networks applied to images, we show for the first time that light\ntransport can be learned directly in 3D. The benefit of 3D over 2D is, that the\nformer can also correctly capture illumination effects related to occluded\nand/or semi-transparent geometry. To learn 3D light transport, we represent the\n3D scene as an unstructured 3D point cloud, which is later, during rendering,\nprojected to the 2D output image. Thus, we suggest a two-stage operator\ncomprising of a 3D network that first transforms the point cloud into a latent\nrepresentation, which is later on projected to the 2D output image using a\ndedicated 3D-2D network in a second step. We will show that our approach\nresults in improved quality in terms of temporal coherence while retaining most\nof the computational efficiency of common 2D methods. As a consequence, the\nproposed two stage-operator serves as a valuable extension to modern deferred\nshading approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04756v2"
    },
    {
        "title": "Multilevel active registration for kinect human body scans: from low\n  quality to high quality",
        "authors": [
            "Zongyi Xu",
            "Qianni Zhang",
            "Shiyang Cheng"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Registration of 3D human body has been a challenging research topic for over\ndecades. Most of the traditional human body registration methods require manual\nassistance, or other auxiliary information such as texture and markers. The\nmajority of these methods are tailored for high-quality scans from expensive\nscanners. Following the introduction of the low-quality scans from\ncost-effective devices such as Kinect, the 3D data capturing of human body\nbecomes more convenient and easier. However, due to the inevitable holes,\nnoises and outliers in the low-quality scan, the registration of human body\nbecomes even more challenging. To address this problem, we propose a fully\nautomatic active registration method which deforms a high-resolution template\nmesh to match the low-quality human body scans. Our registration method\noperates on two levels of statistical shape models: (1) the first level is a\nholistic body shape model that defines the basic figure of human; (2) the\nsecond level includes a set of shape models for every body part, aiming at\ncapturing more body details. Our fitting procedure follows a coarse-to-fine\napproach that is robust and efficient. Experiments show that our method is\ncomparable with the state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.10175v1"
    },
    {
        "title": "PointCleanNet: Learning to Denoise and Remove Outliers from Dense Point\n  Clouds",
        "authors": [
            "Marie-Julie Rakotosaona",
            "Vittorio La Barbera",
            "Paul Guerrero",
            "Niloy J. Mitra",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Point clouds obtained with 3D scanners or by image-based reconstruction\ntechniques are often corrupted with significant amount of noise and outliers.\nTraditional methods for point cloud denoising largely rely on local surface\nfitting (e.g., jets or MLS surfaces), local or non-local averaging, or on\nstatistical assumptions about the underlying noise model. In contrast, we\ndevelop a simple data-driven method for removing outliers and reducing noise in\nunordered point clouds. We base our approach on a deep learning architecture\nadapted from PCPNet, which was recently proposed for estimating local 3D shape\nproperties in point clouds. Our method first classifies and discards outlier\nsamples, and then estimates correction vectors that project noisy points onto\nthe original clean surfaces. The approach is efficient and robust to varying\namounts of noise and outliers, while being able to handle large densely-sampled\npoint clouds. In our extensive evaluation, both on synthesic and real data, we\nshow an increased robustness to strong noise levels compared to various\nstate-of-the-art methods, enabling accurate surface reconstruction from\nextremely noisy real data obtained by range scans. Finally, the simplicity and\nuniversality of our approach makes it very easy to integrate in any existing\ngeometry processing pipeline.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.01060v3"
    },
    {
        "title": "A novel 3D display based on micro-volumetric scanning and real time\n  reconstruction of holograms principle",
        "authors": [
            "Guangjun Wang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The present study proposes a novel 3D display contains a micro-volumetric\nscanning system (MVS) and a real time reconstruction hologram system (RTRH).\n",
        "pdf_link": "http://arxiv.org/pdf/1901.05064v1"
    },
    {
        "title": "Metasurfaces for near-eye augmented reality",
        "authors": [
            "Shoufeng Lan",
            "Xueyue Zhang",
            "Mohammad Taghinejad",
            "Sean Rodrigues",
            "Kyu-Tae Lee",
            "Zhaocheng Liu",
            "Wenshan Cai"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Augmented reality (AR) has the potential to revolutionize the way in which\ninformation is presented by overlaying virtual information onto a person's\ndirect view of their real-time surroundings. By placing the display on the\nsurface of the eye, a contact lens display (CLD) provides a versatile solution\nfor compact AR. However, an unaided human eye cannot visualize patterns on the\nCLD simply because of the limited accommodation of the eye. Here, we introduce\na holographic display technology that casts virtual information directly to the\nretina so that the eye sees it while maintaining the visualization of the\nreal-world intact. The key to our design is to introduce metasurfaces to create\na phase distribution that projects virtual information in a pixel-by-pixel\nmanner. Unlike conventional holographic techniques, our metasurface-based\ntechnique is able to display arbitrary patterns using a single passive\nhologram. With a small form-factor, the designed metasurface empowers near-eye\nAR excluding the need of extra optical elements, such as a spatial light\nmodulator, for dynamic image control.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.06408v1"
    },
    {
        "title": "Geodesic Centroidal Voronoi Tessellations: Theories, Algorithms and\n  Applications",
        "authors": [
            "Zipeng Ye",
            "Ran Yi",
            "Minjing Yu",
            "Yong-Jin Liu",
            "Ying He"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Nowadays, big data of digital media (including images, videos and 3D\ngraphical models) are frequently modeled as low-dimensional manifold meshes\nembedded in a high-dimensional feature space. In this paper, we summarized our\nrecent work on geodesic centroidal Voronoi tessellations(GCVTs), which are\nintrinsic geometric structures on manifold meshes. We show that GCVT can find a\nwidely range of interesting applications in computer vision and graphics, due\nto the efficiency of search, location and indexing inherent in these intrinsic\ngeometric structures. Then we present the challenging issues of how to build\nthe combinatorial structures of GCVTs and establish their time and space\ncomplexities, including both theoretical and algorithmic results.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.00523v1"
    },
    {
        "title": "RadVR: A 6DOF Virtual Reality Daylighting Analysis Tool",
        "authors": [
            "Mohammad Keshavarzi",
            "Luisa Caldas",
            "Luis Santos"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This work introduces RadVR, a virtual reality tool for daylighting analysis\nthat simultaneously combines qualitative assessments through immersive\nreal-time renderings with quantitative physically correct daylighting\nsimulations in a 6DOF virtual environment. By taking a 3D building model with\nmaterial properties as input, RadVR allows users to (1) perform\nphysically-based daylighting simulations via Radiance, (2) study sunlight in\ndifferent hours-of-the-year, (3) interact with a 9-point-in-time matrix for the\nmost representative times of the year, and (4) visualize, compare, and analyze\ndaylighting simulation results. With an end-to-end workflow, RadVR integrates\nwith 3D modeling software that is commonly used by building designers.\nAdditionally, by conducting user experiments we compare the proposed system\nwith DIVA for Rhino, a Radiance-based tool that uses conventional 2D-displays.\nThe results show that RadVR can provide promising assistance in spatial\nunderstanding tasks, navigation, and sun position analysis in virtual reality.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.01652v2"
    },
    {
        "title": "Efficient Cloth Simulation using Miniature Cloth and Upscaling Deep\n  Neural Networks",
        "authors": [
            "Tae Min Lee",
            "Young Jin Oh",
            "In-Kwon Lee"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Cloth simulation requires a fast and stable method for interactively and\nrealistically visualizing fabric materials using computer graphics. We propose\nan efficient cloth simulation method using miniature cloth simulation and\nupscaling Deep Neural Networks (DNN). The upscaling DNNs generate the target\ncloth simulation from the results of physically-based simulations of a\nminiature cloth that has similar physical properties to those of the target\ncloth. We have verified the utility of the proposed method through experiments,\nand the results demonstrate that it is possible to generate fast and stable\ncloth simulations under various conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.03953v1"
    },
    {
        "title": "Prescription AR: A Fully-Customized Prescription-Embedded Augmented\n  Reality Display",
        "authors": [
            "Jui-Yi Wu",
            "Jonghyun Kim"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper, we present a fully-customized AR display design that considers\nthe user's prescription, interpupillary distance, and taste of fashion. A\nfree-form image combiner embedded inside the prescription lens provides\naugmented images onto the vision-corrected real world. We establish a\nprescription-embedded AR display optical design method as well as the\ncustomization method for individual users. Our design can cover myopia,\nhyperopia, astigmatism, and presbyopia, and allows the eye-contact interaction\nwith privacy protection. A 169$g$ dynamic prototype showed a 40$^\\circ$\n$\\times$ 20 $^\\circ$ virtual image with a 23 cpd resolution at center field and\n6 mm $\\times$ 4 mm eye box, with the vision-correction and varifocal (0.5-3$m$)\ncapability.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.04353v1"
    },
    {
        "title": "Improving the Projection of Global Structures in Data through Spanning\n  Trees",
        "authors": [
            "Daniel Alcaide",
            "Jan Aerts"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The connection of edges in a graph generates a structure that is independent\nof a coordinate system. This visual metaphor allows creating a more flexible\nrepresentation of data than a two-dimensional scatterplot. In this work, we\npresent STAD (Spanning Trees as Approximation of Data), a dimensionality\nreduction method to approximate the high-dimensional structure into a graph\nwith or without formulating prior hypotheses. STAD generates an abstract\nrepresentation of high-dimensional data by giving each data point a location in\na graph which preserves the distances in the original high-dimensional space.\nThe STAD graph is built upon the Minimum Spanning Tree (MST) to which new edges\nare added until the correlation between the distances from the graph and the\noriginal dataset is maximized. Additionally, STAD supports the inclusion of\nadditional functions to focus the exploration and allow the analysis of data\nfrom new perspectives, emphasizing traits in data which otherwise would remain\nhidden. We demonstrate the effectiveness of our method by applying it to two\nreal-world datasets: traffic density in Barcelona and temporal measurements of\nair quality in Castile and Le\\'on in Spain.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.05783v1"
    },
    {
        "title": "RayTracer.jl: A Differentiable Renderer that supports Parameter\n  Optimization for Scene Reconstruction",
        "authors": [
            "Avik Pal"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper, we present RayTracer.jl, a renderer in Julia that is fully\ndifferentiable using source-to-source Automatic Differentiation (AD). This\nmeans that RayTracer not only renders 2D images from 3D scene parameters, but\nit can be used to optimize for model parameters that generate a target image in\na Differentiable Programming (DP) pipeline. We interface our renderer with the\ndeep learning library Flux for use in combination with neural networks. We\ndemonstrate the use of this differentiable renderer in rendering tasks and in\nsolving inverse graphics problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.07198v3"
    },
    {
        "title": "Motion Browser: Visualizing and Understanding Complex Upper Limb\n  Movement Under Obstetrical Brachial Plexus Injuries",
        "authors": [
            "Gromit Yeuk-Yin Chan",
            "Luis Gustavo Nonato",
            "Alice Chu",
            "Preeti Raghavan",
            "Viswanath Aluru",
            "Claudio T. Silva"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The brachial plexus is a complex network of peripheral nerves that enables\nsensing from and control of the movements of the arms and hand. Nowadays, the\ncoordination between the muscles to generate simple movements is still not well\nunderstood, hindering the knowledge of how to best treat patients with this\ntype of peripheral nerve injury. To acquire enough information for medical data\nanalysis, physicians conduct motion analysis assessments with patients to\nproduce a rich dataset of electromyographic signals from multiple muscles\nrecorded with joint movements during real-world tasks. However, tools for the\nanalysis and visualization of the data in a succinct and interpretable manner\nare currently not available. Without the ability to integrate, compare, and\ncompute multiple data sources in one platform, physicians can only compute\nsimple statistical values to describe patient's behavior vaguely, which limits\nthe possibility to answer clinical questions and generate hypotheses for\nresearch. To address this challenge, we have developed \\systemname, an\ninteractive visual analytics system which provides an efficient framework to\nextract and compare muscle activity patterns from the patient's limbs and\ncoordinated views to help users analyze muscle signals, motion data, and video\ninformation to address different tasks. The system was developed as a result of\na collaborative endeavor between computer scientists and orthopedic surgery and\nrehabilitation physicians. We present case studies showing physicians can\nutilize the information displayed to understand how individuals coordinate\ntheir muscles to initiate appropriate treatment and generate new hypotheses for\nfuture research.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.09146v1"
    },
    {
        "title": "ICE: An Interactive Configuration Explorer for High Dimensional\n  Categorical Parameter Spaces",
        "authors": [
            "Anjul Tyagi",
            "Zhen Cao",
            "Tyler Estro",
            "Erez Zadok",
            "Klaus Mueller"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  There are many applications where users seek to explore the impact of the\nsettings of several categorical variables with respect to one dependent\nnumerical variable. For example, a computer systems analyst might want to study\nhow the type of file system or storage device affects system performance. A\nusual choice is the method of Parallel Sets designed to visualize multivariate\ncategorical variables. However, we found that the magnitude of the parameter\nimpacts on the numerical variable cannot be easily observed here. We also\nattempted a dimension reduction approach based on Multiple Correspondence\nAnalysis but found that the SVD-generated 2D layout resulted in a loss of\ninformation. We hence propose a novel approach, the Interactive Configuration\nExplorer (ICE), which directly addresses the need of analysts to learn how the\ndependent numerical variable is affected by the parameter settings given\nmultiple optimization objectives. No information is lost as ICE shows the\ncomplete distribution and statistics of the dependent variable in context with\neach categorical variable. Analysts can interactively filter the variables to\noptimize for certain goals such as achieving a system with maximum performance,\nlow variance, etc. Our system was developed in tight collaboration with a group\nof systems performance researchers and its final effectiveness was evaluated\nwith expert interviews, a comparative user study, and two case studies.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12627v2"
    },
    {
        "title": "pylustrator: Code generation for reproducible figures for publication",
        "authors": [
            "Richard Gerum"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  One major challenge in science is to make all results potentially\nreproducible. Thus, along with the raw data, every step from basic processing\nof the data, evaluation, to the generation of the figures, has to be documented\nas clearly as possible. While there are many programming libraries that cover\nthe basic processing and plotting steps (e.g. Matplotlib in Python), no library\nyet addresses the reproducible composing of single plots into meaningful\nfigures for publication. Thus, up to now it is still state-of-the-art to\ngenerate publishable figures using image-processing or vector-drawing software\nleading to unwanted alterations of the presented data in the worst case and to\nfigure quality reduction in the best case. Pylustrator a open source library\nbased on the Matplotlib aims to fill this gap and provides a tool to easily\ngenerate the code necessary to compose publication figures from single plots.\nIt provides a graphical user interface where the user can interactively compose\nthe figures. All changes are tracked and converted to code that is\nautomatically integrated into the calling script file. Thus, this software\nprovides the missing link from raw data to the complete plot published in\nscientific journals and thus contributes to the transparency of the complete\nevaluation procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.00279v2"
    },
    {
        "title": "Hash-Based Ray Path Prediction: Skipping BVH Traversal Computation by\n  Exploiting Ray Locality",
        "authors": [
            "Francois Demoullin",
            "Ayub Gubran",
            "Tor Aamodt"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  State-of-the-art ray tracing techniques operate on hierarchical acceleration\nstructures such as BVH trees which wrap objects in a scene into bounding\nvolumes of decreasing sizes. Acceleration structures reduce the amount of\nray-scene intersections that a ray has to perform to find the intersecting\nobject. However, we observe a large amount of redundancy when rays are\ntraversing these acceleration structures. While modern acceleration structures\nexplore the spatial organization of the scene, they neglect similarities\nbetween rays that traverse the structures and thereby cause redundant\ntraversals. This paper provides a limit study of a new promising technique,\nHash-Based Ray Path Prediction (HRPP), which exploits the similarity between\nrays to predict leaf nodes to avoid redundant acceleration structure\ntraversals. Our data shows that acceleration structure traversal consumes a\nsignificant proportion of the ray tracing rendering time regardless of the\nplatform or the target image quality. Our study quantifies unused ray locality\nand evaluates the theoretical potential for improved ray traversal performance\nfor both coherent and seemingly incoherent rays. We show that HRPP is able to\nskip, on average, 40% of all hit-all traversal computations.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.01304v1"
    },
    {
        "title": "Interactive Light Field Tilt-Shift Refocus with Generalized\n  Shift-and-Sum",
        "authors": [
            "Martin Alain",
            "Weston Aenchbacher",
            "Aljosa Smolic"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Since their introduction more than two decades ago, light fields have gained\nconsiderable interest in graphics and vision communities due to their ability\nto provide the user with interactive visual content. One of the earliest and\nmost common light field operations is digital refocus, enabling the user to\nchoose the focus and depth-of-field for the image after capture. A common\ninteractive method for such an operation utilizes disparity estimations,\nreadily available from the light field, to allow the user to point-and-click on\nthe image to chose the location of the refocus plane.\n  In this paper, we address the interactivity of a lesser-known light field\noperation: refocus to a non-frontoparallel plane, simulating the result of\ntraditional tilt-shift photography. For this purpose we introduce a generalized\nshift-and-sum framework. Further, we show that the inclusion of depth\ninformation allows for intuitive interactive methods for placement of the\nrefocus plane. In addition to refocusing, light fields also enable the user to\ninteract with the viewpoint, which can be easily included in the proposed\ngeneralized shift-and-sum framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.04699v1"
    },
    {
        "title": "Point cloud ridge-valley feature enhancement based on position and\n  normal guidance",
        "authors": [
            "Jianhui Nie",
            "Zhaochen Zhang",
            "Ye Liu",
            "Hao Gao",
            "Feng Xu",
            "WenKai Shi"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Ridge-valley features are important elements of point clouds, as they contain\nrich surface information. To recognize these features from point clouds, this\npaper introduces an extreme point distance (EPD) criterion with scale\nindependence. Compared with traditional methods, the EPD greatly reduces the\nnumber of potential feature points and improves the robustness of multiscale\nfeature point recognition. On this basis, a feature enhancement algorithm based\non user priori guidance is proposed that adjusts the coordinates of the feature\narea by solving an objective equation containing the expected position and\nnormal constraints. Since the expected normal can be expressed as a function of\nneighborhood point coordinates, the above objective equation can be converted\ninto linear sparse equations with enhanced feature positions as variables, and\nthus, the closed solution can be obtained. In addition, a parameterization\nmethod for scattered point clouds based on feature line guidance is proposed,\nwhich reduces the number of unknowns by 2/3 and eliminates lateral sliding in\nthe direction perpendicular to feature lines. Finally, the application of the\nalgorithm in multiscale ridge-valley feature recognition, freeform surface\nfeature enhancement and computer-aided design (CAD) workpiece sharp feature\nrestoration verifies its effectiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.04942v1"
    },
    {
        "title": "PRS-Net: Planar Reflective Symmetry Detection Net for 3D Models",
        "authors": [
            "Lin Gao",
            "Ling-Xiao Zhang",
            "Hsien-Yu Meng",
            "Yi-Hui Ren",
            "Yu-Kun Lai",
            "Leif Kobbelt"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In geometry processing, symmetry is a universal type of high-level structural\ninformation of 3D models and benefits many geometry processing tasks including\nshape segmentation, alignment, matching, and completion. Thus it is an\nimportant problem to analyze various symmetry forms of 3D shapes. Planar\nreflective symmetry is the most fundamental one. Traditional methods based on\nspatial sampling can be time-consuming and may not be able to identify all the\nsymmetry planes. In this paper, we present a novel learning framework to\nautomatically discover global planar reflective symmetry of a 3D shape. Our\nframework trains an unsupervised 3D convolutional neural network to extract\nglobal model features and then outputs possible global symmetry parameters,\nwhere input shapes are represented using voxels. We introduce a dedicated\nsymmetry distance loss along with a regularization loss to avoid generating\nduplicated symmetry planes. Our network can also identify generalized cylinders\nby predicting their rotation axes. We further provide a method to remove\ninvalid and duplicated planes and axes. We demonstrate that our method is able\nto produce reliable and accurate results. Our neural network based method is\nhundreds of times faster than the state-of-the-art methods, which are based on\nsampling. Our method is also robust even with noisy or incomplete input\nsurfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.06511v6"
    },
    {
        "title": "Animating Landscape: Self-Supervised Learning of Decoupled Motion and\n  Appearance for Single-Image Video Synthesis",
        "authors": [
            "Yuki Endo",
            "Yoshihiro Kanamori",
            "Shigeru Kuriyama"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Automatic generation of a high-quality video from a single image remains a\nchallenging task despite the recent advances in deep generative models. This\npaper proposes a method that can create a high-resolution, long-term animation\nusing convolutional neural networks (CNNs) from a single landscape image where\nwe mainly focus on skies and waters. Our key observation is that the motion\n(e.g., moving clouds) and appearance (e.g., time-varying colors in the sky) in\nnatural scenes have different time scales. We thus learn them separately and\npredict them with decoupled control while handling future uncertainty in both\npredictions by introducing latent codes. Unlike previous methods that infer\noutput frames directly, our CNNs predict spatially-smooth intermediate data,\ni.e., for motion, flow fields for warping, and for appearance, color transfer\nmaps, via self-supervised learning, i.e., without explicitly-provided ground\ntruth. These intermediate data are applied not to each previous output frame,\nbut to the input image only once for each output frame. This design is crucial\nto alleviate error accumulation in long-term predictions, which is the\nessential problem in previous recurrent approaches. The output frames can be\nlooped like cinemagraph, and also be controlled directly by specifying latent\ncodes or indirectly via visual annotations. We demonstrate the effectiveness of\nour method through comparisons with the state-of-the-arts on video prediction\nas well as appearance manipulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.07192v1"
    },
    {
        "title": "Gloss, Color and Topography Scanning for Reproducing a Painting's\n  Appearance using 3D printing",
        "authors": [
            "Willemijn Elkhuizen",
            "Tessa Essers",
            "Yu Song",
            "Jo Geraedts",
            "Clemens Weijkamp",
            "Joris Dik",
            "Sylvia Pont"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  High fidelity reproductions of paintings provide new opportunities to museums\nin preserving and providing access to cultural heritage. This paper presents an\nintegrated system which is able to capture and fabricate color, topography and\ngloss of a painting, of which gloss capturing forms the most important\ncontribution. A 3D imaging system, using fringe-encoded stereo imaging, is\nextended to capture spatially-varying gloss, utilizing specular reflectance\npolarization. The gloss is measured by sampling the specular reflection around\nBrewster's angle, where these reflections are effectively polarized, and can be\nseparated from the unpolarized, diffuse reflectance. Off-center gloss\nmeasurements are calibrated relative to the center measurement. Off-specular\ngloss measurements, following from local variation of the surface normal, are\nmasked based on the height map and corrected. Shadowed regions, caused by the\n3D relief, are treated similarly. The area of a single capture is approximately\n180x90mm at a resolution of 25x25micron. Aligned color, height, and gloss tiles\nare stitched together, registering overlapping color areas. These maps are\ninputs for a 3D printer. Two paintings were reproduced to verify the\neffectiveness and efficiency of the proposed system. One painting was scanned\nfour times, consecutively rotated by 90 degrees, to evaluate the influence of\nthe scanning system geometric configuration on the gloss measurement.\nExperimental results show that the method is sufficiently fast for practical\napplication. The results can well be used for the purpose of physical\nreproduction and other applications needing first-order estimates of the\nappearance. Our method to extend appearance scanning with gloss measurements is\na valuable addition in the quest for realistic reproductions, in terms of its\npractical applicability and its perceptual added value, when added to color and\ntopography.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.10836v1"
    },
    {
        "title": "LaplacianNet: Learning on 3D Meshes with Laplacian Encoding and Pooling",
        "authors": [
            "Yi-Ling Qiao",
            "Lin Gao",
            "Jie Yang",
            "Paul L. Rosin",
            "Yu-Kun Lai",
            "Xilin Chen"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  3D models are commonly used in computer vision and graphics. With the wider\navailability of mesh data, an efficient and intrinsic deep learning approach to\nprocessing 3D meshes is in great need. Unlike images, 3D meshes have irregular\nconnectivity, requiring careful design to capture relations in the data. To\nutilize the topology information while staying robust under different\ntriangulation, we propose to encode mesh connectivity using Laplacian spectral\nanalysis, along with Mesh Pooling Blocks (MPBs) that can split the surface\ndomain into local pooling patches and aggregate global information among them.\nWe build a mesh hierarchy from fine to coarse using Laplacian spectral\nclustering, which is flexible under isometric transformation. Inside the MPBs\nthere are pooling layers to collect local information and multi-layer\nperceptrons to compute vertex features with increasing complexity. To obtain\nthe relationships among different clusters, we introduce a Correlation Net to\ncompute a correlation matrix, which can aggregate the features globally by\nmatrix multiplication with cluster features. Our network architecture is\nflexible enough to be used on meshes with different numbers of vertices. We\nconduct several experiments including shape segmentation and classification,\nand our LaplacianNet outperforms state-of-the-art algorithms for these tasks on\nShapeNet and COSEG datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.14063v1"
    },
    {
        "title": "A Feature-aware SPH for Isotropic Unstructured Mesh Generation",
        "authors": [
            "Zhe Ji",
            "Lin Fu",
            "Xiangyu Hu",
            "Nikolaus Adams"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we present a feature-aware SPH method for the concurrent and\nautomated isotropic unstructured mesh generation. Two additional objectives are\nachieved with the proposed method compared to the original SPH-based mesh\ngenerator (Fu et al., 2019). First, a feature boundary correction term is\nintroduced to address the issue of incomplete kernel support at the boundary\nvicinity. The mesh generation of feature curves, feature surfaces and volumes\ncan be handled concurrently without explicitly following a dimensional\nsequence. Second, a two-phase model is proposed to characterize the\nmesh-generation procedure by a feature-size-adaptation phase and a\nmesh-quality-optimization phase. By proposing a new error measurement criterion\nand an adaptive control system with two sets of simulation parameters, the\nobjectives of faster feature-size adaptation and local mesh-quality improvement\nare merged into a consistent framework. The proposed method is validated with a\nset of 2D and 3D numerical tests with different complexities and scales. The\nresults demonstrate that high-quality meshes are generated with a significant\nspeedup of convergence.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.01061v1"
    },
    {
        "title": "STD-Net: Structure-preserving and Topology-adaptive Deformation Network\n  for 3D Reconstruction from a Single Image",
        "authors": [
            "Aihua Mao",
            "Canglan Dai",
            "Lin Gao",
            "Ying He",
            "Yong-jin Liu"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  3D reconstruction from a single view image is a long-standing prob-lem in\ncomputer vision. Various methods based on different shape representations(such\nas point cloud or volumetric representations) have been proposed. However,the\n3D shape reconstruction with fine details and complex structures are still\nchal-lenging and have not yet be solved. Thanks to the recent advance of the\ndeepshape representations, it becomes promising to learn the structure and\ndetail rep-resentation using deep neural networks. In this paper, we propose a\nnovel methodcalled STD-Net to reconstruct the 3D models utilizing the mesh\nrepresentationthat is well suitable for characterizing complex structure and\ngeometry details.To reconstruct complex 3D mesh models with fine details, our\nmethod consists of(1) an auto-encoder network for recovering the structure of\nan object with bound-ing box representation from a single image, (2) a\ntopology-adaptive graph CNNfor updating vertex position for meshes of complex\ntopology, and (3) an unifiedmesh deformation block that deforms the structural\nboxes into structure-awaremeshed models. Experimental results on the images\nfrom ShapeNet show that ourproposed STD-Net has better performance than other\nstate-of-the-art methods onreconstructing 3D objects with complex structures\nand fine geometric details.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.03551v1"
    },
    {
        "title": "Enabling Viewpoint Learning through Dynamic Label Generation",
        "authors": [
            "Michael Schelling",
            "Pedro Hermosilla",
            "Pere-Pau Vazquez",
            "Timo Ropinski"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Optimal viewpoint prediction is an essential task in many computer graphics\napplications. Unfortunately, common viewpoint qualities suffer from two major\ndrawbacks: dependency on clean surface meshes, which are not always available,\nand the lack of closed-form expressions, which requires a costly search\ninvolving rendering. To overcome these limitations we propose to separate\nviewpoint selection from rendering through an end-to-end learning approach,\nwhereby we reduce the influence of the mesh quality by predicting viewpoints\nfrom unstructured point clouds instead of polygonal meshes. While this makes\nour approach insensitive to the mesh discretization during evaluation, it only\nbecomes possible when resolving label ambiguities that arise in this context.\nTherefore, we additionally propose to incorporate the label generation into the\ntraining procedure, making the label decision adaptive to the current network\npredictions. We show how our proposed approach allows for learning viewpoint\npredictions for models from different object categories and for different\nviewpoint qualities. Additionally, we show that prediction times are reduced\nfrom several minutes to a fraction of a second, as compared to state-of-the-art\n(SOTA) viewpoint quality evaluation. We will further release the code and\ntraining data, which will to our knowledge be the biggest viewpoint quality\ndataset available.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.04651v2"
    },
    {
        "title": "Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation",
        "authors": [
            "Jiazhao Zhang",
            "Chenyang Zhu",
            "Lintao Zheng",
            "Kai Xu"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Online semantic 3D segmentation in company with real-time RGB-D\nreconstruction poses special challenges such as how to perform 3D convolution\ndirectly over the progressively fused 3D geometric data, and how to smartly\nfuse information from frame to frame. We propose a novel fusion-aware 3D point\nconvolution which operates directly on the geometric surface being\nreconstructed and exploits effectively the inter-frame correlation for high\nquality 3D feature learning. This is enabled by a dedicated dynamic data\nstructure which organizes the online acquired point cloud with global-local\ntrees. Globally, we compile the online reconstructed 3D points into an\nincrementally growing coordinate interval tree, enabling fast point insertion\nand neighborhood query. Locally, we maintain the neighborhood information for\neach point using an octree whose construction benefits from the fast query of\nthe global tree.Both levels of trees update dynamically and help the 3D\nconvolution effectively exploits the temporal coherence for effective\ninformation fusion across RGB-D frames.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.06233v4"
    },
    {
        "title": "Automatic Modelling of Human Musculoskeletal Ligaments -- Framework\n  Overview and Model Quality Evaluation",
        "authors": [
            "Noura Hamze",
            "Lukas Nocker",
            "Nikolaus Rauch",
            "Markus Walzthöni",
            "Fabio Carrillo",
            "Philipp Fürnstahl",
            "Matthias Harders"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Accurate segmentation of connective soft tissues is still a challenging task,\nwhich hinders the generation of corresponding geometric models for\nbiomechanical computations. Alternatively, one could predict ligament insertion\nsites and then approximate the shapes, based on anatomical knowledge and\nmorphological studies. Here, we describe a corresponding integrated framework\nfor the automatic modelling of human musculoskeletal ligaments. We combine\nstatistical shape modelling with geometric algorithms to automatically identify\ninsertion sites, based on which geometric surface and volume meshes are\ncreated. For demonstrating a clinical use case, the framework has been applied\nto generate models of the interosseous membrane in the forearm. For the\nadoption to the forearm anatomy, ligament insertion sites in the statistical\nmodel were defined according to anatomical predictions following an approach\nproposed in prior work. For evaluation we compared the generated sites, as well\nas the ligament shapes, to data obtained from a cadaveric study, involving five\nforearms with a total of 15 ligaments. Our framework permitted the creation of\n3D models approximating ligaments' shapes with good fidelity. However, we found\nthat the statistical model trained with the state-of-the-art prediction of the\ninsertion sites was not always reliable. Using that model, average mean square\nerrors as well as Hausdorff distances of the meshes increased by more than one\norder of magnitude, as compared to employing the known insertion locations of\nthe cadaveric study. Using the latter an average mean square error of 0.59 mm\nand an average Hausdorff distance of less than 7 mm resulted, for the complete\nset of ligaments. In conclusion, the presented approach for generating ligament\nshapes from insertion points appears to be feasible but the detection of the\ninsertion sites with a SSM is too inaccurate.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.11025v1"
    },
    {
        "title": "Virtual reality for 3D histology: multi-scale visualization of organs\n  with interactive feature exploration",
        "authors": [
            "Kaisa Liimatainen",
            "Leena Latonen",
            "Masi Valkonen",
            "Kimmo Kartasalo",
            "Pekka Ruusuvuori"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Virtual reality (VR) enables data visualization in an immersive and engaging\nmanner, and it can be used for creating ways to explore scientific data. Here,\nwe use VR for visualization of 3D histology data, creating a novel interface\nfor digital pathology. Our contribution includes 3D modeling of a whole organ\nand embedded objects of interest, fusing the models with associated\nquantitative features and full resolution serial section patches, and\nimplementing the virtual reality application. Our VR application is multi-scale\nin nature, covering two object levels representing different ranges of detail,\nnamely organ level and sub-organ level. In addition, the application includes\nseveral data layers, including the measured histology image layer and multiple\nrepresentations of quantitative features computed from the histology. In this\ninteractive VR application, the user can set visualization properties, select\ndifferent samples and features, and interact with various objects. In this\nwork, we used whole mouse prostates (organ level) with prostate cancer tumors\n(sub-organ objects of interest) as example cases, and included quantitative\nhistological features relevant for tumor biology in the VR model. Due to\nautomated processing of the histology data, our application can be easily\nadopted to visualize other organs and pathologies from various origins. Our\napplication enables a novel way for exploration of high-resolution,\nmultidimensional data for biomedical research purposes, and can also be used in\nteaching and researcher training.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.11148v1"
    },
    {
        "title": "Iconify: Converting Photographs into Icons",
        "authors": [
            "Takuro Karamatsu",
            "Gibran Benitez-Garcia",
            "Keiji Yanai",
            "Seiichi Uchida"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we tackle a challenging domain conversion task between photo\nand icon images. Although icons often originate from real object images (i.e.,\nphotographs), severe abstractions and simplifications are applied to generate\nicon images by professional graphic designers. Moreover, there is no one-to-one\ncorrespondence between the two domains, for this reason we cannot use it as the\nground-truth for learning a direct conversion function. Since generative\nadversarial networks (GAN) can undertake the problem of domain conversion\nwithout any correspondence, we test CycleGAN and UNIT to generate icons from\nobjects segmented from photo images. Our experiments with several image\ndatasets prove that CycleGAN learns sufficient abstraction and simplification\nability to generate icon-like images.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.03179v1"
    },
    {
        "title": "NiLBS: Neural Inverse Linear Blend Skinning",
        "authors": [
            "Timothy Jeruzalski",
            "David I. W. Levin",
            "Alec Jacobson",
            "Paul Lalonde",
            "Mohammad Norouzi",
            "Andrea Tagliasacchi"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this technical report, we investigate efficient representations of\narticulated objects (e.g. human bodies), which is an important problem in\ncomputer vision and graphics. To deform articulated geometry, existing\napproaches represent objects as meshes and deform them using \"skinning\"\ntechniques. The skinning operation allows a wide range of deformations to be\nachieved with a small number of control parameters. This paper introduces a\nmethod to invert the deformations undergone via traditional skinning techniques\nvia a neural network parameterized by pose. The ability to invert these\ndeformations allows values (e.g., distance function, signed distance function,\noccupancy) to be pre-computed at rest pose, and then efficiently queried when\nthe character is deformed. We leave empirical evaluation of our approach to\nfuture work.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.05980v1"
    },
    {
        "title": "Robust and efficient tool path generation for poor-quality triangular\n  mesh surface machining",
        "authors": [
            "Qiang Zou"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper presents a new method to generate iso-scallop tool paths for\ntriangular mesh surfaces. With the popularity of 3D scanning techniques,\nscanning-derived mesh surfaces have seen a significant increase in their\napplication to machining. Quite often, such mesh surfaces exhibit defects such\nas noises, which differentiate them from the good-quality mesh surfaces\nprevious research work focuses on. To generate tool paths for such poor-quality\nmesh surfaces, the primary challenge lies in robustness against the defects. In\nthis work, a robust tool path generation method is proposed for poor-quality\nmesh surfaces. In addition to robustness, the method is quite efficient,\nproviding the benefit of faster iterations and improved integration between\nscanning and machining. The fundamental principle of the method is to convert\nthe tool path generation problem to the heat diffusion problem that has robust\nand efficient algorithms available. The effectiveness of the method will be\ndemonstrated by a series of case studies and comparisons.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.09136v1"
    },
    {
        "title": "A scriptable, generative modelling system for dynamic 3D meshes",
        "authors": [
            "Jon McCormack",
            "Ben Porter",
            "James Wetter"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We describe a flexible, script-based system for the procedural generation and\nanimation of 3D geometry. Dynamic triangular meshes are generated through the\nreal-time execution of scripts written in the Lua programming language. Tight\nintegration between the programming environment, runtime engine and graphics\nvisualisation enables a workflow between coding and visual results that\nencourages experimentation and rapid prototyping. The system has been used\nsuccessfully to generate a variety of complex, dynamic organic forms including\ncomplex branching structures, scalable symmetric manifolds and abstract organic\nforms. We use examples in each of these areas to detail the main features of\nthe system, which include a set of flexible 3D mesh operations integrated with\na Lua-based L-system interpreter that creates geometry using generalised\ncylinders.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.10354v1"
    },
    {
        "title": "Deep Feature-preserving Normal Estimation for Point Cloud Filtering",
        "authors": [
            "Dening Lu",
            "Xuequan Lu",
            "Yangxing Sun",
            "Jun Wang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Point cloud filtering, the main bottleneck of which is removing noise\n(outliers) while preserving geometric features, is a fundamental problem in 3D\nfield. The two-step schemes involving normal estimation and position update\nhave been shown to produce promising results. Nevertheless, the current normal\nestimation methods including optimization ones and deep learning ones, often\neither have limited automation or cannot preserve sharp features. In this\npaper, we propose a novel feature-preserving normal estimation method for point\ncloud filtering with preserving geometric features. It is a learning method and\nthus achieves automatic prediction for normals. For training phase, we first\ngenerate patch based samples which are then fed to a classification network to\nclassify feature and non-feature points. We finally train the samples of\nfeature and non-feature points separately, to achieve decent results. Regarding\ntesting, given a noisy point cloud, its normals can be automatically estimated.\nFor further point cloud filtering, we iterate the above normal estimation and a\ncurrent position update algorithm for a few times. Various experiments\ndemonstrate that our method outperforms state-of-the-art normal estimation\nmethods and point cloud filtering techniques, in terms of both quality and\nquantity.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11563v1"
    },
    {
        "title": "Deep Photon Mapping",
        "authors": [
            "Shilin Zhu",
            "Zexiang Xu",
            "Henrik Wann Jensen",
            "Hao Su",
            "Ravi Ramamoorthi"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Recently, deep learning-based denoising approaches have led to dramatic\nimprovements in low sample-count Monte Carlo rendering. These approaches are\naimed at path tracing, which is not ideal for simulating challenging light\ntransport effects like caustics, where photon mapping is the method of choice.\nHowever, photon mapping requires very large numbers of traced photons to\nachieve high-quality reconstructions. In this paper, we develop the first deep\nlearning-based method for particle-based rendering, and specifically focus on\nphoton density estimation, the core of all particle-based methods. We train a\nnovel deep neural network to predict a kernel function to aggregate photon\ncontributions at shading points. Our network encodes individual photons into\nper-photon features, aggregates them in the neighborhood of a shading point to\nconstruct a photon local context vector, and infers a kernel function from the\nper-photon and photon local context features. This network is easy to\nincorporate in many previous photon mapping methods (by simply swapping the\nkernel density estimator) and can produce high-quality reconstructions of\ncomplex global illumination effects like caustics with an order of magnitude\nfewer photons compared to previous photon mapping methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12069v1"
    },
    {
        "title": "Interactive Video Stylization Using Few-Shot Patch-Based Training",
        "authors": [
            "Ondřej Texler",
            "David Futschik",
            "Michal Kučera",
            "Ondřej Jamriška",
            "Šárka Sochorová",
            "Menglei Chai",
            "Sergey Tulyakov",
            "Daniel Sýkora"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we present a learning-based method to the keyframe-based video\nstylization that allows an artist to propagate the style from a few selected\nkeyframes to the rest of the sequence. Its key advantage is that the resulting\nstylization is semantically meaningful, i.e., specific parts of moving objects\nare stylized according to the artist's intention. In contrast to previous style\ntransfer techniques, our approach does not require any lengthy pre-training\nprocess nor a large training dataset. We demonstrate how to train an appearance\ntranslation network from scratch using only a few stylized exemplars while\nimplicitly preserving temporal consistency. This leads to a video stylization\nframework that supports real-time inference, parallel processing, and random\naccess to an arbitrary output frame. It can also merge the content from\nmultiple keyframes without the need to perform an explicit blending operation.\nWe demonstrate its practical utility in various interactive scenarios, where\nthe user paints over a selected keyframe and sees her style transferred to an\nexisting recorded sequence or a live video stream.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.14489v1"
    },
    {
        "title": "On Elastic Geodesic Grids and Their Planar to Spatial Deployment",
        "authors": [
            "Stefan Pillwein",
            "Kurt Leimer",
            "Michael Birsak",
            "Przemyslaw Musialski"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose a novel type of planar-to-spatial deployable structures that we\ncall elastic geodesic grids. Our approach aims at the approximation of freeform\nsurfaces with spatial grids of bent lamellas which can be deployed from a\nplanar configuration using a simple kinematic mechanism. Such elastic\nstructures are easy-to-fabricate and easy-to-deploy and approximate shapes\nwhich combine physics and aesthetics. We propose a solution based on networks\nof geodesic curves on target surfaces and we introduce a set of conditions and\nassumptions which can be closely met in practice. Our formulation allows for a\npurely geometric approach which avoids the necessity of numerical shape\noptimization by building on top of theoretical insights from differential\ngeometry. We propose a solution for the design, computation, and physical\nsimulation of elastic geodesic grids, and present several fabricated\nsmall-scale examples with varying complexity. Moreover, we provide an empirical\nproof of our method by comparing the results to laser-scans of the fabricated\nmodels. Our method is intended as a form-finding tool for elastic gridshells in\narchitecture and other creative disciplines and should give the designer an\neasy-to-handle way for the exploration of such structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00201v1"
    },
    {
        "title": "Multi-Axis Support-Free Printing of Freeform Parts with Lattice Infill\n  Structures",
        "authors": [
            "Yamin Li",
            "Kai Tang",
            "Dong He",
            "Xiangyu Wang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In additive manufacturing, infill structures are commonly used to reduce the\nweight and cost of a solid part. Currently, most infill structure generation\nmethods are based on the conventional 2.5-axis printing configuration, which,\nalthough able to satisfy the self-supporting condition on the infills, suffer\nfrom the well-known stair-case effect on the finished surface and the need of\nextensive support for overhang features. In this paper, based on the emerging\ncontinuous multi-axis printing configuration, we present a new lattice infill\nstructure generation algorithm, which is able to achieve both the\nself-supporting condition for the infills and the support-free requirement at\nthe boundary surface of the part. The algorithm critically relies on the use of\nthree mutually orthogonal geodesic distance fields that are embedded in the\ntetrahedral mesh of the solid model. The intersection between the iso-geodesic\ndistance surfaces of these three geodesic distance fields naturally forms the\ndesired lattice of infill structure, while the density of the infills can be\nconveniently controlled by adjusting the iso-values. The lattice infill pattern\nin each curved slicing layer is trimmed to conform to an Eulerian graph so to\ngenerate a continuous printing path, which can effectively reduce the nozzle\nretractions during the printing process. In addition, to cater to the\ncollision-free requirement and to improve the printing efficiency, we also\npropose a printing sequence optimization algorithm for determining a\ncollision-free order of printing of the connected lattice infills, which seeks\nto reduce the air-move length of the nozzle. Ample experiments in both computer\nsimulation and physical printing are performed, and the results give a\npreliminary confirmation of the advantages of our methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00413v1"
    },
    {
        "title": "Computational LEGO Technic Design",
        "authors": [
            "Hao Xu",
            "Ka-Hei Hui",
            "Chi-Wing Fu",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce a method to automatically compute LEGO Technic models from user\ninput sketches, optionally with motion annotations. The generated models\nresemble the input sketches with coherently-connected bricks and simple\nlayouts, while respecting the intended symmetry and mechanical properties\nexpressed in the inputs. This complex computational assembly problem involves\nan immense search space, and a much richer brick set and connection mechanisms\nthan regular LEGO. To address it, we first comprehensively model the brick\nproperties and connection mechanisms, then formulate the construction\nrequirements into an objective function, accounting for faithfulness to input\nsketch, model simplicity, and structural integrity. Next, we model the problem\nas a sketch cover, where we iteratively refine a random initial layout to cover\nthe input sketch, while guided by the objective. At last, we provide a working\nsystem to analyze the balance, stress, and assemblability of the generated\nmodel. To evaluate our method, we compared it with four baselines and\nprofessional designs by a LEGO expert, demonstrating the superiority of our\nautomatic designs. Also, we recruited several users to try our system, employed\nit to create models of varying forms and complexities, and physically built\nmost of them.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.02245v1"
    },
    {
        "title": "Guided Fine-Tuning for Large-Scale Material Transfer",
        "authors": [
            "Valentin Deschaintre",
            "George Drettakis",
            "Adrien Bousseau"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a method to transfer the appearance of one or a few exemplar\nSVBRDFs to a target image representing similar materials. Our solution is\nextremely simple: we fine-tune a deep appearance-capture network on the\nprovided exemplars, such that it learns to extract similar SVBRDF values from\nthe target image. We introduce two novel material capture and design workflows\nthat demonstrate the strength of this simple approach. Our first workflow\nallows to produce plausible SVBRDFs of large-scale objects from only a few\npictures. Specifically, users only need take a single picture of a large\nsurface and a few close-up flash pictures of some of its details. We use\nexisting methods to extract SVBRDF parameters from the close-ups, and our\nmethod to transfer these parameters to the entire surface, enabling the\nlightweight capture of surfaces several meters wide such as murals, floors and\nfurniture. In our second workflow, we provide a powerful way for users to\ncreate large SVBRDFs from internet pictures by transferring the appearance of\nexisting, pre-designed SVBRDFs. By selecting different exemplars, users can\ncontrol the materials assigned to the target image, greatly enhancing the\ncreative possibilities offered by deep appearance capture.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.03059v2"
    },
    {
        "title": "A Robust Interactive Facial Animation Editing System",
        "authors": [
            "Eloïse Berson",
            "Catherine Soladié",
            "Vincent Barrielle",
            "Nicolas Stoiber"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Over the past few years, the automatic generation of facial animation for\nvirtual characters has garnered interest among the animation research and\nindustry communities. Recent research contributions leverage machine-learning\napproaches to enable impressive capabilities at generating plausible facial\nanimation from audio and/or video signals. However, these approaches do not\naddress the problem of animation edition, meaning the need for correcting an\nunsatisfactory baseline animation or modifying the animation content itself. In\nfacial animation pipelines, the process of editing an existing animation is\njust as important and time-consuming as producing a baseline. In this work, we\npropose a new learning-based approach to easily edit a facial animation from a\nset of intuitive control parameters. To cope with high-frequency components in\nfacial movements and preserve a temporal coherency in the animation, we use a\nresolution-preserving fully convolutional neural network that maps control\nparameters to blendshapes coefficients sequences. We stack an additional\nresolution-preserving animation autoencoder after the regressor to ensure that\nthe system outputs natural-looking animation. The proposed system is robust and\ncan handle coarse, exaggerated edits from non-specialist users. It also retains\nthe high-frequency motion of the facial animation.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.09367v1"
    },
    {
        "title": "A Survey of Algorithms for Geodesic Paths and Distances",
        "authors": [
            "Keenan Crane",
            "Marco Livesu",
            "Enrico Puppo",
            "Yipeng Qin"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Numerical computation of shortest paths or geodesics on curved domains, as\nwell as the associated geodesic distance, arises in a broad range of\napplications across digital geometry processing, scientific computing, computer\ngraphics, and computer vision. Relative to Euclidean distance computation,\nthese tasks are complicated by the influence of curvature on the behavior of\nshortest paths, as well as the fact that the representation of the domain may\nitself be approximate. In spite of the difficulty of this problem, recent\nliterature has developed a wide variety of sophisticated methods that enable\nrapid queries of geodesic information, even on relatively large models. This\nsurvey reviews the major categories of approaches to the computation of\ngeodesic paths and distances, highlighting common themes and opportunities for\nfuture improvement.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.10430v1"
    },
    {
        "title": "InCorr: Interactive Data-Driven Correlation Panels for Digital Outcrop\n  Analysis",
        "authors": [
            "Thomas Ortner",
            "Andreas Walch",
            "Rebecca Nowak",
            "Robert Barnes",
            "Thomas Höllt",
            "Eduard Gröller"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Geological analysis of 3D Digital Outcrop Models (DOMs) for reconstruction of\nancient habitable environments is a key aspect of the upcoming ESA ExoMars 2022\nRosalind Franklin Rover and the NASA 2020 Rover Perseverance missions in\nseeking signs of past life on Mars. Geologists measure and interpret 3D DOMs,\ncreate sedimentary logs and combine them in `correlation panels' to map the\nextents of key geological horizons, and build a stratigraphic model to\nunderstand their position in the ancient landscape. Currently, the creation of\ncorrelation panels is completely manual and therefore time-consuming, and\ninflexible. With InCorr we present a visualization solution that encompasses a\n3D logging tool and an interactive data-driven correlation panel that evolves\nwith the stratigraphic analysis. For the creation of InCorr we closely\ncooperated with leading planetary geologists in the form of a design study. We\nverify our results by recreating an existing correlation analysis with InCorr\nand validate our correlation panel against a manually created illustration.\nFurther, we conducted a user-study with a wider circle of geologists. Our\nevaluation shows that InCorr efficiently supports the domain experts in\ntackling their research questions and that it has the potential to\nsignificantly impact how geologists work with digital outcrop representations\nin general.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11512v2"
    },
    {
        "title": "Silhouette Vectorization by Affine Scale-space",
        "authors": [
            "Yuchen He",
            "Sung Ha Kang",
            "Jean-Michel Morel"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Silhouettes or 2D planar shapes are extremely important in human\ncommunication, which involves many logos, graphics symbols and fonts in vector\nform. Many more shapes can be extracted from image by binarization or\nsegmentation, thus in raster form that requires a vectorization. There is a\nneed for disposing of a mathematically well defined and justified shape\nvectorization process, which in addition provides a minimal set of control\npoints with geometric meaning. In this paper we propose a silhouette\nvectorization method which extracts the outline of a 2D shape from a raster\nbinary image, and converts it to a combination of cubic B\\'{e}zier polygons and\nperfect circles. Starting from the boundary curvature extrema computed at\nsub-pixel level, we identify a set of control points based on the affine\nscale-space induced by the outline. These control points capture similarity\ninvariant geometric features of the given silhouette and give precise locations\nof the shape's corners.of the given silhouette. Then, piecewise B\\'{e}zier\ncubics are computed by least-square fitting combined with an adaptive splitting\nto guarantee a predefined accuracy. When there are no curvature extrema\nidentified, either the outline is recognized as a circle using the\nisoperimetric inequality, or a pair of the most distant outline points are\nchosen to initiate the fitting. Given their construction, most of our control\npoints are geometrically stable under affine transformations. By comparing with\nother feature detectors, we show that our method can be used as a reliable\nfeature point detector for silhouettes. Compared to state-of-the-art image\nvectorization software, our algorithm demonstrates superior reduction on the\nnumber of control points, while maintaining high accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.12117v1"
    },
    {
        "title": "A Progressive Approach to Scalar Field Topology",
        "authors": [
            "Jules Vidal",
            "Pierre Guillou",
            "Julien Tierny"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper introduces progressive algorithms for the topological analysis of\nscalar data. Our approach is based on a hierarchical representation of the\ninput data and the fast identification of topologically invariant vertices,\nwhich are vertices that have no impact on the topological description of the\ndata and for which we show that no computation is required as they are\nintroduced in the hierarchy. This enables the definition of efficient\ncoarse-to-fine topological algorithms, which leverage fast update mechanisms\nfor ordinary vertices and avoid computation for the topologically invariant\nones. We demonstrate our approach with two examples of topological algorithms\n(critical point extraction and persistence diagram computation), which generate\ninterpretable outputs upon interruption requests and which progressively refine\nthem otherwise. Experiments on real-life datasets illustrate that our\nprogressive strategy, in addition to the continuous visual feedback it\nprovides, even improves run time performance with regard to non-progressive\nalgorithms and we describe further accelerations with shared-memory\nparallelism. We illustrate the utility of our approach in batch-mode and\ninteractive setups, where it respectively enables the control of the execution\ntime of complete topological pipelines as well as previews of the topological\nfeatures found in a dataset, with progressive updates delivered within\ninteractive times.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.14766v2"
    },
    {
        "title": "Understanding the Stability of Deep Control Policies for Biped\n  Locomotion",
        "authors": [
            "Hwangpil Park",
            "Ri Yu",
            "Yoonsang Lee",
            "Kyungho Lee",
            "Jehee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Achieving stability and robustness is the primary goal of biped locomotion\ncontrol. Recently, deep reinforce learning (DRL) has attracted great attention\nas a general methodology for constructing biped control policies and\ndemonstrated significant improvements over the previous state-of-the-art.\nAlthough deep control policies have advantages over previous controller design\napproaches, many questions remain unanswered. Are deep control policies as\nrobust as human walking? Does simulated walking use similar strategies as human\nwalking to maintain balance? Does a particular gait pattern similarly affect\nhuman and simulated walking? What do deep policies learn to achieve improved\ngait stability? The goal of this study is to answer these questions by\nevaluating the push-recovery stability of deep policies compared to human\nsubjects and a previous feedback controller. We also conducted experiments to\nevaluate the effectiveness of variants of DRL algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.15242v1"
    },
    {
        "title": "Dynamic Facial Asset and Rig Generation from a Single Scan",
        "authors": [
            "Jiaman Li",
            "Zhengfei Kuang",
            "Yajie Zhao",
            "Mingming He",
            "Karl Bladin",
            "Hao Li"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The creation of high-fidelity computer-generated (CG) characters used in film\nand gaming requires intensive manual labor and a comprehensive set of facial\nassets to be captured with complex hardware, resulting in high cost and long\nproduction cycles. In order to simplify and accelerate this digitization\nprocess, we propose a framework for the automatic generation of high-quality\ndynamic facial assets, including rigs which can be readily deployed for artists\nto polish. Our framework takes a single scan as input to generate a set of\npersonalized blendshapes, dynamic and physically-based textures, as well as\nsecondary facial components (e.g., teeth and eyeballs). Built upon a facial\ndatabase consisting of pore-level details, with over $4,000$ scans of varying\nexpressions and identities, we adopt a self-supervised neural network to learn\npersonalized blendshapes from a set of template expressions. We also model the\njoint distribution between identities and expressions, enabling the inference\nof the full set of personalized blendshapes with dynamic appearances from a\nsingle neutral input scan. Our generated personalized face rig assets are\nseamlessly compatible with cutting-edge industry pipelines for facial animation\nand rendering. We demonstrate that our framework is robust and effective by\ninferring on a wide range of novel subjects, and illustrate compelling\nrendering results while animating faces with generated customized\nphysically-based dynamic textures.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.00560v2"
    },
    {
        "title": "Light Stage Super-Resolution: Continuous High-Frequency Relighting",
        "authors": [
            "Tiancheng Sun",
            "Zexiang Xu",
            "Xiuming Zhang",
            "Sean Fanello",
            "Christoph Rhemann",
            "Paul Debevec",
            "Yun-Ta Tsai",
            "Jonathan T. Barron",
            "Ravi Ramamoorthi"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The light stage has been widely used in computer graphics for the past two\ndecades, primarily to enable the relighting of human faces. By capturing the\nappearance of the human subject under different light sources, one obtains the\nlight transport matrix of that subject, which enables image-based relighting in\nnovel environments. However, due to the finite number of lights in the stage,\nthe light transport matrix only represents a sparse sampling on the entire\nsphere. As a consequence, relighting the subject with a point light or a\ndirectional source that does not coincide exactly with one of the lights in the\nstage requires interpolation and resampling the images corresponding to nearby\nlights, and this leads to ghosting shadows, aliased specularities, and other\nartifacts. To ameliorate these artifacts and produce better results under\narbitrary high-frequency lighting, this paper proposes a learning-based\nsolution for the \"super-resolution\" of scans of human faces taken from a light\nstage. Given an arbitrary \"query\" light direction, our method aggregates the\ncaptured images corresponding to neighboring lights in the stage, and uses a\nneural network to synthesize a rendering of the face that appears to be\nilluminated by a \"virtual\" light source at the query location. This neural\nnetwork must circumvent the inherent aliasing and regularity of the light stage\ndata that was used for training, which we accomplish through the use of\nregularized traditional interpolation methods within our network. Our learned\nmodel is able to produce renderings for arbitrary light directions that exhibit\nrealistic shadows and specular highlights, and is able to generalize across a\nwide variety of subjects.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08888v1"
    },
    {
        "title": "A Convenient Generalization of Schlick's Bias and Gain Functions",
        "authors": [
            "Jonathan T. Barron"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a generalization of Schlick's bias and gain functions -- simple\nparametric curve-shaped functions for inputs in [0, 1]. Our single function\nincludes both bias and gain as special cases, and is able to describe other\nsmooth and monotonic curves with variable degrees of asymmetry.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.09714v1"
    },
    {
        "title": "SEG-MAT: 3D Shape Segmentation Using Medial Axis Transform",
        "authors": [
            "Cheng Lin",
            "Lingjie Liu",
            "Changjian Li",
            "Leif Kobbelt",
            "Bin Wang",
            "Shiqing Xin",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Segmenting arbitrary 3D objects into constituent parts that are structurally\nmeaningful is a fundamental problem encountered in a wide range of computer\ngraphics applications. Existing methods for 3D shape segmentation suffer from\ncomplex geometry processing and heavy computation caused by using low-level\nfeatures and fragmented segmentation results due to the lack of global\nconsideration. We present an efficient method, called SEG-MAT, based on the\nmedial axis transform (MAT) of the input shape. Specifically, with the rich\ngeometrical and structural information encoded in the MAT, we are able to\ndevelop a simple and principled approach to effectively identify the various\ntypes of junctions between different parts of a 3D shape. Extensive evaluations\nand comparisons show that our method outperforms the state-of-the-art methods\nin terms of segmentation quality and is also one order of magnitude faster.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.11488v1"
    },
    {
        "title": "Unmasking Communication Partners: A Low-Cost AI Solution for Digitally\n  Removing Head-Mounted Displays in VR-Based Telepresence",
        "authors": [
            "Philipp Ladwig",
            "Alexander Pech",
            "Ralf Dörner",
            "Christian Geiger"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Face-to-face conversation in Virtual Reality (VR) is a challenge when\nparticipants wear head-mounted displays (HMD). A significant portion of a\nparticipant's face is hidden and facial expressions are difficult to perceive.\nPast research has shown that high-fidelity face reconstruction with personal\navatars in VR is possible under laboratory conditions with high-cost hardware.\nIn this paper, we propose one of the first low-cost systems for this task which\nuses only open source, free software and affordable hardware. Our approach is\nto track the user's face underneath the HMD utilizing a Convolutional Neural\nNetwork (CNN) and generate corresponding expressions with Generative\nAdversarial Networks (GAN) for producing RGBD images of the person's face. We\nuse commodity hardware with low-cost extensions such as 3D-printed mounts and\nminiature cameras. Our approach learns end-to-end without manual intervention,\nruns in real time, and can be trained and executed on an ordinary gaming\ncomputer. We report evaluation results showing that our low-cost system does\nnot achieve the same fidelity of research prototypes using high-end hardware\nand closed source software, but it is capable of creating individual facial\navatars with person-specific characteristics in movements and expressions.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.03630v1"
    },
    {
        "title": "Slice and Dice: A Physicalization Workflow for Anatomical Edutainment",
        "authors": [
            "Renata G. Raidou",
            "M. Eduard Gröller",
            "Hsiang-Yun Wu"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  During the last decades, anatomy has become an interesting topic in\neducation---even for laymen or schoolchildren. As medical imaging techniques\nbecome increasingly sophisticated, virtual anatomical education applications\nhave emerged. Still, anatomical models are often preferred, as they facilitate\n3D localization of anatomical structures. Recently, data physicalizations\n(i.e., physical visualizations) have proven to be effective and\nengaging---sometimes, even more than their virtual counterparts. So far,\nmedical data physicalizations involve mainly 3D printing, which is still\nexpensive and cumbersome. We investigate alternative forms of physicalizations,\nwhich use readily available technologies (home printers) and inexpensive\nmaterials (paper or semi-transparent films) to generate crafts for anatomical\nedutainment. To the best of our knowledge, this is the first computer-generated\ncrafting approach within an anatomical edutainment context. Our approach\nfollows a cost-effective, simple, and easy-to-employ workflow, resulting in\nassemblable data sculptures (i.e., semi-transparent sliceforms). It primarily\nsupports volumetric data (such as CT or MRI), but mesh data can also be\nimported. An octree slices the imported volume and an optimization step\nsimplifies the slice configuration, proposing the optimal order for easy\nassembly. A packing algorithm places the resulting slices with their labels,\nannotations, and assembly instructions on a paper or transparent film of\nuser-selected size, to be printed, assembled into a sliceform, and explored. We\nconducted two user studies to assess our approach, demonstrating that it is an\ninitial positive step towards the successful creation of interactive and\nengaging anatomical physicalizations.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.05689v1"
    },
    {
        "title": "SHAD3S: A model to Sketch, Shade and Shadow",
        "authors": [
            "Raghav B. Venkataramaiyer",
            "Abhishek Joshi",
            "Saisha Narang",
            "Vinay P. Namboodiri"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Hatching is a common method used by artists to accentuate the third dimension\nof a sketch, and to illuminate the scene. Our system SHAD3S attempts to compete\nwith a human at hatching generic three-dimensional (3D) shapes, and also tries\nto assist her in a form exploration exercise. The novelty of our approach lies\nin the fact that we make no assumptions about the input other than that it\nrepresents a 3D shape, and yet, given a contextual information of illumination\nand texture, we synthesise an accurate hatch pattern over the sketch, without\naccess to 3D or pseudo 3D. In the process, we contribute towards a) a cheap yet\neffective method to synthesise a sufficiently large high fidelity dataset,\npertinent to task; b) creating a pipeline with conditional generative\nadversarial network (CGAN); and c) creating an interactive utility with GIMP,\nthat is a tool for artists to engage with automated hatching or a\nform-exploration exercise. User evaluation of the tool suggests that the model\nperformance does generalise satisfactorily over diverse input, both in terms of\nstyle as well as shape. A simple comparison of inception scores suggest that\nthe generated distribution is as diverse as the ground truth.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.06822v3"
    },
    {
        "title": "Normalized Weighting Schemes for Image Interpolation Algorithms",
        "authors": [
            "Olivier Rukundo"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Image interpolation algorithms pervade many modern image processing and\nanalysis applications. However, when their weighting schemes inefficiently\ngenerate very unrealistic estimates, they may negatively affect the performance\nof the end user applications. Therefore, in this work, the author introduced\nfour weighting schemes based on some geometric shapes for digital image\ninterpolation operations. And, the quantity used to express the extent of each\nshape weight was the normalized area, especially when the sums of areas\nexceeded a unit square size. The introduced four weighting schemes are based on\nthe minimum side based diameter (MD) of a regular tetragon, hypotenuse based\nradius (HR), the virtual pixel length based height for the area of the triangle\n(AT), and the virtual pixel length for hypotenuse based radius for the area of\nthe circle (AC). At the smaller scaling ratio, the image interpolation\nalgorithm based on the HR scheme scored the highest at 66.6 % among non\ntraditional image interpolation algorithms presented. But, at the higher\nscaling ratio, the AC scheme based image interpolation algorithm scored the\nhighest at 66.6 % among non traditional algorithms presented and, here, its\nimage interpolation quality was generally superior or comparable to the quality\nof images interpolated by both non traditional and traditional algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.08559v6"
    },
    {
        "title": "FTK: A Simplicial Spacetime Meshing Framework for Robust and Scalable\n  Feature Tracking",
        "authors": [
            "Hanqi Guo",
            "David Lenz",
            "Jiayi Xu",
            "Xin Liang",
            "Wenbin He",
            "Iulian R. Grindeanu",
            "Han-Wei Shen",
            "Tom Peterka",
            "Todd Munson",
            "Ian Foster"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present the Feature Tracking Kit (FTK), a framework that simplifies,\nscales, and delivers various feature-tracking algorithms for scientific data.\nThe key of FTK is our high-dimensional simplicial meshing scheme that\ngeneralizes both regular and unstructured spatial meshes to spacetime while\ntessellating spacetime mesh elements into simplices. The benefits of using\nsimplicial spacetime meshes include (1) reducing ambiguity cases for feature\nextraction and tracking, (2) simplifying the handling of degeneracies using\nsymbolic perturbations, and (3) enabling scalable and parallel processing. The\nuse of simplicial spacetime meshing simplifies and improves the implementation\nof several feature-tracking algorithms for critical points, quantum vortices,\nand isosurfaces. As a software framework, FTK provides end users with\nVTK/ParaView filters, Python bindings, a command line interface, and\nprogramming interfaces for feature-tracking applications. We demonstrate use\ncases as well as scalability studies through both synthetic data and scientific\napplications including Tokamak, fluid dynamics, and superconductivity\nsimulations. We also conduct end-to-end performance studies on the Summit\nsupercomputer. FTK is open-sourced under the MIT license:\nhttps://github.com/hguo/ftk\n",
        "pdf_link": "http://arxiv.org/pdf/2011.08697v2"
    },
    {
        "title": "To Explore What Isn't There -- Glyph-based Visualization for Analysis of\n  Missing Values",
        "authors": [
            "Sara Johansson Fernstad",
            "Jimmy Johansson"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper contributes a novel visualization method, Missingness Glyph, for\nanalysis and exploration of missing values in data. Missing values are a common\nchallenge in most data generating domains and may cause a range of analysis\nissues. Missingness in data may indicate potential problems in data collection\nand pre-processing, or highlight important data characteristics. While the\ndevelopment and improvement of statistical methods for dealing with missing\ndata is a research area in its own right, mainly focussing on replacing missing\nvalues with estimated values, considerably less focus has been put on\nvisualization of missing values. Nonetheless, visualization and explorative\nanalysis has great potential to support understanding of missingness in data,\nand to enable gaining of novel insights into patterns of missingness in a way\nthat statistical methods are unable to. The Missingness Glyph supports\nidentification of relevant missingness patterns in data, and is evaluated and\ncompared to two other visualization methods in context of the missingness\npatterns. The results are promising and confirms that the Missingness Glyph in\nseveral cases perform better than the alternative visualization methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.12125v1"
    },
    {
        "title": "StyleUV: Diverse and High-fidelity UV Map Generative Model",
        "authors": [
            "Myunggi Lee",
            "Wonwoong Cho",
            "Moonheum Kim",
            "David Inouye",
            "Nojun Kwak"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Reconstructing 3D human faces in the wild with the 3D Morphable Model (3DMM)\nhas become popular in recent years. While most prior work focuses on estimating\nmore robust and accurate geometry, relatively little attention has been paid to\nimproving the quality of the texture model. Meanwhile, with the advent of\nGenerative Adversarial Networks (GANs), there has been great progress in\nreconstructing realistic 2D images. Recent work demonstrates that GANs trained\nwith abundant high-quality UV maps can produce high-fidelity textures superior\nto those produced by existing methods. However, acquiring such high-quality UV\nmaps is difficult because they are expensive to acquire, requiring laborious\nprocesses to refine. In this work, we present a novel UV map generative model\nthat learns to generate diverse and realistic synthetic UV maps without\nrequiring high-quality UV maps for training. Our proposed framework can be\ntrained solely with in-the-wild images (i.e., UV maps are not required) by\nleveraging a combination of GANs and a differentiable renderer. Both\nquantitative and qualitative evaluations demonstrate that our proposed texture\nmodel produces more diverse and higher fidelity textures compared to existing\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.12893v1"
    },
    {
        "title": "An XR rapid prototyping framework for interoperability across the\n  reality spectrum",
        "authors": [
            "Efstratios Geronikolakis",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Applications of the Extended Reality (XR) spectrum, a superset of Mixed,\nAugmented and Virtual Reality, are gaining prominence and can be employed in a\nvariety of areas, such as virtual museums. Examples can be found in the areas\nof education, cultural heritage, health/treatment, entertainment, marketing,\nand more. The majority of computer graphics applications nowadays are used to\noperate only in one of the above realities. The lack of applications across the\nXR spectrum is a real shortcoming. There are many advantages resulting from\nthis problem's solution. Firstly, releasing an application across the XR\nspectrum could contribute in discovering its most suitable reality. Moreover,\nan application could be more immersive within a particular reality, depending\non its context. Furthermore, its availability increases to a broader range of\nusers. For instance, if an application is released both in Virtual and\nAugmented Reality, it is accessible to users that may lack the possession of a\nVR headset, but not of a mobile AR device. The question that arises at this\npoint, would be \"Is it possible for a full s/w application stack to be\nconverted across XR without sacrificing UI/UX in a semi-automatic way?\". It may\nbe quite difficult, depending on the architecture and application\nimplementation. Most companies nowadays support only one reality, due to their\nlack of UI/UX software architecture or resources to support the complete XR\nspectrum. In this work, we present an \"automatic reality transition\" in the\ncontext of virtual museum applications. We propose a development framework,\nwhich will automatically allow this XR transition. This framework transforms\nany XR project into different realities such as Augmented or Virtual. It also\nreduces the development time while increasing the XR availability of 3D\napplications, encouraging developers to release applications across the XR\nspectrum.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.01771v2"
    },
    {
        "title": "Instanced model simplification using combined geometric and\n  appearance-related metric",
        "authors": [
            "Sadia Tariq",
            "Anis Ur Rahman",
            "Tahir Azim",
            "Rehman Gull Khan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Evolution of 3D graphics and graphical worlds has brought issues like content\noptimization, real-time processing, rendering, and shared storage limitation\nunder consideration. Generally, different simplification approaches are used to\nmake 3D meshes viable for rendering. However, many of these approaches ignore\nvertex attributes for instanced 3D meshes. In this paper, we implement and\nevaluate a simple and improved version to simplify instanced 3D textured\nmodels. The approach uses different vertex attributes in addition to geometry\nto simplify mesh instances. The resulting simplified models demonstrate\nefficient time-space requirements and better visual quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.02570v1"
    },
    {
        "title": "ARC: Alignment-based Redirection Controller for Redirected Walking in\n  Complex Environments",
        "authors": [
            "Niall L. Williams",
            "Aniket Bera",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a novel redirected walking controller based on alignment that\nallows the user to explore large and complex virtual environments, while\nminimizing the number of collisions with obstacles in the physical environment.\nOur alignment-based redirection controller, ARC, steers the user such that\ntheir proximity to obstacles in the physical environment matches the proximity\nto obstacles in the virtual environment as closely as possible. To quantify a\ncontroller's performance in complex environments, we introduce a new metric,\nComplexity Ratio (CR), to measure the relative environment complexity and\ncharacterize the difference in navigational complexity between the physical and\nvirtual environments. Through extensive simulation-based experiments, we show\nthat ARC significantly outperforms current state-of-the-art controllers in its\nability to steer the user on a collision-free path. We also show through\nquantitative and qualitative measures of performance that our controller is\nrobust in complex environments with many obstacles. Our method is applicable to\narbitrary environments and operates without any user input or parameter\ntweaking, aside from the layout of the environments. We have implemented our\nalgorithm on the Oculus Quest head-mounted display and evaluated its\nperformance in environments with varying complexity. Our project website is\navailable at https://gamma.umd.edu/arc/.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.04912v4"
    },
    {
        "title": "Closed-form Quadrangulation of N-Sided Patches",
        "authors": [
            "Marco Tarini"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We analyze the problem of quadrangulating a $n$-sided patch, each side at its\nboundary subdivided into a given number of edges, using a single irregular\nvertex (or none, when $n = 4$) that breaks the otherwise fully regular lattice.\nWe derive, in an analytical closed-form, (1) the necessary and sufficient\nconditions that a patch must meet to admit this quadrangulation, and (2) a full\ndescription of the resulting tessellation(s).\n",
        "pdf_link": "http://arxiv.org/pdf/2101.11569v2"
    },
    {
        "title": "A Momentum-Conserving Implicit Material Point Method for Surface\n  Energies with Spatial Gradients",
        "authors": [
            "Jingyu Chen",
            "Victoria Kala",
            "Alan Marquez-Razon",
            "Elias Gueidon",
            "David A. B. Hyde",
            "Joseph Teran"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a novel Material Point Method (MPM) discretization of surface\ntension forces that arise from spatially varying surface energies. These\nvariations typically arise from surface energy dependence on temperature and/or\nconcentration. Furthermore, since the surface energy is an interfacial property\ndepending on the types of materials on either side of an interface, spatial\nvariation is required for modeling the contact angle at the triple junction\nbetween a liquid, solid and surrounding air. Our discretization is based on the\nsurface energy itself, rather than on the associated traction condition most\ncommonly used for discretization with particle methods. Our energy based\napproach automatically captures surface gradients without the explicit need to\nresolve them as in traction condition based approaches. We include an implicit\ndiscretization of thermomechanical material coupling with a novel\nparticle-based enforcement of Robin boundary conditions associated with\nconvective heating. Lastly, we design a particle resampling approach needed to\nachieve perfect conservation of linear and angular momentum with\nAffineParticle-In-Cell (APIC) [Jiang et al. 2015]. We show that our approach\nenables implicit time stepping for complex behaviors like the Marangoni effect\nand hydrophobicity/hydrophilicity. We demonstrate the robustness and utility of\nour method by simulating materials that exhibit highly diverse degrees of\nsurface tension and thermomechanical effects, such as water, wine and wax.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.12408v1"
    },
    {
        "title": "AMP: Adversarial Motion Priors for Stylized Physics-Based Character\n  Control",
        "authors": [
            "Xue Bin Peng",
            "Ze Ma",
            "Pieter Abbeel",
            "Sergey Levine",
            "Angjoo Kanazawa"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Synthesizing graceful and life-like behaviors for physically simulated\ncharacters has been a fundamental challenge in computer animation. Data-driven\nmethods that leverage motion tracking are a prominent class of techniques for\nproducing high fidelity motions for a wide range of behaviors. However, the\neffectiveness of these tracking-based methods often hinges on carefully\ndesigned objective functions, and when applied to large and diverse motion\ndatasets, these methods require significant additional machinery to select the\nappropriate motion for the character to track in a given scenario. In this\nwork, we propose to obviate the need to manually design imitation objectives\nand mechanisms for motion selection by utilizing a fully automated approach\nbased on adversarial imitation learning. High-level task objectives that the\ncharacter should perform can be specified by relatively simple reward\nfunctions, while the low-level style of the character's behaviors can be\nspecified by a dataset of unstructured motion clips, without any explicit clip\nselection or sequencing. These motion clips are used to train an adversarial\nmotion prior, which specifies style-rewards for training the character through\nreinforcement learning (RL). The adversarial RL procedure automatically selects\nwhich motion to perform, dynamically interpolating and generalizing from the\ndataset. Our system produces high-quality motions that are comparable to those\nachieved by state-of-the-art tracking-based techniques, while also being able\nto easily accommodate large datasets of unstructured motion clips. Composition\nof disparate skills emerges automatically from the motion prior, without\nrequiring a high-level motion planner or other task-specific annotations of the\nmotion clips. We demonstrate the effectiveness of our framework on a diverse\ncast of complex simulated characters and a challenging suite of motor control\ntasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.02180v2"
    },
    {
        "title": "Real-time visio-haptic interaction with static soft tissue model shaving\n  geometric and material nonlinearity",
        "authors": [
            "Igor Peterlik",
            "Mert Sedef",
            "Cagatay Basdogan",
            "Ludek Matyska"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We propose a new approach allowing visio-haptic interaction with a FE model\nof a human liver having both non-linear geometric and material properties. The\nmaterial properties used in the model are extracted from the experimental data\nof pig liver to make the simulations more realistic. Our computational approach\nconsists of two main steps: a pre-computation of the configuration space of all\npossible deformation states of the model, followed by the interpolation of the\nprecomputed data for the calculation of the reaction forces displayed to the\nuser through a haptic device during the real-time interactions. No a priori\nassumptions or modeling simplifications about the mathematical complexity of\nthe underlying soft tissue model, size and irregularity of the FE mesh are\nnecessary. We show that deformation and force response of the liver in\nsimulations are heavily influenced by the material model, boundary conditions,\npath of the loading and the type of function used for the interpolation of the\npre-computed data.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.04387v1"
    },
    {
        "title": "A Review of the State-of-the-Art on Tours for Dynamic Visualization of\n  High-dimensional Data",
        "authors": [
            "Stuart Lee",
            "Dianne Cook",
            "Natalia da Silva",
            "Ursula Laa",
            "Earo Wang",
            "Nick Spyrison",
            "H. Sherry Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This article discusses a high-dimensional visualization technique called the\ntour, which can be used to view data in more than three dimensions. We review\nthe theory and history behind the technique, as well as modern software\ndevelopments and applications of the tour that are being found across the\nsciences and machine learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.08016v2"
    },
    {
        "title": "dualFace:Two-Stage Drawing Guidance for Freehand Portrait Sketching",
        "authors": [
            "Zhengyu Huang",
            "Yichen Peng",
            "Tomohiro Hibino",
            "Chunqi Zhao",
            "Haoran Xie",
            "Tsukasa Fukusato",
            "Kazunori Miyata"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this paper, we propose dualFace, a portrait drawing interface to assist\nusers with different levels of drawing skills to complete recognizable and\nauthentic face sketches. dualFace consists of two-stage drawing assistance to\nprovide global and local visual guidance: global guidance, which helps users\ndraw contour lines of portraits (i.e., geometric structure), and local\nguidance, which helps users draws details of facial parts (which conform to\nuser-drawn contour lines), inspired by traditional artist workflows in portrait\ndrawing. In the stage of global guidance, the user draws several contour lines,\nand dualFace then searches several relevant images from an internal database\nand displays the suggested face contour lines over the background of the\ncanvas. In the stage of local guidance, we synthesize detailed portrait images\nwith a deep generative model from user-drawn contour lines, but use the\nsynthesized results as detailed drawing guidance. We conducted a user study to\nverify the effectiveness of dualFace, and we confirmed that dualFace\nsignificantly helps achieve a detailed portrait sketch. see\nhttp://www.jaist.ac.jp/~xie/dualface.html\n",
        "pdf_link": "http://arxiv.org/pdf/2104.12297v1"
    },
    {
        "title": "TransPose: Real-time 3D Human Translation and Pose Estimation with Six\n  Inertial Sensors",
        "authors": [
            "Xinyu Yi",
            "Yuxiao Zhou",
            "Feng Xu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Motion capture is facing some new possibilities brought by the inertial\nsensing technologies which do not suffer from occlusion or wide-range\nrecordings as vision-based solutions do. However, as the recorded signals are\nsparse and quite noisy, online performance and global translation estimation\nturn out to be two key difficulties. In this paper, we present TransPose, a\nDNN-based approach to perform full motion capture (with both global\ntranslations and body poses) from only 6 Inertial Measurement Units (IMUs) at\nover 90 fps. For body pose estimation, we propose a multi-stage network that\nestimates leaf-to-full joint positions as intermediate results. This design\nmakes the pose estimation much easier, and thus achieves both better accuracy\nand lower computation cost. For global translation estimation, we propose a\nsupporting-foot-based method and an RNN-based method to robustly solve for the\nglobal translations with a confidence-based fusion technique. Quantitative and\nqualitative comparisons show that our method outperforms the state-of-the-art\nlearning- and optimization-based methods with a large margin in both accuracy\nand efficiency. As a purely inertial sensor-based approach, our method is not\nlimited by environmental settings (e.g., fixed cameras), making the capture\nfree from common difficulties such as wide-range motion space and strong\nocclusion.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.04605v1"
    },
    {
        "title": "Fit4CAD: A point cloud benchmark for fitting simple geometric primitives\n  in CAD objects",
        "authors": [
            "Chiara Romanengo",
            "Andrea Raffo",
            "Yifan Qie",
            "Nabil Anwer",
            "Bianca Falcidieno"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We propose Fit4CAD, a benchmark for the evaluation and comparison of methods\nfor fitting simple geometric primitives in point clouds representing CAD\nobjects. This benchmark is meant to help both method developers and those who\nwant to identify the best performing tools. The Fit4CAD dataset is composed by\n225 high quality point clouds, each of which has been obtained by sampling a\nCAD object. The way these elements were created by using existing platforms and\ndatasets makes the benchmark easily expandable. The dataset is already split\ninto a training set and a test set. To assess performance and accuracy of the\ndifferent primitive fitting methods, various measures are defined. To\ndemonstrate the effective use of Fit4CAD, we have tested it on two methods\nbelonging to two different categories of approaches to the primitive fitting\nproblem: a clustering method based on a primitive growing framework and a\nparametric method based on the Hough transform.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.06858v3"
    },
    {
        "title": "Guided Facial Skin Color Correction",
        "authors": [
            "Keiichiro Shirai",
            "Tatsuya Baba",
            "Shunsuke Ono",
            "Masahiro Okuda",
            "Yusuke Tatesumi",
            "Paul Perrotin"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This paper proposes an automatic image correction method for portrait\nphotographs, which promotes consistency of facial skin color by suppressing\nskin color changes due to background colors. In portrait photographs, skin\ncolor is often distorted due to the lighting environment (e.g., light reflected\nfrom a colored background wall and over-exposure by a camera strobe), and if\nthe photo is artificially combined with another background color, this color\nchange is emphasized, resulting in an unnatural synthesized result. In our\nframework, after roughly extracting the face region and rectifying the skin\ncolor distribution in a color space, we perform color and brightness correction\naround the face in the original image to achieve a proper color balance of the\nfacial image, which is not affected by luminance and background colors. Unlike\nconventional algorithms for color correction, our final result is attained by a\ncolor correction process with a guide image. In particular, our guided image\nfiltering for the color correction does not require a perfectly-aligned guide\nimage required in the original guide image filtering method proposed by He et\nal. Experimental results show that our method generates more natural results\nthan conventional methods on not only headshot photographs but also natural\nscene photographs. We also show automatic yearbook style photo generation as an\nanother application.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.09034v1"
    },
    {
        "title": "A GAN-Like Approach for Physics-Based Imitation Learning and Interactive\n  Character Control",
        "authors": [
            "Pei Xu",
            "Ioannis Karamouzas"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a simple and intuitive approach for interactive control of\nphysically simulated characters. Our work builds upon generative adversarial\nnetworks (GAN) and reinforcement learning, and introduces an imitation learning\nframework where an ensemble of classifiers and an imitation policy are trained\nin tandem given pre-processed reference clips. The classifiers are trained to\ndiscriminate the reference motion from the motion generated by the imitation\npolicy, while the policy is rewarded for fooling the discriminators. Using our\nGAN-based approach, multiple motor control policies can be trained separately\nto imitate different behaviors. In runtime, our system can respond to external\ncontrol signal provided by the user and interactively switch between different\npolicies. Compared to existing methods, our proposed approach has the following\nattractive properties: 1) achieves state-of-the-art imitation performance\nwithout manually designing and fine tuning a reward function; 2) directly\ncontrols the character without having to track any target reference pose\nexplicitly or implicitly through a phase state; and 3) supports interactive\npolicy switching without requiring any motion generation or motion matching\nmechanism. We highlight the applicability of our approach in a range of\nimitation and interactive control tasks, while also demonstrating its ability\nto withstand external perturbations as well as to recover balance. Overall, our\napproach generates high-fidelity motion, has low runtime cost, and can be\neasily integrated into interactive applications and games.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.10066v4"
    },
    {
        "title": "Neural Radiosity",
        "authors": [
            "Saeed Hadadan",
            "Shuhong Chen",
            "Matthias Zwicker"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We introduce Neural Radiosity, an algorithm to solve the rendering equation\nby minimizing the norm of its residual similar as in traditional radiosity\ntechniques. Traditional basis functions used in radiosity techniques, such as\npiecewise polynomials or meshless basis functions are typically limited to\nrepresenting isotropic scattering from diffuse surfaces. Instead, we propose to\nleverage neural networks to represent the full four-dimensional radiance\ndistribution, directly optimizing network parameters to minimize the norm of\nthe residual. Our approach decouples solving the rendering equation from\nrendering (perspective) images similar as in traditional radiosity techniques,\nand allows us to efficiently synthesize arbitrary views of a scene. In\naddition, we propose a network architecture using geometric learnable features\nthat improves convergence of our solver compared to previous techniques. Our\napproach leads to an algorithm that is simple to implement, and we demonstrate\nits effectiveness on a variety of scenes with non-diffuse surfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.12319v2"
    },
    {
        "title": "Passing Multi-Channel Material Textures to a 3-Channel Loss",
        "authors": [
            "Thomas Chambon",
            "Eric Heitz",
            "Laurent Belcour"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Our objective is to compute a textural loss that can be used to train texture\ngenerators with multiple material channels typically used for physically based\nrendering such as albedo, normal, roughness, metalness, ambient occlusion, etc.\nNeural textural losses often build on top of the feature spaces of pretrained\nconvolutional neural networks. Unfortunately, these pretrained models are only\navailable for 3-channel RGB data and hence limit neural textural losses to this\nformat. To overcome this limitation, we show that passing random triplets to a\n3-channel loss provides a multi-channel loss that can be used to generate\nhigh-quality material textures.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.13012v1"
    },
    {
        "title": "Integer Coordinates for Intrinsic Geometry Processing",
        "authors": [
            "Mark Gillespie",
            "Nicholas Sharp",
            "Keenan Crane"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this work, we present a general, efficient, and provably robust\nrepresentation for intrinsic triangulations. These triangulations have emerged\nas a powerful tool for robust geometry processing of surface meshes, taking a\nlow-quality mesh and retriangulating it with high-quality intrinsic triangles.\nHowever, existing representations either support only edge flips, or do not\noffer a robust procedure to recover the common subdivision, that is, how the\nintrinsic triangulation sits along the original surface. To build a\ngeneral-purpose robust structure, we extend the framework of normal\ncoordinates, which have been deeply studied in topology, as well as the more\nrecent idea of roundabouts from geometry processing, to support a variety of\nmesh processing operations like vertex insertions, edge splits, etc. The basic\nidea is to store an integer per mesh edge counting the number of times a curve\ncrosses that edge. We show that this paradigm offers a highly effective\nrepresentation for intrinsic triangulations with strong robustness guarantees.\nThe resulting data structure is general and efficient, while offering a\nguarantee of always encoding a valid subdivision. Among other things, this\nallows us to generate a high-quality intrinsic Delaunay refinement of all\nmanifold meshes in the challenging Thingi10k dataset for the first time. This\nenables a broad class of existing surface geometry algorithms to be applied\nout-of-the-box to low-quality triangulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.00220v1"
    },
    {
        "title": "Deep Medial Fields",
        "authors": [
            "Daniel Rebain",
            "Ke Li",
            "Vincent Sitzmann",
            "Soroosh Yazdani",
            "Kwang Moo Yi",
            "Andrea Tagliasacchi"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Implicit representations of geometry, such as occupancy fields or signed\ndistance fields (SDF), have recently re-gained popularity in encoding 3D solid\nshape in a functional form. In this work, we introduce medial fields: a field\nfunction derived from the medial axis transform (MAT) that makes available\ninformation about the underlying 3D geometry that is immediately useful for a\nnumber of downstream tasks. In particular, the medial field encodes the local\nthickness of a 3D shape, and enables O(1) projection of a query point onto the\nmedial axis. To construct the medial field we require nothing but the SDF of\nthe shape itself, thus allowing its straightforward incorporation in any\napplication that relies on signed distance fields. Working in unison with the\nO(1) surface projection supported by the SDF, the medial field opens the door\nfor an entirely new set of efficient, shape-aware operations on implicit\nrepresentations. We present three such applications, including a modification\nto sphere tracing that renders implicit representations with better convergence\nproperties, a fast construction method for memory-efficient rigid-body\ncollision proxies, and an efficient approximation of ambient occlusion that\nremains stable with respect to viewpoint variations.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.03804v1"
    },
    {
        "title": "Neural UpFlow: A Scene Flow Learning Approach to Increase the Apparent\n  Resolution of Particle-Based Liquids",
        "authors": [
            "Bruno Roy",
            "Pierre Poulin",
            "Eric Paquette"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a novel up-resing technique for generating high-resolution liquids\nbased on scene flow estimation using deep neural networks. Our approach infers\nand synthesizes small- and large-scale details solely from a low-resolution\nparticle-based liquid simulation. The proposed network leverages neighborhood\ncontributions to encode inherent liquid properties throughout convolutions. We\nalso propose a particle-based approach to interpolate between liquids generated\nfrom varying simulation discretizations using a state-of-the-art bidirectional\noptical flow solver method for fluids in addition to a novel key-event\ntopological alignment constraint. In conjunction with the neighborhood\ncontributions, our loss formulation allows the inference model throughout\nepochs to reward important differences in regard to significant gaps in\nsimulation discretizations. Even when applied in an untested simulation setup,\nour approach is able to generate plausible high-resolution details. Using this\ninterpolation approach and the predicted displacements, our approach combines\nthe input liquid properties with the predicted motion to infer semi-Lagrangian\nadvection. We furthermore showcase how the proposed interpolation approach can\nfacilitate generating large simulation datasets with a subset of initial\ncondition parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.05143v1"
    },
    {
        "title": "Interactive Modelling of Volumetric Musculoskeletal Anatomy",
        "authors": [
            "Rinat Abdrashitov",
            "Seungbae Bang",
            "David I. W. Levin",
            "Karan Singh",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a new approach for modelling musculoskeletal anatomy. Unlike\nprevious methods, we do not model individual muscle shapes as geometric\nprimitives (polygonal meshes, NURBS etc.). Instead, we adopt a volumetric\nsegmentation approach where every point in our volume is assigned to a muscle,\nfat, or bone tissue. We provide an interactive modelling tool where the user\ncontrols the segmentation via muscle curves and we visualize the muscle shapes\nusing volumetric rendering. Muscle curves enable intuitive yet powerful control\nover the muscle shapes. This representation allows us to automatically handle\nintersections between different tissues (musclemuscle, muscle-bone, and\nmuscle-skin) during the modelling and automates computation of muscle fiber\nfields. We further introduce a novel algorithm for converting the volumetric\nmuscle representation into tetrahedral or surface geometry for use in\ndownstream tasks. Additionally, we introduce an interactive skeleton authoring\ntool that allows the users to create skeletal anatomy starting from only a skin\nmesh using a library of bone parts.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.05161v1"
    },
    {
        "title": "DiffCloth: Differentiable Cloth Simulation with Dry Frictional Contact",
        "authors": [
            "Yifei Li",
            "Tao Du",
            "Kui Wu",
            "Jie Xu",
            "Wojciech Matusik"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Cloth simulation has wide applications in computer animation, garment design,\nand robot-assisted dressing. This work presents a differentiable cloth\nsimulator whose additional gradient information facilitates cloth-related\napplications. Our differentiable simulator extends a state-of-the-art cloth\nsimulator based on Projective Dynamics (PD) and with dry frictional contact. We\ndraw inspiration from previous work to propose a fast and novel method for\nderiving gradients in PD-based cloth simulation with dry frictional contact.\nFurthermore, we conduct a comprehensive analysis and evaluation of the\nusefulness of gradients in contact-rich cloth simulation. Finally, we\ndemonstrate the efficacy of our simulator in a number of downstream\napplications, including system identification, trajectory optimization for\nassisted dressing, closed-loop control, inverse design, and real-to-sim\ntransfer. We observe a substantial speedup obtained from using our gradient\ninformation in solving most of these applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.05306v3"
    },
    {
        "title": "Deep Direct Volume Rendering: Learning Visual Feature Mappings From\n  Exemplary Images",
        "authors": [
            "Jakob Weiss",
            "Nassir Navab"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Volume Rendering is an important technique for visualizing three-dimensional\nscalar data grids and is commonly employed for scientific and medical image\ndata. Direct Volume Rendering (DVR) is a well established and efficient\nrendering algorithm for volumetric data. Neural rendering uses deep neural\nnetworks to solve inverse rendering tasks and applies techniques similar to\nDVR. However, it has not been demonstrated successfully for the rendering of\nscientific volume data.\n  In this work, we introduce Deep Direct Volume Rendering (DeepDVR), a\ngeneralization of DVR that allows for the integration of deep neural networks\ninto the DVR algorithm. We conceptualize the rendering in a latent color space,\nthus enabling the use of deep architectures to learn implicit mappings for\nfeature extraction and classification, replacing explicit feature design and\nhand-crafted transfer functions. Our generalization serves to derive novel\nvolume rendering architectures that can be trained end-to-end directly from\nexamples in image space, obviating the need to manually define and fine-tune\nmultidimensional transfer functions while providing superior classification\nstrength. We further introduce a novel stepsize annealing scheme to accelerate\nthe training of DeepDVR models and validate its effectiveness in a set of\nexperiments. We validate our architectures on two example use cases: (1)\nlearning an optimized rendering from manually adjusted reference images for a\nsingle volume and (2) learning advanced visualization concepts like shading and\nsemantic colorization that generalize to unseen volume data.\n  We find that deep volume rendering architectures with explicit modeling of\nthe DVR pipeline effectively enable end-to-end learning of scientific volume\nrendering tasks from target images.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.05429v1"
    },
    {
        "title": "Learning Perceptual Manifold of Fonts",
        "authors": [
            "Haoran Xie",
            "Yuki Fujita",
            "Kazunori Miyata"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Along the rapid development of deep learning techniques in generative models,\nit is becoming an urgent issue to combine machine intelligence with human\nintelligence to solve the practical applications. Motivated by this\nmethodology, this work aims to adjust the machine generated character fonts\nwith the effort of human workers in the perception study. Although numerous\nfonts are available online for public usage, it is difficult and challenging to\ngenerate and explore a font to meet the preferences for common users. To solve\nthe specific issue, we propose the perceptual manifold of fonts to visualize\nthe perceptual adjustment in the latent space of a generative model of fonts.\nIn our framework, we adopt the variational autoencoder network for the font\ngeneration. Then, we conduct a perceptual study on the generated fonts from the\nmulti-dimensional latent space of the generative model. After we obtained the\ndistribution data of specific preferences, we utilize manifold learning\napproach to visualize the font distribution. In contrast to the conventional\nuser interface in our user study, the proposed font-exploring user interface is\nefficient and helpful in the designated user preference.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.09198v1"
    },
    {
        "title": "Statistical Rendering for Visualization of Red Sea Eddy Simulation Data",
        "authors": [
            "Tushar M. Athawale",
            "Alireza Entezari",
            "Bei Wang",
            "Chris R. Johnson"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Analyzing the effects of ocean eddies is important in oceanology for gaining\ninsights into transport of energy and biogeochemical particles. We present an\napplication of statistical visualization algorithms for the analysis of the Red\nSea eddy simulation ensemble. Specifically, we demonstrate the applications of\nstatistical volume rendering and statistical Morse complex summary maps to a\nvelocity magnitude field for studying the eddy positions in the flow dataset.\nIn statistical volume rendering, we model per-voxel data uncertainty using\nnoise models, such as parametric and nonparametric, and study the propagation\nof uncertainty into the volume rendering pipeline. In the statistical Morse\ncomplex summary maps, we derive histograms charactering uncertainty of gradient\nflow destinations to understand Morse complex topological variations across the\nensemble. We demonstrate the utility of our statistical visualizations for an\neffective analysis of the potential eddy positions and their spatial\nuncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.12138v1"
    },
    {
        "title": "Real-time Neural Radiance Caching for Path Tracing",
        "authors": [
            "Thomas Müller",
            "Fabrice Rousselle",
            "Jan Novák",
            "Alexander Keller"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a real-time neural radiance caching method for path-traced global\nillumination. Our system is designed to handle fully dynamic scenes, and makes\nno assumptions about the lighting, geometry, and materials. The data-driven\nnature of our approach sidesteps many difficulties of caching algorithms, such\nas locating, interpolating, and updating cache points. Since pretraining neural\nnetworks to handle novel, dynamic scenes is a formidable generalization\nchallenge, we do away with pretraining and instead achieve generalization via\nadaptation, i.e. we opt for training the radiance cache while rendering. We\nemploy self-training to provide low-noise training targets and simulate\ninfinite-bounce transport by merely iterating few-bounce training updates. The\nupdates and cache queries incur a mild overhead -- about 2.6ms on full HD\nresolution -- thanks to a streaming implementation of the neural network that\nfully exploits modern hardware. We demonstrate significant noise reduction at\nthe cost of little induced bias, and report state-of-the-art, real-time\nperformance on a number of challenging scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.12372v2"
    },
    {
        "title": "Fast Linking Numbers for Topology Verification of Loopy Structures",
        "authors": [
            "Ante Qu",
            "Doug L. James"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  It is increasingly common to model, simulate, and process complex materials\nbased on loopy structures, such as in yarn-level cloth garments, which possess\ntopological constraints between inter-looping curves. While the input model may\nsatisfy specific topological linkages between pairs of closed loops, subsequent\nprocessing may violate those topological conditions. In this paper, we explore\na family of methods for efficiently computing and verifying linking numbers\nbetween closed curves, and apply these to applications in geometry processing,\nanimation, and simulation, so as to verify that topological invariants are\npreserved during and after processing of the input models. Our method has three\nstages: (1) we identify potentially interacting loop-loop pairs, then (2)\ncarefully discretize each loop's spline curves into line segments so as to\nenable (3) efficient linking number evaluation using accelerated kernels based\non either counting projected segment-segment crossings, or by evaluating the\nGauss linking integral using direct or fast summation methods (Barnes-Hut or\nfast multipole methods). We evaluate CPU and GPU implementations of these\nmethods on a suite of test problems, including yarn-level cloth and chainmail,\nthat involve significant processing: physics-based relaxation and animation,\nuser-modeled deformations, curve compression and reparameterization. We show\nthat topology errors can be efficiently identified to enable more robust\nprocessing of loopy structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.12655v1"
    },
    {
        "title": "Free-viewpoint Indoor Neural Relighting from Multi-view Stereo",
        "authors": [
            "Julien Philip",
            "Sébastien Morgenthaler",
            "Michaël Gharbi",
            "George Drettakis"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We introduce a neural relighting algorithm for captured indoors scenes, that\nallows interactive free-viewpoint navigation. Our method allows illumination to\nbe changed synthetically, while coherently rendering cast shadows and complex\nglossy materials. We start with multiple images of the scene and a 3D mesh\nobtained by multi-view stereo (MVS) reconstruction. We assume that lighting is\nwell-explained as the sum of a view-independent diffuse component and a\nview-dependent glossy term concentrated around the mirror reflection direction.\nWe design a convolutional network around input feature maps that facilitate\nlearning of an implicit representation of scene materials and illumination,\nenabling both relighting and free-viewpoint navigation. We generate these input\nmaps by exploiting the best elements of both image-based and physically-based\nrendering. We sample the input views to estimate diffuse scene irradiance, and\ncompute the new illumination caused by user-specified light sources using path\ntracing. To facilitate the network's understanding of materials and synthesize\nplausible glossy reflections, we reproject the views and compute mirror images.\nWe train the network on a synthetic dataset where each scene is also\nreconstructed with MVS. We show results of our algorithm relighting real indoor\nscenes and performing free-viewpoint navigation with complex and realistic\nglossy reflections, which so far remained out of reach for view-synthesis\ntechniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.13299v1"
    },
    {
        "title": "Learning-based pose edition for efficient and interactive design",
        "authors": [
            "Léon Victor",
            "Alexandre Meyer",
            "Saïda Bouakaz"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Authoring an appealing animation for a virtual character is a challenging\ntask. In computer-aided keyframe animation artists define the key poses of a\ncharacter by manipulating its underlying skeletons. To look plausible, a\ncharacter pose must respect many ill-defined constraints, and so the resulting\nrealism greatly depends on the animator's skill and knowledge. Animation\nsoftware provide tools to help in this matter, relying on various algorithms to\nautomatically enforce some of these constraints. The increasing availability of\nmotion capture data has raised interest in data-driven approaches to pose\ndesign, with the potential of shifting more of the task of assessing realism\nfrom the artist to the computer, and to provide easier access to nonexperts. In\nthis article, we propose such a method, relying on neural networks to\nautomatically learn the constraints from the data. We describe an efficient\ntool for pose design, allowing na{\\\"i}ve users to intuitively manipulate a pose\nto create character animations.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.00397v1"
    },
    {
        "title": "EmoGen: Quantifiable Emotion Generation and Analysis for Experimental\n  Psychology",
        "authors": [
            "Nadejda Roubtsova",
            "Martin Parsons",
            "Nicola Binetti",
            "Isabelle Mareschal",
            "Essi Viding",
            "Darren Cosker"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  3D facial modelling and animation in computer vision and graphics\ntraditionally require either digital artist's skill or complex pipelines with\nobjective-function-based solvers to fit models to motion capture. This\ninaccessibility of quality modelling to a non-expert is an impediment to\neffective quantitative study of facial stimuli in experimental psychology. The\nEmoGen methodology we present in this paper solves the issue democratising\nfacial modelling technology. EmoGen is a robust and configurable framework\nletting anyone author arbitrary quantifiable facial expressions in 3D through a\nuser-guided genetic algorithm search. Beyond sample generation, our methodology\nis made complete with techniques to analyse distributions of these expressions\nin a principled way. This paper covers the technical aspects of expression\ngeneration, specifically our production-quality facial blendshape model,\nautomatic corrective mechanisms of implausible facial configurations in the\nabsence of artist's supervision and the genetic algorithm implementation\nemployed in the model space search. Further, we provide a comparative\nevaluation of ways to quantify generated facial expressions in the blendshape\nand geometric domains and compare them theoretically and empirically. The\npurpose of this analysis is 1. to define a similarity cost function to simulate\nmodel space search for convergence and parameter dependence assessment of the\ngenetic algorithm and 2. to inform the best practices in the data distribution\nanalysis for experimental psychology.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.00480v1"
    },
    {
        "title": "An Analytical Survey on Recent Trends in High Dimensional Data\n  Visualization",
        "authors": [
            "Alexander Kiefer",
            "Md. Khaledur Rahman"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Data visualization is the process by which data of any size or dimensionality\nis processed to produce an understandable set of data in a lower\ndimensionality, allowing it to be manipulated and understood more easily by\npeople. The goal of our paper is to survey the performance of current\nhigh-dimensional data visualization techniques and quantify their strengths and\nweaknesses through relevant quantitative measures, including runtime, memory\nusage, clustering quality, separation quality, global structure preservation,\nand local structure preservation. To perform the analysis, we select a subset\nof state-of-the-art methods. Our work shows how the selected algorithms produce\nembeddings with unique qualities that lend themselves towards certain tasks,\nand how each of these algorithms are constrained by compute resources.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.01887v1"
    },
    {
        "title": "Egocentric Videoconferencing",
        "authors": [
            "Mohamed Elgharib",
            "Mohit Mendiratta",
            "Justus Thies",
            "Matthias Nießner",
            "Hans-Peter Seidel",
            "Ayush Tewari",
            "Vladislav Golyanik",
            "Christian Theobalt"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We introduce a method for egocentric videoconferencing that enables\nhands-free video calls, for instance by people wearing smart glasses or other\nmixed-reality devices. Videoconferencing portrays valuable non-verbal\ncommunication and face expression cues, but usually requires a front-facing\ncamera. Using a frontal camera in a hands-free setting when a person is on the\nmove is impractical. Even holding a mobile phone camera in the front of the\nface while sitting for a long duration is not convenient. To overcome these\nissues, we propose a low-cost wearable egocentric camera setup that can be\nintegrated into smart glasses. Our goal is to mimic a classical video call, and\ntherefore, we transform the egocentric perspective of this camera into a front\nfacing video. To this end, we employ a conditional generative adversarial\nneural network that learns a transition from the highly distorted egocentric\nviews to frontal views common in videoconferencing. Our approach learns to\ntransfer expression details directly from the egocentric view without using a\ncomplex intermediate parametric expressions model, as it is used by related\nface reenactment methods. We successfully handle subtle expressions, not easily\ncaptured by parametric blendshape-based solutions, e.g., tongue movement, eye\nmovements, eye blinking, strong expressions and depth varying movements. To get\ncontrol over the rigid head movements in the target view, we condition the\ngenerator on synthetic renderings of a moving neutral face. This allows us to\nsynthesis results at different head poses. Our technique produces temporally\nsmooth video-realistic renderings in real-time using a video-to-video\ntranslation network in conjunction with a temporal discriminator. We\ndemonstrate the improved capabilities of our technique by comparing against\nrelated state-of-the art approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.03109v1"
    },
    {
        "title": "Never 'Drop the Ball' in the Operating Room: An efficient hand-based VR\n  HMD controller interpolation algorithm, for collaborative, networked virtual\n  environments",
        "authors": [
            "Manos Kamarianakis",
            "Nick Lydatakis",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this work, we propose two algorithms that can be applied in the context of\na networked virtual environment to efficiently handle the interpolation of\ndisplacement data for hand-based VR HMDs. Our algorithms, based on the use of\ndual-quaternions and multivectors respectively, impact the network consumption\nrate and are highly effective in scenarios involving multiple users. We\nillustrate convincing results in a modern game engine and a medical VR\ncollaborative training scenario.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.04875v1"
    },
    {
        "title": "COLiER: Collaborative Editing of Raster Images",
        "authors": [
            "Ulrike Bath",
            "Sumit Shekhar",
            "Jürgen Döllner",
            "Matthias Trapp"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Various web-based image-editing tools and web-based collaborative tools exist\nin isolation. Research focusing to bridge the gap between these two domains is\nsparse. We respond to the above and develop prototype groupware for real-time\ncollaborative editing of raster images in a web browser. To better understand\nthe requirements, we conduct a preliminary user study and establish\ncommunication and synchronization as key elements. The existing groupware for\ntext documents, presentations, and vector graphics handles the above through\nwell-established techniques. However, those cannot be extended as it is for\nraster graphics manipulation. To this end, we develop a document model that is\nmaintained by a server and is delivered and synchronized to multiple clients.\nOur prototypical implementation is based on a scalable client-server\narchitecture: using WebGL for interactive browser-based rendering and WebSocket\nconnections to maintain synchronization. We evaluate our work qualitatively\nthrough a post-deployment user study for three different scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.05962v2"
    },
    {
        "title": "Synthesizing Human Faces using Latent Space Factorization and Local\n  Weights (Extended Version)",
        "authors": [
            "Minyoung Kim",
            "Young J. Kim"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We propose a 3D face generative model with local weights to increase the\nmodel's variations and expressiveness. The proposed model allows partial\nmanipulation of the face while still learning the whole face mesh. For this\npurpose, we address an effective way to extract local facial features from the\nentire data and explore a way to manipulate them during a holistic generation.\nFirst, we factorize the latent space of the whole face to the subspace\nindicating different parts of the face. In addition, local weights generated by\nnon-negative matrix factorization are applied to the factorized latent space so\nthat the decomposed part space is semantically meaningful. We experiment with\nour model and observe that effective facial part manipulation is possible and\nthat the model's expressiveness is improved.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.08737v1"
    },
    {
        "title": "SuperCaustics: Real-time, open-source simulation of transparent objects\n  for deep learning applications",
        "authors": [
            "Mehdi Mousavi",
            "Rolando Estrada"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Transparent objects are a very challenging problem in computer vision. They\nare hard to segment or classify due to their lack of precise boundaries, and\nthere is limited data available for training deep neural networks. As such,\ncurrent solutions for this problem employ rigid synthetic datasets, which lack\nflexibility and lead to severe performance degradation when deployed on\nreal-world scenarios. In particular, these synthetic datasets omit features\nsuch as refraction, dispersion and caustics due to limitations in the rendering\npipeline. To address this issue, we present SuperCaustics, a real-time,\nopen-source simulation of transparent objects designed for deep learning\napplications. SuperCaustics features extensive modules for stochastic\nenvironment creation; uses hardware ray-tracing to support caustics,\ndispersion, and refraction; and enables generating massive datasets with\nmulti-modal, pixel-perfect ground truth annotations. To validate our proposed\nsystem, we trained a deep neural network from scratch to segment transparent\nobjects in difficult lighting scenarios. Our neural network achieved\nperformance comparable to the state-of-the-art on a real-world dataset using\nonly 10% of the training data and in a fraction of the training time. Further\nexperiments show that a model trained with SuperCaustics can segment different\ntypes of caustics, even in images with multiple overlapping transparent\nobjects. To the best of our knowledge, this is the first such result for a\nmodel trained on synthetic data. Both our open-source code and experimental\ndata are freely available online.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.11008v2"
    },
    {
        "title": "Learning GAN-based Foveated Reconstruction to Recover Perceptually\n  Important Image Features",
        "authors": [
            "Luca Surace",
            "Marek Wernikowski",
            "Cara Tursun",
            "Karol Myszkowski",
            "Radosław Mantiuk",
            "Piotr Didyk"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  A foveated image can be entirely reconstructed from a sparse set of samples\ndistributed according to the retinal sensitivity of the human visual system,\nwhich rapidly decreases with increasing eccentricity. The use of Generative\nAdversarial Networks has recently been shown to be a promising solution for\nsuch a task, as they can successfully hallucinate missing image information. As\nin the case of other supervised learning approaches, the definition of the loss\nfunction and the training strategy heavily influence the quality of the output.\nIn this work,we consider the problem of efficiently guiding the training of\nfoveated reconstruction techniques such that they are more aware of the\ncapabilities and limitations of the human visual system, and thus can\nreconstruct visually important image features. Our primary goal is to make the\ntraining procedure less sensitive to distortions that humans cannot detect and\nfocus on penalizing perceptually important artifacts. Given the nature of\nGAN-based solutions, we focus on the sensitivity of human vision to\nhallucination in case of input samples with different densities. We propose\npsychophysical experiments, a dataset, and a procedure for training foveated\nimage reconstruction. The proposed strategy renders the generator network\nflexible by penalizing only perceptually important deviations in the output. As\na result, the method emphasized the recovery of perceptually important image\nfeatures. We evaluated our strategy and compared it with alternative solutions\nby using a newly trained objective metric, a recent foveated video quality\nmetric, and user experiments. Our evaluations revealed significant improvements\nin the perceived image reconstruction quality compared with the standard\nGAN-based training approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.03499v3"
    },
    {
        "title": "A computational medical XR discipline",
        "authors": [
            "George Papagiannakis",
            "Walter Greenleaf",
            "Michael Cole",
            "Mark Zhang",
            "Rabi Datta",
            "Mathias Delahaye",
            "Eleni Grigoriou",
            "Manos Kamarianakis",
            "Antonis Protopsaltis",
            "Philippe Bijlenga",
            "Nadia Magnenat-Thalmann",
            "Eleftherios Tsiridis",
            "Eustathios Kenanidis",
            "Kyriakos Vamvakidis",
            "Ioannis Koutelidakis",
            "Oliver A Kannape"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Computational Medical Extended Reality (CMXR), brings together life sciences\nand neuroscience with mathematics, engineering and computer science. It unifies\ncomputational science (scientific computing) with intelligent extended reality\nand spatial computing for the medical field. It significantly differs from\nprevious \"Clinical XR\" or \"Medical XR\" terms, as it is focusing on how to\nintegrate computational methods from neural simulation to computational\ngeometry, computational vision and computer graphics with deep learning models\nto solve specific hard problems in medicine and neuroscience: from\nlow/no-code/genAI authoring platforms to deep learning XR systems for training,\nplanning, operative navigation, therapy and rehabilitation.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.04136v6"
    },
    {
        "title": "Stroke Correspondence by Labeling Closed Areas",
        "authors": [
            "Ryoma Miyauchi",
            "Tsukasa Fukusato",
            "Haoran Xie",
            "Kazunori Miyata"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Constructing stroke correspondences between keyframes is one of the most\nimportant processes in the production pipeline of hand-drawn inbetweening\nframes. This process requires time-consuming manual work imposing a tremendous\nburden on the animators. We propose a method to estimate stroke correspondences\nbetween raster character images (keyframes) without vectorization processes.\nFirst, the proposed system separates the closed areas in each keyframe and\nestimates the correspondences between closed areas by using the characteristics\nof shape, depth, and closed area connection. Second, the proposed system\nestimates stroke correspondences from the estimated closed area\ncorrespondences. We demonstrate the effectiveness of our method by performing a\nuser study and comparing the proposed system with conventional approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.04393v1"
    },
    {
        "title": "Differentiable Surface Rendering via Non-Differentiable Sampling",
        "authors": [
            "Forrester Cole",
            "Kyle Genova",
            "Avneesh Sud",
            "Daniel Vlasic",
            "Zhoutong Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a method for differentiable rendering of 3D surfaces that supports\nboth explicit and implicit representations, provides derivatives at occlusion\nboundaries, and is fast and simple to implement. The method first samples the\nsurface using non-differentiable rasterization, then applies differentiable,\ndepth-aware point splatting to produce the final image. Our approach requires\nno differentiable meshing or rasterization steps, making it efficient for large\n3D models and applicable to isosurfaces extracted from implicit surface\ndefinitions. We demonstrate the effectiveness of our method for implicit-,\nmesh-, and parametric-surface-based inverse rendering and neural-network\ntraining applications. In particular, we show for the first time efficient,\ndifferentiable rendering of an isosurface extracted from a neural radiance\nfield (NeRF), and demonstrate surface-based, rather than volume-based,\nrendering of a NeRF.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.04886v1"
    },
    {
        "title": "\"Deep Cut\": An all-in-one Geometric Algorithm for Unconstrained Cut,\n  Tear and Drill of Soft-bodies in Mobile VR",
        "authors": [
            "Manos Kamarianakis",
            "Nick Lydatakis",
            "Antonis Protopsaltis",
            "John Petropoulos",
            "Michail Tamiolakis",
            "Paul Zikas",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this work, we present an integrated geometric framework: \"deep- cut\" that\nenables for the first time a user to geometrically and algorithmically cut,\ntear and drill the surface of a skinned model without prior constraints,\nlayered on top of a custom soft body mesh deformation algorithm. Both layered\nalgorithms in this frame- work yield real-time results and are amenable for\nmobile Virtual Reality, in order to be utilized in a variety of interactive\napplication scenarios. Our framework dramatically improves real-time user\nexperience and task performance in VR, without pre-calculated or artificially\ndesigned cuts, tears, drills or surface deformations via predefined rigged\nanimations, which is the current state-of-the-art in mobile VR. Thus our\nframework improves user experience on one hand, on the other hand saves both\ntime and costs from expensive, manual, labour-intensive design pre-calculation\nstages.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.05281v1"
    },
    {
        "title": "Ships, Splashes, and Waves on a Vast Ocean",
        "authors": [
            "Libo Huang",
            "Ziyin Qu",
            "Xun Tan",
            "Xinxin Zhang",
            "Dominik L. Michels",
            "Chenfanfu Jiang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The simulation of large open water surface is challenging using a uniform\nvolumetric discretization of the Navier-Stokes equations. Simulating water\nsplashes near moving objects, which height field methods for water waves cannot\ncapture, necessitates high resolutions. Such simulations can be carried out\nusing the Fluid-Implicit-Particle (FLIP) method. However, the FLIP method is\nnot efficient for the long-lasting water waves that propagate to long\ndistances, which require sufficient depth for a correct dispersion\nrelationship. This paper presents a new method to tackle this dilemma through\nan efficient hybridization of volumetric and surface-based advection-projection\ndiscretizations. We design a hybrid time-stepping algorithm that combines a\nFLIP domain and an adaptively remeshed Boundary Element Method (BEM) domain for\nthe incompressible Euler equations. The resulting framework captures the\ndetailed water splashes near moving objects with the FLIP method, and produces\nconvincing water waves with correct dispersion relationships at modest\nadditional costs.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.05481v3"
    },
    {
        "title": "Fast Approximation of Persistence Diagrams with Guarantees",
        "authors": [
            "Jules Vidal",
            "Julien Tierny"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This paper presents an algorithm for the efficient approximation of the\nsaddle-extremum persistence diagram of a scalar field. Vidal et al. introduced\nrecently a fast algorithm for such an approximation (by interrupting a\nprogressive computation framework). However, no theoretical guarantee was\nprovided regarding its approximation quality. In this work, we revisit the\nprogressive framework of Vidal et al. and we introduce in contrast a novel\napproximation algorithm, with a user controlled approximation error,\nspecifically, on the Bottleneck distance to the exact solution. Our approach is\nbased on a hierarchical representation of the input data, and relies on local\nsimplifications of the scalar field to accelerate the computation, while\nmaintaining a controlled bound on the output error. The locality of our\napproach enables further speedups thanks to shared memory parallelism.\nExperiments conducted on real life datasets show that for a mild error\ntolerance (5% relative Bottleneck distance), our approach improves runtime\nperformance by 18% on average (and up to 48% on large, noisy datasets) in\ncomparison to standard, exact, publicly available implementations. In addition\nto the strong guarantees on its approximation error, we show that our algorithm\nalso provides in practice outputs which are on average 5 times more accurate\n(in terms of the L2-Wasserstein distance) than a naive approximation baseline.\nWe illustrate the utility of our approach for interactive data exploration and\nwe document visualization strategies for conveying the uncertainty related to\nour approximations.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.05766v1"
    },
    {
        "title": "Autocomplete Repetitive Stroking with Image Guidance",
        "authors": [
            "Yilan Chen",
            "Kin Chung Kwan",
            "Li-Yi Wei",
            "Hongbo Fu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Image-guided drawing can compensate for the lack of skills but often requires\na significant number of repetitive strokes to create textures. Existing\nautomatic stroke synthesis methods are usually limited to predefined styles or\nrequire indirect manipulation that may break the spontaneous flow of drawing.\nWe present a method to autocomplete repetitive short strokes during users'\nnormal drawing process. Users can draw over a reference image as usual. At the\nsame time, our system silently analyzes the input strokes and the reference to\ninfer strokes that follow users' input style when certain repetition is\ndetected. Users can accept, modify, or ignore the system predictions and\ncontinue drawing, thus maintaining the fluid control of drawing. Our key idea\nis to jointly analyze image regions and operation history for detecting and\npredicting repetitions. The proposed system can effectively reduce users'\nworkload in drawing repetitive short strokes and facilitates users in creating\nresults with rich patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.07115v1"
    },
    {
        "title": "Non-Photorealistic Rendering of Layered Materials: A Multispectral\n  Approach",
        "authors": [
            "Corey Toler-Franklin",
            "Shashank Ranjan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present multispectral rendering techniques for visualizing layered\nmaterials found in biological specimens. We are the first to use acquired data\nfrom the near-infrared and ultraviolet spectra for non-photorealistic rendering\n(NPR). Several plant and animal species are more comprehensively understood by\nmultispectral analysis. However, traditional NPR techniques ignore unique\ninformation outside the visible spectrum. We introduce algorithms and\nprinciples for processing wavelength dependent surface normals and reflectance.\nOur registration and feature detection methods are used to formulate\nstylization effects not considered by current NPR methods including: Spectral\nBand Shading which isolates and emphasizes shape features at specific\nwavelengths at multiple scales. Experts in our user study demonstrate the\neffectiveness of our system for applications in the biological sciences.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.00780v1"
    },
    {
        "title": "Dynamic Scene Novel View Synthesis via Deferred Spatio-temporal\n  Consistency",
        "authors": [
            "Beatrix-Emőke Fülöp-Balogh",
            "Eleanor Tursman",
            "James Tompkin",
            "Julie Digne",
            "Nicolas Bonneel"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Structure from motion (SfM) enables us to reconstruct a scene via casual\ncapture from cameras at different viewpoints, and novel view synthesis (NVS)\nallows us to render a captured scene from a new viewpoint. Both are hard with\ncasual capture and dynamic scenes: SfM produces noisy and spatio-temporally\nsparse reconstructed point clouds, resulting in NVS with spatio-temporally\ninconsistent effects. We consider SfM and NVS parts together to ease the\nchallenge. First, for SfM, we recover stable camera poses, then we defer the\nrequirement for temporally-consistent points across the scene and reconstruct\nonly a sparse point cloud per timestep that is noisy in space-time. Second, for\nNVS, we present a variational diffusion formulation on depths and colors that\nlets us robustly cope with the noise by enforcing spatio-temporal consistency\nvia per-pixel reprojection weights derived from the input views. Together, this\ndeferred approach generates novel views for dynamic scenes without requiring\nchallenging spatio-temporally consistent reconstructions nor training complex\nmodels on large datasets. We demonstrate our algorithm on real-world dynamic\nscenes against classic and more recent learning-based baseline approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.01018v1"
    },
    {
        "title": "Per Garment Capture and Synthesis for Real-time Virtual Try-on",
        "authors": [
            "Toby Chong",
            "I-Chao Shen",
            "Nobuyuki Umetani",
            "Takeo Igarashi"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Virtual try-on is a promising application of computer graphics and human\ncomputer interaction that can have a profound real-world impact especially\nduring this pandemic. Existing image-based works try to synthesize a try-on\nimage from a single image of a target garment, but it inherently limits the\nability to react to possible interactions. It is difficult to reproduce the\nchange of wrinkles caused by pose and body size change, as well as pulling and\nstretching of the garment by hand. In this paper, we propose an alternative per\ngarment capture and synthesis workflow to handle such rich interactions by\ntraining the model with many systematically captured images. Our workflow is\ncomposed of two parts: garment capturing and clothed person image synthesis. We\ndesigned an actuated mannequin and an efficient capturing process that collects\nthe detailed deformations of the target garments under diverse body sizes and\nposes. Furthermore, we proposed to use a custom-designed measurement garment,\nand we captured paired images of the measurement garment and the target\ngarments. We then learn a mapping between the measurement garment and the\ntarget garments using deep image-to-image translation. The customer can then\ntry on the target garments interactively during online shopping.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04654v1"
    },
    {
        "title": "Temporal Parameter-free Deep Skinning of Animated Meshes",
        "authors": [
            "Anastasia Moutafidou",
            "Vasileios Toulatzis",
            "Ioannis Fudos"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In computer graphics, animation compression is essential for efficient\nstorage, streaming and reproduction of animated meshes. Previous work has\npresented efficient techniques for compression by deriving skinning\ntransformations and weights using clustering of vertices based on geometric\nfeatures of vertices over time. In this work we present a novel approach that\nassigns vertices to bone-influenced clusters and derives weights using deep\nlearning through a training set that consists of pairs of vertex trajectories\n(temporal vertex sequences) and the corresponding weights drawn from fully\nrigged animated characters. The approximation error of the resulting linear\nblend skinning scheme is significantly lower than the error of competent\nprevious methods by producing at the same time a minimal number of bones.\nFurthermore, the optimal set of transformation and vertices is derived in fewer\niterations due to the better initial positioning in the multidimensional\nvariable space. Our method requires no parameters to be determined or tuned by\nthe user during the entire process of compressing a mesh animation sequence.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.07249v1"
    },
    {
        "title": "FreeStyleGAN: Free-view Editable Portrait Rendering with the Camera\n  Manifold",
        "authors": [
            "Thomas Leimkühler",
            "George Drettakis"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Current Generative Adversarial Networks (GANs) produce photorealistic\nrenderings of portrait images. Embedding real images into the latent space of\nsuch models enables high-level image editing. While recent methods provide\nconsiderable semantic control over the (re-)generated images, they can only\ngenerate a limited set of viewpoints and cannot explicitly control the camera.\nSuch 3D camera control is required for 3D virtual and mixed reality\napplications. In our solution, we use a few images of a face to perform 3D\nreconstruction, and we introduce the notion of the GAN camera manifold, the key\nelement allowing us to precisely define the range of images that the GAN can\nreproduce in a stable manner. We train a small face-specific neural implicit\nrepresentation network to map a captured face to this manifold and complement\nit with a warping scheme to obtain free-viewpoint novel-view synthesis. We show\nhow our approach - due to its precise camera control - enables the integration\nof a pre-trained StyleGAN into standard 3D rendering pipelines, allowing e.g.,\nstereo rendering or consistent insertion of faces in synthetic 3D environments.\nOur solution proposes the first truly free-viewpoint rendering of realistic\nfaces at interactive rates, using only a small number of casual photos as\ninput, while simultaneously allowing semantic editing capabilities, such as\nfacial expression or lighting changes.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.09378v1"
    },
    {
        "title": "Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation",
        "authors": [
            "Yuanxun Lu",
            "Jinxiang Chai",
            "Xun Cao"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  To the best of our knowledge, we first present a live system that generates\npersonalized photorealistic talking-head animation only driven by audio signals\nat over 30 fps. Our system contains three stages. The first stage is a deep\nneural network that extracts deep audio features along with a manifold\nprojection to project the features to the target person's speech space. In the\nsecond stage, we learn facial dynamics and motions from the projected audio\nfeatures. The predicted motions include head poses and upper body motions,\nwhere the former is generated by an autoregressive probabilistic model which\nmodels the head pose distribution of the target person. Upper body motions are\ndeduced from head poses. In the final stage, we generate conditional feature\nmaps from previous predictions and send them with a candidate image set to an\nimage-to-image translation network to synthesize photorealistic renderings. Our\nmethod generalizes well to wild audio and successfully synthesizes\nhigh-fidelity personalized facial details, e.g., wrinkles, teeth. Our method\nalso allows explicit control of head poses. Extensive qualitative and\nquantitative evaluations, along with user studies, demonstrate the superiority\nof our method over state-of-the-art techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.10595v2"
    },
    {
        "title": "Mixed Integer Neural Inverse Design",
        "authors": [
            "Navid Ansari",
            "Hans-Peter Seidel",
            "Vahid Babaei"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In computational design and fabrication, neural networks are becoming\nimportant surrogates for bulky forward simulations. A long-standing,\nintertwined question is that of inverse design: how to compute a design that\nsatisfies a desired target performance? Here, we show that the piecewise linear\nproperty, very common in everyday neural networks, allows for an inverse design\nformulation based on mixed-integer linear programming. Our mixed-integer\ninverse design uncovers globally optimal or near optimal solutions in a\nprincipled manner. Furthermore, our method significantly facilitates emerging,\nbut challenging, combinatorial inverse design tasks, such as material\nselection. For problems where finding the optimal solution is not desirable or\ntractable, we develop an efficient yet near-optimal hybrid optimization.\nEventually, our method is able to find solutions provably robust to possible\nfabrication perturbations among multiple designs with similar performances.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.12888v2"
    },
    {
        "title": "Supporting Unified Shader Specialization by Co-opting C++ Features",
        "authors": [
            "Kerry A. Seitz Jr.",
            "Theresa Foley",
            "Serban D. Porumbescu",
            "John D. Owens"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Modern unified programming models (such as CUDA and SYCL) that combine host\n(CPU) code and GPU code into the same programming language, same file, and same\nlexical scope lack adequate support for GPU code specialization, which is a key\noptimization in real-time graphics. Furthermore, current methods used to\nimplement specialization do not translate to a unified environment. In this\npaper, we create a unified shader programming environment in C++ that provides\nfirst-class support for specialization by co-opting C++'s attribute and virtual\nfunction features and reimplementing them with alternate semantics to express\nthe services required. By co-opting existing features, we enable programmers to\nuse familiar C++ programming techniques to write host and GPU code together,\nwhile still achieving efficient generated C++ and HLSL code via our\nsource-to-source translator.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.14682v2"
    },
    {
        "title": "SliceHub: Augmenting Shared 3D Model Repositories with Slicing Results\n  for 3D Printing",
        "authors": [
            "Faraz Faruqi",
            "Kenneth Friedman",
            "Leon Cheng",
            "Michael Wessely",
            "Sriram Subramanian",
            "Stefanie Mueller"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this paper, we explore how to augment shared 3D model repositories, such\nas Thingiverse, with slicing results that are readily available to all users.\nBy having print time and material consumption for different print resolution\nprofiles and model scales available in real-time, users are able to explore\ndifferent slicing configurations efficiently to find the one that best fits\ntheir time and material constraints. To prototype this idea, we build a system\ncalled SliceHub, which consists of three components: (1) a repository with an\nevolving database of 3D models, for which we store the print time and material\nconsumption for various print resolution profiles and model scales, (2) a user\ninterface integrated into an existing slicer that allows users to explore the\nslicing information from the 3D models, and (3) a computational infrastructure\nto quickly generate new slicing results, either through parallel slicing of\nmultiple print resolution profiles and model scales or through interpolation.\nWe motivate our work with a formative study of the challenges faced by users of\nexisting slicers and provide a technical evaluation of the SliceHub system.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.14722v2"
    },
    {
        "title": "Navigating Higher Dimensional Spaces using Hyperbolic Geometry",
        "authors": [
            "Eryk Kopczyński",
            "Dorota Celińska-Kopczyńska"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Higher-dimensional spaces are ubiquitous in applications of mathematics. Yet,\nas we live in a three-dimensional space, visualizing, say, a four-dimensional\nspace is challenging. We introduce a novel method of interactive visualization\nof higher-dimensional grids, based on hyperbolic geometry. In our approach,\nvisualized objects are adjacent on the screen if and only if they are in\nadjacent cells of the grid. Previous attempts do not show the whole\nhigher-dimensional space at once, put close objects in distant parts of the\nscreen, or map multiple locations to the same point on the screen; our solution\nlacks these disadvantages, making it applicable in data visualization, user\ninterfaces, and game design.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.00327v1"
    },
    {
        "title": "GAN-based Reactive Motion Synthesis with Class-aware Discriminators for\n  Human-human Interaction",
        "authors": [
            "Qianhui Men",
            "Hubert P. H. Shum",
            "Edmond S. L. Ho",
            "Howard Leung"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Creating realistic characters that can react to the users' or another\ncharacter's movement can benefit computer graphics, games and virtual reality\nhugely. However, synthesizing such reactive motions in human-human interactions\nis a challenging task due to the many different ways two humans can interact.\nWhile there are a number of successful researches in adapting the generative\nadversarial network (GAN) in synthesizing single human actions, there are very\nfew on modelling human-human interactions. In this paper, we propose a\nsemi-supervised GAN system that synthesizes the reactive motion of a character\ngiven the active motion from another character. Our key insights are two-fold.\nFirst, to effectively encode the complicated spatial-temporal information of a\nhuman motion, we empower the generator with a part-based long short-term memory\n(LSTM) module, such that the temporal movement of different limbs can be\neffectively modelled. We further include an attention module such that the\ntemporal significance of the interaction can be learned, which enhances the\ntemporal alignment of the active-reactive motion pair. Second, as the reactive\nmotion of different types of interactions can be significantly different, we\nintroduce a discriminator that not only tells if the generated movement is\nrealistic or not, but also tells the class label of the interaction. This\nallows the use of such labels in supervising the training of the generator. We\nexperiment with the SBU and the HHOI datasets. The high quality of the\nsynthetic motion demonstrates the effective design of our generator, and the\ndiscriminability of the synthesis also demonstrates the strength of our\ndiscriminator.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.00380v1"
    },
    {
        "title": "Neural Implicit Surfaces for Efficient and Accurate Collisions in\n  Physically Based Simulations",
        "authors": [
            "Hugo Bertiche",
            "Meysam Madadi",
            "Sergio Escalera"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Current trends in the computer graphics community propose leveraging the\nmassive parallel computational power of GPUs to accelerate physically based\nsimulations. Collision detection and solving is a fundamental part of this\nprocess. It is also the most significant bottleneck on physically based\nsimulations and it easily becomes intractable as the number of vertices in the\nscene increases. Brute force approaches carry a quadratic growth in both\ncomputational time and memory footprint. While their parallelization is trivial\nin GPUs, their complexity discourages from using such approaches. Acceleration\nstructures -- such as BVH -- are often applied to increase performance,\nachieving logarithmic computational times for individual point queries.\nNonetheless, their memory footprint also grows rapidly and their\nparallelization in a GPU is problematic due to their branching nature. We\npropose using implicit surface representations learnt through deep learning for\ncollision handling in physically based simulations. Our proposed architecture\nhas a complexity of O(n) -- or O(1) for a single point query -- and has no\nparallelization issues. We will show how this permits accurate and efficient\ncollision handling in physically based simulations, more specifically, for\ncloth. In our experiments, we query up to 1M points in 300 milliseconds.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.01614v1"
    },
    {
        "title": "Metameric Varifocal Holography",
        "authors": [
            "David R. Walton",
            "Koray Kavaklı",
            "Rafael Kuffner dos Anjos",
            "David Swapp",
            "Tim Weyrich",
            "Hakan Urey",
            "Anthony Steed",
            "Tobias Ritschel",
            "Kaan Akşit"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Computer-Generated Holography (CGH) offers the potential for genuine,\nhigh-quality three-dimensional visuals. However, fulfilling this potential\nremains a practical challenge due to computational complexity and visual\nquality issues. We propose a new CGH method that exploits gaze-contingency and\nperceptual graphics to accelerate the development of practical holographic\ndisplay systems. Firstly, our method infers the user's focal depth and\ngenerates images only at their focus plane without using any moving parts.\nSecond, the images displayed are metamers; in the user's peripheral vision,\nthey need only be statistically correct and blend with the fovea seamlessly.\nUnlike previous methods, our method prioritises and improves foveal visual\nquality without causing perceptually visible distortions at the periphery. To\nenable our method, we introduce a novel metameric loss function that robustly\ncompares the statistics of two given images for a known gaze location. In\nparallel, we implement a model representing the relation between holograms and\ntheir image reconstructions. We couple our differentiable loss function and\nmodel to metameric varifocal holograms using a stochastic gradient descent\nsolver. We evaluate our method with an actual proof-of-concept holographic\ndisplay, and we show that our CGH method leads to practical and perceptually\nthree-dimensional image reconstructions.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.01981v2"
    },
    {
        "title": "Semantic Resizing of Charts Through Generalization:A Case Study with\n  Line Charts",
        "authors": [
            "Vidya Setlur",
            "Haeyong Chung"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Inspired by cartographic generalization principles, we present a\ngeneralization technique for rendering line charts at different sizes,\npreserving the important semantics of the data at that display size. The\nalgorithm automatically determines the generalization operators to be applied\nat that size based on spatial density, distance, and the semantic importance of\nthe various visualization elements in the line chart. A qualitative evaluation\nof the prototype that implemented the algorithm indicates that the generalized\nline charts pre-served the general data shape, while minimizing visual clutter.\nWe identify future opportunities where generalization can be extended and\napplied to other chart types and visual analysis authoring tools.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.12601v1"
    },
    {
        "title": "Stochastic Rounding for Image Interpolation and Scan Conversion",
        "authors": [
            "Olivier Rukundo",
            "Samuel Emil Schmidt"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The stochastic rounding (SR) function is proposed to evaluate and demonstrate\nthe effects of stochastically rounding row and column subscripts in image\ninterpolation and scan conversion. The proposed SR function is based on a\npseudorandom number, enabling the pseudorandom rounding up or down any\nnon-integer row and column subscripts. Also, the SR function exceptionally\nenables rounding up any possible cases of subscript inputs that are inferior to\na pseudorandom number. The algorithm of interest is the nearest-neighbor\ninterpolation (NNI) which is traditionally based on the deterministic rounding\n(DR) function. Experimental simulation results are provided to demonstrate the\nperformance of NNI-SR and NNI-DR algorithms before and after applying smoothing\nand sharpening filters of interest. Additional results are also provided to\ndemonstrate the performance of NNI-SR and NNI-DR interpolated scan conversion\nalgorithms in cardiac ultrasound videos.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.12983v2"
    },
    {
        "title": "Clustering of the Blendshape Facial Model",
        "authors": [
            "Stevo Racković",
            "Cláudia Soares",
            "Dušan Jakovetić",
            "Zoranka Desnica",
            "Relja Ljubobratović"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Digital human animation relies on high-quality 3D models of the human face --\nrigs. A face rig must be accurate and, at the same time, fast to compute. One\nof the most common rigging models is the blendshape model. We present a novel\napproach for learning the inverse rig parameters at increased accuracy and\ndecreased computational cost at the same time. It is based on a two-fold\nclustering of the blendshape face model. Our method focuses exclusively on the\nunderlying space of deformation and produces clusters in both the mesh space\nand the controller space -- something that was not investigated in previous\nliterature. This segmentation finds intuitive and meaningful connections\nbetween groups of vertices on the face and deformation controls, and further\nthese segments can be observed independently. A separate model for solving the\ninverse rig problem is then learned for each segment. Our method is completely\nunsupervised and highly parallelizable.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.15313v1"
    },
    {
        "title": "Principles towards Real-Time Simulation of Material Point Method on\n  Modern GPUs",
        "authors": [
            "Yun Fei",
            "Yuhan Huang",
            "Ming Gao"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Physics-based simulation has been actively employed in generating offline\nvisual effects in the film and animation industry. However, the computations\nrequired for high-quality scenarios are generally immense, deterring its\nadoption in real-time applications, e.g., virtual production, avatar\nlive-streaming, and cloud gaming. We summarize the principles that can\naccelerate the computation pipeline on single-GPU and multi-GPU platforms\nthrough extensive investigation and comprehension of modern GPU architecture.\nWe further demonstrate the effectiveness of these principles by applying them\nto the material point method to build up our framework, which achieves\n$1.7\\times$--$8.6\\times$ speedup on a single GPU and $2.5\\times$--$14.8\\times$\non four GPUs compared to the state-of-the-art. Our pipeline is specifically\ndesigned for real-time applications (i.e., scenarios with small to medium\nparticles) and achieves significant multi-GPU efficiency. We demonstrate our\npipeline by simulating a snow scenario with 1.33M particles and a fountain\nscenario with 143K particles in real-time (on average, 68.5 and 55.9\nframe-per-second, respectively) on four NVIDIA Tesla V100 GPUs interconnected\nwith NVLinks.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.00699v1"
    },
    {
        "title": "OctField: Hierarchical Implicit Functions for 3D Modeling",
        "authors": [
            "Jia-Heng Tang",
            "Weikai Chen",
            "Jie Yang",
            "Bo Wang",
            "Songrun Liu",
            "Bo Yang",
            "Lin Gao"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Recent advances in localized implicit functions have enabled neural implicit\nrepresentation to be scalable to large scenes. However, the regular subdivision\nof 3D space employed by these approaches fails to take into account the\nsparsity of the surface occupancy and the varying granularities of geometric\ndetails. As a result, its memory footprint grows cubically with the input\nvolume, leading to a prohibitive computational cost even at a moderately dense\ndecomposition. In this work, we present a learnable hierarchical implicit\nrepresentation for 3D surfaces, coded OctField, that allows high-precision\nencoding of intricate surfaces with low memory and computational budget. The\nkey to our approach is an adaptive decomposition of 3D scenes that only\ndistributes local implicit functions around the surface of interest. We achieve\nthis goal by introducing a hierarchical octree structure to adaptively\nsubdivide the 3D space according to the surface occupancy and the richness of\npart geometry. As octree is discrete and non-differentiable, we further propose\na novel hierarchical network that models the subdivision of octree cells as a\nprobabilistic process and recursively encodes and decodes both octree structure\nand surface geometry in a differentiable manner. We demonstrate the value of\nOctField for a range of shape modeling and reconstruction tasks, showing\nsuperiority over alternative approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.01067v1"
    },
    {
        "title": "Geometrical holographic display",
        "authors": [
            "Guangjun Wang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Is it possible to realize a holographic display with commercial available\ncomponents and devices? Is it possible to manipulate light to reconstruct light\nfield without using coherent light and complicated optical components? Is it\npossible to minimize the amount of date involved in building 3D scenes? Is it\npossible to design a holographic video display that doesn't require huge\ncomputational cost? Is it possible to realize a holographic video display with\nportable form like a flat-panel display commercialized nowadays? This research\ngives yes answers to all the above questions. A novel geometrical holographic\ndisplay was proposed, which uses geometrical optical principle to reproduce\nrealistic 3D images with all human visual cues and without visual side effects.\nIn addition, a least necessary light field representation was introduced which\ncan provide guidance for how to minimize the amount of data involved when\ndesigning a FP3D or building 3D scenes. Finally, a proof-of-concept prototype\nis set up which can provide true 3D scenes with depth range larger than 5m.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.01909v1"
    },
    {
        "title": "Neural BRDFs: Representation and Operations",
        "authors": [
            "Jiahui Fan",
            "Beibei Wang",
            "Miloš Hašan",
            "Jian Yang",
            "Ling-Qi Yan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Bidirectional reflectance distribution functions (BRDFs) are pervasively used\nin computer graphics to produce realistic physically-based appearance. In\nrecent years, several works explored using neural networks to represent BRDFs,\ntaking advantage of neural networks' high compression rate and their ability to\nfit highly complex functions. However, once represented, the BRDFs will be\nfixed and therefore lack flexibility to take part in follow-up operations. In\nthis paper, we present a form of \"Neural BRDF algebra\", and focus on both\nrepresentation and operations of BRDFs at the same time. We propose a\nrepresentation neural network to compress BRDFs into latent vectors, which is\nable to represent BRDFs accurately. We further propose several operations that\ncan be applied solely in the latent space, such as layering and interpolation.\nSpatial variation is straightforward to achieve by using textures of latent\nvectors. Furthermore, our representation can be efficiently evaluated and\nsampled, providing a competitive solution to more expensive Monte Carlo\nlayering approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.03797v2"
    },
    {
        "title": "Comparative Analysis of Merge Trees using Local Tree Edit Distance",
        "authors": [
            "Raghavendra Sridharamurthy",
            "Vijay Natarajan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Comparative analysis of scalar fields is an important problem with various\napplications including feature-directed visualization and feature tracking in\ntime-varying data. Comparing topological structures that are abstract and\nsuccinct representations of the scalar fields lead to faster and meaningful\ncomparison. While there are many distance or similarity measures to compare\ntopological structures in a global context, there are no known measures for\ncomparing topological structures locally. While the global measures have many\napplications, they do not directly lend themselves to fine-grained analysis\nacross multiple scales. We define a local variant of the tree edit distance and\napply it towards local comparative analysis of merge trees with support for\nfiner analysis. We also present experimental results on time-varying scalar\nfields, 3D cryo-electron microscopy data, and other synthetic data sets to show\nthe utility of this approach in applications like symmetry detection and\nfeature tracking.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.04382v1"
    },
    {
        "title": "Advances in Neural Rendering",
        "authors": [
            "Ayush Tewari",
            "Justus Thies",
            "Ben Mildenhall",
            "Pratul Srinivasan",
            "Edgar Tretschk",
            "Yifan Wang",
            "Christoph Lassner",
            "Vincent Sitzmann",
            "Ricardo Martin-Brualla",
            "Stephen Lombardi",
            "Tomas Simon",
            "Christian Theobalt",
            "Matthias Niessner",
            "Jonathan T. Barron",
            "Gordon Wetzstein",
            "Michael Zollhoefer",
            "Vladislav Golyanik"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Synthesizing photo-realistic images and videos is at the heart of computer\ngraphics and has been the focus of decades of research. Traditionally,\nsynthetic images of a scene are generated using rendering algorithms such as\nrasterization or ray tracing, which take specifically defined representations\nof geometry and material properties as input. Collectively, these inputs define\nthe actual scene and what is rendered, and are referred to as the scene\nrepresentation (where a scene consists of one or more objects). Example scene\nrepresentations are triangle meshes with accompanied textures (e.g., created by\nan artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g.,\nfrom a CT scan), or implicit surface functions (e.g., truncated signed distance\nfields). The reconstruction of such a scene representation from observations\nusing differentiable rendering losses is known as inverse graphics or inverse\nrendering. Neural rendering is closely related, and combines ideas from\nclassical computer graphics and machine learning to create algorithms for\nsynthesizing images from real-world observations. Neural rendering is a leap\nforward towards the goal of synthesizing photo-realistic image and video\ncontent. In recent years, we have seen immense progress in this field through\nhundreds of publications that show different ways to inject learnable\ncomponents into the rendering pipeline. This state-of-the-art report on\nadvances in neural rendering focuses on methods that combine classical\nrendering principles with learned 3D scene representations, often now referred\nto as neural scene representations. A key advantage of these methods is that\nthey are 3D-consistent by design, enabling applications such as novel viewpoint\nsynthesis of a captured scene. In addition to methods that handle static\nscenes, we cover neural scene representations for modeling non-rigidly\ndeforming objects...\n",
        "pdf_link": "http://arxiv.org/pdf/2111.05849v2"
    },
    {
        "title": "ConTesse: Accurate Occluding Contours for Subdivision Surfaces",
        "authors": [
            "Chenxi Liu",
            "Pierre Bénard",
            "Aaron Hertzmann",
            "Shayan Hoshyari"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This paper proposes a method for computing the visible occluding contours of\nsubdivision surfaces. The paper first introduces new theory for contour\nvisibility of smooth surfaces. Necessary and sufficient conditions are\nintroduced for when a sampled occluding contour is valid, that is, when it may\nbe assigned consistent visibility. Previous methods do not guarantee these\nconditions, which helps explain why smooth contour visibility has been such a\nchallenging problem in the past. The paper then proposes an algorithm that,\ngiven a subdivision surface, finds sampled contours satisfying these\nconditions, and then generates a new triangle mesh matching the given occluding\ncontours. The contours of the output triangle mesh may then be rendered with\nstandard non-photorealistic rendering algorithms, using the mesh for visibility\ncomputation. The method can be applied to any triangle mesh, by treating it as\nthe base mesh of a subdivision surface.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.06006v3"
    },
    {
        "title": "Neuromuscular Control of the Face-Head-Neck Biomechanical Complex With\n  Learning-Based Expression Transfer From Images and Videos",
        "authors": [
            "Xiao S. Zeng",
            "Surya Dwarakanath",
            "Wuyue Lu",
            "Masaki Nakada",
            "Demetri Terzopoulos"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The transfer of facial expressions from people to 3D face models is a classic\ncomputer graphics problem. In this paper, we present a novel, learning-based\napproach to transferring facial expressions and head movements from images and\nvideos to a biomechanical model of the face-head-neck complex. Leveraging the\nFacial Action Coding System (FACS) as an intermediate representation of the\nexpression space, we train a deep neural network to take in FACS Action Units\n(AUs) and output suitable facial muscle and jaw activation signals for the\nmusculoskeletal model. Through biomechanical simulation, the activations deform\nthe facial soft tissues, thereby transferring the expression to the model. Our\napproach has advantages over previous approaches. First, the facial expressions\nare anatomically consistent as our biomechanical model emulates the relevant\nanatomy of the face, head, and neck. Second, by training the neural network\nusing data generated from the biomechanical model itself, we eliminate the\nmanual effort of data collection for expression transfer. The success of our\napproach is demonstrated through experiments involving the transfer onto our\nface-head-neck model of facial expressions and head poses from a range of\nfacial images and videos.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.06517v1"
    },
    {
        "title": "Rhythm is a Dancer: Music-Driven Motion Synthesis with Global Structure",
        "authors": [
            "Andreas Aristidou",
            "Anastasios Yiannakidis",
            "Kfir Aberman",
            "Daniel Cohen-Or",
            "Ariel Shamir",
            "Yiorgos Chrysanthou"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Synthesizing human motion with a global structure, such as a choreography, is\na challenging task. Existing methods tend to concentrate on local smooth pose\ntransitions and neglect the global context or the theme of the motion. In this\nwork, we present a music-driven motion synthesis framework that generates\nlong-term sequences of human motions which are synchronized with the input\nbeats, and jointly form a global structure that respects a specific dance\ngenre. In addition, our framework enables generation of diverse motions that\nare controlled by the content of the music, and not only by the beat. Our\nmusic-driven dance synthesis framework is a hierarchical system that consists\nof three levels: pose, motif, and choreography. The pose level consists of an\nLSTM component that generates temporally coherent sequences of poses. The motif\nlevel guides sets of consecutive poses to form a movement that belongs to a\nspecific distribution using a novel motion perceptual-loss. And the\nchoreography level selects the order of the performed movements and drives the\nsystem to follow the global structure of a dance genre. Our results demonstrate\nthe effectiveness of our music-driven framework to generate natural and\nconsistent movements on various dance types, having control over the content of\nthe synthesized motions, and respecting the overall structure of the dance.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.12159v1"
    },
    {
        "title": "Path Guiding Using Spatio-Directional Mixture Models",
        "authors": [
            "Ana Dodik",
            "Marios Papas",
            "Cengiz Öztireli",
            "Thomas Müller"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We propose a learning-based method for light-path construction in path\ntracing algorithms, which iteratively optimizes and samples from what we refer\nto as spatio-directional Gaussian mixture models (SDMMs). In particular, we\napproximate incident radiance as an online-trained $5$D mixture that is\naccelerated by a $k$D-tree. Using the same framework, we approximate BSDFs as\npre-trained $n$D mixtures, where $n$ is the number of BSDF parameters. Such an\napproach addresses two major challenges in path-guiding models. First, the $5$D\nradiance representation naturally captures correlation between the spatial and\ndirectional dimensions. Such correlations are present in e.g. parallax and\ncaustics. Second, by using a tangent-space parameterization of Gaussians, our\nspatio-directional mixtures can perform approximate product sampling with\narbitrarily oriented BSDFs. Existing models are only able to do this by either\nforegoing anisotropy of the mixture components or by representing the radiance\nfield in local (normal aligned) coordinates, which both make the radiance field\nmore difficult to learn. An additional benefit of the tangent-space\nparameterization is that each individual Gaussian is mapped to the solid sphere\nwith low distortion near its center of mass. Our method performs especially\nwell on scenes with small, localized luminaires that induce high\nspatio-directional correlation in the incident radiance.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.13094v2"
    },
    {
        "title": "Is Projection Mapping Natural? Towards Physical World Augmentation\n  Consistent with Light Field Context",
        "authors": [
            "Daisuke Iwai"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Projection mapping seamlessly merges real and virtual worlds. Although much\neffort was made to improve its image qualities so far, projection mapping is\nstill unnatural. We introduce the first steps towards natural projection\nmapping by making the projection results consistent with the light field\ncontext of our daily scene.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.00731v1"
    },
    {
        "title": "Fast Neural Representations for Direct Volume Rendering",
        "authors": [
            "Sebastian Weiss",
            "Philipp Hermüller",
            "Rüdiger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Despite the potential of neural scene representations to effectively compress\n3D scalar fields at high reconstruction quality, the computational complexity\nof the training and data reconstruction step using scene representation\nnetworks limits their use in practical applications. In this paper, we analyze\nwhether scene representation networks can be modified to reduce these\nlimitations and whether such architectures can also be used for temporal\nreconstruction tasks. We propose a novel design of scene representation\nnetworks using GPU tensor cores to integrate the reconstruction seamlessly into\non-chip raytracing kernels, and compare the quality and performance of this\nnetwork to alternative network- and non-network-based compression schemes. The\nresults indicate competitive quality of our design at high compression rates,\nand significantly faster decoding times and lower memory consumption during\ndata reconstruction. We investigate how density gradients can be computed using\nthe network and show an extension where density, gradient and curvature are\npredicted jointly. As an alternative to spatial super-resolution approaches for\ntime-varying fields, we propose a solution that builds upon latent-space\ninterpolation to enable random access reconstruction at arbitrary granularity.\nWe summarize our findings in the form of an assessment of the strengths and\nlimitations of scene representation networks \\changed{for compression domain\nvolume rendering, and outline future research directions.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01579v2"
    },
    {
        "title": "Direction-Oriented Stress-Constrained Topology Optimization of\n  Orthotropic Materials",
        "authors": [
            "Ahmed Moter",
            "Mohamed Abdelhamid",
            "Aleksander Czekanski"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Efficient optimization of topology and raster angle has shown unprecedented\nenhancements in the mechanical properties of 3D printed materials. Topology\noptimization helps reduce the waste of raw material in the fabrication of 3D\nprinted parts, thus decreasing production costs associated with manufacturing\nlighter structures. Fiber orientation plays an important role in increasing the\nstiffness of a structure. This paper develops and tests a new method for\nhandling stress constraints in topology and fiber orientation optimization of\n3D printed orthotropic structures. The stress constraints are coupled with an\nobjective function that maximizes stiffness. This is accomplished by using the\nmodified solid isotropic material with penalization method with the method of\nmoving asymptotes as the mathematical optimizer. Each element has a fictitious\ndensity and an angle as the main design variables. To reduce the number of\nstress constraints and thus the computational cost, a new clustering strategy\nis employed in which the highest stresses in the principal material coordinates\nare grouped separately into two clusters using an adjusted $P$-norm. A detailed\ndescription of the formulation and sensitivity analysis is discussed. While we\npresent an analysis of 2D structures in the numerical examples section, the\nmethod can also be used for 3D structures, as the formulation is generic. Our\nresults show that this method can produce efficient structures suitable for 3D\nprinting while thresholding the stresses.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.02030v2"
    },
    {
        "title": "N-Cloth: Predicting 3D Cloth Deformation with Mesh-Based Networks",
        "authors": [
            "Yudi Li",
            "Min Tang",
            "Yun Yang",
            "Zi Huang",
            "Ruofeng Tong",
            "Shuangcai Yang",
            "Yao Li",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a novel mesh-based learning approach (N-Cloth) for plausible 3D\ncloth deformation prediction. Our approach is general and can handle cloth or\nobstacles represented by triangle meshes with arbitrary topologies. We use\ngraph convolution to transform the cloth and object meshes into a latent space\nto reduce the non-linearity in the mesh space. Our network can predict the\ntarget 3D cloth mesh deformation based on the initial state of the cloth mesh\ntemplate and the target obstacle mesh. Our approach can handle complex cloth\nmeshes with up to 100K triangles and scenes with various objects corresponding\nto SMPL humans, non-SMPL humans or rigid bodies. In practice, our approach can\nbe used to generate plausible cloth simulation at 30-45 fps on an NVIDIA\nGeForce RTX 3090 GPU. We highlight its benefits over prior learning-based\nmethods and physically-based cloth simulators.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.06397v3"
    },
    {
        "title": "3D-TSV: The 3D Trajectory-based Stress Visualizer",
        "authors": [
            "Junpeng Wang",
            "Christoph Neuhauser",
            "Jun Wu",
            "Xifeng Gao",
            "Rüdiger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present the 3D Trajectory-based Stress Visualizer (3D-TSV), a visual\nanalysis tool for the exploration of the principal stress directions in 3D\nsolids under load. 3D-TSV provides a modular and generic implementation of key\nalgorithms required for a trajectory-based visual analysis of principal stress\ndirections, including the automatic seeding of space-filling stress lines,\ntheir extraction using numerical schemes, their mapping to an effective\nrenderable representation, and rendering options to convey structures with\nspecial mechanical properties. In the design of 3D-TSV, several perceptual\nchallenges have been addressed when simultaneously visualizing three mutually\northogonal stress directions via lines. We present a novel algorithm for\ngenerating a space-filling and evenly spaced set of mutually orthogonal lines.\nThe algorithm further considers the locations of lines to obtain a more regular\nappearance, and enables the extraction of a level-of-detail representation with\nadjustable sparseness of the trajectories along a certain stress direction. To\nconvey ambiguities in the orientation of the principal stress directions, the\nuser can select a combined visualization of two principal directions via\noriented ribbons. Additional depth cues improve the perception of the spatial\nrelationships between trajectories. 3D-TSV is accessible to end users via a\nC++- and OpenGL-based rendering frontend that is seamlessly connected to a\nMatLab-based extraction backend. The code (BSD license) of 3D-TSV as well as\nscripts to make ANSYS and ABAQUS simulation results accessible to the 3D-TSV\nbackend are publicly available.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.09202v3"
    },
    {
        "title": "Weaving patterns inspired by the pentagon snub subdivision scheme",
        "authors": [
            "Henriette Lipschütz",
            "Ulrich Reitebuch",
            "Martin Skrodzki",
            "Konrad Polthier"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Various computer simulations regarding, e.g., the weather or structural\nmechanics, solve complex problems on a two-dimensional domain. They mostly do\nso by splitting the input domain into a finite set of smaller and simpler\nelements on which the simulation can be run fast and efficiently. This process\nof splitting can be automatized by using subdivision schemes. Given the wide\nrange of simulation problems to be tackled, an equally wide range of\nsubdivision schemes is available. They create subdivisions that are (mainly)\ncomprised of triangles, quadrilaterals, or hexagons. Furthermore, they ensure\nthat (almost) all vertices have the same number of neighboring vertices. This\npaper illustrates a subdivision scheme that splits the input domain into\npentagons. Repeated application of the scheme gives rise to fractal-like\nstructures. Furthermore, the resulting subdivided domain admits to certain\nweaving patterns. These patterns are subsequently generalized to several other\nsubdivision schemes. As a final contribution, we provide paper models\nillustrating the weaving patterns induced by the pentagonal subdivision scheme.\nFurthermore, we present a jigsaw puzzle illustrating both the subdivision\nprocess and the induced weaving pattern. These transform the visual and\nabstract mathematical algorithms into tactile objects that offer exploration\npossibilities aside from the visual.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.15057v2"
    },
    {
        "title": "Embodied Hands: Modeling and Capturing Hands and Bodies Together",
        "authors": [
            "Javier Romero",
            "Dimitrios Tzionas",
            "Michael J. Black"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Humans move their hands and bodies together to communicate and solve tasks.\nCapturing and replicating such coordinated activity is critical for virtual\ncharacters that behave realistically. Surprisingly, most methods treat the 3D\nmodeling and tracking of bodies and hands separately. Here we formulate a model\nof hands and bodies interacting together and fit it to full-body 4D sequences.\nWhen scanning or capturing the full body in 3D, hands are small and often\npartially occluded, making their shape and pose hard to recover. To cope with\nlow-resolution, occlusion, and noise, we develop a new model called MANO (hand\nModel with Articulated and Non-rigid defOrmations). MANO is learned from around\n1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand\nposes. The model is realistic, low-dimensional, captures non-rigid shape\nchanges with pose, is compatible with standard graphics packages, and can fit\nany human hand. MANO provides a compact mapping from hand poses to pose blend\nshape corrections and a linear manifold of pose synergies. We attach MANO to a\nstandard parameterized 3D body shape model (SMPL), resulting in a fully\narticulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting\ncomplex, natural, activities of subjects captured with a 4D scanner. The\nfitting is fully automatic and results in full body models that move naturally\nwith detailed hand motions and a realism not seen before in full body\nperformance capture. The models and data are freely available for research\npurposes in our website (http://mano.is.tue.mpg.de).\n",
        "pdf_link": "http://arxiv.org/pdf/2201.02610v1"
    },
    {
        "title": "Grassmannian Shape Representations for Aerodynamic Applications",
        "authors": [
            "Olga A. Doronina",
            "Zachary J. Grey",
            "Andrew Glaws"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Airfoil shape design is a classical problem in engineering and manufacturing.\nOur motivation is to combine principled physics-based considerations for the\nshape design problem with modern computational techniques informed by a\ndata-driven approach. Traditional analyses of airfoil shapes emphasize a\nflow-based sensitivity to deformations which can be represented generally by\naffine transformations (rotation, scaling, shearing, translation). We present a\nnovel representation of shapes which decouples affine-style deformations from a\nrich set of data-driven deformations over a submanifold of the Grassmannian.\nThe Grassmannian representation, informed by a database of physically relevant\nairfoils, offers (i) a rich set of novel 2D airfoil deformations not previously\ncaptured in the data, (ii) improved low-dimensional parameter domain for\ninferential statistics informing design/manufacturing, and (iii) consistent 3D\nblade representation and perturbation over a sequence of nominal shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.04649v1"
    },
    {
        "title": "An Open Platform for Research about Cognitive Load in Virtual Reality",
        "authors": [
            "Olivier Augereau",
            "Gabriel Brocheton",
            "Pedro Paulo Do Prado Neto"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The cognitive load can be used to assess if someone is struggling while\nperforming a task. It can be used in many different situations such as in\ndriving, piloting, studying, playing, working, etc. This information can help\nto design better systems and even to create interactive systems that can be\naware of the user's cognitive load and adapt itself to the user. We propose an\nopen source platform that can be used for doing research about cognitive load\nin virtual reality (VR). Our platform can be used for stimulating cognitive\nload through several VR scenes and for analyzing cognitive load through\nobjective and subjective measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.06273v1"
    },
    {
        "title": "A Real-Time Rendering Method for Light Field Display",
        "authors": [
            "Quanzhen Wan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  A real-time elemental image array (EIA) generation method which does not\nsacrifice accuracy nor rely on high-performance hardware is developed, through\nraytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both\noffline and online working flow, experiments will verified the effectiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.08266v4"
    },
    {
        "title": "Exploring Differential Geometry in Neural Implicits",
        "authors": [
            "Tiago Novello",
            "Guilherme Schardong",
            "Luiz Schirmer",
            "Vinicius da Silva",
            "Helio Lopes",
            "Luiz Velho"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce a neural implicit framework that exploits the differentiable\nproperties of neural networks and the discrete geometry of point-sampled\nsurfaces to approximate them as the level sets of neural implicit functions.\n  To train a neural implicit function, we propose a loss functional that\napproximates a signed distance function, and allows terms with high-order\nderivatives, such as the alignment between the principal directions of\ncurvature, to learn more geometric details. During training, we consider a\nnon-uniform sampling strategy based on the curvatures of the point-sampled\nsurface to prioritize points with more geometric details. This sampling implies\nfaster learning while preserving geometric accuracy when compared with previous\napproaches.\n  We also use the analytical derivatives of a neural implicit function to\nestimate the differential measures of the underlying point-sampled surface.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.09263v4"
    },
    {
        "title": "Sketch-based 3D Shape Modeling from Sparse Point Clouds",
        "authors": [
            "Xusheng Du",
            "Yi He",
            "Xi Yang",
            "Chia-Ming Chang",
            "Haoran Xie"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  3D modeling based on point clouds is an efficient way to reconstruct and\ncreate detailed 3D content. However, the geometric procedure may lose accuracy\ndue to high redundancy and the absence of an explicit structure. In this work,\nwe propose a human-in-the-loop sketch-based point cloud reconstruction\nframework to leverage users cognitive abilities in geometry extraction. We\npresent an interactive drawing interface for 3D model creation from point cloud\ndata with the help of user sketches. We adopt an optimization method in which\nthe user can continuously edit the contours extracted from the obtained 3D\nmodel and retrieve the model iteratively. Finally, we verify the proposed user\ninterface for modeling from sparse point clouds. see video here\nhttps://www.youtube.com/watch?v=0H19NyXDRJE .\n",
        "pdf_link": "http://arxiv.org/pdf/2201.11287v1"
    },
    {
        "title": "Wassersplines for Neural Vector Field--Controlled Animation",
        "authors": [
            "Paul Zhang",
            "Dmitriy Smirnov",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Much of computer-generated animation is created by manipulating meshes with\nrigs. While this approach works well for animating articulated objects like\nanimals, it has limited flexibility for animating less structured free-form\nobjects. We introduce Wassersplines, a novel trajectory inference method for\nanimating unstructured densities based on recent advances in continuous\nnormalizing flows and optimal transport. The key idea is to train a\nneurally-parameterized velocity field that represents the motion between\nkeyframes. Trajectories are then computed by advecting keyframes through the\nvelocity field. We solve an additional Wasserstein barycenter interpolation\nproblem to guarantee strict adherence to keyframes. Our tool can stylize\ntrajectories through a variety of PDE-based regularizers to create different\nvisual effects. We demonstrate our tool on various keyframe interpolation\nproblems to produce temporally-coherent animations without meshing or rigging.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.11940v2"
    },
    {
        "title": "Generative GaitNet",
        "authors": [
            "Jungnam Park",
            "Sehee Min",
            "Phil Sik Chang",
            "Jaedong Lee",
            "Moonseok Park",
            "Jehee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Understanding the relation between anatomy andgait is key to successful\npredictive gait simulation. Inthis paper, we present Generative GaitNet, which\nisa novel network architecture based on deep reinforce-ment learning for\ncontrolling a comprehensive, full-body, musculoskeletal model with 304\nHill-type mus-culotendons. The Generative Gait is a pre-trained, in-tegrated\nsystem of artificial neural networks learnedin a 618-dimensional continuous\ndomain of anatomyconditions (e.g., mass distribution, body proportion,bone\ndeformity, and muscle deficits) and gait condi-tions (e.g., stride and\ncadence). The pre-trained Gait-Net takes anatomy and gait conditions as input\nandgenerates a series of gait cycles appropriate to theconditions through\nphysics-based simulation. We willdemonstrate the efficacy and expressive power\nof Gen-erative GaitNet to generate a variety of healthy andpathologic human\ngaits in real-time physics-based sim-ulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.12044v1"
    },
    {
        "title": "Differentiable Neural Radiosity",
        "authors": [
            "Saeed Hadadan",
            "Matthias Zwicker"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce Differentiable Neural Radiosity, a novel method of representing\nthe solution of the differential rendering equation using a neural network.\nInspired by neural radiosity techniques, we minimize the norm of the residual\nof the differential rendering equation to directly optimize our network. The\nnetwork is capable of outputting continuous, view-independent gradients of the\nradiance field with respect to scene parameters, taking into account\ndifferential global illumination effects while keeping memory and time\ncomplexity constant in path length. To solve inverse rendering problems, we use\na pre-trained instance of our network that represents the differential radiance\nfield with respect to a limited number of scene parameters. In our experiments,\nwe leverage this to achieve faster and more accurate convergence compared to\nother techniques such as Automatic Differentiation, Radiative Backpropagation,\nand Path Replay Backpropagation.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.13190v1"
    },
    {
        "title": "Extension: Adaptive Sampling with Implicit Radiance Field",
        "authors": [
            "Yuchi Huo"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This manuscript discusses the extension of adaptive light field sampling with\nimplicit radiance fields.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.00855v3"
    },
    {
        "title": "Eikonal Fields for Refractive Novel-View Synthesis",
        "authors": [
            "Mojtaba Bemana",
            "Karol Myszkowski",
            "Jeppe Revall Frisvad",
            "Hans-Peter Seidel",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We tackle the problem of generating novel-view images from collections of 2D\nimages showing refractive and reflective objects. Current solutions assume\nopaque or transparent light transport along straight paths following the\nemission-absorption model. Instead, we optimize for a field of 3D-varying Index\nof Refraction (IoR) and trace light through it that bends toward the spatial\ngradients of said IoR according to the laws of eikonal light transport.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.00948v3"
    },
    {
        "title": "Multi-Criteria Assessment of Shape Quality in CAD Systems of the Future",
        "authors": [
            "Valerijan Muftejev",
            "Rushan Ziatdinov",
            "Rifkat Nabiyev"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Unlike many other works, where authors are usually focused on one or two\nquality criteria, the current manuscript, which is a generalization of the\narticle [35] published in Russian, offers a multi-criteria approach to the\nassessment of the shape quality of curves that constitute component parts of\nthe surfaces used for the computer modelling of object shapes in various types\nof design. Based on the analysis of point particle motion along a curved path,\nrequirements for the quality of functional curves are proposed: a high order of\nsmoothness, a minimum number of curvature extrema, minimization of the maximum\nvalue of curvature and its variation rate, minimization of the potential energy\nof the curve, and aesthetic analysis from the standpoint of the laws of\ntechnical aesthetics. The authors do not set themselves the task of giving a\nsimple and precise mathematical definition of such curves. On the contrary,\nthis category can include various curves that meet certain quality criteria,\nthe refinement and addition of which is possible in the near future.\nEngineering practice shows that quality criteria can change over time, which\ndoes not diminish the need to develop multi-criteria methods for assessing the\nquality of geometric shapes. Technical issues faced during edge rounding in 3D\nmodels that affect the quality of industrial design product shape have been\nreviewed as an example of the imperfection of existing CAD systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.01428v1"
    },
    {
        "title": "Symmetric Volume Maps: Order-Invariant Volumetric Mesh Correspondence\n  with Free Boundary",
        "authors": [
            "S. Mazdak Abulnaga",
            "Oded Stein",
            "Polina Golland",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Although shape correspondence is a central problem in geometry processing,\nmost methods for this task apply only to two-dimensional surfaces. The\nneglected task of volumetric correspondence--a natural extension relevant to\nshapes extracted from simulation, medical imaging, and volume\nrendering--presents unique challenges that do not appear in the two-dimensional\ncase. In this work, we propose a method for mapping between volumes represented\nas tetrahedral meshes. Our formulation minimizes a distortion energy designed\nto extract maps symmetrically, i.e., without dependence on the ordering of the\nsource and target domains. We accompany our method with theoretical discussion\ndescribing the consequences of this symmetry assumption, leading us to select a\nsymmetrized ARAP energy that favors isometric correspondences. Our final\nformulation optimizes for near-isometry while matching the boundary. We\ndemonstrate our method on a diverse geometric dataset, producing low-distortion\nmatchings that align closely to the boundary.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02568v2"
    },
    {
        "title": "Motion Puzzle: Arbitrary Motion Style Transfer by Body Part",
        "authors": [
            "Deok-Kyeong Jang",
            "Soomin Park",
            "Sung-Hee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This paper presents Motion Puzzle, a novel motion style transfer network that\nadvances the state-of-the-art in several important respects. The Motion Puzzle\nis the first that can control the motion style of individual body parts,\nallowing for local style editing and significantly increasing the range of\nstylized motions. Designed to keep the human's kinematic structure, our\nframework extracts style features from multiple style motions for different\nbody parts and transfers them locally to the target body parts. Another major\nadvantage is that it can transfer both global and local traits of motion style\nby integrating the adaptive instance normalization and attention modules while\nkeeping the skeleton topology. Thus, it can capture styles exhibited by dynamic\nmovements, such as flapping and staggering, significantly better than previous\nwork. In addition, our framework allows for arbitrary motion style transfer\nwithout datasets with style labeling or motion pairing, making many publicly\navailable motion datasets available for training. Our framework can be easily\nintegrated with motion generation frameworks to create many applications, such\nas real-time motion transfer. We demonstrate the advantages of our framework\nwith a number of examples and comparisons with previous work.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05274v2"
    },
    {
        "title": "Artemis: Articulated Neural Pets with Appearance and Motion synthesis",
        "authors": [
            "Haimin Luo",
            "Teng Xu",
            "Yuheng Jiang",
            "Chenglin Zhou",
            "Qiwei Qiu",
            "Yingliang Zhang",
            "Wei Yang",
            "Lan Xu",
            "Jingyi Yu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We, humans, are entering into a virtual era and indeed want to bring animals\nto the virtual world as well for companion. Yet, computer-generated (CGI) furry\nanimals are limited by tedious off-line rendering, let alone interactive motion\ncontrol. In this paper, we present ARTEMIS, a novel neural modeling and\nrendering pipeline for generating ARTiculated neural pets with appEarance and\nMotion synthesIS. Our ARTEMIS enables interactive motion control, real-time\nanimation, and photo-realistic rendering of furry animals. The core of our\nARTEMIS is a neural-generated (NGI) animal engine, which adopts an efficient\noctree-based representation for animal animation and fur rendering. The\nanimation then becomes equivalent to voxel-level deformation based on explicit\nskeletal warping. We further use a fast octree indexing and efficient\nvolumetric rendering scheme to generate appearance and density features maps.\nFinally, we propose a novel shading network to generate high-fidelity details\nof appearance and opacity under novel poses from appearance and density feature\nmaps. For the motion control module in ARTEMIS, we combine state-of-the-art\nanimal motion capture approach with recent neural character control scheme. We\nintroduce an effective optimization scheme to reconstruct the skeletal motion\nof real animals captured by a multi-view RGB and Vicon camera array. We feed\nall the captured motion into a neural character control scheme to generate\nabstract control signals with motion styles. We further integrate ARTEMIS into\nexisting engines that support VR headsets, providing an unprecedented immersive\nexperience where a user can intimately interact with a variety of virtual\nanimals with vivid movements and photo-realistic appearance. We make available\nour ARTEMIS model and dynamic furry animal dataset at\nhttps://haiminluo.github.io/publication/artemis/.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05628v3"
    },
    {
        "title": "Optimal lofted B-spline surface interpolation based on serial closed\n  contours",
        "authors": [
            "Shutao Tang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Modern shape design and capture techniques often lead to the geometric data\npresented in the form of serial rows of data points. In general, the number of\ndata points varies from row to row. Lofted or skinned B-spline surface\ninterpolation is a technique that generates a B-spline surface that passes\nthrough these data points precisely. The traditional process often causes a\nlarge increase in the number of control points of the resulting B-spline\nsurface. Much of the work to date in mitigating the effects of this increase\nhas been restricted to open section-curves. The lofting of sequential closed\ncontours using the interpolation technique has not been addressed in the\nexisting literature. In this paper, we present two novel conjectures relating\nto closed B-spline curve interpolation. We derive the equivalent closed\nB-spline interpolation condition of the well-established Schoenberg-Whitney\ncondition for open B-spline interpolation, a condition that the parameter\nvalues and the domain knots should satisfy to guarantee the system matrix is\nalways invertible or full-rank. We then apply the interpolation condition to\nthe problem of lofted B-spline surface interpolation to serial closed contours.\nThe correctness of these conjectures is validated via numerical results and\nseveral practical experiments. Github repository\nhttps://github.com/ShutaoTang/LBSI-Project\n",
        "pdf_link": "http://arxiv.org/pdf/2202.06330v1"
    },
    {
        "title": "Geometry-Aware Planar Embedding of Treelike Structures",
        "authors": [
            "Ping Hu",
            "Saeed Boorboor",
            "Joseph Marino",
            "Arie E. Kaufman"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The growing complexity of spatial and structural information in 3D data makes\ndata inspection and visualization a challenging task. We describe a method to\ncreate a planar embedding of 3D treelike structures using their skeleton\nrepresentations. Our method maintains the original geometry, without overlaps,\nto the best extent possible, allowing exploration of the topology within a\nsingle view. We present a novel camera view generation method which maximizes\nthe visible geometric attributes (segment shape and relative placement between\nsegments). Camera views are created for individual segments and are used to\ndetermine local bending angles at each node by projecting them to 2D. The final\nembedding is generated by minimizing an energy function (the weights of which\nare user adjustable) based on branch length and the 2D angles, while avoiding\nintersections. The user can also interactively modify segment placement within\nthe 2D embedding, and the overall embedding will update accordingly. A global\nto local interactive exploration is provided using hierarchical camera views\nthat are created for subtrees within the structure. We evaluate our method both\nqualitatively and quantitatively and demonstrate our results by constructing\nplanar visualizations of line data (traced neurons) and volume data (CT\nvascular and bronchial data\n",
        "pdf_link": "http://arxiv.org/pdf/2202.10551v1"
    },
    {
        "title": "Differentiable Iterated Function Systems",
        "authors": [
            "Cory Braker Scott"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This preliminary paper presents initial explorations in rendering Iterated\nFunction System (IFS) fractals using a differentiable rendering pipeline.\nDifferentiable rendering is a recent innovation at the intersection of computer\ngraphics and machine learning. A fractal rendering pipeline composed of\ndifferentiable operations opens up many possibilities for generating fractals\nthat meet particular criteria. In this paper I demonstrate this pipeline by\ngenerating IFS fractals with fixed points that resemble a given target image -\na famous problem known as the \\emph{inverse IFS problem}. The main\ncontributions of this work are as follows: 1) I demonstrate (and make code\navailable) this rendering pipeline; 2) I discuss some of the nuances and\npitfalls in gradient-descent-based optimization over fractal structures; 3) I\ndiscuss best practices to address some of these pitfalls; and finally 4) I\ndiscuss directions for further experiments to validate the technique.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.01231v2"
    },
    {
        "title": "Parametric/direct CAD integration",
        "authors": [
            "Qiang Zou"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In the history of computer-aided design (CAD), feature-based parametric\nmodeling and boundary representation-based direct modeling are two of the most\nimportant CAD paradigms, developed respectively in the late 1980s and the late\n2000s. They have complementary advantages and limitations, thereby offering\nhuge potential for improvement towards an integrated CAD modeling scheme. Some\nbelieve that their integration will be the key characteristic of next\ngeneration CAD software. This paper provides a brief review on current\nparametric/direct integration approaches. Their basic ideas, advantages, and\ndisadvantages will be discussed. The main result reads that existing\nintegration approaches are far from being completed if seamless\nparametric/direct integration is desired. It is hoped that, by outlining what\nhas already been made possible and what still remains problematic, more\nresearchers will be attracted to work on this very important research topic of\nparametric/direct integration.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.02252v1"
    },
    {
        "title": "DrawingInStyles: Portrait Image Generation and Editing with Spatially\n  Conditioned StyleGAN",
        "authors": [
            "Wanchao Su",
            "Hui Ye",
            "Shu-Yu Chen",
            "Lin Gao",
            "Hongbo Fu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The research topic of sketch-to-portrait generation has witnessed a boost of\nprogress with deep learning techniques. The recently proposed StyleGAN\narchitectures achieve state-of-the-art generation ability but the original\nStyleGAN is not friendly for sketch-based creation due to its unconditional\ngeneration nature. To address this issue, we propose a direct conditioning\nstrategy to better preserve the spatial information under the StyleGAN\nframework. Specifically, we introduce Spatially Conditioned StyleGAN\n(SC-StyleGAN for short), which explicitly injects spatial constraints to the\noriginal StyleGAN generation process. We explore two input modalities, sketches\nand semantic maps, which together allow users to express desired generation\nresults more precisely and easily. Based on SC-StyleGAN, we present\nDrawingInStyles, a novel drawing interface for non-professional users to easily\nproduce high-quality, photo-realistic face images with precise control, either\nfrom scratch or editing existing ones. Qualitative and quantitative evaluations\nshow the superior generation ability of our method to existing and alternative\nsolutions. The usability and expressiveness of our system are confirmed by a\nuser study.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.02762v3"
    },
    {
        "title": "A Survey on Reinforcement Learning Methods in Character Animation",
        "authors": [
            "Ariel Kwiatkowski",
            "Eduardo Alvarado",
            "Vicky Kalogeiton",
            "C. Karen Liu",
            "Julien Pettré",
            "Michiel van de Panne",
            "Marie-Paule Cani"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Reinforcement Learning is an area of Machine Learning focused on how agents\ncan be trained to make sequential decisions, and achieve a particular goal\nwithin an arbitrary environment. While learning, they repeatedly take actions\nbased on their observation of the environment, and receive appropriate rewards\nwhich define the objective. This experience is then used to progressively\nimprove the policy controlling the agent's behavior, typically represented by a\nneural network. This trained module can then be reused for similar problems,\nwhich makes this approach promising for the animation of autonomous, yet\nreactive characters in simulators, video games or virtual reality environments.\nThis paper surveys the modern Deep Reinforcement Learning methods and discusses\ntheir possible applications in Character Animation, from skeletal control of a\nsingle, physically-based character to navigation controllers for individual\nagents and virtual crowds. It also describes the practical side of training DRL\nsystems, comparing the different frameworks available to build such agents.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.04735v1"
    },
    {
        "title": "Triangular Character Animation Sampling with Motion, Emotion, and\n  Relation",
        "authors": [
            "Yizhou Zhao",
            "Liang Qiu",
            "Wensi Ai",
            "Pan Lu",
            "Song-Chun Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Dramatic progress has been made in animating individual characters. However,\nwe still lack automatic control over activities between characters, especially\nthose involving interactions. In this paper, we present a novel energy-based\nframework to sample and synthesize animations by associating the characters'\nbody motions, facial expressions, and social relations. We propose a\nSpatial-Temporal And-Or graph (ST-AOG), a stochastic grammar model, to encode\nthe contextual relationship between motion, emotion, and relation, forming a\ntriangle in a conditional random field. We train our model from a labeled\ndataset of two-character interactions. Experiments demonstrate that our method\ncan recognize the social relation between two characters and sample new scenes\nof vivid motion and emotion using Markov Chain Monte Carlo (MCMC) given the\nsocial relation. Thus, our method can provide animators with an automatic way\nto generate 3D character animations, help synthesize interactions between\nNon-Player Characters (NPCs), and enhance machine emotion intelligence (EQ) in\nvirtual reality (VR).\n",
        "pdf_link": "http://arxiv.org/pdf/2203.04930v1"
    },
    {
        "title": "Active Exploration for Neural Global Illumination of Variable Scenes",
        "authors": [
            "Stavros Diolatzis",
            "Julien Philip",
            "George Drettakis"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Neural rendering algorithms introduce a fundamentally new approach for\nphotorealistic rendering, typically by learning a neural representation of\nillumination on large numbers of ground truth images. When training for a given\nvariable scene, i.e., changing objects, materials, lights and viewpoint, the\nspace D of possible training data instances quickly becomes unmanageable as the\ndimensions of variable parameters increase. We introduce a novel Active\nExploration method using Markov Chain Monte Carlo, which explores D, generating\nsamples (i.e., ground truth renderings) that best help training and interleaves\ntraining and on-the-fly sample data generation. We introduce a self-tuning\nsample reuse strategy to minimize the expensive step of rendering training\nsamples. We apply our approach on a neural generator that learns to render\nnovel scene instances given an explicit parameterization of the scene\nconfiguration. Our results show that Active Exploration trains our network much\nmore efficiently than uniformly sampling, and together with our resolution\nenhancement approach, achieves better quality than uniform sampling at\nconvergence. Our method allows interactive rendering of hard light transport\npaths (e.g., complex caustics) -- that require very high samples counts to be\ncaptured -- and provides dynamic scene navigation and manipulation, after\ntraining for 5-18 hours depending on required quality and variations.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08272v1"
    },
    {
        "title": "Microstructure Surface Reconstruction from SEM Images: An Alternative to\n  Digital Image Correlation (DIC)",
        "authors": [
            "Khalid El-Awady"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We reconstruct a 3D model of the surface of a material undergoing fatigue\ntesting and experiencing cracking. Specifically we reconstruct the surface\ndepth (out of plane intrusions and extrusions) and lateral (in-plane) motion\nfrom multiple views of the sample at the end of the experiment, combined with a\nreverse optical flow propagation backwards in time that utilizes interim single\nview images. These measurements can be mapped to a material strain tensor which\nhelps to understand material life and predict failure. This approach offers an\nalternative to the commonly used Digital Image Correlation (DIC) technique\nwhich relies on tracking a speckle pattern applied to the material surface. DIC\nonly produces in-plane (2D) measurements whereas our approach is 3D and\nnon-invasive (requires no pattern being applied to the material).\n",
        "pdf_link": "http://arxiv.org/pdf/2203.13438v1"
    },
    {
        "title": "UrbanFlow: Designing Comfortable Outdoor Areas",
        "authors": [
            "Daoming Liu",
            "Florian Rist",
            "Helmut Pottmann",
            "Dominik Michels"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Design decisions in urban planning have to be made with particular\ncarefulness as the resulting constraints are binding for the whole\narchitectural design that follows. In this context, investigating and\noptimizing the airflow in urban environments is critical to design comfortable\noutdoor areas as unwanted effects such as windy areas and the formation of heat\npockets have to be avoided. Our UrbanFlow framework enables interactive\narchitectural design allowing for decision making based on simulating urban\nflow. Compared to real-time fluid flow simulation, enabling interactive\narchitecture design poses an even higher computational efficiency challenge as\nevaluating a design by simulation usually requires hundreds of time steps. This\nis addressed based on a highly efficient Eulerian fluid simulator in which we\nincorporate a unified porosity model which is devised to encode digital urban\nmodels containing objects such as buildings and trees. UrbanFlow is equipped\nwith an optimization routine enabling the direct computation of design\nadaptations improving livability and comfort for given parameterized\narchitectural designs. To ensure convergence of the optimization process,\ninstead of the classical Navier-Stokes equations, the Reynolds-averaged\nNavier-Stokes equations are solved as this can be done on a relatively coarse\ngrid and allows for the decoupling of the effects of turbulent eddies which are\ntaken into account using a separate turbulence model. As we demonstrate on a\nreal-world example taken from an ongoing architectural competition, this\nresults in a fast convergence of the optimization process which computes a\ndesign adaptation avoiding heat pockets as well as uncomfortable windy areas.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.01117v1"
    },
    {
        "title": "Dressi: A Hardware-Agnostic Differentiable Renderer with Reactive Shader\n  Packing and Soft Rasterization",
        "authors": [
            "Yusuke Takimoto",
            "Hiroyuki Sato",
            "Hikari Takehara",
            "Keishiro Uragaki",
            "Takehiro Tawara",
            "Xiao Liang",
            "Kentaro Oku",
            "Wataru Kishimoto",
            "Bo Zheng"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Differentiable rendering (DR) enables various computer graphics and computer\nvision applications through gradient-based optimization with derivatives of the\nrendering equation. Most rasterization-based approaches are built on\ngeneral-purpose automatic differentiation (AD) libraries and DR-specific\nmodules handcrafted using CUDA. Such a system design mixes DR algorithm\nimplementation and algorithm building blocks, resulting in hardware dependency\nand limited performance. In this paper, we present a practical\nhardware-agnostic differentiable renderer called Dressi, which is based on a\nnew full AD design. The DR algorithms of Dressi are fully written in our\nVulkan-based AD for DR, Dressi-AD, which supports all primitive operations for\nDR. Dressi-AD and our inverse UV technique inside it bring hardware\nindependence and acceleration by graphics hardware. Stage packing, our runtime\noptimization technique, can adapt hardware constraints and efficiently execute\ncomplex computational graphs of DR with reactive cache considering the render\npass hierarchy of Vulkan. HardSoftRas, our novel rendering process, is designed\nfor inverse rendering with a graphics pipeline. Under the limited\nfunctionalities of the graphics pipeline, HardSoftRas can propagate the\ngradients of pixels from the screen space to far-range triangle attributes. Our\nexperiments and applications demonstrate that Dressi establishes hardware\nindependence, high-quality and robust optimization with fast speed, and\nphotorealistic rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.01386v1"
    },
    {
        "title": "Creating good quality meshes from smooth implicit surfaces",
        "authors": [
            "Ágostons Sipos",
            "Péter Salvi"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Visualization of implicit surfaces is an actively researched topic. While\nraytracing can produce high quality images, it is not well suited for creating\na quick preview of the surface. Indirect algorithms (e.g. Marching Cubes)\ncreate an easily renderable triangle mesh, but the result is often not\nsufficiently well-structured for a good approximation of differential surface\nquantities (normals, curvatures, etc.). Post-processing methods usually have a\nconsiderable computational overhead, and high quality is not guaranteed. We\npropose a tessellation algorithm to create nearly isotropic meshes, using\nmulti-sided implicit surfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.06396v1"
    },
    {
        "title": "Data Parallel Path Tracing in Object Space",
        "authors": [
            "Ingo Wald",
            "Steven G Parker"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We investigate the concept of rendering production-style content with full\npath tracing in a data-distributed fashion -- that is, with multiple\ncollaborating nodes and/or GPUs that each store only part of the model. In\nparticular, we propose a new approach to tracing rays across different\nnodes/GPUs that improves over traditional spatial partitioning, can support\nboth object-space and spatial partitioning (or any combination thereof), and\nthat enables multiple techniques for reducing the number of rays sent across\nthe network. We show that this approach can handle different kinds of model\npartitioning strategies, and can ultimately render non-trivial models with full\npath tracing even on quite moderate hardware resources with rather low-end\ninterconnect.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.10170v1"
    },
    {
        "title": "Cellular Topology Optimization on Differentiable Voronoi Diagrams",
        "authors": [
            "Fan Feng",
            "Shiying Xiong",
            "Ziyue Liu",
            "Zangyueyang Xian",
            "Yuqing Zhou",
            "Hiroki Kobayashi",
            "Atsushi Kawamoto",
            "Tsuyoshi Nomura",
            "Bo Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Cellular structures manifest their outstanding mechanical properties in many\nbiological systems. One key challenge for designing and optimizing these\ngeometrically complicated structures lies in devising an effective geometric\nrepresentation to characterize the system's spatially varying cellular\nevolution driven by objective sensitivities. A conventional discrete cellular\nstructure, e.g., a Voronoi diagram, whose representation relies on discrete\nVoronoi cells and faces, lacks its differentiability to facilitate large-scale,\ngradient-based topology optimizations. We propose a topology optimization\nalgorithm based on a differentiable and generalized Voronoi representation that\ncan evolve the cellular structure as a continuous field. The central piece of\nour method is a hybrid particle-grid representation to encode the previously\ndiscrete Voronoi diagram into a continuous density field defined in a Euclidean\nspace. Based on this differentiable representation, we further extend it to\ntackle anisotropic cells, free boundaries, and functionally-graded cellular\nstructures. Our differentiable Voronoi diagram enables the integration of an\neffective cellular representation into the state-of-the-art topology\noptimization pipelines, which defines a novel design space for cellular\nstructures to explore design options effectively that were impractical for\nprevious approaches. We showcase the efficacy of our approach by optimizing\ncellular structures with up to thousands of anisotropic cells, including femur\nbone and Odonata wing.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.10313v3"
    },
    {
        "title": "Real-time Controllable Motion Transition for Characters",
        "authors": [
            "Xiangjun Tang",
            "He Wang",
            "Bo Hu",
            "Xu Gong",
            "Ruifan Yi",
            "Qilong Kou",
            "Xiaogang Jin"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Real-time in-between motion generation is universally required in games and\nhighly desirable in existing animation pipelines. Its core challenge lies in\nthe need to satisfy three critical conditions simultaneously: quality,\ncontrollability and speed, which renders any methods that need offline\ncomputation (or post-processing) or cannot incorporate (often unpredictable)\nuser control undesirable. To this end, we propose a new real-time transition\nmethod to address the aforementioned challenges. Our approach consists of two\nkey components: motion manifold and conditional transitioning. The former\nlearns the important low-level motion features and their dynamics; while the\nlatter synthesizes transitions conditioned on a target frame and the desired\ntransition duration. We first learn a motion manifold that explicitly models\nthe intrinsic transition stochasticity in human motions via a multi-modal\nmapping mechanism. Then, during generation, we design a transition model which\nis essentially a sampling strategy to sample from the learned manifold, based\non the target frame and the aimed transition duration. We validate our method\non different datasets in tasks where no post-processing or offline computation\nis allowed. Through exhaustive evaluation and comparison, we show that our\nmethod is able to generate high-quality motions measured under multiple\nmetrics. Our method is also robust under various target frames (with extreme\ncases).\n",
        "pdf_link": "http://arxiv.org/pdf/2205.02540v1"
    },
    {
        "title": "Photo-to-Shape Material Transfer for Diverse Structures",
        "authors": [
            "Ruizhen Hu",
            "Xiangyu Su",
            "Xiangkai Chen",
            "Oliver Van Kaick",
            "Hui Huang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce a method for assigning photorealistic relightable materials to\n3D shapes in an automatic manner. Our method takes as input a photo exemplar of\na real object and a 3D object with segmentation, and uses the exemplar to guide\nthe assignment of materials to the parts of the shape, so that the appearance\nof the resulting shape is as similar as possible to the exemplar. To accomplish\nthis goal, our method combines an image translation neural network with a\nmaterial assignment neural network. The image translation network translates\nthe color from the exemplar to a projection of the 3D shape and the part\nsegmentation from the projection to the exemplar. Then, the material prediction\nnetwork assigns materials from a collection of realistic materials to the\nprojected parts, based on the translated images and perceptual similarity of\nthe materials. One key idea of our method is to use the translation network to\nestablish a correspondence between the exemplar and shape projection, which\nallows us to transfer materials between objects with diverse structures.\nAnother key idea of our method is to use the two pairs of (color, segmentation)\nimages provided by the image translation to guide the material assignment,\nwhich enables us to ensure the consistency in the assignment. We demonstrate\nthat our method allows us to assign materials to shapes so that their\nappearances better resemble the input exemplars, improving the quality of the\nresults over the state-of-the-art method, and allowing us to automatically\ncreate thousands of shapes with high-quality photorealistic materials. Code and\ndata for this paper are available at https://github.com/XiangyuSu611/TMT.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04018v1"
    },
    {
        "title": "NeuralHDHair: Automatic High-fidelity Hair Modeling from a Single Image\n  Using Implicit Neural Representations",
        "authors": [
            "Keyu Wu",
            "Yifan Ye",
            "Lingchen Yang",
            "Hongbo Fu",
            "Kun Zhou",
            "Youyi Zheng"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Undoubtedly, high-fidelity 3D hair plays an indispensable role in digital\nhumans. However, existing monocular hair modeling methods are either tricky to\ndeploy in digital systems (e.g., due to their dependence on complex user\ninteractions or large databases) or can produce only a coarse geometry. In this\npaper, we introduce NeuralHDHair, a flexible, fully automatic system for\nmodeling high-fidelity hair from a single image. The key enablers of our system\nare two carefully designed neural networks: an IRHairNet (Implicit\nrepresentation for hair using neural network) for inferring high-fidelity 3D\nhair geometric features (3D orientation field and 3D occupancy field)\nhierarchically and a GrowingNet(Growing hair strands using neural network) to\nefficiently generate 3D hair strands in parallel. Specifically, we perform a\ncoarse-to-fine manner and propose a novel voxel-aligned implicit function\n(VIFu) to represent the global hair feature, which is further enhanced by the\nlocal details extracted from a hair luminance map. To improve the efficiency of\na traditional hair growth algorithm, we adopt a local neural implicit function\nto grow strands based on the estimated 3D hair geometric features. Extensive\nexperiments show that our method is capable of constructing a high-fidelity 3D\nhair model from a single image, both efficiently and effectively, and achieves\nthe-state-of-the-art performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04175v1"
    },
    {
        "title": "NeRF-Editing: Geometry Editing of Neural Radiance Fields",
        "authors": [
            "Yu-Jie Yuan",
            "Yang-Tian Sun",
            "Yu-Kun Lai",
            "Yuewen Ma",
            "Rongfei Jia",
            "Lin Gao"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Implicit neural rendering, especially Neural Radiance Field (NeRF), has shown\ngreat potential in novel view synthesis of a scene. However, current NeRF-based\nmethods cannot enable users to perform user-controlled shape deformation in the\nscene. While existing works have proposed some approaches to modify the\nradiance field according to the user's constraints, the modification is limited\nto color editing or object translation and rotation. In this paper, we propose\na method that allows users to perform controllable shape deformation on the\nimplicit representation of the scene, and synthesizes the novel view images of\nthe edited scene without re-training the network. Specifically, we establish a\ncorrespondence between the extracted explicit mesh representation and the\nimplicit neural representation of the target scene. Users can first utilize\nwell-developed mesh-based deformation methods to deform the mesh representation\nof the scene. Our method then utilizes user edits from the mesh representation\nto bend the camera rays by introducing a tetrahedra mesh as a proxy, obtaining\nthe rendering results of the edited scene. Extensive experiments demonstrate\nthat our framework can achieve ideal editing results not only on synthetic\ndata, but also on real scenes captured by users.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04978v1"
    },
    {
        "title": "The Hard Truth about Soft Skills in Game Development",
        "authors": [
            "Benjamin Kenwright"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This article explores the value and measurable effects of hard and soft\nskills in academia when teaching and developing abilities for the game\nindustry. As we discuss, each individuals engagement with the subject directly\nimpacts their performance; which is influenced by their 'soft' skill level.\nStudents that succeed in mastering soft skills earlier on typically have a\ngreater understanding and satisfaction of the subject (able to see the\nunderlying heterogeneous nature of the material). As soft and hard skill don't\njust help individuals achieve their goals (qualifications), they also change\ntheir mindset. While it is important to master both hard and soft skills, often\nwhen we talk about the quality of education (for game development); the measure\nis more towards quantitative measures and assessments (which don't always sit\nwell with soft skills). As it is easy to forget, in this digital age, that\n'people' are at the heart of video game development. Not just about 'code' and\n'technologies'. There exists a complex relationship between hard and soft\nskills and their dual importance is crucial if graduates are to succeed in the\ngame industry.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07875v1"
    },
    {
        "title": "Browser-based Hyperbolic Visualization of Graphs",
        "authors": [
            "Jacob Miller",
            "Stephen Kobourov",
            "Vahan Huroyan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Hyperbolic geometry offers a natural focus + context for data visualization\nand has been shown to underlie real-world complex networks. However, current\nhyperbolic network visualization approaches are limited to special types of\nnetworks and do not scale to large datasets. With this in mind, we designed,\nimplemented, and analyzed three methods for hyperbolic visualization of\nnetworks in the browser based on inverse projections, generalized\nforce-directed algorithms, and hyperbolic multi-dimensional scaling (H-MDS). A\ncomparison with Euclidean MDS shows that H-MDS produces embeddings with lower\ndistortion for several types of networks. All three methods can handle\nnode-link representations and are available in fully functional web-based\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.08028v1"
    },
    {
        "title": "StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D\n  Mutual Learning",
        "authors": [
            "Yi-Hua Huang",
            "Yue He",
            "Yu-Jie Yuan",
            "Yu-Kun Lai",
            "Lin Gao"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  3D scene stylization aims at generating stylized images of the scene from\narbitrary novel views following a given set of style examples, while ensuring\nconsistency when rendered from different views. Directly applying methods for\nimage or video stylization to 3D scenes cannot achieve such consistency. Thanks\nto recently proposed neural radiance fields (NeRF), we are able to represent a\n3D scene in a consistent way. Consistent 3D scene stylization can be\neffectively achieved by stylizing the corresponding NeRF. However, there is a\nsignificant domain gap between style examples which are 2D images and NeRF\nwhich is an implicit volumetric representation. To address this problem, we\npropose a novel mutual learning framework for 3D scene stylization that\ncombines a 2D image stylization network and NeRF to fuse the stylization\nability of 2D stylization network with the 3D consistency of NeRF. We first\npre-train a standard NeRF of the 3D scene to be stylized and replace its color\nprediction module with a style network to obtain a stylized NeRF. It is\nfollowed by distilling the prior knowledge of spatial consistency from NeRF to\nthe 2D stylization network through an introduced consistency loss. We also\nintroduce a mimic loss to supervise the mutual learning of the NeRF style\nmodule and fine-tune the 2D stylization decoder. In order to further make our\nmodel handle ambiguities of 2D stylization results, we introduce learnable\nlatent codes that obey the probability distributions conditioned on the style.\nThey are attached to training samples as conditional inputs to better learn the\nstyle module in our novel stylized NeRF. Experimental results demonstrate that\nour method is superior to existing approaches in both visual quality and\nlong-range consistency.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.12183v2"
    },
    {
        "title": "Learning to Use Chopsticks in Diverse Gripping Styles",
        "authors": [
            "Zeshi Yang",
            "KangKang Yin",
            "Libin Liu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Learning dexterous manipulation skills is a long-standing challenge in\ncomputer graphics and robotics, especially when the task involves complex and\ndelicate interactions between the hands, tools and objects. In this paper, we\nfocus on chopsticks-based object relocation tasks, which are common yet\ndemanding. The key to successful chopsticks skills is steady gripping of the\nsticks that also supports delicate maneuvers. We automatically discover\nphysically valid chopsticks holding poses by Bayesian Optimization (BO) and\nDeep Reinforcement Learning (DRL), which works for multiple gripping styles and\nhand morphologies without the need of example data. Given as input the\ndiscovered gripping poses and desired objects to be moved, we build\nphysics-based hand controllers to accomplish relocation tasks in two stages.\nFirst, kinematic trajectories are synthesized for the chopsticks and hand in a\nmotion planning stage. The key components of our motion planner include a\ngrasping model to select suitable chopsticks configurations for grasping the\nobject, and a trajectory optimization module to generate collision-free\nchopsticks trajectories. Then we train physics-based hand controllers through\nDRL again to track the desired kinematic trajectories produced by the motion\nplanner. We demonstrate the capabilities of our framework by relocating objects\nof various shapes and sizes, in diverse gripping styles and holding positions\nfor multiple hand morphologies. Our system achieves faster learning speed and\nbetter control robustness, when compared to vanilla systems that attempt to\nlearn chopstick-based skills without a gripping pose optimization module and/or\nwithout a kinematic motion planner.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.14313v3"
    },
    {
        "title": "VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for\n  Analysis-by-Synthesis",
        "authors": [
            "Angtian Wang",
            "Peng Wang",
            "Jian Sun",
            "Adam Kortylewski",
            "Alan Yuille"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The Gaussian reconstruction kernels have been proposed by Westover (1990) and\nstudied by the computer graphics community back in the 90s, which gives an\nalternative representation of object 3D geometry from meshes and point clouds.\nOn the other hand, current state-of-the-art (SoTA) differentiable renderers,\nLiu et al. (2019), use rasterization to collect triangles or points on each\nimage pixel and blend them based on the viewing distance. In this paper, we\npropose VoGE, which utilizes the volumetric Gaussian reconstruction kernels as\ngeometric primitives. The VoGE rendering pipeline uses ray tracing to capture\nthe nearest primitives and blends them as mixtures based on their volume\ndensity distributions along the rays. To efficiently render via VoGE, we\npropose an approximate closeform solution for the volume density aggregation\nand a coarse-to-fine rendering strategy. Finally, we provide a CUDA\nimplementation of VoGE, which enables real-time level rendering with a\ncompetitive rendering speed in comparison to PyTorch3D. Quantitative and\nqualitative experiment results show VoGE outperforms SoTA counterparts when\napplied to various vision tasks, e.g., object pose estimation, shape/texture\nfitting, and occlusion reasoning. The VoGE library and demos are available at:\nhttps://github.com/Angtian/VoGE.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.15401v3"
    },
    {
        "title": "SaccadeNet: Towards Real-time Saccade Prediction for Virtual Reality\n  Infinite Walking",
        "authors": [
            "Yashas Joshi",
            "Charalambos Poullis"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Modern Redirected Walking (RDW) techniques significantly outperform classical\nsolutions. Nevertheless, they are often limited by their heavy reliance on\neye-tracking hardware embedded within the VR headset to reveal redirection\nopportunities.\n  We propose a novel RDW technique that leverages the temporary blindness\ninduced due to saccades for redirection. However, unlike the state-of-the-art,\nour approach does not impose additional eye-tracking hardware requirements.\nInstead, SaccadeNet, a deep neural network, is trained on head rotation data to\npredict saccades in real-time during an apparent head rotation. Rigid\ntransformations are then applied to the virtual environment for redirection\nduring the onset duration of these saccades. However, SaccadeNet is only\neffective when combined with moderate cognitive workload that elicits repeated\nhead rotations.\n  We present three user studies. The relationship between head and gaze\ndirections is confirmed in the first user study, followed by the training data\ncollection in our second user study. Then, after some fine-tuning experiments,\nthe performance of our RDW technique is evaluated in a third user study.\nFinally, we present the results demonstrating the efficacy of our approach. It\nallowed users to walk up a straight virtual distance of at least 38 meters from\nwithin a $3.5 x 3.5m^2$ of the physical tracked space. Moreover, our system\nunlocks saccadic redirection on widely used consumer-grade hardware without\neye-tracking.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.15846v1"
    },
    {
        "title": "Sex and Gender in the Computer Graphics Research Literature",
        "authors": [
            "Ana Dodik",
            "Silvia Sellán",
            "Theodore Kim",
            "Amanda Phillips"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We survey the treatment of sex and gender in the Computer Graphics research\nliterature from an algorithmic fairness perspective. The established practices\non the use of gender and sex in our community are scientifically incorrect and\nconstitute a form of algorithmic bias with potential harmful effects. We\npropose ways of addressing these as technical limitations.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.00480v1"
    },
    {
        "title": "Shape, Light, and Material Decomposition from Images using Monte Carlo\n  Rendering and Denoising",
        "authors": [
            "Jon Hasselgren",
            "Nikolai Hofmann",
            "Jacob Munkberg"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Recent advances in differentiable rendering have enabled high-quality\nreconstruction of 3D scenes from multi-view images. Most methods rely on simple\nrendering algorithms: pre-filtered direct lighting or learned representations\nof irradiance. We show that a more realistic shading model, incorporating ray\ntracing and Monte Carlo integration, substantially improves decomposition into\nshape, materials & lighting. Unfortunately, Monte Carlo integration provides\nestimates with significant noise, even at large sample counts, which makes\ngradient-based inverse rendering very challenging. To address this, we\nincorporate multiple importance sampling and denoising in a novel inverse\nrendering pipeline. This substantially improves convergence and enables\ngradient-based optimization at low sample counts. We present an efficient\nmethod to jointly reconstruct geometry (explicit triangle meshes), materials,\nand lighting, which substantially improves material and light separation\ncompared to previous work. We argue that denoising can become an integral part\nof high quality inverse rendering pipelines.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.03380v2"
    },
    {
        "title": "Subjective Quality Assessment for Images Generated by Computer Graphics",
        "authors": [
            "Tao Wang",
            "Zicheng Zhang",
            "Wei Sun",
            "Xiongkuo Min",
            "Wei Lu",
            "Guangtao Zhai"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  With the development of rendering techniques, computer graphics generated\nimages (CGIs) have been widely used in practical application scenarios such as\narchitecture design, video games, simulators, movies, etc. Different from\nnatural scene images (NSIs), the distortions of CGIs are usually caused by poor\nrending settings and limited computation resources. What's more, some CGIs may\nalso suffer from compression distortions in transmission systems like cloud\ngaming and stream media. However, limited work has been put forward to tackle\nthe problem of computer graphics generated images' quality assessment (CG-IQA).\nTherefore, in this paper, we establish a large-scale subjective CG-IQA database\nto deal with the challenge of CG-IQA tasks. We collect 25,454 in-the-wild CGIs\nthrough previous databases and personal collection. After data cleaning, we\ncarefully select 1,200 CGIs to conduct the subjective experiment. Several\npopular no-reference image quality assessment (NR-IQA) methods are tested on\nour database. The experimental results show that the handcrafted-based methods\nachieve low correlation with subjective judgment and deep learning based\nmethods obtain relatively better performance, which demonstrates that the\ncurrent NR-IQA models are not suitable for CG-IQA tasks and more effective\nmodels are urgently needed.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.05008v1"
    },
    {
        "title": "Differentiable Rendering of Neural SDFs through Reparameterization",
        "authors": [
            "Sai Praveen Bangaru",
            "Michaël Gharbi",
            "Tzu-Mao Li",
            "Fujun Luan",
            "Kalyan Sunkavalli",
            "Miloš Hašan",
            "Sai Bi",
            "Zexiang Xu",
            "Gilbert Bernstein",
            "Frédo Durand"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a method to automatically compute correct gradients with respect\nto geometric scene parameters in neural SDF renderers. Recent physically-based\ndifferentiable rendering techniques for meshes have used edge-sampling to\nhandle discontinuities, particularly at object silhouettes, but SDFs do not\nhave a simple parametric form amenable to sampling. Instead, our approach\nbuilds on area-sampling techniques and develops a continuous warping function\nfor SDFs to account for these discontinuities. Our method leverages the\ndistance to surface encoded in an SDF and uses quadrature on sphere tracer\npoints to compute this warping function. We further show that this can be done\nby subsampling the points to make the method tractable for neural SDFs. Our\ndifferentiable renderer can be used to optimize neural shapes from multi-view\nimages and produces comparable 3D reconstructions to recent SDF-based inverse\nrendering methods, without the need for 2D segmentation masks to guide the\ngeometry optimization and no volumetric approximations to the geometry.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.05344v1"
    },
    {
        "title": "TileGen: Tileable, Controllable Material Generation and Capture",
        "authors": [
            "Xilong Zhou",
            "Miloš Hašan",
            "Valentin Deschaintre",
            "Paul Guerrero",
            "Kalyan Sunkavalli",
            "Nima Kalantari"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Recent methods (e.g. MaterialGAN) have used unconditional GANs to generate\nper-pixel material maps, or as a prior to reconstruct materials from input\nphotographs. These models can generate varied random material appearance, but\ndo not have any mechanism to constrain the generated material to a specific\ncategory or to control the coarse structure of the generated material, such as\nthe exact brick layout on a brick wall. Furthermore, materials reconstructed\nfrom a single input photo commonly have artifacts and are generally not\ntileable, which limits their use in practical content creation pipelines. We\npropose TileGen, a generative model for SVBRDFs that is specific to a material\ncategory, always tileable, and optionally conditional on a provided input\nstructure pattern. TileGen is a variant of StyleGAN whose architecture is\nmodified to always produce tileable (periodic) material maps. In addition to\nthe standard \"style\" latent code, TileGen can optionally take a condition\nimage, giving a user direct control over the dominant spatial (and optionally\ncolor) features of the material. For example, in brick materials, the user can\nspecify a brick layout and the brick color, or in leather materials, the\nlocations of wrinkles and folds. Our inverse rendering approach can find a\nmaterial perceptually matching a single target photograph by optimization. This\nreconstruction can also be conditional on a user-provided pattern. The\nresulting materials are tileable, can be larger than the target image, and are\neditable by varying the condition.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.05649v2"
    },
    {
        "title": "Towards Differentiable Rendering for Sidescan Sonar Imagery",
        "authors": [
            "Yiping Xie",
            "Nils Bore",
            "John Folkesson"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Recent advances in differentiable rendering, which allow calculating the\ngradients of 2D pixel values with respect to 3D object models, can be applied\nto estimation of the model parameters by gradient-based optimization with only\n2D supervision. It is easy to incorporate deep neural networks into such an\noptimization pipeline, allowing the leveraging of deep learning techniques.\nThis also largely reduces the requirement for collecting and annotating 3D\ndata, which is very difficult for applications, for example when constructing\ngeometry from 2D sensors. In this work, we propose a differentiable renderer\nfor sidescan sonar imagery. We further demonstrate its ability to solve the\ninverse problem of directly reconstructing a 3D seafloor mesh from only 2D\nsidescan sonar data.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.07821v1"
    },
    {
        "title": "Real-time motion amplification on mobile devices",
        "authors": [
            "Henning U. Voss"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  A simple motion amplification algorithm suitable for real-time applications\non mobile devices, including smartphones, is presented. It is based on motion\nenhancement by moving average differencing (MEMAD), a temporal high-pass filter\nfor video streams. MEMAD can amplify small moving objects or subtle motion in\nlarger objects. It is computationally sufficiently simple to be implemented in\nreal time on smartphones. In the specific implementation as an Android phone\napp, MEMAD is demonstrated on examples chosen such as to motivate applications\nin the engineering, biological, and medical sciences.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.08422v2"
    },
    {
        "title": "Unsupervised Kinematic Motion Detection for Part-segmented 3D Shape\n  Collections",
        "authors": [
            "Xianghao Xu",
            "Yifan Ruan",
            "Srinath Sridhar",
            "Daniel Ritchie"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  3D models of manufactured objects are important for populating virtual worlds\nand for synthetic data generation for vision and robotics. To be most useful,\nsuch objects should be articulated: their parts should move when interacted\nwith. While articulated object datasets exist, creating them is\nlabor-intensive. Learning-based prediction of part motions can help, but all\nexisting methods require annotated training data. In this paper, we present an\nunsupervised approach for discovering articulated motions in a part-segmented\n3D shape collection. Our approach is based on a concept we call category\nclosure: any valid articulation of an object's parts should keep the object in\nthe same semantic category (e.g. a chair stays a chair). We operationalize this\nconcept with an algorithm that optimizes a shape's part motion parameters such\nthat it can transform into other shapes in the collection. We evaluate our\napproach by using it to re-discover part motions from the PartNet-Mobility\ndataset. For almost all shape categories, our method's predicted motion\nparameters have low error with respect to ground truth annotations,\noutperforming two supervised motion prediction methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.08497v1"
    },
    {
        "title": "From Universal Humanoid Control to Automatic Physically Valid Character\n  Creation",
        "authors": [
            "Zhengyi Luo",
            "Ye Yuan",
            "Kris M. Kitani"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Automatically designing virtual humans and humanoids holds great potential in\naiding the character creation process in games, movies, and robots. In some\ncases, a character creator may wish to design a humanoid body customized for\ncertain motions such as karate kicks and parkour jumps. In this work, we\npropose a humanoid design framework to automatically generate physically valid\nhumanoid bodies conditioned on sequence(s) of pre-specified human motions.\nFirst, we learn a generalized humanoid controller trained on a large-scale\nhuman motion dataset that features diverse human motion and body shapes.\nSecond, we use a design-and-control framework to optimize a humanoid's physical\nattributes to find body designs that can better imitate the pre-specified human\nmotion sequence(s). Leveraging the pre-trained humanoid controller and physics\nsimulation as guidance, our method is able to discover new humanoid designs\nthat are customized to perform pre-specified human motions.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.09286v1"
    },
    {
        "title": "Dressing Avatars: Deep Photorealistic Appearance for Physically\n  Simulated Clothing",
        "authors": [
            "Donglai Xiang",
            "Timur Bagautdinov",
            "Tuur Stuyck",
            "Fabian Prada",
            "Javier Romero",
            "Weipeng Xu",
            "Shunsuke Saito",
            "Jingfan Guo",
            "Breannan Smith",
            "Takaaki Shiratori",
            "Yaser Sheikh",
            "Jessica Hodgins",
            "Chenglei Wu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Despite recent progress in developing animatable full-body avatars, realistic\nmodeling of clothing - one of the core aspects of human self-expression -\nremains an open challenge. State-of-the-art physical simulation methods can\ngenerate realistically behaving clothing geometry at interactive rates.\nModeling photorealistic appearance, however, usually requires physically-based\nrendering which is too expensive for interactive applications. On the other\nhand, data-driven deep appearance models are capable of efficiently producing\nrealistic appearance, but struggle at synthesizing geometry of highly dynamic\nclothing and handling challenging body-clothing configurations. To this end, we\nintroduce pose-driven avatars with explicit modeling of clothing that exhibit\nboth photorealistic appearance learned from real-world data and realistic\nclothing dynamics. The key idea is to introduce a neural clothing appearance\nmodel that operates on top of explicit geometry: at training time we use\nhigh-fidelity tracking, whereas at animation time we rely on physically\nsimulated geometry. Our core contribution is a physically-inspired appearance\nnetwork, capable of generating photorealistic appearance with view-dependent\nand dynamic shadowing effects even for unseen body-clothing configurations. We\nconduct a thorough evaluation of our model and demonstrate diverse animation\nresults on several subjects and different types of clothing. Unlike previous\nwork on photorealistic full-body avatars, our approach can produce much richer\ndynamics and more realistic deformations even for many examples of loose\nclothing. We also demonstrate that our formulation naturally allows clothing to\nbe used with avatars of different people while staying fully animatable, thus\nenabling, for the first time, photorealistic avatars with novel clothing.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.15470v2"
    },
    {
        "title": "Sparse Ellipsometry: Portable Acquisition of Polarimetric SVBRDF and\n  Shape with Unstructured Flash Photography",
        "authors": [
            "Inseung Hwang",
            "Daniel S. Jeon",
            "Adolfo Muñoz",
            "Diego Gutierrez",
            "Xin Tong",
            "Min H. Kim"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Ellipsometry techniques allow to measure polarization information of\nmaterials, requiring precise rotations of optical components with different\nconfigurations of lights and sensors. This results in cumbersome capture\ndevices, carefully calibrated in lab conditions, and in very long acquisition\ntimes, usually in the order of a few days per object. Recent techniques allow\nto capture polarimetric spatially-varying reflectance information, but limited\nto a single view, or to cover all view directions, but limited to spherical\nobjects made of a single homogeneous material. We present sparse ellipsometry,\na portable polarimetric acquisition method that captures both polarimetric\nSVBRDF and 3D shape simultaneously. Our handheld device consists of\noff-the-shelf, fixed optical components. Instead of days, the total acquisition\ntime varies between twenty and thirty minutes per object. We develop a complete\npolarimetric SVBRDF model that includes diffuse and specular components, as\nwell as single scattering, and devise a novel polarimetric inverse rendering\nalgorithm with data augmentation of specular reflection samples via generative\nmodeling. Our results show a strong agreement with a recent ground-truth\ndataset of captured polarimetric BRDFs of real-world objects.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.04236v2"
    },
    {
        "title": "A Learned Radiance-Field Representation for Complex Luminaires",
        "authors": [
            "Jorge Condor",
            "Adrián Jarabo"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose an efficient method for rendering complex luminaires using a\nhigh-quality octree-based representation of the luminaire emission. Complex\nluminaires are a particularly challenging problem in rendering, due to their\ncaustic light paths inside the luminaire. We reduce the geometric complexity of\nluminaires by using a simple proxy geometry and encode the visually-complex\nemitted light field by using a neural radiance field. We tackle the multiple\nchallenges of using NeRFs for representing luminaires, including their high\ndynamic range, high-frequency content and null-emission areas, by proposing a\nspecialized loss function. For rendering, we distill our luminaires' NeRF into\na Plenoctree, which we can be easily integrated into traditional rendering\nsystems. Our approach allows for speed-ups of up to 2 orders of magnitude in\nscenes containing complex luminaires introducing minimal error.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.05009v1"
    },
    {
        "title": "Efficient Interpolation-based Pathline Tracing with B-spline Curves in\n  Particle Dataset",
        "authors": [
            "Haoyu Li",
            "Tianyu Xiong",
            "Han-Wei Shen"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Particle tracing through numerical integration is a well-known approach to\ngenerating pathlines for visualization. However, for particle simulations, the\ncomputation of pathlines is expensive, since the interpolation method is\ncomplicated due to the lack of connectivity information. Previous studies\nutilize the k-d tree to reduce the time for neighborhood search. However, the\nefficiency is still limited by the number of tracing time steps. Therefore, we\npropose a novel interpolation-based particle tracing method that first\nrepresents particle data as B-spline curves and interpolates B-spline control\npoints to reduce the number of interpolation time steps. We demonstrate our\napproach achieves good tracing accuracy with much less computation time.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.07224v2"
    },
    {
        "title": "An Exact Bitwise Reversible Integrator",
        "authors": [
            "Jos Stam"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  At a fundamental level most physical equations are time reversible. In this\npaper we propose an integrator that preserves this property at the discrete\ncomputational level. Our simulations can be run forward and backwards and trace\nthe same path exactly bitwise. We achieve this by implementing theoretically\nreversible integrators using a mix of fixed and floating point arithmetic. Our\nmain application is in efficiently implementing the reverse step in the adjoint\nmethod used in optimization. Our integrator has applications in differential\nsimulations and machine learning (backpropagation).\n",
        "pdf_link": "http://arxiv.org/pdf/2207.07695v1"
    },
    {
        "title": "Interactive Volume Visualization via Multi-Resolution Hash Encoding\n  based Neural Representation",
        "authors": [
            "Qi Wu",
            "David Bauer",
            "Michael J. Doyle",
            "Kwan-Liu Ma"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Neural networks have shown great potential in compressing volume data for\nvisualization. However, due to the high cost of training and inference, such\nvolumetric neural representations have thus far only been applied to offline\ndata processing and non-interactive rendering. In this paper, we demonstrate\nthat by simultaneously leveraging modern GPU tensor cores, a native CUDA neural\nnetwork framework, and a well-designed rendering algorithm with macro-cell\nacceleration, we can interactively ray trace volumetric neural representations\n(10-60fps). Our neural representations are also high-fidelity (PSNR > 30dB) and\ncompact (10-1000x smaller). Additionally, we show that it is possible to fit\nthe entire training step inside a rendering loop and skip the pre-training\nprocess completely. To support extreme-scale volume data, we also develop an\nefficient out-of-core training strategy, which allows our volumetric neural\nrepresentation training to potentially scale up to terascale using only an\nNVIDIA RTX 3090 workstation.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.11620v3"
    },
    {
        "title": "RayPC: Interactive Ray Tracing Meets Parallel Coordinates",
        "authors": [
            "Jonathan Fritsch",
            "Markus Flatken",
            "Simon Schneegans",
            "Andreas Gerndt",
            "Ana-Catalina Plesa",
            "Christian Hüttig"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Large-scale numerical simulations of planetary interiors require dedicated\nvisualization algorithms that are able to efficiently extract a large amount of\ninformation in an interactive and user-friendly way. Here we present a software\nframework for the visualization of mantle convection data. This framework\ncombines real-time volume rendering, pathline visualization, and parallel\ncoordinates to explore the fluid dynamics in an interactive way and to identify\ncorrelations between various output variables.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.12011v1"
    },
    {
        "title": "A Repulsive Force Unit for Garment Collision Handling in Neural Networks",
        "authors": [
            "Qingyang Tan",
            "Yi Zhou",
            "Tuanfeng Wang",
            "Duygu Ceylan",
            "Xin Sun",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Despite recent success, deep learning-based methods for predicting 3D garment\ndeformation under body motion suffer from interpenetration problems between the\ngarment and the body. To address this problem, we propose a novel collision\nhandling neural network layer called Repulsive Force Unit (ReFU). Based on the\nsigned distance function (SDF) of the underlying body and the current garment\nvertex positions, ReFU predicts the per-vertex offsets that push any\ninterpenetrating vertex to a collision-free configuration while preserving the\nfine geometric details. We show that ReFU is differentiable with trainable\nparameters and can be integrated into different network backbones that predict\n3D garment deformations. Our experiments show that ReFU significantly reduces\nthe number of collisions between the body and the garment and better preserves\ngeometric details compared to prior methods based on collision loss or\npost-processing optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.13871v2"
    },
    {
        "title": "Interaction Mix and Match: Synthesizing Close Interaction using\n  Conditional Hierarchical GAN with Multi-Hot Class Embedding",
        "authors": [
            "Aman Goel",
            "Qianhui Men",
            "Edmond S. L. Ho"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Synthesizing multi-character interactions is a challenging task due to the\ncomplex and varied interactions between the characters. In particular, precise\nspatiotemporal alignment between characters is required in generating close\ninteractions such as dancing and fighting. Existing work in generating\nmulti-character interactions focuses on generating a single type of reactive\nmotion for a given sequence which results in a lack of variety of the resultant\nmotions. In this paper, we propose a novel way to create realistic human\nreactive motions which are not presented in the given dataset by mixing and\nmatching different types of close interactions. We propose a Conditional\nHierarchical Generative Adversarial Network with Multi-Hot Class Embedding to\ngenerate the Mix and Match reactive motions of the follower from a given motion\nsequence of the leader. Experiments are conducted on both noisy (depth-based)\nand high-quality (MoCap-based) interaction datasets. The quantitative and\nqualitative results show that our approach outperforms the state-of-the-art\nmethods on the given datasets. We also provide an augmented dataset with\nrealistic reactive motions to stimulate future research in this area. The code\nis available at https://github.com/Aman-Goel1/IMM\n",
        "pdf_link": "http://arxiv.org/pdf/2208.00774v2"
    },
    {
        "title": "VolTeMorph: Realtime, Controllable and Generalisable Animation of\n  Volumetric Representations",
        "authors": [
            "Stephan J. Garbin",
            "Marek Kowalski",
            "Virginia Estellers",
            "Stanislaw Szymanowicz",
            "Shideh Rezaeifar",
            "Jingjing Shen",
            "Matthew Johnson",
            "Julien Valentin"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The recent increase in popularity of volumetric representations for scene\nreconstruction and novel view synthesis has put renewed focus on animating\nvolumetric content at high visual quality and in real-time. While implicit\ndeformation methods based on learned functions can produce impressive results,\nthey are `black boxes' to artists and content creators, they require large\namounts of training data to generalise meaningfully, and they do not produce\nrealistic extrapolations outside the training data. In this work we solve these\nissues by introducing a volume deformation method which is real-time, easy to\nedit with off-the-shelf software and can extrapolate convincingly. To\ndemonstrate the versatility of our method, we apply it in two scenarios:\nphysics-based object deformation and telepresence where avatars are controlled\nusing blendshapes. We also perform thorough experiments showing that our method\ncompares favourably to both volumetric approaches combined with implicit\ndeformation and methods based on mesh deformation.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.00949v1"
    },
    {
        "title": "A Brief Note on Building Augmented Reality Models for Scientific\n  Visualization",
        "authors": [
            "Mrudang Mathur",
            "Josef M. Brozovich",
            "Manuel K. Rausch"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Augmented reality (AR) has revolutionized the video game industry by\nproviding interactive, three-dimensional visualization. Interestingly, AR\ntechnology has only been sparsely used in scientific visualization. This is, at\nleast in part, due to the significant technical challenges previously\nassociated with creating and accessing such models. To ease access to AR for\nthe scientific community, we introduce a novel visualization pipeline with\nwhich they can create and render AR models. We demonstrate our pipeline by\nmeans of finite element results, but note that our pipeline is generally\napplicable to data that may be represented through meshed surfaces.\nSpecifically, we use two open-source software packages, ParaView and Blender.\nThe models are then rendered through the <model-viewer> platform, which we\naccess through Android and iOS smartphones. To demonstrate our pipeline, we\nbuild AR models from static and time-series results of finite element\nsimulations discretized with continuum, shell, and beam elements. Moreover, we\nopenly provide python scripts to automate this process. Thus, others may use\nour framework to create and render AR models for their own research and\nteaching activities.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.02022v1"
    },
    {
        "title": "Solving Inverse PDE Problems using Grid-Free Monte Carlo Estimators",
        "authors": [
            "Ekrem Fatih Yılmazer",
            "Delio Vicini",
            "Wenzel Jakob"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Modeling physical phenomena like heat transport and diffusion is crucially\ndependent on the numerical solution of partial differential equations (PDEs). A\nPDE solver finds the solution given coefficients and a boundary condition,\nwhereas an inverse PDE solver goes the opposite way and reconstructs these\ninputs from an existing solution. In this article, we investigate techniques\nfor solving inverse PDE problems using a gradient-based methodology.\nConventional PDE solvers based on the finite element method require a domain\nmeshing step that can be fragile and costly. Grid-free Monte Carlo methods\ninstead stochastically sample paths using variations of the walk on spheres\nalgorithm to construct an unbiased estimator of the solution. The uncanny\nsimilarity of these methods to physically-based rendering algorithms has been\nobserved by several recent works. In the area of rendering, recent progress has\nled to the development of efficient unbiased derivative estimators. They solve\nan adjoint form of the problem and exploit arithmetic invertibility to compute\ngradients using a constant amount of memory and linear time complexity. Could\nthese two lines of work be combined to compute cheap parametric derivatives of\na grid-free PDE solver? We investigate this question and present preliminary\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.02114v1"
    },
    {
        "title": "FauxThrow: Exploring the Effects of Incorrect Point of Release in\n  Throwing Motions",
        "authors": [
            "Goksu Yamac",
            "Carol O'Sullivan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Our aim is to develop a better understanding of how the Point of Release\n(PoR) of a ball affects the perception of animated throwing motions. We present\nthe results of a perceptual study where participants viewed animations of a\nvirtual human throwing a ball, in which the point of release was modified to be\nearly or late. We found that errors in overarm throws with a late PoR are\ndetected more easily than an early PoR, while the opposite is true for underarm\nthrows. The viewpoint and the distance the ball travels also have an effect on\nperceived realism. The results of this research can help improve the\nplausibility of throwing animations in interactive applications such as games\nor VR.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.02166v2"
    },
    {
        "title": "Jacobi Set Driven Search for Flexible Fiber Surface Extraction",
        "authors": [
            "Mohit Sharma",
            "Vijay Natarajan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Isosurfaces are an important tool for analysis and visualization of\nunivariate scalar fields. Earlier works have demonstrated the presence of\ninteresting isosurfaces at isovalues close to critical values. This motivated\nthe development of efficient methods for computing individual components of\nisosurfaces restricted to a region of interest. Generalization of isosurfaces\nto fiber surfaces and critical points to Jacobi sets has resulted in new\napproaches for analyzing bivariate scalar fields. Unlike isosurfaces, there\nexists no output sensitive method for computing fiber surfaces. Existing\nmethods traverse through all the tetrahedra in the domain. In this paper, we\npropose the use of the Jacobi set to identify fiber surface components of\ninterest and present an output sensitive approach for its computation. The\nJacobi edges are used to initiate the search towards seed tetrahedra that\ncontain the fiber surface, thereby reducing the search space. This approach\nalso leads to effective analysis of the bivariate field by supporting the\nidentification of relevant fiber surfaces near Jacobi edges.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.06723v1"
    },
    {
        "title": "A Novel Tree Visualization to Guide Interactive Exploration of\n  Multi-dimensional Topological Hierarchies",
        "authors": [
            "Yarden Livnat",
            "Dan Maljovec",
            "Attila Gyulassy",
            "Dr Baptiste Mouginot",
            "Valerio Pascucci"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Understanding the response of an output variable to multi-dimensional inputs\nlies at the heart of many data exploration endeavours. Topology-based methods,\nin particular Morse theory and persistent homology, provide a useful framework\nfor studying this relationship, as phenomena of interest often appear naturally\nas fundamental features. The Morse-Smale complex captures a wide range of\nfeatures by partitioning the domain of a scalar function into piecewise\nmonotonic regions, while persistent homology provides a means to study these\nfeatures at different scales of simplification. Previous works demonstrated how\nto compute such a representation and its usefulness to gain insight into\nmulti-dimensional data. However, exploration of the multi-scale nature of the\ndata was limited to selecting a single simplification threshold from a plot of\nregion count. In this paper, we present a novel tree visualization that\nprovides a concise overview of the entire hierarchy of topological features.\nThe structure of the tree provides initial insights in terms of the\ndistribution, size, and stability of all partitions. We use regression analysis\nto fit linear models in each partition, and develop local and relative measures\nto further assess uniqueness and the importance of each partition, especially\nwith respect parents/children in the feature hierarchy. The expressiveness of\nthe tree visualization becomes apparent when we encode such measures using\ncolors, and the layout allows an unprecedented level of control over feature\nselection during exploration. For instance, selecting features from multiple\nscales of the hierarchy enables a more nuanced exploration. Finally, we\ndemonstrate our approach using examples from several scientific domains.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.06952v1"
    },
    {
        "title": "Enhancing Dynamic Mode Decomposition Workflow with In-Situ Visualization\n  and Data Compression",
        "authors": [
            "Gabriel F. Barros",
            "Malú Grave",
            "José J. Camata",
            "Alvaro L. G. A. Coutinho"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Modern computational science and engineering applications are being improved\nby the advances in scientific machine learning. Data-driven methods such as\nDynamic Mode Decomposition (DMD) can extract coherent structures from\nspatio-temporal data generated from dynamical systems and infer different\nscenarios for said systems. The spatio-temporal data comes as snapshots\ncontaining spatial information for each time instant. In modern engineering\napplications, the generation of high-dimensional snapshots can be time and/or\nresource-demanding. In the present study, we consider two strategies for\nenhancing DMD workflow in large numerical simulations: (i) snapshots\ncompression to relieve disk pressure; (ii) the use of in situ visualization\nimages to reconstruct the dynamics (or part of) in runtime. We evaluate our\napproaches with two 3D fluid dynamics simulations and consider DMD to\nreconstruct the solutions. Results reveal that snapshot compression\nconsiderably reduces the required disk space. We have observed that lossy\ncompression reduces storage by almost $50\\%$ with low relative errors in the\nsignal reconstructions and other quantities of interest. We also extend our\nanalysis to data generated on-the-fly, using in-situ visualization tools to\ngenerate image files of our state vectors during runtime. On large simulations,\nthe generation of snapshots may be slow enough to use batch algorithms for\ninference. Streaming DMD takes advantage of the incremental SVD algorithm and\nupdates the modes with the arrival of each new snapshot. We use streaming DMD\nto reconstruct the dynamics from in-situ generated images. We show that this\nprocess is efficient, and the reconstructed dynamics are accurate.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.07767v1"
    },
    {
        "title": "SMPL-IK: Learned Morphology-Aware Inverse Kinematics for AI Driven\n  Artistic Workflows",
        "authors": [
            "Vikram Voleti",
            "Boris N. Oreshkin",
            "Florent Bocquelet",
            "Félix G. Harvey",
            "Louis-Simon Ménard",
            "Christopher Pal"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Inverse Kinematics (IK) systems are often rigid with respect to their input\ncharacter, thus requiring user intervention to be adapted to new skeletons. In\nthis paper we aim at creating a flexible, learned IK solver applicable to a\nwide variety of human morphologies. We extend a state-of-the-art machine\nlearning IK solver to operate on the well known Skinned Multi-Person Linear\nmodel (SMPL). We call our model SMPL-IK, and show that when integrated into\nreal-time 3D software, this extended system opens up opportunities for defining\nnovel AI-assisted animation workflows. For example, pose authoring can be made\nmore flexible with SMPL-IK by allowing users to modify gender and body shape\nwhile posing a character. Additionally, when chained with existing pose\nestimation algorithms, SMPL-IK accelerates posing by allowing users to\nbootstrap 3D scenes from 2D images while allowing for further editing. Finally,\nwe propose a novel SMPL Shape Inversion mechanism (SMPL-SI) to map arbitrary\nhumanoid characters to the SMPL space, allowing artists to leverage SMPL-IK on\ncustom characters. In addition to qualitative demos showing proposed tools, we\npresent quantitative SMPL-IK baselines on the H36M and AMASS datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.08274v1"
    },
    {
        "title": "DualMotion: Global-to-Local Casual Motion Design for Character\n  Animations",
        "authors": [
            "Yichen Peng",
            "Chunqi Zhao",
            "Haoran Xie",
            "Tsukasa Fukusato",
            "Kazunori Miyata",
            "Takeo Igarashi"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Animating 3D characters using motion capture data requires basic expertise\nand manual labor. To support the creativity of animation design and make it\neasier for common users, we present a sketch-based interface DualMotion, with\nrough sketches as input for designing daily-life animations of characters, such\nas walking and jumping.Our approach enables to combine global motions of lower\nlimbs and the local motion of the upper limbs in a database by utilizing a\ntwo-stage design strategy. Users are allowed to design a motion by starting\nwith drawing a rough trajectory of a body/lower limb movement in the global\ndesign stage. The upper limb motions are then designed by drawing several more\nrelative motion trajectories in the local design stage. We conduct a user study\nand verify the effectiveness and convenience of the proposed system in creative\nactivities.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.08636v1"
    },
    {
        "title": "The Esports Frontier: Rendering for Competitive Games",
        "authors": [
            "Josef Spjut",
            "Arjun Madhusudan",
            "Benjamin Watson",
            "Ben Boudaoud",
            "Joohwan Kim"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Real-time graphics is commonly thought of as anything exceeding about 30 fps,\nwhere the interactivity of the application becomes fluid enough for high rates\nof interaction. Inspired by esports and competitive gaming, where players\nregularly exceed the threshold for real-time by 10x (esports displays commonly\nreach 360 Hz or beyond), this talk begins the exploration of how rendering has\nthe opportunity to evolve beyond the current state of focus on either image\nquality or frame rate. Esports gamers regularly decline nearly all options for\nincreased image quality in exchange for maximum frame rates. However, there\nremains a distinct opportunity to move beyond the focus on video as a sequence\nof images and instead rethink the pipeline for more continuous updates.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.11774v2"
    },
    {
        "title": "Shape-Guided Mixed Metro Map Layout",
        "authors": [
            "Tobias Batik",
            "Soeren Terziadis",
            "Yu-Shuen Wang",
            "Martin Nöllenburg",
            "Hsiang-Yun Wu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Metro or transit maps, are schematic representations of transit networks to\nfacilitate effective route-finding. These maps are often advertised on a web\npage or pamphlet highlighting routes from source to destination stations. To\nvisually support such route-finding, designers often distort the layout by\nembedding symbolic shapes (e.g., circular routes) in order to guide readers'\nattention (e.g., Moscow map and Japan railway map). However, manually producing\nsuch maps is labor-intensive and the effect of shapes remains unclear. In this\npaper, we propose an approach to generalize such mixed metro maps that take\nuser-defined shapes as an input. In this mixed design, lines that are used to\napproximate the shapes are arranged symbolically, while the remaining lines\nfollow classical layout convention. A three-step algorithm, including (1)\ndetecting and selecting routes for shape approximation, (2) shape and layout\ndeformation, and (3) aligning lines on a grid, is integrated to guarantee good\nvisual quality. Our contribution lies in the definition of the mixed metro map\nproblem and the formulation of design criteria so that the problem can be\nresolved systematically using the optimization paradigm. Finally, we evaluate\nthe performance of our approach and perform a user study to test if the\nembedded shapes are recognizable or reduce the map quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.14261v1"
    },
    {
        "title": "SketchBetween: Video-to-Video Synthesis for Sprite Animation via\n  Sketches",
        "authors": [
            "Dagmar Lukka Loftsdóttir",
            "Matthew Guzdial"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  2D animation is a common factor in game development, used for characters,\neffects and background art. It involves work that takes both skill and time,\nbut parts of which are repetitive and tedious. Automated animation approaches\nexist, but are designed without animators in mind. The focus is heavily on\nreal-life video, which follows strict laws of how objects move, and does not\naccount for the stylistic movement often present in 2D animation. We propose a\nproblem formulation that more closely adheres to the standard workflow of\nanimation. We also demonstrate a model, SketchBetween, which learns to map\nbetween keyframes and sketched in-betweens to rendered sprite animations. We\ndemonstrate that our problem formulation provides the required information for\nthe task and that our model outperforms an existing method.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.00185v1"
    },
    {
        "title": "Cluster-based multidimensional scaling embedding tool for data\n  visualization",
        "authors": [
            "Patricia Hernández-León",
            "Miguel A. Caro"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a new technique for visualizing high-dimensional data called\ncluster MDS (cl-MDS), which addresses a common difficulty of dimensionality\nreduction methods: preserving both local and global structures of the original\nsample in a single 2-dimensional visualization. Its algorithm combines the\nwell-known multidimensional scaling (MDS) tool with the $k$-medoids data\nclustering technique, and enables hierarchical embedding, sparsification and\nestimation of 2-dimensional coordinates for additional points. While cl-MDS is\na generally applicable tool, we also include specific recipes for atomic\nstructure applications. We apply this method to non-linear data of increasing\ncomplexity where different layers of locality are relevant, showing a clear\nimprovement in their retrieval and visualization quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.06614v2"
    },
    {
        "title": "Not As Easy As You Think -- Experiences and Lessons Learnt from Trying\n  to Create a Bottom-Up Visualization Image Typology",
        "authors": [
            "Jian Chen",
            "Petra Isenberg",
            "Robert S. Laramee",
            "Tobias Isenberg",
            "Michael Sedlmair",
            "Torsten Moeller",
            "Han-Wei Shen"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present and discuss the results of a two-year qualitative analysis of\nimages published in IEEE Visualization (VIS) papers. Specifically, we derive a\ntypology of 13 visualization image types, coded to distinguish visualizations\nand several image characteristics. The categorization process required much\nmore time and was more difficult than we initially thought. The resulting\ntypology and image analysis may serve a number of purposes: to study the\nevolution of the community and its research output over time, to facilitate the\ncategorization of visualization images for the purpose of teaching, to identify\nvisual designs for evaluation purposes, or to enable progress towards\nstandardization in visualization. In addition to the typology and image\ncharacterization, we provide a dataset of 6,833 tagged images and an online\ntool that can be used to explore and analyze the large set of tagged images. We\nthus facilitate a discussion of the diverse visualizations used and how they\nare published and communicated in our community.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.07533v2"
    },
    {
        "title": "Human Performance Modeling and Rendering via Neural Animated Mesh",
        "authors": [
            "Fuqiang Zhao",
            "Yuheng Jiang",
            "Kaixin Yao",
            "Jiakai Zhang",
            "Liao Wang",
            "Haizhao Dai",
            "Yuhui Zhong",
            "Yingliang Zhang",
            "Minye Wu",
            "Lan Xu",
            "Jingyi Yu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We have recently seen tremendous progress in the neural advances for\nphoto-real human modeling and rendering. However, it's still challenging to\nintegrate them into an existing mesh-based pipeline for downstream\napplications. In this paper, we present a comprehensive neural approach for\nhigh-quality reconstruction, compression, and rendering of human performances\nfrom dense multi-view videos. Our core intuition is to bridge the traditional\nanimated mesh workflow with a new class of highly efficient neural techniques.\nWe first introduce a neural surface reconstructor for high-quality surface\ngeneration in minutes. It marries the implicit volumetric rendering of the\ntruncated signed distance field (TSDF) with multi-resolution hash encoding. We\nfurther propose a hybrid neural tracker to generate animated meshes, which\ncombines explicit non-rigid tracking with implicit dynamic deformation in a\nself-supervised framework. The former provides the coarse warping back into the\ncanonical space, while the latter implicit one further predicts the\ndisplacements using the 4D hash encoding as in our reconstructor. Then, we\ndiscuss the rendering schemes using the obtained animated meshes, ranging from\ndynamic texturing to lumigraph rendering under various bandwidth settings. To\nstrike an intricate balance between quality and bandwidth, we propose a\nhierarchical solution by first rendering 6 virtual views covering the performer\nand then conducting occlusion-aware neural texture blending. We demonstrate the\nefficacy of our approach in a variety of mesh-based applications and\nphoto-realistic free-view experiences on various platforms, i.e., inserting\nvirtual human performances into real environments through mobile AR or\nimmersively watching talent shows with VR headsets.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.08468v1"
    },
    {
        "title": "FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks",
        "authors": [
            "David Bauer",
            "Qi Wu",
            "Kwan-Liu Ma"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Volume data is found in many important scientific and engineering\napplications. Rendering this data for visualization at high quality and\ninteractive rates for demanding applications such as virtual reality is still\nnot easily achievable even using professional-grade hardware. We introduce\nFoVolNet -- a method to significantly increase the performance of volume data\nvisualization. We develop a cost-effective foveated rendering pipeline that\nsparsely samples a volume around a focal point and reconstructs the full-frame\nusing a deep neural network. Foveated rendering is a technique that prioritizes\nrendering computations around the user's focal point. This approach leverages\nproperties of the human visual system, thereby saving computational resources\nwhen rendering data in the periphery of the user's field of vision. Our\nreconstruction network combines direct and kernel prediction methods to produce\nfast, stable, and perceptually convincing output. With a slim design and the\nuse of quantization, our method outperforms state-of-the-art neural\nreconstruction techniques in both end-to-end frame times and visual quality. We\nconduct extensive evaluations of the system's rendering performance, inference\nspeed, and perceptual properties, and we provide comparisons to competing\nneural image reconstruction techniques. Our test results show that FoVolNet\nconsistently achieves significant time saving over conventional rendering while\npreserving perceptual quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.09965v1"
    },
    {
        "title": "Learning Reconstructability for Drone Aerial Path Planning",
        "authors": [
            "Yilin Liu",
            "Liqiang Lin",
            "Yue Hu",
            "Ke Xie",
            "Chi-Wing Fu",
            "Hao Zhang",
            "Hui Huang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce the first learning-based reconstructability predictor to improve\nview and path planning for large-scale 3D urban scene acquisition using\nunmanned drones. In contrast to previous heuristic approaches, our method\nlearns a model that explicitly predicts how well a 3D urban scene will be\nreconstructed from a set of viewpoints. To make such a model trainable and\nsimultaneously applicable to drone path planning, we simulate the proxy-based\n3D scene reconstruction during training to set up the prediction. Specifically,\nthe neural network we design is trained to predict the scene reconstructability\nas a function of the proxy geometry, a set of viewpoints, and optionally a\nseries of scene images acquired in flight. To reconstruct a new urban scene, we\nfirst build the 3D scene proxy, then rely on the predicted reconstruction\nquality and uncertainty measures by our network, based off of the proxy\ngeometry, to guide the drone path planning. We demonstrate that our data-driven\nreconstructability predictions are more closely correlated to the true\nreconstruction quality than prior heuristic measures. Further, our learned\npredictor can be easily integrated into existing path planners to yield\nimprovements. Finally, we devise a new iterative view planning framework, based\non the learned reconstructability, and show superior performance of the new\nplanner when reconstructing both synthetic and real scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.10174v1"
    },
    {
        "title": "A Data-Centric Methodology and Task Typology for Time-Stamped Event\n  Sequences",
        "authors": [
            "Yasara Peiris",
            "Clara-Maria Barth",
            "Elaine M. Huang",
            "Jürgen Bernard"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Task abstractions and taxonomic structures for tasks are useful for designers\nof interactive data analysis approaches, serving as design targets and\nevaluation criteria alike. For individual data types, dataset-specific\ntaxonomic structures capture unique data characteristics, while being\ngeneralizable across application domains. The creation of dataset-centric but\ndomain-agnostic taxonomic structures is difficult, especially if best practices\nfor a focused data type are still missing, observing experts is not feasible,\nand means for reflection and generalization are scarce. We discovered this need\nfor methodological support when working with time-stamped event sequences, a\ndatatype that has not yet been fully systematically studied in visualization\nresearch. To address this shortcoming, we present a methodology that enables\nresearchers to abstract tasks and build dataset-centric taxonomic structures in\nfive phases (data collection, coding, task categorization, task synthesis, and\naction-target(criterion) crosscut). We validate the methodology by applying it\nto time-stamped event sequences and present a task typology that uses triples\nas a novel language of description for tasks: (1) action, (2) data target, and\n(3) data criterion. We further evaluate the descriptive power of the typology\nwith a real-world case on cybersecurity.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.10181v1"
    },
    {
        "title": "Implicit Conversion of Manifold B-Rep Solids by Neural Halfspace\n  Representation",
        "authors": [
            "Hao-Xiang Guo",
            "Yang Liu",
            "Hao Pan",
            "Baining Guo"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a novel implicit representation -- neural halfspace representation\n(NH-Rep), to convert manifold B-Rep solids to implicit representations. NH-Rep\nis a Boolean tree built on a set of implicit functions represented by the\nneural network, and the composite Boolean function is capable of representing\nsolid geometry while preserving sharp features. We propose an efficient\nalgorithm to extract the Boolean tree from a manifold B-Rep solid and devise a\nneural network-based optimization approach to compute the implicit functions.\nWe demonstrate the high quality offered by our conversion algorithm on ten\nthousand manifold B-Rep CAD models that contain various curved patches\nincluding NURBS, and the superiority of our learning approach over other\nrepresentative implicit conversion algorithms in terms of surface\nreconstruction, sharp feature preservation, signed distance field\napproximation, and robustness to various surface geometry, as well as a set of\napplications supported by NH-Rep.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.10191v1"
    },
    {
        "title": "Spatio-temporal Keyframe Control of Traffic Simulation using\n  Coarse-to-Fine Optimization",
        "authors": [
            "Yi Han",
            "He Wang",
            "Xiaogang Jin"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a novel traffic trajectory editing method which uses\nspatio-temporal keyframes to control vehicles during the simulation to generate\ndesired traffic trajectories. By taking self-motivation, path following and\ncollision avoidance into account, the proposed force-based traffic simulation\nframework updates vehicle's motions in both the Frenet coordinates and the\nCartesian coordinates. With the way-points from users, lane-level navigation\ncan be generated by reference path planning. With a given keyframe, the\ncoarse-to-fine optimization is proposed to efficiently generate the plausible\ntrajectory which can satisfy the spatio-temporal constraints. At first, a\ndirected state-time graph constructed along the reference path is used to\nsearch for a coarse-grained trajectory by mapping the keyframe as the goal.\nThen, using the information extracted from the coarse trajectory as\ninitialization, adjoint-based optimization is applied to generate a finer\ntrajectory with smooth motions based on our force-based simulation. We validate\nour method with extensive experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.12511v1"
    },
    {
        "title": "Realistic Hair Synthesis with Generative Adversarial Networks",
        "authors": [
            "Muhammed Pektas",
            "Aybars Ugur"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Recent successes in generative modeling have accelerated studies on this\nsubject and attracted the attention of researchers. One of the most important\nmethods used to achieve this success is Generative Adversarial Networks (GANs).\nIt has many application areas such as; virtual reality (VR), augmented reality\n(AR), super resolution, image enhancement. Despite the recent advances in hair\nsynthesis and style transfer using deep learning and generative modelling, due\nto the complex nature of hair still contains unsolved challenges. The methods\nproposed in the literature to solve this problem generally focus on making\nhigh-quality hair edits on images. In this thesis, a generative adversarial\nnetwork method is proposed to solve the hair synthesis problem. While\ndeveloping this method, it is aimed to achieve real-time hair synthesis while\nachieving visual outputs that compete with the best methods in the literature.\nThe proposed method was trained with the FFHQ dataset and then its results in\nhair style transfer and hair reconstruction tasks were evaluated. The results\nobtained in these tasks and the operating time of the method were compared with\nMichiGAN, one of the best methods in the literature. The comparison was made at\na resolution of 128x128. As a result of the comparison, it has been shown that\nthe proposed method achieves competitive results with MichiGAN in terms of\nrealistic hair synthesis, and performs better in terms of operating time.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.12875v1"
    },
    {
        "title": "3D Neural Sculpting (3DNS): Editing Neural Signed Distance Functions",
        "authors": [
            "Petros Tzathas",
            "Petros Maragos",
            "Anastasios Roussos"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In recent years, implicit surface representations through neural networks\nthat encode the signed distance have gained popularity and have achieved\nstate-of-the-art results in various tasks (e.g. shape representation, shape\nreconstruction, and learning shape priors). However, in contrast to\nconventional shape representations such as polygon meshes, the implicit\nrepresentations cannot be easily edited and existing works that attempt to\naddress this problem are extremely limited. In this work, we propose the first\nmethod for efficient interactive editing of signed distance functions expressed\nthrough neural networks, allowing free-form editing. Inspired by 3D sculpting\nsoftware for meshes, we use a brush-based framework that is intuitive and can\nin the future be used by sculptors and digital artists. In order to localize\nthe desired surface deformations, we regulate the network by using a copy of it\nto sample the previously expressed surface. We introduce a novel framework for\nsimulating sculpting-style surface edits, in conjunction with interactive\nsurface sampling and efficient adaptation of network weights. We qualitatively\nand quantitatively evaluate our method in various different 3D objects and\nunder many different edits. The reported results clearly show that our method\nyields high accuracy, in terms of achieving the desired edits, while at the\nsame time preserving the geometry outside the interaction areas.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.13971v2"
    },
    {
        "title": "Point normal orientation and surface reconstruction by incorporating\n  isovalue constraints to Poisson equation",
        "authors": [
            "Dong Xiao",
            "Zuoqiang Shi",
            "Siyu Li",
            "Bailin Deng",
            "Bin Wang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Oriented normals are common pre-requisites for many geometric algorithms\nbased on point clouds, such as Poisson surface reconstruction. However, it is\nnot trivial to obtain a consistent orientation. In this work, we bridge\norientation and reconstruction in the implicit space and propose a novel\napproach to orient point cloud normals by incorporating isovalue constraints to\nthe Poisson equation. In implicit surface reconstruction, the reconstructed\nshape is represented as an isosurface of an implicit function defined in the\nambient space. Therefore, when such a surface is reconstructed from a set of\nsample points, the implicit function values at the points should be close to\nthe isovalue corresponding to the surface. Based on this observation and the\nPoisson equation, we propose an optimization formulation that combines isovalue\nconstraints with local consistency requirements for normals. We optimize\nnormals and implicit functions simultaneously and solve for a globally\nconsistent orientation. Thanks to the sparsity of the linear system, our method\ncan work on an average laptop with reasonable computational time. Experiments\nshow that our method can achieve high performance in non-uniform and noisy data\nand manage varying sampling densities, artifacts, multiple connected\ncomponents, and nested surfaces. The source code is available at\n\\url{https://github.com/Submanifold/IsoConstraints}.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.15619v3"
    },
    {
        "title": "Water Simulation and Rendering from a Still Photograph",
        "authors": [
            "Ryusuke Sugimoto",
            "Mingming He",
            "Jing Liao",
            "Pedro V. Sander"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose an approach to simulate and render realistic water animation from\na single still input photograph. We first segment the water surface, estimate\nrendering parameters, and compute water reflection textures with a combination\nof neural networks and traditional optimization techniques. Then we propose an\nimage-based screen space local reflection model to render the water surface\noverlaid on the input image and generate real-time water animation. Our\napproach creates realistic results with no user intervention for a wide variety\nof natural scenes containing large bodies of water with different lighting and\nwater surface conditions. Since our method provides a 3D representation of the\nwater surface, it naturally enables direct editing of water parameters and also\nsupports interactive applications like adding synthetic objects to the scene.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.02553v1"
    },
    {
        "title": "Learning to Learn and Sample BRDFs",
        "authors": [
            "Chen Liu",
            "Michael Fischer",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose a method to accelerate the joint process of physically acquiring\nand learning neural Bi-directional Reflectance Distribution Function (BRDF)\nmodels. While BRDF learning alone can be accelerated by meta-learning,\nacquisition remains slow as it relies on a mechanical process. We show that\nmeta-learning can be extended to optimize the physical sampling pattern, too.\nAfter our method has been meta-trained for a set of fully-sampled BRDFs, it is\nable to quickly train on new BRDFs with up to five orders of magnitude fewer\nphysical acquisition samples at similar quality. Our approach also extends to\nother linear and non-linear BRDF models, which we show in an extensive\nevaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.03510v2"
    },
    {
        "title": "Crack Modeling via Minimum-Weight Surfaces in 3d Voronoi Diagrams",
        "authors": [
            "Christian Jung",
            "Claudia Redenbach"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Shortest paths play an important role in mathematical modeling and image\nprocessing. Usually, shortest path problems are formulated on planar graphs\nthat consist of vertices and weighted arcs. In this context, one is interested\nin finding a path of minimum weight from a start vertex to an end vertex. The\nconcept of minimum-weight surfaces extends shortest paths to 3d. The\nminimum-weight surface problem is formulated on a cellular complex with\nweighted facets. A cycle on the arcs of the complex serves as input and one is\ninterested in finding a surface of minimum weight bounded by that cycle. In\npractice, minimum-weight surfaces can be used to segment 3d images. Vice versa,\nit is possible to use them as a modeling tool for geometric structures such as\ncracks. In this work, we present an approach for using minimum-weight surfaces\nin bounded Voronoi diagrams to generate synthetic 3d images of cracks.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.05093v1"
    },
    {
        "title": "ControlVAE: Model-Based Learning of Generative Controllers for\n  Physics-Based Characters",
        "authors": [
            "Heyuan Yao",
            "Zhenhua Song",
            "Baoquan Chen",
            "Libin Liu"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this paper, we introduce ControlVAE, a novel model-based framework for\nlearning generative motion control policies based on variational autoencoders\n(VAE). Our framework can learn a rich and flexible latent representation of\nskills and a skill-conditioned generative control policy from a diverse set of\nunorganized motion sequences, which enables the generation of realistic human\nbehaviors by sampling in the latent space and allows high-level control\npolicies to reuse the learned skills to accomplish a variety of downstream\ntasks. In the training of ControlVAE, we employ a learnable world model to\nrealize direct supervision of the latent space and the control policy. This\nworld model effectively captures the unknown dynamics of the simulation system,\nenabling efficient model-based learning of high-level downstream tasks. We also\nlearn a state-conditional prior distribution in the VAE-based generative\ncontrol policy, which generates a skill embedding that outperforms the\nnon-conditional priors in downstream tasks. We demonstrate the effectiveness of\nControlVAE using a diverse set of tasks, which allows realistic and interactive\ncontrol of the simulated characters.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.06063v1"
    },
    {
        "title": "Reconstructing Personalized Semantic Facial NeRF Models From Monocular\n  Video",
        "authors": [
            "Xuan Gao",
            "Chenglai Zhong",
            "Jun Xiang",
            "Yang Hong",
            "Yudong Guo",
            "Juyong Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a novel semantic model for human head defined with neural radiance\nfield. The 3D-consistent head model consist of a set of disentangled and\ninterpretable bases, and can be driven by low-dimensional expression\ncoefficients. Thanks to the powerful representation ability of neural radiance\nfield, the constructed model can represent complex facial attributes including\nhair, wearings, which can not be represented by traditional mesh blendshape. To\nconstruct the personalized semantic facial model, we propose to define the\nbases as several multi-level voxel fields. With a short monocular RGB video as\ninput, our method can construct the subject's semantic facial NeRF model with\nonly ten to twenty minutes, and can render a photo-realistic human head image\nin tens of miliseconds with a given expression coefficient and view direction.\nWith this novel representation, we apply it to many tasks like facial\nretargeting and expression editing. Experimental results demonstrate its strong\nrepresentation ability and training/inference speed. Demo videos and released\ncode are provided in our project page:\nhttps://ustc3dv.github.io/NeRFBlendShape/\n",
        "pdf_link": "http://arxiv.org/pdf/2210.06108v1"
    },
    {
        "title": "Morig: Motion-aware rigging of character meshes from point clouds",
        "authors": [
            "Zhan Xu",
            "Yang Zhou",
            "Li Yi",
            "Evangelos Kalogerakis"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present MoRig, a method that automatically rigs character meshes driven by\nsingle-view point cloud streams capturing the motion of performing characters.\nOur method is also able to animate the 3D meshes according to the captured\npoint cloud motion. MoRig's neural network encodes motion cues from the point\nclouds into features that are informative about the articulated parts of the\nperforming character. These motion-aware features guide the inference of an\nappropriate skeletal rig for the input mesh, which is then animated based on\nthe point cloud motion. Our method can rig and animate diverse characters,\nincluding humanoids, quadrupeds, and toys with varying articulation. It\naccounts for occluded regions in the point clouds and mismatches in the part\nproportions between the input mesh and captured character. Compared to other\nrigging approaches that ignore motion cues, MoRig produces more accurate rigs,\nwell-suited for re-targeting motion from captured characters.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.09463v1"
    },
    {
        "title": "Visualizing Squircular Implicit Surfaces",
        "authors": [
            "Chamberlain Fong"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The squircle is an intermediate shape between the square and the circle. In\nthis paper, we examine and discuss equations for different types of squircles.\nWe then build upon these 2D shapes to come-up with various 3D surfaces based on\nsquircles.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.15232v2"
    },
    {
        "title": "Intersection of triangles in space based on cutting off segment",
        "authors": [
            "Irina Bolodurina",
            "Georgii Nigmatulin",
            "Denis Parfenov"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The article proposes a new method for finding the triangle-triangle\nintersection in 3D space, based on the use of computer graphics algorithms --\ncutting off segments on the plane when moving and rotating the beginning of the\ncoordinate axes of space. This method is obtained by synthesis of two methods\nof cutting off segments on the plane -- Cohen-Sutherland algorithm and\nFC-algorithm. In the proposed method, the problem of triangle-triangle\nintersection in 3D space is reduced to a simpler and less resource-intensive\ncut-off problem on the plane. The main feature of the method is the developed\nscheme of coding the points of the cut-off in relation to the triangle segment\nplane. This scheme allows you to get rid of a large number of costly\ncalculations. In the article the cases of intersection of triangles at\nparallelism, intersection and coincidence of planes of triangles are\nconsidered. The proposed method can be used in solving the problem of\ntetrahedron intersection, using the finite element method, as well as in image\nprocessing.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.15472v1"
    },
    {
        "title": "gCoRF: Generative Compositional Radiance Fields",
        "authors": [
            "Mallikarjun BR",
            "Ayush Tewari",
            "Xingang Pan",
            "Mohamed Elgharib",
            "Christian Theobalt"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  3D generative models of objects enable photorealistic image synthesis with 3D\ncontrol. Existing methods model the scene as a global scene representation,\nignoring the compositional aspect of the scene. Compositional reasoning can\nenable a wide variety of editing applications, in addition to enabling\ngeneralizable 3D reasoning. In this paper, we present a compositional\ngenerative model, where each semantic part of the object is represented as an\nindependent 3D representation learned from only in-the-wild 2D data. We start\nwith a global generative model (GAN) and learn to decompose it into different\nsemantic parts using supervision from 2D segmentation masks. We then learn to\ncomposite independently sampled parts in order to create coherent global\nscenes. Different parts can be independently sampled while keeping the rest of\nthe object fixed. We evaluate our method on a wide variety of objects and parts\nand demonstrate editing applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.17344v1"
    },
    {
        "title": "Learning Neural Implicit Representations with Surface Signal\n  Parameterizations",
        "authors": [
            "Yanran Guan",
            "Andrei Chubarau",
            "Ruby Rao",
            "Derek Nowrouzezahrai"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Neural implicit surface representations have recently emerged as popular\nalternative to explicit 3D object encodings, such as polygonal meshes,\ntabulated points, or voxels. While significant work has improved the geometric\nfidelity of these representations, much less attention is given to their final\nappearance. Traditional explicit object representations commonly couple the 3D\nshape data with auxiliary surface-mapped image data, such as diffuse color\ntextures and fine-scale geometric details in normal maps that typically require\na mapping of the 3D surface onto a plane, i.e., a surface parameterization;\nimplicit representations, on the other hand, cannot be easily textured due to\nlack of configurable surface parameterization. Inspired by this digital content\nauthoring methodology, we design a neural network architecture that implicitly\nencodes the underlying surface parameterization suitable for appearance data.\nAs such, our model remains compatible with existing mesh-based digital content\nwith appearance data. Motivated by recent work that overfits compact networks\nto individual 3D objects, we present a new weight-encoded neural implicit\nrepresentation that extends the capability of neural implicit surfaces to\nenable various common and important applications of texture mapping. Our method\noutperforms reasonable baselines and state-of-the-art alternatives.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00519v2"
    },
    {
        "title": "Deep Appearance Prefiltering",
        "authors": [
            "Steve Bako",
            "Pradeep Sen",
            "Anton Kaplanyan"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Physically based rendering of complex scenes can be prohibitively costly with\na potentially unbounded and uneven distribution of complexity across the\nrendered image. The goal of an ideal level of detail (LoD) method is to make\nrendering costs independent of the 3D scene complexity, while preserving the\nappearance of the scene. However, current prefiltering LoD methods are limited\nin the appearances they can support due to their reliance of approximate models\nand other heuristics. We propose the first comprehensive multi-scale LoD\nframework for prefiltering 3D environments with complex geometry and materials\n(e.g., the Disney BRDF), while maintaining the appearance with respect to the\nray-traced reference. Using a multi-scale hierarchy of the scene, we perform a\ndata-driven prefiltering step to obtain an appearance phase function and\ndirectional coverage mask at each scale. At the heart of our approach is a\nnovel neural representation that encodes this information into a compact latent\nform that is easy to decode inside a physically based renderer. Once a scene is\nbaked out, our method requires no original geometry, materials, or textures at\nrender time. We demonstrate that our approach compares favorably to\nstate-of-the-art prefiltering methods and achieves considerable savings in\nmemory for complex scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.05932v1"
    },
    {
        "title": "NeRFFaceEditing: Disentangled Face Editing in Neural Radiance Fields",
        "authors": [
            "Kaiwen Jiang",
            "Shu-Yu Chen",
            "Feng-Lin Liu",
            "Hongbo Fu",
            "Lin Gao"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Recent methods for synthesizing 3D-aware face images have achieved rapid\ndevelopment thanks to neural radiance fields, allowing for high quality and\nfast inference speed. However, existing solutions for editing facial geometry\nand appearance independently usually require retraining and are not optimized\nfor the recent work of generation, thus tending to lag behind the generation\nprocess. To address these issues, we introduce NeRFFaceEditing, which enables\nediting and decoupling geometry and appearance in the pretrained\ntri-plane-based neural radiance field while retaining its high quality and fast\ninference speed. Our key idea for disentanglement is to use the statistics of\nthe tri-plane to represent the high-level appearance of its corresponding\nfacial volume. Moreover, we leverage a generated 3D-continuous semantic mask as\nan intermediary for geometry editing. We devise a geometry decoder (whose\noutput is unchanged when the appearance changes) and an appearance decoder. The\ngeometry decoder aligns the original facial volume with the semantic mask\nvolume. We also enhance the disentanglement by explicitly regularizing rendered\nimages with the same appearance but different geometry to be similar in terms\nof color distribution for each facial component separately. Our method allows\nusers to edit via semantic masks with decoupled control of geometry and\nappearance. Both qualitative and quantitative evaluations show the superior\ngeometry and appearance control abilities of our method compared to existing\nand alternative solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.07968v1"
    },
    {
        "title": "Deep scene-scale material estimation from multi-view indoor captures",
        "authors": [
            "Siddhant Prakash",
            "Gilles Rainer",
            "Adrien Bousseau",
            "George Drettakis"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The movie and video game industries have adopted photogrammetry as a way to\ncreate digital 3D assets from multiple photographs of a real-world scene. But\nphotogrammetry algorithms typically output an RGB texture atlas of the scene\nthat only serves as visual guidance for skilled artists to create material maps\nsuitable for physically-based rendering. We present a learning-based approach\nthat automatically produces digital assets ready for physically-based\nrendering, by estimating approximate material maps from multi-view captures of\nindoor scenes that are used with retopologized geometry. We base our approach\non a material estimation Convolutional Neural Network (CNN) that we execute on\neach input image. We leverage the view-dependent visual cues provided by the\nmultiple observations of the scene by gathering, for each pixel of a given\nimage, the color of the corresponding point in other images. This image-space\nCNN provides us with an ensemble of predictions, which we merge in texture\nspace as the last step of our approach. Our results demonstrate that the\nrecovered assets can be directly used for physically-based rendering and\nediting of real indoor scenes from any viewpoint and novel lighting. Our method\ngenerates approximate material maps in a fraction of time compared to the\nclosest previous solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.08047v1"
    },
    {
        "title": "Learning to Rasterize Differentiably",
        "authors": [
            "Chenghao Wu",
            "Hamila Mailee",
            "Zahra Montazeri",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Differentiable rasterization changes the standard formulation of primitive\nrasterization -- by enabling gradient flow from a pixel to its underlying\ntriangles -- using distribution functions in different stages of rendering,\ncreating a \"soft\" version of the original rasterizer. However, choosing the\noptimal softening function that ensures the best performance and convergence to\na desired goal requires trial and error. Previous work has analyzed and\ncompared several combinations of softening. In this work, we take it a step\nfurther and, instead of making a combinatorial choice of softening operations,\nparameterize the continuous space of common softening operations. We study\nmeta-learning tunable softness functions over a set of inverse rendering tasks\n(2D and 3D shape, pose and occlusion) so it generalizes to new and unseen\ndifferentiable rendering tasks with optimal softness.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.13333v2"
    },
    {
        "title": "Finlay, Thames, Dufay, and Paget color screen process collections: Using\n  digital registration of viewing screens to reveal original color",
        "authors": [
            "Geoffrey Barker",
            "Jan Hubička",
            "Mark Jacobs",
            "Linda Kimrová",
            "Kendra Meyer",
            "Doug Peterson"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We discuss digitization, subsequent digital analysis and processing of\nnegatives (and diapositives) made by Finlay, Thames, Dufay, Paget, and similar\nadditive color screen processes. These early color processes (introduced in the\n1890s and popular until the 1950s) used a special color screen filter and a\nmonochromatic negative. Due to poor stability of dyes used to produce color\nscreens many of the photographs appear faded; others exist only in the form of\n(monochromatic) negatives. We discuss the possibility of digitally\nreconstructing the original color from scans of original negatives or by virtue\nof infrared imaging of original transparencies (which eliminates the physically\ncoupled color filters) and digitally recreating the original color filter\npattern using a new open-source software tool. Photographs taken using additive\ncolor screen processes are some of the very earliest color images of our shared\ncultural heritage. They depict people, places, and events for which there are\nno other surviving color images. We hope that our new software tool can bring\nthese images back to life.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.16076v1"
    },
    {
        "title": "Wearing the Same Outfit in Different Ways -- A Controllable Virtual\n  Try-on Method",
        "authors": [
            "Kedan Li",
            "Jeffrey Zhang",
            "Shao-Yu Chang",
            "David Forsyth"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  An outfit visualization method generates an image of a person wearing real\ngarments from images of those garments. Current methods can produce images that\nlook realistic and preserve garment identity, captured in details such as\ncollar, cuffs, texture, hem, and sleeve length. However, no current method can\nboth control how the garment is worn -- including tuck or untuck, opened or\nclosed, high or low on the waist, etc.. -- and generate realistic images that\naccurately preserve the properties of the original garment. We describe an\noutfit visualization method that controls drape while preserving garment\nidentity. Our system allows instance independent editing of garment drape,\nwhich means a user can construct an edit (e.g. tucking a shirt in a specific\nway) that can be applied to all shirts in a garment collection. Garment detail\nis preserved by relying on a warping procedure to place the garment on the body\nand a generator then supplies fine shading detail. To achieve instance\nindependent control, we use control points with garment category-level\nsemantics to guide the warp. The method produces state-of-the-art quality\nimages, while allowing creative ways to style garments, including allowing tops\nto be tucked or untucked; jackets to be worn open or closed; skirts to be worn\nhigher or lower on the waist; and so on. The method allows interactive control\nto correct errors in individual renderings too. Because the edits are instance\nindependent, they can be applied to large pools of garments automatically and\ncan be conditioned on garment metadata (e.g. all cropped jackets are worn\nclosed or all bomber jackets are worn closed).\n",
        "pdf_link": "http://arxiv.org/pdf/2211.16989v1"
    },
    {
        "title": "LoopDraw: a Loop-Based Autoregressive Model for Shape Synthesis and\n  Editing",
        "authors": [
            "Nam Anh Dinh",
            "Haochen Wang",
            "Greg Shakhnarovich",
            "Rana Hanocka"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  There is no settled universal 3D representation for geometry with many\nalternatives such as point clouds, meshes, implicit functions, and voxels to\nname a few. In this work, we present a new, compelling alternative for\nrepresenting shapes using a sequence of cross-sectional closed loops. The loops\nacross all planes form an organizational hierarchy which we leverage for\nautoregressive shape synthesis and editing. Loops are a non-local description\nof the underlying shape, as simple loop manipulations (such as shifts) result\nin significant structural changes to the geometry. This is in contrast to\nmanipulating local primitives such as points in a point cloud or a triangle in\na triangle mesh. We further demonstrate that loops are intuitive and natural\nprimitive for analyzing and editing shapes, both computationally and for users.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.04981v2"
    },
    {
        "title": "3D Highlighter: Localizing Regions on 3D Shapes via Text Descriptions",
        "authors": [
            "Dale Decatur",
            "Itai Lang",
            "Rana Hanocka"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present 3D Highlighter, a technique for localizing semantic regions on a\nmesh using text as input. A key feature of our system is the ability to\ninterpret \"out-of-domain\" localizations. Our system demonstrates the ability to\nreason about where to place non-obviously related concepts on an input 3D\nshape, such as adding clothing to a bare 3D animal model. Our method\ncontextualizes the text description using a neural field and colors the\ncorresponding region of the shape using a probability-weighted blend. Our\nneural optimization is guided by a pre-trained CLIP encoder, which bypasses the\nneed for any 3D datasets or 3D annotations. Thus, 3D Highlighter is highly\nflexible, general, and capable of producing localizations on a myriad of input\nshapes. Our code is publicly available at\nhttps://github.com/threedle/3DHighlighter.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.11263v1"
    },
    {
        "title": "SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation\n  with Fine-Grained Geometry",
        "authors": [
            "Lin Gao",
            "Jia-Mu Sun",
            "Kaichun Mo",
            "Yu-Kun Lai",
            "Leonidas J. Guibas",
            "Jie Yang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  3D indoor scenes are widely used in computer graphics, with applications\nranging from interior design to gaming to virtual and augmented reality. They\nalso contain rich information, including room layout, as well as furniture\ntype, geometry, and placement. High-quality 3D indoor scenes are highly\ndemanded while it requires expertise and is time-consuming to design\nhigh-quality 3D indoor scenes manually. Existing research only addresses\npartial problems: some works learn to generate room layout, and other works\nfocus on generating detailed structure and geometry of individual furniture\nobjects. However, these partial steps are related and should be addressed\ntogether for optimal synthesis. We propose SCENEHGN, a hierarchical graph\nnetwork for 3D indoor scenes that takes into account the full hierarchy from\nthe room level to the object level, then finally to the object part level.\nTherefore for the first time, our method is able to directly generate plausible\n3D room content, including furniture objects with fine-grained geometry, and\ntheir layout. To address the challenge, we introduce functional regions as\nintermediate proxies between the room and object levels to make learning more\nmanageable. To ensure plausibility, our graph-based representation incorporates\nboth vertical edges connecting child nodes with parent nodes from different\nlevels, and horizontal edges encoding relationships between nodes at the same\nlevel. Extensive experiments demonstrate that our method produces superior\ngeneration results, even when comparing results of partial steps with\nalternative methods that can only achieve these. We also demonstrate that our\nmethod is effective for various applications such as part-level room editing,\nroom interpolation, and room generation by arbitrary room boundaries.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.10237v1"
    },
    {
        "title": "LaplacianFusion: Detailed 3D Clothed-Human Body Reconstruction",
        "authors": [
            "Hyomin Kim",
            "Hyeonseo Nam",
            "Jungeon Kim",
            "Jaesik Park",
            "Seungyong Lee"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose LaplacianFusion, a novel approach that reconstructs detailed and\ncontrollable 3D clothed-human body shapes from an input depth or 3D point cloud\nsequence. The key idea of our approach is to use Laplacian coordinates,\nwell-known differential coordinates that have been used for mesh editing, for\nrepresenting the local structures contained in the input scans, instead of\nimplicit 3D functions or vertex displacements used previously. Our approach\nreconstructs a controllable base mesh using SMPL, and learns a surface function\nthat predicts Laplacian coordinates representing surface details on the base\nmesh. For a given pose, we first build and subdivide a base mesh, which is a\ndeformed SMPL template, and then estimate Laplacian coordinates for the mesh\nvertices using the surface function. The final reconstruction for the pose is\nobtained by integrating the estimated Laplacian coordinates as a whole.\nExperimental results show that our approach based on Laplacian coordinates\nsuccessfully reconstructs more visually pleasing shape details than previous\nmethods. The approach also enables various surface detail manipulations, such\nas detail transfer and enhancement.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14251v1"
    },
    {
        "title": "Sketch2Cloth: Sketch-based 3D Garment Generation with Unsigned Distance\n  Fields",
        "authors": [
            "Yi He",
            "Haoran Xie",
            "Kazunori Miyata"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  3D model reconstruction from a single image has achieved great progress with\nthe recent deep generative models. However, the conventional reconstruction\napproaches with template mesh deformation and implicit fields have difficulty\nin reconstructing non-watertight 3D mesh models, such as garments. In contrast\nto image-based modeling, the sketch-based approach can help users generate 3D\nmodels to meet the design intentions from hand-drawn sketches. In this study,\nwe propose Sketch2Cloth, a sketch-based 3D garment generation system using the\nunsigned distance fields from the user's sketch input. Sketch2Cloth first\nestimates the unsigned distance function of the target 3D model from the sketch\ninput, and extracts the mesh from the estimated field with Marching Cubes. We\nalso provide the model editing function to modify the generated mesh. We\nverified the proposed Sketch2Cloth with quantitative evaluations on garment\ngeneration and editing with a state-of-the-art approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.00167v1"
    },
    {
        "title": "The First Comprehensive Dataset with Multiple Distortion Types for\n  Visual Just-Noticeable Differences",
        "authors": [
            "Yaxuan Liu",
            "Jian Jin",
            "Yuan Xue",
            "Weisi Lin"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Recently, with the development of deep learning, a number of Just Noticeable\nDifference (JND) datasets have been built for JND modeling. However, all the\nexisting JND datasets only label the JND points based on the level of\ncompression distortion. Hence, JND models learned from such datasets can only\nbe used for image/video compression. As known, JND is a major characteristic of\nthe human visual system (HVS), which reflects the maximum visual distortion\nthat the HVS can tolerate. Hence, a generalized JND modeling should take more\nkinds of distortion types into account. To benefit JND modeling, this work\nestablishes a generalized JND dataset with a coarse-to-fine JND selection,\nwhich contains 106 source images and 1,642 JND maps, covering 25 distortion\ntypes. To this end, we proposed a coarse JND candidate selection scheme to\nselect the distorted images from the existing Image Quality Assessment (IQA)\ndatasets as JND candidates instead of generating JND maps ourselves. Then, a\nfine JND selection is carried out on the JND candidates with a crowdsourced\nsubjective assessment.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.02562v2"
    },
    {
        "title": "TACHYON: Efficient Shared Memory Parallel Computation of Extremum Graphs",
        "authors": [
            "Abhijath Ande",
            "Varshini Subhash",
            "Vijay Natarajan"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The extremum graph is a succinct representation of the Morse decomposition of\na scalar field. It has increasingly become a useful data structure that\nsupports topological feature directed visualization of 2D / 3D scalar fields,\nand enables dimensionality reduction together with exploratory analysis of high\ndimensional scalar fields. Current methods that employ the extremum graph\ncompute it either using a simple sequential algorithm for computing the Morse\ndecomposition or by computing the more detailed Morse-Smale complex. Both\napproaches are typically limited to two and three dimensional scalar fields. We\ndescribe a GPU-CPU hybrid parallel algorithm for computing the extremum graph\nof scalar fields in all dimensions. The proposed shared memory algorithm\nutilizes both fine grained parallelism and task parallelism to achieve\nefficiency. An open source software library, TACHYON, that implements the\nalgorithm exhibits superior performance and good scaling behavior.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.02724v1"
    },
    {
        "title": "A Topological Distance between Multi-fields based on Multi-Dimensional\n  Persistence Diagrams",
        "authors": [
            "Yashwanth Ramamurthi",
            "Amit Chattopadhyay"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The problem of computing topological distance between two scalar fields based\non Reeb graphs or contour trees has been studied and applied successfully to\nvarious problems in topological shape matching, data analysis, and\nvisualization. However, generalizing such results for computing distance\nmeasures between two multi-fields based on their Reeb spaces is still in its\ninfancy. Towards this, in the current paper we propose a technique to compute\nan effective distance measure between two multi-fields by computing a novel\n\\emph{multi-dimensional persistence diagram} (MDPD) corresponding to each of\nthe (quantized) Reeb spaces. First, we construct a multi-dimensional Reeb graph\n(MDRG), which is a hierarchical decomposition of the Reeb space into a\ncollection of Reeb graphs. The MDPD corresponding to each MDRG is then computed\nbased on the persistence diagrams of the component Reeb graphs of the MDRG. Our\ndistance measure extends the Wasserstein distance between two persistence\ndiagrams of Reeb graphs to MDPDs of MDRGs. We prove that the proposed measure\nis a pseudo-metric and satisfies a stability property. Effectiveness of the\nproposed distance measure has been demonstrated in (i) shape retrieval contest\ndata - SHREC $2010$ and (ii) Pt-CO bond detection data from computational\nchemistry. Experimental results show that the proposed distance measure based\non the Reeb spaces has more discriminating power in clustering the shapes and\ndetecting the formation of a stable Pt-CO bond as compared to the similar\nmeasures between Reeb graphs.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.03038v2"
    },
    {
        "title": "Scalable and Efficient Functional Map Computations on Dense Meshes",
        "authors": [
            "Robin Magnet",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Spectral geometric methods have brought revolutionary changes to the field of\ngeometry processing. Of particular interest is the study of the Laplacian\nspectrum as a compact, isometry and permutation-invariant representation of a\nshape. Some recent works show how the intrinsic geometry of a full shape can be\nrecovered from its spectrum, but there are approaches that consider the more\nchallenging problem of recovering the geometry from the spectral information of\npartial shapes. In this paper, we propose a possible way to fill this gap. We\nintroduce a learning-based method to estimate the Laplacian spectrum of the\nunion of partial non-rigid 3D shapes, without actually computing the 3D\ngeometry of the union or any correspondence between those partial shapes. We do\nso by operating purely in the spectral domain and by defining the union\noperation between short sequences of eigenvalues. We show that the approximated\nunion spectrum can be used as-is to reconstruct the complete geometry [MRC*19],\nperform region localization on a template [RTO*19] and retrieve shapes from a\ndatabase, generalizing ShapeDNA [RWP06] to work with partialities. Working with\neigenvalues allows us to deal with unknown correspondence, different sampling,\nand different discretizations (point clouds and meshes alike), making this\noperation especially robust and general. Our approach is data-driven and can\ngeneralize to isometric and non-isometric deformations of the surface, as long\nas these stay within the same semantic class (e.g., human bodies or horses), as\nwell as to partiality artifacts not seen at training time.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.05965v1"
    },
    {
        "title": "Animating Explosions",
        "authors": [
            "Gary D. Yngve",
            "James F. O'Brien",
            "Jessica K. Hodgins"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we introduce techniques for animating explosions and their\neffects. The primary effect of an explosion is a disturbance that causes a\nshock wave to propagate through the surrounding medium. This disturbance\ndetermines the behavior of nearly all other secondary effects seen in\nexplosions. We simulate the propagation of an explosion through the surrounding\nair using a computational fluid dynamics model based on the equations for\ncompressible, viscous flow. To model the numerically stable formulation of\nshocks along blast wave fronts, we employ an integration method that can handle\nsteep gradients without introducing inappropriate damping. The system includes\ntwo-way coupling between solid objects and surrounding fluid. Using this\ntechnique, we can generate a variety of effects including shaped explosive\ncharges, a projectile propelled from a chamber by an explosion, and objects\ndamaged by a blast. With appropriate rendering techniques, our explosion model\ncan be used to create such visual effects such as fireballs, dust clouds, and\nthe refraction of light caused by a blast wave.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10541v1"
    },
    {
        "title": "MAD-FC: A Fold Change Visualization with Readability, Proportionality,\n  and Symmetry",
        "authors": [
            "Bruce A. Corliss",
            "Yaotian Wang",
            "Francis P. Driscoll",
            "Heman Shakeri",
            "Philip E. Bourne"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a fold change visualization that demonstrates a combination of\nproperties from log and linear plots of fold change. A useful fold change\nvisualization can exhibit: (1) readability, where fold change values are\nrecoverable from datapoint position; (2) proportionality, where fold change\nvalues of the same direction are proportionally distant from the point of no\nchange; (3) symmetry, where positive and negative fold changes are equidistant\nto the point of no change; and (4) high dynamic range, where datapoint values\nare discernable across orders of magnitude. A linear visualization has\nreadability and partial proportionality but lacks high dynamic range and\nsymmetry (because negative direction fold changes are bound between [0, 1]\nwhile positive are between [1, $\\infty$]). Log plots of fold change have\npartial readability, high dynamic range, and symmetry, but lack proportionality\nbecause of the log transform. We outline a new transform and visualization,\nnamed mirrored axis distortion of fold change (MAD-FC), that extends a linear\nvisualization of fold change data to exhibit readability, proportionality, and\nsymmetry (but still has the limited dynamic range of linear plots). We\nillustrate the use of MAD-FC with biomedical data using various fold change\ncharts. We argue that MAD-FC plots may be a more useful visualization than log\nor linear plots for applications that require a limited dynamic range\n(approximately $\\pm$2 orders of magnitude or $\\pm$8 units in log2 space).\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10829v1"
    },
    {
        "title": "Perceptual Requirements for World-Locked Rendering in AR and VR",
        "authors": [
            "Phillip Guan",
            "Eric Penner",
            "Joel Hegland",
            "Benjamin Letham",
            "Douglas Lanman"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Stereoscopic, head-tracked display systems can show users realistic,\nworld-locked virtual objects and environments. However, discrepancies between\nthe rendering pipeline and physical viewing conditions can lead to perceived\ninstability in the rendered content resulting in reduced immersion and,\npotentially, visually-induced motion sickness. Precise requirements to achieve\nperceptually stable world-locked rendering (WLR) are unknown due to the\nchallenge of constructing a wide field of view, distortion-free display with\nhighly accurate head and eye tracking. We present a system capable of rendering\nvirtual objects over real-world references without perceivable drift under such\nconstraints. This platform is used to study acceptable errors in render camera\nposition for WLR in augmented and virtual reality scenarios, where we find an\norder of magnitude difference in perceptual sensitivity. We conclude with an\nanalytic model which examines changes to apparent depth and visual direction in\nresponse to camera displacement errors.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.15666v2"
    },
    {
        "title": "4D Agnostic Real-Time Facial Animation Pipeline for Desktop Scenarios",
        "authors": [
            "Wei Chen",
            "HongWei Xu",
            "Jelo Wang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a high-precision real-time facial animation pipeline suitable for\nanimators to use on their desktops. This pipeline is about to be launched in\nFACEGOOD's Avatary\\footnote{https://www.avatary.com/} software, which will\naccelerate animators' productivity. The pipeline differs from professional\nhead-mounted facial capture solutions in that it only requires the use of a\nconsumer-grade 3D camera on the desk to achieve high-precision real-time facial\ncapture. The system enables animators to create high-quality facial animations\nwith ease and speed, while reducing the cost and complexity of traditional\nfacial capture solutions. Our approach has the potential to revolutionize the\nway facial animation is done in the entertainment industry.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.02814v1"
    },
    {
        "title": "HyperINR: A Fast and Predictive Hypernetwork for Implicit Neural\n  Representations via Knowledge Distillation",
        "authors": [
            "Qi Wu",
            "David Bauer",
            "Yuyang Chen",
            "Kwan-Liu Ma"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Implicit Neural Representations (INRs) have recently exhibited immense\npotential in the field of scientific visualization for both data generation and\nvisualization tasks. However, these representations often consist of large\nmulti-layer perceptrons (MLPs), necessitating millions of operations for a\nsingle forward pass, consequently hindering interactive visual exploration.\nWhile reducing the size of the MLPs and employing efficient parametric encoding\nschemes can alleviate this issue, it compromises generalizability for unseen\nparameters, rendering it unsuitable for tasks such as temporal\nsuper-resolution. In this paper, we introduce HyperINR, a novel hypernetwork\narchitecture capable of directly predicting the weights for a compact INR. By\nharnessing an ensemble of multiresolution hash encoding units in unison, the\nresulting INR attains state-of-the-art inference performance (up to 100x higher\ninference bandwidth) and can support interactive photo-realistic volume\nvisualization. Additionally, by incorporating knowledge distillation,\nexceptional data and visualization generation quality is achieved, making our\nmethod valuable for real-time parameter exploration. We validate the\neffectiveness of the HyperINR architecture through a comprehensive ablation\nstudy. We showcase the versatility of HyperINR across three distinct scientific\ndomains: novel view synthesis, temporal super-resolution of volume data, and\nvolume rendering with dynamic global shadows. By simultaneously achieving\nefficiency and generalizability, HyperINR paves the way for applying INR in a\nwider array of scientific visualization applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.04188v1"
    },
    {
        "title": "Pixel-wise Guidance for Utilizing Auxiliary Features in Monte Carlo\n  Denoising",
        "authors": [
            "Kyu Beom Han",
            "Olivia G. Odenthal",
            "Woo Jae Kim",
            "Sung-Eui Yoon"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Auxiliary features such as geometric buffers (G-buffers) and path descriptors\n(P-buffers) have been shown to significantly improve Monte Carlo (MC)\ndenoising. However, recent approaches implicitly learn to exploit auxiliary\nfeatures for denoising, which could lead to insufficient utilization of each\ntype of auxiliary features. To overcome such an issue, we propose a denoising\nframework that relies on an explicit pixel-wise guidance for utilizing\nauxiliary features. First, we train two denoisers, each trained by a different\nauxiliary feature (i.e., G-buffers or P-buffers). Then we design our ensembling\nnetwork to obtain per-pixel ensembling weight maps, which represent pixel-wise\nguidance for which auxiliary feature should be dominant at reconstructing each\nindividual pixel and use them to ensemble the two denoised results of our\ndenosiers. We also propagate our pixel-wise guidance to the denoisers by\njointly training the denoisers and the ensembling network, further guiding the\ndenoisers to focus on regions where G-buffers or P-buffers are relatively\nimportant for denoising. Our result and show considerable improvement in\ndenoising performance compared to the baseline denoising model using both\nG-buffers and P-buffers.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.04967v1"
    },
    {
        "title": "Photon Field Networks for Dynamic Real-Time Volumetric Global\n  Illumination",
        "authors": [
            "David Bauer",
            "Qi Wu",
            "Kwan-Liu Ma"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Volume data is commonly found in many scientific disciplines, like medicine,\nphysics, and biology. Experts rely on robust scientific visualization\ntechniques to extract valuable insights from the data. Recent years have shown\npath tracing to be the preferred approach for volumetric rendering, given its\nhigh levels of realism. However, real-time volumetric path tracing often\nsuffers from stochastic noise and long convergence times, limiting interactive\nexploration. In this paper, we present a novel method to enable real-time\nglobal illumination for volume data visualization. We develop Photon Field\nNetworks -- a phase-function-aware, multi-light neural representation of\nindirect volumetric global illumination. The fields are trained on multi-phase\nphoton caches that we compute a priori. Training can be done within seconds,\nafter which the fields can be used in various rendering tasks. To showcase\ntheir potential, we develop a custom neural path tracer, with which our photon\nfields achieve interactive framerates even on large datasets. We conduct\nin-depth evaluations of the method's performance, including visual quality,\nstochastic noise, inference and rendering speeds, and accuracy regarding\nillumination and phase function awareness. Results are compared to ray\nmarching, path tracing and photon mapping. Our findings show that Photon Field\nNetworks can faithfully represent indirect global illumination across the phase\nspectrum while exhibiting less stochastic noise and rendering at a\nsignificantly faster rate than traditional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.07338v1"
    },
    {
        "title": "Text-guided Image-and-Shape Editing and Generation: A Short Survey",
        "authors": [
            "Cheng-Kang Ted Chao",
            "Yotam Gingold"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Image and shape editing are ubiquitous among digital artworks. Graphics\nalgorithms facilitate artists and designers to achieve desired editing intents\nwithout going through manually tedious retouching. In the recent advance of\nmachine learning, artists' editing intents can even be driven by text, using a\nvariety of well-trained neural networks. They have seen to be receiving an\nextensive success on such as generating photorealistic images, artworks and\nhuman poses, stylizing meshes from text, or auto-completion given image and\nshape priors. In this short survey, we provide an overview over 50 papers on\nstate-of-the-art (text-guided) image-and-shape generation techniques. We start\nwith an overview on recent editing algorithms in the introduction. Then, we\nprovide a comprehensive review on text-guided editing techniques for 2D and 3D\nindependently, where each of its sub-section begins with a brief background\nintroduction. We also contextualize editing algorithms under recent implicit\nneural representations. Finally, we conclude the survey with the discussion\nover existing methods and potential research ideas.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09244v1"
    },
    {
        "title": "StyleDEM: a Versatile Model for Authoring Terrains",
        "authors": [
            "Simon Perche",
            "Adrien Peytavie",
            "Bedrich Benes",
            "Eric Galin",
            "Eric Guérin"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Many terrain modelling methods have been proposed for the past decades,\nproviding efficient and often interactive authoring tools. However, they\ngenerally do not include any notion of style, which is a critical aspect for\ndesigners in the entertainment industry. We introduce StyleDEM, a new\ngenerative adversarial network method for terrain synthesis and authoring, with\na versatile toolbox of authoring methods with style. This method starts from an\ninput sketch or an existing terrain. It outputs a terrain with features that\ncan be authored using interactive brushes and enhanced with additional tools\nsuch as style manipulation or super-resolution. The strength of our approach\nresides in the versatility and interoperability of the toolbox.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09626v1"
    },
    {
        "title": "Patch-based 3D Natural Scene Generation from a Single Example",
        "authors": [
            "Weiyu Li",
            "Xuelin Chen",
            "Jue Wang",
            "Baoquan Chen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We target a 3D generative model for general natural scenes that are typically\nunique and intricate. Lacking the necessary volumes of training data, along\nwith the difficulties of having ad hoc designs in presence of varying scene\ncharacteristics, renders existing setups intractable. Inspired by classical\npatch-based image models, we advocate for synthesizing 3D scenes at the patch\nlevel, given a single example. At the core of this work lies important\nalgorithmic designs w.r.t the scene representation and generative patch\nnearest-neighbor module, that address unique challenges arising from lifting\nclassical 2D patch-based framework to 3D generation. These design choices, on a\ncollective level, contribute to a robust, effective, and efficient model that\ncan generate high-quality general natural scenes with both realistic geometric\nstructure and visual appearance, in large quantities and varieties, as\ndemonstrated upon a variety of exemplar scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.12670v2"
    },
    {
        "title": "A Survey of Methods for Converting Unstructured Data to CSG Models",
        "authors": [
            "Pierre-Alain Fayolle",
            "Markus Friedrich"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The goal of this document is to survey existing methods for recovering CSG\nrepresentations from unstructured data such as 3D point-clouds or polygon\nmeshes. We review and discuss related topics such as the segmentation and\nfitting of the input data. We cover techniques from solid modeling and CAD for\npolyhedron to CSG and B-rep to CSG conversion. We look at approaches coming\nfrom program synthesis, evolutionary techniques (such as genetic programming or\ngenetic algorithm), and deep learning methods. Finally, we conclude with a\ndiscussion of techniques for the generation of computer programs representing\nsolids (not just CSG models) and higher-level representations (such as, for\nexample, the ones based on sketch and extrusion or feature based operations).\n",
        "pdf_link": "http://arxiv.org/pdf/2305.01220v1"
    },
    {
        "title": "UrbanBIS: a Large-scale Benchmark for Fine-grained Urban Building\n  Instance Segmentation",
        "authors": [
            "Guoqing Yang",
            "Fuyou Xue",
            "Qi Zhang",
            "Ke Xie",
            "Chi-Wing Fu",
            "Hui Huang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present the UrbanBIS benchmark for large-scale 3D urban understanding,\nsupporting practical urban-level semantic and building-level instance\nsegmentation. UrbanBIS comprises six real urban scenes, with 2.5 billion\npoints, covering a vast area of 10.78 square kilometers and 3,370 buildings,\ncaptured by 113,346 views of aerial photogrammetry. Particularly, UrbanBIS\nprovides not only semantic-level annotations on a rich set of urban objects,\nincluding buildings, vehicles, vegetation, roads, and bridges, but also\ninstance-level annotations on the buildings. Further, UrbanBIS is the first 3D\ndataset that introduces fine-grained building sub-categories, considering a\nwide variety of shapes for different building types. Besides, we propose B-Seg,\na building instance segmentation method to establish UrbanBIS. B-Seg adopts an\nend-to-end framework with a simple yet effective strategy for handling\nlarge-scale point clouds. Compared with mainstream methods, B-Seg achieves\nbetter accuracy with faster inference speed on UrbanBIS. In addition to the\ncarefully-annotated point clouds, UrbanBIS provides high-resolution\naerial-acquisition photos and high-quality large-scale 3D reconstruction\nmodels, which shall facilitate a wide range of studies such as multi-view\nstereo, urban LOD generation, aerial path planning, autonomous navigation, road\nnetwork extraction, and so on, thus serving as an important platform for many\nintelligent city applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.02627v1"
    },
    {
        "title": "Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution",
        "authors": [
            "Hyojoon Park",
            "Sangeetha Grama Srinivasan",
            "Matthew Cong",
            "Doyub Kim",
            "Byungsoo Kim",
            "Jonathan Swartz",
            "Ken Museth",
            "Eftychios Sifakis"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a neural network-based simulation super-resolution framework that\ncan efficiently and realistically enhance a facial performance produced by a\nlow-cost, realtime physics-based simulation to a level of detail that closely\napproximates that of a reference-quality off-line simulator with much higher\nresolution (26x element count in our examples) and accurate physical modeling.\nOur approach is rooted in our ability to construct - via simulation - a\ntraining set of paired frames, from the low- and high-resolution simulators\nrespectively, that are in semantic correspondence with each other. We use face\nanimation as an exemplar of such a simulation domain, where creating this\nsemantic congruence is achieved by simply dialing in the same muscle actuation\ncontrols and skeletal pose in the two simulators. Our proposed neural network\nsuper-resolution framework generalizes from this training set to unseen\nexpressions, compensates for modeling discrepancies between the two simulations\ndue to limited resolution or cost-cutting approximations in the real-time\nvariant, and does not require any semantic descriptors or parameters to be\nprovided as input, other than the result of the real-time simulation. We\nevaluate the efficacy of our pipeline on a variety of expressive performances\nand provide comparisons and ablation experiments for plausible variations and\nalternatives to our proposed scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03216v3"
    },
    {
        "title": "PMP: Learning to Physically Interact with Environments using Part-wise\n  Motion Priors",
        "authors": [
            "Jinseok Bae",
            "Jungdam Won",
            "Donggeun Lim",
            "Cheol-Hui Min",
            "Young Min Kim"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a method to animate a character incorporating multiple part-wise\nmotion priors (PMP). While previous works allow creating realistic articulated\nmotions from reference data, the range of motion is largely limited by the\navailable samples. Especially for the interaction-rich scenarios, it is\nimpractical to attempt acquiring every possible interacting motion, as the\ncombination of physical parameters increases exponentially. The proposed PMP\nallows us to assemble multiple part skills to animate a character, creating a\ndiverse set of motions with different combinations of existing data. In our\npipeline, we can train an agent with a wide range of part-wise priors.\nTherefore, each body part can obtain a kinematic insight of the style from the\nmotion captures, or at the same time extract dynamics-related information from\nthe additional part-specific simulation. For example, we can first train a\ngeneral interaction skill, e.g. grasping, only for the dexterous part, and then\ncombine the expert trajectories from the pre-trained agent with the kinematic\npriors of other limbs. Eventually, our whole-body agent learns a novel physical\ninteraction skill even with the absence of the object trajectories in the\nreference motion sequence.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03249v1"
    },
    {
        "title": "Stochastic Texture Filtering",
        "authors": [
            "Marcos Fajardo",
            "Bartlomiej Wronski",
            "Marco Salvi",
            "Matt Pharr"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  2D texture maps and 3D voxel arrays are widely used to add rich detail to the\nsurfaces and volumes of rendered scenes, and filtered texture lookups are\nintegral to producing high-quality imagery. We show that filtering textures\nafter evaluating lighting, rather than before BSDF evaluation as is current\npractice, gives a more accurate solution to the rendering equation. These\nbenefits are not merely theoretical, but are apparent in common cases. We\nfurther show that stochastically sampling texture filters is crucial for\nenabling this approach, which has not been possible previously except in\nlimited cases. Stochastic texture filtering offers additional benefits,\nincluding efficient implementation of high-quality texture filters and\nefficient filtering of textures stored in compressed and sparse data\nstructures, including neural representations. We demonstrate applications in\nboth real-time and offline rendering and show that the additional stochastic\nerror is minimal. Furthermore, this error is handled well by either\nspatiotemporal denoising or moderate pixel sampling rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.05810v2"
    },
    {
        "title": "Enhancing Vascular Analysis with Distance Visualizations: An Overview\n  and Implementation",
        "authors": [
            "Jan Hombeck",
            "Monique Meuschke",
            "Simon Lieb",
            "Nils Lichtenberg",
            "Felix Fleisch",
            "Maximilian Enderling",
            "Rabi Datta",
            "Michael Krone",
            "Christian Hansen",
            "Bernhard Preim",
            "Kai Lawonn"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In recent years, the use of expressive surface visualizations in the\nrepresentation of vascular structures has gained significant attention. These\nvisualizations provide a comprehensive understanding of complex anatomical\nstructures and are crucial for treatment planning and medical education.\nHowever, to aid decision-making, physicians require visualizations that\naccurately depict anatomical structures and their spatial relationships in a\nclear and well-perceivable manner. This work extends a previous paper and\npresents a thorough examination of common techniques for encoding distance\ninformation of 3D vessel surfaces and provides an implementation of these\nvisualizations. A Unity environment and detailed implementation instructions\nfor sixteen different visualizations are provided. These visualizations can be\nclassified into four categories: fundamental, surface-based, auxiliary, and\nillustrative. Furthermore, this extension includes tools to generate endpoint\nlocations for vascular models. Overall this framework serves as a valuable\nresource for researchers in the field of vascular surface visualization by\nreducing the barrier to entry and promoting further research in this area. By\nproviding an implementation of various visualizations, this paper aims to aid\nin the development of accurate and effective visual representations of vascular\nstructures to assist in treatment planning and medical education.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.06726v2"
    },
    {
        "title": "Scalable Ray Tracing Using the Distributed FrameBuffer",
        "authors": [
            "Will Usher",
            "Ingo Wald",
            "Jefferson Amstutz",
            "Johannes Günther",
            "Carson Brownlee",
            "Valerio Pascucci"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Image- and data-parallel rendering across multiple nodes on high-performance\ncomputing systems is widely used in visualization to provide higher frame\nrates, support large data sets, and render data in situ. Specifically for in\nsitu visualization, reducing bottlenecks incurred by the visualization and\ncompositing is of key concern to reduce the overall simulation runtime.\nMoreover, prior algorithms have been designed to support either image- or\ndata-parallel rendering and impose restrictions on the data distribution,\nrequiring different implementations for each configuration. In this paper, we\nintroduce the Distributed FrameBuffer, an asynchronous image-processing\nframework for multi-node rendering. We demonstrate that our approach achieves\nperformance superior to the state of the art for common use cases, while\nproviding the flexibility to support a wide range of parallel rendering\nalgorithms and data distributions. By building on this framework, we extend the\nopen-source ray tracing library OSPRay with a data-distributed API, enabling\nits use in data-distributed and in situ visualization applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.07083v1"
    },
    {
        "title": "Neural Face Rigging for Animating and Retargeting Facial Meshes in the\n  Wild",
        "authors": [
            "Dafei Qin",
            "Jun Saito",
            "Noam Aigerman",
            "Thibault Groueix",
            "Taku Komura"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose an end-to-end deep-learning approach for automatic rigging and\nretargeting of 3D models of human faces in the wild. Our approach, called\nNeural Face Rigging (NFR), holds three key properties:\n  (i) NFR's expression space maintains human-interpretable editing parameters\nfor artistic controls;\n  (ii) NFR is readily applicable to arbitrary facial meshes with different\nconnectivity and expressions;\n  (iii) NFR can encode and produce fine-grained details of complex expressions\nperformed by arbitrary subjects.\n  To the best of our knowledge, NFR is the first approach to provide realistic\nand controllable deformations of in-the-wild facial meshes, without the manual\ncreation of blendshapes or correspondence. We design a deformation autoencoder\nand train it through a multi-dataset training scheme, which benefits from the\nunique advantages of two data sources: a linear 3DMM with interpretable control\nparameters as in FACS, and 4D captures of real faces with fine-grained details.\nThrough various experiments, we show NFR's ability to automatically produce\nrealistic and accurate facial deformations across a wide range of existing\ndatasets as well as noisy facial scans in-the-wild, while providing\nartist-controlled, editable parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.08296v1"
    },
    {
        "title": "ReMatching: Low-Resolution Representations for Scalable Shape\n  Correspondence",
        "authors": [
            "Filippo Maggioli",
            "Daniele Baieri",
            "Emanuele Rodolà",
            "Simone Melzi"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce \\emph{ReMatching}, a novel shape correspondence solution based\non the functional maps framework. Our method, by exploiting a new and\nappropriate \\emph{re}-meshing paradigm, can target shape-\\emph{matching} tasks\neven on meshes counting millions of vertices, where the original functional\nmaps does not apply or requires a massive computational cost. The core of our\nprocedure is a time-efficient remeshing algorithm which constructs a\nlow-resolution geometry while acting conservatively on the original topology\nand metric. These properties allow translating the functional maps optimization\nproblem on the resulting low-resolution representation, thus enabling efficient\ncomputation of correspondences with functional map approaches. Finally, we\npropose an efficient technique for extending the estimated correspondence to\nthe original meshes. We show that our method is more efficient and effective\nthrough quantitative and qualitative comparisons, outperforming\nstate-of-the-art pipelines in quality and computational cost.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.09274v3"
    },
    {
        "title": "Shortest Path to Boundary for Self-Intersecting Meshes",
        "authors": [
            "He Chen",
            "Elie Diaz",
            "Cem Yuksel"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce a method for efficiently computing the exact shortest path to\nthe boundary of a mesh from a given internal point in the presence of\nself-intersections. We provide a formal definition of shortest boundary paths\nfor self-intersecting objects and present a robust algorithm for computing the\nactual shortest boundary path. The resulting method offers an effective\nsolution for collision and self-collision handling while simulating deformable\nvolumetric objects, using fast simulation techniques that provide no guarantees\non collision resolution. Our evaluation includes complex self-collision\nscenarios with a large number of active contacts, showing that our method can\nsuccessfully handle them by introducing a relatively minor computational\noverhead.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.09778v1"
    },
    {
        "title": "Deep and Fast Approximate Order Independent Transparency",
        "authors": [
            "Grigoris Tsopouridis",
            "Andreas-Alexandros Vasilakis",
            "Ioannis Fudos"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a machine learning approach for efficiently computing order\nindependent transparency (OIT). Our method is fast, requires a small constant\namount of memory (depends only on the screen resolution and not on the number\nof triangles or transparent layers), is more accurate as compared to previous\napproximate methods, works for every scene without setup and is portable to all\nplatforms running even with commodity GPUs. Our method requires a rendering\npass to extract all features that are subsequently used to predict the overall\nOIT pixel color with a pre-trained neural network. We provide a comparative\nexperimental evaluation and shader source code of all methods for reproduction\nof the experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.10197v1"
    },
    {
        "title": "Extracting a functional representation from a dictionary for non-rigid\n  shape matching",
        "authors": [
            "Michele Colombo",
            "Giacomo Boracchi",
            "Simone Melzi"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Shape matching is a fundamental problem in computer graphics with many\napplications. Functional maps translate the point-wise shape-matching problem\ninto its functional counterpart and have inspired numerous solutions over the\nlast decade. Nearly all the solutions based on functional maps rely on the\neigenfunctions of the Laplace-Beltrami Operator (LB) to describe the functional\nspaces defined on the surfaces and then convert the functional correspondences\ninto point-wise correspondences. However, this final step is often error-prone\nand inaccurate in tiny regions and protrusions, where the energy of LB does not\nuniformly cover the surface. We propose a new functional basis Principal\nComponents of a Dictionary (PCD) to address such intrinsic limitation. PCD\nconstructs an orthonormal basis from the Principal Component Analysis (PCA) of\na dictionary of functions defined over the shape. These dictionaries can target\nspecific properties of the final basis, such as achieving an even spreading of\nenergy. Our experimental evaluation compares seven different dictionaries on\nestablished benchmarks, showing that PCD is suited to target different\nshape-matching scenarios, resulting in more accurate point-wise maps than the\nLB basis when used in the same pipeline. This evidence provides a promising\nalternative for improving correspondence estimation, confirming the power and\nflexibility of functional maps.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.10332v1"
    },
    {
        "title": "ProSpect: Prompt Spectrum for Attribute-Aware Personalization of\n  Diffusion Models",
        "authors": [
            "Yuxin Zhang",
            "Weiming Dong",
            "Fan Tang",
            "Nisha Huang",
            "Haibin Huang",
            "Chongyang Ma",
            "Tong-Yee Lee",
            "Oliver Deussen",
            "Changsheng Xu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Personalizing generative models offers a way to guide image generation with\nuser-provided references. Current personalization methods can invert an object\nor concept into the textual conditioning space and compose new natural\nsentences for text-to-image diffusion models. However, representing and editing\nspecific visual attributes such as material, style, and layout remains a\nchallenge, leading to a lack of disentanglement and editability. To address\nthis problem, we propose a novel approach that leverages the step-by-step\ngeneration process of diffusion models, which generate images from low to high\nfrequency information, providing a new perspective on representing, generating,\nand editing images. We develop the Prompt Spectrum Space P*, an expanded\ntextual conditioning space, and a new image representation method called\n\\sysname. ProSpect represents an image as a collection of inverted textual\ntoken embeddings encoded from per-stage prompts, where each prompt corresponds\nto a specific generation stage (i.e., a group of consecutive steps) of the\ndiffusion model. Experimental results demonstrate that P* and ProSpect offer\nbetter disentanglement and controllability compared to existing methods. We\napply ProSpect in various personalized attribute-aware image generation\napplications, such as image-guided or text-driven manipulations of materials,\nstyle, and layout, achieving previously unattainable results from a single\nimage input without fine-tuning the diffusion models. Our source code is\navailable athttps://github.com/zyxElsa/ProSpect.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.16225v3"
    },
    {
        "title": "Acting as Inverse Inverse Planning",
        "authors": [
            "Kartik Chandra",
            "Tzu-Mao Li",
            "Josh Tenenbaum",
            "Jonathan Ragan-Kelley"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Great storytellers know how to take us on a journey. They direct characters\nto act -- not necessarily in the most rational way -- but rather in a way that\nleads to interesting situations, and ultimately creates an impactful experience\nfor audience members looking on.\n  If audience experience is what matters most, then can we help artists and\nanimators *directly* craft such experiences, independent of the concrete\ncharacter actions needed to evoke those experiences? In this paper, we offer a\nnovel computational framework for such tools. Our key idea is to optimize\nanimations with respect to *simulated* audience members' experiences. To\nsimulate the audience, we borrow an established principle from cognitive\nscience: that human social intuition can be modeled as \"inverse planning,\" the\ntask of inferring an agent's (hidden) goals from its (observed) actions.\nBuilding on this model, we treat storytelling as \"*inverse* inverse planning,\"\nthe task of choosing actions to manipulate an inverse planner's inferences. Our\nframework is grounded in literary theory, naturally capturing many storytelling\nelements from first principles. We give a series of examples to demonstrate\nthis, with supporting evidence from human subject studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.16913v1"
    },
    {
        "title": "Toward Understanding Display Size for FPS Esports Aiming",
        "authors": [
            "Josef Spjut",
            "Arjun Madhusudan",
            "Benjamin Watson",
            "Seth Schneider",
            "Ben Boudaoud",
            "Joohwan Kim"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Gamers use a variety of different display sizes, though for PC gaming in\nparticular, monitors in the 24 to 27 inch size range have become most popular.\nParticularly popular among many PC gamers, first person shooter (FPS) games\nrepresent a genre where hand-eye coordination is particularly central to the\nplayer's performance in game. In a carefully designed pair of experiments on\nFPS aiming, we compare player performance across a range of display sizes.\nFirst, we compare 12.5 inch, 17.3 inch and 24 inch monitors on a multi-target\nelimination task. Secondly, we highlight the differences between 24.5 inch and\n27 inch displays with a small target experiment, specifically designed to\namplify these small changes. We find a small, but statistically significant\nimprovement from the larger monitor sizes, which is likely a combined effect\nbetween monitor size, resolution, and the player's natural viewing distance.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.16953v1"
    },
    {
        "title": "Random-Access Neural Compression of Material Textures",
        "authors": [
            "Karthik Vaidyanathan",
            "Marco Salvi",
            "Bartlomiej Wronski",
            "Tomas Akenine-Möller",
            "Pontus Ebelin",
            "Aaron Lefohn"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The continuous advancement of photorealism in rendering is accompanied by a\ngrowth in texture data and, consequently, increasing storage and memory\ndemands. To address this issue, we propose a novel neural compression technique\nspecifically designed for material textures. We unlock two more levels of\ndetail, i.e., 16x more texels, using low bitrate compression, with image\nquality that is better than advanced image compression techniques, such as AVIF\nand JPEG XL. At the same time, our method allows on-demand, real-time\ndecompression with random access similar to block texture compression on GPUs,\nenabling compression on disk and memory. The key idea behind our approach is\ncompressing multiple material textures and their mipmap chains together, and\nusing a small neural network, that is optimized for each material, to\ndecompress them. Finally, we use a custom training implementation to achieve\npractical compression speeds, whose performance surpasses that of general\nframeworks, like PyTorch, by an order of magnitude.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.17105v1"
    },
    {
        "title": "EvIcon: Designing High-Usability Icon with Human-in-the-loop Exploration\n  and IconCLIP",
        "authors": [
            "I-Chao Shen",
            "Fu-Yin Cherng",
            "Takeo Igarashi",
            "Wen-Chieh Lin",
            "Bing-Yu Chen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Interface icons are prevalent in various digital applications. Due to limited\ntime and budgets, many designers rely on informal evaluation, which often\nresults in poor usability icons. In this paper, we propose a unique\nhuman-in-the-loop framework that allows our target users, i.e., novice and\nprofessional UI designers, to improve the usability of interface icons\nefficiently. We formulate several usability criteria into a perceptual\nusability function and enable users to iteratively revise an icon set with an\ninteractive design tool, EvIcon. We take a large-scale pre-trained joint\nimage-text embedding (CLIP) and fine-tune it to embed icon visuals with icon\ntags in the same embedding space (IconCLIP). During the revision process, our\ndesign tool provides two types of instant perceptual usability feedback. First,\nwe provide perceptual usability feedback modeled by deep learning models\ntrained on IconCLIP embeddings and crowdsourced perceptual ratings. Second, we\nuse the embedding space of IconCLIP to assist users in improving icons' visual\ndistinguishability among icons within the user-prepared icon set. To provide\nthe perceptual prediction, we compiled IconCEPT10K, the first large-scale\ndataset of perceptual usability ratings over $10,000$ interface icons, by\nconducting a crowdsourcing study. We demonstrated that our framework could\nbenefit UI designers' interface icon revision process with a wide range of\nprofessional experience. Moreover, the interface icons designed using our\nframework achieved better semantic distance and familiarity, verified by an\nadditional online user study.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.17609v1"
    },
    {
        "title": "CTSN: Predicting Cloth Deformation for Skeleton-based Characters with a\n  Two-stream Skinning Network",
        "authors": [
            "Yudi Li",
            "Min Tang",
            "Yun Yang",
            "Ruofeng Tong",
            "Shuangcai Yang",
            "Yao Li",
            "Bailin An",
            "Qilong Kou"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a novel learning method to predict the cloth deformation for\nskeleton-based characters with a two-stream network. The characters processed\nin our approach are not limited to humans, and can be other skeletal-based\nrepresentations of non-human targets such as fish or pets. We use a novel\nnetwork architecture which consists of skeleton-based and mesh-based residual\nnetworks to learn the coarse and wrinkle features as the overall residual from\nthe template cloth mesh. Our network is used to predict the deformation for\nloose or tight-fitting clothing or dresses. We ensure that the memory footprint\nof our network is low, and thereby result in reduced storage and computational\nrequirements. In practice, our prediction for a single cloth mesh for the\nskeleton-based character takes about 7 milliseconds on an NVIDIA GeForce RTX\n3090 GPU. Compared with prior methods, our network can generate fine\ndeformation results with details and wrinkles.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.18808v1"
    },
    {
        "title": "Simulation and Retargeting of Complex Multi-Character Interactions",
        "authors": [
            "Yunbo Zhang",
            "Deepak Gopinath",
            "Yuting Ye",
            "Jessica Hodgins",
            "Greg Turk",
            "Jungdam Won"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a method for reproducing complex multi-character interactions for\nphysically simulated humanoid characters using deep reinforcement learning. Our\nmethod learns control policies for characters that imitate not only individual\nmotions, but also the interactions between characters, while maintaining\nbalance and matching the complexity of reference data. Our approach uses a\nnovel reward formulation based on an interaction graph that measures distances\nbetween pairs of interaction landmarks. This reward encourages control policies\nto efficiently imitate the character's motion while preserving the spatial\nrelationships of the interactions in the reference motion. We evaluate our\nmethod on a variety of activities, from simple interactions such as a high-five\ngreeting to more complex interactions such as gymnastic exercises, Salsa\ndancing, and box carrying and throwing. This approach can be used to\n``clean-up'' existing motion capture data to produce physically plausible\ninteractions or to retarget motion to new characters with different sizes,\nkinematics or morphologies while maintaining the interactions in the original\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.20041v1"
    },
    {
        "title": "Example-based Motion Synthesis via Generative Motion Matching",
        "authors": [
            "Weiyu Li",
            "Xuelin Chen",
            "Peizhuo Li",
            "Olga Sorkine-Hornung",
            "Baoquan Chen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present GenMM, a generative model that \"mines\" as many diverse motions as\npossible from a single or few example sequences. In stark contrast to existing\ndata-driven methods, which typically require long offline training time, are\nprone to visual artifacts, and tend to fail on large and complex skeletons,\nGenMM inherits the training-free nature and the superior quality of the\nwell-known Motion Matching method. GenMM can synthesize a high-quality motion\nwithin a fraction of a second, even with highly complex and large skeletal\nstructures. At the heart of our generative framework lies the generative motion\nmatching module, which utilizes the bidirectional visual similarity as a\ngenerative cost function to motion matching, and operates in a multi-stage\nframework to progressively refine a random guess using exemplar motion matches.\nIn addition to diverse motion generation, we show the versatility of our\ngenerative framework by extending it to a number of scenarios that are not\npossible with motion matching alone, including motion completion, key\nframe-guided generation, infinite looping, and motion reassembly. Code and data\nfor this paper are at https://wyysf-98.github.io/GenMM/\n",
        "pdf_link": "http://arxiv.org/pdf/2306.00378v1"
    },
    {
        "title": "The Influence of Variable Frame Timing on First-Person Gaming",
        "authors": [
            "Devi Klein",
            "Josef Spjut",
            "Ben Boudaoud",
            "Joohwan Kim"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Variable frame timing (VFT), or changes in the time intervals between\ndiscrete frame images displayed to users, deviates from our traditional\nconceptualization of frame rate in which all frame times are equal. With the\nadvent of variable refresh rate (VRR) monitor technologies, gamers experience\nVFT at the display. VRR, coupled with increased display refresh rates and\nhigh-end hardware, enables smoother variation of frame presentation sequences.\nWe assess the effects of VFT on the perception of smoothness (experiment 1) and\nperformance (experiment 2) in first-person shooter (FPS) gameplay by\nintroducing frequent but relatively small (4-12 ms) variations in frame time\naround typical refresh rates (30-240 Hz). Our results indicate that VFT impacts\nthe perception of smoothness. However, the results from experiment 2 do not\nindicate differences in FPS task performance (i.e., completion time) between\nvariable and constant frame time sequences ranked equally smooth in experiment\n1.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.01691v1"
    },
    {
        "title": "Computational Design of Passive Grippers",
        "authors": [
            "Milin Kodnongbua",
            "Ian Good Yu Lou",
            "Jeffrey Lipton",
            "Adriana Schulz"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This work proposes a novel generative design tool for passive grippers --\nrobot end effectors that have no additional actuation and instead leverage the\nexisting degrees of freedom in a robotic arm to perform grasping tasks. Passive\ngrippers are used because they offer interesting trade-offs between cost and\ncapabilities. However, existing designs are limited in the types of shapes that\ncan be grasped. This work proposes to use rapid-manufacturing and design\noptimization to expand the space of shapes that can be passively grasped. Our\nnovel generative design algorithm takes in an object and its positioning with\nrespect to a robotic arm and generates a 3D printable passive gripper that can\nstably pick the object up. To achieve this, we address the key challenge of\njointly optimizing the shape and the insert trajectory to ensure a passively\nstable grasp. We evaluate our method on a testing suite of 22 objects (23\nexperiments), all of which were evaluated with physical experiments to bridge\nthe virtual-to-real gap. Code and data are at\nhttps://homes.cs.washington.edu/~milink/passive-gripper/\n",
        "pdf_link": "http://arxiv.org/pdf/2306.03174v1"
    },
    {
        "title": "Bidirectional GaitNet: A Bidirectional Prediction Model of Human Gait\n  and Anatomical Conditions",
        "authors": [
            "Jungnam Park",
            "Moon Seok Park",
            "Jehee Lee",
            "Jungdam Won"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a novel generative model, called Bidirectional GaitNet, that\nlearns the relationship between human anatomy and its gait. The simulation\nmodel of human anatomy is a comprehensive, full-body, simulation-ready,\nmusculoskeletal model with 304 Hill-type musculotendon units. The Bidirectional\nGaitNet consists of forward and backward models. The forward model predicts a\ngait pattern of a person with specific physical conditions, while the backward\nmodel estimates the physical conditions of a person when his/her gait pattern\nis provided. Our simulation-based approach first learns the forward model by\ndistilling the simulation data generated by a state-of-the-art predictive gait\nsimulator and then constructs a Variational Autoencoder (VAE) with the learned\nforward model as its decoder. Once it is learned its encoder serves as the\nbackward model. We demonstrate our model on a variety of healthy/impaired gaits\nand validate it in comparison with physical examination data of real patients.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.04161v1"
    },
    {
        "title": "Analysis and Synthesis of Digital Dyadic Sequences",
        "authors": [
            "Abdalla G. M. Ahmed",
            "Mikhail Skopenkov",
            "Markus Hadwiger",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We explore the space of matrix-generated (0, m, 2)-nets and (0, 2)-sequences\nin base 2, also known as digital dyadic nets and sequences. In computer\ngraphics, they are arguably leading the competition for use in rendering. We\nprovide a complete characterization of the design space and count the possible\nnumber of constructions with and without considering possible reorderings of\nthe point set. Based on this analysis, we then show that every digital dyadic\nnet can be reordered into a sequence, together with a corresponding algorithm.\nFinally, we present a novel family of self-similar digital dyadic sequences, to\nbe named $\\xi$-sequences, that spans a subspace with fewer degrees of freedom.\nThose $\\xi$-sequences are extremely efficient to sample and compute, and we\ndemonstrate their advantages over the classic Sobol (0, 2)-sequence.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.06925v2"
    },
    {
        "title": "Pose-aware Attention Network for Flexible Motion Retargeting by Body\n  Part",
        "authors": [
            "Lei Hu",
            "Zihao Zhang",
            "Chongyang Zhong",
            "Boyuan Jiang",
            "Shihong Xia"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Motion retargeting is a fundamental problem in computer graphics and computer\nvision. Existing approaches usually have many strict requirements, such as the\nsource-target skeletons needing to have the same number of joints or share the\nsame topology. To tackle this problem, we note that skeletons with different\nstructure may have some common body parts despite the differences in joint\nnumbers. Following this observation, we propose a novel, flexible motion\nretargeting framework. The key idea of our method is to regard the body part as\nthe basic retargeting unit rather than directly retargeting the whole body\nmotion. To enhance the spatial modeling capability of the motion encoder, we\nintroduce a pose-aware attention network (PAN) in the motion encoding phase.\nThe PAN is pose-aware since it can dynamically predict the joint weights within\neach body part based on the input pose, and then construct a shared latent\nspace for each body part by feature pooling. Extensive experiments show that\nour approach can generate better motion retargeting results both qualitatively\nand quantitatively than state-of-the-art methods. Moreover, we also show that\nour framework can generate reasonable results even for a more challenging\nretargeting scenario, like retargeting between bipedal and quadrupedal\nskeletons because of the body part retargeting strategy and PAN. Our code is\npublicly available.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.08006v1"
    },
    {
        "title": "Ergonomic-Centric Holography: Optimizing Realism,Immersion, and Comfort\n  for Holographic Display",
        "authors": [
            "Liang Shi",
            "DongHun Ryu",
            "Wojciech Matusik"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce ergonomic-centric holography, an algorithmic framework that\nsimultaneously optimizes for realistic incoherent defocus, unrestricted pupil\nmovements in the eye box, and high-order diffractions for filtering-free\nholography. The proposed method outperforms prior algorithms on holographic\ndisplay prototypes operating in unfiltered and pupil-mimicking modes, offering\nthe potential to enhance next-generation virtual and augmented reality\nexperiences.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.08138v2"
    },
    {
        "title": "UniSG^GA: A 3D scenegraph powered by Geometric Algebra unifying\n  geometry, behavior and GNNs towards generative AI",
        "authors": [
            "Manos Kamarianakis",
            "Antonis Protopsaltis",
            "Dimitris Angelis",
            "Paul Zikas",
            "Mike Kentros",
            "George Papagiannakis"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This work presents the introduction of UniSG^GA, a novel integrated\nscenegraph structure, that to incorporates behavior and geometry data on a 3D\nscene. It is specifically designed to seamlessly integrate Graph Neural\nNetworks (GNNs) and address the challenges associated with transforming a 3D\nscenegraph (3D-SG) during generative tasks. To effectively capture and preserve\nthe topological relationships between objects in a simplified way, within the\ngraph representation, we propose UniSG^GA, that seamlessly integrates Geometric\nAlgebra (GA) forms. This novel approach enhances the overall performance and\ncapability of GNNs in handling generative and predictive tasks, opening up new\npossibilities and aiming to lay the foundation for further exploration and\ndevelopment of graph-based generative AI models that can effectively\nincorporate behavior data for enhanced scene generation and synthesis.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.10621v1"
    },
    {
        "title": "Neural ShDF: Reviving an Efficient and Consistent Mesh Segmentation\n  Method",
        "authors": [
            "Bruno Roy"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Partitioning a polygonal mesh into meaningful parts can be challenging. Many\napplications require decomposing such structures for further processing in\ncomputer graphics. In the last decade, several methods were proposed to tackle\nthis problem, at the cost of intensive computational times. Recently, machine\nlearning has proven to be effective for the segmentation task on 3D structures.\nNevertheless, these state-of-the-art methods are often hardly generalizable and\nrequire dividing the learned model into several specific classes of objects to\navoid overfitting. We present a data-driven approach leveraging deep learning\nto encode a mapping function prior to mesh segmentation for multiple\napplications. Our network reproduces a neighborhood map using our knowledge of\nthe \\textsl{Shape Diameter Function} (SDF) method using similarities among\nvertex neighborhoods. Our approach is resolution-agnostic as we downsample the\ninput meshes and query the full-resolution structure solely for neighborhood\ncontributions. Using our predicted SDF values, we can inject the resulting\nstructure into a graph-cut algorithm to generate an efficient and robust mesh\nsegmentation while considerably reducing the required computation times.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.11737v2"
    },
    {
        "title": "Generating Parametric BRDFs from Natural Language Descriptions",
        "authors": [
            "Sean Memery",
            "Osmar Cedron",
            "Kartic Subr"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Artistic authoring of 3D environments is a laborious enterprise that also\nrequires skilled content creators. There have been impressive improvements in\nusing machine learning to address different aspects of generating 3D content,\nsuch as generating meshes, arranging geometry, synthesizing textures, etc. In\nthis paper we develop a model to generate Bidirectional Reflectance\nDistribution Functions (BRDFs) from descriptive textual prompts. BRDFs are four\ndimensional probability distributions that characterize the interaction of\nlight with surface materials. They are either represented parametrically, or by\ntabulating the probability density associated with every pair of incident and\noutgoing angles. The former lends itself to artistic editing while the latter\nis used when measuring the appearance of real materials. Numerous works have\nfocused on hypothesizing BRDF models from images of materials. We learn a\nmapping from textual descriptions of materials to parametric BRDFs. Our model\nis first trained using a semi-supervised approach before being tuned via an\nunsupervised scheme. Although our model is general, in this paper we\nspecifically generate parameters for MDL materials, conditioned on natural\nlanguage descriptions, within NVIDIA's Omniverse platform. This enables use\ncases such as real-time text prompts to change materials of objects in 3D\nenvironments such as \"dull plastic\" or \"shiny iron\". Since the output of our\nmodel is a parametric BRDF, rather than an image of the material, it may be\nused to render materials using any shape under arbitrarily specified viewing\nand lighting conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.15679v2"
    },
    {
        "title": "Fast non-iterative algorithm for 3D point-cloud holography",
        "authors": [
            "Nathan Tessema Ersaro",
            "Cem Yalcin",
            "Liz Murray",
            "Leyla Kabuli",
            "Laura Waller",
            "Rikky Muller"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Recently developed iterative and deep learning-based approaches to\ncomputer-generated holography (CGH) have been shown to achieve high-quality\nphotorealistic 3D images with spatial light modulators. However, such\napproaches remain overly cumbersome for patterning sparse collections of target\npoints across a photoresponsive volume in applications including biological\nmicroscopy and material processing. Specifically, in addition to requiring\nheavy computation that cannot accommodate real-time operation in mobile or\nhardware-light settings, existing sampling-dependent 3D CGH methods preclude\nthe ability to place target points with arbitrary precision, limiting\naccessible depths to a handful of planes. Accordingly, we present a\nnon-iterative point cloud holography algorithm that employs fast deterministic\ncalculations in order to efficiently allocate patches of SLM pixels to\ndifferent target points in the 3D volume and spread the patterning of all\npoints across multiple time frames. Compared to a matched-performance\nimplementation of the iterative Gerchberg-Saxton algorithm, our algorithm's\nrelative computation speed advantage was found to increase with SLM pixel\ncount, exceeding 100,000x at 512x512 array format.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.15682v2"
    },
    {
        "title": "Tensorformer: Normalized Matrix Attention Transformer for High-quality\n  Point Cloud Reconstruction",
        "authors": [
            "Hui Tian",
            "Zheng Qin",
            "Renjiao Yi",
            "Chenyang Zhu",
            "Kai Xu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Surface reconstruction from raw point clouds has been studied for decades in\nthe computer graphics community, which is highly demanded by modeling and\nrendering applications nowadays. Classic solutions, such as Poisson surface\nreconstruction, require point normals as extra input to perform reasonable\nresults. Modern transformer-based methods can work without normals, while the\nresults are less fine-grained due to limited encoding performance in local\nfusion from discrete points. We introduce a novel normalized matrix attention\ntransformer (Tensorformer) to perform high-quality reconstruction. The proposed\nmatrix attention allows for simultaneous point-wise and channel-wise message\npassing, while the previous vector attention loses neighbor point information\nacross different channels. It brings more degree of freedom in feature learning\nand thus facilitates better modeling of local geometries. Our method achieves\nstate-of-the-art on two commonly used datasets, ShapeNetCore and ABC, and\nattains 4% improvements on IOU on ShapeNet. Code can be accessed\nhttps://github.com/THHHomas/Tensorformer6.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.15989v3"
    },
    {
        "title": "Neural directional distance field object representation for\n  uni-directional path-traced rendering",
        "authors": [
            "Annada Prasad Behera",
            "Subhankar Mishra"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Faster rendering of synthetic images is a core problem in the field of\ncomputer graphics. Rendering algorithms, such as path-tracing is dependent on\nparameters like size of the image, number of light bounces, number of samples\nper pixel, all of which, are fixed if one wants to obtain a image of a desired\nquality. It is also dependent on the size and complexity of the scene being\nrendered. One of the largest bottleneck in rendering, particularly when the\nscene is very large, is querying for objects in the path of a given ray in the\nscene. By changing the data type that represents the objects in the scene, one\nmay reduce render time, however, a different representation of a scene requires\nthe modification of the rendering algorithm. In this paper, (a) we introduce\ndirected distance field, as a functional representation of a object; (b) how\nthe directed distance functions, when stored as a neural network, be optimized\nand; (c) how such an object can be rendered with a modified path-tracing\nalgorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.16142v1"
    },
    {
        "title": "Content-Preserving Diffusion Model for Unsupervised AS-OCT image\n  Despeckling",
        "authors": [
            "Li Sanqian",
            "Higashita Risa",
            "Fu Huazhu",
            "Li Heng",
            "Niu Jingxuan",
            "Liu Jiang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Anterior segment optical coherence tomography (AS-OCT) is a non-invasive\nimaging technique that is highly valuable for ophthalmic diagnosis. However,\nspeckles in AS-OCT images can often degrade the image quality and affect\nclinical analysis. As a result, removing speckles in AS-OCT images can greatly\nbenefit automatic ophthalmology analysis. Unfortunately, challenges still exist\nin deploying effective AS-OCT image denoising algorithms, including collecting\nsufficient paired training data and the requirement to preserve consistent\ncontent in medical images. To address these practical issues, we propose an\nunsupervised AS-OCT despeckling algorithm via Content Preserving Diffusion\nModel (CPDM) with statistical knowledge. At the training stage, a Markov chain\ntransforms clean images to white Gaussian noise by repeatedly adding random\nnoise and removes the predicted noise in a reverse procedure. At the inference\nstage, we first analyze the statistical distribution of speckles and convert it\ninto a Gaussian distribution, aiming to match the fast truncated reverse\ndiffusion process. We then explore the posterior distribution of observed\nimages as a fidelity term to ensure content consistency in the iterative\nprocedure. Our experimental results show that CPDM significantly improves image\nquality compared to competitive methods. Furthermore, we validate the benefits\nof CPDM for subsequent clinical analysis, including ciliary muscle (CM)\nsegmentation and scleral spur (SS) localization.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.17717v1"
    },
    {
        "title": "MARF: The Medial Atom Ray Field Object Representation",
        "authors": [
            "Peder Bergebakken Sundt",
            "Theoharis Theoharis"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose Medial Atom Ray Fields (MARFs), a novel neural object\nrepresentation that enables accurate differentiable surface rendering with a\nsingle network evaluation per camera ray. Existing neural ray fields struggle\nwith multi-view consistency and representing surface discontinuities. MARFs\naddress both using a medial shape representation, a dual representation of\nsolid geometry that yields cheap geometrically grounded surface normals, in\nturn enabling computing analytical curvature despite the network having no\nsecond derivative. MARFs map a camera ray to multiple medial intersection\ncandidates, subject to ray-sphere intersection testing. We illustrate how the\nlearned medial shape quantities applies to sub-surface scattering, part\nsegmentation, and aid representing a space of articulated shapes. Able to learn\na space of shape priors, MARFs may prove useful for tasks like shape retrieval\nand shape completion, among others. Code and data can be found at\nhttps://github.com/pbsds/MARF.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.00037v1"
    },
    {
        "title": "Neural Free-Viewpoint Relighting for Glossy Indirect Illumination",
        "authors": [
            "Nithin Raghavan",
            "Yan Xiao",
            "Kai-En Lin",
            "Tiancheng Sun",
            "Sai Bi",
            "Zexiang Xu",
            "Tzu-Mao Li",
            "Ravi Ramamoorthi"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Precomputed Radiance Transfer (PRT) remains an attractive solution for\nreal-time rendering of complex light transport effects such as glossy global\nillumination. After precomputation, we can relight the scene with new\nenvironment maps while changing viewpoint in real-time. However, practical PRT\nmethods are usually limited to low-frequency spherical harmonic lighting.\nAll-frequency techniques using wavelets are promising but have so far had\nlittle practical impact. The curse of dimensionality and much higher data\nrequirements have typically limited them to relighting with fixed view or only\ndirect lighting with triple product integrals. In this paper, we demonstrate a\nhybrid neural-wavelet PRT solution to high-frequency indirect illumination,\nincluding glossy reflection, for relighting with changing view. Specifically,\nwe seek to represent the light transport function in the Haar wavelet basis.\nFor global illumination, we learn the wavelet transport using a small\nmulti-layer perceptron (MLP) applied to a feature field as a function of\nspatial location and wavelet index, with reflected direction and material\nparameters being other MLP inputs. We optimize/learn the feature field\n(compactly represented by a tensor decomposition) and MLP parameters from\nmultiple images of the scene under different lighting and viewing conditions.\nWe demonstrate real-time (512 x 512 at 24 FPS, 800 x 600 at 13 FPS) precomputed\nrendering of challenging scenes involving view-dependent reflections and even\ncaustics.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.06335v1"
    },
    {
        "title": "FACTS: Facial Animation Creation using the Transfer of Styles",
        "authors": [
            "Jack Saunders",
            "Steven Caulkin",
            "Vinay Namboodiri"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The ability to accurately capture and express emotions is a critical aspect\nof creating believable characters in video games and other forms of\nentertainment. Traditionally, this animation has been achieved with artistic\neffort or performance capture, both requiring costs in time and labor. More\nrecently, audio-driven models have seen success, however, these often lack\nexpressiveness in areas not correlated to the audio signal. In this paper, we\npresent a novel approach to facial animation by taking existing animations and\nallowing for the modification of style characteristics. Specifically, we\nexplore the use of a StarGAN to enable the conversion of 3D facial animations\ninto different emotions and person-specific styles. We are able to maintain the\nlip-sync of the animations with this method thanks to the use of a novel\nviseme-preserving loss.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.09480v1"
    },
    {
        "title": "Magic NeRF Lens: Interactive Fusion of Neural Radiance Fields for\n  Virtual Facility Inspection",
        "authors": [
            "Ke Li",
            "Susanne Schmidt",
            "Tim Rolff",
            "Reinhard Bacher",
            "Wim Leemans",
            "Frank Steinicke"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Large industrial facilities such as particle accelerators and nuclear power\nplants are critical infrastructures for scientific research and industrial\nprocesses. These facilities are complex systems that not only require regular\nmaintenance and upgrades but are often inaccessible to humans due to various\nsafety hazards. Therefore, a virtual reality (VR) system that can quickly\nreplicate real-world remote environments to provide users with a high level of\nspatial and situational awareness is crucial for facility maintenance planning.\nHowever, the exact 3D shapes of these facilities are often too complex to be\naccurately modeled with geometric primitives through the traditional\nrasterization pipeline.\n  In this work, we develop Magic NeRF Lens, an interactive framework to support\nfacility inspection in immersive VR using neural radiance fields (NeRF) and\nvolumetric rendering. We introduce a novel data fusion approach that combines\nthe complementary strengths of volumetric rendering and geometric\nrasterization, allowing a NeRF model to be merged with other conventional 3D\ndata, such as a computer-aided design model. We develop two novel 3D magic lens\neffects to optimize NeRF rendering by exploiting the properties of human vision\nand context-aware visualization. We demonstrate the high usability of our\nframework and methods through a technical benchmark, a visual search user\nstudy, and expert reviews. In addition, the source code of our VR NeRF\nframework is made publicly available for future research and development.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.09860v1"
    },
    {
        "title": "A Hierarchical Architecture for Neural Materials",
        "authors": [
            "Bowen Xue",
            "Shuang Zhao",
            "Henrik Wann Jensen",
            "Zahra Montazeri"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Neural reflectance models are capable of reproducing the spatially-varying\nappearance of many real-world materials at different scales. Unfortunately,\nexisting techniques such as NeuMIP have difficulties handling materials with\nstrong shadowing effects or detailed specular highlights. In this paper, we\nintroduce a neural appearance model that offers a new level of accuracy.\nCentral to our model is an inception-based core network structure that captures\nmaterial appearances at multiple scales using parallel-operating kernels and\nensures multi-stage features through specialized convolution layers.\nFurthermore, we encode the inputs into frequency space, introduce a\ngradient-based loss, and employ it adaptive to the progress of the learning\nphase. We demonstrate the effectiveness of our method using a variety of\nsynthetic and real examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.10135v3"
    },
    {
        "title": "Visual Analytics for Understanding Draco's Knowledge Base",
        "authors": [
            "Johanna Schmidt",
            "Bernhard Pointner",
            "Silvia Miksch"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Draco has been developed as an automated visualization recommendation system\nformalizing design knowledge as logical constraints in ASP (Answer-Set\nProgramming). With an increasing set of constraints and incorporated design\nknowledge, even visualization experts lose overview in Draco and struggle to\nretrace the automated recommendation decisions made by the system. Our paper\nproposes an Visual Analytics (VA) approach to visualize and analyze Draco's\nconstraints. Our VA approach is supposed to enable visualization experts to\naccomplish identified tasks regarding the knowledge base and support them in\nbetter understanding Draco. We extend the existing data extraction strategy of\nDraco with a data processing architecture capable of extracting features of\ninterest from the knowledge base. A revised version of the ASP grammar provides\nthe basis for this data processing strategy. The resulting incorporated and\nshared features of the constraints are then visualized using a hypergraph\nstructure inside the radial-arranged constraints of the elaborated\nvisualization. The hierarchical categories of the constraints are indicated by\narcs surrounding the constraints. Our approach is supposed to enable\nvisualization experts to interactively explore the design rules' violations\nbased on highlighting respective constraints or recommendations. A qualitative\nand quantitative evaluation of the prototype confirms the prototype's\neffectiveness and value in acquiring insights into Draco's recommendation\nprocess and design constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.12866v1"
    },
    {
        "title": "Visual Analysis of Displacement Processes in Porous Media using\n  Spatio-Temporal Flow Graphs",
        "authors": [
            "Alexander Straub",
            "Nikolaos Karadimitriou",
            "Guido Reina",
            "Steffen Frey",
            "Holger Steeb",
            "Thomas Ertl"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We developed a new approach comprised of different visualizations for the\ncomparative spatio-temporal analysis of displacement processes in porous media.\nWe aim to analyze and compare ensemble datasets from experiments to gain\ninsight into the influence of different parameters on fluid flow. To capture\nthe displacement of a defending fluid by an invading fluid, we first condense\nan input image series to a single time map. From this map, we generate a\nspatio-temporal flow graph covering the whole process. This graph is further\nsimplified to only reflect topological changes in the movement of the invading\nfluid. Our interactive tools allow the visual analysis of these processes by\nvisualizing the graph structure and the context of the experimental setup, as\nwell as by providing charts for multiple metrics. We apply our approach to\nanalyze and compare ensemble datasets jointly with domain experts, where we\nvary either fluid properties or the solid structure of the porous medium. We\nfinally report the generated insights from the domain experts and discuss our\ncontribution's advantages, generality, and limitations.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14949v2"
    },
    {
        "title": "Mesh Density Adaptation for Template-based Shape Reconstruction",
        "authors": [
            "Yucheol Jung",
            "Hyomin Kim",
            "Gyeongha Hwang",
            "Seung-Hwan Baek",
            "Seungyong Lee"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In 3D shape reconstruction based on template mesh deformation, a\nregularization, such as smoothness energy, is employed to guide the\nreconstruction into a desirable direction. In this paper, we highlight an often\noverlooked property in the regularization: the vertex density in the mesh.\nWithout careful control on the density, the reconstruction may suffer from\nunder-sampling of vertices near shape details. We propose a novel mesh density\nadaptation method to resolve the under-sampling problem. Our mesh density\nadaptation energy increases the density of vertices near complex structures via\ndeformation to help reconstruction of shape details. We demonstrate the\nusability and performance of mesh density adaptation with two tasks, inverse\nrendering and non-rigid surface registration. Our method produces more accurate\nreconstruction results compared to the cases without mesh density adaptation.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.16205v1"
    },
    {
        "title": "Line Harp: Importance-Driven Sonification for Dense Line Charts",
        "authors": [
            "Egil Bru",
            "Thomas Trautner",
            "Stefan Bruckner"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Accessibility in visualization is an important yet challenging topic.\nSonification, in particular, is a valuable yet underutilized technique that can\nenhance accessibility for people with low vision. However, the lower bandwidth\nof the auditory channel makes it difficult to fully convey dense\nvisualizations. For this reason, interactivity is key in making full use of its\npotential. In this paper, we present a novel approach for the sonification of\ndense line charts. We utilize the metaphor of a string instrument, where\nindividual line segments can be \"plucked\". We propose an importance-driven\napproach which encodes the directionality of line segments using frequency and\ndynamically scales amplitude for improved density perception. We discuss the\npotential of our approach based on a set of examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.16589v1"
    },
    {
        "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
        "authors": [
            "Bernhard Kerbl",
            "Georgios Kopanas",
            "Thomas Leimkühler",
            "George Drettakis"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Radiance Field methods have recently revolutionized novel-view synthesis of\nscenes captured with multiple photos or videos. However, achieving high visual\nquality still requires neural networks that are costly to train and render,\nwhile recent faster methods inevitably trade off speed for quality. For\nunbounded and complete scenes (rather than isolated objects) and 1080p\nresolution rendering, no current method can achieve real-time display rates. We\nintroduce three key elements that allow us to achieve state-of-the-art visual\nquality while maintaining competitive training times and importantly allow\nhigh-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution.\nFirst, starting from sparse points produced during camera calibration, we\nrepresent the scene with 3D Gaussians that preserve desirable properties of\ncontinuous volumetric radiance fields for scene optimization while avoiding\nunnecessary computation in empty space; Second, we perform interleaved\noptimization/density control of the 3D Gaussians, notably optimizing\nanisotropic covariance to achieve an accurate representation of the scene;\nThird, we develop a fast visibility-aware rendering algorithm that supports\nanisotropic splatting and both accelerates training and allows realtime\nrendering. We demonstrate state-of-the-art visual quality and real-time\nrendering on several established datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.04079v1"
    },
    {
        "title": "A Novel 3D Mapping Representation and its Applications",
        "authors": [
            "Qiguang Chen",
            "Lok Ming Lui"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The analysis of mapping relationships and distortions in multidimensional\ndata poses a significant challenge in contemporary research. While Beltrami\ncoefficients offer a precise description of distortions in two-dimensional\nmappings, current tools lack this capability in the context of\nthree-dimensional space. This paper presents a novel approach: a 3D\nquasiconformal representation that captures the local dilation of 3D mappings,\nalong with a reconstruction algorithm that establishes a connection between\nthis representation and the corresponding mapping. Experimental results\nshowcase the algorithm's effectiveness in mapping reconstruction, keyframe\ninterpolation, and mapping compression. These features bear a resemblance to\nthe 2D Linear Beltrami Solver technique. The work presented in this paper\noffers a promising solution for the precise analysis and adjustment of\ndistortions in 3D data and mappings.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.05333v2"
    },
    {
        "title": "Evaluation of cinematic volume rendering open-source and commercial\n  solutions for the exploration of congenital heart data",
        "authors": [
            "Irum Baseer",
            "Israel Valverde",
            "Abdel H. Moustafa",
            "Josep Blat",
            "Oscar Camara"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Detailed anatomical information is essential to optimize medical decisions\nfor surgical and pre-operative planning in patients with congenital heart\ndisease. The visualization techniques commonly used in clinical routine for the\nexploration of complex cardiac data are based on multi-planar reformations,\nmaximum intensity projection, and volume rendering, which rely on basic\nlighting models prone to image distortion. On the other hand, cinematic\nrendering (CR), a three-dimensional visualization technique based on\nphysically-based rendering methods, can create volumetric images with high\nfidelity. However, there are a lot of parameters involved in CR that affect the\nvisualization results, thus being dependent on the user's experience and\nrequiring detailed evaluation protocols to compare available solutions. In this\nstudy, we have analyzed the impact of the most relevant parameters in a CR\npipeline developed in the open-source version of the MeVisLab framework for the\nvisualization of the heart anatomy of three congenital patients and two adults\nfrom CT images. The resulting visualizations were compared to a commercial tool\nused in the clinics with a questionnaire filled in by clinical users, providing\nsimilar definitions of structures, depth perception, texture appearance,\nrealism, and diagnostic ability.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.06510v1"
    },
    {
        "title": "Differentiable Shadow Mapping for Efficient Inverse Graphics",
        "authors": [
            "Markus Worchel",
            "Marc Alexa"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We show how shadows can be efficiently generated in differentiable rendering\nof triangle meshes. Our central observation is that pre-filtered shadow\nmapping, a technique for approximating shadows based on rendering from the\nperspective of a light, can be combined with existing differentiable\nrasterizers to yield differentiable visibility information. We demonstrate at\nseveral inverse graphics problems that differentiable shadow maps are orders of\nmagnitude faster than differentiable light transport simulation with similar\naccuracy -- while differentiable rasterization without shadows often fails to\nconverge.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.10896v1"
    },
    {
        "title": "Geo-Sketcher: Rapid 3D Geological Modeling using Geological and\n  Topographic Map Sketches",
        "authors": [
            "Ronan Amorim",
            "Emilio Vital Brazil",
            "Faramarz Samavati",
            "Mario Costa Sousa"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The construction of 3D geological models is an essential task in oil/gas\nexploration, development and production. However, it is a cumbersome,\ntime-consuming and error-prone task mainly because of the model's geometric and\ntopological complexity. The models construction is usually separated into\ninterpretation and 3D modeling, performed by different highly specialized\nindividuals, which leads to inconsistencies and intensifies the challenges. In\naddition, the creation of models following geological rules is paramount for\nproperly depicting static and dynamic properties of oil/gas reservoirs. In this\nwork, we propose a sketch-based approach to expedite the creation of valid 3D\ngeological models by mimicking how domain experts interpret geological\nstructures, allowing creating models directly from interpretation sketches. Our\nsketch-based modeler (Geo-Sketcher) is based on sketches of standard 2D\ntopographic and geological maps, comprised of lines, symbols and annotations.\nWe developed a graph-based representation to enable (1) the automatic\ncomputation of the relative ages of rock series and layers, and (2) the\nembedding of specific geological rules directly in the sketching. We introduce\nthe use of Hermite-Birkhoff Radial Basis Functions to interpolate the\ngeological map constraints, and demonstrate the capabilities of our approach\nwith a variety of results with different levels of complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.12152v1"
    },
    {
        "title": "A Visualization System for Hexahedral Mesh Quality Study",
        "authors": [
            "Lei Si",
            "Guoning Chen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we introduce a new 3D hex mesh visual analysis system that\nemphasizes poor-quality areas with an aggregated glyph, highlights overlapping\nelements, and provides detailed boundary error inspection in three forms. By\nsupporting multi-level analysis through multiple views, our system effectively\nevaluates various mesh models and compares the performance of mesh generation\nand optimization algorithms for hexahedral meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.12158v2"
    },
    {
        "title": "NeuralClothSim: Neural Deformation Fields Meet the Thin Shell Theory",
        "authors": [
            "Navami Kairanda",
            "Marc Habermann",
            "Christian Theobalt",
            "Vladislav Golyanik"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Despite existing 3D cloth simulators producing realistic results, they\npredominantly operate on discrete surface representations (e.g. points and\nmeshes) with a fixed spatial resolution, which often leads to large memory\nconsumption and resolution-dependent simulations. Moreover, back-propagating\ngradients through the existing solvers is difficult, and they cannot be easily\nintegrated into modern neural architectures. In response, this paper re-thinks\nphysically plausible cloth simulation: We propose NeuralClothSim, i.e., a new\nquasistatic cloth simulator using thin shells, in which surface deformation is\nencoded in neural network weights in the form of a neural field. Our\nmemory-efficient solver operates on a new continuous coordinate-based surface\nrepresentation called neural deformation fields (NDFs); it supervises NDF\nequilibria with the laws of the non-linear Kirchhoff-Love shell theory with a\nnon-linear anisotropic material model. NDFs are adaptive: They 1) allocate\ntheir capacity to the deformation details and 2) allow surface state queries at\narbitrary spatial resolutions without re-training. We show how to train\nNeuralClothSim while imposing hard boundary conditions and demonstrate multiple\napplications, such as material interpolation and simulation editing. The\nexperimental results highlight the effectiveness of our continuous neural\nformulation. See our project page: https://4dqv.mpi-inf.mpg.de/NeuralClothSim/.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.12970v3"
    },
    {
        "title": "MagicAvatar: Multimodal Avatar Generation and Animation",
        "authors": [
            "Jianfeng Zhang",
            "Hanshu Yan",
            "Zhongcong Xu",
            "Jiashi Feng",
            "Jun Hao Liew"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This report presents MagicAvatar, a framework for multimodal video generation\nand animation of human avatars. Unlike most existing methods that generate\navatar-centric videos directly from multimodal inputs (e.g., text prompts),\nMagicAvatar explicitly disentangles avatar video generation into two stages:\n(1) multimodal-to-motion and (2) motion-to-video generation. The first stage\ntranslates the multimodal inputs into motion/ control signals (e.g., human\npose, depth, DensePose); while the second stage generates avatar-centric video\nguided by these motion signals. Additionally, MagicAvatar supports avatar\nanimation by simply providing a few images of the target person. This\ncapability enables the animation of the provided human identity according to\nthe specific motion derived from the first stage. We demonstrate the\nflexibility of MagicAvatar through various applications, including text-guided\nand video-guided avatar generation, as well as multimodal avatar animation.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.14748v1"
    },
    {
        "title": "Space Partitioning Schemes and Algorithms for Generating Regular and\n  Spiral Treemaps",
        "authors": [
            "Mehdi Behroozi",
            "Reyhaneh Mohammadi",
            "Cody Dunne"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Treemaps have been widely applied to the visualization of hierarchical data.\nA treemap takes a weighted tree and visualizes its leaves in a nested planar\ngeometric shape, with sub-regions partitioned such that each sub-region has an\narea proportional to the weight of its associated leaf nodes. Efficiently\ngenerating visually appealing treemaps that also satisfy other quality criteria\nis an interesting problem that has been tackled from many directions. We\npresent an optimization model and five new algorithms for this problem,\nincluding two divide and conquer approaches and three spiral treemap\nalgorithms. Our optimization model is able to generate superior treemaps that\ncould serve as a benchmark for comparing the quality of more computationally\nefficient algorithms. Our divide and conquer and spiral algorithms either\nimprove the performance of their existing counterparts with respect to aspect\nratio and stability or perform competitively. Our spiral algorithms also expand\ntheir applicability to a wider range of input scenarios. Four of these\nalgorithms are computationally efficient as well with quasilinear running times\nand the last algorithm achieves a cubic running time. A full version of this\npaper with all appendices, data, and source codes is available at\n\\anonymizeOSF{\\OSFSupplementText}.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.16855v1"
    },
    {
        "title": "A Locality-based Neural Solver for Optical Motion Capture",
        "authors": [
            "Xiaoyu Pan",
            "Bowen Zheng",
            "Xinwei Jiang",
            "Guanglong Xu",
            "Xianli Gu",
            "Jingxiang Li",
            "Qilong Kou",
            "He Wang",
            "Tianjia Shao",
            "Kun Zhou",
            "Xiaogang Jin"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a novel locality-based learning method for cleaning and solving\noptical motion capture data. Given noisy marker data, we propose a new\nheterogeneous graph neural network which treats markers and joints as different\ntypes of nodes, and uses graph convolution operations to extract the local\nfeatures of markers and joints and transform them to clean motions. To deal\nwith anomaly markers (e.g. occluded or with big tracking errors), the key\ninsight is that a marker's motion shows strong correlations with the motions of\nits immediate neighboring markers but less so with other markers, a.k.a.\nlocality, which enables us to efficiently fill missing markers (e.g. due to\nocclusion). Additionally, we also identify marker outliers due to tracking\nerrors by investigating their acceleration profiles. Finally, we propose a\ntraining regime based on representation learning and data augmentation, by\ntraining the model on data with masking. The masking schemes aim to mimic the\noccluded and noisy markers often observed in the real data. Finally, we show\nthat our method achieves high accuracy on multiple metrics across various\ndatasets. Extensive comparison shows our method outperforms state-of-the-art\nmethods in terms of prediction accuracy of occluded marker position error by\napproximately 20%, which leads to a further error reduction on the\nreconstructed joint rotations and positions by 30%. The code and data for this\npaper are available at https://github.com/non-void/LocalMoCap.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.00428v2"
    },
    {
        "title": "MOVIN: Real-time Motion Capture using a Single LiDAR",
        "authors": [
            "Deok-Kyeong Jang",
            "Dongseok Yang",
            "Deok-Yun Jang",
            "Byeoli Choi",
            "Taeil Jin",
            "Sung-Hee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Recent advancements in technology have brought forth new forms of interactive\napplications, such as the social metaverse, where end users interact with each\nother through their virtual avatars. In such applications, precise full-body\ntracking is essential for an immersive experience and a sense of embodiment\nwith the virtual avatar. However, current motion capture systems are not easily\naccessible to end users due to their high cost, the requirement for special\nskills to operate them, or the discomfort associated with wearable devices. In\nthis paper, we present MOVIN, the data-driven generative method for real-time\nmotion capture with global tracking, using a single LiDAR sensor. Our\nautoregressive conditional variational autoencoder (CVAE) model learns the\ndistribution of pose variations conditioned on the given 3D point cloud from\nLiDAR.As a central factor for high-accuracy motion capture, we propose a novel\nfeature encoder to learn the correlation between the historical 3D point cloud\ndata and global, local pose features, resulting in effective learning of the\npose prior. Global pose features include root translation, rotation, and foot\ncontacts, while local features comprise joint positions and rotations.\nSubsequently, a pose generator takes into account the sampled latent variable\nalong with the features from the previous frame to generate a plausible current\npose. Our framework accurately predicts the performer's 3D global information\nand local joint details while effectively considering temporally coherent\nmovements across frames. We demonstrate the effectiveness of our architecture\nthrough quantitative and qualitative evaluations, comparing it against\nstate-of-the-art methods. Additionally, we implement a real-time application to\nshowcase our method in real-world scenarios. MOVIN dataset is available at\n\\url{https://movin3d.github.io/movin_pg2023/}.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.09314v1"
    },
    {
        "title": "Digital analysis of early color photographs taken using regular color\n  screen processes",
        "authors": [
            "Jan Hubička",
            "Linda Kimrová",
            "Kenzie Klaeser",
            "Sara Manco",
            "Doug Peterson"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Some early color photographic processes based on special color screen filters\npose specific challenges in their digitization and digital presentation. Those\nchallenges include dynamic range, resolution, and the difficulty of stitching\ngeometrically-repeating patterns. We describe a novel method used to digitize\nthe collection of early color photographs at the National Geographic Society\nwhich makes use of a custom open-source software tool to analyze and precisely\nstitch regular color screen processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.09631v1"
    },
    {
        "title": "Speculative Progressive Raycasting for Memory Constrained Isosurface\n  Visualization of Massive Volumes",
        "authors": [
            "Will Usher",
            "Landon Dyken",
            "Sidharth Kumar"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  New web technologies have enabled the deployment of powerful GPU-based\ncomputational pipelines that run entirely in the web browser, opening a new\nfrontier for accessible scientific visualization applications. However, these\nnew capabilities do not address the memory constraints of lightweight end-user\ndevices encountered when attempting to visualize the massive data sets produced\nby today's simulations and data acquisition systems. In this paper, we propose\na novel implicit isosurface rendering algorithm for interactive visualization\nof massive volumes within a small memory footprint. We achieve this by\nprogressively traversing a wavefront of rays through the volume and\ndecompressing blocks of the data on-demand to perform implicit ray-isosurface\nintersections. The progressively rendered surface is displayed after each pass\nto improve interactivity. Furthermore, to accelerate rendering and increase GPU\nutilization, we introduce speculative ray-block intersection into our\nalgorithm, where additional blocks are traversed and intersected speculatively\nalong rays as other rays terminate to exploit additional parallelism in the\nworkload. Our entire pipeline is run in parallel on the GPU to leverage the\nparallel computing power that is available even on lightweight end-user\ndevices. We compare our algorithm to the state of the art in low-overhead\nisosurface extraction and demonstrate that it achieves 1.7x-5.7x reductions in\nmemory overhead and up to 8.4x reductions in data decompressed.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.10212v1"
    },
    {
        "title": "Neural Metamaterial Networks for Nonlinear Material Design",
        "authors": [
            "Yue Li",
            "Stelian Coros",
            "Bernhard Thomaszewski"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Nonlinear metamaterials with tailored mechanical properties have applications\nin engineering, medicine, robotics, and beyond. While modeling their\nmacromechanical behavior is challenging in itself, finding structure parameters\nthat lead to ideal approximation of high-level performance goals is a\nchallenging task. In this work, we propose Neural Metamaterial Networks (NMN)\n-- smooth neural representations that encode the nonlinear mechanics of entire\nmetamaterial families. Given structure parameters as input, NMN return\ncontinuously differentiable strain energy density functions, thus guaranteeing\nconservative forces by construction. Though trained on simulation data, NMN do\nnot inherit the discontinuities resulting from topological changes in finite\nelement meshes. They instead provide a smooth map from parameter to performance\nspace that is fully differentiable and thus well-suited for gradient-based\noptimization. On this basis, we formulate inverse material design as a\nnonlinear programming problem that leverages neural networks for both objective\nfunctions and constraints. We use this approach to automatically design\nmaterials with desired strain-stress curves, prescribed directional stiffness\nand Poisson ratio profiles. We furthermore conduct ablation studies on network\nnonlinearities and show the advantages of our approach compared to native-scale\noptimization.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.10600v1"
    },
    {
        "title": "TwinTex: Geometry-aware Texture Generation for Abstracted 3D\n  Architectural Models",
        "authors": [
            "Weidan Xiong",
            "Hongqian Zhang",
            "Botao Peng",
            "Ziyu Hu",
            "Yongli Wu",
            "Jianwei Guo",
            "Hui Huang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Coarse architectural models are often generated at scales ranging from\nindividual buildings to scenes for downstream applications such as Digital Twin\nCity, Metaverse, LODs, etc. Such piece-wise planar models can be abstracted as\ntwins from 3D dense reconstructions. However, these models typically lack\nrealistic texture relative to the real building or scene, making them\nunsuitable for vivid display or direct reference. In this paper, we present\nTwinTex, the first automatic texture mapping framework to generate a\nphoto-realistic texture for a piece-wise planar proxy. Our method addresses\nmost challenges occurring in such twin texture generation. Specifically, for\neach primitive plane, we first select a small set of photos with greedy\nheuristics considering photometric quality, perspective quality and facade\ntexture completeness. Then, different levels of line features (LoLs) are\nextracted from the set of selected photos to generate guidance for later steps.\nWith LoLs, we employ optimization algorithms to align texture with geometry\nfrom local to global. Finally, we fine-tune a diffusion model with a multi-mask\ninitialization component and a new dataset to inpaint the missing region.\nExperimental results on many buildings, indoor scenes and man-made objects of\nvarying complexity demonstrate the generalization ability of our algorithm. Our\napproach surpasses state-of-the-art texture mapping methods in terms of\nhigh-fidelity quality and reaches a human-expert production level with much\nless effort. Project page: https://vcc.tech/research/2023/TwinTex.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.11258v1"
    },
    {
        "title": "Neural Stochastic Screened Poisson Reconstruction",
        "authors": [
            "Silvia Sellán",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Reconstructing a surface from a point cloud is an underdetermined problem. We\nuse a neural network to study and quantify this reconstruction uncertainty\nunder a Poisson smoothness prior. Our algorithm addresses the main limitations\nof existing work and can be fully integrated into the 3D scanning pipeline,\nfrom obtaining an initial reconstruction to deciding on the next best sensor\nposition and updating the reconstruction upon capturing more data.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.11993v1"
    },
    {
        "title": "DROP: Dynamics Responses from Human Motion Prior and Projective Dynamics",
        "authors": [
            "Yifeng Jiang",
            "Jungdam Won",
            "Yuting Ye",
            "C. Karen Liu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Synthesizing realistic human movements, dynamically responsive to the\nenvironment, is a long-standing objective in character animation, with\napplications in computer vision, sports, and healthcare, for motion prediction\nand data augmentation. Recent kinematics-based generative motion models offer\nimpressive scalability in modeling extensive motion data, albeit without an\ninterface to reason about and interact with physics. While\nsimulator-in-the-loop learning approaches enable highly physically realistic\nbehaviors, the challenges in training often affect scalability and adoption. We\nintroduce DROP, a novel framework for modeling Dynamics Responses of humans\nusing generative mOtion prior and Projective dynamics. DROP can be viewed as a\nhighly stable, minimalist physics-based human simulator that interfaces with a\nkinematics-based generative motion prior. Utilizing projective dynamics, DROP\nallows flexible and simple integration of the learned motion prior as one of\nthe projective energies, seamlessly incorporating control provided by the\nmotion prior with Newtonian dynamics. Serving as a model-agnostic plug-in, DROP\nenables us to fully leverage recent advances in generative motion models for\nphysics-based motion synthesis. We conduct extensive evaluations of our model\nacross different motion tasks and various physical perturbations, demonstrating\nthe scalability and diversity of responses.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.13742v1"
    },
    {
        "title": "Diffusion-based Holistic Texture Rectification and Synthesis",
        "authors": [
            "Guoqing Hao",
            "Satoshi Iizuka",
            "Kensho Hara",
            "Edgar Simo-Serra",
            "Hirokatsu Kataoka",
            "Kazuhiro Fukui"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a novel framework for rectifying occlusions and distortions in\ndegraded texture samples from natural images. Traditional texture synthesis\napproaches focus on generating textures from pristine samples, which\nnecessitate meticulous preparation by humans and are often unattainable in most\nnatural images. These challenges stem from the frequent occlusions and\ndistortions of texture samples in natural images due to obstructions and\nvariations in object surface geometry. To address these issues, we propose a\nframework that synthesizes holistic textures from degraded samples in natural\nimages, extending the applicability of exemplar-based texture synthesis\ntechniques. Our framework utilizes a conditional Latent Diffusion Model (LDM)\nwith a novel occlusion-aware latent transformer. This latent transformer not\nonly effectively encodes texture features from partially-observed samples\nnecessary for the generation process of the LDM, but also explicitly captures\nlong-range dependencies in samples with large occlusions. To train our model,\nwe introduce a method for generating synthetic data by applying geometric\ntransformations and free-form mask generation to clean textures. Experimental\nresults demonstrate that our framework significantly outperforms existing\nmethods both quantitatively and quantitatively. Furthermore, we conduct\ncomprehensive ablation studies to validate the different components of our\nproposed framework. Results are corroborated by a perceptual user study which\nhighlights the efficiency of our proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.14759v1"
    },
    {
        "title": "The Shortest Route Is Not Always the Fastest: Probability-Modeled\n  Stereoscopic Eye Movement Completion Time in VR",
        "authors": [
            "Budmonde Duinkharjav",
            "Benjamin Liang",
            "Anjul Patney",
            "Rachel Brown",
            "Qi Sun"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Speed and consistency of target-shifting play a crucial role in human ability\nto perform complex tasks. Shifting our gaze between objects of interest quickly\nand consistently requires changes both in depth and direction. Gaze changes in\ndepth are driven by slow, inconsistent vergence movements which rotate the eyes\nin opposite directions, while changes in direction are driven by ballistic,\nconsistent movements called saccades, which rotate the eyes in the same\ndirection. In the natural world, most of our eye movements are a combination of\nboth types. While scientific consensus on the nature of saccades exists,\nvergence and combined movements remain less understood and agreed upon.\n  We eschew the lack of scientific consensus in favor of proposing an\noperationalized computational model which predicts the speed of any type of\ngaze movement during target-shifting in 3D. To this end, we conduct a\npsychophysical study in a stereo VR environment to collect more than 12,000\ngaze movement trials, analyze the temporal distribution of the observed gaze\nmovements, and fit a probabilistic model to the data. We perform a series of\nobjective measurements and user studies to validate the model. The results\ndemonstrate its predictive accuracy, generalization, as well as applications\nfor optimizing visual performance by altering content placement. Lastly, we\nleverage the model to measure differences in human target-changing time\nrelative to the natural world, as well as suggest scene-aware projection depth.\nBy incorporating the complexities and randomness of human oculomotor control,\nwe hope this research will support new behavior-aware metrics for VR/AR display\ndesign, interface layout, and gaze-contingent rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.15183v2"
    },
    {
        "title": "Joint Sampling and Optimisation for Inverse Rendering",
        "authors": [
            "Martin Balint",
            "Karol Myszkowski",
            "Hans-Peter Seidel",
            "Gurprit Singh"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  When dealing with difficult inverse problems such as inverse rendering, using\nMonte Carlo estimated gradients to optimise parameters can slow down\nconvergence due to variance. Averaging many gradient samples in each iteration\nreduces this variance trivially. However, for problems that require thousands\nof optimisation iterations, the computational cost of this approach rises\nquickly.\n  We derive a theoretical framework for interleaving sampling and optimisation.\nWe update and reuse past samples with low-variance finite-difference estimators\nthat describe the change in the estimated gradients between each iteration. By\ncombining proportional and finite-difference samples, we continuously reduce\nthe variance of our novel gradient meta-estimators throughout the optimisation\nprocess. We investigate how our estimator interlinks with Adam and derive a\nstable combination.\n  We implement our method for inverse path tracing and demonstrate how our\nestimator speeds up convergence on difficult optimisation tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.15676v1"
    },
    {
        "title": "Exploiting Human Color Discrimination for Memory- and Energy-Efficient\n  Image Encoding in Virtual Reality",
        "authors": [
            "Nisarg Ujjainkar",
            "Ethan Shahan",
            "Kenneth Chen",
            "Budmonde Duinkharjav",
            "Qi Sun",
            "Yuhao Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Virtual Reality (VR) has the potential of becoming the next ubiquitous\ncomputing platform. Continued progress in the burgeoning field of VR depends\ncritically on an efficient computing substrate. In particular, DRAM access\nenergy is known to contribute to a significant portion of system energy.\nToday's framebuffer compression system alleviates the DRAM traffic by using a\nnumerically lossless compression algorithm. Being numerically lossless,\nhowever, is unnecessary to preserve perceptual quality for humans. This paper\nproposes a perceptually lossless, but numerically lossy, system to compress\nDRAM traffic. Our idea builds on top of long-established psychophysical studies\nthat show that humans cannot discriminate colors that are close to each other.\nThe discrimination ability becomes even weaker (i.e., more colors are\nperceptually indistinguishable) in our peripheral vision. Leveraging the color\ndiscrimination (in)ability, we propose an algorithm that adjusts pixel colors\nto minimize the bit encoding cost without introducing visible artifacts. The\nalgorithm is coupled with lightweight architectural support that, in real-time,\nreduces the DRAM traffic by 66.9\\% and outperforms existing framebuffer\ncompression mechanisms by up to 20.4\\%. Psychophysical studies on human\nparticipants show that our system introduce little to no perceptual fidelity\ndegradation.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.00441v1"
    },
    {
        "title": "Investigation on a Novel Length-Based Local Linear Subdivision Strategy\n  for Triangular Meshes",
        "authors": [
            "Junyi Shen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Triangular meshes are a widely used representation in the field of 3D\nmodeling. In this paper, we present a novel approach for edge length-based\nlinear subdivision on triangular meshes, along with two auxiliary techniques.\nWe conduct a comprehensive comparison of different subdivision methods in terms\nof computational capabilities and mesh-enhancing abilities. Our proposed\napproach demonstrates improved computational efficiency and generates fewer\nelements with higher quality compared to existing methods. The improvement in\ncomputational efficiency and mesh augmentation capability of our method is\nfurther enhanced when working with the two auxiliary techniques presented in\nthis paper. Our novel strategy represents a significant contribution to the\nfield and has important implications for local mesh refinement, computer-aided\ndesign, and isotropic remeshing.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.01445v1"
    },
    {
        "title": "Variational Barycentric Coordinates",
        "authors": [
            "Ana Dodik",
            "Oded Stein",
            "Vincent Sitzmann",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a variational technique to optimize for generalized barycentric\ncoordinates that offers additional control compared to existing models. Prior\nwork represents barycentric coordinates using meshes or closed-form formulae,\nin practice limiting the choice of objective function. In contrast, we directly\nparameterize the continuous function that maps any coordinate in a polytope's\ninterior to its barycentric coordinates using a neural field. This formulation\nis enabled by our theoretical characterization of barycentric coordinates,\nwhich allows us to construct neural fields that parameterize the entire\nfunction class of valid coordinates. We demonstrate the flexibility of our\nmodel using a variety of objective functions, including multiple smoothness and\ndeformation-aware energies; as a side contribution, we also present\nmathematically-justified means of measuring and minimizing objectives like\ntotal variation on discontinuous neural fields. We offer a practical\nacceleration strategy, present a thorough validation of our algorithm, and\ndemonstrate several applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.03861v1"
    },
    {
        "title": "In the Blink of an Eye: Event-based Emotion Recognition",
        "authors": [
            "Haiwei Zhang",
            "Jiqing Zhang",
            "Bo Dong",
            "Pieter Peers",
            "Wenwei Wu",
            "Xiaopeng Wei",
            "Felix Heide",
            "Xin Yang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce a wearable single-eye emotion recognition device and a real-time\napproach to recognizing emotions from partial observations of an emotion that\nis robust to changes in lighting conditions. At the heart of our method is a\nbio-inspired event-based camera setup and a newly designed lightweight Spiking\nEye Emotion Network (SEEN). Compared to conventional cameras, event-based\ncameras offer a higher dynamic range (up to 140 dB vs. 80 dB) and a higher\ntemporal resolution. Thus, the captured events can encode rich temporal cues\nunder challenging lighting conditions. However, these events lack texture\ninformation, posing problems in decoding temporal information effectively. SEEN\ntackles this issue from two different perspectives. First, we adopt\nconvolutional spiking layers to take advantage of the spiking neural network's\nability to decode pertinent temporal information. Second, SEEN learns to\nextract essential spatial cues from corresponding intensity frames and\nleverages a novel weight-copy scheme to convey spatial attention to the\nconvolutional spiking layers during training and inference. We extensively\nvalidate and demonstrate the effectiveness of our approach on a specially\ncollected Single-eye Event-based Emotion (SEE) dataset. To the best of our\nknowledge, our method is the first eye-based emotion recognition method that\nleverages event-based cameras and spiking neural network.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.04043v1"
    },
    {
        "title": "DragD3D: Realistic Mesh Editing with Rigidity Control Driven by 2D\n  Diffusion Priors",
        "authors": [
            "Tianhao Xie",
            "Eugene Belilovsky",
            "Sudhir Mudur",
            "Tiberiu Popa"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Direct mesh editing and deformation are key components in the geometric\nmodeling and animation pipeline. Mesh editing methods are typically framed as\noptimization problems combining user-specified vertex constraints with a\nregularizer that determines the position of the rest of the vertices. The\nchoice of the regularizer is key to the realism and authenticity of the final\nresult. Physics and geometry-based regularizers are not aware of the global\ncontext and semantics of the object, and the more recent deep learning priors\nare limited to a specific class of 3D object deformations. Our main\ncontribution is a vertex-based mesh editing method called DragD3D based on (1)\na novel optimization formulation that decouples the rotation and stretch\ncomponents of the deformation and combines a 3D geometric regularizer with (2)\nthe recently introduced DDS loss which scores the faithfulness of the rendered\n2D image to one from a diffusion model. Thus, our deformation method achieves\nglobally realistic shape deformation which is not restricted to any class of\nobjects. Our new formulation optimizes directly the transformation of the\nneural Jacobian field explicitly separating the rotational and stretching\ncomponents. The objective function of the optimization combines the approximate\ngradients of DDS and the gradients from the geometric loss to satisfy the\nvertex constraints. Additional user control over desired global shape\ndeformation is made possible by allowing explicit per-triangle deformation\ncontrol as well as explicit separation of rotational and stretching components\nof the deformation. We show that our deformations can be controlled to yield\nrealistic shape deformations that are aware of the global context of the\nobjects, and provide better results than just using geometric regularizers.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.04561v2"
    },
    {
        "title": "Neural Impostor: Editing Neural Radiance Fields with Explicit Shape\n  Manipulation",
        "authors": [
            "Ruiyang Liu",
            "Jinxu Xiang",
            "Bowen Zhao",
            "Ran Zhang",
            "Jingyi Yu",
            "Changxi Zheng"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Neural Radiance Fields (NeRF) have significantly advanced the generation of\nhighly realistic and expressive 3D scenes. However, the task of editing NeRF,\nparticularly in terms of geometry modification, poses a significant challenge.\nThis issue has obstructed NeRF's wider adoption across various applications. To\ntackle the problem of efficiently editing neural implicit fields, we introduce\nNeural Impostor, a hybrid representation incorporating an explicit tetrahedral\nmesh alongside a multigrid implicit field designated for each tetrahedron\nwithin the explicit mesh. Our framework bridges the explicit shape manipulation\nand the geometric editing of implicit fields by utilizing multigrid barycentric\ncoordinate encoding, thus offering a pragmatic solution to deform, composite,\nand generate neural implicit fields while maintaining a complex volumetric\nappearance. Furthermore, we propose a comprehensive pipeline for editing neural\nimplicit fields based on a set of explicit geometric editing operations. We\nshow the robustness and adaptability of our system through diverse examples and\nexperiments, including the editing of both synthetic objects and real captured\ndata. Finally, we demonstrate the authoring process of a hybrid\nsynthetic-captured object utilizing a variety of editing operations,\nunderlining the transformative potential of Neural Impostor in the field of 3D\ncontent creation and manipulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.05391v1"
    },
    {
        "title": "Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic\n  Clothing Driven by Sparse RGB-D Input",
        "authors": [
            "Donglai Xiang",
            "Fabian Prada",
            "Zhe Cao",
            "Kaiwen Guo",
            "Chenglei Wu",
            "Jessica Hodgins",
            "Timur Bagautdinov"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Clothing is an important part of human appearance but challenging to model in\nphotorealistic avatars. In this work we present avatars with dynamically moving\nloose clothing that can be faithfully driven by sparse RGB-D inputs as well as\nbody and face motion. We propose a Neural Iterative Closest Point (N-ICP)\nalgorithm that can efficiently track the coarse garment shape given sparse\ndepth input. Given the coarse tracking results, the input RGB-D images are then\nremapped to texel-aligned features, which are fed into the drivable avatar\nmodels to faithfully reconstruct appearance details. We evaluate our method\nagainst recent image-driven synthesis baselines, and conduct a comprehensive\nanalysis of the N-ICP algorithm. We demonstrate that our method can generalize\nto a novel testing environment, while preserving the ability to produce\nhigh-fidelity and faithful clothing dynamics and appearance.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.05917v2"
    },
    {
        "title": "Neural Bounding",
        "authors": [
            "Stephanie Wenxin Liu",
            "Michael Fischer",
            "Paul D. Yoo",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Bounding volumes are an established concept in computer graphics and vision\ntasks but have seen little change since their early inception. In this work, we\nstudy the use of neural networks as bounding volumes. Our key observation is\nthat bounding, which so far has primarily been considered a problem of\ncomputational geometry, can be redefined as a problem of learning to classify\nspace into free or occupied. This learning-based approach is particularly\nadvantageous in high-dimensional spaces, such as animated scenes with complex\nqueries, where neural networks are known to excel. However, unlocking neural\nbounding requires a twist: allowing -- but also limiting -- false positives,\nwhile ensuring that the number of false negatives is strictly zero. We enable\nsuch tight and conservative results using a dynamically-weighted asymmetric\nloss function. Our results show that our neural bounding produces up to an\norder of magnitude fewer false positives than traditional methods. In addition,\nwe propose an extension of our bounding method using early exits that\naccelerates query speeds by 25%. We also demonstrate that our approach is\napplicable to non-deep learning models that train within seconds. Our project\npage is at: https://wenxin-liu.github.io/neural_bounding/.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.06822v5"
    },
    {
        "title": "Asymptote-based scientific animation",
        "authors": [
            "Migran N. Gevorkyan",
            "Anna V. Korolkova",
            "Dmitry S. Kulyabov"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This article discusses a universal way to create animation using Asymptote\nthe language for vector graphics. The Asymptote language itself has a built-in\nlibrary for creating animations, but its practical use is complicated by an\nextremely brief description in the official documentation and unstable\nexecution of existing examples. The purpose of this article is to eliminate\nthis gap. The method we describe is based on creating a PDF file with frames\nusing Asymptote, with further converting it into a set of PNG images and\nmerging them into a video using FFmpeg. All stages are described in detail,\nwhich allows the reader to use the described method without being familiar with\nthe used utilities.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.06860v1"
    },
    {
        "title": "Discovering Fatigued Movements for Virtual Character Animation",
        "authors": [
            "Noshaba Cheema",
            "Rui Xu",
            "Nam Hee Kim",
            "Perttu Hämäläinen",
            "Vladislav Golyanik",
            "Marc Habermann",
            "Christian Theobalt",
            "Philipp Slusallek"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Virtual character animation and movement synthesis have advanced rapidly\nduring recent years, especially through a combination of extensive motion\ncapture datasets and machine learning. A remaining challenge is interactively\nsimulating characters that fatigue when performing extended motions, which is\nindispensable for the realism of generated animations. However, capturing such\nmovements is problematic, as performing movements like backflips with fatigued\nvariations up to exhaustion raises capture cost and risk of injury.\nSurprisingly, little research has been done on faithful fatigue modeling. To\naddress this, we propose a deep reinforcement learning-based approach, which --\nfor the first time in literature -- generates control policies for full-body\nphysically simulated agents aware of cumulative fatigue. For this, we first\nleverage Generative Adversarial Imitation Learning (GAIL) to learn an expert\npolicy for the skill; Second, we learn a fatigue policy by limiting the\ngenerated constant torque bounds based on endurance time to non-linear, state-\nand time-dependent limits in the joint-actuation space using a\nThree-Compartment Controller (3CC) model. Our results demonstrate that agents\ncan adapt to different fatigue and rest rates interactively, and discover\nrealistic recovery strategies without the need for any captured data of\nfatigued movement.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.08583v1"
    },
    {
        "title": "A Parallel Feature-preserving Mesh Variable Offsetting Method with\n  Dynamic Programming",
        "authors": [
            "Hongyi Cao",
            "Gang Xu",
            "Renshu Gu",
            "Jinlan Xu",
            "Xiaoyu Zhang",
            "Timon Rabczuk"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Mesh offsetting plays an important role in discrete geometric processing. In\nthis paper, we propose a parallel feature-preserving mesh offsetting framework\nwith variable distance. Different from the traditional method based on distance\nand normal vector, a new calculation of offset position is proposed by using\ndynamic programming and quadratic programming, and the sharp feature can be\npreserved after offsetting. Instead of distance implicit field, a spatial\ncoverage region represented by polyhedral for computing offsets is proposed.\nOur method can generate an offsetting model with smaller mesh size, and also\ncan achieve high quality without gaps, holes, and self-intersections. Moreover,\nseveral acceleration techniques are proposed for the efficient mesh offsetting,\nsuch as the parallel computing with grid, AABB tree and rays computing. In\norder to show the efficiency and robustness of the proposed framework, we have\ntested our method on the quadmesh dataset, which is available at\n[https://www.quadmesh.cloud]. The source code of the proposed algorithm is\navailable on GitHub at [https://github.com/iGame-Lab/PFPOffset].\n",
        "pdf_link": "http://arxiv.org/pdf/2310.08997v1"
    },
    {
        "title": "FuseSR: Super Resolution for Real-time Rendering through Efficient\n  Multi-resolution Fusion",
        "authors": [
            "Zhihua Zhong",
            "Jingsen Zhu",
            "Yuxin Dai",
            "Chuankun Zheng",
            "Yuchi Huo",
            "Guanlin Chen",
            "Hujun Bao",
            "Rui Wang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The workload of real-time rendering is steeply increasing as the demand for\nhigh resolution, high refresh rates, and high realism rises, overwhelming most\ngraphics cards. To mitigate this problem, one of the most popular solutions is\nto render images at a low resolution to reduce rendering overhead, and then\nmanage to accurately upsample the low-resolution rendered image to the target\nresolution, a.k.a. super-resolution techniques. Most existing methods focus on\nexploiting information from low-resolution inputs, such as historical frames.\nThe absence of high frequency details in those LR inputs makes them hard to\nrecover fine details in their high-resolution predictions. In this paper, we\npropose an efficient and effective super-resolution method that predicts\nhigh-quality upsampled reconstructions utilizing low-cost high-resolution\nauxiliary G-Buffers as additional input. With LR images and HR G-buffers as\ninput, the network requires to align and fuse features at multi resolution\nlevels. We introduce an efficient and effective H-Net architecture to solve\nthis problem and significantly reduce rendering overhead without noticeable\nquality deterioration. Experiments show that our method is able to produce\ntemporally consistent reconstructions in $4 \\times 4$ and even challenging $8\n\\times 8$ upsampling cases at 4K resolution with real-time performance, with\nsubstantially improved quality and significant performance boost compared to\nexisting works.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.09726v1"
    },
    {
        "title": "MOCHA: Real-Time Motion Characterization via Context Matching",
        "authors": [
            "Deok-Kyeong Jang",
            "Yuting Ye",
            "Jungdam Won",
            "Sung-Hee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Transforming neutral, characterless input motions to embody the distinct\nstyle of a notable character in real time is highly compelling for character\nanimation. This paper introduces MOCHA, a novel online motion characterization\nframework that transfers both motion styles and body proportions from a target\ncharacter to an input source motion. MOCHA begins by encoding the input motion\ninto a motion feature that structures the body part topology and captures\nmotion dependencies for effective characterization. Central to our framework is\nthe Neural Context Matcher, which generates a motion feature for the target\ncharacter with the most similar context to the input motion feature. The\nconditioned autoregressive model of the Neural Context Matcher can produce\ntemporally coherent character features in each time frame. To generate the\nfinal characterized pose, our Characterizer network incorporates the\ncharacteristic aspects of the target motion feature into the input motion\nfeature while preserving its context. This is achieved through a transformer\nmodel that introduces the adaptive instance normalization and context\nmapping-based cross-attention, effectively injecting the character feature into\nthe source feature. We validate the performance of our framework through\ncomparisons with prior work and an ablation study. Our framework can easily\naccommodate various applications, including characterization with only sparse\ninput and real-time characterization. Additionally, we contribute a\nhigh-quality motion dataset comprising six different characters performing a\nrange of motions, which can serve as a valuable resource for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.10079v1"
    },
    {
        "title": "TapMo: Shape-aware Motion Generation of Skeleton-free Characters",
        "authors": [
            "Jiaxu Zhang",
            "Shaoli Huang",
            "Zhigang Tu",
            "Xin Chen",
            "Xiaohang Zhan",
            "Gang Yu",
            "Ying Shan"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Previous motion generation methods are limited to the pre-rigged 3D human\nmodel, hindering their applications in the animation of various non-rigged\ncharacters. In this work, we present TapMo, a Text-driven Animation Pipeline\nfor synthesizing Motion in a broad spectrum of skeleton-free 3D characters. The\npivotal innovation in TapMo is its use of shape deformation-aware features as a\ncondition to guide the diffusion model, thereby enabling the generation of\nmesh-specific motions for various characters. Specifically, TapMo comprises two\nmain components - Mesh Handle Predictor and Shape-aware Diffusion Module. Mesh\nHandle Predictor predicts the skinning weights and clusters mesh vertices into\nadaptive handles for deformation control, which eliminates the need for\ntraditional skeletal rigging. Shape-aware Motion Diffusion synthesizes motion\nwith mesh-specific adaptations. This module employs text-guided motions and\nmesh features extracted during the first stage, preserving the geometric\nintegrity of the animations by accounting for the character's shape and\ndeformation. Trained in a weakly-supervised manner, TapMo can accommodate a\nmultitude of non-human meshes, both with and without associated text motions.\nWe demonstrate the effectiveness and generalizability of TapMo through rigorous\nqualitative and quantitative experiments. Our results reveal that TapMo\nconsistently outperforms existing auto-animation methods, delivering\nsuperior-quality animations for both seen or unseen heterogeneous 3D\ncharacters.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.12678v1"
    },
    {
        "title": "Auxiliary Features-Guided Super Resolution for Monte Carlo Rendering",
        "authors": [
            "Qiqi Hou",
            "Feng Liu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper investigates super resolution to reduce the number of pixels to\nrender and thus speed up Monte Carlo rendering algorithms. While great progress\nhas been made to super resolution technologies, it is essentially an ill-posed\nproblem and cannot recover high-frequency details in renderings. To address\nthis problem, we exploit high-resolution auxiliary features to guide super\nresolution of low-resolution renderings. These high-resolution auxiliary\nfeatures can be quickly rendered by a rendering engine and at the same time\nprovide valuable high-frequency details to assist super resolution. To this\nend, we develop a cross-modality Transformer network that consists of an\nauxiliary feature branch and a low-resolution rendering branch. These two\nbranches are designed to fuse high-resolution auxiliary features with the\ncorresponding low-resolution rendering. Furthermore, we design residual\ndensely-connected Swin Transformer groups to learn to extract representative\nfeatures to enable high-quality super-resolution. Our experiments show that our\nauxiliary features-guided super-resolution method outperforms both\nsuper-resolution methods and Monte Carlo denoising methods in producing\nhigh-quality renderings.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.13235v1"
    },
    {
        "title": "DeepFracture: A Generative Approach for Predicting Brittle Fractures",
        "authors": [
            "Yuhang Huang",
            "Takashi Kanai"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In the realm of brittle fracture animation, generating realistic destruction\nanimations with physics simulation techniques can be computationally expensive.\nAlthough methods using Voronoi diagrams or pre-fractured patterns work for\nreal-time applications, they often lack realism in portraying brittle\nfractures. This paper introduces a novel learning-based approach for seamlessly\nmerging realistic brittle fracture animations with rigid-body simulations. Our\nmethod utilizes BEM brittle fracture simulations to create fractured patterns\nand collision conditions for a given shape, which serve as training data for\nthe learning process. To effectively integrate collision conditions and\nfractured shapes into a deep learning framework, we introduce the concept of\nlatent impulse representation and geometrically-segmented signed distance\nfunction (GS-SDF). The latent impulse representation serves as input, capturing\ninformation about impact forces on the shape's surface. Simultaneously, a\nGS-SDF is used as the output representation of the fractured shape. To address\nthe challenge of optimizing multiple fractured pattern targets with a single\nlatent code, we propose an eight-dimensional latent space based on a normal\ndistribution code within our latent impulse representation design. This\nadaptation effectively transforms our neural network into a generative one. Our\nexperimental results demonstrate that our approach can generate significantly\nmore detailed brittle fractures compared to existing techniques, all while\nmaintaining commendable computational efficiency during run-time.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.13344v1"
    },
    {
        "title": "Single-view 3D reconstruction via inverse procedural modeling",
        "authors": [
            "Albert Garifullin",
            "Nikolay Maiorov",
            "Vladimir Frolov"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose an approach to 3D reconstruction via inverse procedural modeling\nand investigate two variants of this approach. The first option consists in the\nfitting set of input parameters using a genetic algorithm. We demonstrate the\nresults of our work on tree models, complex objects, with the reconstruction of\nwhich most existing methods cannot handle. The second option allows us to\nsignificantly improve the precision by using gradients within memetic\nalgorithm, differentiable rendering and also differentiable procedural\ngenerators. In our work we see 2 main contributions. First, we propose a method\nto join differentiable rendering and inverse procedural modeling. This gives us\nan opportunity to reconstruct 3D model more accurately than existing approaches\nwhen a small number of input images are available (even for single image).\nSecond, we join both differentiable and non-differentiable procedural\ngenerators in a single framework which allow us to apply inverse procedural\nmodeling to fairly complex generators: when gradient is available,\nreconstructions is precise, when gradient is not available, reconstruction is\napproximate, but always high quality without visual artifacts.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.13373v1"
    },
    {
        "title": "DeepIron: Predicting Unwarped Garment Texture from a Single Image",
        "authors": [
            "Hyun-Song Kwon",
            "Sung-Hee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Realistic reconstruction of 3D clothing from an image has wide applications,\nsuch as avatar creation and virtual try-on. This paper presents a novel\nframework that reconstructs the texture map for 3D garments from a single image\nwith pose. Assuming that 3D garments are modeled by stitching 2D garment sewing\npatterns, our specific goal is to generate a texture image for the sewing\npatterns. A key component of our framework, the Texture Unwarper, infers the\noriginal texture image from the input clothing image, which exhibits warping\nand occlusion of texture due to the user's body shape and pose. The Texture\nUnwarper effectively transforms between the input and output images by mapping\nthe latent spaces of the two images. By inferring the unwarped original texture\nof the input garment, our method helps reconstruct 3D garment models that can\nshow high-quality texture images realistically deformed for new poses. We\nvalidate the effectiveness of our approach through a comparison with other\nmethods and ablation studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.15447v2"
    },
    {
        "title": "Real-time Animation Generation and Control on Rigged Models via Large\n  Language Models",
        "authors": [
            "Han Huang",
            "Fernanda De La Torre",
            "Cathy Mengying Fang",
            "Andrzej Banburski-Fahey",
            "Judith Amores",
            "Jaron Lanier"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce a novel method for real-time animation control and generation on\nrigged models using natural language input. First, we embed a large language\nmodel (LLM) in Unity to output structured texts that can be parsed into diverse\nand realistic animations. Second, we illustrate LLM's potential to enable\nflexible state transition between existing animations. We showcase the\nrobustness of our approach through qualitative results on various rigged models\nand motions.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.17838v2"
    },
    {
        "title": "SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data",
        "authors": [
            "Jose Luis Ponton",
            "Haoran Yun",
            "Andreas Aristidou",
            "Carlos Andujar",
            "Nuria Pelechano"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Accurate and reliable human motion reconstruction is crucial for creating\nnatural interactions of full-body avatars in Virtual Reality (VR) and\nentertainment applications. As the Metaverse and social applications gain\npopularity, users are seeking cost-effective solutions to create full-body\nanimations that are comparable in quality to those produced by commercial\nmotion capture systems. In order to provide affordable solutions, though, it is\nimportant to minimize the number of sensors attached to the subject's body.\nUnfortunately, reconstructing the full-body pose from sparse data is a heavily\nunder-determined problem. Some studies that use IMU sensors face challenges in\nreconstructing the pose due to positional drift and ambiguity of the poses. In\nrecent years, some mainstream VR systems have released 6-degree-of-freedom\n(6-DoF) tracking devices providing positional and rotational information.\nNevertheless, most solutions for reconstructing full-body poses rely on\ntraditional inverse kinematics (IK) solutions, which often produce\nnon-continuous and unnatural poses. In this article, we introduce SparsePoser,\na novel deep learning-based solution for reconstructing a full-body pose from a\nreduced set of six tracking devices. Our system incorporates a\nconvolutional-based autoencoder that synthesizes high-quality continuous human\nposes by learning the human motion manifold from motion capture data. Then, we\nemploy a learned IK component, made of multiple lightweight feed-forward neural\nnetworks, to adjust the hands and feet toward the corresponding trackers. We\nextensively evaluate our method on publicly available motion capture datasets\nand with real-time live demos. We show that our method outperforms\nstate-of-the-art techniques using IMU sensors or 6-DoF tracking devices, and\ncan be used for users with different body dimensions and proportions.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.02191v1"
    },
    {
        "title": "BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis",
        "authors": [
            "Hao-Bin Duan",
            "Miao Wang",
            "Jin-Chuan Shi",
            "Xu-Chuan Chen",
            "Yan-Pei Cao"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Synthesizing photorealistic 4D human head avatars from videos is essential\nfor VR/AR, telepresence, and video game applications. Although existing Neural\nRadiance Fields (NeRF)-based methods achieve high-fidelity results, the\ncomputational expense limits their use in real-time applications. To overcome\nthis limitation, we introduce BakedAvatar, a novel representation for real-time\nneural head avatar synthesis, deployable in a standard polygon rasterization\npipeline. Our approach extracts deformable multi-layer meshes from learned\nisosurfaces of the head and computes expression-, pose-, and view-dependent\nappearances that can be baked into static textures for efficient rasterization.\nWe thus propose a three-stage pipeline for neural head avatar synthesis, which\nincludes learning continuous deformation, manifold, and radiance fields,\nextracting layered meshes and textures, and fine-tuning texture details with\ndifferential rasterization. Experimental results demonstrate that our\nrepresentation generates synthesis results of comparable quality to other\nstate-of-the-art methods while significantly reducing the inference time\nrequired. We further showcase various head avatar synthesis results from\nmonocular videos, including view synthesis, face reenactment, expression\nediting, and pose editing, all at interactive frame rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.05521v2"
    },
    {
        "title": "3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score\n  Distillation",
        "authors": [
            "Dale Decatur",
            "Itai Lang",
            "Kfir Aberman",
            "Rana Hanocka"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this work we develop 3D Paintbrush, a technique for automatically\ntexturing local semantic regions on meshes via text descriptions. Our method is\ndesigned to operate directly on meshes, producing texture maps which seamlessly\nintegrate into standard graphics pipelines. We opt to simultaneously produce a\nlocalization map (to specify the edit region) and a texture map which conforms\nto it. This synergistic approach improves the quality of both the localization\nand the stylization. To enhance the details and resolution of the textured\narea, we leverage multiple stages of a cascaded diffusion model to supervise\nour local editing technique with generative priors learned from images at\ndifferent resolutions. Our technique, referred to as Cascaded Score\nDistillation (CSD), simultaneously distills scores at multiple resolutions in a\ncascaded fashion, enabling control over both the granularity and global\nunderstanding of the supervision. We demonstrate the effectiveness of 3D\nPaintbrush to locally texture a variety of shapes within different semantic\nregions. Project page: https://threedle.github.io/3d-paintbrush\n",
        "pdf_link": "http://arxiv.org/pdf/2311.09571v1"
    },
    {
        "title": "Intelligent Generation of Graphical Game Assets: A Conceptual Framework\n  and Systematic Review of the State of the Art",
        "authors": [
            "Kaisei Fukaya",
            "Damon Daylamani-Zad",
            "Harry Agius"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Procedural content generation (PCG) can be applied to a wide variety of tasks\nin games, from narratives, levels and sounds, to trees and weapons. A large\namount of game content is comprised of graphical assets, such as clouds,\nbuildings or vegetation, that do not require gameplay function considerations.\nThere is also a breadth of literature examining the procedural generation of\nsuch elements for purposes outside of games. The body of research, focused on\nspecific methods for generating specific assets, provides a narrow view of the\navailable possibilities. Hence, it is difficult to have a clear picture of all\napproaches and possibilities, with no guide for interested parties to discover\npossible methods and approaches for their needs, and no facility to guide them\nthrough each technique or approach to map out the process of using them.\nTherefore, a systematic literature review has been conducted, yielding 200\naccepted papers. This paper explores state-of-the-art approaches to graphical\nasset generation, examining research from a wide range of applications, inside\nand outside of games. Informed by the literature, a conceptual framework has\nbeen derived to address the aforementioned gaps.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.10129v1"
    },
    {
        "title": "Robust Hole-Detection in Triangular Meshes Irrespective of the Presence\n  of Singular Vertices",
        "authors": [
            "Mauhing Yip",
            "Annette Stahl",
            "Christian Schellewald"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this work, we present a boundary and hole detection approach that\ntraverses all the boundaries of an edge-manifold triangular mesh,\nirrespectively of the presence of singular vertices, and subsequently\ndetermines and labels all holes of the mesh. The proposed automated\nhole-detection method is valuable to the computer-aided design (CAD) community\nas all half-edges within the mesh are utilized and for each half-edge the\nalgorithm guarantees both the existence and the uniqueness of the boundary\nassociated to it. As existing hole-detection approaches assume that singular\nvertices are absent or may require mesh modification, these methods are\nill-equipped to detect boundaries/holes in real-world meshes that contain\nsingular vertices. We demonstrate the method in an underwater autonomous\nrobotic application, exploiting surface reconstruction methods based on point\ncloud data. In such a scenario the determined holes can be interpreted as\ninformation gaps, enabling timely corrective action during the data\nacquisition. However, the scope of our method is not confined to these two\nsectors alone; it is versatile enough to be applied on any edge-manifold\ntriangle mesh. An evaluation of the method is performed on both synthetic and\nreal-world data (including a triangle mesh from a point cloud obtained by a\nmultibeam sonar). The source code of our reference implementation is available:\nhttps://github.com/Mauhing/hole-detection-on-triangle-mesh.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.12466v1"
    },
    {
        "title": "SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh\n  Reconstruction and High-Quality Mesh Rendering",
        "authors": [
            "Antoine Guédon",
            "Vincent Lepetit"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a method to allow precise and extremely fast mesh extraction from\n3D Gaussian Splatting. Gaussian Splatting has recently become very popular as\nit yields realistic rendering while being significantly faster to train than\nNeRFs. It is however challenging to extract a mesh from the millions of tiny 3D\ngaussians as these gaussians tend to be unorganized after optimization and no\nmethod has been proposed so far. Our first key contribution is a regularization\nterm that encourages the gaussians to align well with the surface of the scene.\nWe then introduce a method that exploits this alignment to extract a mesh from\nthe Gaussians using Poisson reconstruction, which is fast, scalable, and\npreserves details, in contrast to the Marching Cubes algorithm usually applied\nto extract meshes from Neural SDFs. Finally, we introduce an optional\nrefinement strategy that binds gaussians to the surface of the mesh, and\njointly optimizes these Gaussians and the mesh through Gaussian splatting\nrendering. This enables easy editing, sculpting, rigging, animating,\ncompositing and relighting of the Gaussians using traditional softwares by\nmanipulating the mesh instead of the gaussians themselves. Retrieving such an\neditable mesh for realistic rendering is done within minutes with our method,\ncompared to hours with the state-of-the-art methods on neural SDFs, while\nproviding a better rendering quality. Our project page is the following:\nhttps://anttwo.github.io/sugar/\n",
        "pdf_link": "http://arxiv.org/pdf/2311.12775v3"
    },
    {
        "title": "Web-Based Dynamic Paintings: Real-Time Interactive Artworks in Web Using\n  a 2.5D Pipeline",
        "authors": [
            "Ergun Akleman",
            "Youyou wang",
            "Yinan Xiong",
            "Anusha Shanker",
            "Fermi Perumal",
            "Ozgur Gonen",
            "Motahareh Fard"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this work, we present a 2.5D pipeline approach to creating dynamic\npaintings that can be re-rendered interactively in real-time on the Web. Using\nthis 2.5D approach, any existing simple painting such as portraits can be\nturned into an interactive dynamic web-based artwork. Our interactive system\nprovides most global illumination effects such as reflection, refraction,\nshadow, and subsurface scattering by processing images. In our system, the\nscene is defined only by a set of images. These include (1) a shape image, (2)\ntwo diffuse images, (3) a background image, (4) one foreground image, and (5)\none transparency image. A shape image is either a normal map or a height. Two\ndiffuse images are usually hand-painted. They are interpolated using\nillumination information. The transparency image is used to define the\ntransparent and reflective regions that can reflect the foreground image and\nrefract the background image, both of which are also hand-drawn. This\nframework, which mainly uses hand-drawn images, provides qualitatively\nconvincing painterly global illumination effects such as reflection and\nrefraction. We also include parameters to provide additional artistic controls.\nFor instance, using our piecewise linear Fresnel function, it is possible to\ncontrol the ratio of reflection and refraction. This system is the result of a\nlong line of research contributions. On the other hand, the art-directed\nFresnel function that provides physically plausible compositing of reflection\nand refraction with artistic control is completely new. Art-directed warping\nequations that provide qualitatively convincing refraction and reflection\neffects with linearized artistic control are also new. You can try our\nweb-based system for interactive dynamic real-time paintings at\nhttp://mock3d.tamu.edu/.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.15354v1"
    },
    {
        "title": "GeoScaler: Geometry and Rendering-Aware Downsampling of 3D Mesh Textures",
        "authors": [
            "Sai Karthikey Pentapati",
            "Anshul Rai",
            "Arkady Ten",
            "Chaitanya Atluru",
            "Alan Bovik"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  High-resolution texture maps are necessary for representing real-world\nobjects accurately with 3D meshes. The large sizes of textures can bottleneck\nthe real-time rendering of high-quality virtual 3D scenes on devices having low\ncomputational budgets and limited memory. Downsampling the texture maps\ndirectly addresses the issue, albeit at the cost of visual fidelity.\nTraditionally, downsampling of texture maps is performed using methods like\nbicubic interpolation and the Lanczos algorithm. These methods ignore the\ngeometric layout of the mesh and its UV parametrization and also do not account\nfor the rendering process used to obtain the final visualization that the users\nwill experience. Towards filling these gaps, we introduce GeoScaler, which is a\nmethod of downsampling texture maps of 3D meshes while incorporating geometric\ncues, and by maximizing the visual fidelity of the rendered views of the\ntextured meshes. We show that the textures generated by GeoScaler deliver\nsignificantly better quality rendered images compared to those generated by\ntraditional downsampling methods\n",
        "pdf_link": "http://arxiv.org/pdf/2311.16581v2"
    },
    {
        "title": "AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text",
        "authors": [
            "Jianfeng Zhang",
            "Xuanmeng Zhang",
            "Huichao Zhang",
            "Jun Hao Liew",
            "Chenxu Zhang",
            "Yi Yang",
            "Jiashi Feng"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We study the problem of creating high-fidelity and animatable 3D avatars from\nonly textual descriptions. Existing text-to-avatar methods are either limited\nto static avatars which cannot be animated or struggle to generate animatable\navatars with promising quality and precise pose control. To address these\nlimitations, we propose AvatarStudio, a coarse-to-fine generative model that\ngenerates explicit textured 3D meshes for animatable human avatars.\nSpecifically, AvatarStudio begins with a low-resolution NeRF-based\nrepresentation for coarse generation, followed by incorporating SMPL-guided\narticulation into the explicit mesh representation to support avatar animation\nand high resolution rendering. To ensure view consistency and pose\ncontrollability of the resulting avatars, we introduce a 2D diffusion model\nconditioned on DensePose for Score Distillation Sampling supervision. By\neffectively leveraging the synergy between the articulated mesh representation\nand the DensePose-conditional diffusion model, AvatarStudio can create\nhigh-quality avatars from text that are ready for animation, significantly\noutperforming previous methods. Moreover, it is competent for many\napplications, e.g., multimodal avatar animations and style-guided avatar\ncreation. For more results, please refer to our project page:\nhttp://jeff95.me/projects/avatarstudio.html\n",
        "pdf_link": "http://arxiv.org/pdf/2311.17917v1"
    },
    {
        "title": "Volumetric Rendering with Baked Quadrature Fields",
        "authors": [
            "Gopal Sharma",
            "Daniel Rebain",
            "Kwang Moo Yi",
            "Andrea Tagliasacchi"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a novel Neural Radiance Field (NeRF) representation for non-opaque\nscenes that enables fast inference by utilizing textured polygons. Despite the\nhigh-quality novel view rendering that NeRF provides, a critical limitation is\nthat it relies on volume rendering that can be computationally expensive and\ndoes not utilize the advancements in modern graphics hardware. Many existing\nmethods fall short when it comes to modelling volumetric effects as they rely\npurely on surface rendering. We thus propose to model the scene with polygons,\nwhich can then be used to obtain the quadrature points required to model\nvolumetric effects, and also their opacity and colour from the texture. To\nobtain such polygonal mesh, we train a specialized field whose zero-crossings\nwould correspond to the quadrature points when volume rendering, and perform\nmarching cubes on this field. We then perform ray-tracing and utilize the\nray-tracing shader to obtain the final colour image. Our method allows an easy\nintegration with existing graphics frameworks allowing rendering speed of over\n100 frames-per-second for a $1920\\times1080$ image, while still being able to\nrepresent non-opaque objects.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.02202v2"
    },
    {
        "title": "DragVideo: Interactive Drag-style Video Editing",
        "authors": [
            "Yufan Deng",
            "Ruida Wang",
            "Yuhao Zhang",
            "Yu-Wing Tai",
            "Chi-Keung Tang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Video generation models have shown their superior ability to generate\nphoto-realistic video. However, how to accurately control (or edit) the video\nremains a formidable challenge. The main issues are: 1) how to perform direct\nand accurate user control in editing; 2) how to execute editings like changing\nshape, expression, and layout without unsightly distortion and artifacts to the\nedited content; and 3) how to maintain spatio-temporal consistency of video\nafter editing. To address the above issues, we propose DragVideo, a general\ndrag-style video editing framework. Inspired by DragGAN, DragVideo addresses\nissues 1) and 2) by proposing the drag-style video latent optimization method\nwhich gives desired control by updating noisy video latent according to drag\ninstructions through video-level drag objective function. We amend issue 3) by\nintegrating the video diffusion model with sample-specific LoRA and Mutual\nSelf-Attention in DragVideo to ensure the edited result is spatio-temporally\nconsistent. We also present a series of testing examples for drag-style video\nediting and conduct extensive experiments across a wide array of challenging\nediting tasks, such as motion, skeleton editing, etc, underscoring DragVideo\ncan edit video in an intuitive, faithful to the user's intention manner, with\nnearly unnoticeable distortion and artifacts, while maintaining spatio-temporal\nconsistency. While traditional prompt-based video editing fails to do the\nformer two and directly applying image drag editing fails in the last,\nDragVideo's versatility and generality are emphasized. Github link:\nhttps://github.com/RickySkywalker/DragVideo-Official.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.02216v3"
    },
    {
        "title": "Revisiting Micro and Macro Expressions in Computer Graphics Characters",
        "authors": [
            "Rubens Montanha",
            "Giovana Raupp",
            "Vitoria Gonzalez",
            "Yanny Partichelli",
            "André Bins",
            "Marcos Ferreira",
            "Victor Araujo",
            "Soraia Musse"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper presents the reproduction of two studies focused on the perception\nof micro and macro expressions of Virtual Humans (VHs) generated by Computer\nGraphics (CG), first described in 2014 and replicated in 2021. The 2014 study\nreferred to a VH realistic, whereas, in 2021, it referred to a VH cartoon. In\nour work, we replicate the study by using a realistic CG character. Our main\ngoals are to compare the perceptions of micro and macro expressions between\nlevels of realism (2021 cartoon versus 2023 realistic) and between realistic\ncharacters in different periods (i.e., 2014 versus 2023). In one of our\nresults, people more easily recognized micro expressions in realistic VHs than\nin a cartoon VH. In another result, we show that the participants' perception\nwas similar for both micro and macro expressions in 2014 and 2023.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.03590v1"
    },
    {
        "title": "Relightable Gaussian Codec Avatars",
        "authors": [
            "Shunsuke Saito",
            "Gabriel Schwartz",
            "Tomas Simon",
            "Junxuan Li",
            "Giljoo Nam"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  The fidelity of relighting is bounded by both geometry and appearance\nrepresentations. For geometry, both mesh and volumetric approaches have\ndifficulty modeling intricate structures like 3D hair geometry. For appearance,\nexisting relighting models are limited in fidelity and often too slow to render\nin real-time with high-resolution continuous environments. In this work, we\npresent Relightable Gaussian Codec Avatars, a method to build high-fidelity\nrelightable head avatars that can be animated to generate novel expressions.\nOur geometry model based on 3D Gaussians can capture 3D-consistent\nsub-millimeter details such as hair strands and pores on dynamic face\nsequences. To support diverse materials of human heads such as the eyes, skin,\nand hair in a unified manner, we present a novel relightable appearance model\nbased on learnable radiance transfer. Together with global illumination-aware\nspherical harmonics for the diffuse components, we achieve real-time relighting\nwith all-frequency reflections using spherical Gaussians. This appearance model\ncan be efficiently relit under both point light and continuous illumination. We\nfurther improve the fidelity of eye reflections and enable explicit gaze\ncontrol by introducing relightable explicit eye models. Our method outperforms\nexisting approaches without compromising real-time performance. We also\ndemonstrate real-time relighting of avatars on a tethered consumer VR headset,\nshowcasing the efficiency and fidelity of our avatars.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.03704v2"
    },
    {
        "title": "Anatomically Constrained Implicit Face Models",
        "authors": [
            "Prashanth Chandran",
            "Gaspard Zoss"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Coordinate based implicit neural representations have gained rapid popularity\nin recent years as they have been successfully used in image, geometry and\nscene modeling tasks. In this work, we present a novel use case for such\nimplicit representations in the context of learning anatomically constrained\nface models. Actor specific anatomically constrained face models are the state\nof the art in both facial performance capture and performance retargeting.\nDespite their practical success, these anatomical models are slow to evaluate\nand often require extensive data capture to be built. We propose the anatomical\nimplicit face model; an ensemble of implicit neural networks that jointly learn\nto model the facial anatomy and the skin surface with high-fidelity, and can\nreadily be used as a drop in replacement to conventional blendshape models.\nGiven an arbitrary set of skin surface meshes of an actor and only a neutral\nshape with estimated skull and jaw bones, our method can recover a dense\nanatomical substructure which constrains every point on the facial surface. We\ndemonstrate the usefulness of our approach in several tasks ranging from shape\nfitting, shape editing, and performance retargeting.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.07538v1"
    },
    {
        "title": "Shape Reconstruction of Trapezoidal Surfaces",
        "authors": [
            "Arvin Rasoulzadeh",
            "Martin Kilian",
            "Georg Nawratil"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  A smooth T-surface can be thought of as a generalization of a surface of\nrevolution in such a way that the axis of rotation is not fixed at one point\nbut rather traces a smooth path on the base plane. Furthermore, the action, by\nwhich the aforementioned surface is obtained does not need to be merely\nrotation but any ``suitable\" planar equiform transformation applied to the\npoints of a certain smooth profile curve. In analogy to the smooth setting, if\nthe axis footpoints sweep a polyline on the base plane and if the profile curve\nis discretely chosen then a T-hedra (discrete T-surface) with trapezoidal faces\nis obtained. The goal of this article is to reconstruct a T-hedron from an\nalready given point cloud of a T-surface. In doing so, a kinematic approach is\ntaken into account, where the algorithm at first tries to find the\naforementioned axis direction associated with the point cloud. Then the\nalgorithm finds a polygonal path through which the axis footpoint moves.\nFinally, by properly cutting the point cloud with the planes passing through\nthe axis and its footpoints, it reconstructs the surface. The presented method\nis demonstrated on base of examples. From an applied point of view, the\nstraightforwardness of the generation of these surfaces predestines them for\nbuilding and design processes. In fact, one can find many built objects\nbelonging to the sub-classes of T-surfaces such as \\emph{surfaces of\nrevolution} and \\emph{moulding surfaces}. Furthermore, the planarity of the\nfaces of the discrete version paves the way for steel/glass construction in\nindustry. Finally, these surfaces are also suitable for transformable designs\nas they allow an isometric deformation.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.09373v1"
    },
    {
        "title": "RNA: Relightable Neural Assets",
        "authors": [
            "Krishna Mullia",
            "Fujun Luan",
            "Xin Sun",
            "Miloš Hašan"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  High-fidelity 3D assets with materials composed of fibers (including hair),\ncomplex layered material shaders, or fine scattering geometry are ubiquitous in\nhigh-end realistic rendering applications. Rendering such models is\ncomputationally expensive due to heavy shaders and long scattering paths.\nMoreover, implementing the shading and scattering models is non-trivial and has\nto be done not only in the 3D content authoring software (which is necessarily\ncomplex), but also in all downstream rendering solutions. For example, web and\nmobile viewers for complex 3D assets are desirable, but frequently cannot\nsupport the full shading complexity allowed by the authoring application. Our\ngoal is to design a neural representation for 3D assets with complex shading\nthat supports full relightability and full integration into existing renderers.\nWe provide an end-to-end shading solution at the first intersection of a ray\nwith the underlying geometry. All shading and scattering is precomputed and\nincluded in the neural asset; no multiple scattering paths need to be traced,\nand no complex shading models need to be implemented to render our assets,\nbeyond a single neural architecture. We combine an MLP decoder with a feature\ngrid. Shading consists of querying a feature vector, followed by an MLP\nevaluation producing the final reflectance value. Our method provides\nhigh-fidelity shading, close to the ground-truth Monte Carlo estimate even at\nclose-up views. We believe our neural assets could be used in practical\nrenderers, providing significant speed-ups and simplifying renderer\nimplementations.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.09398v2"
    },
    {
        "title": "Iterative Motion Editing with Natural Language",
        "authors": [
            "Purvi Goel",
            "Kuan-Chieh Wang",
            "C. Karen Liu",
            "Kayvon Fatahalian"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Text-to-motion diffusion models can generate realistic animations from text\nprompts, but do not support fine-grained motion editing controls. In this\npaper, we present a method for using natural language to iteratively specify\nlocal edits to existing character animations, a task that is common in most\ncomputer animation workflows. Our key idea is to represent a space of motion\nedits using a set of kinematic motion editing operators (MEOs) whose effects on\nthe source motion is well-aligned with user expectations. We provide an\nalgorithm that leverages pre-existing language models to translate textual\ndescriptions of motion edits into source code for programs that define and\nexecute sequences of MEOs on a source animation. We execute MEOs by first\ntranslating them into keyframe constraints, and then use diffusion-based motion\nmodels to generate output motions that respect these constraints. Through a\nuser study and quantitative evaluation, we demonstrate that our system can\nperform motion edits that respect the animator's editing intent, remain\nfaithful to the original animation (it edits the original animation, but does\nnot dramatically change it), and yield realistic character animation results.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.11538v2"
    },
    {
        "title": "SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance\n  Fields",
        "authors": [
            "Kaichen Zhou",
            "Lanqing Hong",
            "Enze Xie",
            "Yongxin Yang",
            "Zhenguo Li",
            "Wei Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Although significant progress has been made in the field of 2D-based\ninteractive editing, fine-grained 3D-based interactive editing remains\nrelatively unexplored. This limitation can be attributed to two main\nchallenges: the lack of an efficient 3D representation robust to different\nmodifications and the absence of an effective 3D interactive segmentation\nmethod. In this paper, we introduce a novel fine-grained interactive 3D\nsegmentation and editing algorithm with radiance fields, which we refer to as\nSERF. Our method entails creating a neural mesh representation by integrating\nmulti-view algorithms with pre-trained 2D models. Building upon this\nrepresentation, we introduce a novel surface rendering technique that preserves\nlocal information and is robust to deformation. Moreover, this representation\nforms the basis for achieving accurate and interactive 3D segmentation without\nrequiring 3D supervision. Harnessing this representation facilitates a range of\ninteractive 3D editing operations, encompassing tasks such as interactive\ngeometry editing and texture painting. Extensive experiments and visualization\nexamples of editing on both real and synthetic data demonstrate the superiority\nof our method on representation quality and editing ability.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.15856v2"
    },
    {
        "title": "Geometric Guidance for the Deployment of Elastic Geodesic Grids",
        "authors": [
            "Stefan Pillwein",
            "Alexander Hentschel",
            "Markus Lukacevic",
            "Przemyslaw Musialski"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Elastic gridshells are advanced free-form structures enabling curved target\nshapes and material-efficient large spans. This paper focuses on a novel type\nof gridshells recently proposed employing a scissor-like deployment mechanism.\nWhile recent form-finding advancements have produced fascinating outcomes, a\nsignificant challenge arises when architecturally implementing such mechanisms:\nfor the realization of real-world structures, professional FEA is necessary.\nHowever, performing Finite Element simulations of these structures proves\nsurprisingly complex due to the requirement of simulating the deployment -- a\ntask nearly unachievable using uninformed approaches. Therefore, geometric\nguidance of the highly elastic gridshells while simulating the expansion is\nessential. Present solutions to this predicament primarily involve rudimentary\ntrial-and-error methods, suitable only for the most basic shapes. We propose a\nsolution involving the provision of geometric guidance via sequences of linear\ndisplacements synchronized with a universal time parameter. When applied to\nchosen positions, this allows for multi-step gridshell deployment and\nsuccessfully avoids undesirable buckling issues. We conclude with successful\ndemonstrations of our method, anticipating our work to pave the way for further\nquantitative explorations of these intriguing structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.17181v1"
    },
    {
        "title": "A Tool for the Procedural Generation of Shaders using Interactive\n  Evolutionary Algorithms",
        "authors": [
            "Elio Sasso",
            "Daniele Loiacono",
            "Pier Luca Lanzi"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a tool for exploring the design space of shaders using an\ninteractive evolutionary algorithm integrated with the Unity editor, a\nwell-known commercial tool for video game development. Our framework leverages\nthe underlying graph-based representation of recent shader editors and\ninteractive evolution to allow designers to explore several visual options\nstarting from an existing shader. Our framework encodes the graph\nrepresentation of a current shader as a chromosome used to seed the evolution\nof a shader population. It applies graph-based recombination and mutation with\na set of heuristics to create feasible shaders. The framework is an extension\nof the Unity editor; thus, designers with little knowledge of evolutionary\ncomputation (and shader programming) can interact with the underlying\nevolutionary engine using the same visual interface used for working on game\nscenes.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.17587v1"
    },
    {
        "title": "Image Sculpting: Precise Object Editing with 3D Geometry Control",
        "authors": [
            "Jiraphon Yenphraphai",
            "Xichen Pan",
            "Sainan Liu",
            "Daniele Panozzo",
            "Saining Xie"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present Image Sculpting, a new framework for editing 2D images by\nincorporating tools from 3D geometry and graphics. This approach differs\nmarkedly from existing methods, which are confined to 2D spaces and typically\nrely on textual instructions, leading to ambiguity and limited control. Image\nSculpting converts 2D objects into 3D, enabling direct interaction with their\n3D geometry. Post-editing, these objects are re-rendered into 2D, merging into\nthe original image to produce high-fidelity results through a coarse-to-fine\nenhancement process. The framework supports precise, quantifiable, and\nphysically-plausible editing options such as pose editing, rotation,\ntranslation, 3D composition, carving, and serial addition. It marks an initial\nstep towards combining the creative freedom of generative models with the\nprecision of graphics pipelines.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.01702v1"
    },
    {
        "title": "Teaching with a companion: the case of gravity",
        "authors": [
            "Iuliia Zhurakovskaia",
            "Jeanne Vezien",
            "Patrick Bourdot"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Virtual Reality (VR) has repeatedly proven its effectiveness in student\nlearning. However, despite its benefits, the student equipped with a personal\nheadset remains isolated from the real world while immersed in a virtual space\nand the classic student-teacher model of learning is difficult to transpose in\nsuch a situation. This study aims to bring the teacher back into the learning\nprocess when students use a VR headset. We describe the benefits of using a\ncompanion for educational purposes, taking as a test case the concept of\ngravity. We present an experimental setup designed to compare three different\nteaching contexts: with a physically present real teacher, using a live video\nof the teacher, and with a VR avatar of the teacher. We designed and evaluated\nthree scenarios to teach the concept of gravity: an introduction to the concept\nof free fall, a parabolic trajectory workshop and a final exercise combining\nboth approaches. Due to sanitary conditions, only pre-tests are reported. The\nresults showed that the effectiveness of using the VR simulations for learning\nand the self-confidence level of the students increased as well. The interviews\nshow that the students ranked the teaching modes in this order: VR companion\nmode, video communication and real teacher.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.01832v1"
    },
    {
        "title": "FROST-BRDF: A Fast and Robust Optimal Sampling Technique for BRDF\n  Acquisition",
        "authors": [
            "Ehsan Miandji",
            "Tanaboon Tongbuasirilai",
            "Saghi Hajisharif",
            "Behnaz Kavoosighafi",
            "Jonas Unger"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Efficient and accurate BRDF acquisition of real world materials is a\nchallenging research problem that requires sampling millions of incident light\nand viewing directions. To accelerate the acquisition process, one needs to\nfind a minimal set of sampling directions such that the recovery of the full\nBRDF is accurate and robust given such samples. In this paper, we formulate\nBRDF acquisition as a compressed sensing problem, where the sensing operator is\none that performs sub-sampling of the BRDF signal according to a set of optimal\nsample directions. To solve this problem, we propose the Fast and Robust\nOptimal Sampling Technique (FROST) for designing a provably optimal\nsub-sampling operator that places light-view samples such that the recovery\nerror is minimized. FROST casts the problem of designing an optimal\nsub-sampling operator for compressed sensing into a sparse representation\nformulation under the Multiple Measurement Vector (MMV) signal model. The\nproposed reformulation is exact, i.e. without any approximations, hence it\nconverts an intractable combinatorial problem into one that can be solved with\nstandard optimization techniques. As a result, FROST is accompanied by strong\ntheoretical guarantees from the field of compressed sensing. We perform a\nthorough analysis of FROST-BRDF using a 10-fold cross-validation with publicly\navailable BRDF datasets and show significant advantages compared to the\nstate-of-the-art with respect to reconstruction quality. Finally, FROST is\nsimple, both conceptually and in terms of implementation, it produces\nconsistent results at each run, and it is at least two orders of magnitude\nfaster than the prior art.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.07283v1"
    },
    {
        "title": "High-Quality Mesh Blendshape Generation from Face Videos via Neural\n  Inverse Rendering",
        "authors": [
            "Xin Ming",
            "Jiawei Li",
            "Jingwang Ling",
            "Libo Zhang",
            "Feng Xu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Readily editable mesh blendshapes have been widely used in animation\npipelines, while recent advancements in neural geometry and appearance\nrepresentations have enabled high-quality inverse rendering. Building upon\nthese observations, we introduce a novel technique that reconstructs mesh-based\nblendshape rigs from single or sparse multi-view videos, leveraging\nstate-of-the-art neural inverse rendering. We begin by constructing a\ndeformation representation that parameterizes vertex displacements into\ndifferential coordinates with tetrahedral connections, allowing for\nhigh-quality vertex deformation on high-resolution meshes. By constructing a\nset of semantic regulations in this representation, we achieve joint\noptimization of blendshapes and expression coefficients. Furthermore, to enable\na user-friendly multi-view setup with unsynchronized cameras, we propose a\nneural regressor to model time-varying motion parameters. This approach\nimplicitly considers the time difference across multiple cameras, enhancing the\naccuracy of motion modeling. Experiments demonstrate that, with the flexible\ninput of single or sparse multi-view videos, we reconstruct personalized\nhigh-fidelity blendshapes. These blendshapes are both geometrically and\nsemantically accurate, and they are compatible with industrial animation\npipelines. Code and data are available at\nhttps://github.com/grignarder/high-quality-blendshape-generation.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.08398v2"
    },
    {
        "title": "PSAvatar: A Point-based Shape Model for Real-Time Head Avatar Animation\n  with 3D Gaussian Splatting",
        "authors": [
            "Zhongyuan Zhao",
            "Zhenyu Bao",
            "Qing Li",
            "Guoping Qiu",
            "Kanglin Liu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Despite much progress, achieving real-time high-fidelity head avatar\nanimation is still difficult and existing methods have to trade-off between\nspeed and quality. 3DMM based methods often fail to model non-facial structures\nsuch as eyeglasses and hairstyles, while neural implicit models suffer from\ndeformation inflexibility and rendering inefficiency. Although 3D Gaussian has\nbeen demonstrated to possess promising capability for geometry representation\nand radiance field reconstruction, applying 3D Gaussian in head avatar creation\nremains a major challenge since it is difficult for 3D Gaussian to model the\nhead shape variations caused by changing poses and expressions. In this paper,\nwe introduce PSAvatar, a novel framework for animatable head avatar creation\nthat utilizes discrete geometric primitive to create a parametric morphable\nshape model and employs 3D Gaussian for fine detail representation and high\nfidelity rendering. The parametric morphable shape model is a Point-based\nMorphable Shape Model (PMSM) which uses points instead of meshes for 3D\nrepresentation to achieve enhanced representation flexibility. The PMSM first\nconverts the FLAME mesh to points by sampling on the surfaces as well as off\nthe meshes to enable the reconstruction of not only surface-like structures but\nalso complex geometries such as eyeglasses and hairstyles. By aligning these\npoints with the head shape in an analysis-by-synthesis manner, the PMSM makes\nit possible to utilize 3D Gaussian for fine detail representation and\nappearance modeling, thus enabling the creation of high-fidelity avatars. We\nshow that PSAvatar can reconstruct high-fidelity head avatars of a variety of\nsubjects and the avatars can be animated in real-time ($\\ge$ 25 fps at a\nresolution of 512 $\\times$ 512 ).\n",
        "pdf_link": "http://arxiv.org/pdf/2401.12900v5"
    },
    {
        "title": "A real-time rendering method for high albedo anisotropic materials with\n  multiple scattering",
        "authors": [
            "Shun Fang",
            "Xing Feng",
            "Ming Cui"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a neural network-based real-time volume rendering method for\nrealistic and efficient rendering of volumetric media. The traditional volume\nrendering method uses path tracing to solve the radiation transfer equation,\nwhich requires a huge amount of calculation and cannot achieve real-time\nrendering. Therefore, this paper uses neural networks to simulate the iterative\nintegration process of solving the radiative transfer equation to speed up the\nvolume rendering of volume media. Specifically, the paper first performs data\nprocessing on the volume medium to generate a variety of sampling features,\nincluding density features, transmittance features and phase features. The\nhierarchical transmittance fields are fed into a 3D-CNN network to compute more\nimportant transmittance features. Secondly, the diffuse reflection sampling\ntemplate and the highlight sampling template are used to layer the three types\nof sampling features into the network. This method can pay more attention to\nlight scattering, highlights and shadows, and then select important channel\nfeatures through the attention module. Finally, the scattering distribution of\nthe center points of all sampling templates is predicted through the backbone\nneural network. This method can achieve realistic volumetric media rendering\neffects and greatly increase the rendering speed while maintaining rendering\nquality, which is of great significance for real-time rendering applications.\nExperimental results indicate that our method outperforms previous methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.14051v1"
    },
    {
        "title": "A High-Performance SurfaceNets Discrete Isocontouring Algorithm",
        "authors": [
            "Will Schroeder",
            "Spiros Tsalikis",
            "Michael Halle",
            "Sarah Frisken"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Isocontouring is one of the most widely used visualization techniques.\nHowever, many popular contouring algorithms were created prior to the advent of\nubiquitous parallel approaches, such as multi-core, shared memory computing\nsystems. With increasing data sizes and computational loads, it is essential to\nreimagine such algorithms to leverage the increased computing capabilities\navailable today. To this end we have redesigned the SurfaceNets algorithm, a\npowerful technique which is often employed to isocontour non-continuous,\ndiscrete, volumetric scalar fields such as segmentation label maps. Label maps\nare ubiquitous to medical computing and biological analysis, used in\napplications ranging from anatomical atlas creation to brain connectomics. This\nnovel Parallel SurfaceNets algorithm has been redesigned using concepts from\nthe high-performance Flying Edges continuous isocontouring algorrithm. It\nconsists of two basic steps, surface extraction followed by constrained\nsmoothing, parallelized over volume edges and employing a double-buffering\nsmoothing approach to guarantee determinism. The algorithm can extract and\nsmooth multiple segmented objects in a single execution, producing a polygonal\n(triangular/quadrilateral) mesh with points and polygons fully shared between\nneighboring objects. Performance is typically one to two orders of magnitude\nfaster than the current sequential algorithms for discrete isosurface\nextraction on small core-count commodity CPU hardware. We demonstrate the\neffectiveness of the algorithm on five different datasets including human torso\nand brain atlases, mouse brain segmentation, and electron microscopy\nconnectomics. The software is currently available under a permissive, open\nsource license in the VTK visualization system.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.14906v1"
    },
    {
        "title": "FabHacks: Transform Everyday Objects into Functional Fixtures",
        "authors": [
            "Yuxuan Mei",
            "Benjamin Jones",
            "Dan Cascaval",
            "Jennifer Mankoff",
            "Etienne Vouga",
            "Adriana Schulz"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Storage, organizing, and decorating are an important part of home design.\nWhile one can buy commercial items for many of these tasks, this can be costly,\nand re-use is more sustainable. An alternative is a \"home hack\", a functional\nassembly that can be constructed from existing household items. However, coming\nup with such hacks requires combining objects to make a physically valid\ndesign, which might be difficult to test if they are large, require nailing or\nscrewing something to the wall, or the designer has mobility limitations. In\nthis work, we present a design and visualization system for creating workable\nfunctional assemblies, FabHacks, which is based on a solver-aided\ndomain-specific language (S-DSL) FabHaL. By analyzing existing home hacks\nshared online, we create a design abstraction for connecting household items\nusing predefined types of connections. We provide a UI for FabHaL that can be\nused to design assemblies that fulfill a given specification. Our system\nleverages a physics-based solver that takes an assembly design and finds its\nexpected physical configuration. Our validation includes a user study showing\nthat users can create assemblies successfully using our UI and explore a range\nof designs.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15279v1"
    },
    {
        "title": "Democratizing the Creation of Animatable Facial Avatars",
        "authors": [
            "Yilin Zhu",
            "Dalton Omens",
            "Haodi He",
            "Ron Fedkiw"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In high-end visual effects pipelines, a customized (and expensive) light\nstage system is (typically) used to scan an actor in order to acquire both\ngeometry and texture for various expressions. Aiming towards democratization,\nwe propose a novel pipeline for obtaining geometry and texture as well as\nenough expression information to build a customized person-specific animation\nrig without using a light stage or any other high-end hardware (or manual\ncleanup). A key novel idea consists of warping real-world images to align with\nthe geometry of a template avatar and subsequently projecting the warped image\ninto the template avatar's texture; importantly, this allows us to leverage\nbaked-in real-world lighting/texture information in order to create surrogate\nfacial features (and bridge the domain gap) for the sake of geometry\nreconstruction. Not only can our method be used to obtain a neutral expression\ngeometry and de-lit texture, but it can also be used to improve avatars after\nthey have been imported into an animation system (noting that such imports tend\nto be lossy, while also hallucinating various features). Since a default\nanimation rig will contain template expressions that do not correctly\ncorrespond to those of a particular individual, we use a Simon Says approach to\ncapture various expressions and build a person-specific animation rig (that\nmoves like they do). Our aforementioned warping/projection method has high\nenough efficacy to reconstruct geometry corresponding to each expressions.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.16534v1"
    },
    {
        "title": "Saccade-Contingent Rendering",
        "authors": [
            "Yuna Kwak",
            "Eric Penner",
            "Xuan Wang",
            "Mohammad R. Saeedpour-Parizi",
            "Olivier Mercier",
            "Xiuyun Wu",
            "T. Scott Murdison",
            "Phillip Guan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Battery-constrained power consumption, compute limitations, and high frame\nrate requirements in head-mounted displays present unique challenges in the\ndrive to present increasingly immersive and comfortable imagery in virtual\nreality. However, humans are not equally sensitive to all regions of the visual\nfield, and perceptually-optimized rendering techniques are increasingly\nutilized to address these bottlenecks. Many of these techniques are\ngaze-contingent and often render reduced detail away from a user's fixation.\nSuch techniques are dependent on spatio-temporally-accurate gaze tracking and\ncan result in obvious visual artifacts when eye tracking is inaccurate. In this\nwork we present a gaze-contingent rendering technique which only requires\nsaccade detection, bypassing the need for highly-accurate eye tracking. In our\nfirst experiment, we show that visual acuity is reduced for several hundred\nmilliseconds after a saccade. In our second experiment, we use these results to\nreduce the rendered image resolution after saccades in a controlled\npsychophysical setup, and find that observers cannot discriminate between\nsaccade-contingent reduced-resolution rendering and full-resolution rendering.\nFinally, in our third experiment, we introduce a 90 pixels per degree headset\nand validate our saccade-contingent rendering method under typical VR viewing\nconditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.16536v1"
    },
    {
        "title": "StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time\n  Rendering",
        "authors": [
            "Lukas Radl",
            "Michael Steiner",
            "Mathias Parger",
            "Alexander Weinrauch",
            "Bernhard Kerbl",
            "Markus Steinberger"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Gaussian Splatting has emerged as a prominent model for constructing 3D\nrepresentations from images across diverse domains. However, the efficiency of\nthe 3D Gaussian Splatting rendering pipeline relies on several simplifications.\nNotably, reducing Gaussian to 2D splats with a single view-space depth\nintroduces popping and blending artifacts during view rotation. Addressing this\nissue requires accurate per-pixel depth computation, yet a full per-pixel sort\nproves excessively costly compared to a global sort operation. In this paper,\nwe present a novel hierarchical rasterization approach that systematically\nresorts and culls splats with minimal processing overhead. Our software\nrasterizer effectively eliminates popping artifacts and view inconsistencies,\nas demonstrated through both quantitative and qualitative measurements.\nSimultaneously, our method mitigates the potential for cheating view-dependent\neffects with popping, ensuring a more authentic representation. Despite the\nelimination of cheating, our approach achieves comparable quantitative results\nfor test images, while increasing the consistency for novel view synthesis in\nmotion. Due to its design, our hierarchical approach is only 4% slower on\naverage than the original Gaussian Splatting. Notably, enforcing consistency\nenables a reduction in the number of Gaussians by approximately half with\nnearly identical quality and view-consistency. Consequently, rendering\nperformance is nearly doubled, making our approach 1.6x faster than the\noriginal Gaussian Splatting, with a 50% reduction in memory requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00525v3"
    },
    {
        "title": "Polycube Layouts via Iterative Dual Loops",
        "authors": [
            "Maxim Snoep",
            "Bettina Speckmann",
            "Kevin Verbeek"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Polycube layouts for 3D models effectively support a wide variety of\napplications such as hexahedral mesh construction, seamless texture mapping,\nspline fitting, and multi-block grid generation. However, the automated\nconstruction of valid polycube layouts suffers from robustness issues: the\nstate-of-the-art deformation-based methods are not guaranteed to find a valid\nsolution. In this paper we present a novel approach which is guaranteed to\nreturn a valid polycube layout for 3D models of genus 0. Our algorithm is based\non a dual representation of polycubes; we construct polycube layouts by\niteratively adding or removing dual loops. The iterative nature of our\nalgorithm facilitates a seamless trade-off between quality and complexity of\nthe solution. Our method is efficient and can be implemented using\ncomparatively simple algorithmic building blocks. We experimentally compare the\nresults of our algorithm against state-of-the-art methods. Our fully automated\nmethod always produces provably valid polycube layouts whose quality - assessed\nvia the quality of derived hexahedral meshes - is on par with state-of-the-art\ndeformation methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00652v2"
    },
    {
        "title": "Mesh-based Gaussian Splatting for Real-time Large-scale Deformation",
        "authors": [
            "Lin Gao",
            "Jie Yang",
            "Bo-Tao Zhang",
            "Jia-Mu Sun",
            "Yu-Jie Yuan",
            "Hongbo Fu",
            "Yu-Kun Lai"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Neural implicit representations, including Neural Distance Fields and Neural\nRadiance Fields, have demonstrated significant capabilities for reconstructing\nsurfaces with complicated geometry and topology, and generating novel views of\na scene. Nevertheless, it is challenging for users to directly deform or\nmanipulate these implicit representations with large deformations in the\nreal-time fashion. Gaussian Splatting(GS) has recently become a promising\nmethod with explicit geometry for representing static scenes and facilitating\nhigh-quality and real-time synthesis of novel views. However,it cannot be\neasily deformed due to the use of discrete Gaussians and lack of explicit\ntopology. To address this, we develop a novel GS-based method that enables\ninteractive deformation. Our key idea is to design an innovative mesh-based GS\nrepresentation, which is integrated into Gaussian learning and manipulation. 3D\nGaussians are defined over an explicit mesh, and they are bound with each\nother: the rendering of 3D Gaussians guides the mesh face split for adaptive\nrefinement, and the mesh face split directs the splitting of 3D Gaussians.\nMoreover, the explicit mesh constraints help regularize the Gaussian\ndistribution, suppressing poor-quality Gaussians(e.g. misaligned\nGaussians,long-narrow shaped Gaussians), thus enhancing visual quality and\navoiding artifacts during deformation. Based on this representation, we further\nintroduce a large-scale Gaussian deformation technique to enable deformable GS,\nwhich alters the parameters of 3D Gaussians according to the manipulation of\nthe associated mesh. Our method benefits from existing mesh deformation\ndatasets for more realistic data-driven Gaussian deformation. Extensive\nexperiments show that our approach achieves high-quality reconstruction and\neffective deformation, while maintaining the promising rendering results at a\nhigh frame rate(65 FPS on average).\n",
        "pdf_link": "http://arxiv.org/pdf/2402.04796v1"
    },
    {
        "title": "Kinematic Motion Retargeting for Contact-Rich Anthropomorphic\n  Manipulations",
        "authors": [
            "Arjun S. Lakshmipathy",
            "Jessica K. Hodgins",
            "Nancy S. Pollard"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Hand motion capture data is now relatively easy to obtain, even for\ncomplicated grasps; however this data is of limited use without the ability to\nretarget it onto the hands of a specific character or robot. The target hand\nmay differ dramatically in geometry, number of degrees of freedom (DOFs), or\nnumber of fingers. We present a simple, but effective framework capable of\nkinematically retargeting multiple human hand-object manipulations from a\npublicly available dataset to a wide assortment of kinematically and\nmorphologically diverse target hands through the exploitation of contact areas.\nWe do so by formulating the retarget operation as a non-isometric shape\nmatching problem and use a combination of both surface contact and marker data\nto progressively estimate, refine, and fit the final target hand trajectory\nusing inverse kinematics (IK). Foundational to our framework is the\nintroduction of a novel shape matching process, which we show enables\npredictable and robust transfer of contact data over full manipulations while\nproviding an intuitive means for artists to specify correspondences with\nrelatively few inputs. We validate our framework through thirty demonstrations\nacross five different hand shapes and six motions of different objects. We\nadditionally compare our method against existing hand retargeting approaches.\nFinally, we demonstrate our method enabling novel capabilities such as object\nsubstitution and the ability to visualize the impact of design choices over\nfull trajectories.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.04820v1"
    },
    {
        "title": "Squidgets: Sketch-based Widget Design and Direct Manipulation of 3D\n  Scene",
        "authors": [
            "Joonho Kim",
            "Karan Singh"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Squidgets or 'sketch-widgets' is a novel stroke-based UI framework for direct\nscene manipulation. Squidgets is motivated by the observation that sketch\nstrokes comprising visual abstractions of scene elements implicitly provide\nnatural handles for the direct manipulation of scene parameters. Configurations\nof such strokes can further be explicitly drawn by users to author custom\nwidgets associated with scene attributes. Users manipulate a scene by simply\ndrawing strokes: a squidget is selected by partially matching the drawn stroke\nagainst both implicit scene contours and explicitly authored curves, and used\nin-situ to interactively control scene parameters associated with the squidget.\nWe present an implementation of squidgets within the 3D modeling animation\nsystem Maya, and report on an evaluation of squidget creation and manipulation,\nby both casual users and professional artists.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.06795v1"
    },
    {
        "title": "VR-CAD Framework for Parametric Data Modification with a 3D Shape-based\n  Interaction",
        "authors": [
            "Yujiro Okuya",
            "Nicolas Ladeveze",
            "Cédric Fleury",
            "Patrick Bourdot"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this poster, we present a new VR-CAD framework, allowing user to modify\nparametric CAD data with 3D interaction in an immersive environment. With this\nframework, users can implicitly modify parameter values of CAD data with\nco-localized 3D shape-based interaction. This poster describes the system\narchitecture and the interaction technique based on it.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09406v1"
    },
    {
        "title": "MeshAC: A 3D Mesh Generation and Adaptation Package for Multiscale\n  Coupling Methods",
        "authors": [
            "Kejie Fu",
            "Mingjie Liao",
            "Yangshuai Wang",
            "Jianjun Chen",
            "Lei Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper introduces the MeshAC package, which generates three-dimensional\nadaptive meshes tailored for the efficient and robust implementation of\nmultiscale coupling methods. While Delaunay triangulation is commonly used for\nmesh generation across the entire computational domain, generating meshes for\nmultiscale coupling methods is more challenging due to intrinsic discrete\nstructures such as defects, and the need to match these structures to the\ncontinuum domain at the interface. The MeshAC package tackles these challenges\nby generating meshes that align with fine-level discrete structures. It also\nincorporates localized modification and reconstruction operations specifically\ndesigned for interfaces. These enhancements improve both the implementation\nefficiency and the quality of the coupled mesh. Furthermore, MeshAC introduces\na novel adaptive feature that utilizes gradient-based a posteriori error\nestimation, which automatically adjusts the atomistic region and continuum\nmesh, ensuring an optimal balance between accuracy and efficiency. This package\ncan be directly applied to the geometry optimization problems of a/c coupling\nin static mechanics, with potential extensions to many other scenarios. Its\ncapabilities are demonstrated for complex material defects, including straight\nedge dislocation in BCC W and double voids in FCC Cu. These results suggest\nthat MeshAC can be a valuable tool for researchers and practitioners in\ncomputational mechanics.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09446v1"
    },
    {
        "title": "GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians",
        "authors": [
            "Haimin Luo",
            "Min Ouyang",
            "Zijun Zhao",
            "Suyi Jiang",
            "Longwen Zhang",
            "Qixuan Zhang",
            "Wei Yang",
            "Lan Xu",
            "Jingyi Yu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Hairstyle reflects culture and ethnicity at first glance. In the digital era,\nvarious realistic human hairstyles are also critical to high-fidelity digital\nhuman assets for beauty and inclusivity. Yet, realistic hair modeling and\nreal-time rendering for animation is a formidable challenge due to its sheer\nnumber of strands, complicated structures of geometry, and sophisticated\ninteraction with light. This paper presents GaussianHair, a novel explicit hair\nrepresentation. It enables comprehensive modeling of hair geometry and\nappearance from images, fostering innovative illumination effects and dynamic\nanimation capabilities. At the heart of GaussianHair is the novel concept of\nrepresenting each hair strand as a sequence of connected cylindrical 3D\nGaussian primitives. This approach not only retains the hair's geometric\nstructure and appearance but also allows for efficient rasterization onto a 2D\nimage plane, facilitating differentiable volumetric rendering. We further\nenhance this model with the \"GaussianHair Scattering Model\", adept at\nrecreating the slender structure of hair strands and accurately capturing their\nlocal diffuse color in uniform lighting. Through extensive experiments, we\nsubstantiate that GaussianHair achieves breakthroughs in both geometric and\nappearance fidelity, transcending the limitations encountered in\nstate-of-the-art methods for hair reconstruction. Beyond representation,\nGaussianHair extends to support editing, relighting, and dynamic rendering of\nhair, offering seamless integration with conventional CG pipeline workflows.\nComplementing these advancements, we have compiled an extensive dataset of real\nhuman hair, each with meticulously detailed strand geometry, to propel further\nresearch in this field.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.10483v1"
    },
    {
        "title": "Improving Efficiency of Iso-Surface Extraction on Implicit Neural\n  Representations Using Uncertainty Propagation",
        "authors": [
            "Haoyu Li",
            "Han-Wei Shen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Implicit Neural representations (INRs) are widely used for scientific data\nreduction and visualization by modeling the function that maps a spatial\nlocation to a data value. Without any prior knowledge about the spatial\ndistribution of values, we are forced to sample densely from INRs to perform\nvisualization tasks like iso-surface extraction which can be very\ncomputationally expensive. Recently, range analysis has shown promising results\nin improving the efficiency of geometric queries, such as ray casting and\nhierarchical mesh extraction, on INRs for 3D geometries by using arithmetic\nrules to bound the output range of the network within a spatial region.\nHowever, the analysis bounds are often too conservative for complex scientific\ndata. In this paper, we present an improved technique for range analysis by\nrevisiting the arithmetic rules and analyzing the probability distribution of\nthe network output within a spatial region. We model this distribution\nefficiently as a Gaussian distribution by applying the central limit theorem.\nExcluding low probability values, we are able to tighten the output bounds,\nresulting in a more accurate estimation of the value range, and hence more\naccurate identification of iso-surface cells and more efficient iso-surface\nextraction on INRs. Our approach demonstrates superior performance in terms of\nthe iso-surface extraction time on four datasets compared to the original range\nanalysis method and can also be generalized to other geometric query tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.13861v1"
    },
    {
        "title": "Cell-Constrained Particles for Incompressible Fluids",
        "authors": [
            "Zohar Levi"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Incompressibility is a fundamental condition in most fluid models.\nAccumulation of simulation errors violates it and causes volume loss. Past work\nsuggested correction methods to battle it. These methods, however, are\nimperfect and in some cases inadequate. We present a method for fluid\nsimulation that strictly enforces incompressibility based on a grid-related\ndefinition of discrete incompressibility.\n  We formulate a linear programming (LP) problem that bounds the number of\nparticles that end up in each grid cell. A variant of the band method is\noffered for acceleration, which requires special constraints to ensure volume\npreservation. Further acceleration is achieved by simplifying the problem and\nadding a special band correction step that is formulated as a minimum-cost flow\nproblem (MCFP). We also address coupling with solids in our framework and\ndemonstrate advantages over prior work.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.17088v1"
    },
    {
        "title": "Block and Detail: Scaffolding Sketch-to-Image Generation",
        "authors": [
            "Vishnu Sarukkai",
            "Lu Yuan",
            "Mia Tang",
            "Maneesh Agrawala",
            "Kayvon Fatahalian"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce a novel sketch-to-image tool that aligns with the iterative\nrefinement process of artists. Our tool lets users sketch blocking strokes to\ncoarsely represent the placement and form of objects and detail strokes to\nrefine their shape and silhouettes. We develop a two-pass algorithm for\ngenerating high-fidelity images from such sketches at any point in the\niterative process. In the first pass we use a ControlNet to generate an image\nthat strictly follows all the strokes (blocking and detail) and in the second\npass we add variation by renoising regions surrounding blocking strokes. We\nalso present a dataset generation scheme that, when used to train a ControlNet\narchitecture, allows regions that do not contain strokes to be interpreted as\nnot-yet-specified regions rather than empty space. We show that this\npartial-sketch-aware ControlNet can generate coherent elements from partial\nsketches that only contain a small number of strokes. The high-fidelity images\nproduced by our approach serve as scaffolds that can help the user adjust the\nshape and proportions of objects or add additional elements to the composition.\nWe demonstrate the effectiveness of our approach with a variety of examples and\nevaluative comparisons. Quantitatively, evaluative user feedback indicates that\nnovice viewers prefer the quality of images from our algorithm over a baseline\nScribble ControlNet for 84% of the pairs and found our images had less\ndistortion in 81% of the pairs.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.18116v2"
    },
    {
        "title": "Development of Context-Sensitive Formulas to Obtain Constant Luminance\n  Perception for a Foreground Object in Front of Backgrounds of Varying\n  Luminance",
        "authors": [
            "Ergun Akleman",
            "Bekir Tevfik Akgun",
            "Adil Alpkocak"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this article, we present a framework for developing context-sensitive\nluminance correction formulas that can produce constant luminance perception\nfor foreground objects. Our formulas make the foreground object slightly\ntranslucent to mix with the blurred version of the background. This mix can\nquickly produce any desired illusion of luminance in foreground objects based\non the luminance of the background. The translucency formula has only one\nparameter; the relative size of the foreground object, which is a number\nbetween zero and one. We have identified the general structure of the\ntranslucency formulas as a power function of the relative size of the\nforeground object. We have implemented a web-based interactive program in\nShadertoy. Using this program, we determined the coefficients of the polynomial\nexponents of the power function. To intuitively control the coefficients of the\npolynomial functions, we have used a B\\'{e}zier form. Our final translucency\nformula uses a quadratic polynomial and requires only three coefficients. We\nalso identified a simpler affine formula, which requires only two coefficients.\nWe made our program publicly available in Shadertoy so that anyone can access\nand improve it. In this article, we also explain how to intuitively change the\npolynomial part of the formula. Using our explanation, users change the\npolynomial part of the formula to obtain their own perceptively constant\nluminance. This can be used as a crowd-sourcing experiment for further\nimprovement of the formula.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.18288v1"
    },
    {
        "title": "Projection Mapping under Environmental Lighting by Replacing Room Lights\n  with Heterogeneous Projectors",
        "authors": [
            "Masaki Takeuchi",
            "Hiroki Kusuyama",
            "Daisuke Iwai",
            "Kosuke Sato"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Projection mapping (PM) is a technique that enhances the appearance of\nreal-world surfaces using projected images, enabling multiple people to view\naugmentations simultaneously, thereby facilitating communication and\ncollaboration. However, PM typically requires a dark environment to achieve\nhigh-quality projections, limiting its practicality. In this paper, we overcome\nthis limitation by replacing conventional room lighting with heterogeneous\nprojectors. These projectors replicate environmental lighting by selectively\nilluminating the scene, excluding the projection target. Our contributions\ninclude a distributed projector optimization framework designed to effectively\nreplicate environmental lighting and the incorporation of a large-aperture\nprojector, in addition to standard projectors, to reduce high-luminance emitted\nrays and hard shadows -- undesirable factors for collaborative tasks in PM. We\nconducted a series of quantitative and qualitative experiments, including user\nstudies, to validate our approach. Our findings demonstrate that our\nprojector-based lighting system significantly enhances the contrast and realism\nof PM results even under environmental lighting compared to typical lights.\nFurthermore, our method facilitates a substantial shift in the perceived color\nmode from the undesirable aperture-color mode, where observers perceive the\nprojected object as self-luminous, to the surface-color mode in PM.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.02547v1"
    },
    {
        "title": "Towards Geometric-Photometric Joint Alignment for Facial Mesh\n  Registration",
        "authors": [
            "Xizhi Wang",
            "Yaxiong Wang",
            "Mengjian Li"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper presents a Geometric-Photometric Joint Alignment(GPJA) method, for\naccurately aligning human expressions by combining geometry and photometric\ninformation. Common practices for registering human heads typically involve\naligning landmarks with facial template meshes using geometry processing\napproaches, but often overlook photometric consistency. GPJA overcomes this\nlimitation by leveraging differentiable rendering to align vertices with target\nexpressions, achieving joint alignment in geometry and photometric appearances\nautomatically, without the need for semantic annotation or aligned meshes for\ntraining. It features a holistic rendering alignment strategy and a multiscale\nregularized optimization for robust and fast convergence. The method utilizes\nderivatives at vertex positions for supervision and employs a gradient-based\nalgorithm which guarantees smoothness and avoids topological defects during the\ngeometry evolution. Experimental results demonstrate faithful alignment under\nvarious expressions, surpassing the conventional ICP-based methods and the\nstate-of-the-art deep learning based method. In practical, our method enhances\nthe efficiency of obtaining topology-consistent face models from multi-view\nstereo facial scanning.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.02629v1"
    },
    {
        "title": "Medial Parametrization of Arbitrary Planar Compact Domains with Dipoles",
        "authors": [
            "Vinayak Krishnamurthy",
            "Ergun Akleman"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present medial parametrization, a new approach to parameterizing any\ncompact planar domain bounded by simple closed curves. The basic premise behind\nour proposed approach is to use two close Voronoi sites, which we call dipoles,\nto construct and reconstruct an approximate piecewise-linear version of the\noriginal boundary and medial axis through Voronoi tessellation. The boundaries\nand medial axes of such planar compact domains offer a natural way to describe\nthe domain's interior. Any compact planar domain is homeomorphic to a compact\nunit circular disk admits a natural parameterization isomorphic to the polar\nparametrization of the disk. Specifically, the medial axis and the boundary\ngeneralize the radial and angular parameters, respectively. In this paper, we\npresent a simple algorithm that puts these principles into practice. The\nalgorithm is based on the simultaneous re-creation of the boundaries of the\ndomain and its medial axis using Voronoi tessellation. This simultaneous\nre-creation provides partitions of the domain into a set of \"skinny\" convex\npolygons wherein each polygon is essentially a subset of the medial edges\n(which we call the spine) connected to the boundary through exactly two\nstraight edges (which we call limbs). This unique structure enables us to\nconvert the original Voronoi tessellation into quadrilaterals and triangles (at\nthe poles of the medial axis) neatly ordered along the domain boundary, thereby\nallowing proper parametrization of the domain. Our approach is agnostic to the\nnumber of holes and disconnected components bounding the domain. We investigate\nthe efficacy of our concept and algorithm through several examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.03622v2"
    },
    {
        "title": "SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded\n  Gaussian Splatting",
        "authors": [
            "Zhijing Shao",
            "Zhaolong Wang",
            "Zhuang Li",
            "Duotun Wang",
            "Xiangru Lin",
            "Yu Zhang",
            "Mingming Fan",
            "Zeyu Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present SplattingAvatar, a hybrid 3D representation of photorealistic\nhuman avatars with Gaussian Splatting embedded on a triangle mesh, which\nrenders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We\ndisentangle the motion and appearance of a virtual human with explicit mesh\ngeometry and implicit appearance modeling with Gaussian Splatting. The\nGaussians are defined by barycentric coordinates and displacement on a triangle\nmesh as Phong surfaces. We extend lifted optimization to simultaneously\noptimize the parameters of the Gaussians while walking on the triangle mesh.\nSplattingAvatar is a hybrid representation of virtual humans where the mesh\nrepresents low-frequency motion and surface deformation, while the Gaussians\ntake over the high-frequency geometry and detailed appearance. Unlike existing\ndeformation methods that rely on an MLP-based linear blend skinning (LBS) field\nfor motion, we control the rotation and translation of the Gaussians directly\nby mesh, which empowers its compatibility with various animation techniques,\ne.g., skeletal animation, blend shapes, and mesh editing. Trainable from\nmonocular videos for both full-body and head avatars, SplattingAvatar shows\nstate-of-the-art rendering quality across multiple datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.05087v1"
    },
    {
        "title": "HeadEvolver: Text to Head Avatars via Expressive and\n  Attribute-Preserving Mesh Deformation",
        "authors": [
            "Duotun Wang",
            "Hengyu Meng",
            "Zeyu Cai",
            "Zhijing Shao",
            "Qianxi Liu",
            "Lin Wang",
            "Mingming Fan",
            "Xiaohang Zhan",
            "Zeyu Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Current text-to-avatar methods often rely on implicit representations (e.g.,\nNeRF, SDF, and DMTet), leading to 3D content that artists cannot easily edit\nand animate in graphics software. This paper introduces a novel framework for\ngenerating stylized head avatars from text guidance, which leverages locally\nlearnable mesh deformation and 2D diffusion priors to achieve high-quality\ndigital assets for attribute-preserving manipulation. Given a template mesh,\nour method represents mesh deformation with per-face Jacobians and adaptively\nmodulates local deformation using a learnable vector field. This vector field\nenables anisotropic scaling while preserving the rotation of vertices, which\ncan better express identity and geometric details. We employ landmark- and\ncontour-based regularization terms to balance the expressiveness and\nplausibility of generated avatars from multiple views without relying on any\nspecific shape prior. Our framework can generate realistic shapes and textures\nthat can be further edited via text, while supporting seamless editing using\nthe preserved attributes from the template mesh, such as 3DMM parameters,\nblendshapes, and UV coordinates. Extensive experiments demonstrate that our\nframework can generate diverse and expressive head avatars with high-quality\nmeshes that artists can easily manipulate in graphics software, facilitating\ndownstream applications such as efficient asset creation and animation with\npreserved attributes.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.09326v3"
    },
    {
        "title": "Bridging 3D Gaussian and Mesh for Freeview Video Rendering",
        "authors": [
            "Yuting Xiao",
            "Xuan Wang",
            "Jiafei Li",
            "Hongrui Cai",
            "Yanbo Fan",
            "Nan Xue",
            "Minghui Yang",
            "Yujun Shen",
            "Shenghua Gao"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This is only a preview version of GauMesh. Recently, primitive-based\nrendering has been proven to achieve convincing results in solving the problem\nof modeling and rendering the 3D dynamic scene from 2D images. Despite this, in\nthe context of novel view synthesis, each type of primitive has its inherent\ndefects in terms of representation ability. It is difficult to exploit the mesh\nto depict the fuzzy geometry. Meanwhile, the point-based splatting (e.g. the 3D\nGaussian Splatting) method usually produces artifacts or blurry pixels in the\narea with smooth geometry and sharp textures. As a result, it is difficult,\neven not impossible, to represent the complex and dynamic scene with a single\ntype of primitive. To this end, we propose a novel approach, GauMesh, to bridge\nthe 3D Gaussian and Mesh for modeling and rendering the dynamic scenes. Given a\nsequence of tracked mesh as initialization, our goal is to simultaneously\noptimize the mesh geometry, color texture, opacity maps, a set of 3D Gaussians,\nand the deformation field. At a specific time, we perform $\\alpha$-blending on\nthe RGB and opacity values based on the merged and re-ordered z-buffers from\nmesh and 3D Gaussian rasterizations. This produces the final rendering, which\nis supervised by the ground-truth image. Experiments demonstrate that our\napproach adapts the appropriate type of primitives to represent the different\nparts of the dynamic scene and outperforms all the baseline methods in both\nquantitative and qualitative comparisons without losing render speed.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.11453v1"
    },
    {
        "title": "View-Consistent 3D Editing with Gaussian Splatting",
        "authors": [
            "Yuxuan Wang",
            "Xuanyu Yi",
            "Zike Wu",
            "Na Zhao",
            "Long Chen",
            "Hanwang Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,\noffering efficient, high-fidelity rendering and enabling precise local\nmanipulations. Currently, diffusion-based 2D editing models are harnessed to\nmodify multi-view rendered images, which then guide the editing of 3DGS models.\nHowever, this approach faces a critical issue of multi-view inconsistency,\nwhere the guidance images exhibit significant discrepancies across views,\nleading to mode collapse and visual artifacts of 3DGS. To this end, we\nintroduce View-consistent Editing (VcEdit), a novel framework that seamlessly\nincorporates 3DGS into image editing processes, ensuring multi-view consistency\nin edited guidance images and effectively mitigating mode collapse issues.\nVcEdit employs two innovative consistency modules: the Cross-attention\nConsistency Module and the Editing Consistency Module, both designed to reduce\ninconsistencies in edited images. By incorporating these consistency modules\ninto an iterative pattern, VcEdit proficiently resolves the issue of multi-view\ninconsistency, facilitating high-quality 3DGS editing across a diverse range of\nscenes. Further video results are shown in http://vcedit.github.io.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.11868v9"
    },
    {
        "title": "ProgrammableGrass: A Shape-Changing Artificial Grass Display Adapted for\n  Dynamic and Interactive Display Features",
        "authors": [
            "Kojiro Tanaka",
            "Akito Mizuno",
            "Toranosuke Kato",
            "Masahiko Mikawa",
            "Makoto Fujisawa"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  There are various proposals for employing grass materials as a green\nlandscape-friendly display. However, it is difficult for current techniques to\ndisplay smooth animations using 8-bit images and to adjust display resolution,\nsimilar to conventional displays. We present ProgrammableGrass, an artificial\ngrass display with scalable resolution, capable of swiftly controlling grass\ncolor at 8-bit levels. This grass display can control grass colors linearly at\nthe 8-bit level, similar to an LCD display, and can also display not only\n8-bit-based images but also videos. This display enables pixel-by-pixel color\ntransitions from yellow to green using fixed-length yellow and\nadjustable-length green grass. We designed a grass module that can be connected\nto other modules. Utilizing a proportional derivative control, the grass colors\nare manipulated to display animations at approximately 10 [fps]. Since the\nrelationship between grass lengths and colors is nonlinear, we developed a\ncalibration system for ProgrammableGrass. We revealed that this calibration\nsystem allows ProgrammableGrass to linearly control grass colors at 8-bit\nlevels through experiments under multiple conditions. Lastly, we demonstrate\nProgrammableGrass to show smooth animations with 8-bit grayscale images.\nMoreover, we show several application examples to illustrate the potential of\nProgrammableGrass. With the advancement of this technology, users will be able\nto treat grass as a green-based interactive display device.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.12387v1"
    },
    {
        "title": "A Physics-embedded Deep Learning Framework for Cloth Simulation",
        "authors": [
            "Zhiwei Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Delicate cloth simulations have long been desired in computer graphics.\nVarious methods were proposed to improve engaged force interactions, collision\nhandling, and numerical integrations. Deep learning has the potential to\nachieve fast and real-time simulation, but common neural network structures\noften demand many parameters to capture cloth dynamics. This paper proposes a\nphysics-embedded learning framework that directly encodes physical features of\ncloth simulation. The convolutional neural network is used to represent spatial\ncorrelations of the mass-spring system, after which three branches are designed\nto learn linear, nonlinear, and time derivate features of cloth physics. The\nframework can also integrate with other external forces and collision handling\nthrough either traditional simulators or sub neural networks. The model is\ntested across different cloth animation cases, without training with new data.\nAgreement with baselines and predictive realism successfully validate its\ngeneralization ability. Inference efficiency of the proposed model also defeats\ntraditional physics simulation. This framework is also designed to easily\nintegrate with other visual refinement techniques like wrinkle carving, which\nleaves significant chances to incorporate prevailing macing learning techniques\nin 3D cloth amination.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.12820v3"
    },
    {
        "title": "GPolylla: Fully GPU-accelerated polygonal mesh generator",
        "authors": [
            "Sergio Salinas-Fernández",
            "Nancy Hitschfeld-Kahler",
            "Roberto Carrasco"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This work presents a fully GPU-accelerated algorithm for the polygonal mesh\ngenerator known as Polylla. Polylla is a tri-to-polygon mesh generator, which\nbenefits from the half-edge data structure to manage any polygonal shape. The\nproposed parallel algorithm introduces a novel approach to modify\ntriangulations to get polygonal meshes using the half-edge data structure in\nparallel on the GPU. By changing the adjacency values of each half-edge, the\nalgorithm accomplish to unlink half-edges that are not used in the new\npolygonal mesh without the need neither removing nor allocating new memory in\nthe GPU. The experimental results show a speedup, reaching up to $\\times 83.2$\nwhen compared to the CPU sequential implementation. Additionally, the speedup\nis $\\times 746.8$ when the cost of copying the data structure from the host\ndevice and back is not included.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.14723v1"
    },
    {
        "title": "Distributed Simulation of Large Multi-body Systems",
        "authors": [
            "Manas Kale",
            "Paul G. Kry"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a technique designed for parallelizing large rigid body\nsimulations, capable of exploiting multiple CPU cores within a computer and\nacross a network. Our approach can be applied to simulate both unilateral and\nbilateral constraints, requiring straightforward modifications to the\nunderlying physics engine. Starting from an approximate partitioning, we\nidentify interface bodies and add them to overlapping sets such that they are\nsimulated by multiple workers. At each timestep, we blend the states of overlap\nbodies using weights based on graph geodesic distances within the constraint\ngraph. The use of overlap simulation also allows us to perform load balancing\nusing efficient local evaluations of the constraint graph. We demonstrate our\ntechnique's scalability and load-balancing capabilities using several\nlarge-scale scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.17261v1"
    },
    {
        "title": "Predicting Perceived Gloss: Do Weak Labels Suffice?",
        "authors": [
            "Julia Guerrero-Viu",
            "J. Daniel Subias",
            "Ana Serrano",
            "Katherine R. Storrs",
            "Roland W. Fleming",
            "Belen Masia",
            "Diego Gutierrez"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Estimating perceptual attributes of materials directly from images is a\nchallenging task due to their complex, not fully-understood interactions with\nexternal factors, such as geometry and lighting. Supervised deep learning\nmodels have recently been shown to outperform traditional approaches, but rely\non large datasets of human-annotated images for accurate perception\npredictions. Obtaining reliable annotations is a costly endeavor, aggravated by\nthe limited ability of these models to generalise to different aspects of\nappearance. In this work, we show how a much smaller set of human annotations\n(\"strong labels\") can be effectively augmented with automatically derived \"weak\nlabels\" in the context of learning a low-dimensional image-computable gloss\nmetric. We evaluate three alternative weak labels for predicting human gloss\nperception from limited annotated data. Incorporating weak labels enhances our\ngloss prediction beyond the current state of the art. Moreover, it enables a\nsubstantial reduction in human annotation costs without sacrificing accuracy,\nwhether working with rendered images or real photographs.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.17672v1"
    },
    {
        "title": "InstructBrush: Learning Attention-based Instruction Optimization for\n  Image Editing",
        "authors": [
            "Ruoyu Zhao",
            "Qingnan Fan",
            "Fei Kou",
            "Shuai Qin",
            "Hong Gu",
            "Wei Wu",
            "Pengcheng Xu",
            "Mingrui Zhu",
            "Nannan Wang",
            "Xinbo Gao"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In recent years, instruction-based image editing methods have garnered\nsignificant attention in image editing. However, despite encompassing a wide\nrange of editing priors, these methods are helpless when handling editing tasks\nthat are challenging to accurately describe through language. We propose\nInstructBrush, an inversion method for instruction-based image editing methods\nto bridge this gap. It extracts editing effects from exemplar image pairs as\nediting instructions, which are further applied for image editing. Two key\ntechniques are introduced into InstructBrush, Attention-based Instruction\nOptimization and Transformation-oriented Instruction Initialization, to address\nthe limitations of the previous method in terms of inversion effects and\ninstruction generalization. To explore the ability of instruction inversion\nmethods to guide image editing in open scenarios, we establish a\nTransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set\nof scenes and editing types. The creation of this benchmark paves the way for\nfurther exploration of instruction inversion. Quantitatively and qualitatively,\nour approach achieves superior performance in editing and is more semantically\nconsistent with the target editing effects.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.18660v1"
    },
    {
        "title": "Fluid Implicit Particle Simulation for CPU and GPU",
        "authors": [
            "Pedro Centeno",
            "João Madeiras Pereira"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  One of the current challenges in physically-based simulations, and, more\nspecifically, fluid simulations, is to produce visually appealing results at\ninteractive rates, capable of being used in multiple forms of media. In recent\ntimes, a lot of effort has been made with regards to this with the use of\nmulti-core architectures, as many of the computations involved in the\nalgorithms for these simulations are very well suited for these architectures.\nAlthough there is a considerable amount of works regarding acceleration\ntechniques in this field, there is yet room to further explore and analyze some\nof them. To investigate this problem, we surveyed the topic of fluid\nsimulations and some of the recent contributions towards this field.\nAdditionally, we implemented two versions of a fluid simulation algorithm, one\non the CPU and the other on the GPU using NVIDIA's CUDA framework, with the\nintent of gaining a better understanding of the effort needed to move these\nsimulations to a multi-core architecture and the performance gains that we get\nwith it.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.01931v2"
    },
    {
        "title": "PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual\n  Observations",
        "authors": [
            "Yang Zheng",
            "Qingqing Zhao",
            "Guandao Yang",
            "Wang Yifan",
            "Donglai Xiang",
            "Florian Dubost",
            "Dmitry Lagun",
            "Thabo Beeler",
            "Federico Tombari",
            "Leonidas Guibas",
            "Gordon Wetzstein"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Modeling and rendering photorealistic avatars is of crucial importance in\nmany applications. Existing methods that build a 3D avatar from visual\nobservations, however, struggle to reconstruct clothed humans. We introduce\nPhysAvatar, a novel framework that combines inverse rendering with inverse\nphysics to automatically estimate the shape and appearance of a human from\nmulti-view video data along with the physical parameters of the fabric of their\nclothes. For this purpose, we adopt a mesh-aligned 4D Gaussian technique for\nspatio-temporal mesh tracking as well as a physically based inverse renderer to\nestimate the intrinsic material properties. PhysAvatar integrates a physics\nsimulator to estimate the physical parameters of the garments using\ngradient-based optimization in a principled manner. These novel capabilities\nenable PhysAvatar to create high-quality novel-view renderings of avatars\ndressed in loose-fitting clothes under motions and lighting conditions not seen\nin the training data. This marks a significant advancement towards modeling\nphotorealistic digital humans using physically based inverse rendering with\nphysics in the loop. Our project website is at:\nhttps://qingqing-zhao.github.io/PhysAvatar\n",
        "pdf_link": "http://arxiv.org/pdf/2404.04421v2"
    },
    {
        "title": "Histropy: A Computer Program for Quantifications of Histograms of 2D\n  Gray-scale Images",
        "authors": [
            "Sagarika Menon",
            "Peter Moeck"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The computer program \"Histropy\" is an interactive Python program for the\nquantification of selected features of two-dimensional (2D) images/patterns (in\neither JPG/JPEG, PNG, GIF, BMP, or baseline TIF/TIFF formats) using\ncalculations based on the pixel intensities in this data, their histograms, and\nuser-selected sections of those histograms. The histograms of these images\ndisplay pixel-intensity values along the x-axis (of a 2D Cartesian plot), with\nthe frequency of each intensity value within the image represented along the\ny-axis. The images need to be of 8-bit or 16-bit information depth and can be\nof arbitrary size. Histropy generates an image's histogram surrounded by a\ngraphical user interface that allows one to select any range of image-pixel\nintensity levels, i.e. sections along the histograms' x-axis, using either the\ncomputer mouse or numerical text entries. The program subsequently calculates\nthe (so-called Monkey Model) Shannon entropy and root-mean-square contrast for\nthe selected section and displays them as part of what we call a\n\"histogram-workspace-plot.\" To support the visual identification of small peaks\nin the histograms, the user can switch between a linear and log-base-10 display\nscale for the y-axis of the histograms. Pixel intensity data from different\nimages can be overlaid onto the same histogram-workspace-plot for visual\ncomparisons. The visual outputs of the program can be saved as\nhistogram-workspace-plots in the PNG format for future usage. The source code\nof the program and a brief user manual are published in the supporting\nmaterials as well as on GitHub. Instead of taking only 2D images as inputs, the\nprogram's functionality could be extended by a few lines of code to other\npotential uses employing data tables with one or two dimensions in the CSV\nformat.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.13497v2"
    },
    {
        "title": "Interactive3D: Create What You Want by Interactive 3D Generation",
        "authors": [
            "Shaocong Dong",
            "Lihe Ding",
            "Zhanpeng Huang",
            "Zibin Wang",
            "Tianfan Xue",
            "Dan Xu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  3D object generation has undergone significant advancements, yielding\nhigh-quality results. However, fall short of achieving precise user control,\noften yielding results that do not align with user expectations, thus limiting\ntheir applicability. User-envisioning 3D object generation faces significant\nchallenges in realizing its concepts using current generative models due to\nlimited interaction capabilities. Existing methods mainly offer two approaches:\n(i) interpreting textual instructions with constrained controllability, or (ii)\nreconstructing 3D objects from 2D images. Both of them limit customization to\nthe confines of the 2D reference and potentially introduce undesirable\nartifacts during the 3D lifting process, restricting the scope for direct and\nversatile 3D modifications. In this work, we introduce Interactive3D, an\ninnovative framework for interactive 3D generation that grants users precise\ncontrol over the generative process through extensive 3D interaction\ncapabilities. Interactive3D is constructed in two cascading stages, utilizing\ndistinct 3D representations. The first stage employs Gaussian Splatting for\ndirect user interaction, allowing modifications and guidance of the generative\ndirection at any intermediate step through (i) Adding and Removing components,\n(ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv)\nSemantic Editing. Subsequently, the Gaussian splats are transformed into\nInstantNGP. We introduce a novel (v) Interactive Hash Refinement module to\nfurther add details and extract the geometry in the second stage. Our\nexperiments demonstrate that Interactive3D markedly improves the\ncontrollability and quality of 3D generation. Our project webpage is available\nat \\url{https://interactive-3d.github.io/}.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.16510v1"
    },
    {
        "title": "A Neural-Network-Based Approach for Loose-Fitting Clothing",
        "authors": [
            "Yongxu Jin",
            "Dalton Omens",
            "Zhenglin Geng",
            "Joseph Teran",
            "Abishek Kumar",
            "Kenji Tashiro",
            "Ronald Fedkiw"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Since loose-fitting clothing contains dynamic modes that have proven to be\ndifficult to predict via neural networks, we first illustrate how to coarsely\napproximate these modes with a real-time numerical algorithm specifically\ndesigned to mimic the most important ballistic features of a classical\nnumerical simulation. Although there is some flexibility in the choice of the\nnumerical algorithm used as a proxy for full simulation, it is essential that\nthe stability and accuracy be independent from any time step restriction or\nsimilar requirements in order to facilitate real-time performance. In order to\nreduce the number of degrees of freedom that require approximations to their\ndynamics, we simulate rigid frames and use skinning to reconstruct a rough\napproximation to a desirable mesh; as one might expect, neural-network-based\nskinning seems to perform better than linear blend skinning in this scenario.\nImproved high frequency deformations are subsequently added to the skinned mesh\nvia a quasistatic neural network (QNN). In contrast to recurrent neural\nnetworks that require a plethora of training data in order to adequately\ngeneralize to new examples, QNNs perform well with significantly less training\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.16896v1"
    },
    {
        "title": "Differentiable Geodesic Distance for Intrinsic Minimization on Triangle\n  Meshes",
        "authors": [
            "Yue Li",
            "Logan Numerow",
            "Bernhard Thomaszewski",
            "Stelian Coros"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Computing intrinsic distances on discrete surfaces is at the heart of many\nminimization problems in geometry processing and beyond. Solving these problems\nis extremely challenging as it demands the computation of on-surface distances\nalong with their derivatives. We present a novel approach for intrinsic\nminimization of distance-based objectives defined on triangle meshes. Using a\nvariational formulation of shortest-path geodesics, we compute first and\nsecond-order distance derivatives based on the implicit function theorem, thus\nopening the door to efficient Newton-type minimization solvers. We demonstrate\nour differentiable geodesic distance framework on a wide range of examples,\nincluding geodesic networks and membranes on surfaces of arbitrary genus,\ntwo-way coupling between hosting surface and embedded system, differentiable\ngeodesic Voronoi diagrams, and efficient computation of Karcher means on\ncomplex shapes. Our analysis shows that second-order descent methods based on\nour differentiable geodesics outperform existing first-order and quasi-Newton\nmethods by large margins.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.18610v1"
    },
    {
        "title": "Differentiable Voronoi Diagrams for Simulation of Cell-Based Mechanical\n  Systems",
        "authors": [
            "Logan Numerow",
            "Yue Li",
            "Stelian Coros",
            "Bernhard Thomaszewski"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Navigating topological transitions in cellular mechanical systems is a\nsignificant challenge for existing simulation methods. While abstract models\nlack predictive capabilities at the cellular level, explicit network\nrepresentations struggle with topology changes, and per-cell representations\nare computationally too demanding for large-scale simulations. To address these\nchallenges, we propose a novel cell-centered approach based on differentiable\nVoronoi diagrams. Representing each cell with a Voronoi site, our method\ndefines shape and topology of the interface network implicitly. In this way, we\nsubstantially reduce the number of problem variables, eliminate the need for\nexplicit contact handling, and ensure continuous geometry changes during\ntopological transitions. Closed-form derivatives of network positions\nfacilitate simulation with Newton-type methods for a wide range of per-cell\nenergies. Finally, we extend our differentiable Voronoi diagrams to enable\ncoupling with arbitrary rigid and deformable boundaries. We apply our approach\nto a diverse set of examples, highlighting splitting and merging of cells as\nwell as neighborhood changes. We illustrate applications to inverse problems by\nmatching soap foam simulations to real-world images. Comparative analysis with\nexplicit cell models reveals that our method achieves qualitatively comparable\nresults at significantly faster computation times.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.18629v1"
    },
    {
        "title": "3D Gaussian Blendshapes for Head Avatar Animation",
        "authors": [
            "Shengjie Ma",
            "Yanlin Weng",
            "Tianjia Shao",
            "Kun Zhou"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce 3D Gaussian blendshapes for modeling photorealistic head\navatars. Taking a monocular video as input, we learn a base head model of\nneutral expression, along with a group of expression blendshapes, each of which\ncorresponds to a basis expression in classical parametric face models. Both the\nneutral model and expression blendshapes are represented as 3D Gaussians, which\ncontain a few properties to depict the avatar appearance. The avatar model of\nan arbitrary expression can be effectively generated by combining the neutral\nmodel and expression blendshapes through linear blending of Gaussians with the\nexpression coefficients. High-fidelity head avatar animations can be\nsynthesized in real time using Gaussian splatting. Compared to state-of-the-art\nmethods, our Gaussian blendshape representation better captures high-frequency\ndetails exhibited in input video, and achieves superior rendering performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.19398v2"
    },
    {
        "title": "TexSliders: Diffusion-Based Texture Editing in CLIP Space",
        "authors": [
            "Julia Guerrero-Viu",
            "Milos Hasan",
            "Arthur Roullier",
            "Midhun Harikumar",
            "Yiwei Hu",
            "Paul Guerrero",
            "Diego Gutierrez",
            "Belen Masia",
            "Valentin Deschaintre"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Generative models have enabled intuitive image creation and manipulation\nusing natural language. In particular, diffusion models have recently shown\nremarkable results for natural image editing. In this work, we propose to apply\ndiffusion techniques to edit textures, a specific class of images that are an\nessential part of 3D content creation pipelines. We analyze existing editing\nmethods and show that they are not directly applicable to textures, since their\ncommon underlying approach, manipulating attention maps, is unsuitable for the\ntexture domain. To address this, we propose a novel approach that instead\nmanipulates CLIP image embeddings to condition the diffusion generation. We\ndefine editing directions using simple text prompts (e.g., \"aged wood\" to \"new\nwood\") and map these to CLIP image embedding space using a texture prior, with\na sampling-based approach that gives us identity-preserving directions in CLIP\nspace. To further improve identity preservation, we project these directions to\na CLIP subspace that minimizes identity variations resulting from entangled\ntexture attributes. Our editing pipeline facilitates the creation of arbitrary\nsliders using natural language prompts only, with no ground-truth annotated\ndata necessary.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.00672v1"
    },
    {
        "title": "DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian\n  Representation",
        "authors": [
            "Sitian Shen",
            "Jing Xu",
            "Yuheng Yuan",
            "Xingyi Yang",
            "Qiuhong Shen",
            "Xinchao Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  User-friendly 3D object editing is a challenging task that has attracted\nsignificant attention recently. The limitations of direct 3D object editing\nwithout 2D prior knowledge have prompted increased attention towards utilizing\n2D generative models for 3D editing. While existing methods like Instruct\nNeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly\ndue to semantic guided editing. In the realm of 3D representation, 3D Gaussian\nSplatting emerges as a promising approach for its efficiency and natural\nexplicit property, facilitating precise editing tasks. Building upon these\ninsights, we propose DragGaussian, a 3D object drag-editing framework based on\n3D Gaussian Splatting, leveraging diffusion models for interactive image\nediting with open-vocabulary input. This framework enables users to perform\ndrag-based editing on pre-trained 3D Gaussian object models, producing modified\n2D images through multi-view consistent editing. Our contributions include the\nintroduction of a new task, the development of DragGaussian for interactive\npoint-based 3D editing, and comprehensive validation of its effectiveness\nthrough qualitative and quantitative experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.05800v1"
    },
    {
        "title": "Diffusion-based Human Motion Style Transfer with Semantic Guidance",
        "authors": [
            "Lei Hu",
            "Zihao Zhang",
            "Yongjing Ye",
            "Yiwen Xu",
            "Shihong Xia"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  3D Human motion style transfer is a fundamental problem in computer graphic\nand animation processing. Existing AdaIN- based methods necessitate datasets\nwith balanced style distribution and content/style labels to train the\nclustered latent space. However, we may encounter a single unseen style example\nin practical scenarios, but not in sufficient quantity to constitute a style\ncluster for AdaIN-based methods. Therefore, in this paper, we propose a novel\ntwo-stage framework for few-shot style transfer learning based on the diffusion\nmodel. Specifically, in the first stage, we pre-train a diffusion-based\ntext-to-motion model as a generative prior so that it can cope with various\ncontent motion inputs. In the second stage, based on the single style example,\nwe fine-tune the pre-trained diffusion model in a few-shot manner to make it\ncapable of style transfer. The key idea is regarding the reverse process of\ndiffusion as a motion-style translation process since the motion styles can be\nviewed as special motion variations. During the fine-tuning for style transfer,\na simple yet effective semantic-guided style transfer loss coordinated with\nstyle example reconstruction loss is introduced to supervise the style transfer\nin CLIP semantic space. The qualitative and quantitative evaluations\ndemonstrate that our method can achieve state-of-the-art performance and has\npractical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.06646v2"
    },
    {
        "title": "Coin3D: Controllable and Interactive 3D Assets Generation with\n  Proxy-Guided Conditioning",
        "authors": [
            "Wenqi Dong",
            "Bangbang Yang",
            "Lin Ma",
            "Xiao Liu",
            "Liyuan Cui",
            "Hujun Bao",
            "Yuewen Ma",
            "Zhaopeng Cui"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  As humans, we aspire to create media content that is both freely willed and\nreadily controlled. Thanks to the prominent development of generative\ntechniques, we now can easily utilize 2D diffusion methods to synthesize images\ncontrolled by raw sketch or designated human poses, and even progressively\nedit/regenerate local regions with masked inpainting. However, similar\nworkflows in 3D modeling tasks are still unavailable due to the lack of\ncontrollability and efficiency in 3D generation. In this paper, we present a\nnovel controllable and interactive 3D assets modeling framework, named Coin3D.\nCoin3D allows users to control the 3D generation using a coarse geometry proxy\nassembled from basic shapes, and introduces an interactive generation workflow\nto support seamless local part editing while delivering responsive 3D object\npreviewing within a few seconds. To this end, we develop several techniques,\nincluding the 3D adapter that applies volumetric coarse shape control to the\ndiffusion model, proxy-bounded editing strategy for precise part editing,\nprogressive volume cache to support responsive preview, and volume-SDS to\nensure consistent mesh reconstruction. Extensive experiments of interactive\ngeneration and editing on diverse shape proxies demonstrate that our method\nachieves superior controllability and flexibility in the 3D assets generation\ntask.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.08054v1"
    },
    {
        "title": "A Simple Approach to Differentiable Rendering of SDFs",
        "authors": [
            "Zichen Wang",
            "Xi Deng",
            "Ziyi Zhang",
            "Wenzel Jakob",
            "Steve Marschner"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a simple algorithm for differentiable rendering of surfaces\nrepresented by Signed Distance Fields (SDF), which makes it easy to integrate\nrendering into gradient-based optimization pipelines. To tackle\nvisibility-related derivatives that make rendering non-differentiable, existing\nphysically based differentiable rendering methods often rely on elaborate\nguiding data structures or reparameterization with a global impact on variance.\nIn this article, we investigate an alternative that embraces nonzero bias in\nexchange for low variance and architectural simplicity. Our method expands the\nlower-dimensional boundary integral into a thin band that is easy to sample\nwhen the underlying surface is represented by an SDF. We demonstrate the\nperformance and robustness of our formulation in end-to-end inverse rendering\ntasks, where it obtains results that are competitive with or superior to\nexisting work.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.08733v2"
    },
    {
        "title": "ContourCraft: Learning to Resolve Intersections in Neural Multi-Garment\n  Simulations",
        "authors": [
            "Artur Grigorev",
            "Giorgio Becherini",
            "Michael J. Black",
            "Otmar Hilliges",
            "Bernhard Thomaszewski"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Learning-based approaches to cloth simulation have started to show their\npotential in recent years. However, handling collisions and intersections in\nneural simulations remains a largely unsolved problem. In this work, we present\n\\moniker{}, a learning-based solution for handling intersections in neural\ncloth simulations. Unlike conventional approaches that critically rely on\nintersection-free inputs, \\moniker{} robustly recovers from intersections\nintroduced through missed collisions, self-penetrating bodies, or errors in\nmanually designed multi-layer outfits. The technical core of \\moniker{} is a\nnovel intersection contour loss that penalizes interpenetrations and encourages\nrapid resolution thereof. We integrate our intersection loss with a\ncollision-avoiding repulsion objective into a neural cloth simulation method\nbased on graph neural networks (GNNs). We demonstrate our method's ability\nacross a challenging set of diverse multi-layer outfits under dynamic human\nmotions. Our extensive analysis indicates that \\moniker{} significantly\nimproves collision handling for learned simulation and produces visually\ncompelling results.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.09522v2"
    },
    {
        "title": "A Comparative Study of Garment Draping Techniques",
        "authors": [
            "Prerana Achar",
            "Mayank Patel",
            "Anushka Mulik",
            "Neha Katre",
            "Stevina Dias",
            "Chirag Raman"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a comparison review that evaluates popular techniques for garment\ndraping for 3D fashion design, virtual try-ons, and animations. A comparative\nstudy is performed between various methods for garment draping of clothing over\nthe human body. These include numerous models, such as physics and machine\nlearning based techniques, collision handling, and more. Performance\nevaluations and trade-offs are discussed to ensure informed decision-making\nwhen choosing the most appropriate approach. These methods aim to accurately\nrepresent deformations and fine wrinkles of digital garments, considering the\nfactors of data requirements, and efficiency, to produce realistic results. The\nresearch can be insightful to researchers, designers, and developers in\nvisualizing dynamic multi-layered 3D clothing.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.11056v1"
    },
    {
        "title": "LAGA: Layered 3D Avatar Generation and Customization via Gaussian\n  Splatting",
        "authors": [
            "Jia Gong",
            "Shenyu Ji",
            "Lin Geng Foo",
            "Kang Chen",
            "Hossein Rahmani",
            "Jun Liu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Creating and customizing a 3D clothed avatar from textual descriptions is a\ncritical and challenging task. Traditional methods often treat the human body\nand clothing as inseparable, limiting users' ability to freely mix and match\ngarments. In response to this limitation, we present LAyered Gaussian Avatar\n(LAGA), a carefully designed framework enabling the creation of high-fidelity\ndecomposable avatars with diverse garments. By decoupling garments from avatar,\nour framework empowers users to conviniently edit avatars at the garment level.\nOur approach begins by modeling the avatar using a set of Gaussian points\norganized in a layered structure, where each layer corresponds to a specific\ngarment or the human body itself. To generate high-quality garments for each\nlayer, we introduce a coarse-to-fine strategy for diverse garment generation\nand a novel dual-SDS loss function to maintain coherence between the generated\ngarments and avatar components, including the human body and other garments.\nMoreover, we introduce three regularization losses to guide the movement of\nGaussians for garment transfer, allowing garments to be freely transferred to\nvarious avatars. Extensive experimentation demonstrates that our approach\nsurpasses existing methods in the generation of 3D clothed humans.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12663v1"
    },
    {
        "title": "Implicit-ARAP: Efficient Handle-Guided Deformation of High-Resolution\n  Meshes and Neural Fields via Local Patch Meshing",
        "authors": [
            "Daniele Baieri",
            "Filippo Maggioli",
            "Zorah Lähner",
            "Simone Melzi",
            "Emanuele Rodolà"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this work, we present the local patch mesh representation for neural\nsigned distance fields. This technique allows to discretize local regions of\nthe level sets of an input SDF by projecting and deforming flat patch meshes\nonto the level set surface, using exclusively the SDF information and its\ngradient. Our analysis reveals this method to be more accurate than the\nstandard marching cubes algorithm for approximating the implicit surface. Then,\nwe apply this representation in the setting of handle-guided deformation: we\nintroduce two distinct pipelines, which make use of 3D neural fields to compute\nAs-Rigid-As-Possible deformations of both high-resolution meshes and neural\nfields under a given set of constraints. We run a comprehensive evaluation of\nour method and various baselines for neural field and mesh deformation which\nshow both pipelines achieve impressive efficiency and notable improvements in\nterms of quality of results and robustness. With our novel pipeline, we\nintroduce a scalable approach to solve a well-established geometry processing\nproblem on high-resolution meshes, and pave the way for extending other\ngeometric tasks to the domain of implicit surfaces via local patch meshing.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12895v2"
    },
    {
        "title": "CraftsMan: High-fidelity Mesh Generation with 3D Native Generation and\n  Interactive Geometry Refiner",
        "authors": [
            "Weiyu Li",
            "Jiarui Liu",
            "Rui Chen",
            "Yixun Liang",
            "Xuelin Chen",
            "Ping Tan",
            "Xiaoxiao Long"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a novel generative 3D modeling system, coined CraftsMan, which can\ngenerate high-fidelity 3D geometries with highly varied shapes, regular mesh\ntopologies, and detailed surfaces, and, notably, allows for refining the\ngeometry in an interactive manner. Despite the significant advancements in 3D\ngeneration, existing methods still struggle with lengthy optimization\nprocesses, irregular mesh topologies, noisy surfaces, and difficulties in\naccommodating user edits, consequently impeding their widespread adoption and\nimplementation in 3D modeling software. Our work is inspired by the craftsman,\nwho usually roughs out the holistic figure of the work first and elaborates the\nsurface details subsequently. Specifically, we employ a 3D native diffusion\nmodel, which operates on latent space learned from latent set-based 3D\nrepresentations, to generate coarse geometries with regular mesh topology in\nseconds. In particular, this process takes as input a text prompt or a\nreference image and leverages a powerful multi-view (MV) diffusion model to\ngenerate multiple views of the coarse geometry, which are fed into our\nMV-conditioned 3D diffusion model for generating the 3D geometry, significantly\nimproving robustness and generalizability. Following that, a normal-based\ngeometry refiner is used to significantly enhance the surface details. This\nrefinement can be performed automatically, or interactively with user-supplied\nedits. Extensive experiments demonstrate that our method achieves high efficacy\nin producing superior-quality 3D assets compared to existing methods. HomePage:\nhttps://craftsman3d.github.io/, Code: https://github.com/wyysf-98/CraftsMan\n",
        "pdf_link": "http://arxiv.org/pdf/2405.14979v1"
    },
    {
        "title": "Don't Splat your Gaussians: Volumetric Ray-Traced Primitives for\n  Modeling and Rendering Scattering and Emissive Media",
        "authors": [
            "Jorge Condor",
            "Sebastien Speierer",
            "Lukas Bode",
            "Aljaz Bozic",
            "Simon Green",
            "Piotr Didyk",
            "Adrian Jarabo"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Efficient scene representations are essential for many computer graphics\napplications. A general unified representation that can handle both surfaces\nand volumes simultaneously, remains a research challenge. Inspired by recent\nmethods for scene reconstruction that leverage mixtures of 3D Gaussians to\nmodel radiance fields, we formalize and generalize the modeling of scattering\nand emissive media using mixtures of simple kernel-based volumetric primitives.\nWe introduce closed-form solutions for transmittance and free-flight distance\nsampling for different kernels, and propose several optimizations to use our\nmethod efficiently within any off-the-shelf volumetric path tracer. We\ndemonstrate our method as a compact and efficient alternative to other forms of\nvolume modeling for forward and inverse rendering of scattering media.\nFurthermore, we adapt and showcase our method in radiance field optimization\nand rendering, providing additional flexibility compared to current state of\nthe art given its ray-tracing formulation. We also introduce the Epanechnikov\nkernel and demonstrate its potential as an efficient alternative to the\ntraditionally-used Gaussian kernel in scene reconstruction tasks. The\nversatility and physically-based nature of our approach allows us to go beyond\nradiance fields and bring to kernel-based modeling and rendering any\npath-tracing enabled functionality such as scattering, relighting and complex\ncamera models.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.15425v2"
    },
    {
        "title": "N-BVH: Neural ray queries with bounding volume hierarchies",
        "authors": [
            "Philippe Weier",
            "Alexander Rath",
            "Élie Michel",
            "Iliyan Georgiev",
            "Philipp Slusallek",
            "Tamy Boubekeur"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Neural representations have shown spectacular ability to compress complex\nsignals in a fraction of the raw data size. In 3D computer graphics, the bulk\nof a scene's memory usage is due to polygons and textures, making them ideal\ncandidates for neural compression. Here, the main challenge lies in finding\ngood trade-offs between efficient compression and cheap inference while\nminimizing training time. In the context of rendering, we adopt a ray-centric\napproach to this problem and devise N-BVH, a neural compression architecture\ndesigned to answer arbitrary ray queries in 3D. Our compact model is learned\nfrom the input geometry and substituted for it whenever a ray intersection is\nqueried by a path-tracing engine. While prior neural compression methods have\nfocused on point queries, ours proposes neural ray queries that integrate\nseamlessly into standard ray-tracing pipelines. At the core of our method, we\nemploy an adaptive BVH-driven probing scheme to optimize the parameters of a\nmulti-resolution hash grid, focusing its neural capacity on the sparse 3D\noccupancy swept by the original surfaces. As a result, our N-BVH can serve\naccurate ray queries from a representation that is more than an order of\nmagnitude more compact, providing faithful approximations of visibility, depth,\nand appearance attributes. The flexibility of our method allows us to combine\nand overlap neural and non-neural entities within the same 3D scene and extends\nto appearance level of detail.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.16237v1"
    },
    {
        "title": "Part123: Part-aware 3D Reconstruction from a Single-view Image",
        "authors": [
            "Anran Liu",
            "Cheng Lin",
            "Yuan Liu",
            "Xiaoxiao Long",
            "Zhiyang Dou",
            "Hao-Xiang Guo",
            "Ping Luo",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Recently, the emergence of diffusion models has opened up new opportunities\nfor single-view reconstruction. However, all the existing methods represent the\ntarget object as a closed mesh devoid of any structural information, thus\nneglecting the part-based structure, which is crucial for many downstream\napplications, of the reconstructed shape. Moreover, the generated meshes\nusually suffer from large noises, unsmooth surfaces, and blurry textures,\nmaking it challenging to obtain satisfactory part segments using 3D\nsegmentation techniques. In this paper, we present Part123, a novel framework\nfor part-aware 3D reconstruction from a single-view image. We first use\ndiffusion models to generate multiview-consistent images from a given image,\nand then leverage Segment Anything Model (SAM), which demonstrates powerful\ngeneralization ability on arbitrary objects, to generate multiview segmentation\nmasks. To effectively incorporate 2D part-based information into 3D\nreconstruction and handle inconsistency, we introduce contrastive learning into\na neural rendering framework to learn a part-aware feature space based on the\nmultiview segmentation masks. A clustering-based algorithm is also developed to\nautomatically derive 3D part segmentation results from the reconstructed\nmodels. Experiments show that our method can generate 3D models with\nhigh-quality segmented parts on various objects. Compared to existing\nunstructured reconstruction methods, the part-aware 3D models from our method\nbenefit some important applications, including feature-preserving\nreconstruction, primitive fitting, and 3D shape editing.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.16888v1"
    },
    {
        "title": "DreamMat: High-quality PBR Material Generation with Geometry- and\n  Light-aware Diffusion Models",
        "authors": [
            "Yuqing Zhang",
            "Yuan Liu",
            "Zhiyu Xie",
            "Lei Yang",
            "Zhongyuan Liu",
            "Mengzhou Yang",
            "Runze Zhang",
            "Qilong Kou",
            "Cheng Lin",
            "Wenping Wang",
            "Xiaogang Jin"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  2D diffusion model, which often contains unwanted baked-in shading effects\nand results in unrealistic rendering effects in the downstream applications.\nGenerating Physically Based Rendering (PBR) materials instead of just RGB\ntextures would be a promising solution. However, directly distilling the PBR\nmaterial parameters from 2D diffusion models still suffers from incorrect\nmaterial decomposition, such as baked-in shading effects in albedo. We\nintroduce DreamMat, an innovative approach to resolve the aforementioned\nproblem, to generate high-quality PBR materials from text descriptions. We find\nout that the main reason for the incorrect material distillation is that\nlarge-scale 2D diffusion models are only trained to generate final shading\ncolors, resulting in insufficient constraints on material decomposition during\ndistillation. To tackle this problem, we first finetune a new light-aware 2D\ndiffusion model to condition on a given lighting environment and generate the\nshading results on this specific lighting condition. Then, by applying the same\nenvironment lights in the material distillation, DreamMat can generate\nhigh-quality PBR materials that are not only consistent with the given geometry\nbut also free from any baked-in shading effects in albedo. Extensive\nexperiments demonstrate that the materials produced through our methods exhibit\ngreater visual appeal to users and achieve significantly superior rendering\nquality compared to baseline methods, which are preferable for downstream tasks\nsuch as game and film production.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17176v1"
    },
    {
        "title": "Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh",
        "authors": [
            "Xiangjun Gao",
            "Xiaoyu Li",
            "Yiyu Zhuang",
            "Qi Zhang",
            "Wenbo Hu",
            "Chaopeng Zhang",
            "Yao Yao",
            "Ying Shan",
            "Long Quan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Neural 3D representations such as Neural Radiance Fields (NeRF), excel at\nproducing photo-realistic rendering results but lack the flexibility for\nmanipulation and editing which is crucial for content creation. Previous works\nhave attempted to address this issue by deforming a NeRF in canonical space or\nmanipulating the radiance field based on an explicit mesh. However,\nmanipulating NeRF is not highly controllable and requires a long training and\ninference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely\nhigh-fidelity novel view synthesis can be achieved using an explicit\npoint-based 3D representation with much faster training and rendering speed.\nHowever, there is still a lack of effective means to manipulate 3DGS freely\nwhile maintaining rendering quality. In this work, we aim to tackle the\nchallenge of achieving manipulable photo-realistic rendering. We propose to\nutilize a triangular mesh to manipulate 3DGS directly with self-adaptation.\nThis approach reduces the need to design various algorithms for different types\nof Gaussian manipulation. By utilizing a triangle shape-aware Gaussian binding\nand adapting method, we can achieve 3DGS manipulation and preserve\nhigh-fidelity rendering after manipulation. Our approach is capable of handling\nlarge deformations, local manipulations, and soft body simulations while\nkeeping high-quality rendering. Furthermore, we demonstrate that our method is\nalso effective with inaccurate meshes extracted from 3DGS. Experiments\nconducted demonstrate the effectiveness of our method and its superiority over\nbaseline approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17811v1"
    },
    {
        "title": "MidSurfer: A Parameter-Free Approach for Mid-Surface Extraction from\n  Segmented Volumetric Data",
        "authors": [
            "Eva Boneš",
            "Dawar Khan",
            "Ciril Bohak",
            "Benjamin A. Barad",
            "Danielle A. Grotjahn",
            "Ivan Viola",
            "Thomas Theußl"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In the field of volumetric data processing and analysis, extracting\nmid-surfaces from thinly bounded compartments is crucial for tasks such as\nsurface area estimation and accurate modeling of biological structures, yet it\nhas lacked a standardized approach. To bridge this gap, we introduce\nMidSurfer--a novel parameter-free method for extracting mid-surfaces from\nsegmented volumetric data. Our method produces smooth, uniformly triangulated\nmeshes that accurately capture the structural features of interest. The process\nbegins with the Ridge Field Transformation step that transforms the segmented\ninput data, followed by the Mid-Polyline Extraction Algorithm that works on\nindividual volume slices. Based on the connectivity of components, this step\ncan result in either single or multiple polyline segments that represent the\nstructural features. These segments form a coherent series across the volume,\ncreating a backbone of regularly distributed points on each slice that\nrepresents the mid-surface. Subsequently, we employ a Polyline Zipper Algorithm\nfor triangulation that connects these polyline segments across neighboring\nslices, yielding a detailed triangulated mid-surface mesh. Our findings\ndemonstrate that this method surpasses previous techniques in versatility,\nsimplicity of use, and accuracy. Our approach is now publicly available as a\nplugin for ParaView, a widely-used multi-platform tool for data analysis and\nvisualization, and can be found at https://github.com/kaust-vislab/MidSurfer .\n",
        "pdf_link": "http://arxiv.org/pdf/2405.19339v1"
    },
    {
        "title": "Audio2Rig: Artist-oriented deep learning tool for facial animation",
        "authors": [
            "Bastien Arcelin",
            "Nicolas Chaverou"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Creating realistic or stylized facial and lip sync animation is a tedious\ntask. It requires lot of time and skills to sync the lips with audio and convey\nthe right emotion to the character's face. To allow animators to spend more\ntime on the artistic and creative part of the animation, we present Audio2Rig:\na new deep learning based tool leveraging previously animated sequences of a\nshow, to generate facial and lip sync rig animation from an audio file. Based\nin Maya, it learns from any production rig without any adjustment and generates\nhigh quality and stylized animations which mimic the style of the show.\nAudio2Rig fits in the animator workflow: since it generates keys on the rig\ncontrollers, the animation can be easily retaken. The method is based on 3\nneural network modules which can learn an arbitrary number of controllers.\nHence, different configurations can be created for specific parts of the face\n(such as the tongue, lips or eyes). With Audio2Rig, animators can also pick\ndifferent emotions and adjust their intensities to experiment or customize the\noutput, and have high level controls on the keyframes setting. Our method shows\nexcellent results, generating fine animation details while respecting the show\nstyle. Finally, as the training relies on the studio data and is done\ninternally, it ensures data privacy and prevents from copyright infringement.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.20412v1"
    },
    {
        "title": "Robust Biharmonic Skinning Using Geometric Fields",
        "authors": [
            "Ana Dodik",
            "Vincent Sitzmann",
            "Justin Solomon",
            "Oded Stein"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Skinning is a popular way to rig and deform characters for animation, to\ncompute reduced-order simulations, and to define features for geometry\nprocessing. Methods built on skinning rely on weight functions that distribute\nthe influence of each degree of freedom across the mesh. Automatic skinning\nmethods generate these weight functions with minimal user input, usually by\nsolving a variational problem on a mesh whose boundary is the skinned surface.\nThis formulation necessitates tetrahedralizing the volume inside the surface,\nwhich brings with it meshing artifacts, the possibility of tetrahedralization\nfailure, and the impossibility of generating weights for surfaces that are not\nclosed. We introduce a mesh-free and robust automatic skinning method that\ngenerates high-quality skinning weights comparable to the current state of the\nart without volumetric meshes. Our method reliably works even on open surfaces\nand triangle soups where current methods fail. We achieve this through the use\nof a Lagrangian representation for skinning weights, which circumvents the need\nfor finite elements while optimizing the biharmonic energy.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.00238v1"
    },
    {
        "title": "PDP: Physics-Based Character Animation via Diffusion Policy",
        "authors": [
            "Takara E. Truong",
            "Michael Piseno",
            "Zhaoming Xie",
            "C. Karen Liu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Generating diverse and realistic human motion that can physically interact\nwith an environment remains a challenging research area in character animation.\nMeanwhile, diffusion-based methods, as proposed by the robotics community, have\ndemonstrated the ability to capture highly diverse and multi-modal skills.\nHowever, naively training a diffusion policy often results in unstable motions\nfor high-frequency, under-actuated control tasks like bipedal locomotion due to\nrapidly accumulating compounding errors, pushing the agent away from optimal\ntraining trajectories. The key idea lies in using RL policies not just for\nproviding optimal trajectories but for providing corrective actions in\nsub-optimal states, giving the policy a chance to correct for errors caused by\nenvironmental stimulus, model errors, or numerical errors in simulation. Our\nmethod, Physics-Based Character Animation via Diffusion Policy (PDP), combines\nreinforcement learning (RL) and behavior cloning (BC) to create a robust\ndiffusion policy for physics-based character animation. We demonstrate PDP on\nperturbation recovery, universal motion tracking, and physics-based\ntext-to-motion synthesis.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.00960v2"
    },
    {
        "title": "RaDe-GS: Rasterizing Depth in Gaussian Splatting",
        "authors": [
            "Baowen Zhang",
            "Chuan Fang",
            "Rakesh Shrestha",
            "Yixun Liang",
            "Xiaoxiao Long",
            "Ping Tan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Gaussian Splatting (GS) has proven to be highly effective in novel view\nsynthesis, achieving high-quality and real-time rendering. However, its\npotential for reconstructing detailed 3D shapes has not been fully explored.\nExisting methods often suffer from limited shape accuracy due to the discrete\nand unstructured nature of Gaussian splats, which complicates the shape\nextraction. While recent techniques like 2D GS have attempted to improve shape\nreconstruction, they often reformulate the Gaussian primitives in ways that\nreduce both rendering quality and computational efficiency. To address these\nproblems, our work introduces a rasterized approach to render the depth maps\nand surface normal maps of general 3D Gaussian splats. Our method not only\nsignificantly enhances shape reconstruction accuracy but also maintains the\ncomputational efficiency intrinsic to Gaussian Splatting. It achieves a Chamfer\ndistance error comparable to NeuraLangelo on the DTU dataset and maintains\nsimilar computational efficiency as the original 3D GS methods. Our method is a\nsignificant advancement in Gaussian Splatting and can be directly integrated\ninto existing Gaussian Splatting-based methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.01467v2"
    },
    {
        "title": "Fluid Implicit Particles on Coadjoint Orbits",
        "authors": [
            "Mohammad Sina Nabizadeh",
            "Ritoban Roy-Chowdhury",
            "Hang Yin",
            "Ravi Ramamoorthi",
            "Albert Chern"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose Coadjoint Orbit FLIP (CO-FLIP), a high order accurate, structure\npreserving fluid simulation method in the hybrid Eulerian-Lagrangian framework.\nWe start with a Hamiltonian formulation of the incompressible Euler Equations,\nand then, using a local, explicit, and high order divergence free\ninterpolation, construct a modified Hamiltonian system that governs our\ndiscrete Euler flow. The resulting discretization, when paired with a geometric\ntime integration scheme, is energy and circulation preserving (formally the\nflow evolves on a coadjoint orbit) and is similar to the Fluid Implicit\nParticle (FLIP) method. CO-FLIP enjoys multiple additional properties including\nthat the pressure projection is exact in the weak sense, and the\nparticle-to-grid transfer is an exact inverse of the grid-to-particle\ninterpolation. The method is demonstrated numerically with outstanding\nstability, energy, and Casimir preservation. We show that the method produces\nbenchmarks and turbulent visual effects even at low grid resolutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.01936v2"
    },
    {
        "title": "Interactive zoom display in smartphone-based digital holographic\n  microscope for 3D imaging",
        "authors": [
            "Yuki Nagahama"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Digital holography has applications in bio-imaging because it can\nsimultaneously obtain the amplitude and phase information of a microscopic\nsample in a single shot, thus facilitating non-contact, noninvasive observation\nof the 3D shape of transparent objects (phase objects, which can be mapped with\nthe phase information,) and moving objects. The combination of digital\nholography and microscopy is called digital holographic microscopy (DHM). In\nthis study, we propose a smartphone-based DHM system for 3D imaging that is\ncompact, inexpensive, and capable of observing objects in real time; this\nsystem includes an optical system comprising a 3D printer using commercially\navailable image sensors and semiconductor lasers; further, an Android-based\napplication is used to reconstruct the holograms acquired by this optical\nsystem, thus outlining the amplitude and phase information of the observed\nobject. Also, by utilizing scalable diffraction calculation methods and\ntouchscreen interaction, we implemented zoom functionality through pinch-in\ngestures. The study results showed that the DHM system successfully obtained\nthe amplitude and phase information of the observed object via the acquired\nholograms in an almost real time manner. Thus, this study showed that it is\npossible to construct a low cost and compact DHM system that includes a 3D\nprinter to construct the optical system and a smartphone application to\nreconstruct the holograms. Furthermore, this smartphone-based DHM system's\nability to capture, reconstruct, and display holograms in real time\ndemonstrates its superiority and novelty over existing systems. This system is\nalso expected to contribute to biology fieldwork and pathological diagnosis in\nremote areas.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.04014v2"
    },
    {
        "title": "ReflectanceFusion: Diffusion-based text to SVBRDF Generation",
        "authors": [
            "Bowen Xue",
            "Giuseppe Claudio Guarnera",
            "Shuang Zhao",
            "Zahra Montazeri"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce Reflectance Diffusion, a new neural text-to-texture model\ncapable of generating high-fidelity SVBRDF maps from textual descriptions. Our\nmethod leverages a tandem neural approach, consisting of two modules, to\naccurately model the distribution of spatially varying reflectance as described\nby text prompts. Initially, we employ a pre-trained stable diffusion 2 model to\ngenerate a latent representation that informs the overall shape of the material\nand serves as our backbone model. Then, our ReflectanceUNet enables fine-tuning\ncontrol over the material's physical appearance and generates SVBRDF maps.\nReflectanceUNet module is trained on an extensive dataset comprising\napproximately 200,000 synthetic spatially varying materials. Our generative\nSVBRDF diffusion model allows for the synthesis of multiple SVBRDF estimates\nfrom a single textual input, offering users the possibility to choose the\noutput that best aligns with their requirements. We illustrate our method's\nversatility by generating SVBRDF maps from a range of textual descriptions,\nboth specific and broad. Our ReflectanceUNet model can integrate optional\nphysical parameters, such as roughness and specularity, enhancing\ncustomization. When the backbone module is fixed, the ReflectanceUNet module\nrefines the material, allowing direct edits to its physical attributes.\nComparative evaluations demonstrate that ReflectanceFusion achieves better\naccuracy than existing text-to-material models, such as Text2Mat, while also\nproviding the benefits of editable and relightable SVBRDF maps.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.14565v1"
    },
    {
        "title": "Toward Ubiquitous 3D Object Digitization: A Wearable Computing Framework\n  for Non-Invasive Physical Property Acquisition",
        "authors": [
            "Yunxiang Zhang",
            "Xin Sun",
            "Dengfeng Li",
            "Xinge Yu",
            "Qi Sun"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Accurately digitizing physical objects is central to many applications,\nincluding virtual/augmented reality, industrial design, and e-commerce. Prior\nresearch has demonstrated efficient and faithful reconstruction of objects'\ngeometric shapes and visual appearances, which suffice for digitally\nrepresenting rigid objects. In comparison, physical properties, such as\nelasticity and pressure, are also indispensable to the behavioral fidelity of\ndigitized deformable objects. However, existing approaches to acquiring these\nquantities either rely on invasive specimen collection or expensive/bulky\nlaboratory setups, making them inapplicable to consumer-level usage.\n  To fill this gap, we propose a wearable and non-invasive computing framework\nthat allows users to conveniently estimate the material elasticity and internal\npressure of deformable objects through finger touches. This is achieved by\nmodeling their local surfaces as pressurized elastic shells and analytically\nderiving the two physical properties from finger-induced wrinkling patterns.\nTogether with photogrammetry-reconstructed geometry and textures, the two\nestimated physical properties enable us to faithfully replicate the motion and\ndeformation behaviors of several deformable objects. For the pressure\nestimation, our model achieves a relative error of 3.5%. In the interaction\nexperiments, the virtual-physical deformation discrepancy measures less than\n10.1%. Generalization to objects of irregular shape further demonstrates the\npotential of our approach in practical applications. We envision this work to\nprovide insights for and motivate research toward democratizing the ubiquitous\nand pervasive digitization of our physical surroundings in daily, industrial,\nand scientific scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.17156v1"
    },
    {
        "title": "Lightweight Predictive 3D Gaussian Splats",
        "authors": [
            "Junli Cao",
            "Vidit Goel",
            "Chaoyang Wang",
            "Anil Kag",
            "Ju Hu",
            "Sergei Korolev",
            "Chenfanfu Jiang",
            "Sergey Tulyakov",
            "Jian Ren"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Recent approaches representing 3D objects and scenes using Gaussian splats\nshow increased rendering speed across a variety of platforms and devices. While\nrendering such representations is indeed extremely efficient, storing and\ntransmitting them is often prohibitively expensive. To represent large-scale\nscenes, one often needs to store millions of 3D Gaussians, occupying gigabytes\nof disk space. This poses a very practical limitation, prohibiting widespread\nadoption.Several solutions have been proposed to strike a balance between disk\nsize and rendering quality, noticeably reducing the visual quality. In this\nwork, we propose a new representation that dramatically reduces the hard drive\nfootprint while featuring similar or improved quality when compared to the\nstandard 3D Gaussian splats. When compared to other compact solutions, ours\noffers higher quality renderings with significantly reduced storage, being able\nto efficiently run on a mobile device in real-time. Our key observation is that\nnearby points in the scene can share similar representations. Hence, only a\nsmall ratio of 3D points needs to be stored. We introduce an approach to\nidentify such points which are called parent points. The discarded points\ncalled children points along with attributes can be efficiently predicted by\ntiny MLPs.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19434v1"
    },
    {
        "title": "3DMeshNet: A Three-Dimensional Differential Neural Network for\n  Structured Mesh Generation",
        "authors": [
            "Jiaming Peng",
            "Xinhai Chen",
            "Jie Liu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Mesh generation is a crucial step in numerical simulations, significantly\nimpacting simulation accuracy and efficiency. However, generating meshes\nremains time-consuming and requires expensive computational resources. In this\npaper, we propose a novel method, 3DMeshNet, for three-dimensional structured\nmesh generation. The method embeds the meshing-related differential equations\ninto the loss function of neural networks, formulating the meshing task as an\nunsupervised optimization problem. It takes geometric points as input to learn\nthe potential mapping between parametric and computational domains. After\nsuitable offline training, 3DMeshNet can efficiently output a three-dimensional\nstructured mesh with a user-defined number of quadrilateral/hexahedral cells\nthrough the feed-forward neural prediction. To enhance training stability and\naccelerate convergence, we integrate loss function reweighting through weight\nadjustments and gradient projection alongside applying finite difference\nmethods to streamline derivative computations in the loss. Experiments on\ndifferent cases show that 3DMeshNet is robust and fast. It outperforms neural\nnetwork-based methods and yields superior meshes compared to traditional mesh\npartitioning methods. 3DMeshNet significantly reduces training times by up to\n85% compared to other neural network-based approaches and lowers meshing\noverhead by 4 to 8 times relative to traditional meshing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.01560v1"
    },
    {
        "title": "Neural Garment Dynamics via Manifold-Aware Transformers",
        "authors": [
            "Peizhuo Li",
            "Tuanfeng Y. Wang",
            "Timur Levent Kesdogan",
            "Duygu Ceylan",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Data driven and learning based solutions for modeling dynamic garments have\nsignificantly advanced, especially in the context of digital humans. However,\nexisting approaches often focus on modeling garments with respect to a fixed\nparametric human body model and are limited to garment geometries that were\nseen during training. In this work, we take a different approach and model the\ndynamics of a garment by exploiting its local interactions with the underlying\nhuman body. Specifically, as the body moves, we detect local garment-body\ncollisions, which drive the deformation of the garment. At the core of our\napproach is a mesh-agnostic garment representation and a manifold-aware\ntransformer network design, which together enable our method to generalize to\nunseen garment and body geometries. We evaluate our approach on a wide variety\nof garment types and motion sequences and provide competitive qualitative and\nquantitative results with respect to the state of the art.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.06101v1"
    },
    {
        "title": "3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes",
        "authors": [
            "Nicolas Moenne-Loccoz",
            "Ashkan Mirzaei",
            "Or Perel",
            "Riccardo de Lutio",
            "Janick Martinez Esturo",
            "Gavriel State",
            "Sanja Fidler",
            "Nicholas Sharp",
            "Zan Gojcic"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Particle-based representations of radiance fields such as 3D Gaussian\nSplatting have found great success for reconstructing and re-rendering of\ncomplex scenes. Most existing methods render particles via rasterization,\nprojecting them to screen space tiles for processing in a sorted order. This\nwork instead considers ray tracing the particles, building a bounding volume\nhierarchy and casting a ray for each pixel using high-performance GPU ray\ntracing hardware. To efficiently handle large numbers of semi-transparent\nparticles, we describe a specialized rendering algorithm which encapsulates\nparticles with bounding meshes to leverage fast ray-triangle intersections, and\nshades batches of intersections in depth-order. The benefits of ray tracing are\nwell-known in computer graphics: processing incoherent rays for secondary\nlighting effects such as shadows and reflections, rendering from\nhighly-distorted cameras common in robotics, stochastically sampling rays, and\nmore. With our renderer, this flexibility comes at little cost compared to\nrasterization. Experiments demonstrate the speed and accuracy of our approach,\nas well as several applications in computer graphics and vision. We further\npropose related improvements to the basic Gaussian representation, including a\nsimple use of generalized kernel functions which significantly reduces particle\nhit counts.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.07090v3"
    },
    {
        "title": "Exploring the Role of Expected Collision Feedback in Crowded Virtual\n  Environments",
        "authors": [
            "Haoran Yun",
            "Jose Luis Ponton",
            "Alejandro Beacco",
            "Carlos Andujar",
            "Nuria Pelechano"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  An increasing number of virtual reality applications require environments\nthat emulate real-world conditions. These environments often involve dynamic\nvirtual humans showing realistic behaviors. Understanding user perception and\nnavigation among these virtual agents is key for designing realistic and\neffective environments featuring groups of virtual humans. While collision risk\nsignificantly influences human locomotion in the real world, this risk is\nlargely absent in virtual settings. This paper studies the impact of the\nexpected collision feedback on user perception and interaction with virtual\ncrowds. We examine the effectiveness of commonly used collision feedback\ntechniques (auditory cues and tactile vibrations) as well as inducing\nparticipants to expect that a physical bump with a real person might occur, as\nif some virtual humans actually correspond to real persons embodied into them\nand sharing the same physical space. Our results indicate that the expected\ncollision feedback significantly influences both participant behavior\n(encompassing global navigation and local movements) and subjective perceptions\nof presence and copresence. Specifically, the introduction of a perceived risk\nof actual collision was found to significantly impact global navigation\nstrategies and increase the sense of presence. Auditory cues had a similar\neffect on global navigation and additionally enhanced the sense of copresence.\nIn contrast, vibrotactile feedback was primarily effective in influencing local\nmovements.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.07992v1"
    },
    {
        "title": "SENC: Handling Self-collision in Neural Cloth Simulation",
        "authors": [
            "Zhouyingcheng Liao",
            "Sinan Wang",
            "Taku Komura"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present SENC, a novel self-supervised neural cloth simulator that\naddresses the challenge of cloth self-collision. This problem has remained\nunresolved due to the gap in simulation setup between recent collision\ndetection and response approaches and self-supervised neural simulators. The\nformer requires collision-free initial setups, while the latter necessitates\nrandom cloth instantiation during training. To tackle this issue, we propose a\nnovel loss based on Global Intersection Analysis (GIA). This loss extracts the\nvolume surrounded by the cloth region that forms the penetration. By\nconstructing an energy based on this volume, our self-supervised neural\nsimulator can effectively address cloth self-collisions. Moreover, we develop a\nself-collision-aware graph neural network capable of learning to handle\nself-collisions, even for parts that are topologically distant from one\nanother. Additionally, we introduce an effective external force scheme that\nenables the simulation to learn the cloth's behavior in response to random\nexternal forces. We validate the efficacy of SENC through extensive\nquantitative and qualitative experiments, demonstrating that it effectively\nreduces cloth self-collision while maintaining high-quality animation results.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.12479v1"
    },
    {
        "title": "DirectL: Efficient Radiance Fields Rendering for 3D Light Field Displays",
        "authors": [
            "Zongyuan Yang",
            "Baolin Liu",
            "Yingde Song",
            "Yongping Xiong",
            "Lan Yi",
            "Zhaohe Zhang",
            "Xunbo Yu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Autostereoscopic display, despite decades of development, has not achieved\nextensive application, primarily due to the daunting challenge of 3D content\ncreation for non-specialists. The emergence of Radiance Field as an innovative\n3D representation has markedly revolutionized the domains of 3D reconstruction\nand generation. This technology greatly simplifies 3D content creation for\ncommon users, broadening the applicability of Light Field Displays (LFDs).\nHowever, the combination of these two fields remains largely unexplored. The\nstandard paradigm to create optimal content for parallax-based light field\ndisplays demands rendering at least 45 slightly shifted views preferably at\nhigh resolution per frame, a substantial hurdle for real-time rendering. We\nintroduce DirectL, a novel rendering paradigm for Radiance Fields on 3D\ndisplays. We thoroughly analyze the interweaved mapping of spatial rays to\nscreen subpixels, precisely determine the light rays entering the human eye,\nand propose subpixel repurposing to significantly reduce the pixel count\nrequired for rendering. Tailored for the two predominant radiance\nfields--Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS), we\npropose corresponding optimized rendering pipelines that directly render the\nlight field images instead of multi-view images. Extensive experiments across\nvarious displays and user study demonstrate that DirectL accelerates rendering\nby up to 40 times compared to the standard paradigm without sacrificing visual\nquality. Its rendering process-only modification allows seamless integration\ninto subsequent radiance field tasks. Finally, we integrate DirectL into\ndiverse applications, showcasing the stunning visual experiences and the\nsynergy between LFDs and Radiance Fields, which unveils tremendous potential\nfor commercialization applications. \\href{direct-l.github.io}{\\textbf{Project\nHomepage}\n",
        "pdf_link": "http://arxiv.org/pdf/2407.14053v1"
    },
    {
        "title": "Differentiable Convex Polyhedra Optimization from Multi-view Images",
        "authors": [
            "Daxuan Ren",
            "Haiyi Mei",
            "Hezi Shi",
            "Jianmin Zheng",
            "Jianfei Cai",
            "Lei Yang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper presents a novel approach for the differentiable rendering of\nconvex polyhedra, addressing the limitations of recent methods that rely on\nimplicit field supervision. Our technique introduces a strategy that combines\nnon-differentiable computation of hyperplane intersection through duality\ntransform with differentiable optimization for vertex positioning with\nthree-plane intersection, enabling gradient-based optimization without the need\nfor 3D implicit fields. This allows for efficient shape representation across a\nrange of applications, from shape parsing to compact mesh reconstruction. This\nwork not only overcomes the challenges of previous approaches but also sets a\nnew standard for representing shapes with convex polyhedra.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.15686v1"
    },
    {
        "title": "SR-CurvANN: Advancing 3D Surface Reconstruction through Curvature-Aware\n  Neural Networks",
        "authors": [
            "Marina Hernández-Bautista",
            "Francisco J. Melero"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Incomplete or missing data in three-dimensional (3D) models can lead to\nerroneous or flawed renderings, limiting their usefulness in applications such\nas visualization, geometric computation, and 3D printing. Conventional\nsurface-repair techniques often fail to infer complex geometric details in\nmissing areas. Neural networks successfully address hole-filling tasks in 2D\nimages using inpainting techniques. The combination of surface reconstruction\nalgorithms, guided by the model's curvature properties and the creativity of\nneural networks in the inpainting processes should provide realistic results in\nthe hole completion task. In this paper, we propose a novel method entitled\nSR-CurvANN (Surface Reconstruction Based on Curvature-Aware Neural Networks)\nthat incorporates neural network-based 2D inpainting to effectively reconstruct\n3D surfaces. We train the neural networks with images that represent planar\nrepresentations of the curvature at vertices of hundreds of 3D models. Once the\nmissing areas have been inferred, a coarse-to-fine surface deformation process\nensures that the surface fits the reconstructed curvature image. Our proposal\nmakes it possible to learn and generalize patterns from a wide variety of\ntraining 3D models, generating comprehensive inpainted curvature images and\nsurfaces. Experiments conducted on 959 models with several holes have\ndemonstrated that SR-CurvANN excels in the shape completion process, filling\nholes with a remarkable level of realism and precision.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.17896v2"
    },
    {
        "title": "A Comparative Study of Neural Surface Reconstruction for Scientific\n  Visualization",
        "authors": [
            "Siyuan Yao",
            "Weixi Song",
            "Chaoli Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This comparative study evaluates various neural surface reconstruction\nmethods, particularly focusing on their implications for scientific\nvisualization through reconstructing 3D surfaces via multi-view rendering\nimages. We categorize ten methods into neural radiance fields and neural\nimplicit surfaces, uncovering the benefits of leveraging distance functions\n(i.e., SDFs and UDFs) to enhance the accuracy and smoothness of the\nreconstructed surfaces. Our findings highlight the efficiency and quality of\nNeuS2 for reconstructing closed surfaces and identify NeUDF as a promising\ncandidate for reconstructing open surfaces despite some limitations. By sharing\nour benchmark dataset, we invite researchers to test the performance of their\nmethods, contributing to the advancement of surface reconstruction solutions\nfor scientific visualization.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.20868v1"
    },
    {
        "title": "Deformable 3D Shape Diffusion Model",
        "authors": [
            "Dengsheng Chen",
            "Jie Hu",
            "Xiaoming Wei",
            "Enhua Wu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The Gaussian diffusion model, initially designed for image generation, has\nrecently been adapted for 3D point cloud generation. However, these adaptations\nhave not fully considered the intrinsic geometric characteristics of 3D shapes,\nthereby constraining the diffusion model's potential for 3D shape manipulation.\nTo address this limitation, we introduce a novel deformable 3D shape diffusion\nmodel that facilitates comprehensive 3D shape manipulation, including point\ncloud generation, mesh deformation, and facial animation. Our approach\ninnovatively incorporates a differential deformation kernel, which deconstructs\nthe generation of geometric structures into successive non-rigid deformation\nstages. By leveraging a probabilistic diffusion model to simulate this\nstep-by-step process, our method provides a versatile and efficient solution\nfor a wide range of applications, spanning from graphics rendering to facial\nexpression animation. Empirical evidence highlights the effectiveness of our\napproach, demonstrating state-of-the-art performance in point cloud generation\nand competitive results in mesh deformation. Additionally, extensive visual\ndemonstrations reveal the significant potential of our approach for practical\napplications. Our method presents a unique pathway for advancing 3D shape\nmanipulation and unlocking new opportunities in the realm of virtual reality.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.21428v1"
    },
    {
        "title": "Accelerating Transfer Function Update for Distance Map based Volume\n  Rendering",
        "authors": [
            "Michael Rauter",
            "Lukas Zimmermann",
            "Markus Zeilinger"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Direct volume rendering using ray-casting is widely used in practice. By\nusing GPUs and applying acceleration techniques as empty space skipping, high\nframe rates are possible on modern hardware. This enables performance-critical\nuse-cases such as virtual reality volume rendering. The currently fastest known\ntechnique uses volumetric distance maps to skip empty sections of the volume\nduring ray-casting but requires the distance map to be updated per transfer\nfunction change. In this paper, we demonstrate a technique for subdividing the\nvolume intensity range into partitions and deriving what we call partitioned\ndistance maps. These can be used to accelerate the distance map computation for\na newly changed transfer function by a factor up to 30. This allows the\ncurrently fastest known empty space skipping approach to be used while\nmaintaining high frame rates even when the transfer function is changed\nfrequently.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.21552v3"
    },
    {
        "title": "Does empirical evidence from healthy aging studies predict a practical\n  difference between visualizations for different age groups?",
        "authors": [
            "S. Shao",
            "Y. Li",
            "A. I. Meso",
            "N. Holliman"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  When communicating critical information to decision-makers, one of the major\nchallenges in visualization is whether the communication is affected by\ndifferent perceptual or cognitive abilities, one major influencing factor is\nage. We review both visualization and psychophysics literature to understand\nwhere quantitative evidence exists on age differences in visual perception.\nUsing contrast sensitivity data from the literature we show how the differences\nbetween visualizations for different age groups can be predicted using a new\nmodel of visible frequency range with age. The model assumed that at threshold\nvalues some visual data will not be visible to older people (spatial frequency\n> 2 and contrast <=0.01). We apply this result to a practical visualization and\nshow an example that at higher levels of contrast, the visual signal should be\nperceivable by all viewers over 20. Universally usable visualization should use\na contrast of 0.02 or higher and be designed to avoid spatial frequencies\ngreater than eight cycles per degree to accommodate all ages. There remains\nmuch research to do on to translate psychophysics results to practical\nquantitative guidelines for visualization producers.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.21767v2"
    },
    {
        "title": "Localized Evaluation for Constructing Discrete Vector Fields",
        "authors": [
            "Tanner Finken",
            "Julien Tierny",
            "Joshua A Levine"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Topological abstractions offer a method to summarize the behavior of vector\nfields but computing them robustly can be challenging due to numerical\nprecision issues. One alternative is to represent the vector field using a\ndiscrete approach, which constructs a collection of pairs of simplices in the\ninput mesh that satisfies criteria introduced by Forman's discrete Morse\ntheory. While numerous approaches exist to compute pairs in the restricted case\nof the gradient of a scalar field, state-of-the-art algorithms for the general\ncase of vector fields require expensive optimization procedures. This paper\nintroduces a fast, novel approach for pairing simplices of two-dimensional,\ntriangulated vector fields that do not vary in time. The key insight of our\napproach is that we can employ a local evaluation, inspired by the approach\nused to construct a discrete gradient field, where every simplex in a mesh is\nconsidered by no more than one of its vertices. Specifically, we observe that\nfor any edge in the input mesh, we can uniquely assign an outward direction of\nflow. We can further expand this consistent notion of outward flow at each\nvertex, which corresponds to the concept of a downhill flow in the case of\nscalar fields. Working with outward flow enables a linear-time algorithm that\nprocesses the (outward) neighborhoods of each vertex one-by-one, similar to the\napproach used for scalar fields. We couple our approach to constructing\ndiscrete vector fields with a method to extract, simplify, and visualize\ntopological features. Empirical results on analytic and simulation data\ndemonstrate drastic improvements in running time, produce features similar to\nthe current state-of-the-art, and show the application of simplification to\nlarge, complex flows.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04769v2"
    },
    {
        "title": "Accelerating In-transit Isosurface Generation With Topology Preserving\n  Compression",
        "authors": [
            "Yanliang Li",
            "Jieyang Chen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Data visualization through isosurface generation is critical in various\nscientific fields, including computational fluid dynamics, medical imaging, and\ngeophysics. However, the high cost of data sharing between simulation sources\nand visualization resources poses a significant challenge. This paper\nintroduces a novel framework that leverages lossy compression to accelerate\nin-transit isosurface generation. Our approach involves a Compressed\nHierarchical Representation (CHR) and topology-preserving compression to ensure\nthe fidelity of the isosurface generation. Experimental evaluations demonstrate\nthat our framework can achieve up to 4x speedup in visualization workflows,\nmaking it a promising solution for real-time scientific data analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.05462v1"
    },
    {
        "title": "A Thermomechanical Hybrid Incompressible Material Point Method",
        "authors": [
            "Victoria Kala",
            "Jingyu Chen",
            "David Hyde",
            "Alexey Stomakhin",
            "Joseph Teran"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a novel hybrid incompressible flow/material point method solver\nfor simulating the combustion of flammable solids. Our approach utilizes a\nsparse grid representation of solid materials in the material point method\nportion of the solver and a hybrid Eulerian/FLIP solver for the incompressible\nportion. We utilize these components to simulate the motion of heated air and\nparticulate matter as they interact with flammable solids, causing\ncombustion-related damage. We include a novel particle sampling strategy to\nincrease Eulerian flow accuracy near regions of high temperature. We also\nsupport control of the flame front propagation speed and the rate of solid\ncombustion in an artistically directable manner. Solid combustion is modeled\nwith temperature-dependent elastoplastic constitutive modeling. We demonstrate\nthe efficacy of our method on various real-world three-dimensional problems,\nincluding a burning match, incense sticks, and a wood log in a fireplace.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.07276v1"
    },
    {
        "title": "CT4D: Consistent Text-to-4D Generation with Animatable Meshes",
        "authors": [
            "Ce Chen",
            "Shaoli Huang",
            "Xuelin Chen",
            "Guangyi Chen",
            "Xiaoguang Han",
            "Kun Zhang",
            "Mingming Gong"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Text-to-4D generation has recently been demonstrated viable by integrating a\n2D image diffusion model with a video diffusion model. However, existing models\ntend to produce results with inconsistent motions and geometric structures over\ntime. To this end, we present a novel framework, coined CT4D, which directly\noperates on animatable meshes for generating consistent 4D content from\narbitrary user-supplied prompts. The primary challenges of our mesh-based\nframework involve stably generating a mesh with details that align with the\ntext prompt while directly driving it and maintaining surface continuity. Our\nCT4D framework incorporates a unique Generate-Refine-Animate (GRA) algorithm to\nenhance the creation of text-aligned meshes. To improve surface continuity, we\ndivide a mesh into several smaller regions and implement a uniform driving\nfunction within each area. Additionally, we constrain the animating stage with\na rigidity regulation to ensure cross-region continuity. Our experimental\nresults, both qualitative and quantitative, demonstrate that our CT4D framework\nsurpasses existing text-to-4D techniques in maintaining interframe consistency\nand preserving global geometry. Furthermore, we showcase that this enhanced\nrepresentation inherently possesses the capability for combinational 4D\ngeneration and texture editing.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.08342v1"
    },
    {
        "title": "End-to-end Surface Optimization for Light Control",
        "authors": [
            "Yuou Sun",
            "Bailin Deng",
            "Juyong Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Designing a freeform surface to reflect or refract light to achieve a target\ndistribution is a challenging inverse problem. In this paper, we propose an\nend-to-end optimization strategy for an optical surface mesh. Our formulation\nleverages a novel differentiable rendering model, and is directly driven by the\ndifference between the resulting light distribution and the target\ndistribution. We also enforce geometric constraints related to fabrication\nrequirements, to facilitate CNC milling and polishing of the designed surface.\nTo address the issue of local minima, we formulate a face-based optimal\ntransport problem between the current mesh and the target distribution, which\nmakes effective large changes to the surface shape. The combination of our\noptimal transport update and rendering-guided optimization produces an optical\nsurface design with a resulting image closely resembling the target, while the\nfabrication constraints in our optimization help to ensure consistency between\nthe rendering model and the final physical results. The effectiveness of our\nalgorithm is demonstrated on a variety of target images using both simulated\nrendering and physical prototypes.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.13117v1"
    },
    {
        "title": "GSDiff: Synthesizing Vector Floorplans via Geometry-enhanced Structural\n  Graph Generation",
        "authors": [
            "Sizhe Hu",
            "Wenming Wu",
            "Yuntao Wang",
            "Benzhu Xu",
            "Liping Zheng"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Automating architectural floorplan design is vital for housing and interior\ndesign, offering a faster, cost-effective alternative to manual sketches by\narchitects. However, existing methods, including rule-based and learning-based\napproaches, face challenges in design complexity and constrained generation\nwith extensive post-processing, and tend to obvious geometric inconsistencies\nsuch as misalignment, overlap, and gaps. In this work, we propose a novel\ngenerative framework for vector floorplan design via structural graph\ngeneration, called GSDiff, focusing on wall junction generation and wall\nsegment prediction to capture both geometric and semantic aspects of structural\ngraphs. To improve the geometric rationality of generated structural graphs, we\npropose two innovative geometry enhancement methods. In wall junction\ngeneration, we propose a novel alignment loss function to improve geometric\nconsistency. In wall segment prediction, we propose a random self-supervision\nmethod to enhance the model's perception of the overall geometric structure,\nthereby promoting the generation of reasonable geometric structures. Employing\nthe diffusion model and the Transformer model, as well as the geometry\nenhancement strategies, our framework can generate wall junctions, wall\nsegments and room polygons with structural and semantic information, resulting\nin structural graphs that accurately represent floorplans. Extensive\nexperiments show that the proposed method surpasses existing techniques,\nenabling free generation and constrained generation, marking a shift towards\nstructure generation in architectural design. Code and data are available at\nhttps://github.com/SizheHu/GSDiff.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.16258v2"
    },
    {
        "title": "Estimation and Visualization of Isosurface Uncertainty from Linear and\n  High-Order Interpolation Methods",
        "authors": [
            "Timbwaoga A. J. Ouermi",
            "Jixian Li",
            "Tushar Athawale",
            "Chris R. Johnson"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Isosurface visualization is fundamental for exploring and analyzing 3D\nvolumetric data. Marching cubes (MC) algorithms with linear interpolation are\ncommonly used for isosurface extraction and visualization. Although linear\ninterpolation is easy to implement, it has limitations when the underlying data\nis complex and high-order, which is the case for most real-world data. Linear\ninterpolation can output vertices at the wrong location. Its inability to deal\nwith sharp features and features smaller than grid cells can lead to an\nincorrect isosurface with holes and broken pieces. Despite these limitations,\nisosurface visualizations typically do not include insight into the spatial\nlocation and the magnitude of these errors. We utilize high-order interpolation\nmethods with MC algorithms and interactive visualization to highlight these\nuncertainties. Our visualization tool helps identify the regions of high\ninterpolation errors. It also allows users to query local areas for details and\ncompare the differences between isosurfaces from different interpolation\nmethods. In addition, we employ high-order methods to identify and reconstruct\npossible features that linear methods cannot detect. We showcase how our\nvisualization tool helps explore and understand the extracted isosurface errors\nthrough synthetic and real-world data.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.00043v1"
    },
    {
        "title": "DiffCSG: Differentiable CSG via Rasterization",
        "authors": [
            "Haocheng Yuan",
            "Adrien Bousseau",
            "Hao Pan",
            "Chengquan Zhang",
            "Niloy J. Mitra",
            "Changjian Li"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Differentiable rendering is a key ingredient for inverse rendering and\nmachine learning, as it allows to optimize scene parameters (shape, materials,\nlighting) to best fit target images. Differentiable rendering requires that\neach scene parameter relates to pixel values through differentiable operations.\nWhile 3D mesh rendering algorithms have been implemented in a differentiable\nway, these algorithms do not directly extend to Constructive-Solid-Geometry\n(CSG), a popular parametric representation of shapes, because the underlying\nboolean operations are typically performed with complex black-box\nmesh-processing libraries. We present an algorithm, DiffCSG, to render CSG\nmodels in a differentiable manner. Our algorithm builds upon CSG rasterization,\nwhich displays the result of boolean operations between primitives without\nexplicitly computing the resulting mesh and, as such, bypasses black-box mesh\nprocessing. We describe how to implement CSG rasterization within a\ndifferentiable rendering pipeline, taking special care to apply antialiasing\nalong primitive intersections to obtain gradients in such critical areas. Our\nalgorithm is simple and fast, can be easily incorporated into modern machine\nlearning setups, and enables a range of applications for computer-aided design,\nincluding direct and image-based editing of CSG primitives. Code and data:\nhttps://yyyyyhc.github.io/DiffCSG/.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.01421v2"
    },
    {
        "title": "Efficient Scene Appearance Aggregation for Level-of-Detail Rendering",
        "authors": [
            "Yang Zhou",
            "Tao Huang",
            "Ravi Ramamoorthi",
            "Pradeep Sen",
            "Ling-Qi Yan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Creating an appearance-preserving level-of-detail (LoD) representation for\narbitrary 3D scenes is a challenging problem. The appearance of a scene is an\nintricate combination of both geometry and material models, and is further\ncomplicated by correlation due to the spatial configuration of scene elements.\nWe present a novel volumetric representation for the aggregated appearance of\ncomplex scenes and an efficient pipeline for LoD generation and rendering. The\ncore of our representation is the Aggregated Bidirectional Scattering\nDistribution Function (ABSDF) that summarizes the far-field appearance of all\nsurfaces inside a voxel. We propose a closed-form factorization of the ABSDF\nthat accounts for spatially varying and orientation-varying material\nparameters. We tackle the challenge of capturing the correlation existing\nlocally within a voxel and globally across different parts of the scene. Our\nmethod faithfully reproduces appearance and achieves higher quality than\nexisting scene filtering methods while being inherently efficient to render.\nThe memory footprint and rendering cost of our representation are independent\nof the original scene complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.03761v1"
    },
    {
        "title": "PhysHand: A Hand Simulation Model with Physiological Geometry, Physical\n  Deformation, and Accurate Contact Handling",
        "authors": [
            "Mingyang Sun",
            "Dongliang Kou",
            "Ruisheng Yuan",
            "Dingkang Yang",
            "Peng Zhai",
            "Xiao Zhao",
            "Yang Jiang",
            "Xiong Li",
            "Jingchen Li",
            "Lihua Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In virtual Hand-Object Interaction (HOI) scenarios, the authenticity of the\nhand's deformation is important to immersive experience, such as natural\nmanipulation or tactile feedback. Unrealistic deformation arises from\nsimplified hand geometry, neglect of the different physics attributes of the\nhand, and penetration due to imprecise contact handling. To address these\nproblems, we propose PhysHand, a novel hand simulation model, which enhances\nthe realism of deformation in HOI. First, we construct a physiologically\nplausible geometry, a layered mesh with a \"skin-flesh-skeleton\" structure.\nSecond, to satisfy the distinct physics features of different soft tissues, a\nconstraint-based dynamics framework is adopted with carefully designed\nlayer-corresponding constraints to maintain flesh attached and skin smooth.\nFinally, we employ an SDF-based method to eliminate the penetration caused by\ncontacts and enhance its accuracy by introducing a novel multi-resolution\nquerying strategy. Extensive experiments have been conducted to demonstrate the\noutstanding performance of PhysHand in calculating deformations and handling\ncontacts. Compared to existing methods, our PhysHand: 1) can compute both\nphysiologically and physically plausible deformation; 2) significantly reduces\nthe depth and count of penetration in HOI.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.05143v1"
    },
    {
        "title": "Holonomy: A Virtual Reality Exploration of Hyperbolic Geometry",
        "authors": [
            "Martin Skrodzki",
            "Scott Jochems",
            "Joris Rijsdijk",
            "Ravi Snellenberg",
            "Rafael Bidarra"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  HOLONOMY is a virtual environment based on the mathematical concept of\nhyperbolic geometry. Unlike other environments, HOLONOMY allows users to\nseamlessly explore an infinite hyperbolic space by physically walking. They use\ntheir body as the controller, eliminating the need for teleportation or other\nartificial VR locomotion methods. This paper discusses the development of\nHOLONOMY, highlighting the technical challenges faced and overcome during its\ncreation, including rendering complex hyperbolic environments, populating the\nspace with objects, and implementing algorithms for finding shortest paths in\nthe underlying non-Euclidean geometry. Furthermore, we present a\nproof-of-concept implementation in the form of a VR navigation game and some\npreliminary learning outcomes from this implementation.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.05460v1"
    },
    {
        "title": "NESI: Shape Representation via Neural Explicit Surface Intersection",
        "authors": [
            "Congyi Zhang",
            "Jinfan Yang",
            "Eric Hedlin",
            "Suzuran Takikawa",
            "Nicholas Vining",
            "Kwang Moo Yi",
            "Wenping Wang",
            "Alla Sheffer"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Compressed representations of 3D shapes that are compact, accurate, and can\nbe processed efficiently directly in compressed form, are extremely useful for\ndigital media applications. Recent approaches in this space focus on learned\nimplicit or parametric representations. While implicits are well suited for\ntasks such as in-out queries, they lack natural 2D parameterization,\ncomplicating tasks such as texture or normal mapping. Conversely, parametric\nrepresentations support the latter tasks but are ill-suited for occupancy\nqueries. We propose a novel learned alternative to these approaches, based on\nintersections of localized explicit, or height-field, surfaces. Since explicits\ncan be trivially expressed both implicitly and parametrically, NESI directly\nsupports a wider range of processing operations than implicit alternatives,\nincluding occupancy queries and parametric access. We represent input shapes\nusing a collection of differently oriented height-field bounded half-spaces\ncombined using volumetric Boolean intersections. We first tightly bound each\ninput using a pair of oppositely oriented height-fields, forming a Double\nHeight-Field (DHF) Hull. We refine this hull by intersecting it with additional\nlocalized height-fields (HFs) that capture surface regions in its interior. We\nminimize the number of HFs necessary to accurately capture each input and\ncompactly encode both the DHF hull and the local HFs as neural functions\ndefined over subdomains of R^2. This reduced dimensionality encoding delivers\nhigh-quality compact approximations. Given similar parameter count, or storage\ncapacity, NESI significantly reduces approximation error compared to the state\nof the art, especially at lower parameter counts.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.06030v1"
    },
    {
        "title": "Particle-Laden Fluid on Flow Maps",
        "authors": [
            "Zhiqi Li",
            "Duowen Chen",
            "Candong Lin",
            "Jinyuan Liu",
            "Bo Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a novel framework for simulating ink as a particle-laden flow\nusing particle flow maps. Our method addresses the limitations of existing\nflow-map techniques, which struggle with dissipative forces like viscosity and\ndrag, thereby extending the application scope from solving the Euler equations\nto solving the Navier-Stokes equations with accurate viscosity and\nladen-particle treatment. Our key contribution lies in a coupling mechanism for\ntwo particle systems, coupling physical sediment particles and virtual flow-map\nparticles on a background grid by solving a Poisson system. We implemented a\nnovel path integral formula to incorporate viscosity and drag forces into the\nparticle flow map process. Our approach enables state-of-the-art simulation of\nvarious particle-laden flow phenomena, exemplified by the bulging and breakup\nof suspension drop tails, torus formation, torus disintegration, and the\ncoalescence of sedimenting drops. In particular, our method delivered\nhigh-fidelity ink diffusion simulations by accurately capturing vortex bulbs,\nviscous tails, fractal branching, and hierarchical structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.06246v1"
    },
    {
        "title": "Jump Restore Light Transport",
        "authors": [
            "Sascha Holl",
            "Hans-Peter Seidel",
            "Gurprit Singh"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Markov chain Monte Carlo (MCMC) algorithms come to rescue when sampling from\na complex, high-dimensional distribution by a conventional method is\nintractable. Even though MCMC is a powerful tool, it is also hard to control\nand tune in practice. Simultaneously achieving both local exploration of the\nstate space and global discovery of the target distribution is a challenging\ntask. In this work, we present a MCMC formulation that subsumes all existing\nMCMC samplers employed in rendering. We then present a novel framework for\nadjusting an arbitrary Markov chain, making it exhibit invariance with respect\nto a specified target distribution. To showcase the potential of the proposed\nframework, we focus on a first simple application in light transport\nsimulation. As a by-product, we introduce continuous-time MCMC sampling to the\ncomputer graphics community. We show how any existing MCMC-based light\ntransport algorithm can be embedded into our framework. We empirically and\ntheoretically prove that this embedding is superior to running the standalone\nalgorithm. In fact, our approach will convert any existing algorithm into a\nhighly parallelizable variant with shorter running time, smaller error and less\nvariance.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.07148v5"
    },
    {
        "title": "Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric\n  Videos",
        "authors": [
            "Yuheng Jiang",
            "Zhehao Shen",
            "Yu Hong",
            "Chengcheng Guo",
            "Yize Wu",
            "Yingliang Zhang",
            "Jingyi Yu",
            "Lan Xu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Volumetric video represents a transformative advancement in visual media,\nenabling users to freely navigate immersive virtual experiences and narrowing\nthe gap between digital and real worlds. However, the need for extensive manual\nintervention to stabilize mesh sequences and the generation of excessively\nlarge assets in existing workflows impedes broader adoption. In this paper, we\npresent a novel Gaussian-based approach, dubbed \\textit{DualGS}, for real-time\nand high-fidelity playback of complex human performance with excellent\ncompression ratios. Our key idea in DualGS is to separately represent motion\nand appearance using the corresponding skin and joint Gaussians. Such an\nexplicit disentanglement can significantly reduce motion redundancy and enhance\ntemporal coherence. We begin by initializing the DualGS and anchoring skin\nGaussians to joint Gaussians at the first frame. Subsequently, we employ a\ncoarse-to-fine training strategy for frame-by-frame human performance modeling.\nIt includes a coarse alignment phase for overall motion prediction as well as a\nfine-grained optimization for robust tracking and high-fidelity rendering. To\nintegrate volumetric video seamlessly into VR environments, we efficiently\ncompress motion using entropy encoding and appearance using codec compression\ncoupled with a persistent codebook. Our approach achieves a compression ratio\nof up to 120 times, only requiring approximately 350KB of storage per frame. We\ndemonstrate the efficacy of our representation through photo-realistic,\nfree-view experiences on VR headsets, enabling users to immersively watch\nmusicians in performance and feel the rhythm of the notes at the performers'\nfingertips.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08353v1"
    },
    {
        "title": "Solid-Fluid Interaction on Particle Flow Maps",
        "authors": [
            "Duowen Chen",
            "Zhiqi Li",
            "Junwei Zhou",
            "Fan Feng",
            "Tao Du",
            "Bo Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a novel solid-fluid interaction method for coupling elastic solids\nwith impulse flow maps. Our key idea is to unify the representation of fluid\nand solid components as particle flow maps with different lengths and dynamics.\nThe solid-fluid coupling is enabled by implementing two novel mechanisms:\nfirst, we developed an impulse-to-velocity transfer mechanism to unify the\nexchanged physical quantities; second, we devised a particle path integral\nmechanism to accumulate coupling forces along each flow-map trajectory. Our\nframework integrates these two mechanisms into an Eulerian-Lagrangian impulse\nfluid simulator to accommodate traditional coupling models, exemplified by the\nMaterial Point Method (MPM) and Immersed Boundary Method (IBM), within a\nparticle flow map framework. We demonstrate our method's efficacy by simulating\nsolid-fluid interactions exhibiting strong vortical dynamics, including various\nvortex shedding and interaction examples across swimming, falling, breezing,\nand combustion.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.09225v1"
    },
    {
        "title": "Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering",
        "authors": [
            "Euntae Choi",
            "Sungjoo Yoo"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose two novel ideas (adoption of deferred rendering and mesh-based\nrepresentation) to improve the quality of 3D Gaussian splatting (3DGS) based\ninverse rendering. We first report a problem incurred by hidden Gaussians,\nwhere Gaussians beneath the surface adversely affect the pixel color in the\nvolume rendering adopted by the existing methods. In order to resolve the\nproblem, we propose applying deferred rendering and report new problems\nincurred in a naive application of deferred rendering to the existing\n3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-based\ninverse rendering under deferred rendering, we propose a novel two-step\ntraining approach which (1) exploits mesh extraction and utilizes a hybrid\nmesh-3DGS representation and (2) applies novel regularization methods to better\nexploit the mesh. Our experiments show that, under relighting, the proposed\nmethod offers significantly better rendering quality than the existing\n3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-based\ninverse rendering method, it gives better rendering quality while offering\nreal-time rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.10335v1"
    },
    {
        "title": "FreeAvatar: Robust 3D Facial Animation Transfer by Learning an\n  Expression Foundation Model",
        "authors": [
            "Feng Qiu",
            "Wei Zhang",
            "Chen Liu",
            "Rudong An",
            "Lincheng Li",
            "Yu Ding",
            "Changjie Fan",
            "Zhipeng Hu",
            "Xin Yu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Video-driven 3D facial animation transfer aims to drive avatars to reproduce\nthe expressions of actors. Existing methods have achieved remarkable results by\nconstraining both geometric and perceptual consistency. However, geometric\nconstraints (like those designed on facial landmarks) are insufficient to\ncapture subtle emotions, while expression features trained on classification\ntasks lack fine granularity for complex emotions. To address this, we propose\n\\textbf{FreeAvatar}, a robust facial animation transfer method that relies\nsolely on our learned expression representation. Specifically, FreeAvatar\nconsists of two main components: the expression foundation model and the facial\nanimation transfer model. In the first component, we initially construct a\nfacial feature space through a face reconstruction task and then optimize the\nexpression feature space by exploring the similarities among different\nexpressions. Benefiting from training on the amounts of unlabeled facial images\nand re-collected expression comparison dataset, our model adapts freely and\neffectively to any in-the-wild input facial images. In the facial animation\ntransfer component, we propose a novel Expression-driven Multi-avatar Animator,\nwhich first maps expressive semantics to the facial control parameters of 3D\navatars and then imposes perceptual constraints between the input and output\nimages to maintain expression consistency. To make the entire process\ndifferentiable, we employ a trained neural renderer to translate rig parameters\ninto corresponding images. Furthermore, unlike previous methods that require\nseparate decoders for each avatar, we propose a dynamic identity injection\nmodule that allows for the joint training of multiple avatars within a single\nnetwork.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.13180v2"
    },
    {
        "title": "Real-time Diverse Motion In-betweening with Space-time Control",
        "authors": [
            "Yuchen Chu",
            "Zeshi Yang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this work, we present a data-driven framework for generating diverse\nin-betweening motions for kinematic characters. Our approach injects dynamic\nconditions and explicit motion controls into the procedure of motion\ntransitions. Notably, this integration enables a finer-grained spatial-temporal\ncontrol by allowing users to impart additional conditions, such as duration,\npath, style, etc., into the in-betweening process. We demonstrate that our\nin-betweening approach can synthesize both locomotion and unstructured motions,\nenabling rich, versatile, and high-quality animation generation.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.00270v1"
    },
    {
        "title": "Deep image-based Adaptive BRDF Measure",
        "authors": [
            "Wen Cao"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Efficient and accurate measurement of the bi-directional reflectance\ndistribution function (BRDF) plays a key role in high quality image rendering\nand physically accurate sensor simulation. However, obtaining the reflectance\nproperties of a material is both time-consuming and challenging. This paper\npresents a novel method for minimizing the number of samples required for high\nquality BRDF capture using a gonio-reflectometer setup. Taking an image of the\nphysical material sample as input a lightweight neural network first estimates\nthe parameters of an analytic BRDF model, and the distribution of the sample\nlocations. In a second step we use an image based loss to find the number of\nsamples required to meet the accuracy required. This approach significantly\naccelerates the measurement process while maintaining a high level of accuracy\nand fidelity in the BRDF representation.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.02917v1"
    },
    {
        "title": "Shrinking: Reconstruction of Parameterized Surfaces from Signed Distance\n  Fields",
        "authors": [
            "Haotian Yin",
            "Przemyslaw Musialski"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a novel method for reconstructing explicit parameterized surfaces\nfrom Signed Distance Fields (SDFs), a widely used implicit neural\nrepresentation (INR) for 3D surfaces. While traditional reconstruction methods\nlike Marching Cubes extract discrete meshes that lose the continuous and\ndifferentiable properties of INRs, our approach iteratively contracts a\nparameterized initial sphere to conform to the target SDF shape, preserving\ndifferentiability and surface parameterization throughout. This enables\ndownstream applications such as texture mapping, geometry processing,\nanimation, and finite element analysis. Evaluated on the typical geometric\nshapes and parts of the ABC dataset, our method achieves competitive\nreconstruction quality, maintaining smoothness and differentiability crucial\nfor advanced computer graphics and geometric deep learning applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.03123v1"
    },
    {
        "title": "Counterpoint: Orchestrating Large-Scale Custom Animated Visualizations",
        "authors": [
            "Venkatesh Sivaraman",
            "Frank Elavsky",
            "Dominik Moritz",
            "Adam Perer"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Custom animated visualizations of large, complex datasets are helpful across\nmany domains, but they are hard to develop. Much of the difficulty arises from\nmaintaining visualization state across many animated graphical elements that\nmay change in number over time. We contribute Counterpoint, a framework for\nstate management designed to help implement such visualizations in JavaScript.\nUsing Counterpoint, developers can manipulate large collections of marks with\nreactive attributes that are easy to render in scalable APIs such as Canvas and\nWebGL. Counterpoint also helps orchestrate the entry and exit of graphical\nelements using the concept of a rendering \"stage.\" Through a performance\nevaluation, we show that Counterpoint adds minimal overhead over current\nhigh-performance rendering techniques while simplifying implementation. We\nprovide two examples of visualizations created using Counterpoint that\nillustrate its flexibility and compatibility with other visualization toolkits\nas well as considerations for users with disabilities. Counterpoint is\nopen-source and available at https://github.com/cmudig/counterpoint.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.05645v1"
    },
    {
        "title": "Focal Surface Holographic Light Transport using Learned Spatially\n  Adaptive Convolutions",
        "authors": [
            "Chuanjun Zheng",
            "Yicheng Zhan",
            "Liang Shi",
            "Ozan Cakmakci",
            "Kaan Akşit"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Computer-Generated Holography (CGH) is a set of algorithmic methods for\nidentifying holograms that reconstruct Three-Dimensional (3D) scenes in\nholographic displays. CGH algorithms decompose 3D scenes into multiplanes at\ndifferent depth levels and rely on simulations of light that propagated from a\nsource plane to a targeted plane. Thus, for n planes, CGH typically optimizes\nholograms using n plane-to-plane light transport simulations, leading to major\ntime and computational demands. Our work replaces multiple planes with a focal\nsurface and introduces a learned light transport model that could propagate a\nlight field from a source plane to the focal surface in a single inference. Our\nlearned light transport model leverages spatially adaptive convolution to\nachieve depth-varying propagation demanded by targeted focal surfaces. The\nproposed model reduces the hologram optimization process up to 1.5x, which\ncontributes to hologram dataset generation and the training of future learned\nCGH models.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.06854v2"
    },
    {
        "title": "Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid\n  Transparency",
        "authors": [
            "Florian Hahlbohm",
            "Fabian Friederichs",
            "Tim Weyrich",
            "Linus Franke",
            "Moritz Kappel",
            "Susana Castillo",
            "Marc Stamminger",
            "Martin Eisemann",
            "Marcus Magnor"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, both\nfor inverse rendering as well as real-time exploration of scenes. In these\napplications, coherence across camera frames and multiple views is crucial, be\nit for robust convergence of a scene reconstruction or for artifact-free\nfly-throughs. Recent work started mitigating artifacts that break multi-view\ncoherence, including popping artifacts due to inconsistent transparency sorting\nand perspective-correct outlines of (2D) splats. At the same time, real-time\nrequirements forced such implementations to accept compromises in how\ntransparency of large assemblies of 3D Gaussians is resolved, in turn breaking\ncoherence in other ways. In our work, we aim at achieving maximum coherence, by\nrendering fully perspective-correct 3D Gaussians while using a high-quality\napproximation of accurate blending, hybrid transparency, on a per-pixel level,\nin order to retain real-time frame rates. Our fast and perspectively accurate\napproach for evaluation of 3D Gaussians does not require matrix inversions,\nthereby ensuring numerical stability and eliminating the need for special\nhandling of degenerate splats, and the hybrid transparency formulation for\nblending maintains similar quality as fully resolved per-pixel transparencies\nat a fraction of the rendering costs. We further show that each of these two\ncomponents can be independently integrated into Gaussian splatting systems. In\ncombination, they achieve up to 2$\\times$ higher frame rates, 2$\\times$ faster\noptimization, and equal or better image quality with fewer rendering artifacts\ncompared to traditional 3DGS on common benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.08129v2"
    },
    {
        "title": "Neurally Integrated Finite Elements for Differentiable Elasticity on\n  Evolving Domains",
        "authors": [
            "Gilles Daviet",
            "Tianchang Shen",
            "Nicholas Sharp",
            "David I. W. Levin"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present an elastic simulator for domains defined as evolving implicit\nfunctions, which is efficient, robust, and differentiable with respect to both\nshape and material. This simulator is motivated by applications in 3D\nreconstruction: it is increasingly effective to recover geometry from observed\nimages as implicit functions, but physical applications require accurately\nsimulating and optimizing-for the behavior of such shapes under deformation,\nwhich has remained challenging. Our key technical innovation is to train a\nsmall neural network to fit quadrature points for robust numerical integration\non implicit grid cells. When coupled with a Mixed Finite Element formulation,\nthis yields a smooth, fully differentiable simulation model connecting the\nevolution of the underlying implicit surface to its elastic response. We\ndemonstrate the efficacy of our approach on forward simulation of implicits,\ndirect simulation of 3D shapes during editing, and novel physics-based shape\nand topology optimizations in conjunction with differentiable rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.09417v1"
    },
    {
        "title": "An Interface Tracking Method with Triangle Edge Cuts",
        "authors": [
            "Mengdi Wang",
            "Matthew Cong",
            "Bo Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper introduces a volume-conserving interface tracking algorithm on\nunstructured triangle meshes. We propose to discretize the interface via\ntriangle edge cuts which represent the intersections between the interface and\nthe triangle mesh edges using a compact 6 numbers per triangle. This enables an\nefficient implicit representation of the sub-triangle polygonal material\nregions without explicitly storing connectivity information. Moreover, we\npropose an efficient advection algorithm for this interface representation that\nis based on geometric queries and does not require an optimization process.\nThis advection algorithm is extended via an area correction step that enforces\nvolume-conservation of the materials. We demonstrate the efficacy of our method\non a variety of advection problems on a triangle mesh and compare its\nperformance to existing interface tracking methods including VOF and MOF.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.11073v2"
    },
    {
        "title": "Triplet: Triangle Patchlet for Mesh-Based Inverse Rendering and Scene\n  Parameters Approximation",
        "authors": [
            "Jiajie Yang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Recent advancements in Radiance Fields have significantly improved novel-view\nsynthesis. However, in many real-world applications, the more advanced\nchallenge lies in inverse rendering, which seeks to derive the physical\nproperties of a scene, including light, geometry, textures, and materials.\nMeshes, as a traditional representation adopted by many simulation pipeline,\nhowever, still show limited influence in radiance field for inverse rendering.\nThis paper introduces a novel framework called Triangle Patchlet (abbr.\nTriplet), a mesh-based representation, to comprehensively approximate these\nscene parameters. We begin by assembling Triplets with either randomly\ngenerated points or sparse points obtained from camera calibration where all\nfaces are treated as an independent element. Next, we simulate the physical\ninteraction of light and optimize the scene parameters using traditional\ngraphics rendering techniques like rasterization and ray tracing, accompanying\nwith density control and propagation. An iterative mesh extracting process is\nalso suggested, where we continue to optimize on geometry and materials with\ngraph-based operation. We also introduce several regulation terms to enable\nbetter generalization of materials property. Our framework could precisely\nestimate the light, materials and geometry with mesh without prior of light,\nmaterials and geometry in a unified framework. Experiments demonstrate that our\napproach can achieve state-of-the-art visual quality while reconstructing\nhigh-quality geometry and accurate material properties.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12414v1"
    },
    {
        "title": "Eyelid Fold Consistency in Facial Modeling",
        "authors": [
            "Lohit Petikam",
            "Charlie Hewitt",
            "Fatemeh Saleh",
            "Tadas Baltrušaitis"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Eyelid shape is integral to identity and likeness in human facial modeling.\nHuman eyelids are diverse in appearance with varied skin fold and epicanthal\nfold morphology between individuals. Existing parametric face models express\neyelid shape variation to an extent, but do not preserve sufficient likeness\nacross a diverse range of individuals. We propose a new definition of eyelid\nfold consistency and implement geometric processing techniques to model diverse\neyelid shapes in a unified topology. Using this method we reprocess data used\nto train a parametric face model and demonstrate significant improvements in\nface-related machine learning tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.13760v1"
    },
    {
        "title": "TexPro: Text-guided PBR Texturing with Procedural Material Modeling",
        "authors": [
            "Ziqiang Dang",
            "Wenqi Dong",
            "Zesong Yang",
            "Bangbang Yang",
            "Liang Li",
            "Yuewen Ma",
            "Zhaopeng Cui"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this paper, we present TexPro, a novel method for high-fidelity material\ngeneration for input 3D meshes given text prompts. Unlike existing\ntext-conditioned texture generation methods that typically generate RGB\ntextures with baked lighting, TexPro is able to produce diverse texture maps\nvia procedural material modeling, which enables physical-based rendering,\nrelighting, and additional benefits inherent to procedural materials.\nSpecifically, we first generate multi-view reference images given the input\ntextual prompt by employing the latest text-to-image model. We then derive\ntexture maps through a rendering-based optimization with recent differentiable\nprocedural materials. To this end, we design several techniques to handle the\nmisalignment between the generated multi-view images and 3D meshes, and\nintroduce a novel material agent that enhances material classification and\nmatching by exploring both part-level understanding and object-aware material\nreasoning. Experiments demonstrate the superiority of the proposed method over\nexisting SOTAs and its capability of relighting.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.15891v1"
    },
    {
        "title": "Toolpath Generation for High Density Spatial Fiber Printing Guided by\n  Principal Stresses",
        "authors": [
            "Tianyu Zhang",
            "Tao Liu",
            "Neelotpal Dutta",
            "Yongxue Chen",
            "Renbo Su",
            "Zhizhou Zhang",
            "Weiming Wang",
            "Charlie C. L. Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  While multi-axis 3D printing can align continuous fibers along principal\nstresses in continuous fiber-reinforced thermoplastic (CFRTP) composites to\nenhance mechanical strength, existing methods have difficulty generating\ntoolpaths with high fiber coverage. This is mainly due to the orientation\nconsistency constraints imposed by vector-field-based methods and the turbulent\nstress fields around stress concentration regions. This paper addresses these\nchallenges by introducing a 2-RoSy representation for computing the direction\nfield, which is then converted into a periodic scalar field to generate partial\niso-curves for fiber toolpaths with nearly equal hatching distance. To improve\nfiber coverage in stress-concentrated regions, such as around holes, we extend\nthe quaternion-based method for curved slicing by incorporating winding\ncompatibility considerations. Our proposed method can achieve toolpaths\ncoverage between 87.5% and 90.6% by continuous fibers with 1.1mm width. Models\nfabricated using our toolpaths show up to 84.6% improvement in failure load and\n54.4% increase in stiffness when compared to the results obtained from\nmulti-axis 3D printing with sparser fibers.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16851v1"
    },
    {
        "title": "Multi-Layer Gaussian Splatting for Immersive Anatomy Visualization",
        "authors": [
            "Constantin Kleinbeck",
            "Hannah Schieber",
            "Klaus Engel",
            "Ralf Gutjahr",
            "Daniel Roth"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In medical image visualization, path tracing of volumetric medical data like\nCT scans produces lifelike three-dimensional visualizations. Immersive VR\ndisplays can further enhance the understanding of complex anatomies. Going\nbeyond the diagnostic quality of traditional 2D slices, they enable interactive\n3D evaluation of anatomies, supporting medical education and planning.\nRendering high-quality visualizations in real-time, however, is computationally\nintensive and impractical for compute-constrained devices like mobile headsets.\n  We propose a novel approach utilizing GS to create an efficient but static\nintermediate representation of CT scans. We introduce a layered GS\nrepresentation, incrementally including different anatomical structures while\nminimizing overlap and extending the GS training to remove inactive Gaussians.\nWe further compress the created model with clustering across layers.\n  Our approach achieves interactive frame rates while preserving anatomical\nstructures, with quality adjustable to the target hardware. Compared to\nstandard GS, our representation retains some of the explorative qualities\ninitially enabled by immersive path tracing. Selective activation and clipping\nof layers are possible at rendering time, adding a degree of interactivity to\notherwise static GS models. This could enable scenarios where high\ncomputational demands would otherwise prohibit using path-traced medical\nvolumes.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16978v1"
    },
    {
        "title": "Progressive Glimmer: Expanding Dimensionality in Multidimensional\n  Scaling",
        "authors": [
            "Marina Evers",
            "David Hägele",
            "Sören Döring",
            "Daniel Weiskopf"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Progressive dimensionality reduction algorithms allow for visually\ninvestigating intermediate results, especially for large data sets. While\ndifferent algorithms exist that progressively increase the number of data\npoints, we propose an algorithm that allows for increasing the number of\ndimensions. Especially in spatio-temporal data, where each spatial location can\nbe seen as one data point and each time step as one dimension, the data is\noften stored in a format that supports quick access to the individual\ndimensions of all points. Therefore, we propose Progressive Glimmer, a\nprogressive multidimensional scaling (MDS) algorithm. We adapt the Glimmer\nalgorithm to support progressive updates for changes in the data's\ndimensionality. We evaluate Progressive Glimmer's embedding quality and\nruntime. We observe that the algorithm provides more stable results, leading to\nvisually consistent results for progressive rendering and making the approach\napplicable to streaming data. We show the applicability of our approach to\nspatio-temporal simulation ensemble data where we add the individual ensemble\nmembers progressively.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.19430v1"
    },
    {
        "title": "All-frequency Full-body Human Image Relighting",
        "authors": [
            "Daichi Tajima",
            "Yoshihiro Kanamori",
            "Yuki Endo"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Relighting of human images enables post-photography editing of lighting\neffects in portraits. The current mainstream approach uses neural networks to\napproximate lighting effects without explicitly accounting for the principle of\nphysical shading. As a result, it often has difficulty representing\nhigh-frequency shadows and shading. In this paper, we propose a two-stage\nrelighting method that can reproduce physically-based shadows and shading from\nlow to high frequencies. The key idea is to approximate an environment light\nsource with a set of a fixed number of area light sources. The first stage\nemploys supervised inverse rendering from a single image using neural networks\nand calculates physically-based shading. The second stage then calculates\nshadow for each area light and sums up to render the final image. We propose to\nmake soft shadow mapping differentiable for the area-light approximation of\nenvironment lighting. We demonstrate that our method can plausibly reproduce\nall-frequency shadows and shading caused by environment illumination, which\nhave been difficult to reproduce using existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.00356v1"
    },
    {
        "title": "Advancing GPU IPC for stiff affine-deformable simulation",
        "authors": [
            "Kemeng Huang",
            "Xinyu Lu",
            "Huancheng Lin",
            "Taku Komura",
            "Minchen Li"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Incremental Potential Contact (IPC) is a widely used, robust, and accurate\nmethod for simulating complex frictional contact behaviors. However, achieving\nhigh efficiency remains a major challenge, particularly as material stiffness\nincreases, which leads to slower Preconditioned Conjugate Gradient (PCG)\nconvergence, even with the state-of-the-art preconditioners. In this paper, we\npropose a fully GPU-optimized IPC simulation framework capable of handling\nmaterials across a wide range of stiffnesses, delivering consistent high\nperformance and scalability with up to 10x speedup over state-of-the-art GPU\nIPC methods. Our framework introduces three key innovations: 1) A novel\nconnectivity-enhanced Multilevel Additive Schwarz (MAS) preconditioner on the\nGPU, designed to efficiently capture both stiff and soft elastodynamics and\nimprove PCG convergence at a reduced preconditioning cost. 2) A C2-continuous\ncubic energy with an analytic eigensystem for strain limiting, enabling more\nparallel-friendly simulations of stiff membranes, such as cloth, without\nmembrane locking. 3) For extremely stiff behaviors where elastic waves are\nbarely visible, we employ affine body dynamics (ABD) with a hash-based\nmulti-layer reduction strategy for fast Hessian assembly and efficient\naffine-deformable coupling. We conduct extensive performance analyses and\nbenchmark studies to compare our framework against state-of-the-art methods and\nalternative design choices. Our system consistently delivers the fastest\nperformance across soft, stiff, and hybrid simulation scenarios, even in cases\nwith high resolution, large deformations, and high-speed impacts. Our framework\nwill be fully open-sourced upon acceptance.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.06224v2"
    },
    {
        "title": "Scaling Mesh Generation via Compressive Tokenization",
        "authors": [
            "Haohan Weng",
            "Zibo Zhao",
            "Biwen Lei",
            "Xianghui Yang",
            "Jian Liu",
            "Zeqiang Lai",
            "Zhuo Chen",
            "Yuhong Liu",
            "Jie Jiang",
            "Chunchao Guo",
            "Tong Zhang",
            "Shenghua Gao",
            "C. L. Philip Chen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a compressive yet effective mesh representation, Blocked and\nPatchified Tokenization (BPT), facilitating the generation of meshes exceeding\n8k faces. BPT compresses mesh sequences by employing block-wise indexing and\npatch aggregation, reducing their length by approximately 75\\% compared to the\noriginal sequences. This compression milestone unlocks the potential to utilize\nmesh data with significantly more faces, thereby enhancing detail richness and\nimproving generation robustness. Empowered with the BPT, we have built a\nfoundation mesh generative model training on scaled mesh data to support\nflexible control for point clouds and images. Our model demonstrates the\ncapability to generate meshes with intricate details and accurate topology,\nachieving SoTA performance on mesh generation and reaching the level for direct\nproduct usage.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07025v1"
    },
    {
        "title": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation",
        "authors": [
            "Rang Meng",
            "Xingyu Zhang",
            "Yuming Li",
            "Chenguang Ma"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Recent work on human animation usually involves audio, pose, or movement maps\nconditions, thereby achieves vivid animation quality. However, these methods\noften face practical challenges due to extra control conditions, cumbersome\ncondition injection modules, or limitation to head region driving. Hence, we\nask if it is possible to achieve striking half-body human animation while\nsimplifying unnecessary conditions. To this end, we propose a half-body human\nanimation method, dubbed EchoMimicV2, that leverages a novel Audio-Pose Dynamic\nHarmonization strategy, including Pose Sampling and Audio Diffusion, to enhance\nhalf-body details, facial and gestural expressiveness, and meanwhile reduce\nconditions redundancy. To compensate for the scarcity of half-body data, we\nutilize Head Partial Attention to seamlessly accommodate headshot data into our\ntraining framework, which can be omitted during inference, providing a free\nlunch for animation. Furthermore, we design the Phase-specific Denoising Loss\nto guide motion, detail, and low-level quality for animation in specific\nphases, respectively. Besides, we also present a novel benchmark for evaluating\nthe effectiveness of half-body human animation. Extensive experiments and\nanalyses demonstrate that EchoMimicV2 surpasses existing methods in both\nquantitative and qualitative evaluations.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.10061v1"
    },
    {
        "title": "FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations",
        "authors": [
            "Hmrishav Bandyopadhyay",
            "Yi-Zhe Song"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Sketch animations offer a powerful medium for visual storytelling, from\nsimple flip-book doodles to professional studio productions. While traditional\nanimation requires teams of skilled artists to draw key frames and in-between\nframes, existing automation attempts still demand significant artistic effort\nthrough precise motion paths or keyframe specification. We present FlipSketch,\na system that brings back the magic of flip-book animation -- just draw your\nidea and describe how you want it to move! Our approach harnesses motion priors\nfrom text-to-video diffusion models, adapting them to generate sketch\nanimations through three key innovations: (i) fine-tuning for sketch-style\nframe generation, (ii) a reference frame mechanism that preserves visual\nintegrity of input sketch through noise refinement, and (iii) a dual-attention\ncomposition that enables fluid motion without losing visual consistency. Unlike\nconstrained vector animations, our raster frames support dynamic sketch\ntransformations, capturing the expressive freedom of traditional animation. The\nresult is an intuitive system that makes sketch animation as simple as doodling\nand describing, while maintaining the artistic essence of hand-drawn animation.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.10818v1"
    },
    {
        "title": "Newclid: A User-Friendly Replacement for AlphaGeometry",
        "authors": [
            "Vladmir Sicca",
            "Tianxiang Xia",
            "Mathïs Fédérico",
            "Philip John Gorinski",
            "Simon Frieder",
            "Shangling Jui"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce a new symbolic solver for geometry, called Newclid, which is\nbased on AlphaGeometry. Newclid contains a symbolic solver called DDARN\n(derived from DDAR-Newclid), which is a significant refactoring and upgrade of\nAlphaGeometry's DDAR symbolic solver by being more user-friendly - both for the\nend user as well as for a programmer wishing to extend the codebase. For the\nprogrammer, improvements include a modularized codebase and new debugging and\nvisualization tools. For the user, Newclid contains a new command line\ninterface (CLI) that provides interfaces for agents to guide DDARN. DDARN is\nflexible with respect to its internal reasoning, which can be steered by\nagents. Further, we support input from GeoGebra to make Newclid accessible for\neducational contexts. Further, the scope of problems that Newclid can solve has\nbeen expanded to include the ability to have an improved understanding of\nmetric geometry concepts (length, angle) and to use theorems such as the\nPythagorean theorem in proofs. Bugs have been fixed, and reproducibility has\nbeen improved. Lastly, we re-evaluated the five remaining problems from the\noriginal AG-30 dataset that AlphaGeometry was not able to solve and contrasted\nthem with the abilities of DDARN, running in breadth-first-search agentic mode\n(which corresponds to how DDARN runs by default), finding that DDARN solves an\nadditional problem. We have open-sourced our code under:\nhttps://github.com/LMCRC/Newclid\n",
        "pdf_link": "http://arxiv.org/pdf/2411.11938v1"
    },
    {
        "title": "DynamicAvatars: Accurate Dynamic Facial Avatars Reconstruction and\n  Precise Editing with Diffusion Models",
        "authors": [
            "Yangyang Qian",
            "Yuan Sun",
            "Yu Guo"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Generating and editing dynamic 3D head avatars are crucial tasks in virtual\nreality and film production. However, existing methods often suffer from facial\ndistortions, inaccurate head movements, and limited fine-grained editing\ncapabilities. To address these challenges, we present DynamicAvatars, a dynamic\nmodel that generates photorealistic, moving 3D head avatars from video clips\nand parameters associated with facial positions and expressions. Our approach\nenables precise editing through a novel prompt-based editing model, which\nintegrates user-provided prompts with guiding parameters derived from large\nlanguage models (LLMs). To achieve this, we propose a dual-tracking framework\nbased on Gaussian Splatting and introduce a prompt preprocessing module to\nenhance editing stability. By incorporating a specialized GAN algorithm and\nconnecting it to our control module, which generates precise guiding parameters\nfrom LLMs, we successfully address the limitations of existing methods.\nAdditionally, we develop a dynamic editing strategy that selectively utilizes\nspecific training datasets to improve the efficiency and adaptability of the\nmodel for dynamic editing tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.15732v1"
    },
    {
        "title": "Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready\n  3D Characters",
        "authors": [
            "Zhiyang Guo",
            "Jinxu Xiang",
            "Kai Ma",
            "Wengang Zhou",
            "Houqiang Li",
            "Ran Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  3D characters are essential to modern creative industries, but making them\nanimatable often demands extensive manual work in tasks like rigging and\nskinning. Existing automatic rigging tools face several limitations, including\nthe necessity for manual annotations, rigid skeleton topologies, and limited\ngeneralization across diverse shapes and poses. An alternative approach is to\ngenerate animatable avatars pre-bound to a rigged template mesh. However, this\nmethod often lacks flexibility and is typically limited to realistic human\nshapes. To address these issues, we present Make-It-Animatable, a novel\ndata-driven method to make any 3D humanoid model ready for character animation\nin less than one second, regardless of its shapes and poses. Our unified\nframework generates high-quality blend weights, bones, and pose\ntransformations. By incorporating a particle-based shape autoencoder, our\napproach supports various 3D representations, including meshes and 3D Gaussian\nsplats. Additionally, we employ a coarse-to-fine representation and a\nstructure-aware modeling strategy to ensure both accuracy and robustness, even\nfor characters with non-standard skeleton structures. We conducted extensive\nexperiments to validate our framework's effectiveness. Compared to existing\nmethods, our approach demonstrates significant improvements in both quality and\nspeed.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.18197v2"
    },
    {
        "title": "Volume Rendering of Human Hand Anatomy",
        "authors": [
            "Jingtao Huang",
            "Bohan Wang",
            "Zhiyuan Gao",
            "Mianlun Zheng",
            "George Matcuk",
            "Jernej Barbic"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We study the design of transfer functions for volumetric rendering of\nmagnetic resonance imaging (MRI) datasets of human hands. Human hands are\nanatomically complex, containing various organs within a limited space, which\npresents challenges for volumetric rendering. We focus on hand musculoskeletal\norgans because they are volumetrically the largest inside the hand, and most\nimportant for the hand's main function, namely manipulation of objects. While\nvolumetric rendering is a mature field, the choice of the transfer function for\nthe different organs is arguably just as important as the choice of the\nspecific volume rendering algorithm; we demonstrate that it significantly\ninfluences the clarity and interpretability of the resulting images. We assume\nthat the hand MRI scans have already been segmented into the different organs\n(bones, muscles, tendons, ligaments, subcutaneous fat, etc.). Our method uses\nthe hand MRI volume data, and the geometry of its inner organs and their known\nsegmentation, to produce high-quality volume rendering images of the hand, and\npermits fine control over the appearance of each tissue. We contribute two\nfamilies of transfer functions to emphasize different hand tissues of interest,\nwhile preserving the visual context of the hand. We also discuss and reduce\nartifacts present in standard volume ray-casting of human hands. We evaluate\nour volumetric rendering on five challenging hand motion sequences. Our\nexperimental results demonstrate that our method improves hand anatomy\nvisualization, compared to standard surface and volume rendering techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.18630v1"
    },
    {
        "title": "VR-Doh: Hands-on 3D Modeling in Virtual Reality",
        "authors": [
            "Zhaofeng Luo",
            "Zhitong Cui",
            "Shijian Luo",
            "Mengyu Chu",
            "Minchen Li"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present VR-Doh, a hands-on 3D modeling system designed for creating and\nmanipulating elastoplastic objects in virtual reality (VR). The system employs\nthe Material Point Method (MPM) for simulating realistic large deformations and\nincorporates optimized Gaussian Splatting for seamless rendering. With direct,\nhand-based interactions, users can naturally sculpt, deform, and edit objects\ninteractively. To achieve real-time performance, we developed localized\nsimulation techniques, optimized collision handling, and separated appearance\nand physical representations, ensuring smooth and responsive user interaction.\nThe system supports both freeform creation and precise adjustments, catering to\ndiverse modeling tasks. A user study involving novice and experienced users\nhighlights the system's intuitive design, immersive feedback, and creative\npotential. Compared to traditional geometry-based modeling tools, our approach\noffers improved accessibility and natural interaction in specific contexts.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.00814v1"
    },
    {
        "title": "Sprite Sheet Diffusion: Generate Game Character for Animation",
        "authors": [
            "Cheng-An Hsieh",
            "Jing Zhang",
            "Ava Yan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In the game development process, creating character animations is a vital\nstep that involves several stages. Typically for 2D games, illustrators begin\nby designing the main character image, which serves as the foundation for all\nsubsequent animations. To create a smooth motion sequence, these subsequent\nanimations involve drawing the character in different poses and actions, such\nas running, jumping, or attacking. This process requires significant manual\neffort from illustrators, as they must meticulously ensure consistency in\ndesign, proportions, and style across multiple motion frames. Each frame is\ndrawn individually, making this a time-consuming and labor-intensive task.\nGenerative models, such as diffusion models, have the potential to\nrevolutionize this process by automating the creation of sprite sheets.\nDiffusion models, known for their ability to generate diverse images, can be\nadapted to create character animations. By leveraging the capabilities of\ndiffusion models, we can significantly reduce the manual workload for\nillustrators, accelerate the animation creation process, and open up new\ncreative possibilities in game development.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.03685v1"
    },
    {
        "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
        "authors": [
            "Mikhail Dereviannykh",
            "Dmitrii Klepikov",
            "Johannes Hanika",
            "Carsten Dachsbacher"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.04634v1"
    },
    {
        "title": "Flexible Mesh Segmentation through Integration of Geometric\n  andTopological Features of Reeb Graphs",
        "authors": [
            "Beguet Florian",
            "Lanquetin Sandrine",
            "Raffin Romain"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Mesh segmentation represents a crucial task in computer graphics and\ngeometric analysis, with diverse applications spanning texture mapping,\nanimation, and beyond. This paper introduces an innovative Reeb graph-based\nmesh segmentation method that seamlessly integrates geometric and topological\nfeatures to achieve flexible and robust segmentation results. The proposed\napproach encompasses three primary phases. First, an enhanced topological\nskeleton construction efficiently captures the Reeb graph structure while\npreserving degenerate critical points. Second, a topological simplification\nprocess employing critical point cancellation reduces graph complexity while\nmaintaining essential shape features and correspondences. Finally, a region\ngrowing algorithm leverages both Reeb graph adjacency and mesh vertex\nconnectivity to generate contiguous, semantically meaningful segments.\n  The presented method exhibits computational efficiency, achieving a\ncomplexity of $O(n \\log n$) for a mesh containing n vertices. Its versatility\nand effectiveness are validated through application to both local\ngeometry-based segmentation using the Shape Index and part-based decomposition\nutilizing the Shape Diameter Function. This flexible framework establishes a\nsolid foundation for advanced analysis and applications across various domains,\noffering new possibilities for mesh processing and understanding.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.05335v1"
    },
    {
        "title": "CHOICE: Coordinated Human-Object Interaction in Cluttered Environments\n  for Pick-and-Place Actions",
        "authors": [
            "Jintao Lu",
            "He Zhang",
            "Yuting Ye",
            "Takaaki Shiratori",
            "Sebastian Starke",
            "Taku Komura"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Animating human-scene interactions such as pick-and-place tasks in cluttered,\ncomplex layouts is a challenging task, with objects of a wide variation of\ngeometries and articulation under scenarios with various obstacles. The main\ndifficulty lies in the sparsity of the motion data compared to the wide\nvariation of the objects and environments as well as the poor availability of\ntransition motions between different tasks, increasing the complexity of the\ngeneralization to arbitrary conditions. To cope with this issue, we develop a\nsystem that tackles the interaction synthesis problem as a hierarchical\ngoal-driven task. Firstly, we develop a bimanual scheduler that plans a set of\nkeyframes for simultaneously controlling the two hands to efficiently achieve\nthe pick-and-place task from an abstract goal signal such as the target object\nselected by the user. Next, we develop a neural implicit planner that generates\nguidance hand trajectories under diverse object shape/types and obstacle\nlayouts. Finally, we propose a linear dynamic model for our DeepPhase\ncontroller that incorporates a Kalman filter to enable smooth transitions in\nthe frequency domain, resulting in a more realistic and effective\nmulti-objective control of the character.Our system can produce a wide range of\nnatural pick-and-place movements with respect to the geometry of objects, the\narticulation of containers and the layout of the objects in the scene.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.06702v1"
    },
    {
        "title": "ProGDF: Progressive Gaussian Differential Field for Controllable and\n  Flexible 3D Editing",
        "authors": [
            "Yian Zhao",
            "Wanshi Xu",
            "Yang Wu",
            "Weiheng Huang",
            "Zhongqian Sun",
            "Wei Yang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  3D editing plays a crucial role in editing and reusing existing 3D assets,\nthereby enhancing productivity. Recently, 3DGS-based methods have gained\nincreasing attention due to their efficient rendering and flexibility. However,\nachieving desired 3D editing results often requires multiple adjustments in an\niterative loop, resulting in tens of minutes of training time cost for each\nattempt and a cumbersome trial-and-error cycle for users. This in-the-loop\ntraining paradigm results in a poor user experience. To address this issue, we\nintroduce the concept of process-oriented modelling for 3D editing and propose\nthe Progressive Gaussian Differential Field (ProGDF), an out-of-loop training\napproach that requires only a single training session to provide users with\ncontrollable editing capability and variable editing results through a\nuser-friendly interface in real-time. ProGDF consists of two key components:\nProgressive Gaussian Splatting (PGS) and Gaussian Differential Field (GDF). PGS\nintroduces the progressive constraint to extract the diverse intermediate\nresults of the editing process and employs rendering quality regularization to\nimprove the quality of these results. Based on these intermediate results, GDF\nleverages a lightweight neural network to model the editing process. Extensive\nresults on two novel applications, namely controllable 3D editing and flexible\nfine-grained 3D manipulation, demonstrate the effectiveness, practicality and\nflexibility of the proposed ProGDF.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.08152v1"
    },
    {
        "title": "Textured Mesh Saliency: Bridging Geometry and Texture for Human\n  Perception in 3D Graphics",
        "authors": [
            "Kaiwei Zhang",
            "Dandan Zhu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Textured meshes significantly enhance the realism and detail of objects by\nmapping intricate texture details onto the geometric structure of 3D models.\nThis advancement is valuable across various applications, including\nentertainment, education, and industry. While traditional mesh saliency studies\nfocus on non-textured meshes, our work explores the complexities introduced by\ndetailed texture patterns. We present a new dataset for textured mesh saliency,\ncreated through an innovative eye-tracking experiment in a six degrees of\nfreedom (6-DOF) VR environment. This dataset addresses the limitations of\nprevious studies by providing comprehensive eye-tracking data from multiple\nviewpoints, thereby advancing our understanding of human visual behavior and\nsupporting more accurate and effective 3D content creation. Our proposed model\npredicts saliency maps for textured mesh surfaces by treating each triangular\nface as an individual unit and assigning a saliency density value to reflect\nthe importance of each local surface region. The model incorporates a texture\nalignment module and a geometric extraction module, combined with an\naggregation module to integrate texture and geometry for precise saliency\nprediction. We believe this approach will enhance the visual fidelity of\ngeometric processing algorithms while ensuring efficient use of computational\nresources, which is crucial for real-time rendering and high-detail\napplications such as VR and gaming.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.08188v1"
    },
    {
        "title": "Design2GarmentCode: Turning Design Concepts to Tangible Garments Through\n  Program Synthesis",
        "authors": [
            "Feng Zhou",
            "Ruiyang Liu",
            "Chen Liu",
            "Gaofeng He",
            "Yong-Lu Li",
            "Xiaogang Jin",
            "Huamin Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Sewing patterns, the essential blueprints for fabric cutting and tailoring,\nact as a crucial bridge between design concepts and producible garments.\nHowever, existing uni-modal sewing pattern generation models struggle to\neffectively encode complex design concepts with a multi-modal nature and\ncorrelate them with vectorized sewing patterns that possess precise geometric\nstructures and intricate sewing relations. In this work, we propose a novel\nsewing pattern generation approach Design2GarmentCode based on Large Multimodal\nModels (LMMs), to generate parametric pattern-making programs from multi-modal\ndesign concepts. LMM offers an intuitive interface for interpreting diverse\ndesign inputs, while pattern-making programs could serve as well-structured and\nsemantically meaningful representations of sewing patterns, and act as a robust\nbridge connecting the cross-domain pattern-making knowledge embedded in LMMs\nwith vectorized sewing patterns. Experimental results demonstrate that our\nmethod can flexibly handle various complex design expressions such as images,\ntextual descriptions, designer sketches, or their combinations, and convert\nthem into size-precise sewing patterns with correct stitches. Compared to\nprevious methods, our approach significantly enhances training efficiency,\ngeneration quality, and authoring flexibility. Our code and data will be\npublicly available.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.08603v2"
    },
    {
        "title": "Meshtron: High-Fidelity, Artist-Like 3D Mesh Generation at Scale",
        "authors": [
            "Zekun Hao",
            "David W. Romero",
            "Tsung-Yi Lin",
            "Ming-Yu Liu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Meshes are fundamental representations of 3D surfaces. However, creating\nhigh-quality meshes is a labor-intensive task that requires significant time\nand expertise in 3D modeling. While a delicate object often requires over\n$10^4$ faces to be accurately modeled, recent attempts at generating\nartist-like meshes are limited to $1.6$K faces and heavy discretization of\nvertex coordinates. Hence, scaling both the maximum face count and vertex\ncoordinate resolution is crucial to producing high-quality meshes of realistic,\ncomplex 3D objects. We present Meshtron, a novel autoregressive mesh generation\nmodel able to generate meshes with up to 64K faces at 1024-level coordinate\nresolution --over an order of magnitude higher face count and $8{\\times}$\nhigher coordinate resolution than current state-of-the-art methods. Meshtron's\nscalability is driven by four key components: (1) an hourglass neural\narchitecture, (2) truncated sequence training, (3) sliding window inference,\n(4) a robust sampling strategy that enforces the order of mesh sequences. This\nresults in over $50{\\%}$ less training memory, $2.5{\\times}$ faster throughput,\nand better consistency than existing works. Meshtron generates meshes of\ndetailed, complex 3D objects at unprecedented levels of resolution and\nfidelity, closely resembling those created by professional artists, and opening\nthe door to more realistic generation of detailed 3D assets for animation,\ngaming, and virtual environments.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.09548v1"
    },
    {
        "title": "AniSora: Exploring the Frontiers of Animation Video Generation in the\n  Sora Era",
        "authors": [
            "Yudong Jiang",
            "Baohan Xu",
            "Siqian Yang",
            "Mingyu Yin",
            "Jing Liu",
            "Chao Xu",
            "Siqi Wang",
            "Yidi Wu",
            "Bingwen Zhu",
            "Xinwen Zhang",
            "Xingyu Zheng",
            "Jixuan Xu",
            "Yue Zhang",
            "Jinlong Hou",
            "Huyang Sun"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Animation has gained significant interest in the recent film and TV industry.\nDespite the success of advanced video generation models like Sora, Kling, and\nCogVideoX in generating natural videos, they lack the same effectiveness in\nhandling animation videos. Evaluating animation video generation is also a\ngreat challenge due to its unique artist styles, violating the laws of physics\nand exaggerated motions. In this paper, we present a comprehensive system,\nAniSora, designed for animation video generation, which includes a data\nprocessing pipeline, a controllable generation model, and an evaluation\ndataset. Supported by the data processing pipeline with over 10M high-quality\ndata, the generation model incorporates a spatiotemporal mask module to\nfacilitate key animation production functions such as image-to-video\ngeneration, frame interpolation, and localized image-guided animation. We also\ncollect an evaluation benchmark of 948 various animation videos, the evaluation\non VBench and human double-blind test demonstrates consistency in character and\nmotion, achieving state-of-the-art results in animation video generation. Our\nevaluation benchmark will be publicly available at\nhttps://github.com/bilibili/Index-anisora.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.10255v3"
    },
    {
        "title": "CK-MPM: A Compact-Kernel Material Point Method",
        "authors": [
            "Michael Liu",
            "Xinlei Wang",
            "Minchen Li"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The Material Point Method (MPM) has become a cornerstone of physics-based\nsimulation, widely used in geomechanics and computer graphics for modeling\nphenomena such as granular flows, viscoelasticity, fracture mechanics, etc.\nDespite its versatility, the original MPM suffers from cell-crossing\ninstabilities caused by discontinuities in particle-grid transfer kernels.\nExisting solutions mitigate these issues by adopting smoother shape functions,\nbut at the cost of increased computational overhead due to larger kernel\nsupport. In this paper, we propose a novel $C^2$-continuous compact kernel for\nMPM that achieves a unique balance between stability and computational\nefficiency. Our method integrates seamlessly with Affine Particle-In-Cell\n(APIC) and Moving Least Squares (MLS) MPM, while only doubling the number of\ngrid nodes associated with each particle compared to linear kernels. At its\ncore is an innovative dual-grid framework, which associates particles with grid\nnodes exclusively within the cells they occupy on two staggered grids, ensuring\nconsistent and stable force computations. To further accelerate performance, we\npresent a GPU-optimized implementation inspired by state-of-the-art massively\nparallel MPM techniques, achieving an additional $2\\times$ speedup in G2P2G\ntransfers over quadratic B-spline MPM. Comprehensive validation through unit\ntests, comparative studies, and stress tests demonstrates the efficacy of our\napproach in conserving both linear and angular momentum, handling stiff\nmaterials, and scaling efficiently for large-scale simulations. Our results\nhighlight the transformative potential of compact, high-order kernels in\nadvancing MPM's capabilities for stable, high-performance simulations, paving\nthe way for more computationally efficient applications in computer graphics\nand beyond.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.10399v1"
    },
    {
        "title": "3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian\n  Splatting",
        "authors": [
            "Qi Wu",
            "Janick Martinez Esturo",
            "Ashkan Mirzaei",
            "Nicolas Moenne-Loccoz",
            "Zan Gojcic"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  3D Gaussian Splatting (3DGS) has shown great potential for efficient\nreconstruction and high-fidelity real-time rendering of complex scenes on\nconsumer hardware. However, due to its rasterization-based formulation, 3DGS is\nconstrained to ideal pinhole cameras and lacks support for secondary lighting\neffects. Recent methods address these limitations by tracing volumetric\nparticles instead, however, this comes at the cost of significantly slower\nrendering speeds. In this work, we propose 3D Gaussian Unscented Transform\n(3DGUT), replacing the EWA splatting formulation in 3DGS with the Unscented\nTransform that approximates the particles through sigma points, which can be\nprojected exactly under any nonlinear projection function. This modification\nenables trivial support of distorted cameras with time dependent effects such\nas rolling shutter, while retaining the efficiency of rasterization.\nAdditionally, we align our rendering formulation with that of tracing-based\nmethods, enabling secondary ray tracing required to represent phenomena such as\nreflections and refraction within the same 3D representation.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.12507v1"
    },
    {
        "title": "DreaMark: Rooting Watermark in Score Distillation Sampling Generated\n  Neural Radiance Fields",
        "authors": [
            "Xingyu Zhu",
            "Xiapu Luo",
            "Xuetao Wei"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Recent advancements in text-to-3D generation can generate neural radiance\nfields (NeRFs) with score distillation sampling, enabling 3D asset creation\nwithout real-world data capture. With the rapid advancement in NeRF generation\nquality, protecting the copyright of the generated NeRF has become increasingly\nimportant. While prior works can watermark NeRFs in a post-generation way, they\nsuffer from two vulnerabilities. First, a delay lies between NeRF generation\nand watermarking because the secret message is embedded into the NeRF model\npost-generation through fine-tuning. Second, generating a non-watermarked NeRF\nas an intermediate creates a potential vulnerability for theft. To address both\nissues, we propose Dreamark to embed a secret message by backdooring the NeRF\nduring NeRF generation. In detail, we first pre-train a watermark decoder.\nThen, the Dreamark generates backdoored NeRFs in a way that the target secret\nmessage can be verified by the pre-trained watermark decoder on an arbitrary\ntrigger viewport. We evaluate the generation quality and watermark robustness\nagainst image- and model-level attacks. Extensive experiments show that the\nwatermarking process will not degrade the generation quality, and the watermark\nachieves 90+% accuracy among both image-level attacks (e.g., Gaussian noise)\nand model-level attacks (e.g., pruning attack).\n",
        "pdf_link": "http://arxiv.org/pdf/2412.15278v1"
    },
    {
        "title": "Robust and Feature-Preserving Offset Meshing",
        "authors": [
            "Hongyi Cao",
            "Gang Xu",
            "Renshu Gu",
            "Jinlan Xu",
            "Xiaoyu Zhang",
            "Timon Rabczuk",
            "Yuzhe Luo",
            "Xifeng Gao"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce a novel offset meshing approach that can robustly handle a 3D\nsurface mesh with an arbitrary geometry and topology configurations, while\nnicely capturing the sharp features on the original input for both inward and\noutward offsets. Compared to the existing approaches focusing on\nconstant-radius offset, to the best of our knowledge, we propose the first-ever\nsolution for mitered offset that can well preserve sharp features. Our method\nis designed based on several core principals: 1) explicitly generating the\noffset vertices and triangles with feature-capturing energy and constraints; 2)\nprioritizing the generation of the offset geometry before establishing its\nconnectivity, 3) employing exact algorithms in critical pipeline steps for\nrobustness, balancing the use of floating-point computations for efficiency, 4)\napplying various conservative speed up strategies including early reject\nnon-contributing computations to the final output. Our approach further\nuniquely supports variable offset distances on input surface elements, offering\na wider range practical applications compared to conventional methods.\n  We have evaluated our method on a subset of Thinkgi10K, containing models\nwith diverse topological and geometric complexities created by practitioners in\nvarious fields. Our results demonstrate the superiority of our approach over\ncurrent state-of-the-art methods in terms of element count, feature\npreservation, and non-uniform offset distances of the resulting offset mesh\nsurfaces, marking a significant advancement in the field.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.15564v1"
    },
    {
        "title": "Color-Name Aware Optimization to Enhance the Perception of Transparent\n  Overlapped Charts",
        "authors": [
            "Kecheng Lu",
            "Lihang Zhu",
            "Yunhai Wang",
            "Qiong Zeng",
            "Weitao Song",
            "Khairi Reda"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Transparency is commonly utilized in visualizations to overlay color-coded\nhistograms or sets, thereby facilitating the visual comparison of categorical\ndata. However, these charts often suffer from significant overlap between\nobjects, resulting in substantial color interactions. Existing color blending\nmodels struggle in these scenarios, frequently leading to ambiguous color\nmappings and the introduction of false colors. To address these challenges, we\npropose an automated approach for generating optimal color encodings to enhance\nthe perception of translucent charts. Our method harnesses color nameability to\nmaximize the association between composite colors and their respective class\nlabels. We introduce a color-name aware (CNA) optimization framework that\ngenerates maximally coherent color assignments and transparency settings while\nensuring perceptual discriminability for all segments in the visualization. We\ndemonstrate the effectiveness of our technique through crowdsourced experiments\nwith composite histograms, showing how our technique can significantly\noutperform both standard and visualization-specific color blending models.\nFurthermore, we illustrate how our approach can be generalized to other\nvisualizations, including parallel coordinates and Venn diagrams. We provide an\nopen-source implementation of our technique as a web-based tool.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.16242v1"
    },
    {
        "title": "FlameForge: Combustion of Generalized Wooden Structures",
        "authors": [
            "Daoming Liu",
            "Jonathan Klein",
            "Florian Rist",
            "Wojciech Pałubicki",
            "Sören Pirk",
            "Dominik L. Michels"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We propose a unified volumetric combustion simulator that supports general\nwooden structures capturing the multi-phase combustion of charring materials.\nComplex geometric structures can conveniently be represented in a voxel grid\nfor the effective evaluation of volumetric effects. In addition, a signed\ndistance field is introduced to efficiently query the surface information\nrequired to compute the insulating effect caused by the char layer.\nNon-charring materials such as acrylic glass or non-combustible materials such\nas stone can also be modeled in the simulator. Adaptive data structures are\nutilized to enable memory-efficient computations within our multiresolution\napproach. The simulator is qualitatively validated by showcasing the numerical\nsimulation of a variety of scenes covering different kinds of structural\nconfigurations and materials. Two-way coupling of our combustion simulator and\nposition-based dynamics is demonstrated capturing characteristic mechanical\ndeformations caused by the combustion process. The volumetric combustion\nprocess of wooden structures is further quantitatively assessed by comparing\nour simulated results to sub-surface measurements of a real-world combustion\nexperiment.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.16735v1"
    },
    {
        "title": "Tuning Nonlinear Elastic Materials under Small and Large Deformations",
        "authors": [
            "Huanyu Chen",
            "Jernej Barbic"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In computer graphics and engineering, nonlinear elastic material properties\nof 3D volumetric solids are typically adjusted by selecting a material family,\nsuch as St. Venant Kirchhoff, Linear Corotational, (Stable) Neo-Hookean, Ogden,\netc., and then selecting the values of the specific parameters for that family,\nsuch as the Lame parameters, Ogden exponents, or whatever the parameterization\nof a particular family may be. However, the relationships between those\nparameter values, and visually intuitive material properties such as object's\n\"stiffness\", volume preservation, or the \"amount of nonlinearity\", are less\nclear and can be tedious to tune. For an arbitrary isotropic hyperelastic\nenergy density function psi that is not parameterized in terms of the Lame\nparameters, it is not even clear what the Lame parameters and Young's modulus\nand Poisson's ratio are. Starting from psi, we first give a concise definition\nof Lame parameters, and therefore Young's modulus and Poisson's ratio. Second,\nwe give a method to adjust the object's three salient properties, namely two\nsmall-deformation properties (overall \"stiffness\", and amount of volume\npreservation, prescribed by object's Young's modulus and Poisson's ratio), and\none large-deformation property (material nonlinearity). We do this in a manner\nwhereby each of these three properties is decoupled from the other two\nproperties, and can therefore be set independently. This permits a new ability,\nnamely \"normalization\" of materials: starting from two distinct materials, we\ncan \"normalize\" them so that they have the same small deformation properties,\nor the same large-deformation nonlinearity behavior, or both. Furthermore, our\nanalysis produced a useful theoretical result, namely it establishes that\nLinear Corotational materials (arguably the most widely used materials in\ncomputer graphics) are the simplest possible nonlinear materials.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.18631v1"
    },
    {
        "title": "SurfPatch: Enabling Patch Matching for Exploratory Stream Surface\n  Visualization",
        "authors": [
            "Delin An",
            "Chaoli Wang"
        ],
        "category": "cs.GR",
        "published_year": "2025",
        "summary": "  Unlike their line-based counterparts, surface-based techniques have yet to be\nthoroughly investigated in flow visualization due to their significant\nplacement, speed, perception, and evaluation challenges. This paper presents\nSurfPatch, a novel framework supporting exploratory stream surface\nvisualization. To begin with, we translate the issue of surface placement to\nsurface selection and trace a large number of stream surfaces from a given flow\nfield dataset. Then, we introduce a three-stage process: vertex-level\nclassification, patch-level matching, and surface-level clustering that\nhierarchically builds the connection between vertices and patches and between\npatches and surfaces. This bottom-up approach enables fine-grained, multiscale\npatch-level matching, sharply contrasts surface-level matching offered by\nexisting works, and provides previously unavailable flexibility during\nquerying. We design an intuitive visual interface for users to conveniently\nvisualize and analyze the underlying collection of stream surfaces in an\nexploratory manner. SurfPatch is not limited to stream surfaces traced from\nsteady flow datasets. We demonstrate its effectiveness through experiments on\nstream surfaces produced from steady and unsteady flows as well as isosurfaces\nextracted from scalar fields. The code is available at\nhttps://github.com/adlsn/SurfPatch.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.02003v1"
    },
    {
        "title": "3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point\n  Cloud or Mesh",
        "authors": [
            "Lewis A G Stuart",
            "Michael P Pound"
        ],
        "category": "cs.GR",
        "published_year": "2025",
        "summary": "  3D Gaussian Splatting (3DGS) excels at producing highly detailed 3D\nreconstructions, but these scenes often require specialised renderers for\neffective visualisation. In contrast, point clouds are a widely used 3D\nrepresentation and are compatible with most popular 3D processing software, yet\nconverting 3DGS scenes into point clouds is a complex challenge. In this work\nwe introduce 3DGS-to-PC, a flexible and highly customisable framework that is\ncapable of transforming 3DGS scenes into dense, high-accuracy point clouds. We\nsample points probabilistically from each Gaussian as a 3D density function. We\nadditionally threshold new points using the Mahalanobis distance to the\nGaussian centre, preventing extreme outliers. The result is a point cloud that\nclosely represents the shape encoded into the 3D Gaussian scene. Individual\nGaussians use spherical harmonics to adapt colours depending on view, and each\npoint may contribute only subtle colour hints to the resulting rendered scene.\nTo avoid spurious or incorrect colours that do not fit with the final point\ncloud, we recalculate Gaussian colours via a customised image rendering\napproach, assigning each Gaussian the colour of the pixel to which it\ncontributes most across all views. 3DGS-to-PC also supports mesh generation\nthrough Poisson Surface Reconstruction, applied to points sampled from\npredicted surface Gaussians. This allows coloured meshes to be generated from\n3DGS scenes without the need for re-training. This package is highly\ncustomisable and capability of simple integration into existing 3DGS pipelines.\n3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloud\nand surface-based formats.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.07478v1"
    },
    {
        "title": "Spin-Weighted Spherical Harmonics for Polarized Light Transport",
        "authors": [
            "Shinyoung Yi",
            "Donggun Kim",
            "Jiwoong Na",
            "Xin Tong",
            "Min H. Kim"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The objective of polarization rendering is to simulate the interaction of\nlight with materials exhibiting polarization-dependent behavior. However,\nintegrating polarization into rendering is challenging and increases\ncomputational costs significantly. The primary difficulty lies in efficiently\nmodeling and computing the complex reflection phenomena associated with\npolarized light. Specifically, frequency-domain analysis, essential for\nefficient environment lighting and storage of complex light interactions, is\nlacking. To efficiently simulate and reproduce polarized light interactions\nusing frequency-domain techniques, we address the challenge of maintaining\ncontinuity in polarized light transport represented by Stokes vectors within\nangular domains. The conventional spherical harmonics method cannot effectively\nhandle continuity and rotation invariance for Stokes vectors. To overcome this,\nwe develop a new method called polarized spherical harmonics (PSH) based on the\nspin-weighted spherical harmonics theory. Our method provides a\nrotation-invariant representation of Stokes vector fields. Furthermore, we\nintroduce frequency domain formulations of polarized rendering equations and\nspherical convolution based on PSH. We first define spherical convolution on\nStokes vector fields in the angular domain, and it also provides efficient\ncomputation of polarized light transport, nearly on an entry-wise product in\nthe frequency domain. Our frequency domain formulation, including spherical\nconvolution, led to the development of the first real-time polarization\nrendering technique under polarized environmental illumination, named\nprecomputed polarized radiance transfer, using our polarized spherical\nharmonics. Results demonstrate that our method can effectively and accurately\nsimulate and reproduce polarized light interactions in complex reflection\nphenomena.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.07582v1"
    },
    {
        "title": "3D Gaussian Splatting with Normal Information for Mesh Extraction and\n  Improved Rendering",
        "authors": [
            "Meenakshi Krishnan",
            "Liam Fowl",
            "Ramani Duraiswami"
        ],
        "category": "cs.GR",
        "published_year": "2025",
        "summary": "  Differentiable 3D Gaussian splatting has emerged as an efficient and flexible\nrendering technique for representing complex scenes from a collection of 2D\nviews and enabling high-quality real-time novel-view synthesis. However, its\nreliance on photometric losses can lead to imprecisely reconstructed geometry\nand extracted meshes, especially in regions with high curvature or fine detail.\nWe propose a novel regularization method using the gradients of a signed\ndistance function estimated from the Gaussians, to improve the quality of\nrendering while also extracting a surface mesh. The regularizing normal\nsupervision facilitates better rendering and mesh reconstruction, which is\ncrucial for downstream applications in video generation, animation, AR-VR and\ngaming. We demonstrate the effectiveness of our approach on datasets such as\nMip-NeRF360, Tanks and Temples, and Deep-Blending. Our method scores higher on\nphotorealism metrics compared to other mesh extracting rendering methods\nwithout compromising mesh quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.08370v1"
    },
    {
        "title": "Holoview: Interactive 3D visualization of medical data in AR",
        "authors": [
            "Pankaj Kaushik",
            "Anshul Goswami",
            "Ojaswa Sharma"
        ],
        "category": "cs.GR",
        "published_year": "2025",
        "summary": "  We introduce HoloView, an innovative augmented reality (AR) system that\nenhances interactive learning of human anatomical structures through immersive\nvisualization. Combining advanced rendering techniques with intuitive\ngesture-based interactions, HoloView provides a comprehensive technical\nsolution for medical education. The system architecture features a distributed\nrendering pipeline that offloads stereoscopic computations to a remote server,\noptimizing performance and enabling high-quality visualization on less powerful\ndevices. To prioritize visual quality in the user's direct line of sight while\nreducing computational load, we implement foveated rendering optimization,\nenhancing the immersive experience. Additionally, a hybrid surface-volume\nrendering technique is used to achieve faster rendering speeds without\nsacrificing visual fidelity. Complemented by a carefully designed user\ninterface and gesture-based interaction system, HoloView allows users to\nnaturally manipulate holographic content and seamlessly navigate the learning\nenvironment. HoloView significantly facilitates anatomical structure\nvisualization and promotes an engaging, user-centric learning experience.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.08736v2"
    },
    {
        "title": "Surface Triangulation -- The Metric Approach",
        "authors": [
            "Emil Saucan"
        ],
        "category": "cs.GR",
        "published_year": "2004",
        "summary": "  We embark in a program of studying the problem of better approximating\nsurfaces by triangulations(triangular meshes) by considering the approximating\ntriangulations as finite metric spaces and the target smooth surface as their\nHaussdorff-Gromov limit. This allows us to define in a more natural way the\nrelevant elements, constants and invariants s.a. principal directions and\nprincipal values, Gaussian and Mean curvature, etc. By a \"natural way\" we mean\nan intrinsic, discrete, metric definitions as opposed to approximating or\nparaphrasing the differentiable notions. In this way we hope to circumvent\ncomputational errors and, indeed, conceptual ones, that are often inherent to\nthe classical, \"numerical\" approach. In this first study we consider the\nproblem of determining the Gaussian curvature of a polyhedral surface, by using\nthe {\\em embedding curvature} in the sense of Wald (and Menger). We present two\nmodalities of employing these definitions for the computation of Gaussian\ncurvature.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0401023v1"
    },
    {
        "title": "On a solution to display non-filled-in quaternionic Julia sets",
        "authors": [
            "Alessandro Rosa"
        ],
        "category": "cs.GR",
        "published_year": "2006",
        "summary": "  During early 1980s, the so-called `escape time' method, developed to display\nthe Julia sets for complex dynamical systems, was exported to quaternions in\norder to draw analogous pictures in this wider numerical field. Despite of the\nfine results in the complex plane, where all topological configurations of\nJulia sets have been successfully displayed, the `escape time' method fails to\nrender properly the non-filled-in variety of quaternionic Julia sets. So their\ndigital visualisation remained an open problem for several years. Both the\nsolution for extending this old method to non-filled-in quaternionic Julia sets\nand its implementation into a program are explained here.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0608003v2"
    },
    {
        "title": "Local Space-Time Smoothing for Version Controlled Documents",
        "authors": [
            "Seungyeon Kim",
            "Guy Lebanon"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  Unlike static documents, version controlled documents are continuously edited\nby one or more authors. Such collaborative revision process makes traditional\nmodeling and visualization techniques inappropriate. In this paper we propose a\nnew representation based on local space-time smoothing that captures important\nrevision patterns. We demonstrate the applicability of our framework using\nexperiments on synthetic and real-world data.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.1410v2"
    },
    {
        "title": "Autoplot: A browser for scientific data on the web",
        "authors": [
            "J. Faden",
            "R. S. Weigel",
            "J. Merka",
            "R. H. W. Friedel"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  Autoplot is software developed for the Virtual Observatories in Heliophysics\nto provide intelligent and automated plotting capabilities for many typical\ndata products that are stored in a variety of file formats or databases.\nAutoplot has proven to be a flexible tool for exploring, accessing, and viewing\ndata resources as typically found on the web, usually in the form of a\ndirectory containing data files with multiple parameters contained in each\nfile. Data from a data source is abstracted into a common internal data model\ncalled QDataSet. Autoplot is built from individually useful components, and can\nbe extended and reused to create specialized data handling and analysis\napplications and is being used in a variety of science visualization and\nanalysis applications. Although originally developed for viewing\nheliophysics-related time series and spectrograms, its flexible and generic\ndata representation model makes it potentially useful for the Earth sciences.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.2447v1"
    },
    {
        "title": "Multi-sensorial interaction with a nano-scale phenomenon : the force\n  curve",
        "authors": [
            "Sylvain Marliere",
            "Daniela Urma",
            "Jean-Loup Florens",
            "Florence Marchi"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  Using Atomic Force Microscopes (AFM) to manipulate nano-objects is an actual\nchallenge for surface scientists. Basic haptic interfacesbetween the AFM and\nexperimentalists have already been implemented. Themulti-sensory renderings\n(seeing, hearing and feeling) studied from acognitive point of view increase\nthe efficiency of the actual interfaces. Toallow the experimentalist to feel\nand touch the nano-world, we add mixedrealities between an AFM and a force\nfeedback device, enriching thus thedirect connection by a modeling engine. We\npresent in this paper the firstresults from a real-time remote-control handling\nof an AFM by our ForceFeedback Gestural Device through the example of the\napproach-retract curve.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.3181v1"
    },
    {
        "title": "Dynamical issues in interactive representation of physical objects",
        "authors": [
            "Jean-Loup Florens",
            "Alina Voda",
            "Daniela Urma"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  The quality of a simulator equipped with a haptic interface is given by the\ndynamical properties of its components: haptic interface, simulator and control\nsystem. Some application areas of such kind of simulator like musical\nsynthesis, animation or more general, instrumental art have specific\nrequirements as for the \"haptic rendering\" of small movements that go beyond\nthe usual haptic interfaces allow. Object properties variability and different\nsituations of object combination represent important aspects of such type of\napplication which makes that the user can be interested as much in the\nrestitution of certain global properties of an entire object domain as in the\nrestitution of properties that are specific to an isolate object. In the\ntraditional approaches, the usual criteria are founded on the paradigm of\ntransparency and are related to the impedance error introduced by the technical\naspects of the system. As a general aim, rather than to minimize these effects,\nwe look to characterize them by physical metaphors conferring to haptic medium\nthe role of a tool. This positioning leads to firstly analyze the natural human\nobject interaction as a simplified evolutive system and then considers its\nsynthesis in the case of the interactive physical simulation. By means of a\nfrequential method, this approach is presented for some elementary\nconfigurations of the simulator\n",
        "pdf_link": "http://arxiv.org/pdf/1005.3185v1"
    },
    {
        "title": "Data visualization in political and social sciences",
        "authors": [
            "Andrei Zinovyev"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  The basic objective of data visualization is to provide an efficient\ngraphical display for summarizing and reasoning about quantitative information.\nDuring the last decades, political science has accumulated a large corpus of\nvarious kinds of data such as comprehensive factbooks and atlases,\ncharacterizing all or most of existing states by multiple and objectively\nassessed numerical indicators within certain time lapse. As a consequence,\nthere exists a continuous trend for political science to gradually become a\nmore quantitative scientific field and to use quantitative information in the\nanalysis and reasoning. It is believed that any objective analysis in political\nscience must be multidimensional and combine various sources of quantitative\ninformation; however, human capabilities for perception of large massifs of\nnumerical information are limited. Hence, methods and approaches for\nvisualization of quantitative and qualitative data (and, especially\nmultivariate data) is an extremely important topic. Data visualization\napproaches can be classified into several groups, starting from creating\ninformative charts and diagrams (statistical graphics and infographics) and\nending with advanced statistical methods for visualizing multidimensional\ntables containing both quantitative and qualitative information. In this\narticle we provide a short review of existing methods of data visualization\nmethods with applications in political and social science.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1188v1"
    },
    {
        "title": "Specular holography",
        "authors": [
            "Matthew Brand"
        ],
        "category": "cs.GR",
        "published_year": "2010",
        "summary": "  By tooling an spot-illuminated surface to control the flow of specular glints\nunder motion, one can produce holographic view-dependent imagery. This paper\npresents the differential equation that governs the shape of the specular\nsurfaces, and illustrates how solutions can be constructed for different kinds\nof motion, lighting, host surface geometries, and fabrication constraints,\nleading to some novel forms of holography.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.0301v1"
    },
    {
        "title": "Visualization of Collaborative Data",
        "authors": [
            "Guobiao Mei",
            "Christian R. Shelton"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  Collaborative data consist of ratings relating two distinct sets of objects:\nusers and items. Much of the work with such data focuses on filtering:\npredicting unknown ratings for pairs of users and items. In this paper we focus\non the problem of visualizing the information. Given all of the ratings, our\ntask is to embed all of the users and items as points in the same Euclidean\nspace. We would like to place users near items that they have rated (or would\nrate) high, and far away from those they would give a low rating. We pose this\nproblem as a real-valued non-linear Bayesian network and employ Markov chain\nMonte Carlo and expectation maximization to find an embedding. We present a\nmetric by which to judge the quality of a visualization and compare our results\nto local linear embedding and Eigentaste on three real-world datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.6850v1"
    },
    {
        "title": "Dynamic Simulation of Soft Heterogeneous Objects",
        "authors": [
            "Jonathan Hiller",
            "Hod Lipson"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  This paper describes a 2D and 3D simulation engine that quantitatively models\nthe statics, dynamics, and non-linear deformation of heterogeneous soft bodies\nin a computationally efficient manner. There is a large body of work simulating\ncompliant mechanisms. These normally assume small deformations with homogeneous\nmaterial properties actuated with external forces. There is also a large body\nof research on physically-based deformable objects for applications in computer\ngraphics with the purpose of generating realistic appearances at the expense of\naccuracy. Here we present a simulation framework in which an object may be\ncomposed of any number of interspersed materials with varying properties\n(stiffness, density, etc.) to enable true heterogeneous multi-material\nsimulation. Collisions are handled to prevent self-penetration due to large\ndeformation, which also allows multiple bodies to interact. A volumetric\nactuation method is implemented to impart motion to the structures which opens\nthe door to the design of novel structures and mechanisms. The simulator was\nimplemented efficiently such that objects with thousands of degrees of freedom\ncan be simulated at suitable framerates for user interaction using a single\nthread of a typical desktop computer. The code is written in platform agnostic\nC++ and is fully open source. This research opens the door to the dynamic\nsimulation of freeform 3D multi-material mechanisms and objects in a manner\nsuitable for design automation.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.2845v1"
    },
    {
        "title": "3D model retrieval using global and local radial distances",
        "authors": [
            "Bo Li",
            "Henry Johan"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  3D model retrieval techniques can be classified as histogram-based,\nview-based and graph-based approaches. We propose a hybrid shape descriptor\nwhich combines the global and local radial distance features by utilizing the\nhistogram-based and view-based approaches respectively. We define an\narea-weighted global radial distance with respect to the center of the bounding\nsphere of the model and encode its distribution into a 2D histogram as the\nglobal radial distance shape descriptor. We then uniformly divide the bounding\ncube of a 3D model into a set of small cubes and define their centers as local\ncenters. Then, we compute the local radial distance of a point based on the\nnearest local center. By sparsely sampling a set of views and encoding the\nlocal radial distance feature on the rendered views by color coding, we extract\nthe local radial distance shape descriptor. Based on these two shape\ndescriptors, we develop a hybrid radial distance shape descriptor for 3D model\nretrieval. Experiment results show that our hybrid shape descriptor outperforms\nseveral typical histogram-based and view-based approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.2081v1"
    },
    {
        "title": "Progressive Compression of 3D Objects with an Adaptive Quantization",
        "authors": [
            "Zeineb Abderrahim",
            "Elhem Techini",
            "Mohamed Salim Bouhlel"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  This paper presents a new progressive compression method for triangular\nmeshes. This method, in fact, is based on a schema of irregular\nmulti-resolution analysis and is centered on the optimization of the\nrate-distortion trade-off. The quantization precision is adapted to each vertex\nduring the encoding / decoding process to optimize the rate-distortion\ncompromise. The Optimization of the treated mesh geometry improves the\napproximation quality and the compression ratio at each level of resolution.\nThe experimental results show that the proposed algorithm gives competitive\nresults compared to the previous works dealing with the rate-distortion\ncompromise.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.3314v1"
    },
    {
        "title": "Composing DTI Visualizations with End-user Programming",
        "authors": [
            "Haipeng Cai",
            "Jian Chen",
            "Alexander P. Auchus",
            "David H. Laidlaw"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We present the design and prototype implementation of a scientific\nvisualization language called Zifazah for composing 3D visualizations of\ndiffusion tensor magnetic resonance imaging (DT-MRI or DTI) data. Unlike\nexisting tools allowing flexible customization of data visualizations that are\nprogrammer-oriented, we focus on domain scientists as end users in order to\nenable them to freely compose visualizations of their scientific data set. We\nanalyzed end-user descriptions extracted from interviews with neurologists and\nphysicians conducting clinical practices using DTI about how they would build\nand use DTI visualizations to collect syntax and semantics for the language\ndesign, and have discovered the elements and structure of the proposed\nlanguage. Zifazah makes use of the initial set of lexical terms and semantics\nto provide a declarative language in the spirit of intuitive syntax and usage.\nThis work contributes three, among others, main design principles for\nscientific visualization language design as well as a practice of such language\nfor DTI visualization with Zifazah. First, Zifazah incorporated visual symbolic\nmapping based on color, size and shape, which is a sub-set of Bertin's taxonomy\nmigrated to scientific visualizations. Second, Zifazah is defined as a spatial\nlanguage whereby lexical representation of spatial relationship for 3D object\nvisualization and manipulations, which is characteristic of scientific data,\ncan be programmed. Third, built on top of Bertin's semiology, flexible data\nencoding specifically for scientific visualizations is integrated in our\nlanguage in order to allow end users to achieve optimal visual composition at\ntheir best. Along with sample scripts representative of our language design\nfeatures, some new DTI visualizations as the running results created by end\nusers using the novel visualization language have also been presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.2923v1"
    },
    {
        "title": "Rigorous asymptotic and moment-preserving diffusion approximations for\n  generalized linear Boltzmann transport in arbitrary dimension",
        "authors": [
            "Eugene d'Eon"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  We derive new diffusion solutions to the monoenergetic generalized linear\nBoltzmann transport equation (GLBE) for the stationary collision density and\nscalar flux about an isotropic point source in an infinite $d$-dimensional\nabsorbing medium with isotropic scattering. We consider both classical\ntransport theory with exponentially-distributed free paths in arbitrary\ndimensions as well as a number of non-classical transport theories\n(non-exponential random flights) that describe a broader class of transport\nprocesses within partially-correlated random media. New rigorous asymptotic\ndiffusion approximations are derived where possible. We also generalize\nGrosjean's moment-preserving approach of separating the first (or uncollided)\ndistribution from the collided portion and approximating only the latter using\ndiffusion. We find that for any spatial dimension and for many free-path\ndistributions Grosjean's approach produces compact, analytic approximations\nthat are, overall, more accurate for high absorption and for small\nsource-detector separations than either $P_1$ diffusion or rigorous asymptotic\ndiffusion. These diffusion-based approximations are exact in the first two even\nspatial moments, which we derive explicitly for various non-classical transport\ntypes. We also discuss connections between the random-flight-theory derivation\nof the Green's function and the discrete spectrum of the transport operator and\nreport some new observations regarding the discrete eigenvalues of the\ntransport operator for general dimensions and free-path distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.1412v5"
    },
    {
        "title": "GPU-based visualization of domain-coloured algebraic Riemann surfaces",
        "authors": [
            "Stefan Kranich"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We examine an algorithm for the visualization of domain-coloured Riemann\nsurfaces of plane algebraic curves. The approach faithfully reproduces the\ntopology and the holomorphic structure of the Riemann surface. We discuss how\nthe algorithm can be implemented efficiently in OpenGL with geometry shaders,\nand (less efficiently) even in WebGL with multiple render targets and floating\npoint textures. While the generation of the surface takes noticeable time in\nboth implementations, the visualization of a cached Riemann surface mesh is\npossible with interactive performance. This allows us to visually explore\notherwise almost unimaginable mathematical objects. As examples, we look at the\ncomplex square root and the folium of Descartes. For the folium of Descartes,\nthe visualization reveals features of the algebraic curve which are not obvious\nfrom its equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04571v3"
    },
    {
        "title": "PolyDepth: Real-time Penetration Depth Computation using Iterative\n  Contact-Space Projection",
        "authors": [
            "Changsoo Je",
            "Min Tang",
            "Youngeun Lee",
            "Minkyoung Lee",
            "Young J. Kim"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We present a real-time algorithm that finds the Penetration Depth (PD)\nbetween general polygonal models based on iterative and local optimization\ntechniques. Given an in-collision configuration of an object in configuration\nspace, we find an initial collision-free configuration using several methods\nsuch as centroid difference, maximally clear configuration, motion coherence,\nrandom configuration, and sampling-based search. We project this configuration\non to a local contact space using a variant of continuous collision detection\nalgorithm and construct a linear convex cone around the projected\nconfiguration. We then formulate a new projection of the in-collision\nconfiguration onto the convex cone as a Linear Complementarity Problem (LCP),\nwhich we solve using a type of Gauss-Seidel iterative algorithm. We repeat this\nprocedure until a locally optimal PD is obtained. Our algorithm can process\ncomplicated models consisting of tens of thousands triangles at interactive\nrates.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.06181v1"
    },
    {
        "title": "Humans Are Easily Fooled by Digital Images",
        "authors": [
            "Victor Schetinger",
            "Manuel M. Oliveira",
            "Roberto da Silva",
            "Tiago J. Carvalho"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Digital images are ubiquitous in our modern lives, with uses ranging from\nsocial media to news, and even scientific papers. For this reason, it is\ncrucial evaluate how accurate people are when performing the task of identify\ndoctored images. In this paper, we performed an extensive user study evaluating\nsubjects capacity to detect fake images. After observing an image, users have\nbeen asked if it had been altered or not. If the user answered the image has\nbeen altered, he had to provide evidence in the form of a click on the image.\nWe collected 17,208 individual answers from 383 users, using 177 images\nselected from public forensic databases. Different from other previously\nstudies, our method propose different ways to avoid lucky guess when evaluating\nusers answers. Our results indicate that people show inaccurate skills at\ndifferentiating between altered and non-altered images, with an accuracy of\n58%, and only identifying the modified images 46.5% of the time. We also track\nuser features such as age, answering time, confidence, providing deep analysis\nof how such variables influence on the users' performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.05301v1"
    },
    {
        "title": "A semi-automatic computer-aided method for surgical template design",
        "authors": [
            "Xiaojun Chen",
            "Lu Xu",
            "Yue Yang",
            "Jan Egger"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  This paper presents a generalized integrated framework of semi-automatic\nsurgical template design. Several algorithms were implemented including the\nmesh segmentation, offset surface generation, collision detection, ruled\nsurface generation, etc., and a special software named TemDesigner was\ndeveloped. With a simple user interface, a customized template can be semi-\nautomatically designed according to the preoperative plan. Firstly, mesh\nsegmentation with signed scalar of vertex is utilized to partition the inner\nsurface from the input surface mesh based on the indicated point loop. Then,\nthe offset surface of the inner surface is obtained through contouring the\ndistance field of the inner surface, and segmented to generate the outer\nsurface. Ruled surface is employed to connect inner and outer surfaces.\nFinally, drilling tubes are generated according to the preoperative plan\nthrough collision detection and merging. It has been applied to the template\ndesign for various kinds of surgeries, including oral implantology, cervical\npedicle screw insertion, iliosacral screw insertion and osteotomy,\ndemonstrating the efficiency, functionality and generality of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.01644v1"
    },
    {
        "title": "A simple method for estimating the fractal dimension from digital\n  images: The compression dimension",
        "authors": [
            "P. Chamorro-Posada"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  The fractal structure of real world objects is often analyzed using digital\nimages. In this context, the compression fractal dimension is put forward. It\nprovides a simple method for the direct estimation of the dimension of fractals\nstored as digital image files. The computational scheme can be implemented\nusing readily available free software. Its simplicity also makes it very\ninteresting for introductory elaborations of basic concepts of fractal\ngeometry, complexity, and information theory. A test of the computational\nscheme using limited-quality images of well-defined fractal sets obtained from\nthe Internet and free software has been performed. Also, a systematic\nevaluation of the proposed method using computer generated images of the\nWeierstrass cosine function shows an accuracy comparable to those of the\nmethods most commonly used to estimate the dimension of fractal data sequences\napplied to the same test problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.02139v2"
    },
    {
        "title": "Computer Aided Restoration of Handwritten Character Strokes",
        "authors": [
            "Barak Sober",
            "David Levin"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  This work suggests a new variational approach to the task of computer aided\nrestoration of incomplete characters, residing in a highly noisy document. We\nmodel character strokes as the movement of a pen with a varying radius.\nFollowing this model, a cubic spline representation is being utilized to\nperform gradient descent steps, while maintaining interpolation at some initial\n(manually sampled) points. The proposed algorithm was utilized in the process\nof restoring approximately 1000 ancient Hebrew characters (dating to ca.\n8th-7th century BCE), some of which are presented herein and show that the\nalgorithm yields plausible results when applied on deteriorated documents.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.07038v2"
    },
    {
        "title": "Opt: A Domain Specific Language for Non-linear Least Squares\n  Optimization in Graphics and Imaging",
        "authors": [
            "Zachary DeVito",
            "Michael Mara",
            "Michael Zollhöfer",
            "Gilbert Bernstein",
            "Jonathan Ragan-Kelley",
            "Christian Theobalt",
            "Pat Hanrahan",
            "Matthew Fisher",
            "Matthias Nießner"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Many graphics and vision problems can be expressed as non-linear least\nsquares optimizations of objective functions over visual data, such as images\nand meshes. The mathematical descriptions of these functions are extremely\nconcise, but their implementation in real code is tedious, especially when\noptimized for real-time performance on modern GPUs in interactive applications.\nIn this work, we propose a new language, Opt (available under\nhttp://optlang.org), for writing these objective functions over image- or\ngraph-structured unknowns concisely and at a high level. Our compiler\nautomatically transforms these specifications into state-of-the-art GPU solvers\nbased on Gauss-Newton or Levenberg-Marquardt methods. Opt can generate\ndifferent variations of the solver, so users can easily explore tradeoffs in\nnumerical precision, matrix-free methods, and solver approaches. In our\nresults, we implement a variety of real-world graphics and vision applications.\nTheir energy functions are expressible in tens of lines of code, and produce\nhighly-optimized GPU solver implementations. These solver have performance\ncompetitive with the best published hand-tuned, application-specific GPU\nsolvers, and orders of magnitude beyond a general-purpose auto-generated\nsolver.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.06525v3"
    },
    {
        "title": "Manifold Approximation by Moving Least-Squares Projection (MMLS)",
        "authors": [
            "Barak Sober",
            "David Levin"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  In order to avoid the curse of dimensionality, frequently encountered in Big\nData analysis, there was a vast development in the field of linear and\nnonlinear dimension reduction techniques in recent years. These techniques\n(sometimes referred to as manifold learning) assume that the scattered input\ndata is lying on a lower dimensional manifold, thus the high dimensionality\nproblem can be overcome by learning the lower dimensionality behavior. However,\nin real life applications, data is often very noisy. In this work, we propose a\nmethod to approximate $\\mathcal{M}$ a $d$-dimensional $C^{m+1}$ smooth\nsubmanifold of $\\mathbb{R}^n$ ($d \\ll n$) based upon noisy scattered data\npoints (i.e., a data cloud). We assume that the data points are located \"near\"\nthe lower dimensional manifold and suggest a non-linear moving least-squares\nprojection on an approximating $d$-dimensional manifold. Under some mild\nassumptions, the resulting approximant is shown to be infinitely smooth and of\nhigh approximation order (i.e., $O(h^{m+1})$, where $h$ is the fill distance\nand $m$ is the degree of the local polynomial approximation). The method\npresented here assumes no analytic knowledge of the approximated manifold and\nthe approximation algorithm is linear in the large dimension $n$. Furthermore,\nthe approximating manifold can serve as a framework to perform operations\ndirectly on the high dimensional data in a computationally efficient manner.\nThis way, the preparatory step of dimension reduction, which induces\ndistortions to the data, can be avoided altogether.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.07104v7"
    },
    {
        "title": "Understanding and Exploiting Object Interaction Landscapes",
        "authors": [
            "Sören Pirk",
            "Vojtech Krs",
            "Kaimo Hu",
            "Suren Deepak Rajasekaran",
            "Hao Kang",
            "Bedrich Benes",
            "Yusuke Yoshiyasu",
            "Leonidas J. Guibas"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Interactions play a key role in understanding objects and scenes, for both\nvirtual and real world agents. We introduce a new general representation for\nproximal interactions among physical objects that is agnostic to the type of\nobjects or interaction involved. The representation is based on tracking\nparticles on one of the participating objects and then observing them with\nsensors appropriately placed in the interaction volume or on the interaction\nsurfaces. We show how to factorize these interaction descriptors and project\nthem into a particular participating object so as to obtain a new functional\ndescriptor for that object, its interaction landscape, capturing its observed\nuse in a spatio-temporal framework. Interaction landscapes are independent of\nthe particular interaction and capture subtle dynamic effects in how objects\nmove and behave when in functional use. Our method relates objects based on\ntheir function, establishes correspondences between shapes based on functional\nkey points and regions, and retrieves peer and partner objects with respect to\nan interaction.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.08685v2"
    },
    {
        "title": "Numerical Inversion of SRNF Maps for Elastic Shape Analysis of\n  Genus-Zero Surfaces",
        "authors": [
            "Hamid Laga",
            "Qian Xie",
            "Ian H. Jermyn",
            "Anuj Srivastava"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  Recent developments in elastic shape analysis (ESA) are motivated by the fact\nthat it provides comprehensive frameworks for simultaneous registration,\ndeformation, and comparison of shapes. These methods achieve computational\nefficiency using certain square-root representations that transform invariant\nelastic metrics into Euclidean metrics, allowing for applications of standard\nalgorithms and statistical tools. For analyzing shapes of embeddings of\n$\\mathbb{S}^2$ in $\\mathbb{R}^3$, Jermyn et al. introduced square-root normal\nfields (SRNFs) that transformed an elastic metric, with desirable invariant\nproperties, into the $\\mathbb{L}^2$ metric. These SRNFs are essentially surface\nnormals scaled by square-roots of infinitesimal area elements. A critical need\nin shape analysis is to invert solutions (deformations, averages, modes of\nvariations, etc) computed in the SRNF space, back to the original surface space\nfor visualizations and inferences. Due to the lack of theory for understanding\nSRNFs maps and their inverses, we take a numerical approach and derive an\nefficient multiresolution algorithm, based on solving an optimization problem\nin the surface space, that estimates surfaces corresponding to given SRNFs.\nThis solution is found effective, even for complex shapes, e.g. human bodies\nand animals, that undergo significant deformations including bending and\nstretching. Specifically, we use this inversion for computing elastic shape\ndeformations, transferring deformations, summarizing shapes, and for finding\nmodes of variability in a given collection, while simultaneously registering\nthe surfaces. We demonstrate the proposed algorithms using a statistical\nanalysis of human body shapes, classification of generic surfaces and analysis\nof brain structures.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.04531v1"
    },
    {
        "title": "SceneSeer: 3D Scene Design with Natural Language",
        "authors": [
            "Angel X. Chang",
            "Mihail Eric",
            "Manolis Savva",
            "Christopher D. Manning"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Designing 3D scenes is currently a creative task that requires significant\nexpertise and effort in using complex 3D design interfaces. This effortful\ndesign process starts in stark contrast to the easiness with which people can\nuse language to describe real and imaginary environments. We present SceneSeer:\nan interactive text to 3D scene generation system that allows a user to design\n3D scenes using natural language. A user provides input text from which we\nextract explicit constraints on the objects that should appear in the scene.\nGiven these explicit constraints, the system then uses a spatial knowledge base\nlearned from an existing database of 3D scenes and 3D object models to infer an\narrangement of the objects forming a natural scene matching the input\ndescription. Using textual commands the user can then iteratively refine the\ncreated scene by adding, removing, replacing, and manipulating objects. We\nevaluate the quality of 3D scenes generated by SceneSeer in a perceptual\nevaluation experiment where we compare against manually designed scenes and\nsimpler baselines for 3D scene generation. We demonstrate how the generated\nscenes can be iteratively refined through simple natural language commands.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00050v1"
    },
    {
        "title": "Topologically Robust 3D Shape Matching via Gradual Deflation and\n  Inflation",
        "authors": [
            "Asli Genctav",
            "Yusuf Sahillioglu",
            "Sibel Tari"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Despite being vastly ignored in the literature, coping with topological noise\nis an issue of increasing importance, especially as a consequence of the\nincreasing number and diversity of 3D polygonal models that are captured by\ndevices of different qualities or synthesized by algorithms of different\nstabilities. One approach for matching 3D shapes under topological noise is to\nreplace the topology-sensitive geodesic distance with distances that are less\nsensitive to topological changes. We propose an alternative approach utilising\ngradual deflation (or inflation) of the shape volume, of which purpose is to\nbring the pair of shapes to be matched to a \\emph{comparable} topology before\nthe search for correspondences. Illustrative experiments using different\ndatasets demonstrate that as the level of topological noise increases, our\napproach outperforms the other methods in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.00274v2"
    },
    {
        "title": "Evaluation of Direct Haptic 4D Volume Rendering of Partially Segmented\n  Data for Liver Puncture Simulation",
        "authors": [
            "Andre Mastmeyer",
            "Dirk Fortmeier",
            "Heinz Handels"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  This work presents an evaluation study using a force feedback evaluation\nframework for a novel direct needle force volume rendering concept in the\ncontext of liver puncture simulation. PTC/PTCD puncture interventions targeting\nthe bile ducts have been selected to illustrate this concept. The haptic\nalgorithms of the simulator system are based on (1) partially segmented patient\nimage data and (2) a non-linear spring model effective at organ borders. The\nprimary aim is to quantitatively evaluate force errors caused by our patient\nmodeling approach, in comparison to haptic force output obtained from using\ngold-standard, completely manually-segmented data. The evaluation of the force\nalgorithms compared to a force output from fully manually segmented\ngold-standard patient models, yields a low mean of 0.12 N root mean squared\nforce error and up to 1.6 N for systematic maximum absolute errors. Force\nerrors were evaluated on 31,222 preplanned test paths from 10 patients. Only\ntwelve percent of the emitted forces along these paths were affected by errors.\nThis is the first study evaluating haptic algorithms with deformable virtual\npatients in silico. We prove haptic rendering plausibility on a very high\nnumber of test paths. Important errors are below just noticeable differences\nfor the hand-arm system.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.07118v1"
    },
    {
        "title": "On the correlation between a level of structure order and properties of\n  composites. In Memory of Yu.L. Klimontovich",
        "authors": [
            "Alexander Herega"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Proposed the computerized method for calculating the relative level of order\ncomposites. Correlation between a level of structure order and properties of\nsolids is shown. Discussed the possibility of clarifying the terminology used\nin describing the structure.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.02721v2"
    },
    {
        "title": "Hierarchical Detail Enhancing Mesh-Based Shape Generation with 3D\n  Generative Adversarial Network",
        "authors": [
            "Chiyu \"Max\" Jiang",
            "Philip Marcus"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Automatic mesh-based shape generation is of great interest across a wide\nrange of disciplines, from industrial design to gaming, computer graphics and\nvarious other forms of digital art. While most traditional methods focus on\nprimitive based model generation, advances in deep learning made it possible to\nlearn 3-dimensional geometric shape representations in an end-to-end manner.\nHowever, most current deep learning based frameworks focus on the\nrepresentation and generation of voxel and point-cloud based shapes, making it\nnot directly applicable to design and graphics communities. This study\naddresses the needs for automatic generation of mesh-based geometries, and\npropose a novel framework that utilizes signed distance function representation\nthat generates detail preserving three-dimensional surface mesh by a deep\nlearning based approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.07581v1"
    },
    {
        "title": "Dynamic Influence Networks for Rule-based Models",
        "authors": [
            "Angus G. Forbes",
            "Andrew Burks",
            "Kristine Lee",
            "Xing Li",
            "Pierre Boutillier",
            "Jean Krivine",
            "Walter Fontana"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  We introduce the Dynamic Influence Network (DIN), a novel visual analytics\ntechnique for representing and analyzing rule-based models of protein-protein\ninteraction networks. Rule-based modeling has proved instrumental in developing\nbiological models that are concise, comprehensible, easily extensible, and that\nmitigate the combinatorial complexity of multi-state and multi-component\nbiological molecules. Our technique visualizes the dynamics of these rules as\nthey evolve over time. Using the data produced by KaSim, an open source\nstochastic simulator of rule-based models written in the Kappa language, DINs\nprovide a node-link diagram that represents the influence that each rule has on\nthe other rules. That is, rather than representing individual biological\ncomponents or types, we instead represent the rules about them (as nodes) and\nthe current influence of these rules (as links). Using our interactive DIN-Viz\nsoftware tool, researchers are able to query this dynamic network to find\nmeaningful patterns about biological processes, and to identify salient aspects\nof complex rule-based models. To evaluate the effectiveness of our approach, we\ninvestigate a simulation of a circadian clock model that illustrates the\noscillatory behavior of the KaiC protein phosphorylation cycle.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.00967v1"
    },
    {
        "title": "High Dynamic Range Imaging Technology",
        "authors": [
            "Alessandro Artusi",
            "Thomas Richter",
            "Touradj Ebrahimi",
            "Rafal K. Mantiuk"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  In this lecture note, we describe high dynamic range (HDR) imaging systems;\nsuch systems are able to represent luminances of much larger brightness and,\ntypically, also a larger range of colors than conventional standard dynamic\nrange (SDR) imaging systems. The larger luminance range greatly improve the\noverall quality of visual content, making it appears much more realistic and\nappealing to observers. HDR is one of the key technologies of the future\nimaging pipeline, which will change the way the digital visual content is\nrepresented and manipulated today.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.11326v1"
    },
    {
        "title": "Geometry Processing of Conventionally Produced Mouse Brain Slice Images",
        "authors": [
            "Nitin Agarwal",
            "Xiangmin Xu",
            "Gopi Meenakshisundaram"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Brain mapping research in most neuroanatomical laboratories relies on\nconventional processing techniques, which often introduce histological\nartifacts such as tissue tears and tissue loss. In this paper we present\ntechniques and algorithms for automatic registration and 3D reconstruction of\nconventionally produced mouse brain slices in a standardized atlas space. This\nis achieved first by constructing a virtual 3D mouse brain model from annotated\nslices of Allen Reference Atlas (ARA). Virtual re-slicing of the reconstructed\nmodel generates ARA-based slice images corresponding to the microscopic images\nof histological brain sections. These image pairs are aligned using a geometric\napproach through contour images. Histological artifacts in the microscopic\nimages are detected and removed using Constrained Delaunay Triangulation before\nperforming global alignment. Finally, non-linear registration is performed by\nsolving Laplace's equation with Dirichlet boundary conditions. Our methods\nprovide significant improvements over previously reported registration\ntechniques for the tested slices in 3D space, especially on slices with\nsignificant histological artifacts. Further, as an application we count the\nnumber of neurons in various anatomical regions using a dataset of 51\nmicroscopic slices from a single mouse brain. This work represents a\nsignificant contribution to this subfield of neuroscience as it provides tools\nto neuroanatomist for analyzing and processing histological data.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09684v1"
    },
    {
        "title": "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based\n  Character Skills",
        "authors": [
            "Xue Bin Peng",
            "Pieter Abbeel",
            "Sergey Levine",
            "Michiel van de Panne"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  A longstanding goal in character animation is to combine data-driven\nspecification of behavior with a system that can execute a similar behavior in\na physical simulation, thus enabling realistic responses to perturbations and\nenvironmental variation. We show that well-known reinforcement learning (RL)\nmethods can be adapted to learn robust control policies capable of imitating a\nbroad range of example motion clips, while also learning complex recoveries,\nadapting to changes in morphology, and accomplishing user-specified goals. Our\nmethod handles keyframed motions, highly-dynamic actions such as\nmotion-captured flips and spins, and retargeted motions. By combining a\nmotion-imitation objective with a task objective, we can train characters that\nreact intelligently in interactive settings, e.g., by walking in a desired\ndirection or throwing a ball at a user-specified target. This approach thus\ncombines the convenience and motion quality of using motion clips to define the\ndesired style and appearance, with the flexibility and generality afforded by\nRL methods and physics-based animation. We further explore a number of methods\nfor integrating multiple clips into the learning process to develop\nmulti-skilled agents capable of performing a rich repertoire of diverse skills.\nWe demonstrate results using multiple characters (human, Atlas robot, bipedal\ndinosaur, dragon) and a large variety of skills, including locomotion,\nacrobatics, and martial arts.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.02717v3"
    },
    {
        "title": "ALIGNet: Partial-Shape Agnostic Alignment via Unsupervised Learning",
        "authors": [
            "Rana Hanocka",
            "Noa Fish",
            "Zhenhua Wang",
            "Raja Giryes",
            "Shachar Fleishman",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The process of aligning a pair of shapes is a fundamental operation in\ncomputer graphics. Traditional approaches rely heavily on matching\ncorresponding points or features to guide the alignment, a paradigm that\nfalters when significant shape portions are missing. These techniques generally\ndo not incorporate prior knowledge about expected shape characteristics, which\ncan help compensate for any misleading cues left by inaccuracies exhibited in\nthe input shapes. We present an approach based on a deep neural network,\nleveraging shape datasets to learn a shape-aware prior for source-to-target\nalignment that is robust to shape incompleteness. In the absence of ground\ntruth alignments for supervision, we train a network on the task of shape\nalignment using incomplete shapes generated from full shapes for\nself-supervision. Our network, called ALIGNet, is trained to warp complete\nsource shapes to incomplete targets, as if the target shapes were complete,\nthus essentially rendering the alignment partial-shape agnostic. We aim for the\nnetwork to develop specialized expertise over the common characteristics of the\nshapes in each dataset, thereby achieving a higher-level understanding of the\nexpected shape space to which a local approach would be oblivious. We constrain\nALIGNet through an anisotropic total variation identity regularization to\npromote piecewise smooth deformation fields, facilitating both partial-shape\nagnosticism and post-deformation applications. We demonstrate that ALIGNet\nlearns to align geometrically distinct shapes, and is able to infer plausible\nmappings even when the target shape is significantly incomplete. We show that\nour network learns the common expected characteristics of shape collections,\nwithout over-fitting or memorization, enabling it to produce plausible\ndeformations on unseen data during test time.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.08497v2"
    },
    {
        "title": "The Topology ToolKit",
        "authors": [
            "Julien Tierny",
            "Guillaume Favelier",
            "Joshua A. Levine",
            "Charles Gueunet",
            "Michael Michaux"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This system paper presents the Topology ToolKit (TTK), a software platform\ndesigned for topological data analysis in scientific visualization. TTK\nprovides a unified, generic, efficient, and robust implementation of key\nalgorithms for the topological analysis of scalar data, including: critical\npoints, integral lines, persistence diagrams, persistence curves, merge trees,\ncontour trees, Morse-Smale complexes, fiber surfaces, continuous scatterplots,\nJacobi sets, Reeb spaces, and more. TTK is easily accessible to end users due\nto a tight integration with ParaView. It is also easily accessible to\ndevelopers through a variety of bindings (Python, VTK/C++) for fast prototyping\nor through direct, dependence-free, C++, to ease integration into pre-existing\ncomplex systems. While developing TTK, we faced several algorithmic and\nsoftware engineering challenges, which we document in this paper. In\nparticular, we present an algorithm for the construction of a discrete gradient\nthat complies to the critical points extracted in the piecewise-linear setting.\nThis algorithm guarantees a combinatorial consistency across the topological\nabstractions supported by TTK, and importantly, a unified implementation of\ntopological data simplification for multi-scale exploration and analysis. We\nalso present a cached triangulation data structure, that supports time\nefficient and generic traversals, which self-adjusts its memory usage on demand\nfor input simplicial meshes and which implicitly emulates a triangulation for\nregular grids with no memory overhead. Finally, we describe an original\nsoftware architecture, which guarantees memory efficient and direct accesses to\nTTK features, while still allowing for researchers powerful and easy bindings\nand extensions. TTK is open source (BSD license) and its code, online\ndocumentation and video tutorials are available on TTK's website.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09110v2"
    },
    {
        "title": "Multi-directional Geodesic Neural Networks via Equivariant Convolution",
        "authors": [
            "Adrien Poulenard",
            "Maks Ovsjanikov"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We propose a novel approach for performing convolution of signals on curved\nsurfaces and show its utility in a variety of geometric deep learning\napplications. Key to our construction is the notion of directional functions\ndefined on the surface, which extend the classic real-valued signals and which\ncan be naturally convolved with with real-valued template functions. As a\nresult, rather than trying to fix a canonical orientation or only keeping the\nmaximal response across all alignments of a 2D template at every point of the\nsurface, as done in previous works, we show how information across all\nrotations can be kept across different layers of the neural network. Our\nconstruction, which we call multi-directional geodesic convolution, or\ndirectional convolution for short, allows, in particular, to propagate and\nrelate directional information across layers and thus different regions on the\nshape. We first define directional convolution in the continuous setting, prove\nits key properties and then show how it can be implemented in practice, for\nshapes represented as triangle meshes. We evaluate directional convolution in a\nwide variety of learning scenarios ranging from classification of signals on\nsurfaces, to shape segmentation and shape matching, where we show a significant\nimprovement over several baselines.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.02303v1"
    },
    {
        "title": "Recurrent Transition Networks for Character Locomotion",
        "authors": [
            "Félix G. Harvey",
            "Christopher Pal"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Manually authoring transition animations for a complete locomotion system can\nbe a tedious and time-consuming task, especially for large games that allow\ncomplex and constrained locomotion movements, where the number of transitions\ngrows exponentially with the number of states. In this paper, we present a\nnovel approach, based on deep recurrent neural networks, to automatically\ngenerate such transitions given a past context of a few frames and a target\ncharacter state to reach. We present the Recurrent Transition Network (RTN),\nbased on a modified version of the Long-Short-Term-Memory (LSTM) network,\ndesigned specifically for transition generation and trained without any gait,\nphase, contact or action labels. We further propose a simple yet principled way\nto initialize the hidden states of the LSTM layer for a given sequence which\nimproves the performance and generalization to new motions. We both\nquantitatively and qualitatively evaluate our system and show that making the\nnetwork terrain-aware by adding a local terrain representation to the input\nyields better performance for rough-terrain navigation on long transitions. Our\nsystem produces realistic and fluid transitions that rival the quality of\nMotion Capture-based ground-truth motions, even before applying any\ninverse-kinematics postprocess. Direct benefits of our approach could be to\naccelerate the creation of transition variations for large coverage, or even to\nentirely replace transition nodes in an animation graph. We further explore\napplications of this model in a animation super-resolution setting where we\ntemporally decompress animations saved at 1 frame per second and show that the\nnetwork is able to reconstruct motions that are hard to distinguish from\nun-compressed locomotion sequences.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.02363v5"
    },
    {
        "title": "SFV: Reinforcement Learning of Physical Skills from Videos",
        "authors": [
            "Xue Bin Peng",
            "Angjoo Kanazawa",
            "Jitendra Malik",
            "Pieter Abbeel",
            "Sergey Levine"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Data-driven character animation based on motion capture can produce highly\nnaturalistic behaviors and, when combined with physics simulation, can provide\nfor natural procedural responses to physical perturbations, environmental\nchanges, and morphological discrepancies. Motion capture remains the most\npopular source of motion data, but collecting mocap data typically requires\nheavily instrumented environments and actors. In this paper, we propose a\nmethod that enables physically simulated characters to learn skills from videos\n(SFV). Our approach, based on deep pose estimation and deep reinforcement\nlearning, allows data-driven animation to leverage the abundance of publicly\navailable video clips from the web, such as those from YouTube. This has the\npotential to enable fast and easy design of character controllers simply by\nquerying for video recordings of the desired behavior. The resulting\ncontrollers are robust to perturbations, can be adapted to new settings, can\nperform basic object interactions, and can be retargeted to new morphologies\nvia reinforcement learning. We further demonstrate that our method can predict\npotential human motions from still images, by forward simulation of learned\ncontrollers initialized from the observed pose. Our framework is able to learn\na broad range of dynamic skills, including locomotion, acrobatics, and martial\narts.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.03599v2"
    },
    {
        "title": "Learning Implicit Fields for Generative Shape Modeling",
        "authors": [
            "Zhiqin Chen",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We advocate the use of implicit fields for learning generative models of\nshapes and introduce an implicit field decoder, called IM-NET, for shape\ngeneration, aimed at improving the visual quality of the generated shapes. An\nimplicit field assigns a value to each point in 3D space, so that a shape can\nbe extracted as an iso-surface. IM-NET is trained to perform this assignment by\nmeans of a binary classifier. Specifically, it takes a point coordinate, along\nwith a feature vector encoding a shape, and outputs a value which indicates\nwhether the point is outside the shape or not. By replacing conventional\ndecoders by our implicit decoder for representation learning (via IM-AE) and\nshape generation (via IM-GAN), we demonstrate superior results for tasks such\nas generative shape modeling, interpolation, and single-view 3D reconstruction,\nparticularly in terms of visual quality. Code and supplementary material are\navailable at https://github.com/czq142857/implicit-decoder.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.02822v5"
    },
    {
        "title": "ABC: A Big CAD Model Dataset For Geometric Deep Learning",
        "authors": [
            "Sebastian Koch",
            "Albert Matveev",
            "Zhongshi Jiang",
            "Francis Williams",
            "Alexey Artemov",
            "Evgeny Burnaev",
            "Marc Alexa",
            "Denis Zorin",
            "Daniele Panozzo"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We introduce ABC-Dataset, a collection of one million Computer-Aided Design\n(CAD) models for research of geometric deep learning methods and applications.\nEach model is a collection of explicitly parametrized curves and surfaces,\nproviding ground truth for differential quantities, patch segmentation,\ngeometric feature detection, and shape reconstruction. Sampling the parametric\ndescriptions of surfaces and curves allows generating data in different formats\nand resolutions, enabling fair comparisons for a wide range of geometric\nlearning algorithms. As a use case for our dataset, we perform a large-scale\nbenchmark for estimation of surface normals, comparing existing data driven\nmethods and evaluating their performance against both the ground truth and\ntraditional normal estimation methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.06216v2"
    },
    {
        "title": "Animating Arbitrary Objects via Deep Motion Transfer",
        "authors": [
            "Aliaksandr Siarohin",
            "Stéphane Lathuilière",
            "Sergey Tulyakov",
            "Elisa Ricci",
            "Nicu Sebe"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This paper introduces a novel deep learning framework for image animation.\nGiven an input image with a target object and a driving video sequence\ndepicting a moving object, our framework generates a video in which the target\nobject is animated according to the driving sequence. This is achieved through\na deep architecture that decouples appearance and motion information. Our\nframework consists of three main modules: (i) a Keypoint Detector unsupervisely\ntrained to extract object keypoints, (ii) a Dense Motion prediction network for\ngenerating dense heatmaps from sparse keypoints, in order to better encode\nmotion information and (iii) a Motion Transfer Network, which uses the motion\nheatmaps and appearance information extracted from the input image to\nsynthesize the output frames. We demonstrate the effectiveness of our method on\nseveral benchmark datasets, spanning a wide variety of object appearances, and\nshow that our approach outperforms state-of-the-art image animation and video\ngeneration methods. Our source code is publicly available.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.08861v3"
    },
    {
        "title": "A Survey on Non-rigid 3D Shape Analysis",
        "authors": [
            "Hamid Laga"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Shape is an important physical property of natural and manmade 3D objects\nthat characterizes their external appearances. Understanding differences\nbetween shapes and modeling the variability within and across shape classes,\nhereinafter referred to as \\emph{shape analysis}, are fundamental problems to\nmany applications, ranging from computer vision and computer graphics to\nbiology and medicine. This chapter provides an overview of some of the recent\ntechniques that studied the shape of 3D objects that undergo non-rigid\ndeformations including bending and stretching. Recent surveys that covered some\naspects such classification, retrieval, recognition, and rigid or nonrigid\nregistration, focused on methods that use shape descriptors. Descriptors,\nhowever, provide abstract representations that do not enable the exploration of\nshape variability. In this chapter, we focus on recent techniques that treated\nthe shape of 3D objects as points in some high dimensional space where paths\ndescribe deformations. Equipping the space with a suitable metric enables the\nquantification of the range of deformations of a given shape, which in turn\nenables (1) comparing and classifying 3D objects based on their shape, (2)\ncomputing smooth deformations, i.e. geodesics, between pairs of objects, and\n(3) modeling and exploring continuous shape variability in a collection of 3D\nmodels. This article surveys and classifies recent developments in this field,\noutlines fundamental issues, discusses their potential applications in computer\nvision and graphics, and highlights opportunities for future research. Our\nprimary goal is to bridge the gap between various techniques that have been\noften independently proposed by different communities including mathematics and\nstatistics, computer vision and graphics, and medical image analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.10111v1"
    },
    {
        "title": "Sampling Using Neural Networks for colorizing the grayscale images",
        "authors": [
            "Wonbong Jang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  The main idea of this paper is to explore the possibilities of generating\nsamples from the neural networks, mostly focusing on the colorization of the\ngrey-scale images. I will compare the existing methods for colorization and\nexplore the possibilities of using new generative modeling to the task of\ncolorization. The contributions of this paper are to compare the existing\nstructures with similar generating structures(Decoders) and to apply the novel\nstructures including Conditional VAE(CVAE), Conditional Wasserstein GAN with\nGradient Penalty(CWGAN-GP), CWGAN-GP with L1 reconstruction loss, Adversarial\nGenerative Encoders(AGE) and Introspective VAE(IVAE). I trained these models\nusing CIFAR-10 images. To measure the performance, I use Inception Score(IS)\nwhich measures how distinctive each image is and how diverse overall samples\nare as well as human eyes for CIFAR-10 images. It turns out that CVAE with L1\nreconstruction loss and IVAE achieve the highest score in IS. CWGAN-GP with L1\ntends to learn faster than CWGAN-GP, but IS does not increase from CWGAN-GP.\nCWGAN-GP tends to generate more diverse images than other models using\nreconstruction loss. Also, I figured out that the proper regularization plays a\nvital role in generative modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.10650v1"
    },
    {
        "title": "A Visually Plausible Grasping System for Object Manipulation and\n  Interaction in Virtual Reality Environments",
        "authors": [
            "Sergiu Oprea",
            "Pablo Martinez-Gonzalez",
            "Alberto Garcia-Garcia",
            "John Alejandro Castro-Vargas",
            "Sergio Orts-Escolano",
            "Jose Garcia-Rodriguez"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Interaction in virtual reality (VR) environments is essential to achieve a\npleasant and immersive experience. Most of the currently existing VR\napplications, lack of robust object grasping and manipulation, which are the\ncornerstone of interactive systems. Therefore, we propose a realistic, flexible\nand robust grasping system that enables rich and real-time interactions in\nvirtual environments. It is visually realistic because it is completely\nuser-controlled, flexible because it can be used for different hand\nconfigurations, and robust because it allows the manipulation of objects\nregardless their geometry, i.e. hand is automatically fitted to the object\nshape. In order to validate our proposal, an exhaustive qualitative and\nquantitative performance analysis has been carried out. On the one hand,\nqualitative evaluation was used in the assessment of the abstract aspects such\nas: hand movement realism, interaction realism and motor control. On the other\nhand, for the quantitative evaluation a novel error metric has been proposed to\nvisually analyze the performed grips. This metric is based on the computation\nof the distance from the finger phalanges to the nearest contact point on the\nobject surface. These contact points can be used with different application\npurposes, mainly in the field of robotics. As a conclusion, system evaluation\nreports a similar performance between users with previous experience in virtual\nreality applications and inexperienced users, referring to a steep learning\ncurve.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.05238v1"
    },
    {
        "title": "Surface Compression Using Dynamic Color Palettes",
        "authors": [
            "Ayub A. Gubran",
            "Felix Huang",
            "Tor M. Aamodt"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Off-chip memory traffic is a major source of power and energy consumption on\nmobile platforms. A large amount of this off-chip traffic is used to manipulate\ngraphics framebuffer surfaces. To cut down the cost of accessing off-chip\nmemory, framebuffer surfaces are compressed to reduce the bandwidth consumed on\nsurface manipulation when rendering or displaying.\n  In this work, we study the compression properties of framebuffer surfaces and\nhighlight the fact that surfaces from different applications have different\ncompression characteristics. We use the results of our analysis to propose a\nscheme, Dynamic Color Palettes (DCP), which achieves higher compression rates\nwith UI and 2D surfaces.\n  DCP is a hardware mechanism for exploiting inter-frame coherence in lossless\nsurface compression; it implements a scheme that dynamically constructs color\npalettes, which are then used to efficiently compress framebuffer surfaces. To\nevaluate DCP, we created an extensive set of OpenGL workload traces from 124\nAndroid applications. We found that DCP improves compression rates by 91% for\nUI and 20% for 2D applications compared to previous proposals. We also evaluate\na hybrid scheme that combines DCP with a generic compression scheme. We found\nthat compression rates improve over previous proposals by 161%, 124% and 83%\nfor UI, 2D and 3D applications, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.06658v1"
    },
    {
        "title": "Procedural Urban Forestry",
        "authors": [
            "Till Niese",
            "Sören Pirk",
            "Matthias Albrecht",
            "Bedrich Benes",
            "Oliver Deussen"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The placement of vegetation plays a central role in the realism of virtual\nscenes. We introduce procedural placement models (PPMs) for vegetation in urban\nlayouts. PPMs are environmentally sensitive to city geometry and allow\nidentifying plausible plant positions based on structural and functional zones\nin an urban layout. PPMs can either be directly used by defining their\nparameters or can be learned from satellite images and land register data.\nTogether with approaches for generating buildings and trees, this allows us to\npopulate urban landscapes with complex 3D vegetation. The effectiveness of our\nframework is shown through examples of large-scale city scenes and close-ups of\nindividually grown tree models; we also validate it by a perceptual user study.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.05567v2"
    },
    {
        "title": "Motion Similarity Modeling -- A State of the Art Report",
        "authors": [
            "Anna Sebernegg",
            "Peter Kán",
            "Hannes Kaufmann"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The analysis of human motion opens up a wide range of possibilities, such as\nrealistic training simulations or authentic motions in robotics or animation.\nOne of the problems underlying motion analysis is the meaningful comparison of\nactions based on similarity measures. Since the motion analysis is\napplication-dependent, it is essential to find the appropriate motion\nsimilarity method for the particular use case. This state of the art report\nprovides an overview of human motion analysis and different similarity modeling\nmethods, while mainly focusing on approaches that work with 3D motion data. The\nsurvey summarizes various similarity aspects and features of motion and\ndescribes approaches to measuring the similarity between two actions.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.05872v1"
    },
    {
        "title": "Self-Sampling for Neural Point Cloud Consolidation",
        "authors": [
            "Gal Metzer",
            "Rana Hanocka",
            "Raja Giryes",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We introduce a novel technique for neural point cloud consolidation which\nlearns from only the input point cloud. Unlike other point upsampling methods\nwhich analyze shapes via local patches, in this work, we learn from global\nsubsets. We repeatedly self-sample the input point cloud with global subsets\nthat are used to train a deep neural network. Specifically, we define source\nand target subsets according to the desired consolidation criteria (e.g.,\ngenerating sharp points or points in sparse regions). The network learns a\nmapping from source to target subsets, and implicitly learns to consolidate the\npoint cloud. During inference, the network is fed with random subsets of points\nfrom the input, which it displaces to synthesize a consolidated point set. We\nleverage the inductive bias of neural networks to eliminate noise and outliers,\na notoriously difficult problem in point cloud consolidation. The shared\nweights of the network are optimized over the entire shape, learning non-local\nstatistics and exploiting the recurrence of local-scale geometries.\nSpecifically, the network encodes the distribution of the underlying shape\nsurface within a fixed set of local kernels, which results in the best\nexplanation of the underlying shape surface. We demonstrate the ability to\nconsolidate point sets from a variety of shapes, while eliminating outliers and\nnoise.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06471v3"
    },
    {
        "title": "FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers\n  in Dynamic Projection Mapping",
        "authors": [
            "Daiki Tone",
            "Daisuke Iwai",
            "Shinsaku Hiura",
            "Kosuke Sato"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper presents a novel active marker for dynamic projection mapping (PM)\nthat emits a temporal blinking pattern of infrared (IR) light representing its\nID. We used a multi-material three dimensional (3D) printer to fabricate a\nprojection object with optical fibers that can guide IR light from LEDs\nattached on the bottom of the object. The aperture of an optical fiber is\ntypically very small; thus, it is unnoticeable to human observers under\nprojection and can be placed on a strongly curved part of a projection surface.\nIn addition, the working range of our system can be larger than previous\nmarker-based methods as the blinking patterns can theoretically be recognized\nby a camera placed at a wide range of distances from markers. We propose an\nautomatic marker placement algorithm to spread multiple active markers over the\nsurface of a projection object such that its pose can be robustly estimated\nusing captured images from arbitrary directions. We also propose an\noptimization framework for determining the routes of the optical fibers in such\na way that collisions of the fibers can be avoided while minimizing the loss of\nlight intensity in the fibers. Through experiments conducted using three\nfabricated objects containing strongly curved surfaces, we confirmed that the\nproposed method can achieve accurate dynamic PMs in a significantly wide\nworking range.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.02159v1"
    },
    {
        "title": "Audio-Visual-Olfactory Resource Allocation for Tri-modal Virtual\n  Environments",
        "authors": [
            "Efstratios Doukakis",
            "Kurt Debattista",
            "Thomas Bashford-Rogers",
            "Amar Dhokia",
            "Ali Asadipour",
            "Alan Chalmers",
            "Carlo Harvey"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Virtual Environments (VEs) provide the opportunity to simulate a wide range\nof applications, from training to entertainment, in a safe and controlled\nmanner. For applications which require realistic representations of real world\nenvironments, the VEs need to provide multiple, physically accurate sensory\nstimuli. However, simulating all the senses that comprise the human sensory\nsystem (HSS) is a task that requires significant computational resources. Since\nit is intractable to deliver all senses at the highest quality, we propose a\nresource distribution scheme in order to achieve an optimal perceptual\nexperience within the given computational budgets. This paper investigates\nresource balancing for multi-modal scenarios composed of aural, visual and\nolfactory stimuli. Three experimental studies were conducted. The first\nexperiment identified perceptual boundaries for olfactory computation. In the\nsecond experiment, participants (N=25) were asked, across a fixed number of\nbudgets (M=5), to identify what they perceived to be the best visual, acoustic\nand olfactory stimulus quality for a given computational budget. Results\ndemonstrate that participants tend to prioritise visual quality compared to\nother sensory stimuli. However, as the budget size is increased, users prefer a\nbalanced distribution of resources with an increased preference for having\nsmell impulses in the VE. Based on the collected data, a quality prediction\nmodel is proposed and its accuracy is validated against previously unused\nbudgets and an untested scenario in a third and final experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.02671v1"
    },
    {
        "title": "Deep No-reference Tone Mapped Image Quality Assessment",
        "authors": [
            "Chandra Sekhar Ravuri",
            "Rajesh Sureddi",
            "Sathya Veera Reddy Dendi",
            "Shanmuganathan Raman",
            "Sumohana S. Channappayya"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The process of rendering high dynamic range (HDR) images to be viewed on\nconventional displays is called tone mapping. However, tone mapping introduces\ndistortions in the final image which may lead to visual displeasure. To\nquantify these distortions, we introduce a novel no-reference quality\nassessment technique for these tone mapped images. This technique is composed\nof two stages. In the first stage, we employ a convolutional neural network\n(CNN) to generate quality aware maps (also known as distortion maps) from tone\nmapped images by training it with the ground truth distortion maps. In the\nsecond stage, we model the normalized image and distortion maps using an\nAsymmetric Generalized Gaussian Distribution (AGGD). The parameters of the AGGD\nmodel are then used to estimate the quality score using support vector\nregression (SVR). We show that the proposed technique delivers competitive\nperformance relative to the state-of-the-art techniques. The novelty of this\nwork is its ability to visualize various distortions as quality maps\n(distortion maps), especially in the no-reference setting, and to use these\nmaps as features to estimate the quality score of tone mapped images.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.03165v1"
    },
    {
        "title": "Correction of Chromatic Aberration from a Single Image Using Keypoints",
        "authors": [
            "Benjamin T. Cecchetto"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we propose a method to correct for chromatic aberration in a\nsingle photograph. Our method replicates what a user would do in a photo\nediting program to account for this defect. We find matching keypoints in each\ncolour channel then align them as a user would.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.03196v1"
    },
    {
        "title": "Computational Design with Crowds",
        "authors": [
            "Yuki Koyama",
            "Takeo Igarashi"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Computational design is aimed at supporting or automating design processes\nusing computational techniques. However, some classes of design tasks involve\ncriteria that are difficult to handle only with computers. For example, visual\ndesign tasks seeking to fulfill aesthetic goals are difficult to handle purely\nwith computers. One promising approach is to leverage human computation; that\nis, to incorporate human input into the computation process. Crowdsourcing\nplatforms provide a convenient way to integrate such human computation into a\nworking system.\n  In this chapter, we discuss such computational design with crowds in the\ndomain of parameter tweaking tasks in visual design. Parameter tweaking is\noften performed to maximize the aesthetic quality of designed objects.\nComputational design powered by crowds can solve this maximization problem by\nleveraging human computation. We discuss the opportunities and challenges of\ncomputational design with crowds with two illustrative examples: (1) estimating\nthe objective function (specifically, preference learning from crowds' pairwise\ncomparisons) to facilitate interactive design exploration by a designer and (2)\ndirectly searching for the optimal parameter setting that maximizes the\nobjective function (specifically, crowds-in-the-loop Bayesian optimization).\n",
        "pdf_link": "http://arxiv.org/pdf/2002.08657v1"
    },
    {
        "title": "PolyGen: An Autoregressive Generative Model of 3D Meshes",
        "authors": [
            "Charlie Nash",
            "Yaroslav Ganin",
            "S. M. Ali Eslami",
            "Peter W. Battaglia"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Polygon meshes are an efficient representation of 3D geometry, and are of\ncentral importance in computer graphics, robotics and games development.\nExisting learning-based approaches have avoided the challenges of working with\n3D meshes, instead using alternative object representations that are more\ncompatible with neural architectures and training approaches. We present an\napproach which models the mesh directly, predicting mesh vertices and faces\nsequentially using a Transformer-based architecture. Our model can condition on\na range of inputs, including object classes, voxels, and images, and because\nthe model is probabilistic it can produce samples that capture uncertainty in\nambiguous scenarios. We show that the model is capable of producing\nhigh-quality, usable meshes, and establish log-likelihood benchmarks for the\nmesh-modelling task. We also evaluate the conditional models on surface\nreconstruction metrics against alternative methods, and demonstrate competitive\nperformance despite not training directly on this task.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.10880v1"
    },
    {
        "title": "Fairy Lights in Femtoseconds: Aerial and Volumetric Graphics Rendered by\n  Focused Femtosecond Laser Combined with Computational Holographic Fields",
        "authors": [
            "Yoichi Ochiai",
            "Kota Kumagai",
            "Takayuki Hoshi",
            "Jun Rekimoto",
            "Satoshi Hasegawa",
            "Yoshio Hayasaki"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We present a method of rendering aerial and volumetric graphics using\nfemtosecond lasers. A high-intensity laser excites a physical matter to emit\nlight at an arbitrary 3D position. Popular applications can then be explored\nespecially since plasma induced by a femtosecond laser is safer than that\ngenerated by a nanosecond laser. There are two methods of rendering graphics\nwith a femtosecond laser in air: Producing holograms using spatial light\nmodulation technology, and scanning of a laser beam by a galvano mirror. The\nholograms and workspace of the system proposed here occupy a volume of up to 1\ncm^3; however, this size is scalable depending on the optical devices and their\nsetup. This paper provides details of the principles, system setup, and\nexperimental evaluation, and discussions on scalability, design space, and\napplications of this system. We tested two laser sources: an adjustable (30-100\nfs) laser which projects up to 1,000 pulses per second at energy up to 7 mJ per\npulse, and a 269-fs laser which projects up to 200,000 pulses per second at an\nenergy up to 50 uJ per pulse. We confirmed that the spatiotemporal resolution\nof volumetric displays, implemented with these laser sources, is 4,000 and\n200,000 dots per second. Although we focus on laser-induced plasma in air, the\ndiscussion presented here is also applicable to other rendering principles such\nas fluorescence and microbubble in solid/liquid materials.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.06668v1"
    },
    {
        "title": "The Vector Space of Convex Curves: How to Mix Shapes",
        "authors": [
            "Dongsung Huh"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We present a novel, log-radius profile representation for convex curves and\ndefine a new operation for combining the shape features of curves. Unlike the\nstandard, angle profile-based methods, this operation accurately combines the\nshape features in a visually intuitive manner. This method have implications in\nshape analysis as well as in investigating how the brain perceives and\ngenerates curved shapes and motions.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.07515v1"
    },
    {
        "title": "Stable and Controllable Neural Texture Synthesis and Style Transfer\n  Using Histogram Losses",
        "authors": [
            "Eric Risser",
            "Pierre Wilmot",
            "Connelly Barnes"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Recently, methods have been proposed that perform texture synthesis and style\ntransfer by using convolutional neural networks (e.g. Gatys et al.\n[2015,2016]). These methods are exciting because they can in some cases create\nresults with state-of-the-art quality. However, in this paper, we show these\nmethods also have limitations in texture quality, stability, requisite\nparameter tuning, and lack of user controls. This paper presents a multiscale\nsynthesis pipeline based on convolutional neural networks that ameliorates\nthese issues. We first give a mathematical explanation of the source of\ninstabilities in many previous approaches. We then improve these instabilities\nby using histogram losses to synthesize textures that better statistically\nmatch the exemplar. We also show how to integrate localized style losses in our\nmultiscale framework. These losses can improve the quality of large features,\nimprove the separation of content and style, and offer artistic controls such\nas paint by numbers. We demonstrate that our approach offers improved quality,\nconvergence in fewer iterations, and more stability over the optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.08893v2"
    },
    {
        "title": "HoNVis: Visualizing and Exploring Higher-Order Networks",
        "authors": [
            "Jun Tao",
            "Jian Xu",
            "Chaoli Wang",
            "Nitesh V. Chawla"
        ],
        "category": "cs.GR",
        "published_year": "2017",
        "summary": "  Unlike the conventional first-order network (FoN), the higher-order network\n(HoN) provides a more accurate description of transitions by creating\nadditional nodes to encode higher-order dependencies. However, there exists no\nvisualization and exploration tool for the HoN. For applications such as the\ndevelopment of strategies to control species invasion through global shipping\nwhich is known to exhibit higher-order dependencies, the existing FoN\nvisualization tools are limited. In this paper, we present HoNVis, a novel\nvisual analytics framework for exploring higher-order dependencies of the\nglobal ocean shipping network. Our framework leverages coordinated multiple\nviews to reveal the network structure at three levels of detail (i.e., the\nglobal, local, and individual port levels). Users can quickly identify ports of\ninterest at the global level and specify a port to investigate its higher-order\nnodes at the individual port level. Investigating a larger-scale impact is\nenabled through the exploration of HoN at the local level. Using the global\nocean shipping network data, we demonstrate the effectiveness of our approach\nwith a real-world use case conducted by domain experts specializing in species\ninvasion. Finally, we discuss the generalizability of this framework to other\nreal-world applications such as information diffusion in social networks and\nepidemic spreading through air transportation.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.00737v1"
    },
    {
        "title": "Benchmark of Polygon Quality Metrics for Polytopal Element Methods",
        "authors": [
            "M. Attene",
            "S. Biasotti",
            "S. Bertoluzza",
            "D. Cabiddu",
            "M. Livesu",
            "G. Patanè",
            "M. Pennacchio",
            "D. Prada",
            "M. Spagnuolo"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Polytopal Element Methods (PEM) allow to solve differential equations on\ngeneral polygonal and polyhedral grids, potentially offering great flexibility\nto mesh generation algorithms. Differently from classical finite element\nmethods, where the relation between the geometric properties of the mesh and\nthe performances of the solver are well known, the characterization of a good\npolytopal element is still subject to ongoing research. Current shape\nregularity criteria are quite restrictive, and greatly limit the set of valid\nmeshes. Nevertheless, numerical experiments revealed that PEM solvers can\nperform well on meshes that are far outside the strict boundaries imposed by\nthe current theory, suggesting that the real capabilities of these methods are\nmuch higher. In this work, we propose a benchmark to study the correlation\nbetween general 2D polygonal meshes and PEM solvers. The benchmark aims to\nexplore the space of 2D polygonal meshes and polygonal quality metrics, in\norder to identify weaker shape-regularity criteria under which the considered\nmethods can reliably work. The proposed tool is quite general, and can be\npotentially used to study any PEM solver. Besides discussing the basics of the\nbenchmark, in the second part of the paper we demonstrate its application on a\nrepresentative member of the PEM family, namely the Virtual Element Method,\nalso discussing our findings.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.01627v1"
    },
    {
        "title": "A Multi-Pass GAN for Fluid Flow Super-Resolution",
        "authors": [
            "Maximilian Werhahn",
            "You Xie",
            "Mengyu Chu",
            "Nils Thuerey"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a novel method to up-sample volumetric functions with generative\nneural networks using several orthogonal passes. Our method decomposes\ngenerative problems on Cartesian field functions into multiple smaller\nsub-problems that can be learned more efficiently. Specifically, we utilize two\nseparate generative adversarial networks: the first one up-scales slices which\nare parallel to the XY-plane, whereas the second one refines the whole volume\nalong the Z-axis working on slices in the YZ-plane. In this way, we obtain full\ncoverage for the 3D target function and can leverage spatio-temporal\nsupervision with a set of discriminators. Additionally, we demonstrate that our\nmethod can be combined with curriculum learning and progressive growing\napproaches. We arrive at a first method that can up-sample volumes by a factor\nof eight along each dimension, i.e., increasing the number of degrees of\nfreedom by 512. Large volumetric up-scaling factors such as this one have\npreviously not been attainable as the required number of weights in the neural\nnetworks renders adversarial training runs prohibitively difficult. We\ndemonstrate the generality of our trained networks with a series of comparisons\nto previous work, a variety of complex 3D results, and an analysis of the\nresulting performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.01689v1"
    },
    {
        "title": "Laplacian Spectral Basis Functions",
        "authors": [
            "G. Patanè"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Representing a signal as a linear combination of a set of basis functions is\ncentral in a wide range of applications, such as approximation, de-noising,\ncompression, shape correspondence and comparison. In this context, our paper\naddresses the main aspects of signal approximation, such as the definition,\ncomputation, and comparison of basis functions on arbitrary 3D shapes. Focusing\non the class of basis functions induced by the Laplace-Beltrami operator and\nits spectrum, we introduce the diffusion and Laplacian spectral basis\nfunctions, which are then compared with the harmonic and Laplacian\neigenfunctions. As main properties of these basis functions, which are commonly\nused for numerical geometry processing and shape analysis, we discuss the\npartition of the unity and non-negativity; the intrinsic definition and\ninvariance with respect to shape transformations (e.g., translation, rotation,\nuniform scaling); the locality, smoothness, and orthogonality; the numerical\nstability with respect to the domain discretisation; the computational cost and\nstorage overhead. Finally, we consider geometric metrics, such as the area,\nconformal, and kernel-based norms, for the comparison and characterisation of\nthe main properties of the Laplacian basis functions.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.03856v1"
    },
    {
        "title": "A Statistical View on Synthetic Aperture Imaging for Occlusion Removal",
        "authors": [
            "Indrajit Kurmi",
            "David C. Schedl",
            "Oliver Bimber"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Synthetic apertures find applications in many fields, such as radar, radio\ntelescopes, microscopy, sonar, ultrasound, LiDAR, and optical imaging. They\napproximate the signal of a single hypothetical wide aperture sensor with\neither an array of static small aperture sensors or a single moving small\naperture sensor. Common sense in synthetic aperture sampling is that a dense\nsampling pattern within a wide aperture is required to reconstruct a clear\nsignal. In this article we show that there exists practical limits to both,\nsynthetic aperture size and number of samples for the application of occlusion\nremoval. This leads to an understanding on how to design synthetic aperture\nsampling patterns and sensors in a most optimal and practically efficient way.\nWe apply our findings to airborne optical sectioning which uses camera drones\nand synthetic aperture imaging to computationally remove occluding vegetation\nor trees for inspecting ground surfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.06600v1"
    },
    {
        "title": "Topologically robust CAD model generation for structural optimisation",
        "authors": [
            "Ge Yin",
            "Xiao Xiao",
            "Fehmi Cirak"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Computer-aided design (CAD) models play a crucial role in the design,\nmanufacturing and maintenance of products. Therefore, the mesh-based finite\nelement descriptions common in structural optimisation must be first translated\ninto CAD models. Currently, this can at best be performed semi-manually. We\npropose a fully automated and topologically accurate approach to synthesise a\nstructurally-sound parametric CAD model from topology optimised finite element\nmodels. Our solution is to first convert the topology optimised structure into\na spatial frame structure and then to regenerate it in a CAD system using\nstandard constructive solid geometry (CSG) operations. The obtained parametric\nCAD models are compact, that is, have as few as possible geometric parameters,\nwhich makes them ideal for editing and further processing within a CAD system.\nThe critical task of converting the topology optimised structure into an\noptimal spatial frame structure is accomplished in several steps. We first\ngenerate from the topology optimised voxel model a one-voxel-wide voxel chain\nmodel using a topology-preserving skeletonisation algorithm from digital\ntopology. The weighted undirected graph defined by the voxel chain model yields\na spatial frame structure after processing it with standard graph algorithms.\nSubsequently, we optimise the cross-sections and layout of the frame members to\nrecover its optimality, which may have been compromised during the conversion\nprocess. At last, we generate the obtained frame structure in a CAD system by\nrepeatedly combining primitive solids, like cylinders and spheres, using\nboolean operations. The resulting solid model is a boundary representation\n(B-Rep) consisting of trimmed non-uniform rational B-spline (NURBS) curves and\nsurfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.07631v2"
    },
    {
        "title": "Interactive Optimization of Generative Image Modeling using Sequential\n  Subspace Search and Content-based Guidance",
        "authors": [
            "Toby Chong Long Hin",
            "I-Chao Shen",
            "Issei Sato",
            "Takeo Igarashi"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Generative image modeling techniques such as GAN demonstrate highly\nconvincing image generation result. However, user interaction is often\nnecessary to obtain the desired results. Existing attempts add interactivity\nbut require either tailored architectures or extra data. We present a\nhuman-in-the-optimization method that allows users to directly explore and\nsearch the latent vector space of generative image modeling. Our system\nprovides multiple candidates by sampling the latent vector space, and the user\nselects the best blending weights within the subspace using multiple sliders.\nIn addition, the user can express their intention through image editing tools.\nThe system samples latent vectors based on inputs and presents new candidates\nto the user iteratively. An advantage of our formulation is that one can apply\nour method to arbitrary pre-trained model without developing specialized\narchitecture or data. We demonstrate our method with various generative image\nmodeling applications, and show superior performance in a comparative user\nstudy with prior art iGAN.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.09840v3"
    },
    {
        "title": "ORRB -- OpenAI Remote Rendering Backend",
        "authors": [
            "Maciek Chociej",
            "Peter Welinder",
            "Lilian Weng"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present the OpenAI Remote Rendering Backend (ORRB), a system that allows\nfast and customizable rendering of robotics environments. It is based on the\nUnity3d game engine and interfaces with the MuJoCo physics simulation library.\nORRB was designed with visual domain randomization in mind. It is optimized for\ncloud deployment and high throughput operation. We are releasing it to the\npublic under a liberal MIT license: https://github.com/openai/orrb .\n",
        "pdf_link": "http://arxiv.org/pdf/1906.11633v1"
    },
    {
        "title": "Optimizing for Aesthetically Pleasing Quadrotor Camera Motion",
        "authors": [
            "Christoph Gebhardt",
            "Stefan Stevsic",
            "Otmar Hilliges"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper we first contribute a large scale online study (N=400) to\nbetter understand aesthetic perception of aerial video. The results indicate\nthat it is paramount to optimize smoothness of trajectories across all\nkeyframes. However, for experts timing control remains an essential tool.\nSatisfying this dual goal is technically challenging because it requires giving\nup desirable properties in the optimization formulation. Second, informed by\nthis study we propose a method that optimizes positional and temporal reference\nfit jointly. This allows to generate globally smooth trajectories, while\nretaining user control over reference timings. The formulation is posed as a\nvariable, infinite horizon, contour-following algorithm. Finally, a comparative\nlab study indicates that our optimization scheme outperforms the\nstate-of-the-art in terms of perceived usability and preference of resulting\nvideos. For novices our method produces smoother and better looking results and\nalso experts benefit from generated timings.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.11686v1"
    },
    {
        "title": "Speech Gesture Generation from the Trimodal Context of Text, Audio, and\n  Speaker Identity",
        "authors": [
            "Youngwoo Yoon",
            "Bok Cha",
            "Joo-Haeng Lee",
            "Minsu Jang",
            "Jaeyeon Lee",
            "Jaehong Kim",
            "Geehyuk Lee"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  For human-like agents, including virtual avatars and social robots, making\nproper gestures while speaking is crucial in human--agent interaction.\nCo-speech gestures enhance interaction experiences and make the agents look\nalive. However, it is difficult to generate human-like gestures due to the lack\nof understanding of how people gesture. Data-driven approaches attempt to learn\ngesticulation skills from human demonstrations, but the ambiguous and\nindividual nature of gestures hinders learning. In this paper, we present an\nautomatic gesture generation model that uses the multimodal context of speech\ntext, audio, and speaker identity to reliably generate gestures. By\nincorporating a multimodal context and an adversarial training scheme, the\nproposed model outputs gestures that are human-like and that match with speech\ncontent and rhythm. We also introduce a new quantitative evaluation metric for\ngesture generation models. Experiments with the introduced metric and\nsubjective human evaluation showed that the proposed gesture generation model\nis better than existing end-to-end generation models. We further confirm that\nour model is able to work with synthesized audio in a scenario where contexts\nare constrained, and show that different gesture styles can be generated for\nthe same speech by specifying different speaker identities in the style\nembedding space that is learned from videos of various speakers. All the code\nand data is available at\nhttps://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02119v1"
    },
    {
        "title": "SketchPatch: Sketch Stylization via Seamless Patch-level Synthesis",
        "authors": [
            "Noa Fish",
            "Lilach Perry",
            "Amit Bermano",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The paradigm of image-to-image translation is leveraged for the benefit of\nsketch stylization via transfer of geometric textural details. Lacking the\nnecessary volumes of data for standard training of translation systems, we\nadvocate for operation at the patch level, where a handful of stylized sketches\nprovide ample mining potential for patches featuring basic geometric\nprimitives. Operating at the patch level necessitates special consideration of\nfull sketch translation, as individual translation of patches with no regard to\nneighbors is likely to produce visible seams and artifacts at patch borders.\nAligned pairs of styled and plain primitives are combined to form input hybrids\ncontaining styled elements around the border and plain elements within, and\ngiven as input to a seamless translation (ST) generator, whose output patches\nare expected to reconstruct the fully styled patch. An adversarial addition\npromotes generalization and robustness to diverse geometries at inference time,\nforming a simple and effective system for arbitrary sketch stylization, as\ndemonstrated upon a variety of styles and sketches.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02216v1"
    },
    {
        "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and\n  Visualization of Highly Sparse and Noisy Image Data",
        "authors": [
            "Yifan Wang",
            "Guoli Yan",
            "Haikuan Zhu",
            "Sagar Buch",
            "Ying Wang",
            "Ewart Mark Haacke",
            "Jing Hua",
            "Zichun Zhong"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The motivation of our work is to present a new visualization-guided computing\nparadigm to combine direct 3D volume processing and volume rendered clues for\neffective 3D exploration such as extracting and visualizing microstructures\nin-vivo. However, it is still challenging to extract and visualize high\nfidelity 3D vessel structure due to its high sparseness, noisiness, and complex\ntopology variations. In this paper, we present an end-to-end deep learning\nmethod, VC-Net, for robust extraction of 3D microvasculature through embedding\nthe image composition, generated by maximum intensity projection (MIP), into 3D\nvolume image learning to enhance the performance. The core novelty is to\nautomatically leverage the volume visualization technique (MIP) to enhance the\n3D data exploration at deep learning level. The MIP embedding features can\nenhance the local vessel signal and are adaptive to the geometric variability\nand scalability of vessels, which is crucial in microvascular tracking. A\nmulti-stream convolutional neural network is proposed to learn the 3D volume\nand 2D MIP features respectively and then explore their inter-dependencies in a\njoint volume-composition embedding space by unprojecting the MIP features into\n3D volume embedding space. The proposed framework can better capture small /\nmicro vessels and improve vessel connectivity. To our knowledge, this is the\nfirst deep learning framework to construct a joint convolutional embedding\nspace, where the computed vessel probabilities from volume rendering based 2D\nprojection and 3D volume can be explored and integrated synergistically.\nExperimental results are compared with the traditional 3D vessel segmentation\nmethods and the deep learning state-of-the-art on public and real patient\n(micro-)cerebrovascular image datasets. Our method demonstrates the potential\nin a powerful MR arteriogram and venogram diagnosis of vascular diseases.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06184v1"
    },
    {
        "title": "ShapeAssembly: Learning to Generate Programs for 3D Shape Structure\n  Synthesis",
        "authors": [
            "R. Kenny Jones",
            "Theresa Barton",
            "Xianghao Xu",
            "Kai Wang",
            "Ellen Jiang",
            "Paul Guerrero",
            "Niloy J. Mitra",
            "Daniel Ritchie"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Manually authoring 3D shapes is difficult and time consuming; generative\nmodels of 3D shapes offer compelling alternatives. Procedural representations\nare one such possibility: they offer high-quality and editable results but are\ndifficult to author and often produce outputs with limited diversity. On the\nother extreme are deep generative models: given enough data, they can learn to\ngenerate any class of shape but their outputs have artifacts and the\nrepresentation is not editable. In this paper, we take a step towards achieving\nthe best of both worlds for novel 3D shape synthesis. We propose ShapeAssembly,\na domain-specific \"assembly-language\" for 3D shape structures. ShapeAssembly\nprograms construct shapes by declaring cuboid part proxies and attaching them\nto one another, in a hierarchical and symmetrical fashion. Its functions are\nparameterized with free variables, so that one program structure is able to\ncapture a family of related shapes. We show how to extract ShapeAssembly\nprograms from existing shape structures in the PartNet dataset. Then we train a\ndeep generative model, a hierarchical sequence VAE, that learns to write novel\nShapeAssembly programs. The program captures the subset of variability that is\ninterpretable and editable. The deep model captures correlations across shape\ncollections that are hard to express procedurally. We evaluate our approach by\ncomparing shapes output by our generated programs to those from other recent\nshape structure synthesis models. We find that our generated shapes are more\nplausible and physically-valid than those of other methods. Additionally, we\nassess the latent spaces of these models, and find that ours is better\nstructured and produces smoother interpolations. As an application, we use our\ngenerative model and differentiable program interpreter to infer and fit shape\nprograms to unstructured geometry, such as point clouds.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.08026v1"
    },
    {
        "title": "Efficient conformal parameterization of multiply-connected surfaces\n  using quasi-conformal theory",
        "authors": [
            "Gary P. T. Choi"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Conformal mapping, a classical topic in complex analysis and differential\ngeometry, has become a subject of great interest in the area of surface\nparameterization in recent decades with various applications in science and\nengineering. However, most of the existing conformal parameterization\nalgorithms only focus on simply-connected surfaces and cannot be directly\napplied to surfaces with holes. In this work, we propose two novel algorithms\nfor computing the conformal parameterization of multiply-connected surfaces. We\nfirst develop an efficient method for conformally parameterizing an open\nsurface with one hole to an annulus on the plane. Based on this method, we then\ndevelop an efficient method for conformally parameterizing an open surface with\n$k$ holes onto a unit disk with $k$ circular holes. The conformality and\nbijectivity of the mappings are ensured by quasi-conformal theory. Numerical\nexperiments and applications are presented to demonstrate the effectiveness of\nthe proposed methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.08279v2"
    },
    {
        "title": "On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes",
        "authors": [
            "Thomas Davies",
            "Derek Nowrouzezahrai",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  A neural implicit outputs a number indicating whether the given query point\nin space is inside, outside, or on a surface. Many prior works have focused on\n_latent-encoded_ neural implicits, where a latent vector encoding of a specific\nshape is also fed as input. While affording latent-space interpolation, this\ncomes at the cost of reconstruction accuracy for any _single_ shape. Training a\nspecific network for each 3D shape, a _weight-encoded_ neural implicit may\nforgo the latent vector and focus reconstruction accuracy on the details of a\nsingle shape. While previously considered as an intermediary representation for\n3D scanning tasks or as a toy-problem leading up to latent-encoding tasks,\nweight-encoded neural implicits have not yet been taken seriously as a 3D shape\nrepresentation. In this paper, we establish that weight-encoded neural\nimplicits meet the criteria of a first-class 3D shape representation. We\nintroduce a suite of technical contributions to improve reconstruction\naccuracy, convergence, and robustness when learning the signed distance field\ninduced by a polygonal mesh -- the _de facto_ standard representation. Viewed\nas a lossy compression, our conversion outperforms standard techniques from\ngeometry processing. Compared to previous latent- and weight-encoded neural\nimplicits we demonstrate superior robustness, scalability, and performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09808v3"
    },
    {
        "title": "Sparse Norm Filtering",
        "authors": [
            "Chengxi Ye",
            "Dacheng Tao",
            "Mingli Song",
            "David W. Jacobs",
            "Min Wu"
        ],
        "category": "cs.GR",
        "published_year": "2013",
        "summary": "  Optimization-based filtering smoothes an image by minimizing a fidelity\nfunction and simultaneously preserves edges by exploiting a sparse norm penalty\nover gradients. It has obtained promising performance in practical problems,\nsuch as detail manipulation, HDR compression and deblurring, and thus has\nreceived increasing attentions in fields of graphics, computer vision and image\nprocessing. This paper derives a new type of image filter called sparse norm\nfilter (SNF) from optimization-based filtering. SNF has a very simple form,\nintroduces a general class of filtering techniques, and explains several\nclassic filters as special implementations of SNF, e.g. the averaging filter\nand the median filter. It has advantages of being halo free, easy to implement,\nand low time and memory costs (comparable to those of the bilateral filter).\nThus, it is more generic than a smoothing operator and can better adapt to\ndifferent tasks. We validate the proposed SNF by a wide variety of applications\nincluding edge-preserving smoothing, outlier tolerant filtering, detail\nmanipulation, HDR compression, non-blind deconvolution, image segmentation, and\ncolorization.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.3971v1"
    },
    {
        "title": "Color Sails: Discrete-Continuous Palettes for Deep Color Exploration",
        "authors": [
            "Maria Shugrina",
            "Amlan Kar",
            "Karan Singh",
            "Sanja Fidler"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We present color sails, a discrete-continuous color gamut representation that\nextends the color gradient analogy to three dimensions and allows interactive\ncontrol of the color blending behavior. Our representation models a wide\nvariety of color distributions in a compact manner, and lends itself to\napplications such as color exploration for graphic design, illustration and\nsimilar fields. We propose a Neural Network that can fit a color sail to any\nimage. Then, the user can adjust color sail parameters to change the base\ncolors, their blending behavior and the number of colors, exploring a wide\nrange of options for the original design. In addition, we propose a Deep\nLearning model that learns to automatically segment an image into\ncolor-compatible alpha masks, each equipped with its own color sail. This\nallows targeted color exploration by either editing their corresponding color\nsails or using standard software packages. Our model is trained on a custom\ndiverse dataset of art and design. We provide both quantitative evaluations,\nand a user study, demonstrating the effectiveness of color sail interaction.\nInteractive demos are available at www.colorsails.com.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.02918v1"
    },
    {
        "title": "Convolutional sparse coding for capturing high speed video content",
        "authors": [
            "Ana Serrano",
            "Elena Garces",
            "Diego Gutierrez",
            "Belen Masia"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Video capture is limited by the trade-off between spatial and temporal\nresolution: when capturing videos of high temporal resolution, the spatial\nresolution decreases due to bandwidth limitations in the capture system.\nAchieving both high spatial and temporal resolution is only possible with\nhighly specialized and very expensive hardware, and even then the same basic\ntrade-off remains. The recent introduction of compressive sensing and sparse\nreconstruction techniques allows for the capture of single-shot high-speed\nvideo, by coding the temporal information in a single frame, and then\nreconstructing the full video sequence from this single coded image and a\ntrained dictionary of image patches. In this paper, we first analyze this\napproach, and find insights that help improve the quality of the reconstructed\nvideos. We then introduce a novel technique, based on convolutional sparse\ncoding (CSC), and show how it outperforms the state-of-the-art, patch-based\napproach in terms of flexibility and efficiency, due to the convolutional\nnature of its filter banks. The key idea for CSC high-speed video acquisition\nis extending the basic formulation by imposing an additional constraint in the\ntemporal dimension, which enforces sparsity of the first-order derivatives over\ntime.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04935v1"
    },
    {
        "title": "Generative Models for Pose Transfer",
        "authors": [
            "Patrick Chao",
            "Alexander Li",
            "Gokul Swamy"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We investigate nearest neighbor and generative models for transferring pose\nbetween persons. We take in a video of one person performing a sequence of\nactions and attempt to generate a video of another person performing the same\nactions. Our generative model (pix2pix) outperforms k-NN at both generating\ncorresponding frames and generalizing outside the demonstrated action set. Our\nmost salient contribution is determining a pipeline (pose detection, face\ndetection, k-NN based pairing) that is effective at perform-ing the desired\ntask. We also detail several iterative improvements and failure modes.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.09070v1"
    },
    {
        "title": "Advances in the Treatment of Trimmed CAD Models due to Isogeometric\n  Analysis",
        "authors": [
            "Benjamin Marussig"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Trimming is a core technique in geometric modeling. Unfortunately, the\nresulting objects do not take the requirements of numerical simulations into\naccount and yield various problems. This paper outlines principal issues of\ntrimmed models and highlights different analysis-suitable strategies to address\nthem. It is discussed that these concepts not only provide important\ncomputational tools for isogeometric analysis, but can also improve the\ntreatment of trimmed models in a design context.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.01192v1"
    },
    {
        "title": "Local Fourier Slice Photography",
        "authors": [
            "Christian Lessig"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Light field cameras provide intriguing possibilities, such as post-capture\nrefocus or the ability to synthesize images from novel viewpoints. This comes,\nhowever, at the price of significant storage requirements. Compression\ntechniques can be used to reduce these but refocusing and reconstruction\nrequire so far again a dense pixel representation. To avoid this, we introduce\nlocal Fourier slice photography that allows for refocused image reconstruction\ndirectly from a sparse wavelet representation of a light field, either to\nobtain an image or a compressed representation of it. The result is made\npossible by wavelets that respect the \"slicing's\" intrinsic structure and\nenable us to derive exact reconstruction filters for the refocused image in\nclosed form. Image reconstruction then amounts to applying these filters to the\nlight field's wavelet coefficients, and hence no reconstruction of a dense\npixel representation is required. We demonstrate that this substantially\nreduces storage requirements and also computation times. We furthermore analyze\nthe computational complexity of our algorithm and show that it scales linearly\nwith the size of the reconstructed region and the non-negligible wavelet\ncoefficients, i.e. with the visual complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.06082v2"
    },
    {
        "title": "Single Image Portrait Relighting",
        "authors": [
            "Tiancheng Sun",
            "Jonathan T. Barron",
            "Yun-Ta Tsai",
            "Zexiang Xu",
            "Xueming Yu",
            "Graham Fyffe",
            "Christoph Rhemann",
            "Jay Busch",
            "Paul Debevec",
            "Ravi Ramamoorthi"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Lighting plays a central role in conveying the essence and depth of the\nsubject in a portrait photograph. Professional photographers will carefully\ncontrol the lighting in their studio to manipulate the appearance of their\nsubject, while consumer photographers are usually constrained to the\nillumination of their environment. Though prior works have explored techniques\nfor relighting an image, their utility is usually limited due to requirements\nof specialized hardware, multiple images of the subject under controlled or\nknown illuminations, or accurate models of geometry and reflectance. To this\nend, we present a system for portrait relighting: a neural network that takes\nas input a single RGB image of a portrait taken with a standard cellphone\ncamera in an unconstrained environment, and from that image produces a relit\nimage of that subject as though it were illuminated according to any provided\nenvironment map. Our method is trained on a small database of 18 individuals\ncaptured under different directional light sources in a controlled light stage\nsetup consisting of a densely sampled sphere of lights. Our proposed technique\nproduces quantitatively superior results on our dataset's validation set\ncompared to prior works, and produces convincing qualitative relighting results\non a dataset of hundreds of real-world cellphone portraits. Because our\ntechnique can produce a 640 $\\times$ 640 image in only 160 milliseconds, it may\nenable interactive user-facing photographic applications in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.00824v1"
    },
    {
        "title": "A Similarity Measure for Material Appearance",
        "authors": [
            "Manuel Lagunas",
            "Sandra Malpica",
            "Ana Serrano",
            "Elena Garces",
            "Diego Gutierrez",
            "Belen Masia"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a model to measure the similarity in appearance between different\nmaterials, which correlates with human similarity judgments. We first create a\ndatabase of 9,000 rendered images depicting objects with varying materials,\nshape and illumination. We then gather data on perceived similarity from\ncrowdsourced experiments; our analysis of over 114,840 answers suggests that\nindeed a shared perception of appearance similarity exists. We feed this data\nto a deep learning architecture with a novel loss function, which learns a\nfeature space for materials that correlates with such perceived appearance\nsimilarity. Our evaluation shows that our model outperforms existing metrics.\nLast, we demonstrate several applications enabled by our metric, including\nappearance-based search for material suggestions, database visualization,\nclustering and summarization, and gamut mapping.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.01562v1"
    },
    {
        "title": "An Incremental Dimensionality Reduction Method for Visualizing Streaming\n  Multidimensional Data",
        "authors": [
            "Takanori Fujiwara",
            "Jia-Kai Chou",
            " Shilpika",
            "Panpan Xu",
            "Liu Ren",
            "Kwan-Liu Ma"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Dimensionality reduction (DR) methods are commonly used for analyzing and\nvisualizing multidimensional data. However, when data is a live streaming feed,\nconventional DR methods cannot be directly used because of their computational\ncomplexity and inability to preserve the projected data positions at previous\ntime points. In addition, the problem becomes even more challenging when the\ndynamic data records have a varying number of dimensions as often found in\nreal-world applications. This paper presents an incremental DR solution. We\nenhance an existing incremental PCA method in several ways to ensure its\nusability for visualizing streaming multidimensional data. First, we use\ngeometric transformation and animation methods to help preserve a viewer's\nmental map when visualizing the incremental results. Second, to handle data\ndimension variants, we use an optimization method to estimate the projected\ndata positions, and also convey the resulting uncertainty in the visualization.\nWe demonstrate the effectiveness of our design with two case studies using\nreal-world datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.04000v3"
    },
    {
        "title": "Toward Standardized Classification of Foveated Displays",
        "authors": [
            "Josef Spjut",
            "Ben Boudaoud",
            "Jonghyun Kim",
            "Trey Greer",
            "Rachel Albert",
            "Michael Stengel",
            "Kaan Aksit",
            "David Luebke"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Emergent in the field of head mounted display design is a desire to leverage\nthe limitations of the human visual system to reduce the computation,\ncommunication, and display workload in power and form-factor constrained\nsystems. Fundamental to this reduced workload is the ability to match display\nresolution to the acuity of the human visual system, along with a resulting\nneed to follow the gaze of the eye as it moves, a process referred to as\nfoveation. A display that moves its content along with the eye may be called a\nFoveated Display, though this term is also commonly used to describe displays\nwith non-uniform resolution that attempt to mimic human visual acuity. We\ntherefore recommend a definition for the term Foveated Display that accepts\nboth of these interpretations. Furthermore, we include a simplified model for\nhuman visual Acuity Distribution Functions (ADFs) at various levels of visual\nacuity, across wide fields of view and propose comparison of this ADF with the\nResolution Distribution Function of a foveated display for evaluation of its\nresolution at a particular gaze direction. We also provide a taxonomy to allow\nthe field to meaningfully compare and contrast various aspects of foveated\ndisplays in a display and optical technology-agnostic manner.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.06229v2"
    },
    {
        "title": "Neural Cages for Detail-Preserving 3D Deformations",
        "authors": [
            "Wang Yifan",
            "Noam Aigerman",
            "Vladimir G. Kim",
            "Siddhartha Chaudhuri",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a novel learnable representation for detail-preserving shape\ndeformation. The goal of our method is to warp a source shape to match the\ngeneral structure of a target shape, while preserving the surface details of\nthe source. Our method extends a traditional cage-based deformation technique,\nwhere the source shape is enclosed by a coarse control mesh termed \\emph{cage},\nand translations prescribed on the cage vertices are interpolated to any point\non the source mesh via special weight functions. The use of this sparse cage\nscaffolding enables preserving surface details regardless of the shape's\nintricacy and topology. Our key contribution is a novel neural network\narchitecture for predicting deformations by controlling the cage. We\nincorporate a differentiable cage-based deformation module in our architecture,\nand train our network end-to-end. Our method can be trained with common\ncollections of 3D models in an unsupervised fashion, without any cage-specific\nannotations. We demonstrate the utility of our method for synthesizing shape\nvariations and deformation transfer.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06395v2"
    },
    {
        "title": "Learned Interpolation for 3D Generation",
        "authors": [
            "Austin Dill",
            "Songwei Ge",
            "Eunsu Kang",
            "Chun-Liang Li",
            "Barnabas Poczos"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In order to generate novel 3D shapes with machine learning, one must allow\nfor interpolation. The typical approach for incorporating this creative process\nis to interpolate in a learned latent space so as to avoid the problem of\ngenerating unrealistic instances by exploiting the model's learned structure.\nThe process of the interpolation is supposed to form a semantically smooth\nmorphing. While this approach is sound for synthesizing realistic media such as\nlifelike portraits or new designs for everyday objects, it subjectively fails\nto directly model the unexpected, unrealistic, or creative. In this work, we\npresent a method for learning how to interpolate point clouds. By encoding\nprior knowledge about real-world objects, the intermediate forms are both\nrealistic and unlike any existing forms. We show not only how this method can\nbe used to generate \"creative\" point clouds, but how the method can also be\nleveraged to generate 3D models suitable for sculpture.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10787v2"
    },
    {
        "title": "Skeleton Extraction from 3D Point Clouds by Decomposing the Object into\n  Parts",
        "authors": [
            "Vijai Jayadevan",
            "Edward Delp",
            "Zygmunt Pizlo"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Decomposing a point cloud into its components and extracting curve skeletons\nfrom point clouds are two related problems. Decomposition of a shape into its\ncomponents is often obtained as a byproduct of skeleton extraction. In this\nwork, we propose to extract curve skeletons, from unorganized point clouds, by\ndecomposing the object into its parts, identifying part skeletons and then\nlinking these part skeletons together to obtain the complete skeleton. We\nbelieve it is the most natural way to extract skeletons in the sense that this\nwould be the way a human would approach the problem. Our parts are generalized\ncylinders (GCs). Since, the axis of a GC is an integral part of its definition,\nthe parts have natural skeletal representations. We use translational symmetry,\nthe fundamental property of GCs, to extract parts from point clouds. We\ndemonstrate how this method can handle a large variety of shapes. We compare\nour method with state of the art methods and show how a part based approach can\ndeal with some of the limitations of other methods. We present an improved\nversion of an existing point set registration algorithm and demonstrate its\nutility in extracting parts from point clouds. We also show how this method can\nbe used to extract skeletons from and identify parts of noisy point clouds. A\npart based approach also provides a natural and intuitive interface for user\ninteraction. We demonstrate the ease with which mistakes, if any, can be fixed\nwith minimal user interaction with the help of a graphical user interface.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11932v1"
    },
    {
        "title": "On Demand Solid Texture Synthesis Using Deep 3D Networks",
        "authors": [
            "Jorge Gutierrez",
            "Julien Rabin",
            "Bruno Galerne",
            "Thomas Hurtut"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper describes a novel approach for on demand volumetric texture\nsynthesis based on a deep learning framework that allows for the generation of\nhigh quality 3D data at interactive rates. Based on a few example images of\ntextures, a generative network is trained to synthesize coherent portions of\nsolid textures of arbitrary sizes that reproduce the visual characteristics of\nthe examples along some directions. To cope with memory limitations and\ncomputation complexity that are inherent to both high resolution and 3D\nprocessing on the GPU, only 2D textures referred to as \"slices\" are generated\nduring the training stage. These synthetic textures are compared to exemplar\nimages via a perceptual loss function based on a pre-trained deep network. The\nproposed network is very light (less than 100k parameters), therefore it only\nrequires sustainable training (i.e. few hours) and is capable of very fast\ngeneration (around a second for $256^3$ voxels) on a single GPU. Integrated\nwith a spatially seeded PRNG the proposed generator network directly returns an\nRGB value given a set of 3D coordinates. The synthesized volumes have good\nvisual results that are at least equivalent to the state-of-the-art patch based\napproaches. They are naturally seamlessly tileable and can be fully generated\nin parallel.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.04528v1"
    },
    {
        "title": "Vid2Curve: Simultaneous Camera Motion Estimation and Thin Structure\n  Reconstruction from an RGB Video",
        "authors": [
            "Peng Wang",
            "Lingjie Liu",
            "Nenglun Chen",
            "Hung-Kuo Chu",
            "Christian Theobalt",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Thin structures, such as wire-frame sculptures, fences, cables, power lines,\nand tree branches, are common in the real world. It is extremely challenging to\nacquire their 3D digital models using traditional image-based or depth-based\nreconstruction methods because thin structures often lack distinct point\nfeatures and have severe self-occlusion. We propose the first approach that\nsimultaneously estimates camera motion and reconstructs the geometry of complex\n3D thin structures in high quality from a color video captured by a handheld\ncamera. Specifically, we present a new curve-based approach to estimate\naccurate camera poses by establishing correspondences between featureless thin\nobjects in the foreground in consecutive video frames, without requiring visual\ntexture in the background scene to lock on. Enabled by this effective\ncurve-based camera pose estimation strategy, we develop an iterative\noptimization method with tailored measures on geometry, topology as well as\nself-occlusion handling for reconstructing 3D thin structures. Extensive\nvalidations on a variety of thin structures show that our method achieves\naccurate camera pose estimation and faithful reconstruction of 3D thin\nstructures with complex shape and topology at a level that has not been\nattained by other existing reconstruction methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03372v3"
    },
    {
        "title": "Sequential Gallery for Interactive Visual Design Optimization",
        "authors": [
            "Yuki Koyama",
            "Issei Sato",
            "Masataka Goto"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Visual design tasks often involve tuning many design parameters. For example,\ncolor grading of a photograph involves many parameters, some of which\nnon-expert users might be unfamiliar with. We propose a novel user-in-the-loop\noptimization method that allows users to efficiently find an appropriate\nparameter set by exploring such a high-dimensional design space through much\neasier two-dimensional search subtasks. This method, called sequential plane\nsearch, is based on Bayesian optimization to keep necessary queries to users as\nfew as possible. To help users respond to plane-search queries, we also propose\nusing a gallery-based interface that provides options in the two-dimensional\nsubspace arranged in an adaptive grid view. We call this interactive framework\nSequential Gallery since users sequentially select the best option from the\noptions provided by the interface. Our experiment with synthetic functions\nshows that our sequential plane search can find satisfactory solutions in fewer\niterations than baselines. We also conducted a preliminary user study, results\nof which suggest that novices can effectively complete search tasks with\nSequential Gallery in a photo-enhancement scenario.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.04107v1"
    },
    {
        "title": "ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills",
        "authors": [
            "Zhaoming Xie",
            "Hung Yu Ling",
            "Nam Hee Kim",
            "Michiel van de Panne"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Humans are highly adept at walking in environments with foot placement\nconstraints, including stepping-stone scenarios where the footstep locations\nare fully constrained. Finding good solutions to stepping-stone locomotion is a\nlongstanding and fundamental challenge for animation and robotics. We present\nfully learned solutions to this difficult problem using reinforcement learning.\nWe demonstrate the importance of a curriculum for efficient learning and\nevaluate four possible curriculum choices compared to a non-curriculum\nbaseline. Results are presented for a simulated human character, a realistic\nbipedal robot simulation and a monster character, in each case producing\nrobust, plausible motions for challenging stepping stone sequences and\nterrains.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.04323v2"
    },
    {
        "title": "Unpaired Motion Style Transfer from Video to Animation",
        "authors": [
            "Kfir Aberman",
            "Yijia Weng",
            "Dani Lischinski",
            "Daniel Cohen-Or",
            "Baoquan Chen"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Transferring the motion style from one animation clip to another, while\npreserving the motion content of the latter, has been a long-standing problem\nin character animation. Most existing data-driven approaches are supervised and\nrely on paired data, where motions with the same content are performed in\ndifferent styles. In addition, these approaches are limited to transfer of\nstyles that were seen during training. In this paper, we present a novel\ndata-driven framework for motion style transfer, which learns from an unpaired\ncollection of motions with style labels, and enables transferring motion styles\nnot observed during training. Furthermore, our framework is able to extract\nmotion styles directly from videos, bypassing 3D reconstruction, and apply them\nto the 3D input motion. Our style transfer network encodes motions into two\nlatent codes, for content and for style, each of which plays a different role\nin the decoding (synthesis) process. While the content code is decoded into the\noutput motion by several temporal convolutional layers, the style code modifies\ndeep features via temporally invariant adaptive instance normalization (AdaIN).\nMoreover, while the content code is encoded from 3D joint rotations, we learn a\ncommon embedding for style from either 3D or 2D joint positions, enabling style\nextraction from videos. Our results are comparable to the state-of-the-art,\ndespite not requiring paired training data, and outperform other methods when\ntransferring previously unseen styles. To our knowledge, we are the first to\ndemonstrate style transfer directly from videos to 3D animations - an ability\nwhich enables one to extend the set of style examples far beyond motions\ncaptured by MoCap systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.05751v1"
    },
    {
        "title": "A Survey on Patch-based Synthesis: GPU Implementation and Optimization",
        "authors": [
            "Hadi Abdi Khojasteh"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This thesis surveys the research in patch-based synthesis and algorithms for\nfinding correspondences between small local regions of images. We additionally\nexplore a large kind of applications of this new fast randomized matching\ntechnique. One of the algorithms we have studied in particular is PatchMatch,\ncan find similar regions or \"patches\" of an image one to two orders of\nmagnitude faster than previous techniques. The algorithmic program is driven by\napplying mathematical properties of nearest neighbors in natural images. It is\nobserved that neighboring correspondences tend to be similar or \"coherent\" and\nuse this observation in algorithm in order to quickly converge to an\napproximate solution. The algorithm is the most general form can find k-nearest\nneighbor matching, using patches that translate, rotate, or scale, using\narbitrary descriptors, and between two or more images. Speed-ups are obtained\nover various techniques in an exceeding range of those areas. We have explored\nmany applications of PatchMatch matching algorithm. In computer graphics, we\nhave explored removing unwanted objects from images, seamlessly moving objects\nin images, changing image aspect ratios, and video summarization. In computer\nvision we have explored denoising images, object detection, detecting image\nforgeries, and detecting symmetries. We conclude by discussing the restrictions\nof our algorithmic program, GPU implementation and areas for future analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.06278v1"
    },
    {
        "title": "Representing Whole Slide Cancer Image Features with Hilbert Curves",
        "authors": [
            "Erich Bremer",
            "Jonas Almeida",
            "Joel Saltz"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Regions of Interest (ROI) contain morphological features in pathology whole\nslide images (WSI) are delimited with polygons[1]. These polygons are often\nrepresented in either a textual notation (with the array of edges) or in a\nbinary mask form. Textual notations have an advantage of human readability and\nportability, whereas, binary mask representations are more useful as the input\nand output of feature-extraction pipelines that employ deep learning\nmethodologies. For any given whole slide image, more than a million cellular\nfeatures can be segmented generating a corresponding number of polygons. The\ncorpus of these segmentations for all processed whole slide images creates\nvarious challenges for filtering specific areas of data for use in interactive\nreal-time and multi-scale displays and analysis. Simple range queries of image\nlocations do not scale and, instead, spatial indexing schemes are required. In\nthis paper we propose using Hilbert Curves simultaneously for spatial indexing\nand as a polygonal ROI representation. This is achieved by using a series of\nHilbert Curves[2] creating an efficient and inherently spatially-indexed\nmachine-usable form. The distinctive property of Hilbert curves that enables\nboth mask and polygon delimitation of ROIs is that the elements of the vector\nextracted ro describe morphological features maintain their relative positions\nfor different scales of the same image.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.06469v1"
    },
    {
        "title": "Generative Adversarial Networks for photo to Hayao Miyazaki style\n  cartoons",
        "authors": [
            "Filip Andersson",
            "Simon Arvidsson"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper takes on the problem of transferring the style of cartoon images\nto real-life photographic images by implementing previous work done by\nCartoonGAN. We trained a Generative Adversial Network(GAN) on over 60 000\nimages from works by Hayao Miyazaki at Studio Ghibli. To evaluate our results,\nwe conducted a qualitative survey comparing our results with two\nstate-of-the-art methods. 117 survey results indicated that our model on\naverage outranked state-of-the-art methods on cartoon-likeness.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.07702v1"
    },
    {
        "title": "Point2Mesh: A Self-Prior for Deformable Meshes",
        "authors": [
            "Rana Hanocka",
            "Gal Metzer",
            "Raja Giryes",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we introduce Point2Mesh, a technique for reconstructing a\nsurface mesh from an input point cloud. Instead of explicitly specifying a\nprior that encodes the expected shape properties, the prior is defined\nautomatically using the input point cloud, which we refer to as a self-prior.\nThe self-prior encapsulates reoccurring geometric repetitions from a single\nshape within the weights of a deep neural network. We optimize the network\nweights to deform an initial mesh to shrink-wrap a single input point cloud.\nThis explicitly considers the entire reconstructed shape, since shared local\nkernels are calculated to fit the overall object. The convolutional kernels are\noptimized globally across the entire shape, which inherently encourages\nlocal-scale geometric self-similarity across the shape surface. We show that\nshrink-wrapping a point cloud with a self-prior converges to a desirable\nsolution; compared to a prescribed smoothness prior, which often becomes\ntrapped in undesirable local minima. While the performance of traditional\nreconstruction approaches degrades in non-ideal conditions that are often\npresent in real world scanning, i.e., unoriented normals, noise and missing\n(low density) parts, Point2Mesh is robust to non-ideal conditions. We\ndemonstrate the performance of Point2Mesh on a large variety of shapes with\nvarying complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.11084v1"
    },
    {
        "title": "A Deep Learning based Fast Signed Distance Map Generation",
        "authors": [
            "Zihao Wang",
            "Clair Vandersteen",
            "Thomas Demarcy",
            "Dan Gnansia",
            "Charles Raffaelli",
            "Nicolas Guevara",
            "Hervé Delingette"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Signed distance map (SDM) is a common representation of surfaces in medical\nimage analysis and machine learning. The computational complexity of SDM for 3D\nparametric shapes is often a bottleneck in many applications, thus limiting\ntheir interest. In this paper, we propose a learning based SDM generation\nneural network which is demonstrated on a tridimensional cochlea shape model\nparameterized by 4 shape parameters. The proposed SDM Neural Network generates\na cochlea signed distance map depending on four input parameters and we show\nthat the deep learning approach leads to a 60 fold improvement in the time of\ncomputation compared to more classical SDM generation methods. Therefore, the\nproposed approach achieves a good trade-off between accuracy and efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.12662v1"
    },
    {
        "title": "Sparse Modeling of Intrinsic Correspondences",
        "authors": [
            "J. Pokrass",
            "A. M. Bronstein",
            "M. M. Bronstein",
            "P. Sprechmann",
            "G. Sapiro"
        ],
        "category": "cs.GR",
        "published_year": "2012",
        "summary": "  We present a novel sparse modeling approach to non-rigid shape matching using\nonly the ability to detect repeatable regions. As the input to our algorithm,\nwe are given only two sets of regions in two shapes; no descriptors are\nprovided so the correspondence between the regions is not know, nor we know how\nmany regions correspond in the two shapes. We show that even with such scarce\ninformation, it is possible to establish very accurate correspondence between\nthe shapes by using methods from the field of sparse modeling, being this, the\nfirst non-trivial use of sparse models in shape correspondence. We formulate\nthe problem of permuted sparse coding, in which we solve simultaneously for an\nunknown permutation ordering the regions on two shapes and for an unknown\ncorrespondence in functional representation. We also propose a robust variant\ncapable of handling incomplete matches. Numerically, the problem is solved\nefficiently by alternating the solution of a linear assignment and a sparse\ncoding problem. The proposed methods are evaluated qualitatively and\nquantitatively on standard benchmarks containing both synthetic and scanned\nobjects.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.6560v1"
    },
    {
        "title": "Persistence Atlas for Critical Point Variability in Ensembles",
        "authors": [
            "Guillaume Favelier",
            "Noura Faraj",
            "Brian Summa",
            "Julien Tierny"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This paper presents a new approach for the visualization and analysis of the\nspatial variability of features of interest represented by critical points in\nensemble data. Our framework, called Persistence Atlas, enables the\nvisualization of the dominant spatial patterns of critical points, along with\nstatistics regarding their occurrence in the ensemble. The persistence atlas\nrepresents in the geometrical domain each dominant pattern in the form of a\nconfidence map for the appearance of critical points. As a by-product, our\nmethod also provides 2-dimensional layouts of the entire ensemble, highlighting\nthe main trends at a global level. Our approach is based on the new notion of\nPersistence Map, a measure of the geometrical density in critical points which\nleverages the robustness to noise of topological persistence to better\nemphasize salient features. We show how to leverage spectral embedding to\nrepresent the ensemble members as points in a low-dimensional Euclidean space,\nwhere distances between points measure the dissimilarities between critical\npoint layouts and where statistical tasks, such as clustering, can be easily\ncarried out. Further, we show how the notion of mandatory critical point can be\nleveraged to evaluate for each cluster confidence regions for the appearance of\ncritical points. Most of the steps of this framework can be trivially\nparallelized and we show how to efficiently implement them. Extensive\nexperiments demonstrate the relevance of our approach. The accuracy of the\nconfidence regions provided by the persistence atlas is quantitatively\nevaluated and compared to a baseline strategy using an off-the-shelf clustering\napproach. We illustrate the importance of the persistence atlas in a variety of\nreal-life datasets, where clear trends in feature layouts are identified and\nanalyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.11212v1"
    },
    {
        "title": "Constrained Generative Adversarial Networks for Interactive Image\n  Generation",
        "authors": [
            "Eric Heim"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Generative Adversarial Networks (GANs) have received a great deal of\nattention due in part to recent success in generating original, high-quality\nsamples from visual domains. However, most current methods only allow for users\nto guide this image generation process through limited interactions. In this\nwork we develop a novel GAN framework that allows humans to be \"in-the-loop\" of\nthe image generation process. Our technique iteratively accepts relative\nconstraints of the form \"Generate an image more like image A than image B\".\nAfter each constraint is given, the user is presented with new outputs from the\nGAN, informing the next round of feedback. This feedback is used to constrain\nthe output of the GAN with respect to an underlying semantic space that can be\ndesigned to model a variety of different notions of similarity (e.g. classes,\nattributes, object relationships, color, etc.). In our experiments, we show\nthat our GAN framework is able to generate images that are of comparable\nquality to equivalent unsupervised GANs while satisfying a large number of the\nconstraints provided by users, effectively changing a GAN into one that allows\nusers interactive control over image generation without sacrificing image\nquality.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.02526v1"
    },
    {
        "title": "Teaching GANs to Sketch in Vector Format",
        "authors": [
            "Varshaneya V",
            "S Balasubramanian",
            "Vineeth N Balasubramanian"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Sketching is more fundamental to human cognition than speech. Deep Neural\nNetworks (DNNs) have achieved the state-of-the-art in speech-related tasks but\nhave not made significant development in generating stroke-based sketches a.k.a\nsketches in vector format. Though there are Variational Auto Encoders (VAEs)\nfor generating sketches in vector format, there is no Generative Adversarial\nNetwork (GAN) architecture for the same. In this paper, we propose a standalone\nGAN architecture SkeGAN and a VAE-GAN architecture VASkeGAN, for sketch\ngeneration in vector format. SkeGAN is a stochastic policy in Reinforcement\nLearning (RL), capable of generating both multidimensional continuous and\ndiscrete outputs. VASkeGAN hybridizes a VAE and a GAN, in order to couple the\nefficient representation of data by VAE with the powerful generating\ncapabilities of a GAN, to produce visually appealing sketches. We also propose\na new metric called the Ske-score which quantifies the quality of vector\nsketches. We have validated that SkeGAN and VASkeGAN generate visually\nappealing sketches by using Human Turing Test and Ske-score.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.03620v1"
    },
    {
        "title": "Multi-modal 3D Shape Reconstruction Under Calibration Uncertainty using\n  Parametric Level Set Methods",
        "authors": [
            "Moshe Eliasof",
            "Andrei Sharf",
            "Eran Treister"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We consider the problem of 3D shape reconstruction from multi-modal data,\ngiven uncertain calibration parameters. Typically, 3D data modalities can be in\ndiverse forms such as sparse point sets, volumetric slices, 2D photos and so\non. To jointly process these data modalities, we exploit a parametric level set\nmethod that utilizes ellipsoidal radial basis functions. This method not only\nallows us to analytically and compactly represent the object, it also confers\non us the ability to overcome calibration related noise that originates from\ninaccurate acquisition parameters. This essentially implicit regularization\nleads to a highly robust and scalable reconstruction, surpassing other\ntraditional methods. In our results we first demonstrate the ability of the\nmethod to compactly represent complex objects. We then show that our\nreconstruction method is robust both to a small number of measurements and to\nnoise in the acquisition parameters. Finally, we demonstrate our reconstruction\nabilities from diverse modalities such as volume slices obtained from liquid\ndisplacement (similar to CTscans and XRays), and visual measurements obtained\nfrom shape silhouettes.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.10379v2"
    },
    {
        "title": "TileGAN: Synthesis of Large-Scale Non-Homogeneous Textures",
        "authors": [
            "Anna Frühstück",
            "Ibraheem Alhashim",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We tackle the problem of texture synthesis in the setting where many input\nimages are given and a large-scale output is required. We build on recent\ngenerative adversarial networks and propose two extensions in this paper.\nFirst, we propose an algorithm to combine outputs of GANs trained on a smaller\nresolution to produce a large-scale plausible texture map with virtually no\nboundary artifacts. Second, we propose a user interface to enable artistic\ncontrol. Our quantitative and qualitative results showcase the generation of\nsynthesized high-resolution maps consisting of up to hundreds of megapixels as\na case in point.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.12795v1"
    },
    {
        "title": "Synthesis of Biologically Realistic Human Motion Using Joint Torque\n  Actuation",
        "authors": [
            "Yifeng Jiang",
            "Tom Van Wouwe",
            "Friedl De Groote",
            "C. Karen Liu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Using joint actuators to drive the skeletal movements is a common practice in\ncharacter animation, but the resultant torque patterns are often unnatural or\ninfeasible for real humans to achieve. On the other hand, physiologically-based\nmodels explicitly simulate muscles and tendons and thus produce more human-like\nmovements and torque patterns. This paper introduces a technique to transform\nan optimal control problem formulated in the muscle-actuation space to an\nequivalent problem in the joint-actuation space, such that the solutions to\nboth problems have the same optimal value. By solving the equivalent problem in\nthe joint-actuation space, we can generate human-like motions comparable to\nthose generated by musculotendon models, while retaining the benefit of simple\nmodeling and fast computation offered by joint-actuation models. Our method\ntransforms constant bounds on muscle activations to nonlinear, state-dependent\ntorque limits in the joint-actuation space. In addition, the metabolic energy\nfunction on muscle activations is transformed to a nonlinear function of joint\ntorques, joint configuration and joint velocity. Our technique can also benefit\npolicy optimization using deep reinforcement learning approach, by providing a\nmore anatomically realistic action space for the agent to explore during the\nlearning process. We take the advantage of the physiologically-based simulator,\nOpenSim, to provide training data for learning the torque limits and the\nmetabolic energy function. Once trained, the same torque limits and the energy\nfunction can be applied to drastically different motor tasks formulated as\neither trajectory optimization or policy learning. Codebase:\nhttps://github.com/jyf588/lrle and https://github.com/jyf588/lrle-rl-examples\n",
        "pdf_link": "http://arxiv.org/pdf/1904.13041v2"
    },
    {
        "title": "StructureNet: Hierarchical Graph Networks for 3D Shape Generation",
        "authors": [
            "Kaichun Mo",
            "Paul Guerrero",
            "Li Yi",
            "Hao Su",
            "Peter Wonka",
            "Niloy Mitra",
            "Leonidas J. Guibas"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The ability to generate novel, diverse, and realistic 3D shapes along with\nassociated part semantics and structure is central to many applications\nrequiring high-quality 3D assets or large volumes of realistic training data. A\nkey challenge towards this goal is how to accommodate diverse shape variations,\nincluding both continuous deformations of parts as well as structural or\ndiscrete alterations which add to, remove from, or modify the shape\nconstituents and compositional structure. Such object structure can typically\nbe organized into a hierarchy of constituent object parts and relationships,\nrepresented as a hierarchy of n-ary graphs. We introduce StructureNet, a\nhierarchical graph network which (i) can directly encode shapes represented as\nsuch n-ary graphs; (ii) can be robustly trained on large and complex shape\nfamilies; and (iii) can be used to generate a great diversity of realistic\nstructured shape geometries. Technically, we accomplish this by drawing\ninspiration from recent advances in graph neural networks to propose an\norder-invariant encoding of n-ary graphs, considering jointly both part\ngeometry and inter-part relations during network training. We extensively\nevaluate the quality of the learned latent spaces for various shape families\nand show significant advantages over baseline and competing methods. The\nlearned latent spaces enable several structure-aware geometry processing\napplications, including shape generation and interpolation, shape editing, or\nshape structure discovery directly from un-annotated images, point clouds, or\npartial scans.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.00575v1"
    },
    {
        "title": "Mesh Variational Autoencoders with Edge Contraction Pooling",
        "authors": [
            "Yu-Jie Yuan",
            "Yu-Kun Lai",
            "Jie Yang",
            "Hongbo Fu",
            "Lin Gao"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  3D shape analysis is an important research topic in computer vision and\ngraphics. While existing methods have generalized image-based deep learning to\nmeshes using graph-based convolutions, the lack of an effective pooling\noperation restricts the learning capability of their networks. In this paper,\nwe propose a novel pooling operation for mesh datasets with the same\nconnectivity but different geometry, by building a mesh hierarchy using mesh\nsimplification. For this purpose, we develop a modified mesh simplification\nmethod to avoid generating highly irregularly sized triangles. Our pooling\noperation effectively encodes the correspondence between coarser and finer\nmeshes in the hierarchy. We then present a variational auto-encoder structure\nwith the edge contraction pooling and graph-based convolutions, to explore\nprobability latent spaces of 3D surfaces. Our network requires far fewer\nparameters than the original mesh VAE and thus can handle denser models thanks\nto our new pooling operation and convolutional kernels. Our evaluation also\nshows that our method has better generalization ability and is more reliable in\nvarious applications, including shape generation, shape interpolation and shape\nembedding.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02507v1"
    },
    {
        "title": "Channel Decomposition into Painting Actions",
        "authors": [
            "Shih-Chieh Su"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  This work presents a method to decompose a convolutional layer of the deep\nneural network into painting actions. To behave like the human painter, these\nactions are driven by the cost simulating the hand movement, the paint color\nchange, the stroke shape and the stroking style. To help planning, the Mask\nR-CNN is applied to detect the object areas and decide the painting order. The\nproposed painting system introduces a variety of extensions in artistic styles,\nbased on the chosen parameters. Further experiments are performed to evaluate\nthe channel penetration and the channel sensitivity on the strokes.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.04694v4"
    },
    {
        "title": "Accelerating ADMM for Efficient Simulation and Optimization",
        "authors": [
            "Juyong Zhang",
            "Yue Peng",
            "Wenqing Ouyang",
            "Bailin Deng"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The alternating direction method of multipliers (ADMM) is a popular approach\nfor solving optimization problems that are potentially non-smooth and with hard\nconstraints. It has been applied to various computer graphics applications,\nincluding physical simulation, geometry processing, and image processing.\nHowever, ADMM can take a long time to converge to a solution of high accuracy.\nMoreover, many computer graphics tasks involve non-convex optimization, and\nthere is often no convergence guarantee for ADMM on such problems since it was\noriginally designed for convex optimization. In this paper, we propose a method\nto speed up ADMM using Anderson acceleration, an established technique for\naccelerating fixed-point iterations. We show that in the general case, ADMM is\na fixed-point iteration of the second primal variable and the dual variable,\nand Anderson acceleration can be directly applied. Additionally, when the\nproblem has a separable target function and satisfies certain conditions, ADMM\nbecomes a fixed-point iteration of only one variable, which further reduces the\ncomputational overhead of Anderson acceleration. Moreover, we analyze a\nparticular non-convex problem structure that is common in computer graphics,\nand prove the convergence of ADMM on such problems under mild assumptions. We\napply our acceleration technique on a variety of optimization problems in\ncomputer graphics, with notable improvement on their convergence speed.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.00470v1"
    },
    {
        "title": "Topologically-Guided Color Image Enhancement",
        "authors": [
            "Junyi Tu",
            "Paul Rosen"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Enhancement is an important step in post-processing digital images for\npersonal use, in medical imaging, and for object recognition. Most existing\nmanual techniques rely on region selection, similarity, and/or thresholding for\nediting, never really considering the topological structure of the image. In\nthis paper, we leverage the contour tree to extract a hierarchical\nrepresentation of the topology of an image. We propose 4 topology-aware\ntransfer functions for editing features of the image using local topological\nproperties, instead of global image properties. Finally, we evaluate our\napproach with grayscale and color images.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.01456v1"
    },
    {
        "title": "Realtime Simulation of Thin-Shell Deformable Materials using CNN-Based\n  Mesh Embedding",
        "authors": [
            "Qingyang Tan",
            "Zherong Pan",
            "Lin Gao",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We address the problem of accelerating thin-shell deformable object\nsimulations by dimension reduction. We present a new algorithm to embed a\nhigh-dimensional configuration space of deformable objects in a low-dimensional\nfeature space, where the configurations of objects and feature points have\napproximate one-to-one mapping. Our key technique is a graph-based\nconvolutional neural network (CNN) defined on meshes with arbitrary topologies\nand a new mesh embedding approach based on physics-inspired loss term. We have\napplied our approach to accelerate high-resolution thin shell simulations\ncorresponding to cloth-like materials, where the configuration space has tens\nof thousands of degrees of freedom. We show that our physics-inspired embedding\napproach leads to higher accuracy compared with prior mesh embedding methods.\nFinally, we show that the temporal evolution of the mesh in the feature space\ncan also be learned using a recurrent neural network (RNN) leading to fully\nlearnable physics simulators. After training our learned simulator runs\n$500-10000\\times$ faster and the accuracy is high enough for robot manipulation\ntasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.12354v4"
    },
    {
        "title": "Feature Extraction in Augmented Reality",
        "authors": [
            "Jekishan K. Parmar",
            "Ankit Desai"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Augmented Reality (AR) is used for various applications associated with the\nreal world. In this paper, first, describe characteristics and essential\nservices of AR. Brief history on Virtual Reality (VR) and AR is also mentioned\nin the introductory section. Then, AR Technologies along with its workflow is\ndepicted, which includes the complete AR Process consisting of the stages of\nImage Acquisition, Feature Extraction, Feature Matching, Geometric\nVerification, and Associated Information Retrieval. Feature extraction is the\nessence of AR hence its details are furnished in the paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.09177v1"
    },
    {
        "title": "Vectorizing Quantum Turbulence Vortex-Core Lines for Real-Time\n  Visualization",
        "authors": [
            "Daoming Liu",
            "Chi Xiong",
            "Xiaopei Liu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Vectorizing vortex-core lines is crucial for high-quality visualization and\nanalysis of turbulence. While several techniques exist in the literature, they\ncan only be applied to classical fluids. Recently, quantum fluids with\nturbulence get more and more attention in physics. It is thus desirable that\nvortex-core lines can also be well extracted and visualized for quantum fluids.\nIn this paper, we aim for this goal and developed an efficient vortex-core line\nvectorization method for quantum fluids, which enables real-time visualization\nof high-resolution quantum turbulence structure. Given the datasets by\nsimulation, our technique is developed from the vortices identified by the\ncirculation-based method. To vectorize the vortex-core lines enclosed by those\nvortices, we propose a novel graph-based data structure, with iterative graph\nreduction and density-guided local optimization, to locate more precisely\nsub-grid-scale vortex-core line samples, which are then vectorized by\ncontinuous curves. This not only represents vortex-core line structures\ncontinuously, but also naturally preserves complex topology, such as branching\nduring reconnection. By vectorization, the memory consumption can be largely\nreduced by orders of magnitude, enabling real-time rendering performance.\nDifferent types of interactive visualizations are demonstrated to show the\neffectiveness of our technique, which could assist further research on quantum\nturbulence.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.12070v1"
    },
    {
        "title": "Geometry-Driven Detection, Tracking and Visual Analysis of Viscous and\n  Gravitational Fingers",
        "authors": [
            "Jiayi Xu",
            "Soumya Dutta",
            "Wenbin He",
            "Joachim Moortgat",
            "Han-Wei Shen"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Viscous and gravitational flow instabilities cause a displacement front to\nbreak up into finger-like fluids. The detection and evolutionary analysis of\nthese fingering instabilities are critical in multiple scientific disciplines\nsuch as fluid mechanics and hydrogeology. However, previous detection methods\nof the viscous and gravitational fingers are based on density thresholding,\nwhich provides limited geometric information of the fingers. The geometric\nstructures of fingers and their evolution are important yet little studied in\nthe literature. In this work, we explore the geometric detection and evolution\nof the fingers in detail to elucidate the dynamics of the instability. We\npropose a ridge voxel detection method to guide the extraction of finger cores\nfrom three-dimensional (3D) scalar fields. After skeletonizing finger cores\ninto skeletons, we design a spanning tree based approach to capture how fingers\nbranch spatially from the finger skeletons. Finally, we devise a novel\ngeometric-glyph augmented tracking graph to study how the fingers and their\nbranches grow, merge, and split over time. Feedback from earth scientists\ndemonstrates the usefulness of our approach to performing spatio-temporal\ngeometric analyses of fingers.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.12383v4"
    },
    {
        "title": "Computing Light Transport Gradients using the Adjoint Method",
        "authors": [
            "Jos Stam"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  This paper proposes a new equation from continuous adjoint theory to compute\nthe gradient of quantities governed by the Transport Theory of light. Unlike\ndiscrete gradients ala autograd, which work at the code level, we first\nformulate the continuous theory and then discretize it. The key insight of this\npaper is that computing gradients in Transport Theory is akin to computing the\nimportance, a quantity adjoint to radiance that satisfies an adjoint equation.\nImportance tells us where to look for light that matters. This is one of the\nkey insights of this paper. In fact, this mathematical journey started from a\nwhimsical thought that these adjoints might be related. Computing gradients is\ntherefore no more complicated than computing the importance field. This insight\nand the following paper hopefully will shed some light on this complicated\nproblem and ease the implementations of gradient computations in existing path\ntracers.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.15059v1"
    },
    {
        "title": "Farthest sampling segmentation of triangulated surfaces",
        "authors": [
            "Victoria Hernández-Mederos",
            "Dimas Martínez",
            "Jorge Estrada-Sarlabous",
            "Valia Guerra-Ones"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper we introduce Farthest Sampling Segmentation (FSS), a new method\nfor segmentation of triangulated surfaces, which consists of two fundamental\nsteps: the computation of a submatrix $W^k$ of the affinity matrix $W$ and the\napplication of the k-means clustering algorithm to the rows of $W^k$. The\nsubmatrix $W^k$ is obtained computing the affinity between all triangles and\nonly a few special triangles: those which are farthest in the defined metric.\nThis is equivalent to select a sample of columns of $W$ without constructing it\ncompletely. The proposed method is computationally cheaper than other\nsegmentation algorithms, since it only calculates few columns of $W$ and it\ndoes not require the eigendecomposition of $W$ or of any submatrix of $W$.\n  We prove that the orthogonal projection of $W$ on the space generated by the\ncolumns of $W^k$ coincides with the orthogonal projection of $W$ on the space\ngenerated by the $k$ eigenvectors computed by Nystr\\\"om's method using the\ncolumns of $W^k$ as a sample of $W$. Further, it is shown that for increasing\nsize $k$, the proximity relationship among the rows of $W^k$ tends to\nfaithfully reflect the proximity among the corresponding rows of $W$.\n  The FSS method does not depend on parameters that must be tuned by hand and\nit is very flexible, since it can handle any metric to define the distance\nbetween triangles. Numerical experiments with several metrics and a large\nvariety of 3D triangular meshes show that the segmentations obtained computing\nless than the 10% of columns $W$ are as good as those obtained from clustering\nthe rows of the full matrix $W$.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.00478v1"
    },
    {
        "title": "LookOut! Interactive Camera Gimbal Controller for Filming Long Takes",
        "authors": [
            "Mohamed Sayed",
            "Robert Cinca",
            "Enrico Costanza",
            "Gabriel Brostow"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The job of a camera operator is challenging, and potentially dangerous, when\nfilming long moving camera shots. Broadly, the operator must keep the actors\nin-frame while safely navigating around obstacles, and while fulfilling an\nartistic vision. We propose a unified hardware and software system that\ndistributes some of the camera operator's burden, freeing them up to focus on\nsafety and aesthetics during a take. Our real-time system provides a solo\noperator with end-to-end control, so they can balance on-set responsiveness to\naction vs planned storyboards and framing, while looking where they're going.\nBy default, we film without a field monitor.\n  Our LookOut system is built around a lightweight commodity camera gimbal\nmechanism, with heavy modifications to the controller, which would normally\njust provide active stabilization. Our control algorithm reacts to speech\ncommands, video, and a pre-made script. Specifically, our automatic monitoring\nof the live video feed saves the operator from distractions. In pre-production,\nan artist uses our GUI to design a sequence of high-level camera \"behaviors.\"\nThose can be specific, based on a storyboard, or looser objectives, such as\n\"frame both actors.\" Then during filming, a machine-readable script, exported\nfrom the GUI, ties together with the sensor readings to drive the gimbal. To\nvalidate our algorithm, we compared tracking strategies, interfaces, and\nhardware protocols, and collected impressions from a) film-makers who used all\naspects of our system, and b) film-makers who watched footage filmed using\nLookOut.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.02087v3"
    },
    {
        "title": "Length Learning for Planar Euclidean Curves",
        "authors": [
            "Barak Or",
            "Liam Hazan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  In this work, we used deep neural networks (DNNs) to solve a fundamental\nproblem in differential geometry. One can find many closed-form expressions for\ncalculating curvature, length, and other geometric properties in the\nliterature. As we know these concepts, we are highly motivated to reconstruct\nthem by using deep neural networks. In this framework, our goal is to learn\ngeometric properties from examples. The simplest geometric object is a curve.\nTherefore, this work focuses on learning the length of planar sampled curves\ncreated by a sine waves dataset. For this reason, the fundamental length axioms\nwere reconstructed using a supervised learning approach. Following these axioms\na simplified DNN model, we call ArcLengthNet, was established. The robustness\nto additive noise and discretization errors were tested.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.01895v1"
    },
    {
        "title": "Neural BRDF Representation and Importance Sampling",
        "authors": [
            "Alejandro Sztrajman",
            "Gilles Rainer",
            "Tobias Ritschel",
            "Tim Weyrich"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Controlled capture of real-world material appearance yields tabulated sets of\nhighly realistic reflectance data. In practice, however, its high memory\nfootprint requires compressing into a representation that can be used\nefficiently in rendering while remaining faithful to the original. Previous\nworks in appearance encoding often prioritised one of these requirements at the\nexpense of the other, by either applying high-fidelity array compression\nstrategies not suited for efficient queries during rendering, or by fitting a\ncompact analytic model that lacks expressiveness. We present a compact neural\nnetwork-based representation of BRDF data that combines high-accuracy\nreconstruction with efficient practical rendering via built-in interpolation of\nreflectance. We encode BRDFs as lightweight networks, and propose a training\nscheme with adaptive angular sampling, critical for the accurate reconstruction\nof specular highlights. Additionally, we propose a novel approach to make our\nrepresentation amenable to importance sampling: rather than inverting the\ntrained networks, we learn to encode them in a more compact embedding that can\nbe mapped to parameters of an analytic BRDF for which importance sampling is\nknown. We evaluate encoding results on isotropic and anisotropic BRDFs from\nmultiple real-world datasets, and importance sampling performance for isotropic\nBRDFs mapped to two different analytic models.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.05963v3"
    },
    {
        "title": "Generative Modelling of BRDF Textures from Flash Images",
        "authors": [
            "Philipp Henzler",
            "Valentin Deschaintre",
            "Niloy J. Mitra",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We learn a latent space for easy capture, consistent interpolation, and\nefficient reproduction of visual material appearance. When users provide a\nphoto of a stationary natural material captured under flashlight illumination,\nfirst it is converted into a latent material code. Then, in the second step,\nconditioned on the material code, our method produces an infinite and diverse\nspatial field of BRDF model parameters (diffuse albedo, normals, roughness,\nspecular albedo) that subsequently allows rendering in complex scenes and\nilluminations, matching the appearance of the input photograph. Technically, we\njointly embed all flash images into a latent space using a convolutional\nencoder, and -- conditioned on these latent codes -- convert random spatial\nfields into fields of BRDF parameters using a convolutional neural network\n(CNN). We condition these BRDF parameters to match the visual characteristics\n(statistics and spectra of visual features) of the input under matching light.\nA user study compares our approach favorably to previous work, even those with\naccess to BRDF supervision.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.11861v2"
    },
    {
        "title": "Simulating deformable objects for computer animation: a numerical\n  perspective",
        "authors": [
            "Uri M. Ascher",
            "Egor Larionov",
            "Seung Heon Sheen",
            "Dinesh K. Pai"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We examine a variety of numerical methods that arise when considering\ndynamical systems in the context of physics-based simulations of deformable\nobjects. Such problems arise in various applications, including animation,\nrobotics, control and fabrication. The goals and merits of suitable numerical\nalgorithms for these applications are different from those of typical numerical\nanalysis research in dynamical systems. Here the mathematical model is not\nfixed a priori but must be adjusted as necessary to capture the desired\nbehaviour, with an emphasis on effectively producing lively animations of\nobjects with complex geometries. Results are often judged by how realistic they\nappear to observers (by the \"eye-norm\") as well as by the efficacy of the\nnumerical procedures employed. And yet, we show that with an adjusted view\nnumerical analysis and applied mathematics can contribute significantly to the\ndevelopment of appropriate methods and their analysis in a variety of areas\nincluding finite element methods, stiff and highly oscillatory ODEs, model\nreduction, and constrained optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01891v2"
    },
    {
        "title": "A Computational Design and Evaluation Tool for 3D Structures with Planar\n  Surfaces",
        "authors": [
            "Chang Liu",
            "Wenzhong Yan",
            "Pehuen Moure",
            "Cody Fan",
            "Ankur Mehta"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Three dimensional (3D) structures composed of planar surfaces can be build\nout of accessible materials using easier fabrication technique with shorter\nfabrication time. To better design 3D structures with planar surfaces,\nrealistic models are required to understand and evaluate mechanical behaviors.\nExisting design tools are either effort-consuming (e.g. finite element\nanalysis) or bounded by assumptions (e.g. numerical solutions). In this\nproject, We have built a computational design tool that is (1) capable of\nrapidly and inexpensively evaluating planar surfaces in 3D structures, with\nsufficient computational efficiency and accuracy; (2) applicable to complex\nboundary conditions and loading conditions, both isotropic materials and\northotropic materials; and (3) suitable for rapid accommodation when design\nparameters need to be adjusted. We demonstrate the efficiency and necessity of\nthis design tool by evaluating a glass table as well as a wood bookcase, and\niteratively designing an origami gripper to satisfy performance requirements.\nThis design tool gives non-expert users as well as engineers a simple and\neffective modus operandi in structural design.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.02114v1"
    },
    {
        "title": "Shape2SAS -- a web application to simulate small-angle scattering data\n  and pair distance distributions from user-defined shapes",
        "authors": [
            "Andreas Haahr Larsen",
            "Emre Brookes",
            "Martin Cramer Pedersen",
            "Jacob Judas Kain Kirkensgaard"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Shape2SAS is a web application that allows researchers and students to build\nintuition and understanding of small-angle scattering. It is available at\nhttps://somo.chem.utk.edu/shape2sas. The user defines a model of arbitrary\nshape by combining geometrical subunits, and Shape2SAS then calculates and\ndisplays the scattering intensity, the pair distance distribution as well as a\nvisualization of the user-defined shape. Simulated data with realistic noise\nare also generated. We demonstrate how Shape2SAS can calculate and display the\ndifferent scattering patterns for various geometrical shapes, such as spheres\nand cylinders. We also demonstrate how the effect of structure factors can be\nvisualized. Finally, we show how multi-contrast particles can readily be\ngenerated, and how the calculated scattering may be used to validate and\nvisualize analytical models generated in analysis software for fitting\nsmall-angle scattering data.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04976v1"
    },
    {
        "title": "A Comprehensive Review of Data-Driven Co-Speech Gesture Generation",
        "authors": [
            "Simbarashe Nyatsanga",
            "Taras Kucherenko",
            "Chaitanya Ahuja",
            "Gustav Eje Henter",
            "Michael Neff"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Gestures that accompany speech are an essential part of natural and efficient\nembodied human communication. The automatic generation of such co-speech\ngestures is a long-standing problem in computer animation and is considered an\nenabling technology in film, games, virtual social spaces, and for interaction\nwith social robots. The problem is made challenging by the idiosyncratic and\nnon-periodic nature of human co-speech gesture motion, and by the great\ndiversity of communicative functions that gestures encompass. Gesture\ngeneration has seen surging interest recently, owing to the emergence of more\nand larger datasets of human gesture motion, combined with strides in\ndeep-learning-based generative models, that benefit from the growing\navailability of data. This review article summarizes co-speech gesture\ngeneration research, with a particular focus on deep generative models. First,\nwe articulate the theory describing human gesticulation and how it complements\nspeech. Next, we briefly discuss rule-based and classical statistical gesture\nsynthesis, before delving into deep learning approaches. We employ the choice\nof input modalities as an organizing principle, examining systems that generate\ngestures from audio, text, and non-linguistic input. We also chronicle the\nevolution of the related training data sets in terms of size, diversity, motion\nquality, and collection method. Finally, we identify key research challenges in\ngesture generation, including data availability and quality; producing\nhuman-like motion; grounding the gesture in the co-occurring speech in\ninteraction with other speakers, and in the environment; performing gesture\nevaluation; and integration of gesture synthesis into applications. We\nhighlight recent approaches to tackling the various key challenges, as well as\nthe limitations of these approaches, and point toward areas of future\ndevelopment.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.05339v4"
    },
    {
        "title": "Rig Inversion by Training a Differentiable Rig Function",
        "authors": [
            "Mathieu Marquis Bolduc",
            "Hau Nghiep Phan"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Rig inversion is the problem of creating a method that can find the rig\nparameter vector that best approximates a given input mesh. In this paper we\npropose to solve this problem by first obtaining a differentiable rig function\nby training a multi layer perceptron to approximate the rig function. This\ndifferentiable rig function can then be used to train a deep learning model of\nrig inversion.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.09567v1"
    },
    {
        "title": "Multi-color Holograms Improve Brightness in Holographic Displays",
        "authors": [
            "Koray Kavaklı",
            "Liang Shi",
            "Hakan Ürey",
            "Wojciech Matusik",
            "Kaan Akşit"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Holographic displays generate Three-Dimensional (3D) images by displaying\nsingle-color holograms time-sequentially, each lit by a single-color light\nsource. However, representing each color one by one limits brightness in\nholographic displays. This paper introduces a new driving scheme for realizing\nbrighter images in holographic displays. Unlike the conventional driving\nscheme, our method utilizes three light sources to illuminate each displayed\nhologram simultaneously at various intensity levels. In this way, our method\nreconstructs a multiplanar three-dimensional target scene using consecutive\nmulti-color holograms and persistence of vision. We co-optimize multi-color\nholograms and required intensity levels from each light source using a gradient\ndescent-based optimizer with a combination of application-specific loss terms.\nWe experimentally demonstrate that our method can increase the intensity levels\nin holographic displays up to three times, reaching a broader range and\nunlocking new potentials for perceptual realism in holographic displays.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.09950v3"
    },
    {
        "title": "iGPSe: A Visual Analytic System for Integrative Genomic Based Cancer\n  Patient Stratification",
        "authors": [
            "Hao Ding",
            "Chao Wang",
            "Kun Huang",
            "Raghu Machiraju"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Background: Cancers are highly heterogeneous with different subtypes. These\nsubtypes often possess different genetic variants, present different\npathological phenotypes, and most importantly, show various clinical outcomes\nsuch as varied prognosis and response to treatment and likelihood for\nrecurrence and metastasis. Recently, integrative genomics (or panomics)\napproaches are often adopted with the goal of combining multiple types of omics\ndata to identify integrative biomarkers for stratification of patients into\ngroups with different clinical outcomes. Results: In this paper we present a\nvisual analytic system called Interactive Genomics Patient Stratification\nexplorer (iGPSe) which significantly reduces the computing burden for\nbiomedical researchers in the process of exploring complicated integrative\ngenomics data. Our system integrates unsupervised clustering with graph and\nparallel sets visualization and allows direct comparison of clinical outcomes\nvia survival analysis. Using a breast cancer dataset obtained from the The\nCancer Genome Atlas (TCGA) project, we are able to quickly explore different\ncombinations of gene expression (mRNA) and microRNA features and identify\npotential combined markers for survival prediction. Conclusions: Visualization\nplays an important role in the process of stratifying given population\npatients. Visual tools allowed for the selection of possibly features across\nvarious datasets for the given patient population. We essentially made a case\nfor visualization for a very important problem in translational informatics.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.2107v1"
    },
    {
        "title": "MCA: Multiresolution Correlation Analysis, a graphical tool for\n  subpopulation identification in single-cell gene expression data",
        "authors": [
            "Justin Feigelman",
            "Fabian J. Theis",
            "Carsten Marr"
        ],
        "category": "cs.GR",
        "published_year": "2014",
        "summary": "  Background: Biological data often originate from samples containing mixtures\nof subpopulations, corresponding e.g. to distinct cellular phenotypes. However,\nidentification of distinct subpopulations may be difficult if biological\nmeasurements yield distributions that are not easily separable. Results: We\npresent Multiresolution Correlation Analysis (MCA), a method for visually\nidentifying subpopulations based on the local pairwise correlation between\ncovariates, without needing to define an a priori interaction scale. We\ndemonstrate that MCA facilitates the identification of differentially regulated\nsubpopulations in simulated data from a small gene regulatory network, followed\nby application to previously published single-cell qPCR data from mouse\nembryonic stem cells. We show that MCA recovers previously identified\nsubpopulations, provides additional insight into the underlying correlation\nstructure, reveals potentially spurious compartmentalizations, and provides\ninsight into novel subpopulations. Conclusions: MCA is a useful method for the\nidentification of subpopulations in low-dimensional expression data, as\nemerging from qPCR or FACS measurements. With MCA it is possible to investigate\nthe robustness of covariate correlations with respect subpopulations,\ngraphically identify outliers, and identify factors contributing to\ndifferential regulation between pairs of covariates. MCA thus provides a\nframework for investigation of expression correlations for genes of interests\nand biological hypothesis generation.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.2112v1"
    },
    {
        "title": "GPU Programming - Speeding Up the 3D Surface Generator VESTA",
        "authors": [
            "B. R. Schlei"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  The novel \"Volume-Enclosing Surface exTraction Algorithm\" (VESTA) generates\ntriangular isosurfaces from computed tomography volumetric images and/or\nthree-dimensional (3D) simulation data. Here, we present various benchmarks for\nGPU-based code implementations of both VESTA and the current state-of-the-art\nMarching Cubes Algorithm (MCA). One major result of this study is that VESTA\nruns significantly faster than the MCA.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.06364v1"
    },
    {
        "title": "Efficient Feature-based Image Registration by Mapping Sparsified\n  Surfaces",
        "authors": [
            "Chun Pang Yung",
            "Gary P. T. Choi",
            "Ke Chen",
            "Lok Ming Lui"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  With the advancement in the digital camera technology, the use of high\nresolution images and videos has been widespread in the modern society. In\nparticular, image and video frame registration is frequently applied in\ncomputer graphics and film production. However, conventional registration\napproaches usually require long computational time for high resolution images\nand video frames. This hinders the application of the registration approaches\nin the modern industries. In this work, we first propose a new image\nrepresentation method to accelerate the registration process by triangulating\nthe images effectively. For each high resolution image or video frame, we\ncompute an optimal coarse triangulation which captures the important features\nof the image. Then, we apply a surface registration algorithm to obtain a\nregistration map which is used to compute the registration of the high\nresolution image. Experimental results suggest that our overall algorithm is\nefficient and capable to achieve a high compression rate while the accuracy of\nthe registration is well retained when compared with the conventional\ngrid-based approach. Also, the computational time of the registration is\nsignificantly reduced using our triangulation-based approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.06215v2"
    },
    {
        "title": "Differential and integral invariants under Mobius transformation",
        "authors": [
            "He Zhang",
            "Hanlin Mo",
            "You Hao",
            "Qi Li",
            "Hua Li"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  One of the most challenging problems in the domain of 2-D image or 3-D shape\nis to handle the non-rigid deformation. From the perspective of transformation\ngroups, the conformal transformation is a key part of the diffeomorphism.\nAccording to the Liouville Theorem, an important part of the conformal\ntransformation is the Mobius transformation, so we focus on Mobius\ntransformation and propose two differential expressions that are invariable\nunder 2-D and 3-D Mobius transformation respectively. Next, we analyze the\nabsoluteness and relativity of invariance on them and their components. After\nthat, we propose integral invariants under Mobius transformation based on the\ntwo differential expressions. Finally, we propose a conjecture about the\nstructure of differential invariants under conformal transformation according\nto our observation on the composition of the above two differential invariants.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.10083v1"
    },
    {
        "title": "Chest X-ray Inpainting with Deep Generative Models",
        "authors": [
            "Ecem Sogancioglu",
            "Shi Hu",
            "Davide Belli",
            "Bram van Ginneken"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Generative adversarial networks have been successfully applied to inpainting\nin natural images. However, the current state-of-the-art models have not yet\nbeen widely adopted in the medical imaging domain. In this paper, we\ninvestigate the performance of three recently published deep learning based\ninpainting models: context encoders, semantic image inpainting, and the\ncontextual attention model, applied to chest x-rays, as the chest exam is the\nmost commonly performed radiological procedure. We train these generative\nmodels on 1.2M 128 $\\times$ 128 patches from 60K healthy x-rays, and learn to\npredict the center 64 $\\times$ 64 region in each patch. We test the models on\nboth the healthy and abnormal radiographs. We evaluate the results by visual\ninspection and comparing the PSNR scores. The outputs of the models are in most\ncases highly realistic. We show that the methods have potential to enhance and\ndetect abnormalities. In addition, we perform a 2AFC observer study and show\nthat an experienced human observer performs poorly in detecting inpainted\nregions, particularly those generated by the contextual attention model.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.01471v1"
    },
    {
        "title": "Visualization of High-dimensional Scalar Functions Using Principal\n  Parameterizations",
        "authors": [
            "Rafael Ballester-Ripoll",
            "Renato Pajarola"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Insightful visualization of multidimensional scalar fields, in particular\nparameter spaces, is key to many fields in computational science and\nengineering. We propose a principal component-based approach to visualize such\nfields that accurately reflects their sensitivity to input parameters. The\nmethod performs dimensionality reduction on the vast $L^2$ Hilbert space formed\nby all possible partial functions (i.e., those defined by fixing one or more\ninput parameters to specific values), which are projected to low-dimensional\nparameterized manifolds such as 3D curves, surfaces, and ensembles thereof. Our\nmapping provides a direct geometrical and visual interpretation in terms of\nSobol's celebrated method for variance-based sensitivity analysis. We\nfurthermore contribute a practical realization of the proposed method by means\nof tensor decomposition, which enables accurate yet interactive integration and\nmultilinear principal component analysis of high-dimensional models.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.03618v1"
    },
    {
        "title": "Learning to Group and Label Fine-Grained Shape Components",
        "authors": [
            "Xiaogang Wang",
            "Bin Zhou",
            "Haiyue Fang",
            "Xiaowu Chen",
            "Qinping Zhao",
            "Kai Xu"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  A majority of stock 3D models in modern shape repositories are assembled with\nmany fine-grained components. The main cause of such data form is the\ncomponent-wise modeling process widely practiced by human modelers. These\nmodeling components thus inherently reflect some function-based shape\ndecomposition the artist had in mind during modeling. On the other hand,\nmodeling components represent an over-segmentation since a functional part is\nusually modeled as a multi-component assembly. Based on these observations, we\nadvocate that labeled segmentation of stock 3D models should not overlook the\nmodeling components and propose a learning solution to grouping and labeling of\nthe fine-grained components. However, directly characterizing the shape of\nindividual components for the purpose of labeling is unreliable, since they can\nbe arbitrarily tiny and semantically meaningless. We propose to generate part\nhypotheses from the components based on a hierarchical grouping strategy, and\nperform labeling on those part groups instead of directly on the components.\nPart hypotheses are mid-level elements which are more probable to carry\nsemantic information. A multiscale 3D convolutional neural network is trained\nto extract context-aware features for the hypotheses. To accomplish a labeled\nsegmentation of the whole shape, we formulate higher-order conditional random\nfields (CRFs) to infer an optimal label assignment for all components.\nExtensive experiments demonstrate that our method achieves significantly robust\nlabeling results on raw 3D models from public shape repositories. Our work also\ncontributes the first benchmark for component-wise labeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.05050v1"
    },
    {
        "title": "SCORES: Shape Composition with Recursive Substructure Priors",
        "authors": [
            "Chenyang Zhu",
            "Kai Xu",
            "Siddhartha Chaudhuri",
            "Renjiao Yi",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  We introduce SCORES, a recursive neural network for shape composition. Our\nnetwork takes as input sets of parts from two or more source 3D shapes and a\nrough initial placement of the parts. It outputs an optimized part structure\nfor the composed shape, leading to high-quality geometry construction. A unique\nfeature of our composition network is that it is not merely learning how to\nconnect parts. Our goal is to produce a coherent and plausible 3D shape,\ndespite large incompatibilities among the input parts. The network may\nsignificantly alter the geometry and structure of the input parts and\nsynthesize a novel shape structure based on the inputs, while adding or\nremoving parts to minimize a structure plausibility loss. We design SCORES as a\nrecursive autoencoder network. During encoding, the input parts are recursively\ngrouped to generate a root code. During synthesis, the root code is decoded,\nrecursively, to produce a new, coherent part assembly. Assembled shape\nstructures may be novel, with little global resemblance to training exemplars,\nyet have plausible substructures. SCORES therefore learns a hierarchical\nsubstructure shape prior based on per-node losses. It is trained on structured\nshapes from ShapeNet, and is applied iteratively to reduce the plausibility\nloss.We showresults of shape composition from multiple sources over different\ncategories of man-made shapes and compare with state-of-the-art alternatives,\ndemonstrating that our network can significantly expand the range of composable\nshapes for assembly-based modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.05398v1"
    },
    {
        "title": "An inverse scattering approach for geometric body generation: a machine\n  learning perspective",
        "authors": [
            "Jinhong Li",
            "Hongyu Liu",
            "Wing-Yan Tsui",
            "Xianchao Wang"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In this paper, we are concerned with the 2D and 3D geometric shape generation\nby prescribing a set of characteristic values of a specific geometric body. One\nof the major motivations of our study is the 3D human body generation in\nvarious applications. We develop a novel method that can generate the desired\nbody with customized characteristic values. The proposed method follows a\nmachine-learning flavour that generates the inferred geometric body with the\ninput characteristic parameters from a training dataset. One of the critical\ningredients and novelties of our method is the borrowing of inverse scattering\ntechniques in the theory of wave propagation to the body generation. This is\ndone by establishing a delicate one-to-one correspondence between a geometric\nbody and the far-field pattern of a source scattering problem governed by the\nHelmholtz system. It in turn enables us to establish a one-to-one\ncorrespondence between the geometric body space and the function space defined\nby the far-field patterns. Hence, the far-field patterns can act as the shape\ngenerators. The shape generation with prescribed characteristic parameters is\nachieved by first manipulating the shape generators and then reconstructing the\ncorresponding geometric body from the obtained shape generator by a stable\nmultiple-frequency Fourier method. Our method is easy to implement and produces\nmore efficient and stable body generations. We provide both theoretical\nanalysis and extensive numerical experiments for the proposed method. The study\nis the first attempt to introduce inverse scattering approaches in combination\nwith machine learning to the geometric body generation and it opens up many\nopportunities for further developments.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.11003v1"
    },
    {
        "title": "An Infinite Parade of Giraffes: Expressive Augmentation and Complexity\n  Layers for Cartoon Drawing",
        "authors": [
            "K. G. Greene"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  In this paper, we explore creative image generation constrained by small\ndata. To partially automate the creation of cartoon sketches consistent with a\nspecific designer's style, where acquiring a very large original image set is\nimpossible or cost prohibitive, we exploit domain specific knowledge for a huge\nreduction in original image requirements, creating an effectively infinite\nnumber of cartoon giraffes from just nine original drawings. We introduce\n\"expressive augmentations\" for cartoon sketches, mathematical transformations\nthat create broad domain appropriate variation, far beyond the usual affine\ntransformations, and we show that chained GANs models trained on the temporal\nstages of drawing or \"complexity layers\" can effectively add character\nappropriate details and finish new drawings in the designer's style.\n  We discuss the application of these tools in design processes for textiles,\ngraphics, architectural elements and interior design.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.07023v1"
    },
    {
        "title": "CompoNet: Learning to Generate the Unseen by Part Synthesis and\n  Composition",
        "authors": [
            "Nadav Schor",
            "Oren Katzir",
            "Hao Zhang",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Data-driven generative modeling has made remarkable progress by leveraging\nthe power of deep neural networks. A reoccurring challenge is how to enable a\nmodel to generate a rich variety of samples from the entire target\ndistribution, rather than only from a distribution confined to the training\ndata. In other words, we would like the generative model to go beyond the\nobserved samples and learn to generate ``unseen'', yet still plausible, data.\nIn our work, we present CompoNet, a generative neural network for 2D or 3D\nshapes that is based on a part-based prior, where the key idea is for the\nnetwork to synthesize shapes by varying both the shape parts and their\ncompositions. Treating a shape not as an unstructured whole, but as a\n(re-)composable set of deformable parts, adds a combinatorial dimension to the\ngenerative process to enrich the diversity of the output, encouraging the\ngenerator to venture more into the ``unseen''. We show that our part-based\nmodel generates richer variety of plausible shapes compared with baseline\ngenerative models. To this end, we introduce two quantitative metrics to\nevaluate the diversity of a generative model and assess how well the generated\ndata covers both the training data and unseen data from the same target\ndistribution. Code is available at https://github.com/nschor/CompoNet.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.07441v4"
    },
    {
        "title": "Photorealistic Facial Synthesis in the Dimensional Affect Space",
        "authors": [
            "Dimitrios Kollias",
            "Shiyang Cheng",
            "Maja Pantic",
            "Stefanos Zafeiriou"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  This paper presents a novel approach for synthesizing facial affect, which is\nbased on our annotating 600,000 frames of the 4DFAB database in terms of\nvalence and arousal. The input of this approach is a pair of these emotional\nstate descriptors and a neutral 2D image of a person to whom the corresponding\naffect will be synthesized. Given this target pair, a set of 3D facial meshes\nis selected, which is used to build a blendshape model and generate the new\nfacial affect. To synthesize the affect on the 2D neutral image, 3DMM fitting\nis performed and the reconstructed face is deformed to generate the target\nfacial expressions. Last, the new face is rendered into the original image.\nBoth qualitative and quantitative experimental studies illustrate the\ngeneration of realistic images, when the neutral image is sampled from a\nvariety of well known databases, such as the Aff-Wild, AFEW, Multi-PIE,\nAFEW-VA, BU-3DFE, Bosphorus.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.08004v1"
    },
    {
        "title": "High-order curvilinear hybrid mesh generation for CFD simulations",
        "authors": [
            "Julian Marcon",
            "Michael Turner",
            "Joaquim Peiró",
            "David Moxey",
            "Claire R. Pollard",
            "Henry Bucklow",
            "Mark Gammon"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We describe a semi-structured method for the generation of high-order hybrid\nmeshes suited for the simulation of high Reynolds number flows. This is\nachieved through the use of highly stretched elements in the viscous boundary\nlayers near the wall surfaces. CADfix is used to first repair any possible\ndefects in the CAD geometry and then generate a medial object based\ndecomposition of the domain that wraps the wall boundaries with partitions\nsuitable for the generation of either prismatic or hexahedral elements. The\nlatter is a novel distinctive feature of the method that permits to obtain\nwell-shaped hexahedral meshes at corners or junctions in the boundary layer.\nThe medial object approach allows greater control on the \"thickness\" of the\nboundary-layer mesh than is generally achievable with advancing layer\ntechniques. CADfix subsequently generates a hybrid straight sided mesh of\nprismatic and hexahedral elements in the near-field region modelling the\nboundary layer, and tetrahedral elements in the far-field region covering the\nrest of the domain. The mesh in the near-field region provides a framework that\nfacilitates the generation, via an isoparametric technique, of layers of highly\nstretched elements with a distribution of points in the direction normal to the\nwall tailored to efficiently and accurately capture the flow in the boundary\nlayer. The final step is the generation of a high-order mesh using NekMesh, a\nhigh-order mesh generator within the Nektar++ framework. NekMesh uses the\nCADfix API as a geometry engine that handles all the geometrical queries to the\nCAD geometry required during the high-order mesh generation process. We will\ndescribe in some detail the methodology using a simple geometry, a NACA wing\ntip, for illustrative purposes. Finally, we will present two examples of\napplication to reasonably complex geometries proposed by NASA as CFD validation\ncases.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.00992v1"
    },
    {
        "title": "An Application of Manifold Learning in Global Shape Descriptors",
        "authors": [
            "Fereshteh S. Bashiri",
            "Reihaneh Rostami",
            "Peggy Peissig",
            "Roshan M. D'Souza",
            "Zeyun Yu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  With the rapid expansion of applied 3D computational vision, shape\ndescriptors have become increasingly important for a wide variety of\napplications and objects from molecules to planets. Appropriate shape\ndescriptors are critical for accurate (and efficient) shape retrieval and 3D\nmodel classification. Several spectral-based shape descriptors have been\nintroduced by solving various physical equations over a 3D surface model. In\nthis paper, for the first time, we incorporate a specific group of techniques\nin statistics and machine learning, known as manifold learning, to develop a\nglobal shape descriptor in the computer graphics domain. The proposed\ndescriptor utilizes the Laplacian Eigenmap technique in which the Laplacian\neigenvalue problem is discretized using an exponential weighting scheme. As a\nresult, our descriptor eliminates the limitations tied to the existing spectral\ndescriptors, namely dependency on triangular mesh representation and high\nintra-class quality of 3D models. We also present a straightforward\nnormalization method to obtain a scale-invariant descriptor. The extensive\nexperiments performed in this study show that the present contribution provides\na highly discriminative and robust shape descriptor under the presence of a\nhigh level of noise, random scale variations, and low sampling rate, in\naddition to the known isometric-invariance property of the Laplace-Beltrami\noperator. The proposed method significantly outperforms state-of-the-art\nalgorithms on several non-rigid shape retrieval benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.02508v1"
    },
    {
        "title": "A Fully Bayesian Infinite Generative Model for Dynamic Texture\n  Segmentation",
        "authors": [
            "Sahar Yousefi",
            "M. T. Manzuri Shalmani",
            "Antoni B. Chan"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Generative dynamic texture models (GDTMs) are widely used for dynamic texture\n(DT) segmentation in the video sequences. GDTMs represent DTs as a set of\nlinear dynamical systems (LDSs). A major limitation of these models concerns\nthe automatic selection of a proper number of DTs. Dirichlet process mixture\n(DPM) models which have appeared recently as the cornerstone of the\nnon-parametric Bayesian statistics, is an optimistic candidate toward resolving\nthis issue. Under this motivation to resolve the aforementioned drawback, we\npropose a novel non-parametric fully Bayesian approach for DT segmentation,\nformulated on the basis of a joint DPM and GDTM construction. This interaction\ncauses the algorithm to overcome the problem of automatic segmentation\nproperly. We derive the Variational Bayesian Expectation-Maximization (VBEM)\ninference for the proposed model. Moreover, in the E-step of inference, we\napply Rauch-Tung-Striebel smoother (RTSS) algorithm on Variational Bayesian\nLDSs. Ultimately, experiments on different video sequences are performed.\nExperiment results indicate that the proposed algorithm outperforms the\nprevious methods in efficiency and accuracy noticeably.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.03968v1"
    },
    {
        "title": "Generation High resolution 3D model from natural language by Generative\n  Adversarial Network",
        "authors": [
            "Kentaro Fukamizu",
            "Masaaki Kondo",
            "Ryuichi Sakamoto"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present a method of generating high resolution 3D shapes from natural\nlanguage descriptions. To achieve this goal, we propose two steps that\ngenerating low resolution shapes which roughly reflect texts and generating\nhigh resolution shapes which reflect the detail of texts. In a previous paper,\nthe authors have shown a method of generating low resolution shapes. We improve\nit to generate 3D shapes more faithful to natural language and test the\neffectiveness of the method. To generate high resolution 3D shapes, we use the\nframework of Conditional Wasserstein GAN. We propose two roles of Critic\nseparately, which calculate the Wasserstein distance between two probability\ndistribution, so that we achieve generating high quality shapes or acceleration\nof learning speed of model. To evaluate our approach, we performed quantitive\nevaluation with several numerical metrics for Critic models. Our method is\nfirst to realize the generation of high quality model by propagating text\nembedding information to high resolution task when generating 3D model.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.07165v1"
    },
    {
        "title": "Shadow Accrual Maps: Efficient Accumulation of City-Scale Shadows Over\n  Time",
        "authors": [
            "Fabio Miranda",
            "Harish Doraiswamy",
            "Marcos Lage",
            "Luc Wilson",
            "Mondrian Hsieh",
            "Claudio T. Silva"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Large scale shadows from buildings in a city play an important role in\ndetermining the environmental quality of public spaces. They can be both\nbeneficial, such as for pedestrians during summer, and detrimental, by\nimpacting vegetation and by blocking direct sunlight. Determining the effects\nof shadows requires the accumulation of shadows over time across different\nperiods in a year. In this paper, we propose a simple yet efficient class of\napproach that uses the properties of sun movement to track the changing\nposition of shadows within a fixed time interval. We use this approach to\nextend two commonly used shadowing techniques, shadow maps and ray tracing, and\ndemonstrate the efficiency of our approach. Our technique is used to develop an\ninteractive visual analysis system, Shadow Profiler, targeted at city planners\nand architects that allows them to test the impact of shadows for different\ndevelopment scenarios. We validate the usefulness of this system through case\nstudies set in Manhattan, a dense borough of New York City.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.04435v1"
    },
    {
        "title": "A Generalized Framework for Edge-preserving and Structure-preserving\n  Image Smoothing",
        "authors": [
            "Wei Liu",
            "Pingping Zhang",
            "Yinjie Lei",
            "Xiaolin Huang",
            "Jie Yang",
            "Ian Reid"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, a non-convex\nnon-smooth optimization framework is proposed to achieve diverse smoothing\nnatures where even contradictive smoothing behaviors can be achieved. To this\nend, we first introduce the truncated Huber penalty function which has seldom\nbeen used in image smoothing. A robust framework is then proposed. When\ncombined with the strong flexibility of the truncated Huber penalty function,\nour framework is capable of a range of applications and can outperform the\nstate-of-the-art approaches in several tasks. In addition, an efficient\nnumerical solution is provided and its convergence is theoretically guaranteed\neven the optimization framework is non-convex and non-smooth. The effectiveness\nand superior performance of our approach are validated through comprehensive\nexperimental results in a range of applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.09642v4"
    },
    {
        "title": "Self-Imitation Learning of Locomotion Movements through Termination\n  Curriculum",
        "authors": [
            "Amin Babadi",
            "Kourosh Naderi",
            "Perttu Hämäläinen"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Animation and machine learning research have shown great advancements in the\npast decade, leading to robust and powerful methods for learning complex\nphysically-based animations. However, learning can take hours or days,\nespecially if no reference movement data is available. In this paper, we\npropose and evaluate a novel combination of techniques for accelerating the\nlearning of stable locomotion movements through self-imitation learning of\nsynthetic animations. First, we produce synthetic and cyclic reference movement\nusing a recent online tree search approach that can discover stable walking\ngaits in a few minutes. This allows us to use reinforcement learning with\nReference State Initialization (RSI) to find a neural network controller for\nimitating the synthesized reference motion. We further accelerate the learning\nusing a novel curriculum learning approach called Termination Curriculum (TC),\nthat adapts the episode termination threshold over time. The combination of the\nRSI and TC ensures that simulation budget is not wasted in regions of the state\nspace not visited by the final policy. As a result, our agents can learn\nlocomotion skills in just a few hours on a modest 4-core computer. We\ndemonstrate this by producing locomotion movements for a variety of characters.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11842v2"
    },
    {
        "title": "ScaleTrotter: Illustrative Visual Travels Across Negative Scales",
        "authors": [
            "Sarkis Halladjian",
            "Haichao Miao",
            "David Kouřil",
            "M. Eduard Gröller",
            "Ivan Viola",
            "Tobias Isenberg"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We present ScaleTrotter, a conceptual framework for an interactive,\nmulti-scale visualization of biological mesoscale data and, specifically,\ngenome data. ScaleTrotter allows viewers to smoothly transition from the\nnucleus of a cell to the atomistic composition of the DNA, while bridging\nseveral orders of magnitude in scale. The challenges in creating an interactive\nvisualization of genome data are fundamentally different in several ways from\nthose in other domains like astronomy that require a multi-scale representation\nas well. First, genome data has intertwined scale levels---the DNA is an\nextremely long, connected molecule that manifests itself at all scale levels.\nSecond, elements of the DNA do not disappear as one zooms out---instead the\nscale levels at which they are observed group these elements differently.\nThird, we have detailed information and thus geometry for the entire dataset\nand for all scale levels, posing a challenge for interactive visual\nexploration. Finally, the conceptual scale levels for genome data are close in\nscale space, requiring us to find ways to visually embed a smaller scale into a\ncoarser one. We address these challenges by creating a new multi-scale\nvisualization concept. We use a scale-dependent camera model that controls the\nvisual embedding of the scales into their respective parents, the rendering of\na subset of the scale hierarchy, and the location, size, and scope of the view.\nIn traversing the scales, ScaleTrotter is roaming between 2D and 3D visual\nrepresentations that are depicted in integrated visuals. We discuss,\nspecifically, how this form of multi-scale visualization follows from the\nspecific characteristics of the genome data and describe its implementation.\nFinally, we discuss the implications of our work to the general illustrative\ndepiction of multi-scale data.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12352v1"
    },
    {
        "title": "Visual Entropy and the Visualization of Uncertainty",
        "authors": [
            "Nicolas S. Holliman",
            "Arzu Coltekin",
            "Sara J. Fernstad",
            "Lucy McLaughlin",
            "Michael D. Simpson",
            "Andrew J. Woods"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Background: Even though data visualizations (and underlying data) almost\nalways contain uncertainty, it remains complex to communicate and interpret\nuncertainty representations. Consequently, uncertainty visualizations for\nnon-expert audiences are rare. Objective: our aim is to rigorously define and\nevaluate the novel use of visual entropy as a measure of shape that allows us\nto construct an ordered scale of glyphs for use in representing both\nuncertainty and value in 2D and 3D environments. Method: We use sample entropy\nas a numerical measure of visual entropy to construct a set of glyphs using R\nand Blender which vary in their complexity. Results: an exact binomial analysis\nof a pairwise comparison of the glyphs shows a majority of participants (n =\n87) ordered each glyph as predicted by the visual entropy score with large\neffect size (Cohen's g > 0.25). We also evaluate whether the glyphs effectively\nrepresent uncertainty using a signal detection method in a search task.\nParticipants (n = 15) were able to find glyphs representing uncertainty with\nhigh sensitivity and low error rates. Conclusion: visual entropy is a\nsuccessful novel approach to representing ordered data and provides a channel\nthat can allow the uncertainty of a measure to be presented alongside its mean\nvalue.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12879v2"
    },
    {
        "title": "Dialog on a canvas with a machine",
        "authors": [
            "Vivien Cabannes",
            "Thomas Kerdreux",
            "Louis Thiry",
            "Tina Campana",
            "Charly Ferrandes"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We propose a new form of human-machine interaction. It is a pictorial game\nconsisting of interactive rounds of creation between artists and a machine.\nThey repetitively paint one after the other. At its rounds, the computer\npartially completes the drawing using machine learning algorithms, and projects\nits additions directly on the canvas, which the artists are free to insert or\nmodify. Alongside fostering creativity, the process is designed to question the\ngrowing interaction between humans and machines.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.04386v2"
    },
    {
        "title": "Single Image BRDF Parameter Estimation with a Conditional Adversarial\n  Network",
        "authors": [
            "Mark Boss",
            "Hendrik P. A. Lensch"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Creating plausible surfaces is an essential component in achieving a high\ndegree of realism in rendering. To relieve artists, who create these surfaces\nin a time-consuming, manual process, automated retrieval of the\nspatially-varying Bidirectional Reflectance Distribution Function (SVBRDF) from\na single mobile phone image is desirable. By leveraging a deep neural network,\nthis casual capturing method can be achieved. The trained network can estimate\nper pixel normal, base color, metallic and roughness parameters from the Disney\nBRDF. The input image is taken with a mobile phone lit by the camera flash. The\nnetwork is trained to compensate for environment lighting and thus learned to\nreduce artifacts introduced by other light sources. These losses contain a\nmulti-scale discriminator with an additional perceptual loss, a rendering loss\nusing a differentiable renderer, and a parameter loss. Besides the local\nprecision, this loss formulation generates material texture maps which are\nglobally more consistent. The network is set up as a generator network trained\nin an adversarial fashion to ensure that only plausible maps are produced. The\nestimated parameters not only reproduce the material faithfully in rendering\nbut capture the style of hand-authored materials due to the more global loss\nterms compared to previous works without requiring additional post-processing.\nBoth the resolution and the quality is improved.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.05148v1"
    },
    {
        "title": "Statistical Parameter Selection for Clustering Persistence Diagrams",
        "authors": [
            "Max Kontak",
            "Jules Vidal",
            "Julien Tierny"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In urgent decision making applications, ensemble simulations are an important\nway to determine different outcome scenarios based on currently available data.\nIn this paper, we will analyze the output of ensemble simulations by\nconsidering so-called persistence diagrams, which are reduced representations\nof the original data, motivated by the extraction of topological features.\nBased on a recently published progressive algorithm for the clustering of\npersistence diagrams, we determine the optimal number of clusters, and\ntherefore the number of significantly different outcome scenarios, by the\nminimization of established statistical score functions. Furthermore, we\npresent a proof-of-concept prototype implementation of the statistical\nselection of the number of clusters and provide the results of an experimental\nstudy, where this implementation has been applied to real-world ensemble data\nsets.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08398v1"
    },
    {
        "title": "Normal Estimation for 3D Point Clouds via Local Plane Constraint and\n  Multi-scale Selection",
        "authors": [
            "Jun Zhou",
            "Hua Huang",
            "Bin Liu",
            "Xiuping Liu"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  In this paper, we propose a normal estimation method for unstructured 3D\npoint clouds. In this method, a feature constraint mechanism called Local Plane\nFeatures Constraint (LPFC) is used and then a multi-scale selection strategy is\nintroduced. The LPEC can be used in a single-scale point network architecture\nfor a more stable normal estimation of the unstructured 3D point clouds. In\nparticular, it can partly overcome the influence of noise on a large sampling\nscale compared to the other methods which only use regression loss for normal\nestimation. For more details, a subnetwork is built after point-wise features\nextracted layers of the network and it gives more constraints to each point of\nthe local patch via a binary classifier in the end. Then we use multi-task\noptimization to train the normal estimation and local plane classification\ntasks simultaneously.Also, to integrate the advantages of multi-scale results,\na scale selection strategy is adopted, which is a data-driven approach for\nselecting the optimal scale around each point and encourages subnetwork\nspecialization. Specifically, we employed a subnetwork called Scale Estimation\nNetwork to extract scale weight information from multi-scale features. More\nanalysis is given about the relations between noise levels, local boundary, and\nscales in the experiment. These relationships can be a better guide to choosing\nparticular scales for a particular model. Besides, the experimental result\nshows that our network can distinguish the points on the fitting plane\naccurately and this can be used to guide the normal estimation and our\nmulti-scale method can improve the results well. Compared to some\nstate-of-the-art surface normal estimators, our method is robust to noise and\ncan achieve competitive results.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08537v1"
    },
    {
        "title": "Real-Time Lip Sync for Live 2D Animation",
        "authors": [
            "Deepali Aneja",
            "Wilmot Li"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  The emergence of commercial tools for real-time performance-based 2D\nanimation has enabled 2D characters to appear on live broadcasts and streaming\nplatforms. A key requirement for live animation is fast and accurate lip sync\nthat allows characters to respond naturally to other actors or the audience\nthrough the voice of a human performer. In this work, we present a deep\nlearning based interactive system that automatically generates live lip sync\nfor layered 2D characters using a Long Short Term Memory (LSTM) model. Our\nsystem takes streaming audio as input and produces viseme sequences with less\nthan 200ms of latency (including processing time). Our contributions include\nspecific design decisions for our feature definition and LSTM configuration\nthat provide a small but useful amount of lookahead to produce accurate lip\nsync. We also describe a data augmentation procedure that allows us to achieve\ngood results with a very small amount of hand-animated training data (13-20\nminutes). Extensive human judgement experiments show that our results are\npreferred over several competing methods, including those that only support\noffline (non-live) processing. Video summary and supplementary results at\nGitHub link: https://github.com/deepalianeja/CharacterLipSync2D\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08685v1"
    },
    {
        "title": "Neural View-Interpolation for Sparse Light Field Video",
        "authors": [
            "Mojtaba Bemana",
            "Karol Myszkowski",
            "Hans-Peter Seidel",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  We suggest representing light field (LF) videos as \"one-off\" neural networks\n(NN), i.e., a learned mapping from view-plus-time coordinates to\nhigh-resolution color values, trained on sparse views. Initially, this sounds\nlike a bad idea for three main reasons: First, a NN LF will likely have less\nquality than a same-sized pixel basis representation. Second, only few training\ndata, e.g., 9 exemplars per frame are available for sparse LF videos. Third,\nthere is no generalization across LFs, but across view and time instead.\nConsequently, a network needs to be trained for each LF video. Surprisingly,\nthese problems can turn into substantial advantages: Other than the linear\npixel basis, a NN has to come up with a compact, non-linear i.e., more\nintelligent, explanation of color, conditioned on the sparse view and time\ncoordinates. As observed for many NN however, this representation now is\ninterpolatable: if the image output for sparse view coordinates is plausible,\nit is for all intermediate, continuous coordinates as well. Our specific\nnetwork architecture involves a differentiable occlusion-aware warping step,\nwhich leads to a compact set of trainable parameters and consequently fast\nlearning and fast execution.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.13921v3"
    },
    {
        "title": "Characterisation of rational and NURBS developable surfaces in Computer\n  Aided Design",
        "authors": [
            "Leonardo Fernandez-Jambrina"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper we provide a characterisation of rational developable surfaces\nin terms of the blossoms of the bounding curves and three rational functions\n$\\Lambda$, $M$, $\\nu$. Properties of developable surfaces are revised in this\nframework. In particular, a closed algebraic formula for the edge of regression\nof the surface is obtained in terms of the functions $\\Lambda$, $M$, $\\nu$,\nwhich are closely related to the ones that appear in the standard decomposition\nof the derivative of the parametrisation of one of the bounding curves in terms\nof the director vector of the rulings and its derivative. It is also shown that\nall rational developable surfaces can be described as the set of developable\nsurfaces which can be constructed with a constant $\\Lambda$, $M$, $\\nu$ . The\nresults are readily extended to rational spline developable surfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.00792v1"
    },
    {
        "title": "Latent Space Subdivision: Stable and Controllable Time Predictions for\n  Fluid Flow",
        "authors": [
            "Steffen Wiewel",
            "Byungsoo Kim",
            "Vinicius C. Azevedo",
            "Barbara Solenthaler",
            "Nils Thuerey"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose an end-to-end trained neural networkarchitecture to robustly\npredict the complex dynamics of fluid flows with high temporal stability. We\nfocus on single-phase smoke simulations in 2D and 3D based on the\nincompressible Navier-Stokes (NS) equations, which are relevant for a wide\nrange of practical problems. To achieve stable predictions for long-term flow\nsequences, a convolutional neural network (CNN) is trained for spatial\ncompression in combination with a temporal prediction network that consists of\nstacked Long Short-Term Memory (LSTM) layers. Our core contribution is a novel\nlatent space subdivision (LSS) to separate the respective input quantities into\nindividual parts of the encoded latent space domain. This allows to\ndistinctively alter the encoded quantities without interfering with the\nremaining latent space values and hence maximizes external control. By\nselectively overwriting parts of the predicted latent space points, our\nproposed method is capable to robustly predict long-term sequences of complex\nphysics problems. In addition, we highlight the benefits of a recurrent\ntraining on the latent space creation, which is performed by the spatial\ncompression network.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.08723v1"
    },
    {
        "title": "A Hybrid Lagrangian/Eulerian Collocated Advection and Projection Method\n  for Fluid Simulation",
        "authors": [
            "Steven W. Gagniere",
            "David A. B. Hyde",
            "Alan Marquez-Razon",
            "Chenfanfu Jiang",
            "Ziheng Ge",
            "Xuchen Han",
            "Qi Guo",
            "Joseph Teran"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a hybrid particle/grid approach for simulating incompressible\nfluids on collocated velocity grids. We interchangeably use particle and grid\nrepresentations of transported quantities to balance efficiency and accuracy. A\nnovel Backward Semi-Lagrangian method is derived to improve accuracy of grid\nbased advection. Our approach utilizes the implicit formula associated with\nsolutions of Burgers' equation. We solve this equation using Newton's method\nenabled by $C^1$ continuous grid interpolation. We enforce incompressibility\nover collocated, rather than staggered grids. Our projection technique is\nvariational and designed for B-spline interpolation over regular grids where\nmultiquadratic interpolation is used for velocity and multilinear interpolation\nfor pressure. Despite our use of regular grids, we extend the variational\ntechnique to allow for cut-cell definition of irregular flow domains for both\nDirichlet and free surface boundary conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.12227v1"
    },
    {
        "title": "Learning to Accelerate Decomposition for Multi-Directional 3D Printing",
        "authors": [
            "Chenming Wu",
            "Yong-Jin Liu",
            "Charlie C. L. Wang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Multi-directional 3D printing has the capability of decreasing or eliminating\nthe need for support structures. Recent work proposed a beam-guided search\nalgorithm to find an optimized sequence of plane-clipping, which gives volume\ndecomposition of a given 3D model. Different printing directions are employed\nin different regions to fabricate a model with tremendously less support (or\neven no support in many cases).To obtain optimized decomposition, a large beam\nwidth needs to be used in the search algorithm, leading to a very\ntime-consuming computation. In this paper, we propose a learning framework that\ncan accelerate the beam-guided search by using a smaller number of the original\nbeam width to obtain results with similar quality. Specifically, we use the\nresults of beam-guided search with large beam width to train a scoring function\nfor candidate clipping planes based on six newly proposed feature metrics. With\nthe help of these feature metrics, both the current and the sequence-dependent\ninformation are captured by the neural network to score candidates of clipping.\nAs a result, we can achieve around 3x computational speed. We test and\ndemonstrate our accelerated decomposition on a large dataset of models for 3D\nprinting.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.03450v3"
    },
    {
        "title": "ARCH: Animatable Reconstruction of Clothed Humans",
        "authors": [
            "Zeng Huang",
            "Yuanlu Xu",
            "Christoph Lassner",
            "Hao Li",
            "Tony Tung"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans),\na novel end-to-end framework for accurate reconstruction of animation-ready 3D\nclothed humans from a monocular image. Existing approaches to digitize 3D\nhumans struggle to handle pose variations and recover details. Also, they do\nnot produce models that are animation ready. In contrast, ARCH is a learned\npose-aware model that produces detailed 3D rigged full-body human avatars from\na single unconstrained RGB image. A Semantic Space and a Semantic Deformation\nField are created using a parametric 3D body estimator. They allow the\ntransformation of 2D/3D clothed humans into a canonical space, reducing\nambiguities in geometry caused by pose variations and occlusions in training\ndata. Detailed surface geometry and appearance are learned using an implicit\nfunction representation with spatial local features. Furthermore, we propose\nadditional per-pixel supervision on the 3D reconstruction using opacity-aware\ndifferentiable rendering. Our experiments indicate that ARCH increases the\nfidelity of the reconstructed humans. We obtain more than 50% lower\nreconstruction errors for standard metrics compared to state-of-the-art methods\non public datasets. We also show numerous qualitative examples of animated,\nhigh-quality reconstructed avatars unseen in the literature so far.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.04572v2"
    },
    {
        "title": "A framework for adaptive width control of dense contour-parallel\n  toolpaths in fused deposition modeling",
        "authors": [
            "Tim Kuipers",
            "Eugeni L. Doubrovski",
            "Jun Wu",
            "Charlie C. L. Wang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  3D printing techniques such as Fused Deposition Modeling (FDM) have enabled\nthe fabrication of complex geometry quickly and cheaply. High stiffness parts\nare produced by filling the 2D polygons of consecutive layers with\ncontour-parallel extrusion toolpaths. Uniform width toolpaths consisting of\ninward offsets from the outline polygons produce over- and underfill regions in\nthe center of the shape, which are especially detrimental to the mechanical\nperformance of thin parts. In order to fill shapes with arbitrary diameter\ndensely the toolpaths require adaptive width. Existing approaches for\ngenerating toolpaths with adaptive width result in a large variation in widths,\nwhich for some hardware systems is difficult to realize accurately. In this\npaper we present a framework which supports multiple schemes to generate\ntoolpaths with adaptive width, by employing a function to decide the number of\nbeads and their widths. Furthermore, we propose a novel scheme which reduces\nextreme bead widths, while limiting the number of altered toolpaths. We\nstatistically validate the effectiveness of our framework and this novel scheme\non a data set of representative 3D models, and physically validate it by\ndeveloping a technique, called back pressure compensation, for off-the-shelf\nFDM systems to effectively realize adaptive width.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.13497v1"
    },
    {
        "title": "Image Morphing with Perceptual Constraints and STN Alignment",
        "authors": [
            "Noa Fish",
            "Richard Zhang",
            "Lilach Perry",
            "Daniel Cohen-Or",
            "Eli Shechtman",
            "Connelly Barnes"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In image morphing, a sequence of plausible frames are synthesized and\ncomposited together to form a smooth transformation between given instances.\nIntermediates must remain faithful to the input, stand on their own as members\nof the set, and maintain a well-paced visual transition from one to the next.\nIn this paper, we propose a conditional GAN morphing framework operating on a\npair of input images. The network is trained to synthesize frames corresponding\nto temporal samples along the transformation, and learns a proper shape prior\nthat enhances the plausibility of intermediate frames. While individual frame\nplausibility is boosted by the adversarial setup, a special training protocol\nproducing sequences of frames, combined with a perceptual similarity loss,\npromote smooth transformation over time. Explicit stating of correspondences is\nreplaced with a grid-based freeform deformation spatial transformer that\npredicts the geometric warp between the inputs, instituting the smooth\ngeometric effect by bringing the shapes into an initial alignment. We provide\ncomparisons to classic as well as latent space morphing techniques, and\ndemonstrate that, given a set of images for self-supervision, our network\nlearns to generate visually pleasing morphing effects featuring believable\nin-betweens, with robustness to changes in shape and texture, requiring no\ncorrespondence annotation.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.14071v1"
    },
    {
        "title": "Informative Scene Decomposition for Crowd Analysis, Comparison and\n  Simulation Guidance",
        "authors": [
            "Feixiang He",
            "Yuanhang Xiang",
            "Xi Zhao",
            "He Wang"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Crowd simulation is a central topic in several fields including graphics. To\nachieve high-fidelity simulations, data has been increasingly relied upon for\nanalysis and simulation guidance. However, the information in real-world data\nis often noisy, mixed and unstructured, making it difficult for effective\nanalysis, therefore has not been fully utilized. With the fast-growing volume\nof crowd data, such a bottleneck needs to be addressed. In this paper, we\npropose a new framework which comprehensively tackles this problem. It centers\nat an unsupervised method for analysis. The method takes as input raw and noisy\ndata with highly mixed multi-dimensional (space, time and dynamics)\ninformation, and automatically structure it by learning the correlations among\nthese dimensions. The dimensions together with their correlations fully\ndescribe the scene semantics which consists of recurring activity patterns in a\nscene, manifested as space flows with temporal and dynamics profiles. The\neffectiveness and robustness of the analysis have been tested on datasets with\ngreat variations in volume, duration, environment and crowd dynamics. Based on\nthe analysis, new methods for data visualization, simulation evaluation and\nsimulation guidance are also proposed. Together, our framework establishes a\nhighly automated pipeline from raw data to crowd analysis, comparison and\nsimulation guidance. Extensive experiments and evaluations have been conducted\nto show the flexibility, versatility and intuitiveness of our framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.14107v1"
    },
    {
        "title": "Deep Geometric Texture Synthesis",
        "authors": [
            "Amir Hertz",
            "Rana Hanocka",
            "Raja Giryes",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Recently, deep generative adversarial networks for image generation have\nadvanced rapidly; yet, only a small amount of research has focused on\ngenerative models for irregular structures, particularly meshes. Nonetheless,\nmesh generation and synthesis remains a fundamental topic in computer graphics.\nIn this work, we propose a novel framework for synthesizing geometric textures.\nIt learns geometric texture statistics from local neighborhoods (i.e., local\ntriangular patches) of a single reference 3D model. It learns deep features on\nthe faces of the input triangulation, which is used to subdivide and generate\noffsets across multiple scales, without parameterization of the reference or\ntarget mesh. Our network displaces mesh vertices in any direction (i.e., in the\nnormal and tangential direction), enabling synthesis of geometric textures,\nwhich cannot be expressed by a simple 2D displacement map. Learning and\nsynthesizing on local geometric patches enables a genus-oblivious framework,\nfacilitating texture transfer between shapes of different genus.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00074v1"
    },
    {
        "title": "On Designing GPU Algorithms with Applications to Mesh Refinement",
        "authors": [
            "Zhenghai Chen",
            "Tiow-Seng Tan",
            "Hong-Yang Ong"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a set of rules to guide the design of GPU algorithms. These rules\nare grounded on the principle of reducing waste in GPU utility to achieve good\nspeed up. In accordance to these rules, we propose GPU algorithms for 2D\nconstrained, 3D constrained and 3D Restricted Delaunay refinement problems\nrespectively. Our algorithms take a 2D planar straight line graph (PSLG) or 3D\npiecewise linear complex (PLC) $\\mathcal{G}$ as input, and generate quality\nmeshes conforming or approximating to $\\mathcal{G}$. The implementation of our\nalgorithms shows that they are the first to run an order of magnitude faster\nthan current state-of-the-art counterparts in sequential and parallel manners\nwhile using similar numbers of Steiner points to produce triangulations of\ncomparable qualities. It thus reduces the computing time of mesh refinement\nfrom possibly hours to a few seconds or minutes for possible use in interactive\ngraphics applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00324v1"
    },
    {
        "title": "Surface Denoising based on Normal Filtering in a Robust Statistics\n  Framework",
        "authors": [
            "Sunil Kumar Yadav",
            "Martin Skrodzki",
            "Eric Zimmermann",
            "Konrad Polthier"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  During a surface acquisition process using 3D scanners, noise is inevitable\nand an important step in geometry processing is to remove these noise\ncomponents from these surfaces (given as points-set or triangulated mesh). The\nnoise-removal process (denoising) can be performed by filtering the surface\nnormals first and by adjusting the vertex positions according to filtered\nnormals afterwards. Therefore, in many available denoising algorithms, the\ncomputation of noise-free normals is a key factor. A variety of filters have\nbeen introduced for noise-removal from normals, with different focus points\nlike robustness against outliers or large amplitude of noise. Although these\nfilters are performing well in different aspects, a unified framework is\nmissing to establish the relation between them and to provide a theoretical\nanalysis beyond the performance of each method.\n  In this paper, we introduce such a framework to establish relations between a\nnumber of widely-used nonlinear filters for face normals in mesh denoising and\nvertex normals in point set denoising. We cover robust statistical estimation\nwith M-smoothers and their application to linear and non-linear normal\nfiltering. Although these methods originate in different mathematical theories\n- which include diffusion-, bilateral-, and directional curvature-based\nalgorithms - we demonstrate that all of them can be cast into a unified\nframework of robust statistics using robust error norms and their corresponding\ninfluence functions. This unification contributes to a better understanding of\nthe individual methods and their relations with each other. Furthermore, the\npresented framework provides a platform for new techniques to combine the\nadvantages of known filters and to compare them with available methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00842v1"
    },
    {
        "title": "ADD: Analytically Differentiable Dynamics for Multi-Body Systems with\n  Frictional Contact",
        "authors": [
            "Moritz Geilinger",
            "David Hahn",
            "Jonas Zehnder",
            "Moritz Bächer",
            "Bernhard Thomaszewski",
            "Stelian Coros"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a differentiable dynamics solver that is able to handle frictional\ncontact for rigid and deformable objects within a unified framework. Through a\nprincipled mollification of normal and tangential contact forces, our method\ncircumvents the main difficulties inherent to the non-smooth nature of\nfrictional contact. We combine this new contact model with fully-implicit time\nintegration to obtain a robust and efficient dynamics solver that is\nanalytically differentiable. In conjunction with adjoint sensitivity analysis,\nour formulation enables gradient-based optimization with adaptive trade-offs\nbetween simulation accuracy and smoothness of objective function landscapes. We\nthoroughly analyse our approach on a set of simulation examples involving rigid\nbodies, visco-elastic materials, and coupled multi-body systems. We furthermore\nshowcase applications of our differentiable simulator to parameter estimation\nfor deformable objects, motion planning for robotic manipulation, trajectory\noptimization for compliant walking robots, as well as efficient self-supervised\nlearning of control policies.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00987v1"
    },
    {
        "title": "Structure-Aware Human-Action Generation",
        "authors": [
            "Ping Yu",
            "Yang Zhao",
            "Chunyuan Li",
            "Junsong Yuan",
            "Changyou Chen"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Generating long-range skeleton-based human actions has been a challenging\nproblem since small deviations of one frame can cause a malformed action\nsequence. Most existing methods borrow ideas from video generation, which\nnaively treat skeleton nodes/joints as pixels of images without considering the\nrich inter-frame and intra-frame structure information, leading to potential\ndistorted actions. Graph convolutional networks (GCNs) is a promising way to\nleverage structure information to learn structure representations. However,\ndirectly adopting GCNs to tackle such continuous action sequences both in\nspatial and temporal spaces is challenging as the action graph could be huge.\nTo overcome this issue, we propose a variant of GCNs to leverage the powerful\nself-attention mechanism to adaptively sparsify a complete action graph in the\ntemporal space. Our method could dynamically attend to important past frames\nand construct a sparse graph to apply in the GCN framework, well-capturing the\nstructure information in action sequences. Extensive experimental results\ndemonstrate the superiority of our method on two standard human action datasets\ncompared with existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.01971v3"
    },
    {
        "title": "Learning Adaptive Sampling and Reconstruction for Volume Visualization",
        "authors": [
            "Sebastian Weiss",
            "Mustafa Işık",
            "Justus Thies",
            "Rüdiger Westermann"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  A central challenge in data visualization is to understand which data samples\nare required to generate an image of a data set in which the relevant\ninformation is encoded. In this work, we make a first step towards answering\nthe question of whether an artificial neural network can predict where to\nsample the data with higher or lower density, by learning of correspondences\nbetween the data, the sampling patterns and the generated images. We introduce\na novel neural rendering pipeline, which is trained end-to-end to generate a\nsparse adaptive sampling structure from a given low-resolution input image, and\nreconstructs a high-resolution image from the sparse set of samples. For the\nfirst time, to the best of our knowledge, we demonstrate that the selection of\nstructures that are relevant for the final visual representation can be jointly\nlearned together with the reconstruction of this representation from these\nstructures. Therefore, we introduce differentiable sampling and reconstruction\nstages, which can leverage back-propagation based on supervised losses solely\non the final image. We shed light on the adaptive sampling patterns generated\nby the network pipeline and analyze its use for volume visualization including\nisosurface and direct volume rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.10093v1"
    },
    {
        "title": "Federated Visualization: A Privacy-preserving Strategy for Aggregated\n  Visual Query",
        "authors": [
            "Wei Chen",
            "Yating Wei",
            "Zhiyong Wang",
            "Shuyue Zhou",
            "Bingru Lin",
            "Zhiguang Zhou"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a novel privacy preservation strategy for decentralized\nvisualization. The key idea is to imitate the flowchart of the federated\nlearning framework, and reformulate the visualization process within a\nfederated infrastructure. The federation of visualization is fulfilled by\nleveraging a shared global module that composes the encrypted externalizations\nof transformed visual features of data pieces in local modules. We design two\nimplementations of federated visualization: a prediction-based scheme, and a\nquery-based scheme. We demonstrate the effectiveness of our approach with a set\nof visual forms, and verify its robustness with evaluations. We report the\nvalue of federated visualization in real scenarios with an expert review.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.15227v2"
    },
    {
        "title": "Photon-Driven Neural Path Guiding",
        "authors": [
            "Shilin Zhu",
            "Zexiang Xu",
            "Tiancheng Sun",
            "Alexandr Kuznetsov",
            "Mark Meyer",
            "Henrik Wann Jensen",
            "Hao Su",
            "Ravi Ramamoorthi"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Although Monte Carlo path tracing is a simple and effective algorithm to\nsynthesize photo-realistic images, it is often very slow to converge to\nnoise-free results when involving complex global illumination. One of the most\nsuccessful variance-reduction techniques is path guiding, which can learn\nbetter distributions for importance sampling to reduce pixel noise. However,\nprevious methods require a large number of path samples to achieve reliable\npath guiding. We present a novel neural path guiding approach that can\nreconstruct high-quality sampling distributions for path guiding from a sparse\nset of samples, using an offline trained neural network. We leverage photons\ntraced from light sources as the input for sampling density reconstruction,\nwhich is highly effective for challenging scenes with strong global\nillumination. To fully make use of our deep neural network, we partition the\nscene space into an adaptive hierarchical grid, in which we apply our network\nto reconstruct high-quality sampling distributions for any local region in the\nscene. This allows for highly efficient path guiding for any path bounce at any\nlocation in path tracing. We demonstrate that our photon-driven neural path\nguiding method can generalize well on diverse challenging testing scenes that\nare not seen in training. Our approach achieves significantly better rendering\nresults of testing scenes than previous state-of-the-art path guiding methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.01775v1"
    },
    {
        "title": "Analysis of a Reduced-Order Model for the Simulation of Elastic\n  Geometric Zigzag-Spring Meta-Materials",
        "authors": [
            "Kurt Leimer",
            "Przemyslaw Musialski"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We analyze the performance of a reduced-order simulation of geometric\nmeta-materials based on zigzag patterns using a simplified representation. As\ngeometric meta-materials we denote planar cellular structures which can be\nfabricated in 2d and bent elastically such that they approximate doubly-curved\n2-manifold surfaces in 3d space. They obtain their elasticity attributes mainly\nfrom the geometry of their cellular elements and their connections. In this\npaper we focus on cells build from so-called zigzag springs. The physical\nproperties of the base material (i.e., the physical substance) influence the\nbehavior as well, but we essentially factor them out by keeping them constant.\n  The simulation of such complex geometric structures comes with a high\ncomputational cost, thus we propose an approach to reduce it by abstracting the\nzigzag cells by a simpler model and by learning the properties of their elastic\ndeformation behavior. In particular, we analyze the influence of the sampling\nof the full parameter space and the expressiveness of the reduced model\ncompared to the full model. Based on these observations, we draw conclusions on\nhow to simulate such complex meso-structures with simpler models.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08070v2"
    },
    {
        "title": "Diptychs of human and machine perceptions",
        "authors": [
            "Vivien Cabannes",
            "Thomas Kerdreux",
            "Louis Thiry"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We propose visual creations that put differences in algorithms and humans\n\\emph{perceptions} into perspective. We exploit saliency maps of neural\nnetworks and visual focus of humans to create diptychs that are\nreinterpretations of an original image according to both machine and human\nattentions. Using those diptychs as a qualitative evaluation of perception, we\ndiscuss some crucial issues of current \\textit{task-oriented} artificial\nintelligence.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.13864v1"
    },
    {
        "title": "papaya2: 2D Irreducible Minkowski Tensor computation",
        "authors": [
            "Fabian M. Schaller",
            "Jenny Wagner",
            "Sebastian C. Kapfer"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  A common challenge in scientific and technical domains is the quantitative\ndescription of geometries and shapes, e.g. in the analysis of microscope\nimagery or astronomical observation data. Frequently, it is desirable to go\nbeyond scalar shape metrics such as porosity and surface to volume ratios\nbecause the samples are anisotropic or because direction-dependent quantities\nsuch as conductances or elasticity are of interest. Minkowski Tensors are a\nsystematic family of versatile and robust higher-order shape descriptors that\nallow for shape characterization of arbitrary order and promise a path to\nsystematic structure-function relationships for direction-dependent properties.\nPapaya2 is a software to calculate 2D higher-order shape metrics with a library\ninterface, support for Irreducible Minkowski Tensors and interpolated marching\nsquares. Extensions to Matlab, JavaScript and Python are provided as well.\nWhile the tensor of inertia is computed by many tools, we are not aware of\nother open-source software which provides higher-rank shape characterization in\n2D.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.15138v1"
    },
    {
        "title": "Modular Primitives for High-Performance Differentiable Rendering",
        "authors": [
            "Samuli Laine",
            "Janne Hellsten",
            "Tero Karras",
            "Yeongho Seol",
            "Jaakko Lehtinen",
            "Timo Aila"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  We present a modular differentiable renderer design that yields performance\nsuperior to previous methods by leveraging existing, highly optimized hardware\ngraphics pipelines. Our design supports all crucial operations in a modern\ngraphics pipeline: rasterizing large numbers of triangles, attribute\ninterpolation, filtered texture lookups, as well as user-programmable shading\nand geometry processing, all in high resolutions. Our modular primitives allow\ncustom, high-performance graphics pipelines to be built directly within\nautomatic differentiation frameworks such as PyTorch or TensorFlow. As a\nmotivating application, we formulate facial performance capture as an inverse\nrendering problem and show that it can be solved efficiently using our tools.\nOur results indicate that this simple and straightforward approach achieves\nexcellent geometric correspondence between rendered results and reference\nimagery.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.03277v1"
    },
    {
        "title": "Conversion Between Cubic Bezier Curves and Catmull-Rom Splines",
        "authors": [
            "Soroosh Tayebi Arasteh",
            "Adam Kalisz"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Splines are one of the main methods of mathematically representing\ncomplicated shapes, which have become the primary technique in the fields of\nComputer Graphics (CG) and Computer-Aided Geometric Design (CAGD) for modeling\ncomplex surfaces. Among all, B\\'ezier and Catmull-Rom splines are the most\ncommon in the sub-fields of engineering. In this paper, we focus on conversion\nbetween cubic B\\'ezier and Catmull-Rom curve segments, rather than going\nthrough their properties. By deriving the conversion equations, we aim at\nconverting the original set of the control points of either of the Catmull-Rom\nor B\\'ezier cubic curves to a new set of control points, which corresponds to\napproximately the same shape as the original curve, when considered as the set\nof the control points of the other curve. Due to providing simple linear\ntransformations of control points, the method is very simple, efficient, and\neasy to implement, which is further validated in this paper using some\nnumerical and visual examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.08232v3"
    },
    {
        "title": "ScalarFlow: A Large-Scale Volumetric Data Set of Real-world Scalar\n  Transport Flows for Computer Animation and Machine Learning",
        "authors": [
            "Marie-Lena Eckert",
            "Kiwon Um",
            "Nils Thuerey"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  In this paper, we present ScalarFlow, a first large-scale data set of\nreconstructions of real-world smoke plumes. We additionally propose a framework\nfor accurate physics-based reconstructions from a small number of video\nstreams. Central components of our algorithm are a novel estimation of unseen\ninflow regions and an efficient regularization scheme. Our data set includes a\nlarge number of complex and natural buoyancy-driven flows. The flows transition\nto turbulent flows and contain observable scalar transport processes. As such,\nthe ScalarFlow data set is tailored towards computer graphics, vision, and\nlearning applications. The published data set will contain volumetric\nreconstructions of velocity and density, input image sequences, together with\ncalibration data, code, and instructions how to recreate the commodity hardware\ncapture setup. We further demonstrate one of the many potential application\nareas: a first perceptual evaluation study, which reveals that the complexity\nof the captured flows requires a huge simulation resolution for regular solvers\nin order to recreate at least parts of the natural complexity contained in the\ncaptured data.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.10284v1"
    },
    {
        "title": "Learning to dance: A graph convolutional adversarial network to generate\n  realistic dance motions from audio",
        "authors": [
            "João P. Ferreira",
            "Thiago M. Coutinho",
            "Thiago L. Gomes",
            "José F. Neto",
            "Rafael Azevedo",
            "Renato Martins",
            "Erickson R. Nascimento"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Synthesizing human motion through learning techniques is becoming an\nincreasingly popular approach to alleviating the requirement of new data\ncapture to produce animations. Learning to move naturally from music, i.e., to\ndance, is one of the more complex motions humans often perform effortlessly.\nEach dance movement is unique, yet such movements maintain the core\ncharacteristics of the dance style. Most approaches addressing this problem\nwith classical convolutional and recursive neural models undergo training and\nvariability issues due to the non-Euclidean geometry of the motion manifold\nstructure.In this paper, we design a novel method based on graph convolutional\nnetworks to tackle the problem of automatic dance generation from audio\ninformation. Our method uses an adversarial learning scheme conditioned on the\ninput music audios to create natural motions preserving the key movements of\ndifferent music styles. We evaluate our method with three quantitative metrics\nof generative methods and a user study. The results suggest that the proposed\nGCN model outperforms the state-of-the-art dance generation method conditioned\non music in different experiments. Moreover, our graph-convolutional approach\nis simpler, easier to be trained, and capable of generating more realistic\nmotion styles regarding qualitative and different quantitative metrics. It also\npresented a visual movement perceptual quality comparable to real motion data.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.12999v2"
    },
    {
        "title": "UniCon: Universal Neural Controller For Physics-based Character Motion",
        "authors": [
            "Tingwu Wang",
            "Yunrong Guo",
            "Maria Shugrina",
            "Sanja Fidler"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  The field of physics-based animation is gaining importance due to the\nincreasing demand for realism in video games and films, and has recently seen\nwide adoption of data-driven techniques, such as deep reinforcement learning\n(RL), which learn control from (human) demonstrations. While RL has shown\nimpressive results at reproducing individual motions and interactive\nlocomotion, existing methods are limited in their ability to generalize to new\nmotions and their ability to compose a complex motion sequence interactively.\nIn this paper, we propose a physics-based universal neural controller (UniCon)\nthat learns to master thousands of motions with different styles by learning on\nlarge-scale motion datasets. UniCon is a two-level framework that consists of a\nhigh-level motion scheduler and an RL-powered low-level motion executor, which\nis our key innovation. By systematically analyzing existing multi-motion RL\nframeworks, we introduce a novel objective function and training techniques\nwhich make a significant leap in performance. Once trained, our motion executor\ncan be combined with different high-level schedulers without the need for\nretraining, enabling a variety of real-time interactive applications. We show\nthat UniCon can support keyboard-driven control, compose motion sequences drawn\nfrom a large pool of locomotion and acrobatics skills and teleport a person\ncaptured on video to a physics-based virtual avatar. Numerical and qualitative\nresults demonstrate a significant improvement in efficiency, robustness and\ngeneralizability of UniCon over prior state-of-the-art, showcasing\ntransferability to unseen motions, unseen humanoid models and unseen\nperturbation.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.15119v1"
    },
    {
        "title": "GPU Optimization for High-Quality Kinetic Fluid Simulation",
        "authors": [
            "Yixin Chen",
            "Wei Li",
            "Rui Fan",
            "Xiaopei Liu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Fluid simulations are often performed using the incompressible Navier-Stokes\nequations (INSE), leading to sparse linear systems which are difficult to solve\nefficiently in parallel. Recently, kinetic methods based on the\nadaptive-central-moment multiple-relaxation-time (ACM-MRT) model have\ndemonstrated impressive capabilities to simulate both laminar and turbulent\nflows, with quality matching or surpassing that of state-of-the-art INSE\nsolvers. Furthermore, due to its local formulation, this method presents the\nopportunity for highly scalable implementations on parallel systems such as\nGPUs. However, an efficient ACM-MRT-based kinetic solver needs to overcome a\nnumber of computational challenges, especially when dealing with complex solids\ninside the fluid domain. In this paper, we present multiple novel GPU\noptimization techniques to efficiently implement high-quality ACM-MRT-based\nkinetic fluid simulations in domains containing complex solids. Our techniques\ninclude a new communication-efficient data layout, a load-balanced\nimmersed-boundary method, a multi-kernel launch method using a simplified\nformulation of ACM-MRT calculations to enable greater parallelism, and the\nintegration of these techniques into a parametric cost model to enable\nautomated parameter search to achieve optimal execution performance. We also\nextended our method to multi-GPU systems to enable large-scale simulations. To\ndemonstrate the state-of-the-art performance and high visual quality of our\nsolver, we present extensive experimental results and comparisons to other\nsolvers.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.11856v1"
    },
    {
        "title": "Learning Spectral Unions of Partial Deformable 3D Shapes",
        "authors": [
            "Luca Moschella",
            "Simone Melzi",
            "Luca Cosmo",
            "Filippo Maggioli",
            "Or Litany",
            "Maks Ovsjanikov",
            "Leonidas Guibas",
            "Emanuele Rodolà"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Spectral geometric methods have brought revolutionary changes to the field of\ngeometry processing. Of particular interest is the study of the Laplacian\nspectrum as a compact, isometry and permutation-invariant representation of a\nshape. Some recent works show how the intrinsic geometry of a full shape can be\nrecovered from its spectrum, but there are approaches that consider the more\nchallenging problem of recovering the geometry from the spectral information of\npartial shapes. In this paper, we propose a possible way to fill this gap. We\nintroduce a learning-based method to estimate the Laplacian spectrum of the\nunion of partial non-rigid 3D shapes, without actually computing the 3D\ngeometry of the union or any correspondence between those partial shapes. We do\nso by operating purely in the spectral domain and by defining the union\noperation between short sequences of eigenvalues. We show that the approximated\nunion spectrum can be used as-is to reconstruct the complete geometry [MRC*19],\nperform region localization on a template [RTO*19] and retrieve shapes from a\ndatabase, generalizing ShapeDNA [RWP06] to work with partialities. Working with\neigenvalues allows us to deal with unknown correspondence, different sampling,\nand different discretizations (point clouds and meshes alike), making this\noperation especially robust and general. Our approach is data-driven and can\ngeneralize to isometric and non-isometric deformations of the surface, as long\nas these stay within the same semantic class (e.g., human bodies or horses), as\nwell as to partiality artifacts not seen at training time.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.00514v3"
    },
    {
        "title": "NeuMIP: Multi-Resolution Neural Materials",
        "authors": [
            "Alexandr Kuznetsov",
            "Krishna Mullia",
            "Zexiang Xu",
            "Miloš Hašan",
            "Ravi Ramamoorthi"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We propose NeuMIP, a neural method for representing and rendering a variety\nof material appearances at different scales. Classical prefiltering\n(mipmapping) methods work well on simple material properties such as diffuse\ncolor, but fail to generalize to normals, self-shadowing, fibers or more\ncomplex microstructures and reflectances. In this work, we generalize\ntraditional mipmap pyramids to pyramids of neural textures, combined with a\nfully connected network. We also introduce neural offsets, a novel method which\nallows rendering materials with intricate parallax effects without any\ntessellation. This generalizes classical parallax mapping, but is trained\nwithout supervision by any explicit heightfield. Neural materials within our\nsystem support a 7-dimensional query, including position, incoming and outgoing\ndirection, and the desired filter kernel size. The materials have small storage\n(on the order of standard mipmapping except with more texture channels), and\ncan be integrated within common Monte-Carlo path tracing systems. We\ndemonstrate our method on a variety of materials, resulting in complex\nappearance across levels of detail, with accurate parallax, self-shadowing, and\nother effects.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.02789v1"
    },
    {
        "title": "Fabrication-aware Design for Furniture with Planar Pieces",
        "authors": [
            "Wenzhong Yan",
            "Dawei Zhao",
            "Ankur Mehta"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We propose a computational design tool to enable casual end-users to easily\ndesign, fabricate, and assemble flat-pack furniture with guaranteed\nmanufacturability. Using our system, users select parameterized components from\na library and constrain their dimensions. Then they abstractly specify\nconnections among components to define the furniture. Once fabrication\nspecifications (e.g. materials) designated, the mechanical implementation of\nthe furniture is automatically handled by leveraging encoded domain expertise.\nAfterwards, the system outputs 3D models for visualization and mechanical\ndrawings for fabrication. We demonstrate the validity of our approach by\ndesigning, fabricating, and assembling a variety of flat-pack (scaled)\nfurniture on demand.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.05052v1"
    },
    {
        "title": "ShapeMOD: Macro Operation Discovery for 3D Shape Programs",
        "authors": [
            "R. Kenny Jones",
            "David Charatan",
            "Paul Guerrero",
            "Niloy J. Mitra",
            "Daniel Ritchie"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  A popular way to create detailed yet easily controllable 3D shapes is via\nprocedural modeling, i.e. generating geometry using programs. Such programs\nconsist of a series of instructions along with their associated parameter\nvalues. To fully realize the benefits of this representation, a shape program\nshould be compact and only expose degrees of freedom that allow for meaningful\nmanipulation of output geometry. One way to achieve this goal is to design\nhigher-level macro operators that, when executed, expand into a series of\ncommands from the base shape modeling language. However, manually authoring\nsuch macros, much like shape programs themselves, is difficult and largely\nrestricted to domain experts. In this paper, we present ShapeMOD, an algorithm\nfor automatically discovering macros that are useful across large datasets of\n3D shape programs. ShapeMOD operates on shape programs expressed in an\nimperative, statement-based language. It is designed to discover macros that\nmake programs more compact by minimizing the number of function calls and free\nparameters required to represent an input shape collection. We run ShapeMOD on\nmultiple collections of programs expressed in a domain-specific language for 3D\nshape structures. We show that it automatically discovers a concise set of\nmacros that abstract out common structural and parametric patterns that\ngeneralize over large shape collections. We also demonstrate that the macros\nfound by ShapeMOD improve performance on downstream tasks including shape\ngenerative modeling and inferring programs from point clouds. Finally, we\nconduct a user study that indicates that ShapeMOD's discovered macros make\ninteractive shape editing more efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.06392v3"
    },
    {
        "title": "Signed Distance Function Computation from an Implicit Surface",
        "authors": [
            "Pierre-Alain Fayolle"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We describe in this short note a technique to convert an implicit surface\ninto a Signed Distance Function (SDF) while exactly preserving the zero\nlevel-set of the implicit. The proposed approach relies on embedding the input\nimplicit in the final layer of a neural network, which is trained to minimize a\nloss function characterizing the SDF.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.08057v2"
    },
    {
        "title": "Voxel Structure-based Mesh Reconstruction from a 3D Point Cloud",
        "authors": [
            "Chenlei Lv",
            "Weisi Lin",
            "Baoquan Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Mesh reconstruction from a 3D point cloud is an important topic in the fields\nof computer graphic, computer vision, and multimedia analysis. In this paper,\nwe propose a voxel structure-based mesh reconstruction framework. It provides\nthe intrinsic metric to improve the accuracy of local region detection. Based\non the detected local regions, an initial reconstructed mesh can be obtained.\nWith the mesh optimization in our framework, the initial reconstructed mesh is\noptimized into an isotropic one with the important geometric features such as\nexternal and internal edges. The experimental results indicate that our\nframework shows great advantages over peer ones in terms of mesh quality,\ngeometric feature keeping, and processing speed.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.10622v3"
    },
    {
        "title": "Efficient Hyperparameter Optimization for Physics-based Character\n  Animation",
        "authors": [
            "Zeshi Yang",
            "Zhiqi Yin"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Physics-based character animation has seen significant advances in recent\nyears with the adoption of Deep Reinforcement Learning (DRL). However,\nDRL-based learning methods are usually computationally expensive and their\nperformance crucially depends on the choice of hyperparameters. Tuning\nhyperparameters for these methods often requires repetitive training of control\npolicies, which is even more computationally prohibitive. In this work, we\npropose a novel Curriculum-based Multi-Fidelity Bayesian Optimization framework\n(CMFBO) for efficient hyperparameter optimization of DRL-based character\ncontrol systems. Using curriculum-based task difficulty as fidelity criterion,\nour method improves searching efficiency by gradually pruning search space\nthrough evaluation on easier motor skill tasks. We evaluate our method on two\nphysics-based character control tasks: character morphology optimization and\nhyperparameter tuning of DeepMimic. Our algorithm significantly outperforms\nstate-of-the-art hyperparameter optimization methods applicable for\nphysics-based character animation. In particular, we show that hyperparameters\noptimized through our algorithm result in at least 5x efficiency gain comparing\nto author-released settings in DeepMimic.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.12365v1"
    },
    {
        "title": "Surface Multigrid via Intrinsic Prolongation",
        "authors": [
            "Hsueh-Ti Derek Liu",
            "Jiayi Eris Zhang",
            "Mirela Ben-Chen",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This paper introduces a novel geometric multigrid solver for unstructured\ncurved surfaces. Multigrid methods are highly efficient iterative methods for\nsolving systems of linear equations. Despite the success in solving problems\ndefined on structured domains, generalizing multigrid to unstructured curved\ndomains remains a challenging problem. The critical missing ingredient is a\nprolongation operator to transfer functions across different multigrid levels.\nWe propose a novel method for computing the prolongation for triangulated\nsurfaces based on intrinsic geometry, enabling an efficient geometric multigrid\nsolver for curved surfaces. Our surface multigrid solver achieves better\nconvergence than existing multigrid methods. Compared to direct solvers, our\nsolver is orders of magnitude faster. We evaluate our method on many geometry\nprocessing applications and a wide variety of complex shapes with and without\nboundaries. By simply replacing the direct solver, we upgrade existing\nalgorithms to interactive frame rates, and shift the computational bottleneck\naway from solving linear systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.13755v2"
    },
    {
        "title": "Orienting Point Clouds with Dipole Propagation",
        "authors": [
            "Gal Metzer",
            "Rana Hanocka",
            "Denis Zorin",
            "Raja Giryes",
            "Daniele Panozzo",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Establishing a consistent normal orientation for point clouds is a\nnotoriously difficult problem in geometry processing, requiring attention to\nboth local and global shape characteristics. The normal direction of a point is\na function of the local surface neighborhood; yet, point clouds do not disclose\nthe full underlying surface structure. Even assuming known geodesic proximity,\ncalculating a consistent normal orientation requires the global context. In\nthis work, we introduce a novel approach for establishing a globally consistent\nnormal orientation for point clouds. Our solution separates the local and\nglobal components into two different sub-problems. In the local phase, we train\na neural network to learn a coherent normal direction per patch (i.e.,\nconsistently oriented normals within a single patch). In the global phase, we\npropagate the orientation across all coherent patches using a dipole\npropagation. Our dipole propagation decides to orient each patch using the\nelectric field defined by all previously orientated patches. This gives rise to\na global propagation that is stable, as well as being robust to nearby\nsurfaces, holes, sharp features and noise.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01604v1"
    },
    {
        "title": "Learning Skeletal Articulations with Neural Blend Shapes",
        "authors": [
            "Peizhuo Li",
            "Kfir Aberman",
            "Rana Hanocka",
            "Libin Liu",
            "Olga Sorkine-Hornung",
            "Baoquan Chen"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Animating a newly designed character using motion capture (mocap) data is a\nlong standing problem in computer animation. A key consideration is the\nskeletal structure that should correspond to the available mocap data, and the\nshape deformation in the joint regions, which often requires a tailored,\npose-specific refinement. In this work, we develop a neural technique for\narticulating 3D characters using enveloping with a pre-defined skeletal\nstructure which produces high quality pose dependent deformations. Our\nframework learns to rig and skin characters with the same articulation\nstructure (e.g., bipeds or quadrupeds), and builds the desired skeleton\nhierarchy into the network architecture. Furthermore, we propose neural blend\nshapes--a set of corrective pose-dependent shapes which improve the deformation\nquality in the joint regions in order to address the notorious artifacts\nresulting from standard rigging and skinning. Our system estimates neural blend\nshapes for input meshes with arbitrary connectivity, as well as weighting\ncoefficients which are conditioned on the input joint rotations. Unlike recent\ndeep learning techniques which supervise the network with ground-truth rigging\nand skinning parameters, our approach does not assume that the training data\nhas a specific underlying deformation model. Instead, during training, the\nnetwork observes deformed shapes and learns to infer the corresponding rig,\nskin and blend shapes using indirect supervision. During inference, we\ndemonstrate that our network generalizes to unseen characters with arbitrary\nmesh connectivity, including unrigged characters built by 3D artists.\nConforming to standard skeletal animation models enables direct plug-and-play\nin standard animation software, as well as game engines.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.02451v1"
    },
    {
        "title": "Software Compensation of Undesirable Racking Motion of H-frame 3D\n  Printers using Filtered B-Splines",
        "authors": [
            "Nosakhare Edoimioya",
            "Keval S. Ramani",
            "Chinedum E. Okwudire"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The H-frame (also known as H-Bot) architecture is a simple and elegant\ntwo-axis parallel positioning system used to construct the XY stage of 3D\nprinters. It holds potential for high speed and excellent dynamic performance\ndue to the use of frame-mounted motors that reduce the moving mass of the\nprinter while allowing for the use of (heavy) higher torque motors. However,\nthe H-frame's dynamic accuracy is limited during high-acceleration and\nhigh-speed motion due to racking -- i.e., parasitic torsional motions of the\nprinter's gantry due to a force couple. Mechanical solutions to the racking\nproblem are either costly or detract from the simplicity of the H-frame. In\nthis paper, we introduce a feedforward software compensation algorithm, based\non the filtered B-splines (FBS) method, that rectifies errors due to racking.\nThe FBS approach expresses the motion command to the machine as a linear\ncombination of B-splines. The B-splines are filtered through an identified\nmodel of the machine dynamics and the control points of the B-spline based\nmotion command are optimized such that the tracking error is minimized. To\ncompensate racking using the FBS algorithm, an accurate frequency response\nfunction of the racking motion is obtained and coupled to the H-frame's x- and\ny-axis dynamics with a kinematic model. The result is a coupled linear\nparameter varying model of the H-frame that is utilized in the FBS framework to\ncompensate racking. An approximation of the proposed racking compensation\nalgorithm, that decouples the x- and y-axis compensation, is developed to\nsignificantly improve its computational efficiency with almost no loss of\ncompensation accuracy. Experiments on an H-frame 3D printer demonstrate a 43\npercent improvement in the shape accuracy of a printed part using the proposed\nalgorithm compared to the standard FBS approach without racking compensation.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.09878v1"
    },
    {
        "title": "A Variational Loop Shrinking Analogy for Handle and Tunnel Detection and\n  Reeb Graph Construction on Surfaces",
        "authors": [
            "Alexander Weinrauch",
            "Hans-Peter Seidel",
            "Daniel Mlakar",
            "Markus Steinberger",
            "Rhaleb Zayer"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The humble loop shrinking property played a central role in the inception of\nmodern topology but it has been eclipsed by more abstract algebraic formalism.\nThis is particularly true in the context of detecting relevant non-contractible\nloops on surfaces where elaborate homological and/or graph theoretical\nconstructs are favored in algorithmic solutions. In this work, we devise a\nvariational analogy to the loop shrinking property and show that it yields a\nsimple, intuitive, yet powerful solution allowing a streamlined treatment of\nthe problem of handle and tunnel loop detection. Our formalization tracks the\nevolution of a diffusion front randomly initiated on a single location on the\nsurface. Capitalizing on a diffuse interface representation combined with a set\nof rules for concurrent front interactions, we develop a dynamic data structure\nfor tracking the evolution on the surface encoded as a sparse matrix which\nserves for performing both diffusion numerics and loop detection and acts as\nthe workhorse of our fully parallel implementation. The substantiated results\nsuggest our approach outperforms state of the art and robustly copes with\nhighly detailed geometric models. As a byproduct, our approach can be used to\nconstruct Reeb graphs by diffusion thus avoiding commonly encountered issues\nwhen using Morse functions.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.13168v1"
    },
    {
        "title": "Local Latent Representation based on Geometric Convolution for Particle\n  Data Feature Exploration",
        "authors": [
            "Haoyu Li",
            "Han-Wei Shen"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Feature related particle data analysis plays an important role in many\nscientific applications such as fluid simulations, cosmology simulations and\nmolecular dynamics. Compared to conventional methods that use hand-crafted\nfeature descriptors, some recent studies focus on transforming the data into a\nnew latent space, where features are easier to be identified, compared and\nextracted. However, it is challenging to transform particle data into latent\nrepresentations, since the convolution neural networks used in prior studies\nrequire the data presented in regular grids. In this paper, we adopt Geometric\nConvolution, a neural network building block designed for 3D point clouds, to\ncreate latent representations for scientific particle data. These latent\nrepresentations capture both the particle positions and their physical\nattributes in the local neighborhood so that features can be extracted by\nclustering in the latent space, and tracked by applying tracking algorithms\nsuch as mean-shift. We validate the extracted features and tracking results\nfrom our approach using datasets from three applications and show that they are\ncomparable to the methods that define hand-crafted features for each specific\ndataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.13240v2"
    },
    {
        "title": "MeshCNN Fundamentals: Geometric Learning through a Reconstructable\n  Representation",
        "authors": [
            "Amir Barda",
            "Yotam Erel",
            "Amit H. Bermano"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Mesh-based learning is one of the popular approaches nowadays to learn\nshapes. The most established backbone in this field is MeshCNN. In this paper,\nwe propose infusing MeshCNN with geometric reasoning to achieve higher quality\nlearning. Through careful analysis of the way geometry is represented\nthrough-out the network, we submit that this representation should be rigid\nmotion invariant, and should allow reconstructing the original geometry.\nAccordingly, we introduce the first and second fundamental forms as an\nedge-centric, rotation and translation invariant, reconstructable\nrepresentation. In addition, we update the originally proposed pooling scheme\nto be more geometrically driven. We validate our analysis through\nexperimentation, and present consistent improvement upon the MeshCNN baseline,\nas well as other more elaborate state-of-the-art architectures. Furthermore, we\ndemonstrate this fundamental forms-based representation opens the door to\naccessible generative machine learning over meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.13277v1"
    },
    {
        "title": "Z2P: Instant Visualization of Point Clouds",
        "authors": [
            "Gal Metzer",
            "Rana Hanocka",
            "Raja Giryes",
            "Niloy J. Mitra",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present a technique for visualizing point clouds using a neural network.\nOur technique allows for an instant preview of any point cloud, and bypasses\nthe notoriously difficult surface reconstruction problem or the need to\nestimate oriented normals for splat-based rendering. We cast the preview\nproblem as a conditional image-to-image translation task, and design a neural\nnetwork that translates point depth-map directly into an image, where the point\ncloud is visualized as though a surface was reconstructed from it. Furthermore,\nthe resulting appearance of the visualized point cloud can be, optionally,\nconditioned on simple control variables (e.g., color and light). We demonstrate\nthat our technique instantly produces plausible images, and can, on-the-fly\neffectively handle noise, non-uniform sampling, and thin surfaces sheets.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.14548v2"
    },
    {
        "title": "An App for the Discovery of Properties of Poncelet Triangles",
        "authors": [
            "Iverton Darlan",
            "Dan Reznik"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We describe a newly-developed, free, browser-based application, for the\ninteractive exploration of the dynamic geometry of Poncelet families of\ntriangles. The main focus is on responsive display of the beauteous loci of\ncenters of such families, refreshing them smoothly upon any changes in\nsimulation parameters. The app informs the user when curves swept are conics\nand reports if certain metric quantities are conserved. Live simulations can be\neasily shared via a URL. A list of more than 400 pre-made experiments is\nincluded which can be regarded as conjectures and/or exercises. Millions of\nexperiment combinations are possible.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.04521v9"
    },
    {
        "title": "Neural Marching Cubes",
        "authors": [
            "Zhiqin Chen",
            "Hao Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.11272v3"
    },
    {
        "title": "Repulsive Surfaces",
        "authors": [
            "Chris Yu",
            "Caleb Brakensiek",
            "Henrik Schumacher",
            "Keenan Crane"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Functionals that penalize bending or stretching of a surface play a key role\nin geometric and scientific computing, but to date have ignored a very basic\nrequirement: in many situations, surfaces must not pass through themselves or\neach other. This paper develops a numerical framework for optimization of\nsurface geometry while avoiding (self-)collision. The starting point is the\ntangent-point energy, which effectively pushes apart pairs of points that are\nclose in space but distant along the surface. We develop a discretization of\nthis energy for triangle meshes, and introduce a novel acceleration scheme\nbased on a fractional Sobolev inner product. In contrast to similar schemes\ndeveloped for curves, we avoid the complexity of building a multiresolution\nmesh hierarchy by decomposing our preconditioner into two ordinary Poisson\nequations, plus forward application of a fractional differential operator. We\nfurther accelerate this scheme via hierarchical approximation, and describe how\nto incorporate a variety of constraints (on area, volume, etc.). Finally, we\nexplore how this machinery might be applied to problems in mathematical\nvisualization, geometric modeling, and geometry processing.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.01664v1"
    },
    {
        "title": "Wasserstein Distances, Geodesics and Barycenters of Merge Trees",
        "authors": [
            "Mathieu Pont",
            "Jules Vidal",
            "Julie Delon",
            "Julien Tierny"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This paper presents a unified computational framework for the estimation of\ndistances, geodesics and barycenters of merge trees. We extend recent work on\nthe edit distance [106] and introduce a new metric, called the Wasserstein\ndistance between merge trees, which is purposely designed to enable efficient\ncomputations of geodesics and barycenters. Specifically, our new distance is\nstrictly equivalent to the L2-Wasserstein distance between extremum persistence\ndiagrams, but it is restricted to a smaller solution space, namely, the space\nof rooted partial isomorphisms between branch decomposition trees. This enables\na simple extension of existing optimization frameworks [112] for geodesics and\nbarycenters from persistence diagrams to merge trees. We introduce a task-based\nalgorithm which can be generically applied to distance, geodesic, barycenter or\ncluster computation. The task-based nature of our approach enables further\naccelerations with shared-memory parallelism. Extensive experiments on public\nensembles and SciVis contest benchmarks demonstrate the efficiency of our\napproach -- with barycenter computations in the orders of minutes for the\nlargest examples -- as well as its qualitative ability to generate\nrepresentative barycenter merge trees, visually summarizing the features of\ninterest found in the ensemble. We show the utility of our contributions with\ndedicated visualization applications: feature tracking, temporal reduction and\nensemble clustering. We provide a lightweight C++ implementation that can be\nused to reproduce our results.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.07789v2"
    },
    {
        "title": "Learning Activities in Colours and Rainbows for Programming Skill\n  Development",
        "authors": [
            "Jonathan C. Roberts"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present how we have created a series of bilingual (English and Welsh) STEM\nactivities focusing on rainbows, colours, light and optical effects. The\nactivities were motivated by the many rainbows that appeared in windows in the\nUK, in support of the National Health Service at the start of the coronavirus\npandemic. Rainbows are hopeful and are very fitting to be used as a positive\niconic image at a time of much uncertainty. In this paper we explain how we\nhave developed and organised the activities, focusing on colours, computer\ngraphics and computer programming. Each lesson contains one or more activities,\nwhich enable people to take an active role in their learning. We have carefully\nprepared and organised several processes to guide academic colleagues to create\nand publish different activities in the theme. Which means that the activities\nappear similarly structured, can be categorised and searched in a consistent\nway. This structure can act as a blueprint for others to follow and apply to\ndevelop their own online course. The activities incrementally take people\nthrough learning about colour, rainbows, planning what to program, design and\nstrategies to create colourful pictures using simple computer graphics\nprinciples based in processing.org.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.03162v1"
    },
    {
        "title": "DASH: Modularized Human Manipulation Simulation with Vision and Language\n  for Embodied AI",
        "authors": [
            "Yifeng Jiang",
            "Michelle Guo",
            "Jiangshan Li",
            "Ioannis Exarchos",
            "Jiajun Wu",
            "C. Karen Liu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Creating virtual humans with embodied, human-like perceptual and actuation\nconstraints has the promise to provide an integrated simulation platform for\nmany scientific and engineering applications. We present Dynamic and Autonomous\nSimulated Human (DASH), an embodied virtual human that, given natural language\ncommands, performs grasp-and-stack tasks in a physically-simulated cluttered\nenvironment solely using its own visual perception, proprioception, and touch,\nwithout requiring human motion data. By factoring the DASH system into a vision\nmodule, a language module, and manipulation modules of two skill categories, we\ncan mix and match analytical and machine learning techniques for different\nmodules so that DASH is able to not only perform randomly arranged tasks with a\nhigh success rate, but also do so under anthropomorphic constraints and with\nfluid and diverse motions. The modular design also favors analysis and\nextensibility to more complex manipulation skills.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.12536v1"
    },
    {
        "title": "CodeNeRF: Disentangled Neural Radiance Fields for Object Categories",
        "authors": [
            "Wonbong Jang",
            "Lourdes Agapito"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  CodeNeRF is an implicit 3D neural representation that learns the variation of\nobject shapes and textures across a category and can be trained, from a set of\nposed images, to synthesize novel views of unseen objects. Unlike the original\nNeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture\nby learning separate embeddings. At test time, given a single unposed image of\nan unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and\nappearance codes via optimization. Unseen objects can be reconstructed from a\nsingle image, and then rendered from new viewpoints or their shape and texture\nedited by varying the latent codes. We conduct experiments on the SRN\nbenchmark, which show that CodeNeRF generalises well to unseen objects and\nachieves on-par performance with methods that require known camera pose at test\ntime. Our results on real-world images demonstrate that CodeNeRF can bridge the\nsim-to-real gap. Project page: \\url{https://github.com/wayne1123/code-nerf}\n",
        "pdf_link": "http://arxiv.org/pdf/2109.01750v1"
    },
    {
        "title": "SFCDecomp: Multicriteria Optimized Tool Path Planning in 3D Printing\n  using Space-Filling Curve Based Domain Decomposition",
        "authors": [
            "Prashant Gupta",
            "Yiran Guo",
            "Narasimha Boddeti",
            "Bala Krishnamoorthy"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We explore efficient optimization of toolpaths based on multiple criteria for\nlarge instances of 3D printing problems. We first show that the minimum turn\ncost 3D printing problem is NP-hard, even when the region is a simple polygon.\nWe develop SFCDecomp, a space filling curve based decomposition framework to\nsolve large instances of 3D printing problems efficiently by solving these\noptimization subproblems independently. For the Buddha model, our framework\nbuilds toolpaths over a total of 799,716 nodes across 169 layers, and for the\nBunny model it builds toolpaths over 812,733 nodes across 360 layers. Building\non SFCDecomp, we develop a multicriteria optimization approach for toolpath\nplanning. We demonstrate the utility of our framework by maximizing or\nminimizing tool path edge overlap between adjacent layers, while jointly\nminimizing turn costs. Strength testing of a tensile test specimen printed with\ntool paths that maximize or minimize adjacent layer edge overlaps reveal\nsignificant differences in tensile strength between the two classes of prints.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.01769v2"
    },
    {
        "title": "GeneNet VR: Interactive visualization of large-scale biological networks\n  using a standalone headset",
        "authors": [
            "Álvaro Martínez Fernández",
            "Lars Ailo Bongo",
            "Edvard Pedersen"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Visualizations are an essential part of biomedical analysis result\ninterpretation. Often, interactive networks are used to visualize the data.\nHowever, the high interconnectivity, and high dimensionality of the data often\nresults in information overload, making it hard to interpret the results. To\naddress the information overload problem, existing solutions typically either\nuse data reduction, reduced interactivity, or expensive hardware. We propose\nusing the affordable Oculus Quest Virtual Reality (VR) headset for interactive\nvisualization of large-scale biological networks.\n  We present the design and implementation of our solution, GeneNet VR, and we\nevaluate its scalability and usability using large gene-to-gene interaction\nnetworks. We achieve the 72 FPS required by the Oculus performance guidelines\nfor the largest of our networks (2693 genes) using both a GPU and the Oculus\nQuest standalone. We found from our interviews with biomedical researchers that\nGeneNet VR is innovative, interesting, and easy to use for novice VR users.\n  We believe affordable hardware like the Oculus Quest has a big potential for\nbiological data analysis. However, additional work is required to evaluate its\nbenefits to improve knowledge discovery for real data analysis use cases.\n  GeneNet VR is open-sourced: https://github.com/kolibrid/GeneNet-VR. A video\ndemonstrating GeneNet VR used to explore large biological networks:\nhttps://youtu.be/N4QDZiZqVNY.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.02937v1"
    },
    {
        "title": "Reinforcement Learning for Load-balanced Parallel Particle Tracing",
        "authors": [
            "Jiayi Xu",
            "Hanqi Guo",
            "Han-Wei Shen",
            "Mukund Raj",
            "Skylar W. Wurster",
            "Tom Peterka"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We explore an online reinforcement learning (RL) paradigm to dynamically\noptimize parallel particle tracing performance in distributed-memory systems.\nOur method combines three novel components: (1) a work donation algorithm, (2)\na high-order workload estimation model, and (3) a communication cost model.\nFirst, we design an RL-based work donation algorithm. Our algorithm monitors\nworkloads of processes and creates RL agents to donate data blocks and\nparticles from high-workload processes to low-workload processes to minimize\nprogram execution time. The agents learn the donation strategy on the fly based\non reward and cost functions designed to consider processes' workload changes\nand data transfer costs of donation actions. Second, we propose a workload\nestimation model, helping RL agents estimate the workload distribution of\nprocesses in future computations. Third, we design a communication cost model\nthat considers both block and particle data exchange costs, helping RL agents\nmake effective decisions with minimized communication costs. We demonstrate\nthat our algorithm adapts to different flow behaviors in large-scale fluid\ndynamics, ocean, and weather simulation data. Our algorithm improves parallel\nparticle tracing performance in terms of parallel efficiency, load balance, and\ncosts of I/O and communication for evaluations with up to 16,384 processors.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.05679v2"
    },
    {
        "title": "Cross-layer Visualization and Profiling of Network and I/O Communication\n  for HPC Clusters",
        "authors": [
            "Pouya Kousha",
            "Quentin Anthony",
            "Hari Subramoni",
            "Dhabaleswar K. Panda"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Understanding and visualizing the full-stack performance trade-offs and\ninterplay between HPC applications, MPI libraries, the communication fabric,\nand the file system is a challenging endeavor. Designing a holistic profiling\nand visualization method for HPC communication networks is challenging since\ndifferent levels of communication coexist and interact with each other on the\ncommunication fabric. A breakdown of traffic is essential to understand the\ninterplay of different layers along with the application's communication\nbehavior without losing a general view of network traffic. Unfortunately,\nexisting profiling tools are disjoint and either focus on only profiling and\nvisualizing a few levels of the HPC stack, which limits the insights they can\nprovide, or they provide extremely detailed information which necessitates a\nsteep learning curve to understand. We target our profiling tool visualization\nto provide holistic and real-time insights into HPC communication stacks.\n  In this paper, we propose and implement our visualization methods to enable\nholistic insight for representing the cross-stack metrics. Moreover, we propose\nand implement a low-overhead I/O profiling inside the communication library,\ncollect and store the profiling information, and then study the correlation and\nevaluation of I/O traffic with MPI communication using a cross-stack approach\nby INAM. Through experimental evaluations and use cases, we demonstrate novel\nbenefits of our cross-stack communication analysis in real-time to detect\nbottlenecks and understand communication performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.08329v1"
    },
    {
        "title": "Mesh Draping: Parametrization-Free Neural Mesh Transfer",
        "authors": [
            "Amir Hertz",
            "Or Perel",
            "Raja Giryes",
            "Olga Sorkine-Hornung",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Despite recent advances in geometric modeling, 3D mesh modeling still\ninvolves a considerable amount of manual labor by experts. In this paper, we\nintroduce Mesh Draping: a neural method for transferring existing mesh\nstructure from one shape to another. The method drapes the source mesh over the\ntarget geometry and at the same time seeks to preserve the carefully designed\ncharacteristics of the source mesh. At its core, our method deforms the source\nmesh using progressive positional encoding. We show that by leveraging\ngradually increasing frequencies to guide the neural optimization, we are able\nto achieve stable and high quality mesh transfer. Our approach is simple and\nrequires little user guidance, compared to contemporary surface mapping\ntechniques which rely on parametrization or careful manual tuning. Most\nimportantly, Mesh Draping is a parameterization-free method, and thus\napplicable to a variety of target shape representations, including point\nclouds, polygon soups, and non-manifold meshes. We demonstrate that the\ntransferred meshing remains faithful to the source mesh design characteristics,\nand at the same time fits the target geometry well.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.05433v1"
    },
    {
        "title": "Real-time Skeletonization for Sketch-based Modeling",
        "authors": [
            "Jing Ma",
            "Jin Wang",
            "Jituo Li",
            "Dongliang Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Skeleton creation is an important phase in the character animation pipeline.\nHowever, handcrafting skeleton takes extensive labor time and domain knowledge.\nAutomatic skeletonization provides a solution. However, most of the current\napproaches are far from real-time and lack the flexibility to control the\nskeleton complexity. In this paper, we present an efficient skeletonization\nmethod, which can be seamlessly integrated into the sketch-based modeling\nprocess in real-time. The method contains three steps: local sub-skeleton\nextraction; sub-skeleton connection; and global skeleton refinement. Firstly,\nthe local skeleton is extracted from the processed polygon stroke and forms a\nsubpart along with the sub-mesh. Then, local sub-skeletons are connected\naccording to the intersecting relationships and the modeling sequence of\nsubparts. Lastly, a global refinement method is proposed to give users\ncoarse-to-fine control on the connected skeleton. We demonstrate the\neffectiveness of our method on a variety of examples created by both novices\nand professionals.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.05805v1"
    },
    {
        "title": "Modeling 3D geometry using 1D laser distance measurements with\n  application to cylinder for visualization and evaluating surface quality",
        "authors": [
            "Qinwu Xu"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Geometric metrology includes one or two-dimensional (1D or 2D) distance or\nplane measurements, as well as the three-dimensional (3D) scanning. The 1D or\n2D measuring system is unable to obtain advanced 3D feature, while the 3D\nscanning system is relatively costly and time-consuming. Accordingly, in this\nstudy I developed a 3D geometry and surface prediction method by using 1D laser\ndistance measurements to achieve 3D features while saving cost and time. The\nmodel is based on the natural neighbor function for data interpolation and\nlinear model for extrapolation. I implemented the model to the cylinder body\nfor evaluating 3D circularity and surface quality. Results show that the model\ncould achieve reasonable accuracy in constructing the 3D geometry and surface\ndeviation using limited distance measurement (e.g. only 100 data points). It\naccurately predicts the shape of a curved dent (30.48 mm) and identified minor\ndents (less than 0.18mm in depth) which are unable to be detected by eyes and\nfinger touch. It also detects the surface corrugation (1.59 mm) and small local\nfeatures (6.35 mm) using a measurement resolution of 21.34 mm by 15.49 mm. The\npredicted 3D circularity is higher than the measured 2D circularity as\nexpected. The 3D model may be extended to other continuous geometry shapes for\nthe future study.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.14833v1"
    },
    {
        "title": "Theoretical and Empirical Analysis of a Fast Algorithm for Extracting\n  Polygons from Signed Distance Bounds",
        "authors": [
            "Nenad Markuš",
            "Mirko Sužnjević"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Recently there has been renewed interest in signed distance bound\nrepresentations due to their unique properties for 3D shape modelling. This is\nespecially the case for deep learning-based bounds. However, it is beneficial\nto work with polygons in most computer-graphics applications. Thus, in this\npaper we introduce and investigate an asymptotically fast method for\ntransforming signed distance bounds into polygon meshes. This is achieved by\ncombining the principles of sphere tracing (or ray marching) with traditional\npolygonization techniques, such as Marching Cubes. We provide theoretical and\nexperimental evidence that this approach is of the $O(N^2\\log N)$ computational\ncomplexity for a polygonization grid with $N^3$ cells. The algorithm is tested\non both a set of primitive shapes as well as signed distance bounds generated\nfrom point clouds by machine learning (and represented as neural networks).\nGiven its speed, implementation simplicity and portability, we argue that it\ncould prove useful during the modelling stage as well as in shape compression\nfor storage.\n  The code is available here: https://github.com/nenadmarkus/gridhopping\n",
        "pdf_link": "http://arxiv.org/pdf/2111.05778v2"
    },
    {
        "title": "Generalized Deployable Elastic Geodesic Grids",
        "authors": [
            "Stefan Pillwein",
            "Przemyslaw Musialski"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Given a designer created free-form surface in 3d space, our method computes a\ngrid composed of elastic elements which are completely planar and straight.\nOnly by fixing the ends of the planar elements to appropriate locations, the 2d\ngrid bends and approximates the given 3d surface. Our method is based purely on\nthe notions from differential geometry of curves and surfaces and avoids any\nphysical simulations. In particular, we introduce a well-defined elastic grid\nenergy functional that allows identifying networks of curves that minimize the\nbending energy and at the same time nestle to the provided input surface well.\nFurther, we generalize the concept of such grids to cases where the surface\nboundary does not need to be convex, which allows for the creation of\nsophisticated and visually pleasing shapes. The algorithm finally ensures that\nthe 2d grid is perfectly planar, making the resulting gridshells inexpensive,\neasy to fabricate, transport, assemble, and finally also to deploy.\nAdditionally, since the whole structure is pre-strained, it also comes with\nload-bearing capabilities. We evaluate our method using physical simulation and\nwe also provide a full fabrication pipeline for desktop-size models and present\nmultiple examples of surfaces with elliptic and hyperbolic curvature regions.\nOur method is meant as a tool for quick prototyping for designers, architects,\nand engineers since it is very fast and results can be obtained in a matter of\nseconds.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.08883v1"
    },
    {
        "title": "LVAC: Learned Volumetric Attribute Compression for Point Clouds using\n  Coordinate Based Networks",
        "authors": [
            "Berivan Isik",
            "Philip A. Chou",
            "Sung Jin Hwang",
            "Nick Johnston",
            "George Toderici"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We consider the attributes of a point cloud as samples of a vector-valued\nvolumetric function at discrete positions. To compress the attributes given the\npositions, we compress the parameters of the volumetric function. We model the\nvolumetric function by tiling space into blocks, and representing the function\nover each block by shifts of a coordinate-based, or implicit, neural network.\nInputs to the network include both spatial coordinates and a latent vector per\nblock. We represent the latent vectors using coefficients of the\nregion-adaptive hierarchical transform (RAHT) used in the MPEG geometry-based\npoint cloud codec G-PCC. The coefficients, which are highly compressible, are\nrate-distortion optimized by back-propagation through a rate-distortion\nLagrangian loss in an auto-decoder configuration. The result outperforms RAHT\nby 2--4 dB. This is the first work to compress volumetric functions represented\nby local coordinate-based neural networks. As such, we expect it to be\napplicable beyond point clouds, for example to compression of high-resolution\nneural radiance fields.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.08988v1"
    },
    {
        "title": "FaceAtlasAR: Atlas of Facial Acupuncture Points in Augmented Reality",
        "authors": [
            "Menghe Zhang",
            "Jurgen Schulze",
            "Dong Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Acupuncture is a technique in which practitioners stimulate specific points\non the body. Those points, called acupuncture points (or acupoints),\nanatomically define areas on the skin relative to specific landmarks on the\nbody. However, mapping the acupoints to individuals could be challenging for\ninexperienced acupuncturists. In this project, we proposed a system to localize\nand visualize facial acupoints for individuals in an augmented reality (AR)\ncontext. This system combines a face alignment model and a hair segmentation\nmodel to provide dense reference points for acupoints localization in real-time\n(60FPS). The localization process takes the proportional bone (B-cun or\nskeletal) measurement method, which is commonly operated by specialists;\nhowever, in the real practice, operators sometimes find it inaccurate due to\nskill-related error. With this system, users, even without any skills, can\nlocate the facial acupoints as a part of the self-training or self-treatment\nprocess.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.14755v2"
    },
    {
        "title": "The Shape Part Slot Machine: Contact-based Reasoning for Generating 3D\n  Shapes from Parts",
        "authors": [
            "Kai Wang",
            "Paul Guerrero",
            "Vladimir Kim",
            "Siddhartha Chaudhuri",
            "Minhyuk Sung",
            "Daniel Ritchie"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  We present the Shape Part Slot Machine, a new method for assembling novel 3D\nshapes from existing parts by performing contact-based reasoning. Our method\nrepresents each shape as a graph of ``slots,'' where each slot is a region of\ncontact between two shape parts. Based on this representation, we design a\ngraph-neural-network-based model for generating new slot graphs and retrieving\ncompatible parts, as well as a gradient-descent-based optimization scheme for\nassembling the retrieved parts into a complete shape that respects the\ngenerated slot graph. This approach does not require any semantic part labels;\ninterestingly, it also does not require complete part geometries -- reasoning\nabout the slots proves sufficient to generate novel, high-quality 3D shapes. We\ndemonstrate that our method generates shapes that outperform existing\nmodeling-by-assembly approaches regarding quality, diversity, and structural\ncomplexity.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.00584v2"
    },
    {
        "title": "Learning Body-Aware 3D Shape Generative Models",
        "authors": [
            "Bryce Blinn",
            "Alexander Ding",
            "R. Kenny Jones",
            "Manolis Savva",
            "Srinath Sridhar",
            "Daniel Ritchie"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The shape of many objects in the built environment is dictated by their\nrelationships to the human body: how will a person interact with this object?\nExisting data-driven generative models of 3D shapes produce plausible objects\nbut do not reason about the relationship of those objects to the human body. In\nthis paper, we learn body-aware generative models of 3D shapes. Specifically,\nwe train generative models of chairs, an ubiquitous shape category, which can\nbe conditioned on a given body shape or sitting pose. The\nbody-shape-conditioned models produce chairs which will be comfortable for a\nperson with the given body shape; the pose-conditioned models produce chairs\nwhich accommodate the given sitting pose. To train these models, we define a\n\"sitting pose matching\" metric and a novel \"sitting comfort\" metric.\nCalculating these metrics requires an expensive optimization to sit the body\ninto the chair, which is too slow to be used as a loss function for training a\ngenerative model. Thus, we train neural networks to efficiently approximate\nthese metrics. We use our approach to train three body-aware generative shape\nmodels: a structured part-based generator, a point cloud generator, and an\nimplicit surface generator. In all cases, our approach produces models which\nadapt their output chair shapes to input human body specifications.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.07022v3"
    },
    {
        "title": "Meshless Monte Carlo Radiation Transfer Method for Curved Geometries\n  using Signed Distance Functions",
        "authors": [
            "Lewis McMillan",
            "Graham D. Bruce",
            "Kishan Dholakia"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Significance: Monte Carlo radiation transfer (MCRT) is the gold standard of\nmodeling light transport in turbid media. Typical MCRT models use voxels or\nmeshes to approximate experimental geometry. A voxel based geometry does not\nallow for the accurate modeling of smooth curved surfaces, such as may be found\nin biological systems or food and drink packaging.\n  Aim: We present our algorithm which we term signedMCRT (sMCRT), a new\ngeometry-based method which uses signed distance functions (SDF) to represent\nthe geometry of the model. SDFs are capable of modeling smooth curved surfaces\naccurately whilst also modeling complex geometries.\n  Approach: We show that using SDFs to represent the problem's geometry is more\naccurate and can be faster than voxel based methods. sMCRT, can easily be\nincorporated into existing voxel based models.\n  Results: sMCRT is validated against theoretical expressions, and other voxel\nbased MCRT codes. We show that sMCRT can accurately model arbitrary complex\ngeometries such as microvascular vessel network using SDFs. In comparison to\nthe current state-of-the-art in MCRT methods specifically for curved surfaces,\nsMCRT is up-to forty-five times more accurate.\n  Conclusions: sMCRT is a highly accurate, fast MCRT method that outperforms\ncomparable voxel based models due to its ability to model smooth curved\nsurfaces. sMCRT is up-to three times faster than a voxel model for equivalent\nscenarios. sMCRT is publicly available at\nhttps://github.com/lewisfish/signedMCRT\n",
        "pdf_link": "http://arxiv.org/pdf/2112.08035v1"
    },
    {
        "title": "Move As You Like: Image Animation in E-Commerce Scenario",
        "authors": [
            "Borun Xu",
            "Biao Wang",
            "Jiale Tao",
            "Tiezheng Ge",
            "Yuning Jiang",
            "Wen Li",
            "Lixin Duan"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Creative image animations are attractive in e-commerce applications, where\nmotion transfer is one of the import ways to generate animations from static\nimages. However, existing methods rarely transfer motion to objects other than\nhuman body or human face, and even fewer apply motion transfer in practical\nscenarios. In this work, we apply motion transfer on the Taobao product images\nin real e-commerce scenario to generate creative animations, which are more\nattractive than static images and they will bring more benefits. We animate the\nTaobao products of dolls, copper running horses and toy dinosaurs based on\nmotion transfer method for demonstration.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.13647v1"
    },
    {
        "title": "NeuralMLS: Geometry-Aware Control Point Deformation",
        "authors": [
            "Meitar Shechter",
            "Rana Hanocka",
            "Gal Metzer",
            "Raja Giryes",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce NeuralMLS, a space-based deformation technique, guided by a set\nof displaced control points. We leverage the power of neural networks to inject\nthe underlying shape geometry into the deformation parameters. The goal of our\ntechnique is to enable a realistic and intuitive shape deformation. Our method\nis built upon moving least-squares (MLS), since it minimizes a weighted sum of\nthe given control point displacements. Traditionally, the influence of each\ncontrol point on every point in space (i.e., the weighting function) is defined\nusing inverse distance heuristics. In this work, we opt to learn the weighting\nfunction, by training a neural network on the control points from a single\ninput shape, and exploit the innate smoothness of neural networks. Our\ngeometry-aware control point deformation is agnostic to the surface\nrepresentation and quality; it can be applied to point clouds or meshes,\nincluding non-manifold and disconnected surface soups. We show that our\ntechnique facilitates intuitive piecewise smooth deformations, which are well\nsuited for manufactured objects. We show the advantages of our approach\ncompared to existing surface and space-based deformation techniques, both\nquantitatively and qualitatively.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.01873v2"
    },
    {
        "title": "Two Methods for Iso-Surface Extraction from Volumetric Data and Their\n  Comparison",
        "authors": [
            "Vaclav Skala",
            "Alex Brusi"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  There are various methods for extracting iso-surfaces from volumetric data.\nMarching cubes or tetrahedra or raytracing methods are mostly used. There are\nmany specific techniques to increase speed of computation and decrease memory\nrequirements. Although a precision of iso-surface extraction is very important,\ntoo, it is not mentioned usually. A comparison of the selected methods was made\nin different aspects: iso-surface extraction process time, number of triangles\ngenerated and estimation of radius, area and volume errors based on\napproximation of a sphere. Surprisingly, experiments proved that there is no\ndirect relation between precision of extracted and human perception of the\nextracted iso-surface\n",
        "pdf_link": "http://arxiv.org/pdf/2201.03446v1"
    },
    {
        "title": "Real-Time Style Modelling of Human Locomotion via Feature-Wise\n  Transformations and Local Motion Phases",
        "authors": [
            "Ian Mason",
            "Sebastian Starke",
            "Taku Komura"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Controlling the manner in which a character moves in a real-time animation\nsystem is a challenging task with useful applications. Existing style transfer\nsystems require access to a reference content motion clip, however, in\nreal-time systems the future motion content is unknown and liable to change\nwith user input. In this work we present a style modelling system that uses an\nanimation synthesis network to model motion content based on local motion\nphases. An additional style modulation network uses feature-wise\ntransformations to modulate style in real-time. To evaluate our method, we\ncreate and release a new style modelling dataset, 100STYLE, containing over 4\nmillion frames of stylised locomotion data in 100 different styles that present\na number of challenges for existing systems. To model these styles, we extend\nthe local phase calculation with a contact-free formulation. In comparison to\nother methods for real-time style modelling, we show our system is more robust\nand efficient in its style representation while improving motion quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.04439v1"
    },
    {
        "title": "DSNet: Dynamic Skin Deformation Prediction by Recurrent Neural Network",
        "authors": [
            "Hyewon Seo",
            "Kaifeng Zou",
            "Frederic Cordier"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Skin dynamics contributes to the enriched realism of human body models in\nrendered scenes. Traditional methods rely on physics-based simulations to\naccurately reproduce the dynamic behavior of soft tissues. Due to the model\ncomplexity and thus the heavy computation, however, they do not directly offer\npractical solutions to domains where real-time performance is desirable. The\nquality shapes obtained by physics-based simulations are not fully exploited by\nexample-based or more recent datadriven methods neither, with most of them\nhaving focused on the modeling of static skin shapes by leveraging quality\ndata. To address these limitations, we present a learningbased method for\ndynamic skin deformation. At the core of our work is a recurrent neural network\nthat learns to predict the nonlinear, dynamics-dependent shape change over time\nfrom pre-existing mesh deformation sequence data. Our network also learns to\npredict the variation of skin dynamics across different individuals with\nvarying body shapes. After training the network delivers realistic,\nhigh-quality skin dynamics that is specific to a person in a real-time course.\nWe obtain results that significantly saves the computational time, while\nmaintaining comparable prediction quality compared to state-of-the-art results.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.07660v1"
    },
    {
        "title": "Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch",
        "authors": [
            "Zhi Deng",
            "Yang Liu",
            "Hao Pan",
            "Wassim Jabi",
            "Juyong Zhang",
            "Bailin Deng"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The freeform architectural modeling process often involves two important\nstages: concept design and digital modeling. In the first stage, architects\nusually sketch the overall 3D shape and the panel layout on a physical or\ndigital paper briefly. In the second stage, a digital 3D model is created using\nthe sketch as a reference. The digital model needs to incorporate geometric\nrequirements for its components, such as the planarity of panels due to\nconsideration of construction costs, which can make the modeling process more\nchallenging. In this work, we present a novel sketch-based system to bridge the\nconcept design and digital modeling of freeform roof-like shapes represented as\nplanar quadrilateral (PQ) meshes. Our system allows the user to sketch the\nsurface boundary and contour lines under axonometric projection and supports\nthe sketching of occluded regions. In addition, the user can sketch feature\nlines to provide directional guidance to the PQ mesh layout. Given the 2D\nsketch input, we propose a deep neural network to infer in real-time the\nunderlying surface shape along with a dense conjugate direction field, both of\nwhich are used to extract the final PQ mesh. To train and validate our network,\nwe generate a large synthetic dataset that mimics architect sketching of\nfreeform quadrilateral patches. The effectiveness and usability of our system\nare demonstrated with quantitative and qualitative evaluation as well as user\nstudies.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.09367v4"
    },
    {
        "title": "SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation",
        "authors": [
            "Amir Hertz",
            "Or Perel",
            "Raja Giryes",
            "Olga Sorkine-Hornung",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Neural implicit fields are quickly emerging as an attractive representation\nfor learning based techniques. However, adopting them for 3D shape modeling and\nediting is challenging. We introduce a method for $\\mathbf{E}$diting\n$\\mathbf{I}$mplicit $\\mathbf{S}$hapes $\\mathbf{T}$hrough $\\mathbf{P}$art\n$\\mathbf{A}$ware $\\mathbf{G}$enera$\\mathbf{T}$ion, permuted in short as\nSPAGHETTI. Our architecture allows for manipulation of implicit shapes by means\nof transforming, interpolating and combining shape segments together, without\nrequiring explicit part supervision. SPAGHETTI disentangles shape part\nrepresentation into extrinsic and intrinsic geometric information. This\ncharacteristic enables a generative framework with part-level control. The\nmodeling capabilities of SPAGHETTI are demonstrated using an interactive\ngraphical interface, where users can directly edit neural implicit shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.13168v1"
    },
    {
        "title": "Grid-Free Monte Carlo for PDEs with Spatially Varying Coefficients",
        "authors": [
            "Rohan Sawhney",
            "Dario Seyb",
            "Wojciech Jarosz",
            "Keenan Crane"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Partial differential equations (PDEs) with spatially-varying coefficients\narise throughout science and engineering, modeling rich heterogeneous material\nbehavior. Yet conventional PDE solvers struggle with the immense complexity\nfound in nature, since they must first discretize the problem -- leading to\nspatial aliasing, and global meshing/sampling that is costly and error-prone.\nWe describe a method that approximates neither the domain geometry, the problem\ndata, nor the solution space, providing the exact solution (in expectation)\neven for problems with extremely detailed geometry and intricate coefficients.\nOur main contribution is to extend the walk on spheres (WoS) algorithm from\nconstant- to variable-coefficient problems, by drawing on techniques from\nvolumetric rendering. In particular, an approach inspired by null-scattering\nyields unbiased Monte Carlo estimators for a large class of 2nd-order elliptic\nPDEs, which share many attractive features with Monte Carlo rendering: no\nmeshing, trivial parallelism, and the ability to evaluate the solution at any\npoint without solving a global system of equations.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.13240v1"
    },
    {
        "title": "LayoutEnhancer: Generating Good Indoor Layouts from Imperfect Data",
        "authors": [
            "Kurt Leimer",
            "Paul Guerrero",
            "Tomer Weiss",
            "Przemyslaw Musialski"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We address the problem of indoor layout synthesis, which is a topic of\ncontinuing research interest in computer graphics. The newest works made\nsignificant progress using data-driven generative methods; however, these\napproaches rely on suitable datasets. In practice, desirable layout properties\nmay not exist in a dataset, for instance, specific expert knowledge can be\nmissing in the data. We propose a method that combines expert knowledge, for\nexample, knowledge about ergonomics, with a data-driven generator based on the\npopular Transformer architecture. The knowledge is given as differentiable\nscalar functions, which can be used both as weights or as additional terms in\nthe loss function. Using this knowledge, the synthesized layouts can be biased\nto exhibit desirable properties, even if these properties are not present in\nthe dataset. Our approach can also alleviate problems of lack of data and\nimperfections in the data. Our work aims to improve generative machine learning\nfor modeling and provide novel tools for designers and amateurs for the problem\nof interior layout creation.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.00185v2"
    },
    {
        "title": "Development of a neural network to recognize standards and features from\n  3D CAD models",
        "authors": [
            "Alexander Neb",
            "Iyed Briki",
            "Raoul Schoenhof"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Focus of this work is to recognize standards and further features directly\nfrom 3D CAD models. For this reason, a neural network was trained to recognize\nnine classes of machine elements. After the system identified a part as a\nstandard, like a hexagon head screw after the DIN EN ISO 8676, it accesses the\ngeometrical information of the CAD system via the Application Programming\nInterface (API). In the API, the system searches for necessary information to\ndescribe the part appropriately. Based on this information standardized parts\ncan be recognized in detail and supplemented with further information.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.00573v1"
    },
    {
        "title": "Hair Color Digitization through Imaging and Deep Inverse Graphics",
        "authors": [
            "Robin Kips",
            "Panagiotis-Alexandros Bokaris",
            "Matthieu Perrot",
            "Pietro Gori",
            "Isabelle Bloch"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Hair appearance is a complex phenomenon due to hair geometry and how the\nlight bounces on different hair fibers. For this reason, reproducing a specific\nhair color in a rendering environment is a challenging task that requires\nmanual work and expert knowledge in computer graphics to tune the result\nvisually. While current hair capture methods focus on hair shape estimation\nmany applications could benefit from an automated method for capturing the\nappearance of a physical hair sample, from augmented/virtual reality to hair\ndying development. Building on recent advances in inverse graphics and material\ncapture using deep neural networks, we introduce a novel method for hair color\ndigitization. Our proposed pipeline allows capturing the color appearance of a\nphysical hair sample and renders synthetic images of hair with a similar\nappearance, simulating different hair styles and/or lighting environments.\nSince rendering realistic hair images requires path-tracing rendering, the\nconventional inverse graphics approach based on differentiable rendering is\nuntractable. Our method is based on the combination of a controlled imaging\ndevice, a path-tracing renderer, and an inverse graphics model based on\nself-supervised machine learning, which does not require to use differentiable\nrendering to be trained. We illustrate the performance of our hair digitization\nmethod on both real and synthetic images and show that our approach can\naccurately capture and render hair color.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.03723v1"
    },
    {
        "title": "Towards automated Capability Assessment leveraging Deep Learning",
        "authors": [
            "Raoul Schönhof",
            "Manuel Fechter"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Aiming for a higher economic efficiency in manufacturing, an increased degree\nof automation is a key enabler. However, assessing the technical feasibility of\nan automated assembly solution for a dedicated process is difficult and often\ndetermined by the geometry of the given product parts. Among others, decisive\ncriterions of the automation feasibility are the ability to separate and\nisolate single parts or the capability of component self-alignment in final\nposition. To assess the feasibility, a questionnaire based evaluation scheme\nhas been developed and applied by Fraunhofer researchers. However, the results\nstrongly depend on the implicit knowledge and experience of the single engineer\nperforming the assessment. This paper presents NeuroCAD, a software tool that\nautomates the assessment using voxelization techniques. The approach enables\nthe assessment of abstract and production relevant geometries features through\ndeep-learning based on CAD files.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.04051v1"
    },
    {
        "title": "Unsupervised HDR Imaging: What Can Be Learned from a Single 8-bit Video?",
        "authors": [
            "Francesco Banterle",
            "Demetris Marnerides",
            "Kurt Debattista",
            "Thomas Bashford-Rogers"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Recently, Deep Learning-based methods for inverse tone-mapping standard\ndynamic range (SDR) images to obtain high dynamic range (HDR) images have\nbecome very popular. These methods manage to fill over-exposed areas\nconvincingly both in terms of details and dynamic range. Typically, these\nmethods, to be effective, need to learn from large datasets and to transfer\nthis knowledge to the network weights. In this work, we tackle this problem\nfrom a completely different perspective. What can we learn from a single SDR\nvideo? With the presented zero-shot approach, we show that, in many cases, a\nsingle SDR video is sufficient to be able to generate an HDR video of the same\nquality or better than other state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05522v1"
    },
    {
        "title": "CLIPasso: Semantically-Aware Object Sketching",
        "authors": [
            "Yael Vinker",
            "Ehsan Pajouheshgar",
            "Jessica Y. Bo",
            "Roman Christian Bachmann",
            "Amit Haim Bermano",
            "Daniel Cohen-Or",
            "Amir Zamir",
            "Ariel Shamir"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Abstraction is at the heart of sketching due to the simple and minimal nature\nof line drawings. Abstraction entails identifying the essential visual\nproperties of an object or scene, which requires semantic understanding and\nprior knowledge of high-level concepts. Abstract depictions are therefore\nchallenging for artists, and even more so for machines. We present CLIPasso, an\nobject sketching method that can achieve different levels of abstraction,\nguided by geometric and semantic simplifications. While sketch generation\nmethods often rely on explicit sketch datasets for training, we utilize the\nremarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill\nsemantic concepts from sketches and images alike. We define a sketch as a set\nof B\\'ezier curves and use a differentiable rasterizer to optimize the\nparameters of the curves directly with respect to a CLIP-based perceptual loss.\nThe abstraction degree is controlled by varying the number of strokes. The\ngenerated sketches demonstrate multiple levels of abstraction while maintaining\nrecognizability, underlying structure, and essential visual components of the\nsubject drawn.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05822v2"
    },
    {
        "title": "Online Motion Style Transfer for Interactive Character Control",
        "authors": [
            "Yingtian Tang",
            "Jiangtao Liu",
            "Cheng Zhou",
            "Tingguang Li"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Motion style transfer is highly desired for motion generation systems for\ngaming. Compared to its offline counterpart, the research on online motion\nstyle transfer under interactive control is limited. In this work, we propose\nan end-to-end neural network that can generate motions with different styles\nand transfer motion styles in real-time under user control. Our approach\neliminates the use of handcrafted phase features, and could be easily trained\nand directly deployed in game systems. In the experiment part, we evaluate our\napproach from three aspects that are essential for industrial game design:\naccuracy, flexibility, and variety, and our model performs a satisfying result.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.16393v1"
    },
    {
        "title": "Noise-based Enhancement for Foveated Rendering",
        "authors": [
            "Taimoor Tariq",
            "Cara Tursun",
            "Piotr Didyk"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Human visual sensitivity to spatial details declines towards the periphery.\nNovel image synthesis techniques, so-called foveated rendering, exploit this\nobservation and reduce the spatial resolution of synthesized images for the\nperiphery, avoiding the synthesis of high-spatial-frequency details that are\ncostly to generate but not perceived by a viewer. However, contemporary\ntechniques do not make a clear distinction between the range of spatial\nfrequencies that must be reproduced and those that can be omitted. For a given\neccentricity, there is a range of frequencies that are detectable but not\nresolvable. While the accurate reproduction of these frequencies is not\nrequired, an observer can detect their absence if completely omitted. We use\nthis observation to improve the performance of existing foveated rendering\ntechniques. We demonstrate that this specific range of frequencies can be\nefficiently replaced with procedural noise whose parameters are carefully tuned\nto image content and human perception. Consequently, these frequencies do not\nhave to be synthesized during rendering, allowing more aggressive foveation,\nand they can be replaced by noise generated in a less expensive post-processing\nstep, leading to improved performance of the rendering system. Our main\ncontribution is a perceptually-inspired technique for deriving the parameters\nof the noise required for the enhancement and its calibration. The method\noperates on rendering output and runs at rates exceeding 200FPS at 4K\nresolution, making it suitable for integration with real-time foveated\nrendering systems for VR and AR devices. We validate our results and compare\nthem to the existing contrast enhancement technique in user experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.04455v1"
    },
    {
        "title": "OutCast: Outdoor Single-image Relighting with Cast Shadows",
        "authors": [
            "David Griffiths",
            "Tobias Ritschel",
            "Julien Philip"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose a relighting method for outdoor images. Our method mainly focuses\non predicting cast shadows in arbitrary novel lighting directions from a single\nimage while also accounting for shading and global effects such the sun light\ncolor and clouds. Previous solutions for this problem rely on reconstructing\noccluder geometry, e.g. using multi-view stereo, which requires many images of\nthe scene. Instead, in this work we make use of a noisy off-the-shelf\nsingle-image depth map estimation as a source of geometry. Whilst this can be a\ngood guide for some lighting effects, the resulting depth map quality is\ninsufficient for directly ray-tracing the shadows. Addressing this, we propose\na learned image space ray-marching layer that converts the approximate depth\nmap into a deep 3D representation that is fused into occlusion queries using a\nlearned traversal. Our proposed method achieves, for the first time,\nstate-of-the-art relighting results, with only a single image as input. For\nsupplementary material visit our project page at:\nhttps://dgriffiths.uk/outcast.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.09341v1"
    },
    {
        "title": "Designing Perceptual Puzzles by Differentiating Probabilistic Programs",
        "authors": [
            "Kartik Chandra",
            "Tzu-Mao Li",
            "Joshua Tenenbaum",
            "Jonathan Ragan-Kelley"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We design new visual illusions by finding \"adversarial examples\" for\nprincipled models of human perception -- specifically, for probabilistic\nmodels, which treat vision as Bayesian inference. To perform this search\nefficiently, we design a differentiable probabilistic programming language,\nwhose API exposes MCMC inference as a first-class differentiable function. We\ndemonstrate our method by automatically creating illusions for three features\nof human vision: color constancy, size constancy, and face perception.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12301v1"
    },
    {
        "title": "Learning to Get Up",
        "authors": [
            "Tianxin Tao",
            "Matthew Wilson",
            "Ruiyu Gou",
            "Michiel van de Panne"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Getting up from an arbitrary fallen state is a basic human skill. Existing\nmethods for learning this skill often generate highly dynamic and erratic\nget-up motions, which do not resemble human get-up strategies, or are based on\ntracking recorded human get-up motions. In this paper, we present a staged\napproach using reinforcement learning, without recourse to motion capture data.\nThe method first takes advantage of a strong character model, which facilitates\nthe discovery of solution modes. A second stage then learns to adapt the\ncontrol policy to work with progressively weaker versions of the character.\nFinally, a third stage learns control policies that can reproduce the weaker\nget-up motions at much slower speeds. We show that across multiple runs, the\nmethod can discover a diverse variety of get-up strategies, and execute them at\na variety of speeds. The results usually produce policies that use a final\nstand-up strategy that is common to the recovery motions seen from all initial\nstates. However, we also find policies for which different strategies are seen\nfor prone and supine initial fallen states. The learned get-up control\nstrategies often have significant static stability, i.e., they can be paused at\na variety of points during the get-up motion. We further test our method on\nnovel constrained scenarios, such as having a leg and an arm in a cast.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00307v2"
    },
    {
        "title": "Predicting Loose-Fitting Garment Deformations Using Bone-Driven Motion\n  Networks",
        "authors": [
            "Xiaoyu Pan",
            "Jiaming Mai",
            "Xinwei Jiang",
            "Dongxue Tang",
            "Jingxiang Li",
            "Tianjia Shao",
            "Kun Zhou",
            "Xiaogang Jin",
            "Dinesh Manocha"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a learning algorithm that uses bone-driven motion networks to\npredict the deformation of loose-fitting garment meshes at interactive rates.\nGiven a garment, we generate a simulation database and extract virtual bones\nfrom simulated mesh sequences using skin decomposition. At runtime, we\nseparately compute low- and high-frequency deformations in a sequential manner.\nThe low-frequency deformations are predicted by transferring body motions to\nvirtual bones' motions, and the high-frequency deformations are estimated\nleveraging the global information of virtual bones' motions and local\ninformation extracted from low-frequency meshes. In addition, our method can\nestimate garment deformations caused by variations of the simulation parameters\n(e.g., fabric's bending stiffness) using an RBF kernel ensembling trained\nnetworks for different sets of simulation parameters. Through extensive\ncomparisons, we show that our method outperforms state-of-the-art methods in\nterms of prediction accuracy of mesh deformations by about 20% in RMSE and 10%\nin Hausdorff distance and STED. The code and data are available at\nhttps://github.com/non-void/VirtualBones.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.01355v3"
    },
    {
        "title": "ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically\n  Simulated Characters",
        "authors": [
            "Xue Bin Peng",
            "Yunrong Guo",
            "Lina Halper",
            "Sergey Levine",
            "Sanja Fidler"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The incredible feats of athleticism demonstrated by humans are made possible\nin part by a vast repertoire of general-purpose motor skills, acquired through\nyears of practice and experience. These skills not only enable humans to\nperform complex tasks, but also provide powerful priors for guiding their\nbehaviors when learning new tasks. This is in stark contrast to what is common\npractice in physics-based character animation, where control policies are most\ntypically trained from scratch for each task. In this work, we present a\nlarge-scale data-driven framework for learning versatile and reusable skill\nembeddings for physically simulated characters. Our approach combines\ntechniques from adversarial imitation learning and unsupervised reinforcement\nlearning to develop skill embeddings that produce life-like behaviors, while\nalso providing an easy to control representation for use on new downstream\ntasks. Our models can be trained using large datasets of unstructured motion\nclips, without requiring any task-specific annotation or segmentation of the\nmotion data. By leveraging a massively parallel GPU-based simulator, we are\nable to train skill embeddings using over a decade of simulated experiences,\nenabling our model to learn a rich and versatile repertoire of skills. We show\nthat a single pre-trained model can be effectively applied to perform a diverse\nset of new tasks. Our system also allows users to specify tasks through simple\nreward functions, and the skill embedding then enables the character to\nautomatically synthesize complex and naturalistic strategies in order to\nachieve the task objectives.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.01906v2"
    },
    {
        "title": "Time-multiplexed Neural Holography: A flexible framework for holographic\n  near-eye displays with fast heavily-quantized spatial light modulators",
        "authors": [
            "Suyeon Choi",
            "Manu Gopakumar",
            " Yifan",
            " Peng",
            "Jonghyun Kim",
            "Matthew O'Toole",
            "Gordon Wetzstein"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Holographic near-eye displays offer unprecedented capabilities for virtual\nand augmented reality systems, including perceptually important focus cues.\nAlthough artificial intelligence--driven algorithms for computer-generated\nholography (CGH) have recently made much progress in improving the image\nquality and synthesis efficiency of holograms, these algorithms are not\ndirectly applicable to emerging phase-only spatial light modulators (SLM) that\nare extremely fast but offer phase control with very limited precision. The\nspeed of these SLMs offers time multiplexing capabilities, essentially enabling\npartially-coherent holographic display modes. Here we report advances in\ncamera-calibrated wave propagation models for these types of holographic\nnear-eye displays and we develop a CGH framework that robustly optimizes the\nheavily quantized phase patterns of fast SLMs. Our framework is flexible in\nsupporting runtime supervision with different types of content, including 2D\nand 2.5D RGBD images, 3D focal stacks, and 4D light fields. Using our\nframework, we demonstrate state-of-the-art results for all of these scenarios\nin simulation and experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.02367v1"
    },
    {
        "title": "GANimator: Neural Motion Synthesis from a Single Sequence",
        "authors": [
            "Peizhuo Li",
            "Kfir Aberman",
            "Zihan Zhang",
            "Rana Hanocka",
            "Olga Sorkine-Hornung"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present GANimator, a generative model that learns to synthesize novel\nmotions from a single, short motion sequence. GANimator generates motions that\nresemble the core elements of the original motion, while simultaneously\nsynthesizing novel and diverse movements. Existing data-driven techniques for\nmotion synthesis require a large motion dataset which contains the desired and\nspecific skeletal structure. By contrast, GANimator only requires training on a\nsingle motion sequence, enabling novel motion synthesis for a variety of\nskeletal structures e.g., bipeds, quadropeds, hexapeds, and more. Our framework\ncontains a series of generative and adversarial neural networks, each\nresponsible for generating motions in a specific frame rate. The framework\nprogressively learns to synthesize motion from random noise, enabling\nhierarchical control over the generated motion content across varying levels of\ndetail. We show a number of applications, including crowd simulation, key-frame\nediting, style transfer, and interactive control, which all learn from a single\ninput sequence. Code and data for this paper are at\nhttps://peizhuoli.github.io/ganimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.02625v1"
    },
    {
        "title": "Neural Jacobian Fields: Learning Intrinsic Mappings of Arbitrary Meshes",
        "authors": [
            "Noam Aigerman",
            "Kunal Gupta",
            "Vladimir G. Kim",
            "Siddhartha Chaudhuri",
            "Jun Saito",
            "Thibault Groueix"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This paper introduces a framework designed to accurately predict piecewise\nlinear mappings of arbitrary meshes via a neural network, enabling training and\nevaluating over heterogeneous collections of meshes that do not share a\ntriangulation, as well as producing highly detail-preserving maps whose\naccuracy exceeds current state of the art. The framework is based on reducing\nthe neural aspect to a prediction of a matrix for a single given point,\nconditioned on a global shape descriptor. The field of matrices is then\nprojected onto the tangent bundle of the given mesh, and used as candidate\njacobians for the predicted map. The map is computed by a standard Poisson\nsolve, implemented as a differentiable layer with cached pre-factorization for\nefficient training. This construction is agnostic to the triangulation of the\ninput, thereby enabling applications on datasets with varying triangulations.\nAt the same time, by operating in the intrinsic gradient domain of each\nindividual mesh, it allows the framework to predict highly-accurate mappings.\nWe validate these properties by conducting experiments over a broad range of\nscenarios, from semantic ones such as morphing, registration, and deformation\ntransfer, to optimization-based ones, such as emulating elastic deformations\nand contact correction, as well as being the first work, to our knowledge, to\ntackle the task of learning to compute UV parameterizations of arbitrary\nmeshes. The results exhibit the high accuracy of the method as well as its\nversatility, as it is readily applied to the above scenarios without any\nchanges to the framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.02904v1"
    },
    {
        "title": "Approximate Convex Decomposition for 3D Meshes with Collision-Aware\n  Concavity and Tree Search",
        "authors": [
            "Xinyue Wei",
            "Minghua Liu",
            "Zhan Ling",
            "Hao Su"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Approximate convex decomposition aims to decompose a 3D shape into a set of\nalmost convex components, whose convex hulls can then be used to represent the\ninput shape. It thus enables efficient geometry processing algorithms\nspecifically designed for convex shapes and has been widely used in game\nengines, physics simulations, and animation. While prior works can capture the\nglobal structure of input shapes, they may fail to preserve fine-grained\ndetails (e.g., filling a toaster's slots), which are critical for retaining the\nfunctionality of objects in interactive environments. In this paper, we propose\na novel method that addresses the limitations of existing approaches from three\nperspectives: (a) We introduce a novel collision-aware concavity metric that\nexamines the distance between a shape and its convex hull from both the\nboundary and the interior. The proposed concavity preserves collision\nconditions and is more robust to detect various approximation errors. (b) We\ndecompose shapes by directly cutting meshes with 3D planes. It ensures\ngenerated convex hulls are intersection-free and avoids voxelization errors.\n(c) Instead of using a one-step greedy strategy, we propose employing a\nmulti-step tree search to determine the cutting planes, which leads to a\nglobally better solution and avoids unnecessary cuttings. Through extensive\nevaluation on a large-scale articulated object dataset, we show that our method\ngenerates decompositions closer to the original shape with fewer components. It\nthus supports delicate and efficient object interaction in downstream\napplications. We will release our implementation to facilitate future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.02961v1"
    },
    {
        "title": "Conditional Vector Graphics Generation for Music Cover Images",
        "authors": [
            "Valeria Efimova",
            "Ivan Jarsky",
            "Ilya Bizyaev",
            "Andrey Filchenkov"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Generative Adversarial Networks (GAN) have motivated a rapid growth of the\ndomain of computer image synthesis. As almost all the existing image synthesis\nalgorithms consider an image as a pixel matrix, the high-resolution image\nsynthesis is complicated.A good alternative can be vector images. However, they\nbelong to the highly sophisticated parametric space, which is a restriction for\nsolving the task of synthesizing vector graphics by GANs. In this paper, we\nconsider a specific application domain that softens this restriction\ndramatically allowing the usage of vector image synthesis.\n  Music cover images should meet the requirements of Internet streaming\nservices and printing standards, which imply high resolution of graphic\nmaterials without any additional requirements on the content of such images.\nExisting music cover image generation services do not analyze tracks\nthemselves; however, some services mostly consider only genre tags. To generate\nmusic covers as vector images that reflect the music and consist of simple\ngeometric objects, we suggest a GAN-based algorithm called CoverGAN. The\nassessment of resulting images is based on their correspondence to the music\ncompared with AttnGAN and DALL-E text-to-image generation according to title or\nlyrics. Moreover, the significance of the patterns found by CoverGAN has been\nevaluated in terms of the correspondence of the generated cover images to the\nmusical tracks. Listeners evaluate the music covers generated by the proposed\nalgorithm as quite satisfactory and corresponding to the tracks. Music cover\nimages generation code and demo are available at\nhttps://github.com/IzhanVarsky/CoverGAN.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07301v1"
    },
    {
        "title": "Hidden Degrees of Freedom in Implicit Vortex Filaments",
        "authors": [
            "Sadashige Ishida",
            "Chris Wojtan",
            "Albert Chern"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This paper presents a new representation of curve dynamics, with applications\nto vortex filaments in fluid dynamics. Instead of representing these filaments\nwith explicit curve geometry and Lagrangian equations of motion, we represent\ncurves implicitly with a new co-dimensional 2 level set description. Our\nimplicit representation admits several redundant mathematical degrees of\nfreedom in both the configuration and the dynamics of the curves, which can be\ntailored specifically to improve numerical robustness, in contrast to naive\napproaches for implicit curve dynamics that suffer from overwhelming numerical\nstability problems. Furthermore, we note how these hidden degrees of freedom\nperfectly map to a Clebsch representation in fluid dynamics. Motivated by these\nobservations, we introduce untwisted level set functions and non-swirling\ndynamics which successfully regularize sources of numerical instability,\nparticularly in the twisting modes around curve filaments. A consequence is a\nnovel simulation method which produces stable dynamics for large numbers of\ninteracting vortex filaments and effortlessly handles topological changes and\nre-connection events.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.02008v5"
    },
    {
        "title": "Physics Informed Neural Fields for Smoke Reconstruction with Sparse Data",
        "authors": [
            "Mengyu Chu",
            "Lingjie Liu",
            "Quan Zheng",
            "Erik Franz",
            "Hans-Peter Seidel",
            "Christian Theobalt",
            "Rhaleb Zayer"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  High-fidelity reconstruction of fluids from sparse multiview RGB videos\nremains a formidable challenge due to the complexity of the underlying physics\nas well as complex occlusion and lighting in captures. Existing solutions\neither assume knowledge of obstacles and lighting, or only focus on simple\nfluid scenes without obstacles or complex lighting, and thus are unsuitable for\nreal-world scenes with unknown lighting or arbitrary obstacles. We present the\nfirst method to reconstruct dynamic fluid by leveraging the governing physics\n(ie, Navier -Stokes equations) in an end-to-end optimization from sparse videos\nwithout taking lighting conditions, geometry information, or boundary\nconditions as input. We provide a continuous spatio-temporal scene\nrepresentation using neural networks as the ansatz of density and velocity\nsolution functions for fluids as well as the radiance field for static objects.\nWith a hybrid architecture that separates static and dynamic contents, fluid\ninteractions with static obstacles are reconstructed for the first time without\nadditional geometry input or human labeling. By augmenting time-varying neural\nradiance fields with physics-informed deep learning, our method benefits from\nthe supervision of images and physical priors. To achieve robust optimization\nfrom sparse views, we introduced a layer-by-layer growing strategy to\nprogressively increase the network capacity. Using progressively growing models\nwith a new regularization term, we manage to disentangle density-color\nambiguity in radiance fields without overfitting. A pretrained\ndensity-to-velocity fluid model is leveraged in addition as the data prior to\navoid suboptimal velocity which underestimates vorticity but trivially fulfills\nphysical equations. Our method exhibits high-quality results with relaxed\nconstraints and strong flexibility on a representative set of synthetic and\nreal flow captures.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.06577v1"
    },
    {
        "title": "SHREC 2022: Fitting and recognition of simple geometric primitives on\n  point clouds",
        "authors": [
            "Chiara Romanengo",
            "Andrea Raffo",
            "Silvia Biasotti",
            "Bianca Falcidieno",
            "Vlassis Fotis",
            "Ioannis Romanelis",
            "Eleftheria Psatha",
            "Konstantinos Moustakas",
            "Ivan Sipiran",
            "Quang-Thuc Nguyen",
            "Chi-Bien Chu",
            "Khoi-Nguyen Nguyen-Ngoc",
            "Dinh-Khoi Vo",
            "Tuan-An To",
            "Nham-Tan Nguyen",
            "Nhat-Quynh Le-Pham",
            "Hai-Dang Nguyen",
            "Minh-Triet Tran",
            "Yifan Qie",
            "Nabil Anwer"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This paper presents the methods that have participated in the SHREC 2022\ntrack on the fitting and recognition of simple geometric primitives on point\nclouds. As simple primitives we mean the classical surface primitives derived\nfrom constructive solid geometry, i.e., planes, spheres, cylinders, cones and\ntori. The aim of the track is to evaluate the quality of automatic algorithms\nfor fitting and recognising geometric primitives on point clouds. Specifically,\nthe goal is to identify, for each point cloud, its primitive type and some\ngeometric descriptors. For this purpose, we created a synthetic dataset,\ndivided into a training set and a test set, containing segments perturbed with\ndifferent kinds of point cloud artifacts. Among the six participants to this\ntrack, two are based on direct methods, while four are either fully based on\ndeep learning or combine direct and neural approaches. The performance of the\nmethods is evaluated using various classification and approximation measures.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.07636v2"
    },
    {
        "title": "Gaussian Blue Noise",
        "authors": [
            "Abdalla G. M. Ahmed",
            "Jing Ren",
            "Peter Wonka"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Among the various approaches for producing point distributions with blue\nnoise spectrum, we argue for an optimization framework using Gaussian kernels.\nWe show that with a wise selection of optimization parameters, this approach\nattains unprecedented quality, provably surpassing the current state of the art\nattained by the optimal transport (BNOT) approach. Further, we show that our\nalgorithm scales smoothly and feasibly to high dimensions while maintaining the\nsame quality, realizing unprecedented high-quality high-dimensional blue noise\nsets. Finally, we show an extension to adaptive sampling.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.07798v1"
    },
    {
        "title": "MoDi: Unconditional Motion Synthesis from Diverse Data",
        "authors": [
            "Sigal Raab",
            "Inbal Leibovitch",
            "Peizhuo Li",
            "Kfir Aberman",
            "Olga Sorkine-Hornung",
            "Daniel Cohen-Or"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The emergence of neural networks has revolutionized the field of motion\nsynthesis. Yet, learning to unconditionally synthesize motions from a given\ndistribution remains challenging, especially when the motions are highly\ndiverse. In this work, we present MoDi -- a generative model trained in an\nunsupervised setting from an extremely diverse, unstructured and unlabeled\ndataset. During inference, MoDi can synthesize high-quality, diverse motions.\nDespite the lack of any structure in the dataset, our model yields a\nwell-behaved and highly structured latent space, which can be semantically\nclustered, constituting a strong motion prior that facilitates various\napplications including semantic editing and crowd simulation. In addition, we\npresent an encoder that inverts real motions into MoDi's natural motion\nmanifold, issuing solutions to various ill-posed challenges such as completion\nfrom prefix and spatial editing. Our qualitative and quantitative experiments\nachieve state-of-the-art results that outperform recent SOTA techniques. Code\nand trained models are available at https://sigal-raab.github.io/MoDi.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.08010v3"
    },
    {
        "title": "Towards Better User Studies in Computer Graphics and Vision",
        "authors": [
            "Zoya Bylinskii",
            "Laura Herman",
            "Aaron Hertzmann",
            "Stefanie Hutka",
            "Yile Zhang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Online crowdsourcing platforms have made it increasingly easy to perform\nevaluations of algorithm outputs with survey questions like \"which image is\nbetter, A or B?\", leading to their proliferation in vision and graphics\nresearch papers. Results of these studies are often used as quantitative\nevidence in support of a paper's contributions. On the one hand we argue that,\nwhen conducted hastily as an afterthought, such studies lead to an increase of\nuninformative, and, potentially, misleading conclusions. On the other hand, in\nthese same communities, user research is underutilized in driving project\ndirection and forecasting user needs and reception. We call for increased\nattention to both the design and reporting of user studies in computer vision\nand graphics papers towards (1) improved replicability and (2) improved project\ndirection. Together with this call, we offer an overview of methodologies from\nuser experience research (UXR), human-computer interaction (HCI), and applied\nperception to increase exposure to the available methodologies and best\npractices. We discuss foundational user research methods (e.g., needfinding)\nthat are presently underutilized in computer vision and graphics research, but\ncan provide valuable project direction. We provide further pointers to the\nliterature for readers interested in exploring other UXR methodologies.\nFinally, we describe broader open issues and recommendations for the research\ncommunity.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.11461v3"
    },
    {
        "title": "Perspective (In)consistency of Paint by Text",
        "authors": [
            "Hany Farid"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Type \"a sea otter with a pearl earring by Johannes Vermeer\" or \"a photo of a\nteddy bear on a skateboard in Times Square\" into OpenAI's DALL-E-2\npaint-by-text synthesis engine and you will not be disappointed by the\ndelightful and eerily pertinent results. The ability to synthesize highly\nrealistic images -- with seemingly no limitation other than our imagination --\nis sure to yield many exciting and creative applications. These images are also\nlikely to pose new challenges to the photo-forensic community. Motivated by the\nfact that paint by text is not based on explicit geometric modeling, and the\nhuman visual system's often obliviousness to even glaring geometric\ninconsistencies, we provide an initial exploration of the perspective\nconsistency of DALL-E-2 synthesized images to determine if geometric-based\nforensic analyses will prove fruitful in detecting this new breed of synthetic\nmedia.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.14617v1"
    },
    {
        "title": "Rendering along the Hilbert Curve",
        "authors": [
            "Alexander Keller",
            "Carsten Wächter",
            "Nikolaus Binder"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Based on the seminal work on Array-RQMC methods and rank-1 lattice sequences\nby Pierre L'Ecuyer and collaborators, we introduce efficient deterministic\nalgorithms for image synthesis. Enumerating a low discrepancy sequence along\nthe Hilbert curve superimposed on the raster of pixels of an image, we achieve\nnoise characteristics that are desirable with respect to the human visual\nsystem, especially at very low sampling rates. As compared to the state of the\nart, our simple algorithms neither require randomization, nor costly\noptimization, nor lookup tables. We analyze correlations of space-filling\ncurves and low discrepancy sequences, and demonstrate the benefits of the new\nalgorithms in a professional, massively parallel light transport simulation and\nrendering system.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.05415v1"
    },
    {
        "title": "Display of 3D Illuminations using Flying Light Specks",
        "authors": [
            "Shahram Ghandeharizadeh"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This paper presents techniques to display 3D illuminations using Flying Light\nSpecks, FLSs. Each FLS is a miniature (hundreds of micrometers) sized drone\nwith one or more light sources to generate different colors and textures with\nadjustable brightness. It is network enabled with a processor and local\nstorage. Synchronized swarms of cooperating FLSs render illumination of virtual\nobjects in a pre-specified 3D volume, an FLS display. We present techniques to\ndisplay both static and motion illuminations. Our display techniques consider\nthe limited flight time of an FLS on a fully charged battery and the duration\nof time to charge the FLS battery. Moreover, our techniques assume failure of\nFLSs is the norm rather than an exception. We present a hardware and a software\narchitecture for an FLS-display along with a family of techniques to compute\nflight paths of FLSs for illuminations. With motion illuminations, one\ntechnique (ICF) minimizes the overall distance traveled by the FLSs\nsignificantly when compared with the other techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.08346v1"
    },
    {
        "title": "Principal Geodesic Analysis of Merge Trees (and Persistence Diagrams)",
        "authors": [
            "Mathieu Pont",
            "Jules Vidal",
            "Julien Tierny"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This paper presents a computational framework for the Principal Geodesic\nAnalysis of merge trees (MT-PGA), a novel adaptation of the celebrated\nPrincipal Component Analysis (PCA) framework [87] to the Wasserstein metric\nspace of merge trees [92]. We formulate MT-PGA computation as a constrained\noptimization problem, aiming at adjusting a basis of orthogonal geodesic axes,\nwhile minimizing a fitting energy. We introduce an efficient, iterative\nalgorithm which exploits shared-memory parallelism, as well as an analytic\nexpression of the fitting energy gradient, to ensure fast iterations. Our\napproach also trivially extends to extremum persistence diagrams. Extensive\nexperiments on public ensembles demonstrate the efficiency of our approach -\nwith MT-PGA computations in the orders of minutes for the largest examples. We\nshow the utility of our contributions by extending to merge trees two typical\nPCA applications. First, we apply MT-PGA to data reduction and reliably\ncompress merge trees by concisely representing them by their first coordinates\nin the MT-PGA basis. Second, we present a dimensionality reduction framework\nexploiting the first two directions of the MT-PGA basis to generate\ntwo-dimensional layouts of the ensemble. We augment these layouts with\npersistence correlation views, enabling global and local visual inspections of\nthe feature variability in the ensemble. In both applications, quantitative\nexperiments assess the relevance of our framework. Finally, we provide a C++\nimplementation that can be used to reproduce our results.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.10960v3"
    },
    {
        "title": "VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space\n  Exploration of Ensemble Simulations",
        "authors": [
            "Neng Shi",
            "Jiayi Xu",
            "Haoyu Li",
            "Hanqi Guo",
            "Jonathan Woodring",
            "Han-Wei Shen"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We propose VDL-Surrogate, a view-dependent neural-network-latent-based\nsurrogate model for parameter space exploration of ensemble simulations that\nallows high-resolution visualizations and user-specified visual mappings.\nSurrogate-enabled parameter space exploration allows domain scientists to\npreview simulation results without having to run a large number of\ncomputationally costly simulations. Limited by computational resources,\nhowever, existing surrogate models may not produce previews with sufficient\nresolution for visualization and analysis. To improve the efficient use of\ncomputational resources and support high-resolution exploration, we perform ray\ncasting from different viewpoints to collect samples and produce compact latent\nrepresentations. This latent encoding process reduces the cost of surrogate\nmodel training while maintaining the output quality. In the model training\nstage, we select viewpoints to cover the whole viewing sphere and train\ncorresponding VDL-Surrogate models for the selected viewpoints. In the model\ninference stage, we predict the latent representations at previously selected\nviewpoints and decode the latent representations to data space. For any given\nviewpoint, we make interpolations over decoded data at selected viewpoints and\ngenerate visualizations with user-specified visual mappings. We show the\neffectiveness and efficiency of VDL-Surrogate in cosmological and ocean\nsimulations with quantitative and qualitative evaluations. Source code is\npublicly available at https://github.com/trainsn/VDL-Surrogate.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.13091v3"
    },
    {
        "title": "Learning to Generate 3D Shapes from a Single Example",
        "authors": [
            "Rundi Wu",
            "Changxi Zheng"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Existing generative models for 3D shapes are typically trained on a large 3D\ndataset, often of a specific object category. In this paper, we investigate the\ndeep generative model that learns from only a single reference 3D shape.\nSpecifically, we present a multi-scale GAN-based model designed to capture the\ninput shape's geometric features across a range of spatial scales. To avoid\nlarge memory and computational cost induced by operating on the 3D volume, we\nbuild our generator atop the tri-plane hybrid representation, which requires\nonly 2D convolutions. We train our generative model on a voxel pyramid of the\nreference shape, without the need of any external supervision or manual\nannotation. Once trained, our model can generate diverse and high-quality 3D\nshapes possibly of different sizes and aspect ratios. The resulting shapes\npresent variations across different scales, and at the same time retain the\nglobal structure of the reference shape. Through extensive evaluation, both\nqualitative and quantitative, we demonstrate that our model can generate 3D\nshapes of various types.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.02946v2"
    },
    {
        "title": "Generating Pixel Art Character Sprites using GANs",
        "authors": [
            "Flávio Coutinho",
            "Luiz Chaimowicz"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Iterating on creating pixel art character sprite sheets is essential to the\ngame development process. However, it can take a lot of effort until the final\nversions containing different poses and animation clips are achieved. This\npaper investigates using conditional generative adversarial networks to aid the\ndesigners in creating such sprite sheets. We propose an architecture based on\nPix2Pix to generate images of characters facing a target side (e.g., right)\ngiven sprites of them in a source pose (e.g., front). Experiments with small\npixel art datasets yielded promising results, resulting in models with varying\ndegrees of generalization, sometimes capable of generating images very close to\nthe ground truth. We analyze the results through visual inspection and\nquantitatively with FID.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.06413v1"
    },
    {
        "title": "Automatic Testing and Validation of Level of Detail Reductions Through\n  Supervised Learning",
        "authors": [
            "Matilda Tamm",
            "Olivia Shamon",
            "Hector Anadon Leon",
            "Konrad Tollmar",
            "Linus Gisslén"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Modern video games are rapidly growing in size and scale, and to create rich\nand interesting environments, a large amount of content is needed. As a\nconsequence, often several thousands of detailed 3D assets are used to create a\nsingle scene. As each asset's polygon mesh can contain millions of polygons,\nthe number of polygons that need to be drawn every frame may exceed several\nbillions. Therefore, the computational resources often limit how many detailed\nobjects that can be displayed in a scene. To push this limit and to optimize\nperformance one can reduce the polygon count of the assets when possible.\nBasically, the idea is that an object at farther distance from the capturing\ncamera, consequently with relatively smaller screen size, its polygon count may\nbe reduced without affecting the perceived quality. Level of Detail (LOD)\nrefers to the complexity level of a 3D model representation. The process of\nremoving complexity is often called LOD reduction and can be done automatically\nwith an algorithm or by hand by artists. However, this process may lead to\ndeterioration of the visual quality if the different LODs differ significantly,\nor if LOD reduction transition is not seamless. Today the validation of these\nresults is mainly done manually requiring an expert to visually inspect the\nresults. However, this process is slow, mundane, and therefore prone to error.\nHerein we propose a method to automate this process based on the use of deep\nconvolutional networks. We report promising results and envision that this\nmethod can be used to automate the process of LOD reduction testing and\nvalidation.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.12674v1"
    },
    {
        "title": "ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech",
        "authors": [
            "Saeed Ghorbani",
            "Ylva Ferstl",
            "Daniel Holden",
            "Nikolaus F. Troje",
            "Marc-André Carbonneau"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present ZeroEGGS, a neural network framework for speech-driven gesture\ngeneration with zero-shot style control by example. This means style can be\ncontrolled via only a short example motion clip, even for motion styles unseen\nduring training. Our model uses a Variational framework to learn a style\nembedding, making it easy to modify style through latent space manipulation or\nblending and scaling of style embeddings. The probabilistic nature of our\nframework further enables the generation of a variety of outputs given the same\ninput, addressing the stochastic nature of gesture motion. In a series of\nexperiments, we first demonstrate the flexibility and generalizability of our\nmodel to new speakers and styles. In a user study, we then show that our model\noutperforms previous state-of-the-art techniques in naturalness of motion,\nappropriateness for speech, and style portrayal. Finally, we release a\nhigh-quality dataset of full-body gesture motion including fingers, with\nspeech, spanning across 19 different styles.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.07556v2"
    },
    {
        "title": "Digital twins for city simulation: Automatic, efficient, and robust mesh\n  generation for large-scale city modeling and simulation",
        "authors": [
            "Vasilis Naserentin",
            "Anders Logg"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The concept of creating digital twins, connected digital models of physical\nsystems, is gaining increasing attention for modeling and simulation of whole\ncities. The basis for building a digital twin of a city is the generation of a\n3D city model, often represented as a mesh. Creating and updating such models\nis a tedious process that requires manual work and considerable effort,\nespecially in the modeling of building geometries. In the current paper, we\npresent a novel algorithm and implementation for automatic, efficient, and\nrobust mesh generation for large-scale city modeling and simulation. The\nalgorithm relies on standard, publicly available data, in particular 2D\ncadastral maps (building footprints) and 3D point clouds obtained from aerial\nscanning. The algorithm generates LoD1.2 city models in the form of both\ntriangular surface meshes, suitable for visualisation, and high-quality\ntetrahedral volume meshes, suitable for simulation. Our tests demonstrate good\nperformance and scaling and indicate good avenues for further optimization\nbased on parallelisation. The long-term goal is a generic digital twin of\ncities volume mesh generator that provides (nearly) real-time mesh manipulation\nin LoD2.x.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.05250v1"
    },
    {
        "title": "Differentiable Hybrid Traffic Simulation",
        "authors": [
            "Sanghyun Son",
            "Yi-Ling Qiao",
            "Jason Sewall",
            "Ming C. Lin"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We introduce a novel differentiable hybrid traffic simulator, which simulates\ntraffic using a hybrid model of both macroscopic and microscopic models and can\nbe directly integrated into a neural network for traffic control and flow\noptimization. This is the first differentiable traffic simulator for\nmacroscopic and hybrid models that can compute gradients for traffic states\nacross time steps and inhomogeneous lanes. To compute the gradient flow between\ntwo types of traffic models in a hybrid framework, we present a novel\nintermediate conversion component that bridges the lanes in a differentiable\nmanner as well. We also show that we can use analytical gradients to accelerate\nthe overall process and enhance scalability. Thanks to these gradients, our\nsimulator can provide more efficient and scalable solutions for complex\nlearning and control problems posed in traffic engineering than other existing\nalgorithms. Refer to https://sites.google.com/umd.edu/diff-hybrid-traffic-sim\nfor our project.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.08046v1"
    },
    {
        "title": "Deep Learning based Super-Resolution for Medical Volume Visualization\n  with Direct Volume Rendering",
        "authors": [
            "Sudarshan Devkota",
            "Sumanta Pattanaik"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Modern-day display systems demand high-quality rendering. However, rendering\nat higher resolution requires a large number of data samples and is\ncomputationally expensive. Recent advances in deep learning-based image and\nvideo super-resolution techniques motivate us to investigate such networks for\nhigh-fidelity upscaling of frames rendered at a lower resolution to a higher\nresolution. While our work focuses on super-resolution of medical volume\nvisualization performed with direct volume rendering, it is also applicable for\nvolume visualization with other rendering techniques. We propose a\nlearning-based technique where our proposed system uses color information along\nwith other supplementary features gathered from our volume renderer to learn\nefficient upscaling of a low-resolution rendering to a higher-resolution space.\nFurthermore, to improve temporal stability, we also implement the temporal\nreprojection technique for accumulating history samples in volumetric\nrendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.08080v1"
    },
    {
        "title": "An Improved Structured Mesh Generation Method Based on Physics-informed\n  Neural Networks",
        "authors": [
            "Xinhai Chen",
            "Jie Liu",
            "Junjun Yan",
            "Zhichao Wang",
            "Chunye Gong"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Mesh generation remains a key technology in many areas where numerical\nsimulations are required. As numerical algorithms become more efficient and\ncomputers become more powerful, the percentage of time devoted to mesh\ngeneration becomes higher. In this paper, we present an improved structured\nmesh generation method. The method formulates the meshing problem as a global\noptimization problem related to a physics-informed neural network. The mesh is\nobtained by intelligently solving the physical boundary-constrained partial\ndifferential equations. To improve the prediction accuracy of the neural\nnetwork, we also introduce a novel auxiliary line strategy and an efficient\nnetwork model during meshing. The strategy first employs a priori auxiliary\nlines to provide ground truth data and then uses these data to construct a loss\nterm to better constrain the convergence of the subsequent training. The\nexperimental results indicate that the proposed method is effective and robust.\nIt can accurately approximate the mapping (transformation) from the\ncomputational domain to the physical domain and enable fast high-quality\nstructured mesh generation.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.09546v1"
    },
    {
        "title": "Thermodynamics-informed neural networks for physically realistic mixed\n  reality",
        "authors": [
            "Quercus Hernández",
            "Alberto Badías",
            "Francisco Chinesta",
            "Elías Cueto"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  The imminent impact of immersive technologies in society urges for active\nresearch in real-time and interactive physics simulation for virtual worlds to\nbe realistic. In this context, realistic means to be compliant to the laws of\nphysics. In this paper we present a method for computing the dynamic response\nof (possibly non-linear and dissipative) deformable objects induced by\nreal-time user interactions in mixed reality using deep learning. The\ngraph-based architecture of the method ensures the thermodynamic consistency of\nthe predictions, whereas the visualization pipeline allows a natural and\nrealistic user experience. Two examples of virtual solids interacting with\nvirtual or physical solids in mixed reality scenarios are provided to prove the\nperformance of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.13414v2"
    },
    {
        "title": "Watch Your Step: Real-Time Adaptive Character Stepping",
        "authors": [
            "Ben Kenwright"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  An effective 3D stepping control algorithm that is computationally fast,\nrobust, and easy to implement is extremely important and valuable to character\nanimation research. In this paper, we present a novel technique for generating\ndynamic, interactive, and controllable biped stepping motions. Our approach\nuses a low-dimensional physics-based model to create balanced humanoid avatars\nthat can handle a wide variety of interactive situations, such as terrain\nheight shifting and push exertions, while remaining upright and balanced. We\naccomplish this by combining the popular inverted-pendulum model with an\nankle-feedback torque and variable leg-length mechanism to create a\ncontrollable solution that can adapt to unforeseen circumstances in real-time\nwithout key-framed data, any offline pre-processing, or on-line optimizations\njoint torque computations. We explain and address oversimplifications and\nlimitations with the basic IP model and the reasons for extending the model by\nmeans of additional control mechanisms. We demonstrate a simple and fast\napproach for extending the IP model based on an ankle-torque and variable leg\nlengths approximation without hindering the extremely attractive properties\n(i.e., computational speed, robustness, and simplicity) that make the IP model\nso ideal for generating upright responsive balancing biped movements. Finally,\nwhile our technique focuses on lower body motions, it can, nevertheless, handle\nboth small and large push forces even during terrain height variations.\nMoreover, our model effectively creates human-like motions that synthesize\nlow-level upright stepping movements, and can be combined with additional\ncontroller techniques to produce whole body autonomous agents.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.14730v1"
    },
    {
        "title": "NaRPA: Navigation and Rendering Pipeline for Astronautics",
        "authors": [
            "Roshan Thomas Eapen",
            "Ramchander Rao Bhaskara",
            "Manoranjan Majji"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  This paper presents Navigation and Rendering Pipeline for Astronautics\n(NaRPA) - a novel ray-tracing-based computer graphics engine to model and\nsimulate light transport for space-borne imaging. NaRPA incorporates lighting\nmodels with attention to atmospheric and shading effects for the synthesis of\nspace-to-space and ground-to-space virtual observations. In addition to image\nrendering, the engine also possesses point cloud, depth, and contour map\ngeneration capabilities to simulate passive and active vision-based sensors and\nto facilitate the designing, testing, or verification of visual navigation\nalgorithms. Physically based rendering capabilities of NaRPA and the efficacy\nof the proposed rendering algorithm are demonstrated using applications in\nrepresentative space-based environments. A key demonstration includes NaRPA as\na tool for generating stereo imagery and application in 3D coordinate\nestimation using triangulation. Another prominent application of NaRPA includes\na novel differentiable rendering approach for image-based attitude estimation\nis proposed to highlight the efficacy of the NaRPA engine for simulating\nvision-based navigation and guidance operations.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.01566v1"
    },
    {
        "title": "Quantifying spatial, temporal, angular and spectral structure of\n  effective daylight in perceptually meaningful ways",
        "authors": [
            "Cehao Yu",
            "Maarten Wijntjes",
            "Elmar Eisemann",
            "Sylvia Pont"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  We present a method to capture the 7-dimensional light field structure, and\ntranslate it into perceptually-relevant information. Our spectral cubic\nillumination method quantifies objective correlates of perceptually relevant\ndiffuse and directed light components, including their variations over time,\nspace, in color and direction, and the environment's response to sky and\nsunlight. We applied it 'in the wild', capturing how light on a sunny day\ndiffers between light and shadow, and how light varies over sunny and cloudy\ndays. We discuss the added value of our method for capturing nuanced lighting\neffects on scene and object appearance, such as chromatic gradients.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.14859v1"
    },
    {
        "title": "Talking Head Generation with Probabilistic Audio-to-Visual Diffusion\n  Priors",
        "authors": [
            "Zhentao Yu",
            "Zixin Yin",
            "Deyu Zhou",
            "Duomin Wang",
            "Finn Wong",
            "Baoyuan Wang"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this paper, we introduce a simple and novel framework for one-shot\naudio-driven talking head generation. Unlike prior works that require\nadditional driving sources for controlled synthesis in a deterministic manner,\nwe instead probabilistically sample all the holistic lip-irrelevant facial\nmotions (i.e. pose, expression, blink, gaze, etc.) to semantically match the\ninput audio while still maintaining both the photo-realism of audio-lip\nsynchronization and the overall naturalness. This is achieved by our newly\nproposed audio-to-visual diffusion prior trained on top of the mapping between\naudio and disentangled non-lip facial representations. Thanks to the\nprobabilistic nature of the diffusion prior, one big advantage of our framework\nis it can synthesize diverse facial motion sequences given the same audio clip,\nwhich is quite user-friendly for many real applications. Through comprehensive\nevaluations on public benchmarks, we conclude that (1) our diffusion prior\noutperforms auto-regressive prior significantly on almost all the concerned\nmetrics; (2) our overall system is competitive with prior works in terms of\naudio-lip synchronization but can effectively sample rich and natural-looking\nlip-irrelevant facial motions while still semantically harmonized with the\naudio input.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.04248v1"
    },
    {
        "title": "Patches of developable surfaces bounded by NURBS curves",
        "authors": [
            "L. Fernandez-Jambrina"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In this talk we review the problem of constructing a developable surface\npatch bounded by two rational or NURBS (Non-Uniform Rational B-spline) curves.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.06589v2"
    },
    {
        "title": "Synthesizing Physical Character-Scene Interactions",
        "authors": [
            "Mohamed Hassan",
            "Yunrong Guo",
            "Tingwu Wang",
            "Michael Black",
            "Sanja Fidler",
            "Xue Bin Peng"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Movement is how people interact with and affect their environment. For\nrealistic character animation, it is necessary to synthesize such interactions\nbetween virtual characters and their surroundings. Despite recent progress in\ncharacter animation using machine learning, most systems focus on controlling\nan agent's movements in fairly simple and homogeneous environments, with\nlimited interactions with other objects. Furthermore, many previous approaches\nthat synthesize human-scene interactions require significant manual labeling of\nthe training data. In contrast, we present a system that uses adversarial\nimitation learning and reinforcement learning to train physically-simulated\ncharacters that perform scene interaction tasks in a natural and life-like\nmanner. Our method learns scene interaction behaviors from large unstructured\nmotion datasets, without manual annotation of the motion data. These scene\ninteractions are learned using an adversarial discriminator that evaluates the\nrealism of a motion within the context of a scene. The key novelty involves\nconditioning both the discriminator and the policy networks on scene context.\nWe demonstrate the effectiveness of our approach through three challenging\nscene interaction tasks: carrying, sitting, and lying down, which require\ncoordination of a character's movements in relation to objects in the\nenvironment. Our policies learn to seamlessly transition between different\nbehaviors like idling, walking, and sitting. By randomizing the properties of\nthe objects and their placements during training, our method is able to\ngeneralize beyond the objects and scenarios depicted in the training dataset,\nproducing natural character-scene interactions for a wide variety of object\nshapes and placements. The approach takes physics-based character motion\ngeneration a step closer to broad applicability.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.00883v1"
    },
    {
        "title": "Example-Based Sampling with Diffusion Models",
        "authors": [
            "Bastien Doignies",
            "Nicolas Bonneel",
            "David Coeurjolly",
            "Julie Digne",
            "Loïs Paulin",
            "Jean-Claude Iehl",
            "Victor Ostromoukhov"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Much effort has been put into developing samplers with specific properties,\nsuch as producing blue noise, low-discrepancy, lattice or Poisson disk samples.\nThese samplers can be slow if they rely on optimization processes, may rely on\na wide range of numerical methods, are not always differentiable. The success\nof recent diffusion models for image generation suggests that these models\ncould be appropriate for learning how to generate point sets from examples.\nHowever, their convolutional nature makes these methods impractical for dealing\nwith scattered data such as point sets. We propose a generic way to produce 2-d\npoint sets imitating existing samplers from observed point sets using a\ndiffusion model. We address the problem of convolutional layers by leveraging\nneighborhood information from an optimal transport matching to a uniform grid,\nthat allows us to benefit from fast convolutions on grids, and to support the\nexample-based learning of non-uniform sampling patterns. We demonstrate how the\ndifferentiability of our approach can be used to optimize point sets to enforce\nproperties.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.05116v1"
    },
    {
        "title": "Adaptive Approximate Implicitization of Planar Parametric Curves via\n  Weak Gradient Constraints",
        "authors": [
            "Minghao Guo",
            "Yan Gao",
            "Zheng Pan"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Converting a parametric curve into the implicit form, which is called\nimplicitization, has always been a popular but challenging problem in geometric\nmodeling and related applications. However, the existing methods mostly suffer\nfrom the problems of maintaining geometric features and choosing a reasonable\nimplicit degree. The present paper has two contributions. We first introduce a\nnew regularization constraint(called the weak gradient constraint) for both\npolynomial and non-polynomial curves, which efficiently possesses shape\npreserving. We then propose two adaptive algorithms of approximate\nimplicitization for polynomial and non-polynomial curves respectively, which\nfind the ``optimal'' implicit degree based on the behavior of the weak gradient\nconstraint. More precisely, the idea is gradually increasing the implicit\ndegree, until there is no obvious improvement in the weak gradient loss of the\noutputs. Experimental results have shown the effectiveness and high quality of\nour proposed methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.11767v1"
    },
    {
        "title": "RipViz: Finding Rip Currents by Learning Pathline Behavior",
        "authors": [
            "Akila de Silva",
            "Mona Zhao",
            "Donald Stewart",
            "Fahim Hasan Khan",
            "Gregory Dusek",
            "James Davis",
            "Alex Pang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a hybrid machine learning and flow analysis feature detection\nmethod, RipViz, to extract rip currents from stationary videos. Rip currents\nare dangerous strong currents that can drag beachgoers out to sea. Most people\nare either unaware of them or do not know what they look like. In some\ninstances, even trained personnel such as lifeguards have difficulty\nidentifying them. RipViz produces a simple, easy to understand visualization of\nrip location overlaid on the source video. With RipViz, we first obtain an\nunsteady 2D vector field from the stationary video using optical flow. Movement\nat each pixel is analyzed over time. At each seed point, sequences of short\npathlines, rather a single long pathline, are traced across the frames of the\nvideo to better capture the quasi-periodic flow behavior of wave activity.\nBecause of the motion on the beach, the surf zone, and the surrounding areas,\nthese pathlines may still appear very cluttered and incomprehensible.\nFurthermore, lay audiences are not familiar with pathlines and may not know how\nto interpret them. To address this, we treat rip currents as a flow anomaly in\nan otherwise normal flow. To learn about the normal flow behavior, we train an\nLSTM autoencoder with pathline sequences from normal ocean, foreground, and\nbackground movements. During test time, we use the trained LSTM autoencoder to\ndetect anomalous pathlines (i.e., those in the rip zone). The origination\npoints of such anomalous pathlines, over the course of the video, are then\npresented as points within the rip zone. RipViz is fully automated and does not\nrequire user input. Feedback from domain expert suggests that RipViz has the\npotential for wider use.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.12983v1"
    },
    {
        "title": "Force-Directed Graph Layouts Revisited: A New Force Based on the\n  T-Distribution",
        "authors": [
            "Fahai Zhong",
            "Mingliang Xue",
            "Jian Zhang",
            "Fan Zhang",
            "Rui Ban",
            "Oliver Deussen",
            "Yunhai Wang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we propose the t-FDP model, a force-directed placement method\nbased on a novel bounded short-range force (t-force) defined by Student's\nt-distribution. Our formulation is flexible, exerts limited repulsive forces\nfor nearby nodes and can be adapted separately in its short- and long-range\neffects. Using such forces in force-directed graph layouts yields better\nneighborhood preservation than current methods, while maintaining low stress\nerrors. Our efficient implementation using a Fast Fourier Transform is one\norder of magnitude faster than state-of-the-art methods and two orders faster\non the GPU, enabling us to perform parameter tuning by globally and locally\nadjusting the t-force in real-time for complex graphs. We demonstrate the\nquality of our approach by numerical evaluation against state-of-the-art\napproaches and extensions for interactive exploration.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.03964v1"
    },
    {
        "title": "NEPHELE: A Neural Platform for Highly Realistic Cloud Radiance Rendering",
        "authors": [
            "Haimin Luo",
            "Siyuan Zhang",
            "Fuqiang Zhao",
            "Haotian Jing",
            "Penghao Wang",
            "Zhenxiao Yu",
            "Dongxue Yan",
            "Junran Ding",
            "Boyuan Zhang",
            "Qiang Hu",
            "Shu Yin",
            "Lan Xu",
            "JIngyi Yu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We have recently seen tremendous progress in neural rendering (NR) advances,\ni.e., NeRF, for photo-real free-view synthesis. Yet, as a local technique based\non a single computer/GPU, even the best-engineered Instant-NGP or i-NGP cannot\nreach real-time performance when rendering at a high resolution, and often\nrequires huge local computing resources. In this paper, we resort to cloud\nrendering and present NEPHELE, a neural platform for highly realistic cloud\nradiance rendering. In stark contrast with existing NR approaches, our NEPHELE\nallows for more powerful rendering capabilities by combining multiple remote\nGPUs and facilitates collaboration by allowing multiple people to view the same\nNeRF scene simultaneously. We introduce i-NOLF to employ opacity light fields\nfor ultra-fast neural radiance rendering in a one-query-per-ray manner. We\nfurther resemble the Lumigraph with geometry proxies for fast ray querying and\nsubsequently employ a small MLP to model the local opacity lumishperes for\nhigh-quality rendering. We also adopt Perfect Spatial Hashing in i-NOLF to\nenhance cache coherence. As a result, our i-NOLF achieves an order of magnitude\nperformance gain in terms of efficiency than i-NGP, especially for the\nmulti-user multi-viewpoint setting under cloud rendering scenarios. We further\ntailor a task scheduler accompanied by our i-NOLF representation and\ndemonstrate the advance of our methodological design through a comprehensive\ncloud platform, consisting of a series of cooperated modules, i.e., render\nfarms, task assigner, frame composer, and detailed streaming strategies. Using\nsuch a cloud platform compatible with neural rendering, we further showcase the\ncapabilities of our cloud radiance rendering through a series of applications,\nranging from cloud VR/AR rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.04086v1"
    },
    {
        "title": "MeshDiffusion: Score-based Generative 3D Mesh Modeling",
        "authors": [
            "Zhen Liu",
            "Yao Feng",
            "Michael J. Black",
            "Derek Nowrouzezahrai",
            "Liam Paull",
            "Weiyang Liu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We consider the task of generating realistic 3D shapes, which is useful for a\nvariety of applications such as automatic scene generation and physical\nsimulation. Compared to other 3D representations like voxels and point clouds,\nmeshes are more desirable in practice, because (1) they enable easy and\narbitrary manipulation of shapes for relighting and simulation, and (2) they\ncan fully leverage the power of modern graphics pipelines which are mostly\noptimized for meshes. Previous scalable methods for generating meshes typically\nrely on sub-optimal post-processing, and they tend to produce overly-smooth or\nnoisy surfaces without fine-grained geometric details. To overcome these\nshortcomings, we take advantage of the graph structure of meshes and use a\nsimple yet very effective generative modeling method to generate 3D meshes.\nSpecifically, we represent meshes with deformable tetrahedral grids, and then\ntrain a diffusion model on this direct parametrization. We demonstrate the\neffectiveness of our model on multiple generative tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.08133v2"
    },
    {
        "title": "MusicFace: Music-driven Expressive Singing Face Synthesis",
        "authors": [
            "Pengfei Liu",
            "Wenjin Deng",
            "Hengda Li",
            "Jintai Wang",
            "Yinglin Zheng",
            "Yiwei Ding",
            "Xiaohu Guo",
            "Ming Zeng"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  It is still an interesting and challenging problem to synthesize a vivid and\nrealistic singing face driven by music signal. In this paper, we present a\nmethod for this task with natural motions of the lip, facial expression, head\npose, and eye states. Due to the coupling of the mixed information of human\nvoice and background music in common signals of music audio, we design a\ndecouple-and-fuse strategy to tackle the challenge. We first decompose the\ninput music audio into human voice stream and background music stream. Due to\nthe implicit and complicated correlation between the two-stream input signals\nand the dynamics of the facial expressions, head motions and eye states, we\nmodel their relationship with an attention scheme, where the effects of the two\nstreams are fused seamlessly. Furthermore, to improve the expressiveness of the\ngenerated results, we propose to decompose head movements generation into speed\ngeneration and direction generation, and decompose eye states generation into\nthe short-time eye blinking generation and the long-time eye closing generation\nto model them separately. We also build a novel SingingFace Dataset to support\nthe training and evaluation of this task, and to facilitate future works on\nthis topic. Extensive experiments and user study show that our proposed method\nis capable of synthesizing vivid singing face, which is better than\nstate-of-the-art methods qualitatively and quantitatively.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.14044v1"
    },
    {
        "title": "Dr. KID: Direct Remeshing and K-set Isometric Decomposition for Scalable\n  Physicalization of Organic Shapes",
        "authors": [
            "Dawar Khan",
            "Ciril Bohak",
            "Ivan Viola"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Dr. KID is an algorithm that uses isometric decomposition for the\nphysicalization of potato-shaped organic models in a puzzle fashion. The\nalgorithm begins with creating a simple, regular triangular surface mesh of\norganic shapes, followed by iterative k-means clustering and remeshing. For\nclustering, we need similarity between triangles (segments) which is defined as\na distance function. The distance function maps each triangle's shape to a\nsingle point in the virtual 3D space. Thus, the distance between the triangles\nindicates their degree of dissimilarity. K-means clustering uses this distance\nand sorts of segments into k classes. After this, remeshing is applied to\nminimize the distance between triangles within the same cluster by making their\nshapes identical. Clustering and remeshing are repeated until the distance\nbetween triangles in the same cluster reaches an acceptable threshold. We adopt\na curvature-aware strategy to determine the surface thickness and finalize\npuzzle pieces for 3D printing. Identical hinges and holes are created for\nassembling the puzzle components. For smoother outcomes, we use triangle\nsubdivision along with curvature-aware clustering, generating curved triangular\npatches for 3D printing. Our algorithm was evaluated using various models, and\nthe 3D-printed results were analyzed. Findings indicate that our algorithm\nperforms reliably on target organic shapes with minimal loss of input geometry.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.02941v2"
    },
    {
        "title": "Deep Dynamic Cloud Lighting",
        "authors": [
            "Pinar Satilmis",
            "Thomas Bashford-Rogers"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Sky illumination is a core source of lighting in rendering, and a substantial\namount of work has been developed to simulate lighting from clear skies.\nHowever, in reality, clouds substantially alter the appearance of the sky and\nsubsequently change the scene's illumination. While there have been recent\nadvances in developing sky models which include clouds, these all neglect cloud\nmovement which is a crucial component of cloudy sky appearance. In any sort of\nvideo or interactive environment, it can be expected that clouds will move,\nsometimes quite substantially in a short period of time. Our work proposes a\nsolution to this which enables whole-sky dynamic cloud synthesis for the first\ntime. We achieve this by proposing a multi-timescale sky appearance model which\nlearns to predict the sky illumination over various timescales, and can be used\nto add dynamism to previous static, cloudy sky lighting approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09317v1"
    },
    {
        "title": "Coevolution of Camouflage",
        "authors": [
            "Craig Reynolds"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Camouflage in nature seems to arise from competition between predator and\nprey. To survive, predators must find prey, and prey must avoid being found.\nThis work simulates an abstract model of that adversarial relationship. It\nlooks at crypsis through evolving prey camouflage patterns (as color textures)\nin competition with evolving predator vision. During their \"lifetime\" predators\nlearn to better locate camouflaged prey. The environment for this 2D simulation\nis provided by a set of photographs, typically of natural scenes. This model is\nbased on two evolving populations, one of prey and another of predators. Mutual\nconflict between these populations can produce both effective prey camouflage\nand predators skilled at \"breaking\" camouflage. The result is an open source\nartificial life model to help study camouflage in nature, and the perceptual\nphenomenon of camouflage more generally.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.11793v2"
    },
    {
        "title": "Contour Completion by Transformers and Its Application to Vector Font\n  Data",
        "authors": [
            "Yusuke Nagata",
            "Brian Kenji Iwana",
            "Seiichi Uchida"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In documents and graphics, contours are a popular format to describe specific\nshapes. For example, in the True Type Font (TTF) file format, contours describe\nvector outlines of typeface shapes. Each contour is often defined as a sequence\nof points. In this paper, we tackle the contour completion task. In this task,\nthe input is a contour sequence with missing points, and the output is a\ngenerated completed contour. This task is more difficult than image completion\nbecause, for images, the missing pixels are indicated. Since there is no such\nindication in the contour completion task, we must solve the problem of missing\npart detection and completion simultaneously. We propose a Transformer-based\nmethod to solve this problem and show the results of the typeface contour\ncompletion.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13988v1"
    },
    {
        "title": "Composite Motion Learning with Task Control",
        "authors": [
            "Pei Xu",
            "Xiumin Shang",
            "Victor Zordan",
            "Ioannis Karamouzas"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a deep learning method for composite and task-driven motion\ncontrol for physically simulated characters. In contrast to existing\ndata-driven approaches using reinforcement learning that imitate full-body\nmotions, we learn decoupled motions for specific body parts from multiple\nreference motions simultaneously and directly by leveraging the use of multiple\ndiscriminators in a GAN-like setup. In this process, there is no need of any\nmanual work to produce composite reference motions for learning. Instead, the\ncontrol policy explores by itself how the composite motions can be combined\nautomatically. We further account for multiple task-specific rewards and train\na single, multi-objective control policy. To this end, we propose a novel\nframework for multi-objective learning that adaptively balances the learning of\ndisparate motions from multiple sources and multiple goal-directed control\nobjectives. In addition, as composite motions are typically augmentations of\nsimpler behaviors, we introduce a sample-efficient method for training\ncomposite control policies in an incremental manner, where we reuse a\npre-trained policy as the meta policy and train a cooperative policy that\nadapts the meta one for new composite tasks. We show the applicability of our\napproach on a variety of challenging multi-objective tasks involving both\ncomposite motion imitation and multiple goal-directed control.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03286v1"
    },
    {
        "title": "Data-Free Learning of Reduced-Order Kinematics",
        "authors": [
            "Nicholas Sharp",
            "Cristian Romero",
            "Alec Jacobson",
            "Etienne Vouga",
            "Paul G. Kry",
            "David I. W. Levin",
            "Justin Solomon"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Physical systems ranging from elastic bodies to kinematic linkages are\ndefined on high-dimensional configuration spaces, yet their typical low-energy\nconfigurations are concentrated on much lower-dimensional subspaces. This work\naddresses the challenge of identifying such subspaces automatically: given as\ninput an energy function for a high-dimensional system, we produce a\nlow-dimensional map whose image parameterizes a diverse yet low-energy\nsubmanifold of configurations. The only additional input needed is a single\nseed configuration for the system to initialize our procedure; no dataset of\ntrajectories is required. We represent subspaces as neural networks that map a\nlow-dimensional latent vector to the full configuration space, and propose a\ntraining scheme to fit network parameters to any system of interest. This\nformulation is effective across a very general range of physical systems; our\nexperiments demonstrate not only nonlinear and very low-dimensional elastic\nbody and cloth subspaces, but also more general systems like colliding rigid\nbodies and linkages. We briefly explore applications built on this formulation,\nincluding manipulation, latent interpolation, and sampling.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03846v1"
    },
    {
        "title": "A Practical Walk-on-Boundary Method for Boundary Value Problems",
        "authors": [
            "Ryusuke Sugimoto",
            "Terry Chen",
            "Yiti Jiang",
            "Christopher Batty",
            "Toshiya Hachisuka"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce the walk-on-boundary (WoB) method for solving boundary value\nproblems to computer graphics. WoB is a grid-free Monte Carlo solver for\ncertain classes of second order partial differential equations. A similar Monte\nCarlo solver, the walk-on-spheres (WoS) method, has been recently popularized\nin computer graphics due to its advantages over traditional spatial\ndiscretization-based alternatives. We show that WoB's intrinsic properties\nyield further advantages beyond those of WoS. Unlike WoS, WoB naturally\nsupports various boundary conditions (Dirichlet, Neumann, Robin, and mixed) for\nboth interior and exterior domains. WoB builds upon boundary integral\nformulations, and it is mathematically more similar to light transport\nsimulation in rendering than the random walk formulation of WoS. This\nsimilarity between WoB and rendering allows us to implement WoB on top of Monte\nCarlo ray tracing, and to incorporate advanced rendering techniques (e.g.,\nbidirectional estimators with multiple importance sampling, the virtual point\nlights method, and Markov chain Monte Carlo) into WoB. WoB does not suffer from\nthe intrinsic bias of WoS near the boundary and can estimate solutions\nprecisely on the boundary. Our numerical results highlight the advantages of\nWoB over WoS as an attractive alternative to solve boundary value problems\nbased on Monte Carlo.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04403v2"
    },
    {
        "title": "Estimating Discrete Total Curvature with Per Triangle Normal Variation",
        "authors": [
            "Crane He Chen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce a novel approach for measuring the total curvature at every\ntriangle of a discrete surface. This method takes advantage of the relationship\nbetween per triangle total curvature and the Dirichlet energy of the Gauss map.\nThis new tool can be used on both triangle meshes and point clouds and has\nnumerous applications. In this study, we demonstrate the effectiveness of our\ntechnique by using it for feature-aware mesh decimation, and show that it\noutperforms existing curvature-estimation methods from popular libraries such\nas Meshlab, Trimesh2, and Libigl. When estimating curvature on point clouds,\nour method outperforms popular libraries PCL and CGAL.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.12653v2"
    },
    {
        "title": "Generative Adversarial Shaders for Real-Time Realism Enhancement",
        "authors": [
            "Arturo Salmi",
            "Szabolcs Cséfalvay",
            "James Imber"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Application of realism enhancement methods, particularly in real-time and\nresource-constrained settings, has been frustrated by the expense of existing\nmethods. These achieve high quality results only at the cost of long runtimes\nand high bandwidth, memory, and power requirements. We present an efficient\nalternative: a high-performance, generative shader-based approach that adapts\nmachine learning techniques to real-time applications, even in\nresource-constrained settings such as embedded and mobile GPUs. The proposed\nlearnable shader pipeline comprises differentiable functions that can be\ntrained in an end-to-end manner using an adversarial objective, allowing for\nfaithful reproduction of the appearance of a target image set without manual\ntuning. The shader pipeline is optimized for highly efficient execution on the\ntarget device, providing temporally stable, faster-than-real time results with\nquality competitive with many neural network-based methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.04629v1"
    },
    {
        "title": "QuestEnvSim: Environment-Aware Simulated Motion Tracking from Sparse\n  Sensors",
        "authors": [
            "Sunmin Lee",
            "Sebastian Starke",
            "Yuting Ye",
            "Jungdam Won",
            "Alexander Winkler"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Replicating a user's pose from only wearable sensors is important for many\nAR/VR applications. Most existing methods for motion tracking avoid environment\ninteraction apart from foot-floor contact due to their complex dynamics and\nhard constraints. However, in daily life people regularly interact with their\nenvironment, e.g. by sitting on a couch or leaning on a desk. Using\nReinforcement Learning, we show that headset and controller pose, if combined\nwith physics simulation and environment observations can generate realistic\nfull-body poses even in highly constrained environments. The physics simulation\nautomatically enforces the various constraints necessary for realistic poses,\ninstead of manually specifying them as in many kinematic approaches. These hard\nconstraints allow us to achieve high-quality interaction motions without\ntypical artifacts such as penetration or contact sliding. We discuss three\nfeatures, the environment representation, the contact reward and scene\nrandomization, crucial to the performance of the method. We demonstrate the\ngenerality of the approach through various examples, such as sitting on chairs,\na couch and boxes, stepping over boxes, rocking a chair and turning an office\nchair. We believe these are some of the highest-quality results achieved for\nmotion tracking from sparse sensor with scene interaction.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.05666v1"
    },
    {
        "title": "SENS: Part-Aware Sketch-based Implicit Neural Shape Modeling",
        "authors": [
            "Alexandre Binninger",
            "Amir Hertz",
            "Olga Sorkine-Hornung",
            "Daniel Cohen-Or",
            "Raja Giryes"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present SENS, a novel method for generating and editing 3D models from\nhand-drawn sketches, including those of abstract nature. Our method allows\nusers to quickly and easily sketch a shape, and then maps the sketch into the\nlatent space of a part-aware neural implicit shape architecture. SENS analyzes\nthe sketch and encodes its parts into ViT patch encoding, subsequently feeding\nthem into a transformer decoder that converts them to shape embeddings suitable\nfor editing 3D neural implicit shapes. SENS provides intuitive sketch-based\ngeneration and editing, and also succeeds in capturing the intent of the user's\nsketch to generate a variety of novel and expressive 3D shapes, even from\nabstract and imprecise sketches. Additionally, SENS supports refinement via\npart reconstruction, allowing for nuanced adjustments and artifact removal. It\nalso offers part-based modeling capabilities, enabling the combination of\nfeatures from multiple sketches to create more complex and customized 3D\nshapes. We demonstrate the effectiveness of our model compared to the\nstate-of-the-art using objective metric evaluation criteria and a user study,\nboth indicating strong performance on sketches with a medium level of\nabstraction. Furthermore, we showcase our method's intuitive sketch-based shape\nediting capabilities, and validate it through a usability study.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.06088v2"
    },
    {
        "title": "Strokes2Surface: Recovering Curve Networks From 4D Architectural Design\n  Sketches",
        "authors": [
            "S. Rasoulzadeh",
            "M. Wimmer",
            "P. Stauss",
            "I. Kovacic"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present Strokes2Surface, an offline geometry reconstruction pipeline that\nrecovers well-connected curve networks from imprecise 4D sketches to bridge\nconcept design and digital modeling stages in architectural design. The input\nto our pipeline consists of 3D strokes' polyline vertices and their timestamps\nas the 4th dimension, along with additional metadata recorded throughout\nsketching. Inspired by architectural sketching practices, our pipeline combines\na classifier and two clustering models to achieve its goal. First, with a set\nof extracted hand-engineered features from the sketch, the classifier\nrecognizes the type of individual strokes between those depicting boundaries\n(Shape strokes) and those depicting enclosed areas (Scribble strokes). Next,\nthe two clustering models parse strokes of each type into distinct groups, each\nrepresenting an individual edge or face of the intended architectural object.\nCurve networks are then formed through topology recovery of consolidated Shape\nclusters and surfaced using Scribble clusters guiding the cycle discovery. Our\nevaluation is threefold: We confirm the usability of the Strokes2Surface\npipeline in architectural design use cases via a user study, we validate our\nchoice of features via statistical analysis and ablation studies on our\ncollected dataset, and we compare our outputs against a range of\nreconstructions computed using alternative methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.07220v4"
    },
    {
        "title": "A novel explicit design method for complex thin-walled structures based\n  on embedded solid moving morphable components",
        "authors": [
            "Wendong Huo",
            "Chang Liu",
            "Yunpu Liu",
            "Zongliang Du",
            "Weisheng Zhang",
            "Xu Guo"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this article, a novel explicit approach for designing complex thin-walled\nstructures based on the Moving Morphable Component (MMC) method is proposed,\nwhich provides a unified framework to systematically address various design\nissues, including topology optimization, reinforced-rib layout optimization,\nand sandwich structure design problems. The complexity of thin-walled\nstructures mainly comes from flexible geometries and the variation of\nthickness. On the one hand, the geometric complexity of thin-walled structures\nleads to the difficulty in automatically describing material distribution\n(e.g., reinforced ribs). On the other hand, thin-walled structures with\ndifferent thicknesses require various hypotheses (e.g., Kirchhoff-Love shell\ntheory and Reissner-Mindlin shell theory) to ensure the precision of structural\nresponses. Whereas for cases that do not fit the shell hypothesis, the\nprecision loss of response solutions is nonnegligible in the optimization\nprocess since the accumulation of errors will cause entirely different designs.\nHence, the current article proposes a novel embedded solid component to tackle\nthese challenges. The geometric constraints that make the components fit to the\ncurved thin-walled structure are whereby satisfied. Compared with traditional\nstrategies, the proposed method is free from the limit of shell assumptions of\nstructural analysis and can achieve optimized designs with clear load\ntransmission paths at the cost of few design variables and degrees of freedom\nfor finite element analysis (FEA). Finally, we apply the proposed method to\nseveral representative examples to demonstrate its effectiveness, efficiency,\nversatility, and potential to handle complex industrial structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.10449v1"
    },
    {
        "title": "Align, Adapt and Inject: Sound-guided Unified Image Generation",
        "authors": [
            "Yue Yang",
            "Kaipeng Zhang",
            "Yuying Ge",
            "Wenqi Shao",
            "Zeyue Xue",
            "Yu Qiao",
            "Ping Luo"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Text-guided image generation has witnessed unprecedented progress due to the\ndevelopment of diffusion models. Beyond text and image, sound is a vital\nelement within the sphere of human perception, offering vivid representations\nand naturally coinciding with corresponding scenes. Taking advantage of sound\ntherefore presents a promising avenue for exploration within image generation\nresearch. However, the relationship between audio and image supervision remains\nsignificantly underdeveloped, and the scarcity of related, high-quality\ndatasets brings further obstacles. In this paper, we propose a unified\nframework 'Align, Adapt, and Inject' (AAI) for sound-guided image generation,\nediting, and stylization. In particular, our method adapts input sound into a\nsound token, like an ordinary word, which can plug and play with existing\npowerful diffusion-based Text-to-Image (T2I) models. Specifically, we first\ntrain a multi-modal encoder to align audio representation with the pre-trained\ntextual manifold and visual manifold, respectively. Then, we propose the audio\nadapter to adapt audio representation into an audio token enriched with\nspecific semantics, which can be injected into a frozen T2I model flexibly. In\nthis way, we are able to extract the dynamic information of varied sounds,\nwhile utilizing the formidable capability of existing T2I models to facilitate\nsound-guided image generation, editing, and stylization in a convenient and\ncost-effective manner. The experiment results confirm that our proposed AAI\noutperforms other text and sound-guided state-of-the-art methods. And our\naligned multi-modal encoder is also competitive with other approaches in the\naudio-visual retrieval and audio-text retrieval tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.11504v1"
    },
    {
        "title": "Visual Analysis of Large Multi-Field AMR Data on GPUs Using Interactive\n  Volume Lines",
        "authors": [
            "Stefan Zellmann",
            "Serkan Demirci",
            "Uğur Güdükbay"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  To visually compare ensembles of volumes, dynamic volume lines (DVLs)\nrepresent each ensemble member as a 1D polyline. To compute these, the volume\ncells are sorted on a space-filling curve and scaled by the ensemble's local\nvariation. The resulting 1D plot can augment or serve as an alternative to a 3D\nvolume visualization free of visual clutter and occlusion. Interactively\ncomputing DVLs is challenging when the data is large, and the volume grid is\nnot structured/regular, as is often the case with computational fluid dynamics\nsimulations. We extend DVLs to support large-scale, multi-field adaptive mesh\nrefinement (AMR) data that can be explored interactively. Our GPU-based system\nupdates the DVL representation whenever the data or the alpha transfer function\nchanges. We demonstrate and evaluate our interactive prototype using large AMR\nvolumes from astrophysics simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.11612v1"
    },
    {
        "title": "Facial Expression Re-targeting from a Single Character",
        "authors": [
            "Ariel Larey",
            "Omri Asraf",
            "Adam Kelder",
            "Itzik Wilf",
            "Ofer Kruzel",
            "Nati Daniel"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Video retargeting for digital face animation is used in virtual reality,\nsocial media, gaming, movies, and video conference, aiming to animate avatars'\nfacial expressions based on videos of human faces. The standard method to\nrepresent facial expressions for 3D characters is by blendshapes, a vector of\nweights representing the avatar's neutral shape and its variations under facial\nexpressions, e.g., smile, puff, blinking. Datasets of paired frames with\nblendshape vectors are rare, and labeling can be laborious, time-consuming, and\nsubjective. In this work, we developed an approach that handles the lack of\nappropriate datasets. Instead, we used a synthetic dataset of only one\ncharacter. To generalize various characters, we re-represented each frame to\nface landmarks. We developed a unique deep-learning architecture that groups\nlandmarks for each facial organ and connects them to relevant blendshape\nweights. Additionally, we incorporated complementary methods for facial\nexpressions that landmarks did not represent well and gave special attention to\neye expressions. We have demonstrated the superiority of our approach to\nprevious research in qualitative and quantitative metrics. Our approach\nachieved a higher MOS of 68% and a lower MSE of 44.2% when tested on videos\nwith various users and expressions.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12188v1"
    },
    {
        "title": "Text-driven Talking Face Synthesis by Reprogramming Audio-driven Models",
        "authors": [
            "Jeongsoo Choi",
            "Minsu Kim",
            "Se Jin Park",
            "Yong Man Ro"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  In this paper, we present a method for reprogramming pre-trained audio-driven\ntalking face synthesis models to operate in a text-driven manner. Consequently,\nwe can easily generate face videos that articulate the provided textual\nsentences, eliminating the necessity of recording speech for each inference, as\nrequired in the audio-driven model. To this end, we propose to embed the input\ntext into the learned audio latent space of the pre-trained audio-driven model,\nwhile preserving the face synthesis capability of the original pre-trained\nmodel. Specifically, we devise a Text-to-Audio Embedding Module (TAEM) which\nmaps a given text input into the audio latent space by modeling pronunciation\nand duration characteristics. Furthermore, to consider the speaker\ncharacteristics in audio while using text inputs, TAEM is designed to accept a\nvisual speaker embedding. The visual speaker embedding is derived from a single\ntarget face image and enables improved mapping of input text to the learned\naudio latent space by incorporating the speaker characteristics inherent in the\naudio. The main advantages of the proposed framework are that 1) it can be\napplied to diverse audio-driven talking face synthesis models and 2) we can\ngenerate talking face videos with either text inputs or audio inputs with high\nflexibility.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.16003v2"
    },
    {
        "title": "Bluefish: Composing Diagrams with Declarative Relations",
        "authors": [
            "Josh Pollock",
            "Catherine Mei",
            "Grace Huang",
            "Elliot Evans",
            "Daniel Jackson",
            "Arvind Satyanarayan"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Diagrams are essential tools for problem-solving and communication as they\nexternalize conceptual structures using spatial relationships. But when picking\na diagramming framework, users are faced with a dilemma. They can either use a\nhighly expressive but low-level toolkit, whose API does not match their\ndomain-specific concepts, or select a high-level typology, which offers a\nrecognizable vocabulary but supports a limited range of diagrams. To address\nthis gap, we introduce Bluefish: a diagramming framework inspired by\ncomponent-based user interface (UI) libraries. Bluefish lets users create\ndiagrams using relations: declarative, composable, and extensible diagram\nfragments that relax the concept of a UI component. Unlike a component, a\nrelation does not have sole ownership over its children nor does it need to\nfully specify their layout. To render diagrams, Bluefish extends a traditional\ntree-based scenegraph to a compound graph that captures both hierarchical and\nadjacent relationships between nodes. To evaluate our system, we construct a\ndiverse example gallery covering many domains including mathematics, physics,\ncomputer science, and even cooking. We show that Bluefish's relations are\neffective declarative primitives for diagrams. Bluefish is open source, and we\naim to shape it into both a usable tool and a research platform.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.00146v4"
    },
    {
        "title": "Neural Stream Functions",
        "authors": [
            "Skylar Wolfgang Wurster",
            "Hanqi Guo",
            "Tom Peterka",
            "Han-Wei Shen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a neural network approach to compute stream functions, which are\nscalar functions with gradients orthogonal to a given vector field. As a\nresult, isosurfaces of the stream function extract stream surfaces, which can\nbe visualized to analyze flow features. Our approach takes a vector field as\ninput and trains an implicit neural representation to learn a stream function\nfor that vector field. The network learns to map input coordinates to a stream\nfunction value by minimizing the inner product of the gradient of the neural\nnetwork's output and the vector field. Since stream function solutions may not\nbe unique, we give optional constraints for the network to learn particular\nstream functions of interest. Specifically, we introduce regularizing loss\nfunctions that can optionally be used to generate stream function solutions\nwhose stream surfaces follow the flow field's curvature, or that can learn a\nstream function that includes a stream surface passing through a seeding rake.\nWe also discuss considerations for properly visualizing the trained implicit\nnetwork and extracting artifact-free surfaces. We compare our results with\nother implicit solutions and present qualitative and quantitative results for\nseveral synthetic and simulated vector fields.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.08142v1"
    },
    {
        "title": "ExWarp: Extrapolation and Warping-based Temporal Supersampling for\n  High-frequency Displays",
        "authors": [
            "Akanksha Dixit",
            "Yashashwee Chakrabarty",
            "Smruti R. Sarangi"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  High-frequency displays are gaining immense popularity because of their\nincreasing use in video games and virtual reality applications. However, the\nissue is that the underlying GPUs cannot continuously generate frames at this\nhigh rate -- this results in a less smooth and responsive experience.\nFurthermore, if the frame rate is not synchronized with the refresh rate, the\nuser may experience screen tearing and stuttering. Previous works propose\nincreasing the frame rate to provide a smooth experience on modern displays by\npredicting new frames based on past or future frames. Interpolation and\nextrapolation are two widely used algorithms that predict new frames.\nInterpolation requires waiting for the future frame to make a prediction, which\nadds additional latency. On the other hand, extrapolation provides a better\nquality of experience because it relies solely on past frames -- it does not\nincur any additional latency. The simplest method to extrapolate a frame is to\nwarp the previous frame using motion vectors; however, the warped frame may\ncontain improperly rendered visual artifacts due to dynamic objects -- this\nmakes it very challenging to design such a scheme. Past work has used DNNs to\nget good accuracy, however, these approaches are slow. This paper proposes\nExwarp -- an approach based on reinforcement learning (RL) to intelligently\nchoose between the slower DNN-based extrapolation and faster warping-based\nmethods to increase the frame rate by 4x with an almost negligible reduction in\nthe perceived image quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.12607v1"
    },
    {
        "title": "The Visual Language of Fabrics",
        "authors": [
            "Valentin Deschaintre",
            "Julia Guerrero-Viu",
            "Diego Gutierrez",
            "Tamy Boubekeur",
            "Belen Masia"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce text2fabric, a novel dataset that links free-text descriptions\nto various fabric materials. The dataset comprises 15,000 natural language\ndescriptions associated to 3,000 corresponding images of fabric materials.\nTraditionally, material descriptions come in the form of tags/keywords, which\nlimits their expressivity, induces pre-existing knowledge of the appropriate\nvocabulary, and ultimately leads to a chopped description system. Therefore, we\nstudy the use of free-text as a more appropriate way to describe material\nappearance, taking the use case of fabrics as a common item that non-experts\nmay often deal with. Based on the analysis of the dataset, we identify a\ncompact lexicon, set of attributes and key structure that emerge from the\ndescriptions. This allows us to accurately understand how people describe\nfabrics and draw directions for generalization to other types of materials. We\nalso show that our dataset enables specializing large vision-language models\nsuch as CLIP, creating a meaningful latent space for fabric appearance, and\nsignificantly improving applications such as fine-grained material retrieval\nand automatic captioning.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.13681v1"
    },
    {
        "title": "DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation",
        "authors": [
            "Qiaosong Qi",
            "Le Zhuo",
            "Aixi Zhang",
            "Yue Liao",
            "Fei Fang",
            "Si Liu",
            "Shuicheng Yan"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  When hearing music, it is natural for people to dance to its rhythm.\nAutomatic dance generation, however, is a challenging task due to the physical\nconstraints of human motion and rhythmic alignment with target music.\nConventional autoregressive methods introduce compounding errors during\nsampling and struggle to capture the long-term structure of dance sequences. To\naddress these limitations, we present a novel cascaded motion diffusion model,\nDiffDance, designed for high-resolution, long-form dance generation. This model\ncomprises a music-to-dance diffusion model and a sequence super-resolution\ndiffusion model. To bridge the gap between music and motion for conditional\ngeneration, DiffDance employs a pretrained audio representation learning model\nto extract music embeddings and further align its embedding space to motion via\ncontrastive loss. During training our cascaded diffusion model, we also\nincorporate multiple geometric losses to constrain the model outputs to be\nphysically plausible and add a dynamic loss weight that adaptively changes over\ndiffusion timesteps to facilitate sample diversity. Through comprehensive\nexperiments performed on the benchmark dataset AIST++, we demonstrate that\nDiffDance is capable of generating realistic dance sequences that align\neffectively with the input music. These results are comparable to those\nachieved by state-of-the-art autoregressive methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.02915v1"
    },
    {
        "title": "Flexible Isosurface Extraction for Gradient-Based Mesh Optimization",
        "authors": [
            "Tianchang Shen",
            "Jacob Munkberg",
            "Jon Hasselgren",
            "Kangxue Yin",
            "Zian Wang",
            "Wenzheng Chen",
            "Zan Gojcic",
            "Sanja Fidler",
            "Nicholas Sharp",
            "Jun Gao"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This work considers gradient-based mesh optimization, where we iteratively\noptimize for a 3D surface mesh by representing it as the isosurface of a scalar\nfield, an increasingly common paradigm in applications including\nphotogrammetry, generative modeling, and inverse physics. Existing\nimplementations adapt classic isosurface extraction algorithms like Marching\nCubes or Dual Contouring; these techniques were designed to extract meshes from\nfixed, known fields, and in the optimization setting they lack the degrees of\nfreedom to represent high-quality feature-preserving meshes, or suffer from\nnumerical instabilities. We introduce FlexiCubes, an isosurface representation\nspecifically designed for optimizing an unknown mesh with respect to geometric,\nvisual, or even physical objectives. Our main insight is to introduce\nadditional carefully-chosen parameters into the representation, which allow\nlocal flexible adjustments to the extracted mesh geometry and connectivity.\nThese parameters are updated along with the underlying scalar field via\nautomatic differentiation when optimizing for a downstream task. We base our\nextraction scheme on Dual Marching Cubes for improved topological properties,\nand present extensions to optionally generate tetrahedral and\nhierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on\nboth synthetic benchmarks and real-world applications, showing that it offers\nsignificant improvements in mesh quality and geometric fidelity.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.05371v1"
    },
    {
        "title": "Neural Categorical Priors for Physics-Based Character Control",
        "authors": [
            "Qingxu Zhu",
            "He Zhang",
            "Mengting Lan",
            "Lei Han"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Recent advances in learning reusable motion priors have demonstrated their\neffectiveness in generating naturalistic behaviors. In this paper, we propose a\nnew learning framework in this paradigm for controlling physics-based\ncharacters with significantly improved motion quality and diversity over\nexisting state-of-the-art methods. The proposed method uses reinforcement\nlearning (RL) to initially track and imitate life-like movements from\nunstructured motion clips using the discrete information bottleneck, as adopted\nin the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure\ncompresses the most relevant information from the motion clips into a compact\nyet informative latent space, i.e., a discrete space over vector quantized\ncodes. By sampling codes in the space from a trained categorical prior\ndistribution, high-quality life-like behaviors can be generated, similar to the\nusage of VQ-VAE in computer vision. Although this prior distribution can be\ntrained with the supervision of the encoder's output, it follows the original\nmotion clip distribution in the dataset and could lead to imbalanced behaviors\nin our setting. To address the issue, we further propose a technique named\nprior shifting to adjust the prior distribution using curiosity-driven RL. The\noutcome distribution is demonstrated to offer sufficient behavioral diversity\nand significantly facilitates upper-level policy learning for downstream tasks.\nWe conduct comprehensive experiments using humanoid characters on two\nchallenging downstream tasks, sword-shield striking and two-player boxing game.\nOur results demonstrate that the proposed framework is capable of controlling\nthe character to perform considerably high-quality movements in terms of\nbehavioral strategies, diversity, and realism. Videos, codes, and data are\navailable at https://tencent-roboticsx.github.io/NCP/.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.07200v3"
    },
    {
        "title": "Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural\n  Networks",
        "authors": [
            "Amin Heyrani Nobari",
            "Justin Rey",
            "Suhas Kodali",
            "Matthew Jones",
            "Faez Ahmed"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Computational Fluid Dynamics (CFD) is widely used in different engineering\nfields, but accurate simulations are dependent upon proper meshing of the\nsimulation domain. While highly refined meshes may ensure precision, they come\nwith high computational costs. Similarly, adaptive remeshing techniques require\nmultiple simulations and come at a great computational cost. This means that\nthe meshing process is reliant upon expert knowledge and years of experience.\nAutomating mesh generation can save significant time and effort and lead to a\nfaster and more efficient design process. This paper presents a machine\nlearning-based scheme that utilizes Graph Neural Networks (GNN) and expert\nguidance to automatically generate CFD meshes for aircraft models. In this\nwork, we introduce a new 3D segmentation algorithm that outperforms two\nstate-of-the-art models, PointNet++ and PointMLP, for surface classification.\nWe also present a novel approach to project predictions from 3D mesh\nsegmentation models to CAD surfaces using the conformal predictions method,\nwhich provides marginal statistical guarantees and robust uncertainty\nquantification and handling. We demonstrate that the addition of conformal\npredictions effectively enables the model to avoid under-refinement, hence\nfailure, in CFD meshing even for weak and less accurate models. Finally, we\ndemonstrate the efficacy of our approach through a real-world case study that\ndemonstrates that our automatically generated mesh is comparable in quality to\nexpert-generated meshes and enables the solver to converge and produce accurate\nresults. Furthermore, we compare our approach to the alternative of adaptive\nremeshing in the same case study and find that our method is 5 times faster in\nthe overall process of simulation. The code and data for this project are made\npublicly available at https://github.com/ahnobari/AutoSurf.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.07358v1"
    },
    {
        "title": "Motion In-Betweening with Phase Manifolds",
        "authors": [
            "Paul Starke",
            "Sebastian Starke",
            "Taku Komura",
            "Frank Steinicke"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This paper introduces a novel data-driven motion in-betweening system to\nreach target poses of characters by making use of phases variables learned by a\nPeriodic Autoencoder. Our approach utilizes a mixture-of-experts neural network\nmodel, in which the phases cluster movements in both space and time with\ndifferent expert weights. Each generated set of weights then produces a\nsequence of poses in an autoregressive manner between the current and target\nstate of the character. In addition, to satisfy poses which are manually\nmodified by the animators or where certain end effectors serve as constraints\nto be reached by the animation, a learned bi-directional control scheme is\nimplemented to satisfy such constraints. The results demonstrate that using\nphases for motion in-betweening tasks sharpen the interpolated movements, and\nfurthermore stabilizes the learning process. Moreover, using phases for motion\nin-betweening tasks can also synthesize more challenging movements beyond\nlocomotion behaviors. Additionally, style control is enabled between given\ntarget keyframes. Our proposed framework can compete with popular\nstate-of-the-art methods for motion in-betweening in terms of motion quality\nand generalization, especially in the existence of long transition durations.\nOur framework contributes to faster prototyping workflows for creating animated\ncharacter sequences, which is of enormous interest for the game and film\nindustry.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.12751v1"
    },
    {
        "title": "Dynamic Mesh-Aware Radiance Fields",
        "authors": [
            "Yi-Ling Qiao",
            "Alexander Gao",
            "Yiran Xu",
            "Yue Feng",
            "Jia-Bin Huang",
            "Ming C. Lin"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Embedding polygonal mesh assets within photorealistic Neural Radience Fields\n(NeRF) volumes, such that they can be rendered and their dynamics simulated in\na physically consistent manner with the NeRF, is under-explored from the system\nperspective of integrating NeRF into the traditional graphics pipeline. This\npaper designs a two-way coupling between mesh and NeRF during rendering and\nsimulation. We first review the light transport equations for both mesh and\nNeRF, then distill them into an efficient algorithm for updating radiance and\nthroughput along a cast ray with an arbitrary number of bounces. To resolve the\ndiscrepancy between the linear color space that the path tracer assumes and the\nsRGB color space that standard NeRF uses, we train NeRF with High Dynamic Range\n(HDR) images. We also present a strategy to estimate light sources and cast\nshadows on the NeRF. Finally, we consider how the hybrid surface-volumetric\nformulation can be efficiently integrated with a high-performance physics\nsimulator that supports cloth, rigid and soft bodies. The full rendering and\nsimulation system can be run on a GPU at interactive rates. We show that a\nhybrid system approach outperforms alternatives in visual realism for mesh\ninsertion, because it allows realistic light transport from volumetric NeRF\nmedia onto surfaces, which affects the appearance of reflective/refractive\nsurfaces and illumination of diffuse surfaces informed by the dynamic scene.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.04581v1"
    },
    {
        "title": "Efficient Graphics Representation with Differentiable Indirection",
        "authors": [
            "Sayantan Datta",
            "Carl Marshall",
            "Derek Nowrouzezahrai",
            "Zhao Dong",
            "Zhengqin Li"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce differentiable indirection -- a novel learned primitive that\nemploys differentiable multi-scale lookup tables as an effective substitute for\ntraditional compute and data operations across the graphics pipeline. We\ndemonstrate its flexibility on a number of graphics tasks, i.e., geometric and\nimage representation, texture mapping, shading, and radiance field\nrepresentation. In all cases, differentiable indirection seamlessly integrates\ninto existing architectures, trains rapidly, and yields both versatile and\nefficient results.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.08387v2"
    },
    {
        "title": "Learning based 2D Irregular Shape Packing",
        "authors": [
            "Zeshi Yang",
            "Zherong Pan",
            "Manyi Li",
            "Kui Wu",
            "Xifeng Gao"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  2D irregular shape packing is a necessary step to arrange UV patches of a 3D\nmodel within a texture atlas for memory-efficient appearance rendering in\ncomputer graphics. Being a joint, combinatorial decision-making problem\ninvolving all patch positions and orientations, this problem has well-known\nNP-hard complexity. Prior solutions either assume a heuristic packing order or\nmodify the upstream mesh cut and UV mapping to simplify the problem, which\neither limits the packing ratio or incurs robustness or generality issues.\nInstead, we introduce a learning-assisted 2D irregular shape packing method\nthat achieves a high packing quality with minimal requirements from the input.\nOur method iteratively selects and groups subsets of UV patches into\nnear-rectangular super patches, essentially reducing the problem to\nbin-packing, based on which a joint optimization is employed to further improve\nthe packing ratio. In order to efficiently deal with large problem instances\nwith hundreds of patches, we train deep neural policies to predict nearly\nrectangular patch subsets and determine their relative poses, leading to linear\ntime scaling with the number of patches. We demonstrate the effectiveness of\nour method on three datasets for UV packing, where our method achieves a higher\npacking ratio over several widely used baselines with competitive computational\nspeed.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.10329v1"
    },
    {
        "title": "Multisource Holography",
        "authors": [
            "Grace Kuo",
            "Florian Schiffers",
            "Douglas Lanman",
            "Oliver Cossairt",
            "Nathan Matsuda"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Holographic displays promise several benefits including high quality 3D\nimagery, accurate accommodation cues, and compact form-factors. However,\nholography relies on coherent illumination which can create undesirable speckle\nnoise in the final image. Although smooth phase holograms can be speckle-free,\ntheir non-uniform eyebox makes them impractical, and speckle mitigation with\npartially coherent sources also reduces resolution. Averaging sequential frames\nfor speckle reduction requires high speed modulators and consumes temporal\nbandwidth that may be needed elsewhere in the system.\n  In this work, we propose multisource holography, a novel architecture that\nuses an array of sources to suppress speckle in a single frame without\nsacrificing resolution. By using two spatial light modulators, arranged\nsequentially, each source in the array can be controlled almost independently\nto create a version of the target content with different speckle. Speckle is\nthen suppressed when the contributions from the multiple sources are averaged\nat the image plane. We introduce an algorithm to calculate multisource\nholograms, analyze the design space, and demonstrate up to a 10 dB increase in\npeak signal-to-noise ratio compared to an equivalent single source system.\nFinally, we validate the concept with a benchtop experimental prototype by\nproducing both 2D images and focal stacks with natural defocus cues.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.10816v1"
    },
    {
        "title": "C$\\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for\n  Physics-based Characters",
        "authors": [
            "Zhiyang Dou",
            "Xuelin Chen",
            "Qingnan Fan",
            "Taku Komura",
            "Wenping Wang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present C$\\cdot$ASE, an efficient and effective framework that learns\nconditional Adversarial Skill Embeddings for physics-based characters. Our\nphysically simulated character can learn a diverse repertoire of skills while\nproviding controllability in the form of direct manipulation of the skills to\nbe performed. C$\\cdot$ASE divides the heterogeneous skill motions into distinct\nsubsets containing homogeneous samples for training a low-level conditional\nmodel to learn conditional behavior distribution. The skill-conditioned\nimitation learning naturally offers explicit control over the character's\nskills after training. The training course incorporates the focal skill\nsampling, skeletal residual forces, and element-wise feature masking to balance\ndiverse skills of varying complexities, mitigate dynamics mismatch to master\nagile motions and capture more general behavior characteristics, respectively.\nOnce trained, the conditional model can produce highly diverse and realistic\nskills, outperforming state-of-the-art models, and can be repurposed in various\ndownstream tasks. In particular, the explicit skill control handle allows a\nhigh-level policy or user to direct the character with desired skill\nspecifications, which we demonstrate is advantageous for interactive character\nanimation.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.11351v1"
    },
    {
        "title": "pyParaOcean: A System for Visual Analysis of Ocean Data",
        "authors": [
            "Toshit Jain",
            "Varun Singh",
            "Vijay Kumar Boda",
            "Upkar Singh",
            "Ingrid Hotz",
            "P. N. Vinayachandran",
            "Vijay Natarajan"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Visual analysis is well adopted within the field of oceanography for the\nanalysis of model simulations, detection of different phenomena and events, and\ntracking of dynamic processes. With increasing data sizes and the availability\nof multivariate dynamic data, there is a growing need for scalable and\nextensible tools for visualization and interactive exploration. We describe\npyParaOcean, a visualization system that supports several tasks routinely used\nin the visual analysis of ocean data. The system is available as a plugin to\nParaview and is hence able to leverage its distributed computing capabilities\nand its rich set of generic analysis and visualization functionalities.\npyParaOcean provides modules to support different visual analysis tasks\nspecific to ocean data, such as eddy identification and salinity movement\ntracking. These modules are available as Paraview filters and this seamless\nintegration results in a system that is easy to install and use. A case study\non the Bay of Bengal illustrates the utility of the system for the study of\nocean phenomena and processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.14328v1"
    },
    {
        "title": "A Tutorial on Uniform B-Spline",
        "authors": [
            "Yi Zhou"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This document facilitates understanding of core concepts about uniform\nB-spline and its matrix representation.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.15477v1"
    },
    {
        "title": "Emotional Listener Portrait: Neural Listener Head Generation with\n  Emotion",
        "authors": [
            "Luchuan Song",
            "Guojun Yin",
            "Zhenchao Jin",
            "Xiaoyi Dong",
            "Chenliang Xu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Listener head generation centers on generating non-verbal behaviors (e.g.,\nsmile) of a listener in reference to the information delivered by a speaker. A\nsignificant challenge when generating such responses is the non-deterministic\nnature of fine-grained facial expressions during a conversation, which varies\ndepending on the emotions and attitudes of both the speaker and the listener.\nTo tackle this problem, we propose the Emotional Listener Portrait (ELP), which\ntreats each fine-grained facial motion as a composition of several discrete\nmotion-codewords and explicitly models the probability distribution of the\nmotions under different emotion in conversation. Benefiting from the\n``explicit'' and ``discrete'' design, our ELP model can not only automatically\ngenerate natural and diverse responses toward a given speaker via sampling from\nthe learned distribution but also generate controllable responses with a\npredetermined attitude. Under several quantitative metrics, our ELP exhibits\nsignificant improvements compared to previous methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.00068v2"
    },
    {
        "title": "AdaptNet: Policy Adaptation for Physics-Based Character Control",
        "authors": [
            "Pei Xu",
            "Kaixiang Xie",
            "Sheldon Andrews",
            "Paul G. Kry",
            "Michael Neff",
            "Morgan McGuire",
            "Ioannis Karamouzas",
            "Victor Zordan"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Motivated by humans' ability to adapt skills in the learning of new ones,\nthis paper presents AdaptNet, an approach for modifying the latent space of\nexisting policies to allow new behaviors to be quickly learned from like tasks\nin comparison to learning from scratch. Building on top of a given\nreinforcement learning controller, AdaptNet uses a two-tier hierarchy that\naugments the original state embedding to support modest changes in a behavior\nand further modifies the policy network layers to make more substantive\nchanges. The technique is shown to be effective for adapting existing\nphysics-based controllers to a wide range of new styles for locomotion, new\ntask targets, changes in character morphology and extensive changes in\nenvironment. Furthermore, it exhibits significant increase in learning\nefficiency, as indicated by greatly reduced training times when compared to\ntraining from scratch or using other approaches that modify existing policies.\nCode is available at https://motion-lab.github.io/AdaptNet.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.00239v3"
    },
    {
        "title": "Explorable Mesh Deformation Subspaces from Unstructured Generative\n  Models",
        "authors": [
            "Arman Maesumi",
            "Paul Guerrero",
            "Vladimir G. Kim",
            "Matthew Fisher",
            "Siddhartha Chaudhuri",
            "Noam Aigerman",
            "Daniel Ritchie"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Exploring variations of 3D shapes is a time-consuming process in traditional\n3D modeling tools. Deep generative models of 3D shapes often feature continuous\nlatent spaces that can, in principle, be used to explore potential variations\nstarting from a set of input shapes. In practice, doing so can be problematic:\nlatent spaces are high dimensional and hard to visualize, contain shapes that\nare not relevant to the input shapes, and linear paths through them often lead\nto sub-optimal shape transitions. Furthermore, one would ideally be able to\nexplore variations in the original high-quality meshes used to train the\ngenerative model, not its lower-quality output geometry. In this paper, we\npresent a method to explore variations among a given set of landmark shapes by\nconstructing a mapping from an easily-navigable 2D exploration space to a\nsubspace of a pre-trained generative model. We first describe how to find a\nmapping that spans the set of input landmark shapes and exhibits smooth\nvariations between them. We then show how to turn the variations in this\nsubspace into deformation fields, to transfer those variations to high-quality\nmeshes for the landmark shapes. Our results show that our method can produce\nvisually-pleasing and easily-navigable 2D exploration spaces for several\ndifferent shape categories, especially as compared to prior work on learning\ndeformation spaces for 3D shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.07814v1"
    },
    {
        "title": "Training and Predicting Visual Error for Real-Time Applications",
        "authors": [
            "João Libório Cardoso",
            "Bernhard Kerbl",
            "Lei Yang",
            "Yury Uralsky",
            "Michael Wimmer"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Visual error metrics play a fundamental role in the quantification of\nperceived image similarity. Most recently, use cases for them in real-time\napplications have emerged, such as content-adaptive shading and shading reuse\nto increase performance and improve efficiency. A wide range of different\nmetrics has been established, with the most sophisticated being capable of\ncapturing the perceptual characteristics of the human visual system. However,\ntheir complexity, computational expense, and reliance on reference images to\ncompare against prevent their generalized use in real-time, restricting such\napplications to using only the simplest available metrics. In this work, we\nexplore the abilities of convolutional neural networks to predict a variety of\nvisual metrics without requiring either reference or rendered images.\nSpecifically, we train and deploy a neural network to estimate the visual error\nresulting from reusing shading or using reduced shading rates. The resulting\nmodels account for 70%-90% of the variance while achieving up to an order of\nmagnitude faster computation times. Our solution combines image-space\ninformation that is readily available in most state-of-the-art deferred shading\npipelines with reprojection from previous frames to enable an adequate estimate\nof visual errors, even in previously unseen regions. We describe a suitable\nconvolutional network architecture and considerations for data preparation for\ntraining. We demonstrate the capability of our network to predict complex error\nmetrics at interactive rates in a real-time application that implements\ncontent-adaptive shading in a deferred pipeline. Depending on the portion of\nunseen image regions, our approach can achieve up to $2\\times$ performance\ncompared to state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.09125v1"
    },
    {
        "title": "ShanshuiDaDA: An Interactive, Generative System towards Chinese Shanshui\n  Painting",
        "authors": [
            "Aven Le Zhou",
            "Qiufeng Wang",
            "Cheng-Hung Lo",
            "Kaizhu Huang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Shanshui, which means mountain and water, is an East Asian traditional brush\npainting involving natural landscapes. This paper proposes an interactive and\ngenerative system based on a Generative Adversarial Network(GAN), which helps\nusers draw Shanshui easily. We name this system and installation ShanshuiDaDA.\nShanshuiDaDA is trained with CycleGAN and wrapped with a web-based interface.\nWhen participants scribble lines and sketch the landscape, the ShanshuiDaDA\nwill assist them in generating and creating a Chinese \"Shanshui\" painting in\nreal time.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.19803v1"
    },
    {
        "title": "Arkade: k-Nearest Neighbor Search With Non-Euclidean Distances using GPU\n  Ray Tracing",
        "authors": [
            "Durga Mandarapu",
            "Vani Nagarajan",
            "Artem Pelenitsyn",
            "Milind Kulkarni"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  High-performance implementations of $k$-Nearest Neighbor Search ($k$NN) in\nlow dimensions use tree-based data structures. Tree algorithms are hard to\nparallelize on GPUs due to their irregularity. However, newer Nvidia GPUs offer\nhardware support for tree operations through ray-tracing cores. Recent works\nhave proposed using RT cores to implement $k$NN search, but they all have a\nhardware-imposed constraint on the distance metric used in the search -- the\nEuclidean distance. We propose and implement two reductions to support $k$NN\nfor a broad range of distances other than the Euclidean distance: Arkade\nFilter-Refine and Arkade Monotone Transformation, each of which allows\nnon-Euclidean distance-based nearest neighbor queries to be performed in terms\nof the Euclidean distance. With our reductions, we observe that $k$NN search\ntime speedups range between $1.6$x-$200$x and $1.3$x-$33.1$x over various\nstate-of-the-art GPU shader core and RT core baselines, respectively. In\nevaluation, we provide several insights on RT architectures' ability to\nefficiently build and traverse the tree by analyzing the $k$NN search time\ntrends.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.09168v2"
    },
    {
        "title": "PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics",
        "authors": [
            "Tianyi Xie",
            "Zeshun Zong",
            "Yuxing Qiu",
            "Xuan Li",
            "Yutao Feng",
            "Yin Yang",
            "Chenfanfu Jiang"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce PhysGaussian, a new method that seamlessly integrates physically\ngrounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel\nmotion synthesis. Employing a custom Material Point Method (MPM), our approach\nenriches 3D Gaussian kernels with physically meaningful kinematic deformation\nand mechanical stress attributes, all evolved in line with continuum mechanics\nprinciples. A defining characteristic of our method is the seamless integration\nbetween physical simulation and visual rendering: both components utilize the\nsame 3D Gaussian kernels as their discrete representations. This negates the\nnecessity for triangle/tetrahedron meshing, marching cubes, \"cage meshes,\" or\nany other geometry embedding, highlighting the principle of \"what you see is\nwhat you simulate (WS$^2$).\" Our method demonstrates exceptional versatility\nacross a wide variety of materials--including elastic entities, metals,\nnon-Newtonian fluids, and granular materials--showcasing its strong\ncapabilities in creating diverse visual content with novel viewpoints and\nmovements. Our project page is at: https://xpandora.github.io/PhysGaussian/\n",
        "pdf_link": "http://arxiv.org/pdf/2311.12198v3"
    },
    {
        "title": "Unrolling Virtual Worlds for Immersive Experiences",
        "authors": [
            "Alexey Tikhonov",
            "Anton Repushko"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  This research pioneers a method for generating immersive worlds, drawing\ninspiration from elements of vintage adventure games like Myst and employing\nmodern text-to-image models. We explore the intricate conversion of 2D\npanoramas into 3D scenes using equirectangular projections, addressing the\ndistortions in perception that occur as observers navigate within the\nencompassing sphere. Our approach employs a technique similar to \"inpainting\"\nto rectify distorted projections, enabling the smooth construction of locally\ncoherent worlds. This provides extensive insight into the interrelation of\ntechnology, perception, and experiential reality within human-computer\ninteraction.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.17924v1"
    },
    {
        "title": "Efficient Incremental Potential Contact for Actuated Face Simulation",
        "authors": [
            "Bo Li",
            "Lingchen Yang",
            "Barbara Solenthaler"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We present a quasi-static finite element simulator for human face animation.\nWe model the face as an actuated soft body, which can be efficiently simulated\nusing Projective Dynamics (PD). We adopt Incremental Potential Contact (IPC) to\nhandle self-intersection. However, directly integrating IPC into the simulation\nwould impede the high efficiency of the PD solver, since the stiffness matrix\nin the global step is no longer constant and cannot be pre-factorized. We\nnotice that the actual number of vertices affected by the collision is only a\nsmall fraction of the whole model, and by utilizing this fact we effectively\ndecrease the scale of the linear system to be solved. With the proposed\noptimization method for collision, we achieve high visual fidelity at a\nrelatively low performance overhead.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.02999v1"
    },
    {
        "title": "3D-PreMise: Can Large Language Models Generate 3D Shapes with Sharp\n  Features and Parametric Control?",
        "authors": [
            "Zeqing Yuan",
            "Haoxuan Lan",
            "Qiang Zou",
            "Junbo Zhao"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Recent advancements in implicit 3D representations and generative models have\nmarkedly propelled the field of 3D object generation forward. However, it\nremains a significant challenge to accurately model geometries with defined\nsharp features under parametric controls, which is crucial in fields like\nindustrial design and manufacturing. To bridge this gap, we introduce a\nframework that employs Large Language Models (LLMs) to generate text-driven 3D\nshapes, manipulating 3D software via program synthesis. We present 3D-PreMise,\na dataset specifically tailored for 3D parametric modeling of industrial\nshapes, designed to explore state-of-the-art LLMs within our proposed pipeline.\nOur work reveals effective generation strategies and delves into the\nself-correction capabilities of LLMs using a visual interface. Our work\nhighlights both the potential and limitations of LLMs in 3D parametric modeling\nfor industrial applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.06437v1"
    },
    {
        "title": "Diverse Part Synthesis for 3D Shape Creation",
        "authors": [
            "Yanran Guan",
            "Oliver van Kaick"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Methods that use neural networks for synthesizing 3D shapes in the form of a\npart-based representation have been introduced over the last few years. These\nmethods represent shapes as a graph or hierarchy of parts and enable a variety\nof applications such as shape sampling and reconstruction. However, current\nmethods do not allow easily regenerating individual shape parts according to\nuser preferences. In this paper, we investigate techniques that allow the user\nto generate multiple, diverse suggestions for individual parts. Specifically,\nwe experiment with multimodal deep generative models that allow sampling\ndiverse suggestions for shape parts and focus on models which have not been\nconsidered in previous work on shape synthesis. To provide a comparative study\nof these techniques, we introduce a method for synthesizing 3D shapes in a\npart-based representation and evaluate all the part suggestion techniques\nwithin this synthesis method. In our method, which is inspired by previous\nwork, shapes are represented as a set of parts in the form of implicit\nfunctions which are then positioned in space to form the final shape. Synthesis\nin this representation is enabled by a neural network architecture based on an\nimplicit decoder and a spatial transformer. We compare the various multimodal\ngenerative models by evaluating their performance in generating part\nsuggestions. Our contribution is to show with qualitative and quantitative\nevaluations which of the new techniques for multimodal part generation perform\nthe best and that a synthesis method based on the top-performing techniques\nallows the user to more finely control the parts that are generated in the 3D\nshapes while maintaining high shape fidelity when reconstructing shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.09384v5"
    },
    {
        "title": "PatternPortrait: Draw Me Like One of Your Scribbles",
        "authors": [
            "Sabine Wieluch",
            "Friedhelm Schwenker"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper introduces a process for generating abstract portrait drawings\nfrom pictures. Their unique style is created by utilizing single freehand\npattern sketches as references to generate unique patterns for shading. The\nmethod involves extracting facial and body features from images and\ntransforming them into vector lines. A key aspect of the research is the\ndevelopment of a graph neural network architecture designed to learn sketch\nstroke representations in vector form, enabling the generation of diverse\nstroke variations. The combination of these two approaches creates joyful\nabstract drawings that are realized via a pen plotter. The presented process\ngarnered positive feedback from an audience of approximately 280 participants.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.13001v1"
    },
    {
        "title": "GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface\n  Scattering Representation",
        "authors": [
            "Barış Yıldırım",
            "Murat Kurt"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper presents a plugin that adds a representation of homogeneous and\nheterogeneous, optically thick, translucent materials on the Blender 3D\nmodeling tool. The working principle of this plugin is based on a combination\nof Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based\nsubsurface scattering method (GenSSS). The proposed plugin has been implemented\nusing Mitsuba renderer, which is an open source rendering software. The\nproposed plugin has been validated on measured subsurface scattering data. It's\nshown that the proposed plugin visualizes homogeneous and heterogeneous\nsubsurface scattering effects, accurately, compactly and computationally\nefficiently.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15245v1"
    },
    {
        "title": "Gaussian Splashing: Unified Particles for Versatile Motion Synthesis and\n  Rendering",
        "authors": [
            "Yutao Feng",
            "Xiang Feng",
            "Yintong Shang",
            "Ying Jiang",
            "Chang Yu",
            "Zeshun Zong",
            "Tianjia Shao",
            "Hongzhi Wu",
            "Kun Zhou",
            "Chenfanfu Jiang",
            "Yin Yang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We demonstrate the feasibility of integrating physics-based animations of\nsolids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in\nvirtual scenes reconstructed using 3DGS. Leveraging the coherence of the\nGaussian Splatting and Position-Based Dynamics (PBD) in the underlying\nrepresentation, we manage rendering, view synthesis, and the dynamics of solids\nand fluids in a cohesive manner. Similar to GaussianShader, we enhance each\nGaussian kernel with an added normal, aligning the kernel's orientation with\nthe surface normal to refine the PBD simulation. This approach effectively\neliminates spiky noises that arise from rotational deformation in solids. It\nalso allows us to integrate physically based rendering to augment the dynamic\nsurface reflections on fluids. Consequently, our framework is capable of\nrealistically reproducing surface highlights on dynamic fluids and facilitating\ninteractions between scene objects and fluids from new views. For more\ninformation, please visit our project page at\n\\url{https://gaussiansplashing.github.io/}.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15318v2"
    },
    {
        "title": "Neural Rendering and Its Hardware Acceleration: A Review",
        "authors": [
            "Xinkai Yan",
            "Jieting Xu",
            "Yuchi Huo",
            "Hujun Bao"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Neural rendering is a new image and video generation method based on deep\nlearning. It combines the deep learning model with the physical knowledge of\ncomputer graphics, to obtain a controllable and realistic scene model, and\nrealize the control of scene attributes such as lighting, camera parameters,\nposture and so on. On the one hand, neural rendering can not only make full use\nof the advantages of deep learning to accelerate the traditional forward\nrendering process, but also provide new solutions for specific tasks such as\ninverse rendering and 3D reconstruction. On the other hand, the design of\ninnovative hardware structures that adapt to the neural rendering pipeline\nbreaks through the parallel computing and power consumption bottleneck of\nexisting graphics processors, which is expected to provide important support\nfor future key areas such as virtual and augmented reality, film and television\ncreation and digital entertainment, artificial intelligence and the metaverse.\nIn this paper, we review the technical connotation, main challenges, and\nresearch progress of neural rendering. On this basis, we analyze the common\nrequirements of neural rendering pipeline for hardware acceleration and the\ncharacteristics of the current hardware acceleration architecture, and then\ndiscuss the design challenges of neural rendering processor architecture.\nFinally, the future development trend of neural rendering processor\narchitecture is prospected.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00028v1"
    },
    {
        "title": "FlashTex: Fast Relightable Mesh Texturing with LightControlNet",
        "authors": [
            "Kangle Deng",
            "Timothy Omernick",
            "Alexander Weiss",
            "Deva Ramanan",
            "Jun-Yan Zhu",
            "Tinghui Zhou",
            "Maneesh Agrawala"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Manually creating textures for 3D meshes is time-consuming, even for expert\nvisual content creators. We propose a fast approach for automatically texturing\nan input 3D mesh based on a user-provided text prompt. Importantly, our\napproach disentangles lighting from surface material/reflectance in the\nresulting texture so that the mesh can be properly relit and rendered in any\nlighting environment. We introduce LightControlNet, a new text-to-image model\nbased on the ControlNet architecture, which allows the specification of the\ndesired lighting as a conditioning image to the model. Our text-to-texture\npipeline then constructs the texture in two stages. The first stage produces a\nsparse set of visually consistent reference views of the mesh using\nLightControlNet. The second stage applies a texture optimization based on Score\nDistillation Sampling (SDS) that works with LightControlNet to increase the\ntexture quality while disentangling surface material from lighting. Our\nalgorithm is significantly faster than previous text-to-texture methods, while\nproducing high-quality and relightable textures.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.13251v3"
    },
    {
        "title": "Image-To-Mesh Conversion for Biomedical Simulations",
        "authors": [
            "Fotis Drakopoulos",
            "Kevin Garner",
            "Christopher Rector",
            "Nikos Chrisochoides"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Converting a three-dimensional medical image into a 3D mesh that satisfies\nboth the quality and fidelity constraints of predictive simulations and\nimage-guided surgical procedures remains a critical problem. Presented is an\nimage-to-mesh conversion method called CBC3D. It first discretizes a segmented\nimage by generating an adaptive Body-Centered Cubic (BCC) mesh of high-quality\nelements. Next, the tetrahedral mesh is converted into a mixed-element mesh of\ntetrahedra, pentahedra, and hexahedra to decrease element count while\nmaintaining quality. Finally, the mesh surfaces are deformed to their\ncorresponding physical image boundaries, improving the mesh's fidelity. The\ndeformation scheme builds upon the ITK open-source library and is based on the\nconcept of energy minimization, relying on a multi-material point-based\nregistration. It uses non-connectivity patterns to implicitly control the\nnumber of extracted feature points needed for the registration and, thus,\nadjusts the trade-off between the achieved mesh fidelity and the deformation\nspeed. We compare CBC3D with four widely used and state-of-the-art homegrown\nimage-to-mesh conversion methods from industry and academia. Results indicate\nthat the CBC3D meshes (i) achieve high fidelity, (ii) keep the element count\nreasonably low, and (iii) exhibit good element quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.18596v1"
    },
    {
        "title": "Computer-Controlled 3D Freeform Surface Weaving",
        "authors": [
            "Xiangjia Chen",
            "Lip M. Lai",
            "Zishun Liu",
            "Chengkai Dai",
            "Isaac C. W. Leung",
            "Charlie C. L. Wang",
            "Yeung Yam"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this paper, we present a new computer-controlled weaving technology that\nenables the fabrication of woven structures in the shape of given 3D surfaces\nby using threads in non-traditional materials with high bending-stiffness,\nallowing for multiple applications with the resultant woven fabrics. A new\nweaving machine and a new manufacturing process are developed to realize the\nfunction of 3D surface weaving by the principle of short-row shaping. A\ncomputational solution is investigated to convert input 3D freeform surfaces\ninto the corresponding weaving operations (indicated as W-code) to guide the\noperation of this system. A variety of examples using cotton threads,\nconductive threads and optical fibres are fabricated by our prototype system to\ndemonstrate its functionality.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00473v2"
    },
    {
        "title": "Compositional Neural Textures",
        "authors": [
            "Peihan Tu",
            "Li-Yi Wei",
            "Matthias Zwicker"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Texture plays a vital role in enhancing visual richness in both real\nphotographs and computer-generated imagery. However, the process of editing\ntextures often involves laborious and repetitive manual adjustments of textons,\nwhich are the recurring local patterns that characterize textures. This work\nintroduces a fully unsupervised approach for representing textures using a\ncompositional neural model that captures individual textons. We represent each\ntexton as a 2D Gaussian function whose spatial support approximates its shape,\nand an associated feature that encodes its detailed appearance. By modeling a\ntexture as a discrete composition of Gaussian textons, the representation\noffers both expressiveness and ease of editing. Textures can be edited by\nmodifying the compositional Gaussians within the latent space, and new textures\ncan be efficiently synthesized by feeding the modified Gaussians through a\ngenerator network in a feed-forward manner. This approach enables a wide range\nof applications, including transferring appearance from an image texture to\nanother image, diversifying textures,texture interpolation, revealing/modifying\ntexture variations, edit propagation, texture animation, and direct texton\nmanipulation. The proposed approach contributes to advancing texture analysis,\nmodeling, and editing techniques, and opens up new possibilities for creating\nvisually appealing images with controllable textures.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.12509v2"
    },
    {
        "title": "Taming Diffusion Probabilistic Models for Character Control",
        "authors": [
            "Rui Chen",
            "Mingyi Shi",
            "Shaoli Huang",
            "Ping Tan",
            "Taku Komura",
            "Xuelin Chen"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a novel character control framework that effectively utilizes\nmotion diffusion probabilistic models to generate high-quality and diverse\ncharacter animations, responding in real-time to a variety of dynamic\nuser-supplied control signals. At the heart of our method lies a\ntransformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM),\nwhich takes as input the character's historical motion and can generate a range\nof diverse potential future motions conditioned on high-level, coarse user\ncontrol. To meet the demands for diversity, controllability, and computational\nefficiency required by a real-time controller, we incorporate several key\nalgorithmic designs. These include separate condition tokenization,\nclassifier-free guidance on past motion, and heuristic future trajectory\nextension, all designed to address the challenges associated with taming motion\ndiffusion probabilistic models for character control. As a result, our work\nrepresents the first model that enables real-time generation of high-quality,\ndiverse character animations based on user interactive control, supporting\nanimating the character in multiple styles with a single unified model. We\nevaluate our method on a diverse set of locomotion skills, demonstrating the\nmerits of our method over existing character controllers. Project page and\nsource codes: https://aiganimation.github.io/CAMDM/\n",
        "pdf_link": "http://arxiv.org/pdf/2404.15121v1"
    },
    {
        "title": "DreamCraft: Text-Guided Generation of Functional 3D Environments in\n  Minecraft",
        "authors": [
            "Sam Earle",
            "Filippos Kokkinos",
            "Yuhe Nie",
            "Julian Togelius",
            "Roberta Raileanu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Procedural Content Generation (PCG) algorithms enable the automatic\ngeneration of complex and diverse artifacts. However, they don't provide\nhigh-level control over the generated content and typically require domain\nexpertise. In contrast, text-to-3D methods allow users to specify desired\ncharacteristics in natural language, offering a high amount of flexibility and\nexpressivity. But unlike PCG, such approaches cannot guarantee functionality,\nwhich is crucial for certain applications like game design. In this paper, we\npresent a method for generating functional 3D artifacts from free-form text\nprompts in the open-world game Minecraft. Our method, DreamCraft, trains\nquantized Neural Radiance Fields (NeRFs) to represent artifacts that, when\nviewed in-game, match given text descriptions. We find that DreamCraft produces\nmore aligned in-game artifacts than a baseline that post-processes the output\nof an unconstrained NeRF. Thanks to the quantized representation of the\nenvironment, functional constraints can be integrated using specialized loss\nterms. We show how this can be leveraged to generate 3D structures that match a\ntarget distribution or obey certain adjacency rules over the block types.\nDreamCraft inherits a high degree of expressivity and controllability from the\nNeRF, while still being able to incorporate functional constraints through\ndomain-specific objectives.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.15538v1"
    },
    {
        "title": "CWF: Consolidating Weak Features in High-quality Mesh Simplification",
        "authors": [
            "Rui Xu",
            "Longdu Liu",
            "Ningna Wang",
            "Shuangmin Chen",
            "Shiqing Xin",
            "Xiaohu Guo",
            "Zichun Zhong",
            "Taku Komura",
            "Wenping Wang",
            "Changhe Tu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In mesh simplification, common requirements like accuracy, triangle quality,\nand feature alignment are often considered as a trade-off. Existing algorithms\nconcentrate on just one or a few specific aspects of these requirements. For\nexample, the well-known Quadric Error Metrics (QEM) approach prioritizes\naccuracy and can preserve strong feature lines/points as well but falls short\nin ensuring high triangle quality and may degrade weak features that are not as\ndistinctive as strong ones. In this paper, we propose a smooth functional that\nsimultaneously considers all of these requirements. The functional comprises a\nnormal anisotropy term and a Centroidal Voronoi Tessellation (CVT) energy term,\nwith the variables being a set of movable points lying on the surface. The\nformer inherits the spirit of QEM but operates in a continuous setting, while\nthe latter encourages even point distribution, allowing various surface\nmetrics. We further introduce a decaying weight to automatically balance the\ntwo terms. We selected 100 CAD models from the ABC dataset, along with 21\norganic models, to compare the existing mesh simplification algorithms with\nours. Experimental results reveal an important observation: the introduction of\na decaying weight effectively reduces the conflict between the two terms and\nenables the alignment of weak features. This distinctive feature sets our\napproach apart from most existing mesh simplification methods and demonstrates\nsignificant potential in shape understanding.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.15661v1"
    },
    {
        "title": "Velocity-Based Monte Carlo Fluids",
        "authors": [
            "Ryusuke Sugimoto",
            "Christopher Batty",
            "Toshiya Hachisuka"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a velocity-based Monte Carlo fluid solver that overcomes the\nlimitations of its existing vorticity-based counterpart. Because the\nvelocity-based formulation is more commonly used in graphics, our Monte Carlo\nsolver can be readily extended with various techniques from the fluid\nsimulation literature. We derive our method by solving the Navier-Stokes\nequations via operator splitting and designing a pointwise Monte Carlo\nestimator for each substep. We reformulate the projection and diffusion steps\nas integration problems based on the recently introduced walk-on-boundary\ntechnique [Sugimoto et al. 2023]. We transform the volume integral arising from\nthe source term of the pressure Poisson equation into a form more amenable to\npractical numerical evaluation. Our resulting velocity-based formulation allows\nfor the proper simulation of scenes that the prior vorticity-based Monte Carlo\nmethod [Rioux-Lavoie and Sugimoto et al. 2022] either simulates incorrectly or\ncannot support. We demonstrate that our method can easily incorporate\nadvancements drawn from conventional non-Monte Carlo methods by showing how one\ncan straightforwardly add buoyancy effects, divergence control capabilities,\nand numerical dissipation reduction methods, such as advection-reflection and\nPIC/FLIP methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.16274v1"
    },
    {
        "title": "One Noise to Rule Them All: Learning a Unified Model of\n  Spatially-Varying Noise Patterns",
        "authors": [
            "Arman Maesumi",
            "Dylan Hu",
            "Krishi Saripalli",
            "Vladimir G. Kim",
            "Matthew Fisher",
            "Sören Pirk",
            "Daniel Ritchie"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Procedural noise is a fundamental component of computer graphics pipelines,\noffering a flexible way to generate textures that exhibit \"natural\" random\nvariation. Many different types of noise exist, each produced by a separate\nalgorithm. In this paper, we present a single generative model which can learn\nto generate multiple types of noise as well as blend between them. In addition,\nit is capable of producing spatially-varying noise blends despite not having\naccess to such data for training. These features are enabled by training a\ndenoising diffusion model using a novel combination of data augmentation and\nnetwork conditioning techniques. Like procedural noise generators, the model's\nbehavior is controllable via interpretable parameters and a source of\nrandomness. We use our model to produce a variety of visually compelling noise\ntextures. We also present an application of our model to improving inverse\nprocedural material design; using our model in place of fixed-type noise nodes\nin a procedural material graph results in higher-fidelity material\nreconstructions without needing to know the type of noise in advance.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.16292v1"
    },
    {
        "title": "Bootstrap 3D Reconstructed Scenes from 3D Gaussian Splatting",
        "authors": [
            "Yifei Gao",
            "Jie Ou",
            "Lei Wang",
            "Jun Cheng"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Recent developments in neural rendering techniques have greatly enhanced the\nrendering of photo-realistic 3D scenes across both academic and commercial\nfields. The latest method, known as 3D Gaussian Splatting (3D-GS), has set new\nbenchmarks for rendering quality and speed. Nevertheless, the limitations of\n3D-GS become pronounced in synthesizing new viewpoints, especially for views\nthat greatly deviate from those seen during training. Additionally, issues such\nas dilation and aliasing arise when zooming in or out. These challenges can all\nbe traced back to a single underlying issue: insufficient sampling. In our\npaper, we present a bootstrapping method that significantly addresses this\nproblem. This approach employs a diffusion model to enhance the rendering of\nnovel views using trained 3D-GS, thereby streamlining the training process. Our\nresults indicate that bootstrapping effectively reduces artifacts, as well as\nclear enhancements on the evaluation metrics. Furthermore, we show that our\nmethod is versatile and can be easily integrated, allowing various 3D\nreconstruction projects to benefit from our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.18669v2"
    },
    {
        "title": "Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis",
        "authors": [
            "Zeyi Zhang",
            "Tenglong Ao",
            "Yuyao Zhang",
            "Qingzhe Gao",
            "Chuan Lin",
            "Baoquan Chen",
            "Libin Liu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this work, we present Semantic Gesticulator, a novel framework designed to\nsynthesize realistic gestures accompanying speech with strong semantic\ncorrespondence. Semantically meaningful gestures are crucial for effective\nnon-verbal communication, but such gestures often fall within the long tail of\nthe distribution of natural human motion. The sparsity of these movements makes\nit challenging for deep learning-based systems, trained on moderately sized\ndatasets, to capture the relationship between the movements and the\ncorresponding speech semantics. To address this challenge, we develop a\ngenerative retrieval framework based on a large language model. This framework\nefficiently retrieves suitable semantic gesture candidates from a motion\nlibrary in response to the input speech. To construct this motion library, we\nsummarize a comprehensive list of commonly used semantic gestures based on\nfindings in linguistics, and we collect a high-quality motion dataset\nencompassing both body and hand movements. We also design a novel GPT-based\nmodel with strong generalization capabilities to audio, capable of generating\nhigh-quality gestures that match the rhythm of speech. Furthermore, we propose\na semantic alignment mechanism to efficiently align the retrieved semantic\ngestures with the GPT's output, ensuring the naturalness of the final\nanimation. Our system demonstrates robustness in generating gestures that are\nrhythmically coherent and semantically explicit, as evidenced by a\ncomprehensive collection of examples. User studies confirm the quality and\nhuman-likeness of our results, and show that our system outperforms\nstate-of-the-art systems in terms of semantic appropriateness by a clear\nmargin.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.09814v2"
    },
    {
        "title": "Exposure Diffusion: HDR Image Generation by Consistent LDR denoising",
        "authors": [
            "Mojtaba Bemana",
            "Thomas Leimkühler",
            "Karol Myszkowski",
            "Hans-Peter Seidel",
            "Tobias Ritschel"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We demonstrate generating high-dynamic range (HDR) images using the concerted\naction of multiple black-box, pre-trained low-dynamic range (LDR) image\ndiffusion models. Common diffusion models are not HDR as, first, there is no\nsufficiently large HDR image dataset available to re-train them, and second,\neven if it was, re-training such models is impossible for most compute budgets.\nInstead, we seek inspiration from the HDR image capture literature that\ntraditionally fuses sets of LDR images, called \"brackets\", to produce a single\nHDR image. We operate multiple denoising processes to generate multiple LDR\nbrackets that together form a valid HDR result. To this end, we introduce an\nexposure consistency term into the diffusion process to couple the brackets\nsuch that they agree across the exposure range they share. We demonstrate HDR\nversions of state-of-the-art unconditional and conditional as well as\nrestoration-type (LDR2HDR) generative modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.14304v1"
    },
    {
        "title": "End-to-End Hybrid Refractive-Diffractive Lens Design with Differentiable\n  Ray-Wave Model",
        "authors": [
            "Xinge Yang",
            "Matheus Souza",
            "Kunyi Wang",
            "Praneeth Chakravarthula",
            "Qiang Fu",
            "Wolfgang Heidrich"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Hybrid refractive-diffractive lenses combine the light efficiency of\nrefractive lenses with the information encoding power of diffractive optical\nelements (DOE), showing great potential as the next generation of imaging\nsystems. However, accurately simulating such hybrid designs is generally\ndifficult, and in particular, there are no existing differentiable image\nformation models for hybrid lenses with sufficient accuracy.\n  In this work, we propose a new hybrid ray-tracing and wave-propagation\n(ray-wave) model for accurate simulation of both optical aberrations and\ndiffractive phase modulation, where the DOE is placed between the last\nrefractive surface and the image sensor, i.e. away from the Fourier plane that\nis often used as a DOE position. The proposed ray-wave model is fully\ndifferentiable, enabling gradient back-propagation for end-to-end co-design of\nrefractive-diffractive lens optimization and the image reconstruction network.\nWe validate the accuracy of the proposed model by comparing the simulated point\nspread functions (PSFs) with theoretical results, as well as simulation\nexperiments that show our model to be more accurate than solutions implemented\nin commercial software packages like Zemax. We demonstrate the effectiveness of\nthe proposed model through real-world experiments and show significant\nimprovements in both aberration correction and extended depth-of-field (EDoF)\nimaging. We believe the proposed model will motivate further investigation into\na wide range of applications in computational imaging, computational\nphotography, and advanced optical design. Code will be released upon\npublication.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.00834v1"
    },
    {
        "title": "Stabler Neo-Hookean Simulation: Absolute Eigenvalue Filtering for\n  Projected Newton",
        "authors": [
            "Honglin Chen",
            "Hsueh-Ti Derek Liu",
            "David I. W. Levin",
            "Changxi Zheng",
            "Alec Jacobson"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Volume-preserving hyperelastic materials are widely used to model\nnear-incompressible materials such as rubber and soft tissues. However, the\nnumerical simulation of volume-preserving hyperelastic materials is notoriously\nchallenging within this regime due to the non-convexity of the energy function.\nIn this work, we identify the pitfalls of the popular eigenvalue clamping\nstrategy for projecting Hessian matrices to positive semi-definiteness during\nNewton's method. We introduce a novel eigenvalue filtering strategy for\nprojected Newton's method to stabilize the optimization of Neo-Hookean energy\nand other volume-preserving variants under high Poisson's ratio (near 0.5) and\nlarge initial volume change. Our method only requires a single line of code\nchange in the existing projected Newton framework, while achieving significant\nimprovement in both stability and convergence speed. We demonstrate the\neffectiveness and efficiency of our eigenvalue projection scheme on a variety\nof challenging examples and over different deformations on a large dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.05928v3"
    },
    {
        "title": "Learning Images Across Scales Using Adversarial Training",
        "authors": [
            "Krzysztof Wolski",
            "Adarsh Djeacoumar",
            "Alireza Javanmardi",
            "Hans-Peter Seidel",
            "Christian Theobalt",
            "Guillaume Cordonnier",
            "Karol Myszkowski",
            "George Drettakis",
            "Xingang Pan",
            "Thomas Leimkühler"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The real world exhibits rich structure and detail across many scales of\nobservation. It is difficult, however, to capture and represent a broad\nspectrum of scales using ordinary images. We devise a novel paradigm for\nlearning a representation that captures an orders-of-magnitude variety of\nscales from an unstructured collection of ordinary images. We treat this\ncollection as a distribution of scale-space slices to be learned using\nadversarial training, and additionally enforce coherency across slices. Our\napproach relies on a multiscale generator with carefully injected procedural\nfrequency content, which allows to interactively explore the emerging\ncontinuous scale space. Training across vastly different scales poses\nchallenges regarding stability, which we tackle using a supervision scheme that\ninvolves careful sampling of scales. We show that our generator can be used as\na multiscale generative model, and for reconstructions of scale spaces from\nunstructured patches. Significantly outperforming the state of the art, we\ndemonstrate zoom-in factors of up to 256x at high quality and scale\nconsistency.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.08924v1"
    },
    {
        "title": "Learning from landmarks, curves, surfaces, and shapes in Geomstats",
        "authors": [
            "Luís F. Pereira",
            "Alice Le Brigant",
            "Adele Myers",
            "Emmanuel Hartman",
            "Amil Khan",
            "Malik Tuerkoen",
            "Trey Dold",
            "Mengyang Gu",
            "Pablo Suárez-Serrato",
            "Nina Miolane"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce the shape module of the Python package Geomstats to analyze\nshapes of objects represented as landmarks, curves and surfaces across fields\nof natural sciences and engineering. The shape module first implements widely\nused shape spaces, such as the Kendall shape space, as well as elastic spaces\nof discrete curves and surfaces. The shape module further implements the\nabstract mathematical structures of group actions, fiber bundles, quotient\nspaces and associated Riemannian metrics which allow users to build their own\nshape spaces. The Riemannian geometry tools enable users to compare, average,\ninterpolate between shapes inside a given shape space. These essential\noperations can then be leveraged to perform statistics and machine learning on\nshape data. We present the object-oriented implementation of the shape module\nalong with illustrative examples and show how it can be used to perform\nstatistics and machine learning on shape spaces.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.10437v1"
    },
    {
        "title": "Pattern or Artifact? Interactively Exploring Embedding Quality with\n  TRACE",
        "authors": [
            "Edith Heiter",
            "Liesbet Martens",
            "Ruth Seurinck",
            "Martin Guilliams",
            "Tijl De Bie",
            "Yvan Saeys",
            "Jefrey Lijffijt"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper presents TRACE, a tool to analyze the quality of 2D embeddings\ngenerated through dimensionality reduction techniques. Dimensionality reduction\nmethods often prioritize preserving either local neighborhoods or global\ndistances, but insights from visual structures can be misleading if the\nobjective has not been achieved uniformly. TRACE addresses this challenge by\nproviding a scalable and extensible pipeline for computing both local and\nglobal quality measures. The interactive browser-based interface allows users\nto explore various embeddings while visually assessing the pointwise embedding\nquality. The interface also facilitates in-depth analysis by highlighting\nhigh-dimensional nearest neighbors for any group of points and displaying\nhigh-dimensional distances between points. TRACE enables analysts to make\ninformed decisions regarding the most suitable dimensionality reduction method\nfor their specific use case, by showing the degree and location where structure\nis preserved in the reduced space.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.12953v1"
    },
    {
        "title": "DragPoser: Motion Reconstruction from Variable Sparse Tracking Signals\n  via Latent Space Optimization",
        "authors": [
            "Jose Luis Ponton",
            "Eduard Pujol",
            "Andreas Aristidou",
            "Carlos Andujar",
            "Nuria Pelechano"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  High-quality motion reconstruction that follows the user's movements can be\nachieved by high-end mocap systems with many sensors. However, obtaining such\nanimation quality with fewer input devices is gaining popularity as it brings\nmocap closer to the general public. The main challenges include the loss of\nend-effector accuracy in learning-based approaches, or the lack of naturalness\nand smoothness in IK-based solutions. In addition, such systems are often\nfinely tuned to a specific number of trackers and are highly sensitive to\nmissing data e.g., in scenarios where a sensor is occluded or malfunctions. In\nresponse to these challenges, we introduce DragPoser, a novel\ndeep-learning-based motion reconstruction system that accurately represents\nhard and dynamic on-the-fly constraints, attaining real-time high end-effectors\nposition accuracy. This is achieved through a pose optimization process within\na structured latent space. Our system requires only one-time training on a\nlarge human motion dataset, and then constraints can be dynamically defined as\nlosses, while the pose is iteratively refined by computing the gradients of\nthese losses within the latent space. To further enhance our approach, we\nincorporate a Temporal Predictor network, which employs a Transformer\narchitecture to directly encode temporality within the latent space. This\nnetwork ensures the pose optimization is confined to the manifold of valid\nposes and also leverages past pose data to predict temporally coherent poses.\nResults demonstrate that DragPoser surpasses both IK-based and the latest\ndata-driven methods in achieving precise end-effector positioning, while it\nproduces natural poses and temporally coherent motion. In addition, our system\nshowcases robustness against on-the-fly constraint modifications, and exhibits\nexceptional adaptability to various input configurations and changes.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.14567v1"
    },
    {
        "title": "Time-varying Extremum Graphs",
        "authors": [
            "Somenath Das",
            "Raghavendra Sridharamurthy",
            "Vijay Natarajan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce time-varying extremum graph (TVEG), a topological structure to\nsupport visualization and analysis of a time-varying scalar field. The extremum\ngraph is a substructure of the Morse-Smale complex. It captures the adjacency\nrelationship between cells in the Morse decomposition of a scalar field. We\ndefine the TVEG as a time-varying extension of the extremum graph and\ndemonstrate how it captures salient feature tracks within a dynamic scalar\nfield. We formulate the construction of the TVEG as an optimization problem and\ndescribe an algorithm for computing the graph. We also demonstrate the\ncapabilities of \\TVEG towards identification and exploration of topological\nevents such as deletion, generation, split, and merge within a dynamic scalar\nfield via comprehensive case studies including a viscous fingers and a 3D von\nK\\'arm\\'an vortex street dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.17652v1"
    },
    {
        "title": "Beat-It: Beat-Synchronized Multi-Condition 3D Dance Generation",
        "authors": [
            "Zikai Huang",
            "Xuemiao Xu",
            "Cheng Xu",
            "Huaidong Zhang",
            "Chenxi Zheng",
            "Jing Qin",
            "Shengfeng He"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Dance, as an art form, fundamentally hinges on the precise synchronization\nwith musical beats. However, achieving aesthetically pleasing dance sequences\nfrom music is challenging, with existing methods often falling short in\ncontrollability and beat alignment. To address these shortcomings, this paper\nintroduces Beat-It, a novel framework for beat-specific, key pose-guided dance\ngeneration. Unlike prior approaches, Beat-It uniquely integrates explicit beat\nawareness and key pose guidance, effectively resolving two main issues: the\nmisalignment of generated dance motions with musical beats, and the inability\nto map key poses to specific beats, critical for practical choreography. Our\napproach disentangles beat conditions from music using a nearest beat distance\nrepresentation and employs a hierarchical multi-condition fusion mechanism.\nThis mechanism seamlessly integrates key poses, beats, and music features,\nmitigating condition conflicts and offering rich, multi-conditioned guidance\nfor dance generation. Additionally, a specially designed beat alignment loss\nensures the generated dance movements remain in sync with the designated beats.\nExtensive experiments confirm Beat-It's superiority over existing\nstate-of-the-art methods in terms of beat alignment and motion controllability.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.07554v1"
    },
    {
        "title": "Neural Geometry Processing via Spherical Neural Surfaces",
        "authors": [
            "Romy Williamson",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Neural surfaces (e.g., neural map encoding, deep implicits and neural\nradiance fields) have recently gained popularity because of their generic\nstructure (e.g., multi-layer perceptron) and easy integration with modern\nlearning-based setups. Traditionally, we have a rich toolbox of geometry\nprocessing algorithms designed for polygonal meshes to analyze and operate on\nsurface geometry. In the absence of an analogous toolbox, neural\nrepresentations are typically discretized and converted into a mesh, before\napplying any geometry processing algorithm. This is unsatisfactory and, as we\ndemonstrate, unnecessary. In this work, we propose a spherical neural surface\nrepresentation for genus-0 surfaces and demonstrate how to compute core\ngeometric operators directly on this representation. Namely, we estimate\nsurface normals and first and second fundamental forms of the surface, as well\nas compute surface gradient, surface divergence and Laplace-Beltrami operator\non scalar/vector fields defined on the surface. Our representation is fully\nseamless, overcoming a key limitation of similar explicit representations such\nas Neural Surface Maps [Morreale et al. 2021]. These operators, in turn, enable\ngeometry processing directly on the neural representations without any\nunnecessary meshing. We demonstrate illustrative applications in (neural)\nspectral analysis, heat flow and mean curvature flow, and evaluate robustness\nto isometric shape variations. We propose theoretical formulations and validate\ntheir numerical estimates, against analytical estimates, mesh-based baselines,\nand neural alternatives, where available. By systematically linking neural\nsurface representations with classical geometry processing algorithms, we\nbelieve that this work can become a key ingredient in enabling neural geometry\nprocessing. Code will be released upon acceptance, accessible from the project\nwebpage.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.07755v2"
    },
    {
        "title": "A Unified Differentiable Boolean Operator with Fuzzy Logic",
        "authors": [
            "Hsueh-Ti Derek Liu",
            "Maneesh Agrawala",
            "Cem Yuksel",
            "Tim Omernick",
            "Vinith Misra",
            "Stefano Corazza",
            "Morgan McGuire",
            "Victor Zordan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper presents a unified differentiable boolean operator for implicit\nsolid shape modeling using Constructive Solid Geometry (CSG). Traditional CSG\nrelies on min, max operators to perform boolean operations on implicit shapes.\nBut because these boolean operators are discontinuous and discrete in the\nchoice of operations, this makes optimization over the CSG representation\nchallenging. Drawing inspiration from fuzzy logic, we present a unified boolean\noperator that outputs a continuous function and is differentiable with respect\nto operator types. This enables optimization of both the primitives and the\nboolean operations employed in CSG with continuous optimization techniques,\nsuch as gradient descent. We further demonstrate that such a continuous boolean\noperator allows modeling of both sharp mechanical objects and smooth organic\nshapes with the same framework. Our proposed boolean operator opens up new\npossibilities for future research toward fully continuous CSG optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.10954v1"
    },
    {
        "title": "ParamsDrag: Interactive Parameter Space Exploration via Image-Space\n  Dragging",
        "authors": [
            "Guan Li",
            "Yang Liu",
            "Guihua Shan",
            "Shiyu Cheng",
            "Weiqun Cao",
            "Junpeng Wang",
            "Ko-Chih Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Numerical simulation serves as a cornerstone in scientific modeling, yet the\nprocess of fine-tuning simulation parameters poses significant challenges.\nConventionally, parameter adjustment relies on extensive numerical simulations,\ndata analysis, and expert insights, resulting in substantial computational\ncosts and low efficiency. The emergence of deep learning in recent years has\nprovided promising avenues for more efficient exploration of parameter spaces.\nHowever, existing approaches often lack intuitive methods for precise parameter\nadjustment and optimization. To tackle these challenges, we introduce\nParamsDrag, a model that facilitates parameter space exploration through direct\ninteraction with visualizations. Inspired by DragGAN, our ParamsDrag model\noperates in three steps. First, the generative component of ParamsDrag\ngenerates visualizations based on the input simulation parameters. Second, by\ndirectly dragging structure-related features in the visualizations, users can\nintuitively understand the controlling effect of different parameters. Third,\nwith the understanding from the earlier step, users can steer ParamsDrag to\nproduce dynamic visual outcomes. Through experiments conducted on real-world\nsimulations and comparisons with state-of-the-art deep learning-based\napproaches, we demonstrate the efficacy of our solution.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.14100v1"
    },
    {
        "title": "Uncertainty-Aware Deep Neural Representations for Visual Analysis of\n  Vector Field Data",
        "authors": [
            "Atul Kumar",
            "Siddharth Garg",
            "Soumya Dutta"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The widespread use of Deep Neural Networks (DNNs) has recently resulted in\ntheir application to challenging scientific visualization tasks. While advanced\nDNNs demonstrate impressive generalization abilities, understanding factors\nlike prediction quality, confidence, robustness, and uncertainty is crucial.\nThese insights aid application scientists in making informed decisions.\nHowever, DNNs lack inherent mechanisms to measure prediction uncertainty,\nprompting the creation of distinct frameworks for constructing robust\nuncertainty-aware models tailored to various visualization tasks. In this work,\nwe develop uncertainty-aware implicit neural representations to model\nsteady-state vector fields effectively. We comprehensively evaluate the\nefficacy of two principled deep uncertainty estimation techniques: (1) Deep\nEnsemble and (2) Monte Carlo Dropout, aimed at enabling uncertainty-informed\nvisual analysis of features within steady vector field data. Our detailed\nexploration using several vector data sets indicate that uncertainty-aware\nmodels generate informative visualization results of vector field features.\nFurthermore, incorporating prediction uncertainty improves the resilience and\ninterpretability of our DNN model, rendering it applicable for the analysis of\nnon-trivial vector field data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.16119v2"
    },
    {
        "title": "Strategy and Skill Learning for Physics-based Table Tennis Animation",
        "authors": [
            "Jiashun Wang",
            "Jessica Hodgins",
            "Jungdam Won"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Recent advancements in physics-based character animation leverage deep\nlearning to generate agile and natural motion, enabling characters to execute\nmovements such as backflips, boxing, and tennis. However, reproducing the\nselection and use of diverse motor skills in dynamic environments to solve\ncomplex tasks, as humans do, still remains a challenge. We present a strategy\nand skill learning approach for physics-based table tennis animation. Our\nmethod addresses the issue of mode collapse, where the characters do not fully\nutilize the motor skills they need to perform to execute complex tasks. More\nspecifically, we demonstrate a hierarchical control system for diversified\nskill learning and a strategy learning framework for effective decision-making.\nWe showcase the efficacy of our method through comparative analysis with\nstate-of-the-art methods, demonstrating its capabilities in executing various\nskills for table tennis. Our strategy learning framework is validated through\nboth agent-agent interaction and human-agent interaction in Virtual Reality,\nhandling both competitive and cooperative tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.16210v1"
    },
    {
        "title": "Improving multidimensional projection quality with user-specific metrics\n  and optimal scaling",
        "authors": [
            "Maniru Ibrahim"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The growing prevalence of high-dimensional data has fostered the development\nof multidimensional projection (MP) techniques, such as t-SNE, UMAP, and LAMP,\nfor data visualization and exploration. However, conventional MP methods\ntypically employ generic quality metrics, neglecting individual user\npreferences. This study proposes a new framework that tailors MP techniques\nbased on user-specific quality criteria, enhancing projection interpretability.\n  Our approach combines three visual quality metrics, stress, neighborhood\npreservation, and silhouette score, to create a composite metric for a precise\nMP evaluation. We then optimize the projection scale by maximizing the\ncomposite metric value. We conducted an experiment involving two users with\ndifferent projection preferences, generating projections using t-SNE, UMAP, and\nLAMP. Users rate projections according to their criteria, producing two\ntraining sets. We derive optimal weights for each set and apply them to other\ndatasets to determine the best projections per user.\n  Our findings demonstrate that personalized projections effectively capture\nuser preferences, fostering better data exploration and enabling more informed\ndecision-making. This user-centric approach promotes advancements in\nmultidimensional projection techniques that accommodate diverse user\npreferences and enhance interpretability.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.16328v1"
    },
    {
        "title": "NARVis: Neural Accelerated Rendering for Real-Time Scientific Point\n  Cloud Visualization",
        "authors": [
            "Srinidhi Hegde",
            "Kaur Kullman",
            "Thomas Grubb",
            "Leslie Lait",
            "Stephen Guimond",
            "Matthias Zwicker"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Exploring scientific datasets with billions of samples in real-time\nvisualization presents a challenge - balancing high-fidelity rendering with\nspeed. This work introduces a novel renderer - Neural Accelerated Renderer\n(NAR), that uses the neural deferred rendering framework to visualize\nlarge-scale scientific point cloud data. NAR augments a real-time point cloud\nrendering pipeline with high-quality neural post-processing, making the\napproach ideal for interactive visualization at scale. Specifically, we train a\nneural network to learn the point cloud geometry from a high-performance\nmulti-stream rasterizer and capture the desired postprocessing effects from a\nconventional high-quality renderer. We demonstrate the effectiveness of NAR by\nvisualizing complex multidimensional Lagrangian flow fields and photometric\nscans of a large terrain and compare the renderings against the\nstate-of-the-art high-quality renderers. Through extensive evaluation, we\ndemonstrate that NAR prioritizes speed and scalability while retaining high\nvisual fidelity. We achieve competitive frame rates of $>$ 126 fps for\ninteractive rendering of $>$ 350M points (i.e., an effective throughput of $>$\n44 billion points per second) using $\\sim$12 GB of memory on RTX 2080 Ti GPU.\nFurthermore, we show that NAR is generalizable across different point clouds\nwith similar visualization needs and the desired post-processing effects could\nbe obtained with substantial high quality even at lower resolutions of the\noriginal point cloud, further reducing the memory requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19097v1"
    },
    {
        "title": "StyleRF-VolVis: Style Transfer of Neural Radiance Fields for Expressive\n  Volume Visualization",
        "authors": [
            "Kaiyuan Tang",
            "Chaoli Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In volume visualization, visualization synthesis has attracted much attention\ndue to its ability to generate novel visualizations without following the\nconventional rendering pipeline. However, existing solutions based on\ngenerative adversarial networks often require many training images and take\nsignificant training time. Still, issues such as low quality, consistency, and\nflexibility persist. This paper introduces StyleRF-VolVis, an innovative style\ntransfer framework for expressive volume visualization (VolVis) via neural\nradiance field (NeRF). The expressiveness of StyleRF-VolVis is upheld by its\nability to accurately separate the underlying scene geometry (i.e., content)\nand color appearance (i.e., style), conveniently modify color, opacity, and\nlighting of the original rendering while maintaining visual content consistency\nacross the views, and effectively transfer arbitrary styles from reference\nimages to the reconstructed 3D scene. To achieve these, we design a base NeRF\nmodel for scene geometry extraction, a palette color network to classify\nregions of the radiance field for photorealistic editing, and an unrestricted\ncolor network to lift the color palette constraint via knowledge distillation\nfor non-photorealistic editing. We demonstrate the superior quality,\nconsistency, and flexibility of StyleRF-VolVis by experimenting with various\nvolume rendering scenes and reference images and comparing StyleRF-VolVis\nagainst other image-based (AdaIN), video-based (ReReVST), and NeRF-based (ARF\nand SNeRF) style rendering solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.00150v1"
    },
    {
        "title": "DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer\n  Normalization Mamba-2 framework",
        "authors": [
            "Fan Zhang",
            "Naye Ji",
            "Fuxing Gao",
            "Bozuo Zhao",
            "Jingmei Wu",
            "Yanbing Jiang",
            "Hui Du",
            "Zhenqing Ye",
            "Jiayang Zhu",
            "WeiFan Zhong",
            "Leyao Yan",
            "Xiaomeng Ma"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Speech-driven gesture generation is an emerging domain within virtual human\ncreation, where current methods predominantly utilize Transformer-based\narchitectures that necessitate extensive memory and are characterized by slow\ninference speeds. In response to these limitations, we propose\n\\textit{DiM-Gestures}, a novel end-to-end generative model crafted to create\nhighly personalized 3D full-body gestures solely from raw speech audio,\nemploying Mamba-based architectures. This model integrates a Mamba-based fuzzy\nfeature extractor with a non-autoregressive Adaptive Layer Normalization\n(AdaLN) Mamba-2 diffusion architecture. The extractor, leveraging a Mamba\nframework and a WavLM pre-trained model, autonomously derives implicit,\ncontinuous fuzzy features, which are then unified into a singular latent\nfeature. This feature is processed by the AdaLN Mamba-2, which implements a\nuniform conditional mechanism across all tokens to robustly model the interplay\nbetween the fuzzy features and the resultant gesture sequence. This innovative\napproach guarantees high fidelity in gesture-speech synchronization while\nmaintaining the naturalness of the gestures. Employing a diffusion model for\ntraining and inference, our framework has undergone extensive subjective and\nobjective evaluations on the ZEGGS and BEAT datasets. These assessments\nsubstantiate our model's enhanced performance relative to contemporary\nstate-of-the-art methods, demonstrating competitive outcomes with the DiTs\narchitecture (Persona-Gestors) while optimizing memory usage and accelerating\ninference speed.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.00370v1"
    },
    {
        "title": "Sampling for View Synthesis: From Local Light Field Fusion to Neural\n  Radiance Fields and Beyond",
        "authors": [
            "Ravi Ramamoorthi"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Capturing and rendering novel views of complex real-world scenes is a\nlong-standing problem in computer graphics and vision, with applications in\naugmented and virtual reality, immersive experiences and 3D photography. The\nadvent of deep learning has enabled revolutionary advances in this area,\nclassically known as image-based rendering. However, previous approaches\nrequire intractably dense view sampling or provide little or no guidance for\nhow users should sample views of a scene to reliably render high-quality novel\nviews. Local light field fusion proposes an algorithm for practical view\nsynthesis from an irregular grid of sampled views that first expands each\nsampled view into a local light field via a multiplane image scene\nrepresentation, then renders novel views by blending adjacent local light\nfields. Crucially, we extend traditional plenoptic sampling theory to derive a\nbound that specifies precisely how densely users should sample views of a given\nscene when using our algorithm. We achieve the perceptual quality of Nyquist\nrate view sampling while using up to 4000x fewer views. Subsequent developments\nhave led to new scene representations for deep learning with view synthesis,\nnotably neural radiance fields, but the problem of sparse view synthesis from a\nsmall number of images has only grown in importance. We reprise some of the\nrecent results on sparse and even single image view synthesis, while posing the\nquestion of whether prescriptive sampling guidelines are feasible for the new\ngeneration of image-based rendering algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04586v1"
    },
    {
        "title": "Uncertainty-Informed Volume Visualization using Implicit Neural\n  Representation",
        "authors": [
            "Shanu Saklani",
            "Chitwan Goel",
            "Shrey Bansal",
            "Zhe Wang",
            "Soumya Dutta",
            "Tushar M. Athawale",
            "David Pugmire",
            "Christopher R. Johnson"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The increasing adoption of Deep Neural Networks (DNNs) has led to their\napplication in many challenging scientific visualization tasks. While advanced\nDNNs offer impressive generalization capabilities, understanding factors such\nas model prediction quality, robustness, and uncertainty is crucial. These\ninsights can enable domain scientists to make informed decisions about their\ndata. However, DNNs inherently lack ability to estimate prediction uncertainty,\nnecessitating new research to construct robust uncertainty-aware visualization\ntechniques tailored for various visualization tasks. In this work, we propose\nuncertainty-aware implicit neural representations to model scalar field data\nsets effectively and comprehensively study the efficacy and benefits of\nestimated uncertainty information for volume visualization tasks. We evaluate\nthe effectiveness of two principled deep uncertainty estimation techniques: (1)\nDeep Ensemble and (2) Monte Carlo Dropout (MCDropout). These techniques enable\nuncertainty-informed volume visualization in scalar field data sets. Our\nextensive exploration across multiple data sets demonstrates that\nuncertainty-aware models produce informative volume visualization results.\nMoreover, integrating prediction uncertainty enhances the trustworthiness of\nour DNN model, making it suitable for robustly analyzing and visualizing\nreal-world scientific volumetric data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.06018v1"
    },
    {
        "title": "Meta-Learning Empowered Meta-Face: Personalized Speaking Style\n  Adaptation for Audio-Driven 3D Talking Face Animation",
        "authors": [
            "Xukun Zhou",
            "Fengxin Li",
            "Ziqiao Peng",
            "Kejian Wu",
            "Jun He",
            "Biao Qin",
            "Zhaoxin Fan",
            "Hongyan Liu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Audio-driven 3D face animation is increasingly vital in live streaming and\naugmented reality applications. While remarkable progress has been observed,\nmost existing approaches are designed for specific individuals with predefined\nspeaking styles, thus neglecting the adaptability to varied speaking styles. To\naddress this limitation, this paper introduces MetaFace, a novel methodology\nmeticulously crafted for speaking style adaptation. Grounded in the novel\nconcept of meta-learning, MetaFace is composed of several key components: the\nRobust Meta Initialization Stage (RMIS) for fundamental speaking style\nadaptation, the Dynamic Relation Mining Neural Process (DRMN) for forging\nconnections between observed and unobserved speaking styles, and the Low-rank\nMatrix Memory Reduction Approach to enhance the efficiency of model\noptimization as well as learning style details. Leveraging these novel designs,\nMetaFace not only significantly outperforms robust existing baselines but also\nestablishes a new state-of-the-art, as substantiated by our experimental\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09357v1"
    },
    {
        "title": "Data-Driven Nonlinear Deformation Design of 3D-Printable Shells",
        "authors": [
            "Samuel Silverman",
            "Kelsey L. Snapp",
            "Keith A. Brown",
            "Emily Whiting"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Designing and fabricating structures with specific mechanical properties\nrequires understanding the intricate relationship between design parameters and\nperformance. Understanding the design-performance relationship becomes\nincreasingly complicated for nonlinear deformations. Though successful at\nmodeling elastic deformations, simulation-based techniques struggle to model\nlarge elastoplastic deformations exhibiting plasticity and densification. We\npropose a neural network trained on experimental data to learn the\ndesign-performance relationship between 3D-printable shells and their\ncompressive force-displacement behavior. Trained on thousands of physical\nexperiments, our network aids in both forward and inverse design to generate\nshells exhibiting desired elastoplastic and hyperelastic deformations. We\nvalidate a subset of generated designs through fabrication and testing.\nFurthermore, we demonstrate the network's inverse design efficacy in generating\ncustom shells for several applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.15097v1"
    },
    {
        "title": "G-Style: Stylized Gaussian Splatting",
        "authors": [
            "Áron Samuel Kovács",
            "Pedro Hermosilla",
            "Renata G. Raidou"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce G-Style, a novel algorithm designed to transfer the style of an\nimage onto a 3D scene represented using Gaussian Splatting. Gaussian Splatting\nis a powerful 3D representation for novel view synthesis, as -- compared to\nother approaches based on Neural Radiance Fields -- it provides fast scene\nrenderings and user control over the scene. Recent pre-prints have demonstrated\nthat the style of Gaussian Splatting scenes can be modified using an image\nexemplar. However, since the scene geometry remains fixed during the\nstylization process, current solutions fall short of producing satisfactory\nresults. Our algorithm aims to address these limitations by following a\nthree-step process: In a pre-processing step, we remove undesirable Gaussians\nwith large projection areas or highly elongated shapes. Subsequently, we\ncombine several losses carefully designed to preserve different scales of the\nstyle in the image, while maintaining as much as possible the integrity of the\noriginal scene content. During the stylization process and following the\noriginal design of Gaussian Splatting, we split Gaussians where additional\ndetail is necessary within our scene by tracking the gradient of the stylized\ncolor. Our experiments demonstrate that G-Style generates high-quality\nstylizations within just a few minutes, outperforming existing methods both\nqualitatively and quantitatively.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.15695v2"
    },
    {
        "title": "Deep Neural Implicit Representation of Accessibility for Multi-Axis\n  Manufacturing",
        "authors": [
            "George P. Harabin",
            "Amir Mirzendehdel",
            "Morad Behandish"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  One of the main concerns in design and process planning for multi-axis\nadditive and subtractive manufacturing is collision avoidance between moving\nobjects (e.g., tool assemblies) and stationary objects (e.g., a part unified\nwith fixtures). The collision measure for various pairs of relative rigid\ntranslations and rotations between the two pointsets can be conceptualized by a\ncompactly supported scalar field over the 6D non-Euclidean configuration space.\nExplicit representation and computation of this field is costly in both time\nand space. If we fix $O(m)$ sparsely sampled rotations (e.g., tool\norientations), computation of the collision measure field as a convolution of\nindicator functions of the 3D pointsets over a uniform grid (i.e., voxelized\ngeometry) of resolution $O(n^3)$ via fast Fourier transforms (FFTs) scales as\nin $O(mn^3 \\log n)$ in time and $O(mn^3)$ in space. In this paper, we develop\nan implicit representation of the collision measure field via deep neural\nnetworks (DNNs). We show that our approach is able to accurately interpolate\nthe collision measure from a sparse sampling of rotations, and can represent\nthe collision measure field with a small memory footprint. Moreover, we show\nthat this representation can be efficiently updated through fine-tuning to more\nefficiently train the network on multi-resolution data, as well as accommodate\nincremental changes to the geometry (such as might occur in iterative processes\nsuch as topology optimization of the part subject to CNC tool accessibility\nconstraints).\n",
        "pdf_link": "http://arxiv.org/pdf/2409.02115v2"
    },
    {
        "title": "Large Étendue 3D Holographic Display with Content-adaptive Dynamic\n  Fourier Modulation",
        "authors": [
            "Brian Chao",
            "Manu Gopakumar",
            "Suyeon Choi",
            "Jonghyun Kim",
            "Liang Shi",
            "Gordon Wetzstein"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Emerging holographic display technology offers unique capabilities for\nnext-generation virtual reality systems. Current holographic near-eye displays,\nhowever, only support a small \\'etendue, which results in a direct tradeoff\nbetween achievable field of view and eyebox size. \\'Etendue expansion has\nrecently been explored, but existing approaches are either fundamentally\nlimited in the image quality that can be achieved or they require extremely\nhigh-speed spatial light modulators.\n  We describe a new \\'etendue expansion approach that combines multiple\ncoherent sources with content-adaptive amplitude modulation of the hologram\nspectrum in the Fourier plane. To generate time-multiplexed phase and amplitude\npatterns for our spatial light modulators, we devise a pupil-aware\ngradient-descent-based computer-generated holography algorithm that is\nsupervised by a large-baseline target light field. Compared with relevant\nbaseline approaches, our method demonstrates significant improvements in image\nquality and \\'etendue in simulation and with an experimental holographic\ndisplay prototype.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.03143v2"
    },
    {
        "title": "Exploring Fungal Morphology Simulation and Dynamic Light Containment\n  from a Graphics Generation Perspective",
        "authors": [
            "Kexin Wang",
            "Ivy He",
            "Jinke Li",
            "Ali Asadipour",
            "Yitong Sun"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Fungal simulation and control are considered crucial techniques in Bio-Art\ncreation. However, coding algorithms for reliable fungal simulations have posed\nsignificant challenges for artists. This study equates fungal morphology\nsimulation to a two-dimensional graphic time-series generation problem. We\npropose a zero-coding, neural network-driven cellular automaton. Fungal spread\npatterns are learned through an image segmentation model and a time-series\nprediction model, which then supervise the training of neural network cells,\nenabling them to replicate real-world spreading behaviors. We further\nimplemented dynamic containment of fungal boundaries with lasers. Synchronized\nwith the automaton, the fungus successfully spreads into pre-designed complex\nshapes in reality.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.05171v1"
    },
    {
        "title": "An Eulerian Vortex Method on Flow Maps",
        "authors": [
            "Sinan Wang",
            "Yitong Deng",
            "Molin Deng",
            "Hong-Xing Yu",
            "Junwei Zhou",
            "Duowen Chen",
            "Taku Komura",
            "Jiajun Wu",
            "Bo Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present an Eulerian vortex method based on the theory of flow maps to\nsimulate the complex vortical motions of incompressible fluids. Central to our\nmethod is the novel incorporation of the flow-map transport equations for line\nelements, which, in combination with a bi-directional marching scheme for flow\nmaps, enables the high-fidelity Eulerian advection of vorticity variables. The\nfundamental motivation is that, compared to impulse $\\mathbf{m}$, which has\nbeen recently bridged with flow maps to encouraging results, vorticity\n$\\boldsymbol{\\omega}$ promises to be preferable for its numerical stability and\nphysical interpretability. To realize the full potential of this novel\nformulation, we develop a new Poisson solving scheme for vorticity-to-velocity\nreconstruction that is both efficient and able to accurately handle the\ncoupling near solid boundaries. We demonstrate the efficacy of our approach\nwith a range of vortex simulation examples, including leapfrog vortices, vortex\ncollisions, cavity flow, and the formation of complex vortical structures due\nto solid-fluid interactions.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.06201v2"
    },
    {
        "title": "Multi-scale Cycle Tracking in Dynamic Planar Graphs",
        "authors": [
            "Farhan Rasheed",
            "Abrar Naseer",
            "Emma Nilsson",
            "Talha Bin Masood",
            "Ingrid Hotz"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper presents a nested tracking framework for analyzing cycles in 2D\nforce networks within granular materials. These materials are composed of\ninteracting particles, whose interactions are described by a force network.\nUnderstanding the cycles within these networks at various scales and their\nevolution under external loads is crucial, as they significantly contribute to\nthe mechanical and kinematic properties of the system. Our approach involves\ncomputing a cycle hierarchy by partitioning the 2D domain into segments bounded\nby cycles in the force network. We can adapt concepts from nested tracking\ngraphs originally developed for merge trees by leveraging the duality between\nthis partitioning and the cycles. We demonstrate the effectiveness of our\nmethod on two force networks derived from experiments with photoelastic disks.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.06476v1"
    },
    {
        "title": "WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users",
        "authors": [
            "Yunzhi Li",
            "Vimal Mollyn",
            "Kuang Yuan",
            "Patrick Carrington"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Despite researchers having extensively studied various ways to track body\npose on-the-go, most prior work does not take into account wheelchair users,\nleading to poor tracking performance. Wheelchair users could greatly benefit\nfrom this pose information to prevent injuries, monitor their health, identify\nenvironmental accessibility barriers, and interact with gaming and VR\nexperiences. In this work, we present WheelPoser, a real-time pose estimation\nsystem specifically designed for wheelchair users. Our system uses only four\nstrategically placed IMUs on the user's body and wheelchair, making it far more\npractical than prior systems using cameras and dense IMU arrays. WheelPoser is\nable to track a wheelchair user's pose with a mean joint angle error of 14.30\ndegrees and a mean joint position error of 6.74 cm, more than three times\nbetter than similar systems using sparse IMUs. To train our system, we collect\na novel WheelPoser-IMU dataset, consisting of 167 minutes of paired IMU sensor\nand motion capture data of people in wheelchairs, including wheelchair-specific\nmotions such as propulsion and pressure relief. Finally, we explore the\npotential application space enabled by our system and discuss future\nopportunities. Open-source code, models, and dataset can be found here:\nhttps://github.com/axle-lab/WheelPoser.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08494v1"
    },
    {
        "title": "Rapid Prototyping of 3D Microstructures: A Simplified Grayscale\n  Lithography Encoding Method Using Blender",
        "authors": [
            "Fabricio Frizera Borghi",
            "Mohammed Bendimerad",
            "Marie-Ly Chapon",
            "Tatiana Petithory",
            "Laurent Vonna",
            "Laurent Pieuchot"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The democratization of fabrication equipment has spurred recent interest in\nmaskless grayscale lithography for both 2D and 3D microfabrication. However,\nthe design of suitable template images remains a challenge. This work presents\na simplified method for encoding 3D objects into grayscale image files\noptimized for grayscale lithography. Leveraging the widely used, open-source 3D\nmodeling software Blender, we developed a robust approach to convert geometric\nheights into grayscale levels and generate image files through top-view\nrendering. Our method accurately reproduced the overall shape of simple\nstructures like stairs and ramps compared to the original designs. We extended\nthis approach to complex 3D sinusoidal surfaces, achieving similar results.\nGiven the increasing accessibility and user-friendliness of digital rendering\ntools, this study offers a promising strategy for rapid prototyping of initial\ndesigns with minimal effort.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.16749v1"
    },
    {
        "title": "Galerkin Method of Regularized Stokeslets for Procedural Fluid Flow with\n  Control Curves",
        "authors": [
            "Ryusuke Sugimoto",
            "Jeff Lait",
            "Christopher Batty",
            "Toshiya Hachisuka"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a new procedural incompressible velocity field authoring tool,\nwhich lets users design a volumetric flow by directly specifying velocity along\ncontrol curves. Our method combines the Method of Regularized Stokeslets with\nGalerkin discretization. Based on the highly viscous Stokes flow assumption, we\nfind the force along a given set of curves that satisfies the velocity\nconstraints along them. We can then evaluate the velocity anywhere inside the\nsurrounding infinite 2D or 3D domain. We also show the extension of our method\nto control the angular velocity along control curves. Compared to a collocation\ndiscretization, our method is not very sensitive to the vertex sampling rate\nalong control curves and only requires a small linear system solve.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.18276v1"
    },
    {
        "title": "FürElise: Capturing and Physically Synthesizing Hand Motions of Piano\n  Performance",
        "authors": [
            "Ruocheng Wang",
            "Pei Xu",
            "Haochen Shi",
            "Elizabeth Schumann",
            "C. Karen Liu"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Piano playing requires agile, precise, and coordinated hand control that\nstretches the limits of dexterity. Hand motion models with the sophistication\nto accurately recreate piano playing have a wide range of applications in\ncharacter animation, embodied AI, biomechanics, and VR/AR. In this paper, we\nconstruct a first-of-its-kind large-scale dataset that contains approximately\n10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153\npieces of classical music. To capture natural performances, we designed a\nmarkerless setup in which motions are reconstructed from multi-view videos\nusing state-of-the-art pose estimation models. The motion data is further\nrefined via inverse kinematics using the high-resolution MIDI key-pressing data\nobtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the\ncollected dataset, we developed a pipeline that can synthesize\nphysically-plausible hand motions for musical scores outside of the dataset.\nOur approach employs a combination of imitation learning and reinforcement\nlearning to obtain policies for physics-based bimanual control involving the\ninteraction between hands and piano keys. To solve the sampling efficiency\nproblem with the large motion dataset, we use a diffusion model to generate\nnatural reference motions, which provide high-level trajectory and fingering\n(finger order and placement) information. However, the generated reference\nmotion alone does not provide sufficient accuracy for piano performance\nmodeling. We then further augmented the data by using musical similarity to\nretrieve similar motions from the captured dataset to boost the precision of\nthe RL policy. With the proposed method, our model generates natural, dexterous\nmotions that generalize to music from outside the training dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.05791v1"
    },
    {
        "title": "ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling",
        "authors": [
            "Deok-Kyeong Jang",
            "Dongseok Yang",
            "Deok-Yun Jang",
            "Byeoli Choi",
            "Donghoon Shin",
            "Sung-hee Lee"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  This paper introduces ELMO, a real-time upsampling motion capture framework\ndesigned for a single LiDAR sensor. Modeled as a conditional autoregressive\ntransformer-based upsampling motion generator, ELMO achieves 60 fps motion\ncapture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is\nthe coupling of the self-attention mechanism with thoughtfully designed\nembedding modules for motion and point clouds, significantly elevating the\nmotion quality. To facilitate accurate motion capture, we develop a one-time\nskeleton calibration model capable of predicting user skeleton offsets from a\nsingle-frame point cloud. Additionally, we introduce a novel data augmentation\ntechnique utilizing a LiDAR simulator, which enhances global root tracking to\nimprove environmental understanding. To demonstrate the effectiveness of our\nmethod, we compare ELMO with state-of-the-art methods in both image-based and\npoint cloud-based motion capture. We further conduct an ablation study to\nvalidate our design principles. ELMO's fast inference time makes it well-suited\nfor real-time applications, exemplified in our demo video featuring live\nstreaming and interactive gaming scenarios. Furthermore, we contribute a\nhigh-quality LiDAR-mocap synchronized dataset comprising 20 different subjects\nperforming a range of motions, which can serve as a valuable resource for\nfuture research. The dataset and evaluation code are available at {\\blue\n\\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}\n",
        "pdf_link": "http://arxiv.org/pdf/2410.06963v2"
    },
    {
        "title": "Trust-Region Eigenvalue Filtering for Projected Newton",
        "authors": [
            "Honglin Chen",
            "Hsueh-Ti Derek Liu",
            "Alec Jacobson",
            "David I. W. Levin",
            "Changxi Zheng"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce a novel adaptive eigenvalue filtering strategy to stabilize and\naccelerate the optimization of Neo-Hookean energy and its variants under the\nProjected Newton framework. For the first time, we show that Newton's method,\nProjected Newton with eigenvalue clamping and Projected Newton with absolute\neigenvalue filtering can be unified using ideas from the generalized trust\nregion method. Based on the trust-region fit, our model adaptively chooses the\ncorrect eigenvalue filtering strategy to apply during the optimization. Our\nmethod is simple but effective, requiring only two lines of code change in the\nexisting Projected Newton framework. We validate our model outperforms\nstand-alone variants across a number of experiments on quasistatic simulation\nof deformable solids over a large dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.10102v1"
    },
    {
        "title": "SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian\n  Surfel Head Avatars",
        "authors": [
            "Jaeseong Lee",
            "Taewoong Kang",
            "Marcel C. Bühler",
            "Min-Jung Kim",
            "Sungwon Hwang",
            "Junha Hyung",
            "Hyojin Jang",
            "Jaegul Choo"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Recent advancements in head avatar rendering using Gaussian primitives have\nachieved significantly high-fidelity results. Although precise head geometry is\ncrucial for applications like mesh reconstruction and relighting, current\nmethods struggle to capture intricate geometric details and render unseen poses\ndue to their reliance on similarity transformations, which cannot handle\nstretch and shear transforms essential for detailed deformations of geometry.\nTo address this, we propose SurFhead, a novel method that reconstructs riggable\nhead geometry from RGB videos using 2D Gaussian surfels, which offer\nwell-defined geometric properties, such as precise depth from fixed ray\nintersections and normals derived from their surface orientation, making them\nadvantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of\nboth normals and images, even in extreme poses, by leveraging classical\nmesh-based deformation transfer and affine transformation interpolation.\nSurFhead introduces precise geometric deformation and blends surfels through\npolar decomposition of transformations, including those affecting normals. Our\nkey contribution lies in bridging classical graphics techniques, such as\nmesh-based deformation, with modern Gaussian primitives, achieving\nstate-of-the-art geometry reconstruction and rendering quality. Unlike previous\navatar rendering approaches, SurFhead enables efficient reconstruction driven\nby Gaussian primitives while preserving high-fidelity geometry.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.11682v1"
    },
    {
        "title": "EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room\n  Layout Editing",
        "authors": [
            "Kaizhi Zheng",
            "Xiaotong Chen",
            "Xuehai He",
            "Jing Gu",
            "Linjie Li",
            "Zhengyuan Yang",
            "Kevin Lin",
            "Jianfeng Wang",
            "Lijuan Wang",
            "Xin Eric Wang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Given the steep learning curve of professional 3D software and the\ntime-consuming process of managing large 3D assets, language-guided 3D scene\nediting has significant potential in fields such as virtual reality, augmented\nreality, and gaming. However, recent approaches to language-guided 3D scene\nediting either require manual interventions or focus only on appearance\nmodifications without supporting comprehensive scene layout changes. In\nresponse, we propose Edit-Room, a unified framework capable of executing a\nvariety of layout edits through natural language commands, without requiring\nmanual intervention. Specifically, EditRoom leverages Large Language Models\n(LLMs) for command planning and generates target scenes using a diffusion-based\nmethod, enabling six types of edits: rotate, translate, scale, replace, add,\nand remove. To address the lack of data for language-guided 3D scene editing,\nwe have developed an automatic pipeline to augment existing 3D scene synthesis\ndatasets and introduced EditRoom-DB, a large-scale dataset with 83k editing\npairs, for training and evaluation. Our experiments demonstrate that our\napproach consistently outperforms other baselines across all metrics,\nindicating higher accuracy and coherence in language-guided scene layout\nediting.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12836v1"
    },
    {
        "title": "Toroidal density-equalizing map for genus-one surfaces",
        "authors": [
            "Shunyu Yao",
            "Gary P. T. Choi"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Density-equalizing map is a shape deformation technique originally developed\nfor cartogram creation and sociological data visualization on planar\ngeographical maps. In recent years, there has been an increasing interest in\ndeveloping density-equalizing mapping methods for surface and volumetric\ndomains and applying them to various problems in geometry processing and\nimaging science. However, the existing surface density-equalizing mapping\nmethods are only applicable to surfaces with relatively simple topologies but\nnot surfaces with topological holes. In this work, we develop a novel algorithm\nfor computing density-equalizing maps for toroidal surfaces. In particular,\ndifferent shape deformation effects can be easily achieved by prescribing\ndifferent population functions on the torus and performing diffusion-based\ndeformations on a planar domain with periodic boundary conditions. Furthermore,\nthe proposed toroidal density-equalizing mapping method naturally leads to an\neffective method for computing toroidal parameterizations of genus-one surfaces\nwith controllable shape changes, with the toroidal area-preserving\nparameterization being a prime example. Experimental results are presented to\ndemonstrate the effectiveness of our proposed methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16833v2"
    },
    {
        "title": "The evolution of volumetric video: A survey of smart transcoding and\n  compression approaches",
        "authors": [
            "Preetish Kakkar",
            "Hariharan Ragothaman"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Volumetric video, the capture and display of three-dimensional (3D) imagery,\nhas emerged as a revolutionary technology poised to transform the media\nlandscape, enabling immersive experiences that transcend the limitations of\ntraditional 2D video. One of the key challenges in this domain is the efficient\ndelivery of these high-bandwidth, data-intensive volumetric video streams,\nwhich requires innovative transcoding and compression techniques. This research\npaper explores the state-of-the-art in volumetric video compression and\ndelivery, with a focus on the potential of AI-driven solutions to address the\nunique challenges posed by this emerging medium.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.02095v2"
    },
    {
        "title": "Physically Based Neural Bidirectional Reflectance Distribution Function",
        "authors": [
            "Chenliang Zhou",
            "Alejandro Sztrajman",
            "Gilles Rainer",
            "Fangcheng Zhong",
            "Fazilet Gokbudak",
            "Zhilin Guo",
            "Weihao Xia",
            "Rafal Mantiuk",
            "Cengiz Oztireli"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce the physically based neural bidirectional reflectance\ndistribution function (PBNBRDF), a novel, continuous representation for\nmaterial appearance based on neural fields. Our model accurately reconstructs\nreal-world materials while uniquely enforcing physical properties for realistic\nBRDFs, specifically Helmholtz reciprocity via reparametrization and energy\npassivity via efficient analytical integration. We conduct a systematic\nanalysis demonstrating the benefits of adhering to these physical laws on the\nvisual quality of reconstructed materials. Additionally, we enhance the color\naccuracy of neural BRDFs by introducing chromaticity enforcement supervising\nthe norms of RGB channels. Through both qualitative and quantitative\nexperiments on multiple databases of measured real-world BRDFs, we show that\nadhering to these physical constraints enables neural fields to more faithfully\nand stably represent the original data and achieve higher rendering quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.02347v1"
    },
    {
        "title": "gDist: Efficient Distance Computation between 3D Meshes on GPU",
        "authors": [
            "Peng Fang",
            "Wei Wang",
            "Ruofeng Tong",
            "Hailong Li",
            "Min Tang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Computing maximum/minimum distances between 3D meshes is crucial for various\napplications, i.e., robotics, CAD, VR/AR, etc. In this work, we introduce a\nhighly parallel algorithm (gDist) optimized for Graphics Processing Units\n(GPUs), which is capable of computing the distance between two meshes with over\n15 million triangles in less than 0.4 milliseconds (Fig. 1). By testing on\nbenchmarks with varying characteristics, the algorithm achieves remarkable\nspeedups over prior CPU-based and GPU-based algorithms on a commodity GPU\n(NVIDIA GeForce RTX 4090). Notably, the algorithm consistently maintains\nhigh-speed performance, even in challenging scenarios that pose difficulties\nfor prior algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.11244v1"
    },
    {
        "title": "Polyhedral Discretizations for Elliptic PDEs",
        "authors": [
            "Junyu Liu",
            "Daniele Panozzo",
            "Mario Botsch",
            "Teseo Schneider"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We study the use of polyhedral discretizations for the solution of heat\ndiffusion and elastodynamic problems in computer graphics. Polyhedral meshes\nare more natural for certain applications than pure triangular or quadrilateral\nmeshes, which thus received significant interest as an alternative\nrepresentation. We consider finite element methods using barycentric\ncoordinates as basis functions and the modern virtual finite element approach.\nWe evaluate them on a suite of classical graphics problems to understand their\nbenefits and limitations compared to standard techniques on simplicial\ndiscretizations. Our analysis provides recommendations and a benchmark for\ndeveloping polyhedral meshing techniques and corresponding analysis techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.06164v1"
    },
    {
        "title": "SyncViolinist: Music-Oriented Violin Motion Generation Based on Bowing\n  and Fingering",
        "authors": [
            "Hiroki Nishizawa",
            "Keitaro Tanaka",
            "Asuka Hirata",
            "Shugo Yamaguchi",
            "Qi Feng",
            "Masatoshi Hamanaka",
            "Shigeo Morishima"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Automatically generating realistic musical performance motion can greatly\nenhance digital media production, often involving collaboration between\nprofessionals and musicians. However, capturing the intricate body, hand, and\nfinger movements required for accurate musical performances is challenging.\nExisting methods often fall short due to the complex mapping between audio and\nmotion, typically requiring additional inputs like scores or MIDI data. In this\nwork, we present SyncViolinist, a multi-stage end-to-end framework that\ngenerates synchronized violin performance motion solely from audio input. Our\nmethod overcomes the challenge of capturing both global and fine-grained\nperformance features through two key modules: a bowing/fingering module and a\nmotion generation module. The bowing/fingering module extracts detailed playing\ninformation from the audio, which the motion generation module uses to create\nprecise, coordinated body motions reflecting the temporal granularity and\nnature of the violin performance. We demonstrate the effectiveness of\nSyncViolinist with significantly improved qualitative and quantitative results\nfrom unseen violin performance audio, outperforming state-of-the-art methods.\nExtensive subjective evaluations involving professional violinists further\nvalidate our approach. The code and dataset are available at\nhttps://github.com/Kakanat/SyncViolinist.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.08343v1"
    },
    {
        "title": "ConvMesh: Reimagining Mesh Quality Through Convex Optimization",
        "authors": [
            "Alexander Valverde"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Mesh generation has become a critical topic in recent years, forming the\nfoundation of all 3D objects used across various applications, such as virtual\nreality, gaming, and 3D printing. With advancements in computational resources\nand machine learning, neural networks have emerged as powerful tools for\ngenerating high-quality 3D object representations, enabling accurate scene and\nobject reconstructions. Despite these advancements, many methods produce meshes\nthat lack realism or exhibit geometric and textural flaws, necessitating\nadditional processing to improve their quality. This research introduces a\nconvex optimization programming called disciplined convex programming to\nenhance existing meshes by refining their texture and geometry with a conic\nsolver. By focusing on a sparse set of point clouds from both the original and\ntarget meshes, this method demonstrates significant improvements in mesh\nquality with minimal data requirements. To evaluate the approach, the classical\ndolphin mesh dataset from Facebook AI was used as a case study, with\noptimization performed using the CVXPY library. The results reveal promising\npotential for streamlined and effective mesh refinement.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.08484v1"
    },
    {
        "title": "Novel 3D Binary Indexed Tree for Volume Computation of 3D Reconstructed\n  Models from Volumetric Data",
        "authors": [
            "Quoc-Bao Nguyen-Le",
            "Tuan-Hy Le",
            "Anh-Triet Do"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In the burgeoning field of medical imaging, precise computation of 3D volume\nholds a significant importance for subsequent qualitative analysis of 3D\nreconstructed objects. Combining multivariate calculus, marching cube\nalgorithm, and binary indexed tree data structure, we developed an algorithm\nfor efficient computation of intrinsic volume of any volumetric data recovered\nfrom computed tomography (CT) or magnetic resonance (MR). We proposed the 30\nconfigurations of volume values based on the polygonal mesh generation method.\nOur algorithm processes the data in scan-line order simultaneously with\nreconstruction algorithm to create a Fenwick tree, ensuring query time much\nfaster and assisting users' edition of slicing or transforming model. We tested\nthe algorithm's accuracy on simple 3D objects (e.g., sphere, cylinder) to\ncomplicated structures (e.g., lungs, cardiac chambers). The result deviated\nwithin $\\pm 0.004 \\text{cm}^3$ and there is still room for further improvement.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.10441v1"
    },
    {
        "title": "GraphicsDreamer: Image to 3D Generation with Physical Consistency",
        "authors": [
            "Pei Chen",
            "Fudong Wang",
            "Yixuan Tong",
            "Jingdong Chen",
            "Ming Yang",
            "Minghui Yang"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Recently, the surge of efficient and automated 3D AI-generated content (AIGC)\nmethods has increasingly illuminated the path of transforming human imagination\ninto complex 3D structures. However, the automated generation of 3D content is\nstill significantly lags in industrial application. This gap exists because 3D\nmodeling demands high-quality assets with sharp geometry, exquisite topology,\nand physically based rendering (PBR), among other criteria. To narrow the\ndisparity between generated results and artists' expectations, we introduce\nGraphicsDreamer, a method for creating highly usable 3D meshes from single\nimages. To better capture the geometry and material details, we integrate the\nPBR lighting equation into our cross-domain diffusion model, concurrently\npredicting multi-view color, normal, depth images, and PBR materials. In the\ngeometry fusion stage, we continue to enforce the PBR constraints, ensuring\nthat the generated 3D objects possess reliable texture details, supporting\nrealistic relighting. Furthermore, our method incorporates topology\noptimization and fast UV unwrapping capabilities, allowing the 3D products to\nbe seamlessly imported into graphics engines. Extensive experiments demonstrate\nthat our model can produce high quality 3D assets in a reasonable time cost\ncompared to previous methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.14214v1"
    },
    {
        "title": "FlairGPT: Repurposing LLMs for Interior Designs",
        "authors": [
            "Gabrielle Littlefair",
            "Niladri Shekhar Dutt",
            "Niloy J. Mitra"
        ],
        "category": "cs.GR",
        "published_year": "2025",
        "summary": "  Interior design involves the careful selection and arrangement of objects to\ncreate an aesthetically pleasing, functional, and harmonized space that aligns\nwith the client's design brief. This task is particularly challenging, as a\nsuccessful design must not only incorporate all the necessary objects in a\ncohesive style, but also ensure they are arranged in a way that maximizes\naccessibility, while adhering to a variety of affordability and usage\nconsiderations. Data-driven solutions have been proposed, but these are\ntypically room- or domain-specific and lack explainability in their design\ndesign considerations used in producing the final layout. In this paper, we\ninvestigate if large language models (LLMs) can be directly utilized for\ninterior design. While we find that LLMs are not yet capable of generating\ncomplete layouts, they can be effectively leveraged in a structured manner,\ninspired by the workflow of interior designers. By systematically probing LLMs,\nwe can reliably generate a list of objects along with relevant constraints that\nguide their placement. We translate this information into a design layout\ngraph, which is then solved using an off-the-shelf constrained optimization\nsetup to generate the final layouts. We benchmark our algorithm in various\ndesign configurations against existing LLM-based methods and human designs, and\nevaluate the results using a variety of quantitative and qualitative metrics\nalong with user studies. In summary, we demonstrate that LLMs, when used in a\nstructured manner, can effectively generate diverse high-quality layouts,\nmaking them a viable solution for creating large-scale virtual scenes. Project\nwebpage at https://flairgpt.github.io/\n",
        "pdf_link": "http://arxiv.org/pdf/2501.04648v1"
    },
    {
        "title": "A Scalable System for Visual Analysis of Ocean Data",
        "authors": [
            "Toshit Jain",
            "Upkar Singh",
            "Varun Singh",
            "Vijay Kumar Boda",
            "Ingrid Hotz",
            "Sathish S. Vadhiyar",
            "P. N. Vinayachandran",
            "Vijay Natarajan"
        ],
        "category": "cs.GR",
        "published_year": "2025",
        "summary": "  Oceanographers rely on visual analysis to interpret model simulations,\nidentify events and phenomena, and track dynamic ocean processes. The ever\nincreasing resolution and complexity of ocean data due to its dynamic nature\nand multivariate relationships demands a scalable and adaptable visualization\ntool for interactive exploration. We introduce pyParaOcean, a scalable and\ninteractive visualization system designed specifically for ocean data analysis.\npyParaOcean offers specialized modules for common oceanographic analysis tasks,\nincluding eddy identification and salinity movement tracking. These modules\nseamlessly integrate with ParaView as filters, ensuring a user-friendly and\neasy-to-use system while leveraging the parallelization capabilities of\nParaView and a plethora of inbuilt general-purpose visualization\nfunctionalities. The creation of an auxiliary dataset stored as a Cinema\ndatabase helps address I/O and network bandwidth bottlenecks while supporting\nthe generation of quick overview visualizations. We present a case study on the\nBay of Bengal (BoB) to demonstrate the utility of the system and scaling\nstudies to evaluate the efficiency of the system.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.05009v1"
    },
    {
        "title": "Learning Style Similarity for Searching Infographics",
        "authors": [
            "Babak Saleh",
            "Mira Dontcheva",
            "Aaron Hertzmann",
            "Zhicheng Liu"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  Infographics are complex graphic designs integrating text, images, charts and\nsketches. Despite the increasing popularity of infographics and the rapid\ngrowth of online design portfolios, little research investigates how we can\ntake advantage of these design resources. In this paper we present a method for\nmeasuring the style similarity between infographics. Based on human perception\ndata collected from crowdsourced experiments, we use computer vision and\nmachine learning algorithms to learn a style similarity metric for infographic\ndesigns. We evaluate different visual features and learning algorithms and find\nthat a combination of color histograms and Histograms-of-Gradients (HoG)\nfeatures is most effective in characterizing the style of infographics. We\ndemonstrate our similarity metric on a preliminary image retrieval test.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.01214v1"
    },
    {
        "title": "Simulation of bifurcated stent grafts to treat abdominal aortic\n  aneurysms (AAA)",
        "authors": [
            "Jan Egger",
            "Stefan Großkopf",
            "Bernd Freisleben"
        ],
        "category": "cs.GR",
        "published_year": "2016",
        "summary": "  In this paper a method is introduced, to visualize bifurcated stent grafts in\nCT-Data. The aim is to improve therapy planning for minimal invasive treatment\nof abdominal aortic aneurysms (AAA). Due to precise measurement of the\nabdominal aortic aneurysm and exact simulation of the bifurcated stent graft,\nphysicians are supported in choosing a suitable stent prior to an intervention.\nThe presented method can be used to measure the dimensions of the abdominal\naortic aneurysm as well as simulate a bifurcated stent graft. Both of these\nprocedures are based on a preceding segmentation and skeletonization of the\naortic, right and left iliac. Using these centerlines (aortic, right and left\niliac) a bifurcated initial stent is constructed. Through the implementation of\nan ACM method the initial stent is fit iteratively to the vessel walls - due to\nthe influence of external forces (distance- as well as balloonforce). Following\nthe fitting process, the crucial values for choosing a bifurcated stent graft\nare measured, e.g. aortic diameter, right and left common iliac diameter,\nminimum diameter of distal neck. The selected stent is then simulated to the\nCT-Data - starting with the initial stent. It hereby becomes apparent if the\ndimensions of the bifurcated stent graft are exact, i.e. the fitting to the\narteries was done properly and no ostium was covered.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.02490v1"
    },
    {
        "title": "HexaShrink, an exact scalable framework for hexahedral meshes with\n  attributes and discontinuities: multiresolution rendering and storage of\n  geoscience models",
        "authors": [
            "Jean-Luc Peyrot",
            "Laurent Duval",
            "Frédéric Payan",
            "Lauriane Bouard",
            "Lénaïc Chizat",
            "Sébastien Schneider",
            "Marc Antonini"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  With huge data acquisition progresses realized in the past decades and\nacquisition systems now able to produce high resolution grids and point clouds,\nthe digitization of physical terrains becomes increasingly more precise. Such\nextreme quantities of generated and modeled data greatly impact computational\nperformances on many levels of high-performance computing (HPC): storage media,\nmemory requirements, transfer capability, and finally simulation interactivity,\nnecessary to exploit this instance of big data. Efficient representations and\nstorage are thus becoming \"enabling technologies'' in HPC experimental and\nsimulation science. We propose HexaShrink, an original decomposition scheme for\nstructured hexahedral volume meshes. The latter are used for instance in\nbiomedical engineering, materials science, or geosciences. HexaShrink provides\na comprehensive framework allowing efficient mesh visualization and storage.\nIts exactly reversible multiresolution decomposition yields a hierarchy of\nmeshes of increasing levels of details, in terms of either geometry, continuous\nor categorical properties of cells. Starting with an overview of volume meshes\ncompression techniques, our contribution blends coherently different\nmultiresolution wavelet schemes in different dimensions. It results in a global\nframework preserving discontinuities (faults) across scales, implemented as a\nfully reversible upscaling at different resolutions. Experimental results are\nprovided on meshes of varying size and complexity. They emphasize the\nconsistency of the proposed representation, in terms of visualization,\nattribute downsampling and distribution at different resolutions. Finally,\nHexaShrink yields gains in storage space when combined to lossless compression\ntechniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.07614v2"
    },
    {
        "title": "ShapeNet: An Information-Rich 3D Model Repository",
        "authors": [
            "Angel X. Chang",
            "Thomas Funkhouser",
            "Leonidas Guibas",
            "Pat Hanrahan",
            "Qixing Huang",
            "Zimo Li",
            "Silvio Savarese",
            "Manolis Savva",
            "Shuran Song",
            "Hao Su",
            "Jianxiong Xiao",
            "Li Yi",
            "Fisher Yu"
        ],
        "category": "cs.GR",
        "published_year": "2015",
        "summary": "  We present ShapeNet: a richly-annotated, large-scale repository of shapes\nrepresented by 3D CAD models of objects. ShapeNet contains 3D models from a\nmultitude of semantic categories and organizes them under the WordNet taxonomy.\nIt is a collection of datasets providing many semantic annotations for each 3D\nmodel such as consistent rigid alignments, parts and bilateral symmetry planes,\nphysical sizes, keywords, as well as other planned annotations. Annotations are\nmade available through a public web-based interface to enable data\nvisualization of object attributes, promote data-driven geometric analysis, and\nprovide a large-scale quantitative benchmark for research in computer graphics\nand vision. At the time of this technical report, ShapeNet has indexed more\nthan 3,000,000 models, 220,000 models out of which are classified into 3,135\ncategories (WordNet synsets). In this report we describe the ShapeNet effort as\na whole, provide details for all currently available datasets, and summarize\nfuture plans.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.03012v1"
    },
    {
        "title": "TopoMap: A 0-dimensional Homology Preserving Projection of\n  High-Dimensional Data",
        "authors": [
            "Harish Doraiswamy",
            "Julien Tierny",
            "Paulo J. S. Silva",
            "Luis Gustavo Nonato",
            "Claudio Silva"
        ],
        "category": "cs.GR",
        "published_year": "2020",
        "summary": "  Multidimensional Projection is a fundamental tool for high-dimensional data\nanalytics and visualization. With very few exceptions, projection techniques\nare designed to map data from a high-dimensional space to a visual space so as\nto preserve some dissimilarity (similarity) measure, such as the Euclidean\ndistance for example. In fact, although adopting distinct mathematical\nformulations designed to favor different aspects of the data, most\nmultidimensional projection methods strive to preserve dissimilarity measures\nthat encapsulate geometric properties such as distances or the proximity\nrelation between data objects. However, geometric relations are not the only\ninteresting property to be preserved in a projection. For instance, the\nanalysis of particular structures such as clusters and outliers could be more\nreliably performed if the mapping process gives some guarantee as to\ntopological invariants such as connected components and loops. This paper\nintroduces TopoMap, a novel projection technique which provides topological\nguarantees during the mapping process. In particular, the proposed method\nperforms the mapping from a high-dimensional space to a visual space, while\npreserving the 0-dimensional persistence diagram of the Rips filtration of the\nhigh-dimensional data, ensuring that the filtrations generate the same\nconnected components when applied to the original as well as projected data.\nThe presented case studies show that the topological guarantee provided by\nTopoMap not only brings confidence to the visual analytic process but also can\nbe used to assist in the assessment of other projection methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.01512v1"
    },
    {
        "title": "MVF Designer: Design and Visualization of Morse Vector Fields",
        "authors": [
            "Youjia Zhou",
            "Janis Lazovskis",
            "Michael J. Catanzaro",
            "Matthew Zabka",
            "Bei Wang"
        ],
        "category": "cs.GR",
        "published_year": "2019",
        "summary": "  Vector field design on surfaces was originally motivated by applications in\ngraphics such as texture synthesis and rendering. In this paper, we consider\nthe idea of vector field design with a new motivation from computational\ntopology. We are interested in designing and visualizing vector fields to aid\nthe study of Morse functions, Morse vector fields, and Morse-Smale complexes.\nTo achieve such a goal, we present MVF Designer, a new interactive design\nsystem that provides fine-grained control over vector field geometry, enables\nthe editing of vector field topology, and supports a design process in a simple\nand efficient way using elementary moves, which are actions that initiate or\nadvance our design process. Our system allows mathematicians to explore the\ncomplex configuration spaces of Morse functions, their gradients, and their\nassociated Morse-Smale complexes. Understanding these spaces will help us\nexpand further their applicability in topological data analysis and\nvisualization.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.09580v1"
    },
    {
        "title": "Dynamic Storyboard Generation in an Engine-based Virtual Environment for\n  Video Production",
        "authors": [
            "Anyi Rao",
            "Xuekun Jiang",
            "Yuwei Guo",
            "Linning Xu",
            "Lei Yang",
            "Libiao Jin",
            "Dahua Lin",
            "Bo Dai"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Amateurs working on mini-films and short-form videos usually spend lots of\ntime and effort on the multi-round complicated process of setting and adjusting\nscenes, plots, and cameras to deliver satisfying video shots. We present\nVirtual Dynamic Storyboard (VDS) to allow users storyboarding shots in virtual\nenvironments, where the filming staff can easily test the settings of shots\nbefore the actual filming. VDS runs on a \"propose-simulate-discriminate\" mode:\nGiven a formatted story script and a camera script as input, it generates\nseveral character animation and camera movement proposals following predefined\nstory and cinematic rules to allow an off-the-shelf simulation engine to render\nvideos. To pick up the top-quality dynamic storyboard from the candidates, we\nequip it with a shot ranking discriminator based on shot quality criteria\nlearned from professional manual-created data. VDS is comprehensively validated\nvia extensive experiments and user studies, demonstrating its efficiency,\neffectiveness, and great potential in assisting amateur video production.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.12688v3"
    },
    {
        "title": "Computational Fluid Dynamics on 3D Point Set Surfaces",
        "authors": [
            "Hassan Bouchiba",
            "Simon Santoso",
            "Jean-Emmanuel Deschaud",
            "Luisa Rocha-Da-Silva",
            "François Goulette",
            "Thierry Coupez"
        ],
        "category": "cs.GR",
        "published_year": "2018",
        "summary": "  Computational fluid dynamics (CFD) in many cases requires designing 3D models\nmanually, which is a tedious task that requires specific skills. In this paper,\nwe present a novel method for performing CFD directly on scanned 3D point\nclouds. The proposed method builds an anisotropic volumetric tetrahedral mesh\nadapted around a point-sampled surface, without an explicit surface\nreconstruction step. The surface is represented by a new extended implicit\nmoving least squares (EIMLS) scalar representation that extends the definition\nof the function to the entire computational domain, which makes it possible for\nuse in immersed boundary flow simulations. The workflow we present allows us to\ncompute flows around point-sampled geometries automatically. It also gives a\nbetter control of the precision around the surface with a limited number of\ncomputational nodes, which is a critical issue in CFD.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04944v1"
    },
    {
        "title": "Voice2Mesh: Cross-Modal 3D Face Model Generation from Voices",
        "authors": [
            "Cho-Ying Wu",
            "Ke Xu",
            "Chin-Cheng Hsu",
            "Ulrich Neumann"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  This work focuses on the analysis that whether 3D face models can be learned\nfrom only the speech inputs of speakers. Previous works for cross-modal face\nsynthesis study image generation from voices. However, image synthesis includes\nvariations such as hairstyles, backgrounds, and facial textures, that are\narguably irrelevant to voice or without direct studies to show correlations. We\ninstead investigate the ability to reconstruct 3D faces to concentrate on only\ngeometry, which is more physiologically grounded. We propose both the\nsupervised learning and unsupervised learning frameworks. Especially we\ndemonstrate how unsupervised learning is possible in the absence of a direct\nvoice-to-3D-face dataset under limited availability of 3D face scans when the\nmodel is equipped with knowledge distillation. To evaluate the performance, we\nalso propose several metrics to measure the geometric fitness of two 3D faces\nbased on points, lines, and regions. We find that 3D face shapes can be\nreconstructed from voices. Experimental results suggest that 3D faces can be\nreconstructed from voices, and our method can improve the performance over the\nbaseline. The best performance gains (15% - 20%) on ear-to-ear distance ratio\nmetric (ER) coincides with the intuition that one can roughly envision whether\na speaker's face is overall wider or thinner only from a person's voice. See\nour project page for codes and data.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.10299v1"
    },
    {
        "title": "Generative Adversarial Networks with Conditional Neural Movement\n  Primitives for An Interactive Generative Drawing Tool",
        "authors": [
            "Suzan Ece Ada",
            "M. Yunus Seker"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  Sketches are abstract representations of visual perception and visuospatial\nconstruction. In this work, we proposed a new framework, Generative Adversarial\nNetworks with Conditional Neural Movement Primitives (GAN-CNMP), that\nincorporates a novel adversarial loss on CNMP to increase sketch smoothness and\nconsistency. Through the experiments, we show that our model can be trained\nwith few unlabeled samples, can construct distributions automatically in the\nlatent space, and produces better results than the base model in terms of shape\nconsistency and smoothness.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.14934v2"
    },
    {
        "title": "Sound-Guided Semantic Image Manipulation",
        "authors": [
            "Seung Hyun Lee",
            "Wonseok Roh",
            "Wonmin Byeon",
            "Sang Ho Yoon",
            "Chan Young Kim",
            "Jinkyu Kim",
            "Sangpil Kim"
        ],
        "category": "cs.GR",
        "published_year": "2021",
        "summary": "  The recent success of the generative model shows that leveraging the\nmulti-modal embedding space can manipulate an image using text information.\nHowever, manipulating an image with other sources rather than text, such as\nsound, is not easy due to the dynamic characteristics of the sources.\nEspecially, sound can convey vivid emotions and dynamic expressions of the real\nworld. Here, we propose a framework that directly encodes sound into the\nmulti-modal (image-text) embedding space and manipulates an image from the\nspace. Our audio encoder is trained to produce a latent representation from an\naudio input, which is forced to be aligned with image and text representations\nin the multi-modal embedding space. We use a direct latent optimization method\nbased on aligned embeddings for sound-guided image manipulation. We also show\nthat our method can mix text and audio modalities, which enrich the variety of\nthe image modification. We verify the effectiveness of our sound-guided image\nmanipulation quantitatively and qualitatively. We also show that our method can\nmix different modalities, i.e., text and audio, which enrich the variety of the\nimage modification. The experiments on zero-shot audio classification and\nsemantic-level image classification show that our proposed model outperforms\nother text and sound-guided state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.00007v1"
    },
    {
        "title": "DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific\n  Visualization",
        "authors": [
            "Chaoli Wang",
            "Jun Han"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Since 2016, we have witnessed the tremendous growth of artificial\nintelligence+visualization (AI+VIS) research. However, existing survey papers\non AI+VIS focus on visual analytics and information visualization, not\nscientific visualization (SciVis). In this paper, we survey related deep\nlearning (DL) works in SciVis, specifically in the direction of DL4SciVis:\ndesigning DL solutions for solving SciVis problems. To stay focused, we\nprimarily consider works that handle scalar and vector field data but exclude\nmesh data. We classify and discuss these works along six dimensions: domain\nsetting, research task, learning type, network architecture, loss function, and\nevaluation metric. The paper concludes with a discussion of the remaining gaps\nto fill along the discussed dimensions and the grand challenges we need to\ntackle as a community. This state-of-the-art survey guides SciVis researchers\nin gaining an overview of this emerging topic and points out future directions\nto grow this research.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.06504v1"
    },
    {
        "title": "Minority Report: A Graph Network Oracle for In Situ Visualization",
        "authors": [
            "Krishna Kumar",
            "Paul Navrátil",
            "Andrew Solis",
            "Joseph Vantassel"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  In situ visualization techniques are hampered by a lack of foresight: crucial\nsimulation phenomena can be missed due to a poor sampling rate or insufficient\ndetail at critical timesteps. Keeping a human in the loop is impractical, and\ndefining statistical triggers can be difficult. This paper demonstrates the\npotential for using a machine-learning-based simulation surrogate as an oracle\nto identify expected critical regions of a large-scale simulation. These\ncritical regions are used to drive the in situ analysis, providing greater data\nfidelity and analysis resolution with an equivalent I/O budget to a traditional\nin situ framework. We develop a distributed asynchronous in situ visualization\nby integrating TACC Galaxy with CB-Geo MPM for material point simulation of\ngranular flows. We employ a PyTorch-based 3D Graph Network Simulator (GNS)\ntrained on granular flow problems as an oracle to predict the dynamics of\ngranular flows. Critical regions of interests are manually tagged in GNS for in\nsitu rendering in MPM.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.12683v1"
    },
    {
        "title": "Automating Rigid Origami Design",
        "authors": [
            "Jeremia Geiger",
            "Karolis Martinkus",
            "Oliver Richter",
            "Roger Wattenhofer"
        ],
        "category": "cs.GR",
        "published_year": "2022",
        "summary": "  Rigid origami has shown potential in large diversity of practical\napplications. However, current rigid origami crease pattern design mostly\nrelies on known tessellations. This strongly limits the diversity and novelty\nof patterns that can be created. In this work, we build upon the recently\ndeveloped principle of three units method to formulate rigid origami design as\na discrete optimization problem, the rigid origami game. Our implementation\nallows for a simple definition of diverse objectives and thereby expands the\npotential of rigid origami further to optimized, application-specific crease\npatterns. We showcase the flexibility of our formulation through use of a\ndiverse set of search methods in several illustrative case studies. We are not\nonly able to construct various patterns that approximate given target shapes,\nbut to also specify abstract, function-based rewards which result in novel,\nfoldable and functional designs for everyday objects.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.13219v2"
    },
    {
        "title": "ShapeCoder: Discovering Abstractions for Visual Programs from\n  Unstructured Primitives",
        "authors": [
            "R. Kenny Jones",
            "Paul Guerrero",
            "Niloy J. Mitra",
            "Daniel Ritchie"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Programs are an increasingly popular representation for visual data, exposing\ncompact, interpretable structure that supports manipulation. Visual programs\nare usually written in domain-specific languages (DSLs). Finding \"good\"\nprograms, that only expose meaningful degrees of freedom, requires access to a\nDSL with a \"good\" library of functions, both of which are typically authored by\ndomain experts. We present ShapeCoder, the first system capable of taking a\ndataset of shapes, represented with unstructured primitives, and jointly\ndiscovering (i) useful abstraction functions and (ii) programs that use these\nabstractions to explain the input shapes. The discovered abstractions capture\ncommon patterns (both structural and parametric) across the dataset, so that\nprograms rewritten with these abstractions are more compact, and expose fewer\ndegrees of freedom. ShapeCoder improves upon previous abstraction discovery\nmethods, finding better abstractions, for more complex inputs, under less\nstringent input assumptions. This is principally made possible by two\nmethodological advancements: (a) a shape to program recognition network that\nlearns to solve sub-problems and (b) the use of e-graphs, augmented with a\nconditional rewrite scheme, to determine when abstractions with complex\nparametric expressions can be applied, in a tractable manner. We evaluate\nShapeCoder on multiple datasets of 3D shapes, where primitive decompositions\nare either parsed from manual annotations or produced by an unsupervised cuboid\nabstraction method. In all domains, ShapeCoder discovers a library of\nabstractions that capture high-level relationships, remove extraneous degrees\nof freedom, and achieve better dataset compression compared with alternative\napproaches. Finally, we investigate how programs rewritten to use discovered\nabstractions prove useful for downstream tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.05661v1"
    },
    {
        "title": "A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony\n  in Talking Head Generation",
        "authors": [
            "Louis Airale",
            "Dominique Vaufreydaz",
            "Xavier Alameda-Pineda"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Animating still face images with deep generative models using a speech input\nsignal is an active research topic and has seen important recent\nprogress.However, much of the effort has been put into lip syncing and\nrendering quality while the generation of natural head motion, let alone the\naudio-visual correlation between head motion and speech, has often been\nneglected.In this work, we propose a multi-scale audio-visual synchrony loss\nand a multi-scale autoregressive GAN to better handle short and long-term\ncorrelation between speech and the dynamics of the head and lips.In particular,\nwe train a stack of syncer models on multimodal input pyramids and use these\nmodels as guidance in a multi-scale generator network to produce audio-aligned\nmotion unfolding over diverse time scales.Both the pyramid of audio-visual\nsyncers and the generative models are trained in a low-dimensional space that\nfully preserves dynamics cues.The experiments show significant improvements\nover the state-of-the-art in head motion dynamics quality and especially in\nmulti-scale audio-visual synchrony on a collection of benchmark datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.03270v2"
    },
    {
        "title": "Efficient Inverse-designed Structural Infill for Complex Engineering\n  Structures",
        "authors": [
            "Peter Dørffler Ladegaard Jensen",
            "Tim Felle Olsen",
            "J. Andreas Bærentzen",
            "Niels Aage",
            "Ole Sigmund"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Inverse design of high-resolution and fine-detailed 3D lightweight mechanical\nstructures is notoriously expensive due to the need for vast computational\nresources and the use of very fine-scaled complex meshes. Furthermore, in\ndesigning for additive manufacturing, infill is often neglected as a component\nof the optimized structure. In this paper, both concerns are addressed using a\nde-homogenization topology optimization procedure on complex engineering\nstructures discretized by 3D unstructured hexahedrals.\n  Using a rectangular-hole microstructure (reminiscent to the stiffness optimal\northogonal rank-3 multi-scale) as a base material for the multi-scale\noptimization, a coarse-scale optimized geometry can be obtained using\nhomogenization-based topology optimization. Due to the microstructure\nperiodicity, this coarse-scale geometry can be up-sampled to a fine physical\ngeometry with optimized infill, with minor loss in structural performance and\nat a fraction of the cost of a fine-scale solution. The upsampling on 3D\nunstructured grids is achieved through stream surface tracing which aligns with\nthe optimized local orientation. The periodicity of the physical geometry can\nbe tuned, such that the material serves as a structural component and also as\nan efficient infill for additive manufacturing designs.\n  The method is demonstrated through three examples. It achieves comparable\nstructural performance to state-of-the-art methods but stands out for its\nsignificant computational time reduction, much faster than the base-line\nmethod. By allowing multiple active layers, the mapped solution becomes more\nmechanically stable, leading to an increased critical buckling load factor\nwithout additional computational expense. The proposed approach achieves\npromising results, benchmarking against large-scale SIMP models demonstrates\ncomputational efficiency improvements of up to 250 times.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.09518v1"
    },
    {
        "title": "MAGMA: Music Aligned Generative Motion Autodecoder",
        "authors": [
            "Sohan Anisetty",
            "Amit Raj",
            "James Hays"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Mapping music to dance is a challenging problem that requires spatial and\ntemporal coherence along with a continual synchronization with the music's\nprogression. Taking inspiration from large language models, we introduce a\n2-step approach for generating dance using a Vector Quantized-Variational\nAutoencoder (VQ-VAE) to distill motion into primitives and train a Transformer\ndecoder to learn the correct sequencing of these primitives. We also evaluate\nthe importance of music representations by comparing naive music feature\nextraction using Librosa to deep audio representations generated by\nstate-of-the-art audio compression algorithms. Additionally, we train\nvariations of the motion generator using relative and absolute positional\nencodings to determine the effect on generated motion quality when generating\narbitrarily long sequence lengths. Our proposed approach achieve\nstate-of-the-art results in music-to-motion generation benchmarks and enables\nthe real-time generation of considerably longer motion sequences, the ability\nto chain multiple motion sequences seamlessly, and easy customization of motion\nsequences to meet style requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.01202v1"
    },
    {
        "title": "Neural Stress Fields for Reduced-order Elastoplasticity and Fracture",
        "authors": [
            "Zeshun Zong",
            "Xuan Li",
            "Minchen Li",
            "Maurizio M. Chiaramonte",
            "Wojciech Matusik",
            "Eitan Grinspun",
            "Kevin Carlberg",
            "Chenfanfu Jiang",
            "Peter Yichen Chen"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We propose a hybrid neural network and physics framework for reduced-order\nmodeling of elastoplasticity and fracture. State-of-the-art scientific\ncomputing models like the Material Point Method (MPM) faithfully simulate\nlarge-deformation elastoplasticity and fracture mechanics. However, their long\nruntime and large memory consumption render them unsuitable for applications\nconstrained by computation time and memory usage, e.g., virtual reality. To\novercome these barriers, we propose a reduced-order framework. Our key\ninnovation is training a low-dimensional manifold for the Kirchhoff stress\nfield via an implicit neural representation. This low-dimensional neural stress\nfield (NSF) enables efficient evaluations of stress values and,\ncorrespondingly, internal forces at arbitrary spatial locations. In addition,\nwe also train neural deformation and affine fields to build low-dimensional\nmanifolds for the deformation and affine momentum fields. These neural stress,\ndeformation, and affine fields share the same low-dimensional latent space,\nwhich uniquely embeds the high-dimensional simulation state. After training, we\nrun new simulations by evolving in this single latent space, which drastically\nreduces the computation time and memory consumption. Our general\ncontinuum-mechanics-based reduced-order framework is applicable to any\nphenomena governed by the elastodynamics equation. To showcase the versatility\nof our framework, we simulate a wide range of material behaviors, including\nelastica, sand, metal, non-Newtonian fluids, fracture, contact, and collision.\nWe demonstrate dimension reduction by up to 100,000X and time savings by up to\n10X.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.17790v1"
    },
    {
        "title": "Motion-Conditioned Image Animation for Video Editing",
        "authors": [
            "Wilson Yan",
            "Andrew Brown",
            "Pieter Abbeel",
            "Rohit Girdhar",
            "Samaneh Azadi"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce MoCA, a Motion-Conditioned Image Animation approach for video\nediting. It leverages a simple decomposition of the video editing problem into\nimage editing followed by motion-conditioned image animation. Furthermore,\ngiven the lack of robust evaluation datasets for video editing, we introduce a\nnew benchmark that measures edit capability across a wide variety of tasks,\nsuch as object replacement, background changes, style changes, and motion\nedits. We present a comprehensive human evaluation of the latest video editing\nmethods along with MoCA, on our proposed benchmark. MoCA establishes a new\nstate-of-the-art, demonstrating greater human preference win-rate, and\noutperforming notable recent approaches including Dreamix (63%), MasaCtrl\n(75%), and Tune-A-Video (72%), with especially significant improvements for\nmotion edits.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.18827v1"
    },
    {
        "title": "Fluid Simulation on Neural Flow Maps",
        "authors": [
            "Yitong Deng",
            "Hong-Xing Yu",
            "Diyang Zhang",
            "Jiajun Wu",
            "Bo Zhu"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  We introduce Neural Flow Maps, a novel simulation method bridging the\nemerging paradigm of implicit neural representations with fluid simulation\nbased on the theory of flow maps, to achieve state-of-the-art simulation of\ninviscid fluid phenomena. We devise a novel hybrid neural field representation,\nSpatially Sparse Neural Fields (SSNF), which fuses small neural networks with a\npyramid of overlapping, multi-resolution, and spatially sparse grids, to\ncompactly represent long-term spatiotemporal velocity fields at high accuracy.\nWith this neural velocity buffer in hand, we compute long-term, bidirectional\nflow maps and their Jacobians in a mechanistically symmetric manner, to\nfacilitate drastic accuracy improvement over existing solutions. These\nlong-range, bidirectional flow maps enable high advection accuracy with low\ndissipation, which in turn facilitates high-fidelity incompressible flow\nsimulations that manifest intricate vortical structures. We demonstrate the\nefficacy of our neural fluid simulation in a variety of challenging simulation\nscenarios, including leapfrogging vortices, colliding vortices, vortex\nreconnections, as well as vortex generation from moving obstacles and density\ndifferences. Our examples show increased performance over existing methods in\nterms of energy conservation, visual complexity, adherence to experimental\nobservations, and preservation of detailed vortical structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.14635v1"
    },
    {
        "title": "Spherical Density-Equalizing Map for Genus-0 Closed Surfaces",
        "authors": [
            "Zhiyuan Lyu",
            "Lok Ming Lui",
            "Gary P. T. Choi"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Density-equalizing maps are a class of mapping methods in which the shape\ndeformation is driven by prescribed density information. In recent years, they\nhave been widely used for data visualization on planar domains and planar\nparameterization of open surfaces. However, the theory and computation of\ndensity-equalizing maps for closed surfaces are much less explored. In this\nwork, we develop a novel method for computing spherical density-equalizing maps\nfor genus-0 closed surfaces. Specifically, we first compute a conformal\nparameterization of the given genus-0 closed surface onto the unit sphere.\nThen, we perform density equalization on the spherical domain based on the\ngiven density information to achieve a spherical density-equalizing map. The\nbijectivity of the mapping is guaranteed using quasi-conformal theory. We\nfurther propose a method for incorporating the harmonic energy and landmark\nconstraints into our formulation to achieve landmark-aligned spherical\ndensity-equalizing maps balancing different distortion measures. Using the\nproposed methods, a large variety of spherical parameterizations can be\nachieved. Applications to surface registration, remeshing, and data\nvisualization are presented to demonstrate the effectiveness of our methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.11795v1"
    },
    {
        "title": "FaçAID: A Transformer Model for Neuro-Symbolic Facade Reconstruction",
        "authors": [
            "Aleksander Plocharski",
            "Jan Swidzinski",
            "Joanna Porter-Sobieraj",
            "Przemyslaw Musialski"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We introduce a neuro-symbolic transformer-based model that converts flat,\nsegmented facade structures into procedural definitions using a custom-designed\nsplit grammar. To facilitate this, we first develop a semi-complex split\ngrammar tailored for architectural facades and then generate a dataset\ncomprising of facades alongside their corresponding procedural representations.\nThis dataset is used to train our transformer model to convert segmented, flat\nfacades into the procedural language of our grammar. During inference, the\nmodel applies this learned transformation to new facade segmentations,\nproviding a procedural representation that users can adjust to generate varied\nfacade designs. This method not only automates the conversion of static facade\nimages into dynamic, editable procedural formats but also enhances the design\nflexibility, allowing for easy modifications.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.01829v2"
    },
    {
        "title": "TopoMap++: A faster and more space efficient technique to compute\n  projections with topological guarantees",
        "authors": [
            "Vitoria Guardieiro",
            "Felipe Inagaki de Oliveira",
            "Harish Doraiswamy",
            "Luis Gustavo Nonato",
            "Claudio Silva"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  High-dimensional data, characterized by many features, can be difficult to\nvisualize effectively. Dimensionality reduction techniques, such as PCA, UMAP,\nand t-SNE, address this challenge by projecting the data into a\nlower-dimensional space while preserving important relationships. TopoMap is\nanother technique that excels at preserving the underlying structure of the\ndata, leading to interpretable visualizations. In particular, TopoMap maps the\nhigh-dimensional data into a visual space, guaranteeing that the 0-dimensional\npersistence diagram of the Rips filtration of the visual space matches the one\nfrom the high-dimensional data. However, the original TopoMap algorithm can be\nslow and its layout can be too sparse for large and complex datasets. In this\npaper, we propose three improvements to TopoMap: 1) a more space-efficient\nlayout, 2) a significantly faster implementation, and 3) a novel TreeMap-based\nrepresentation that makes use of the topological hierarchy to aid the\nexploration of the projections. These advancements make TopoMap, now referred\nto as TopoMap++, a more powerful tool for visualizing high-dimensional data\nwhich we demonstrate through different use case scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.07257v1"
    },
    {
        "title": "Sketching With Your Voice: \"Non-Phonorealistic\" Rendering of Sounds via\n  Vocal Imitation",
        "authors": [
            "Matthew Caren",
            "Kartik Chandra",
            "Joshua B. Tenenbaum",
            "Jonathan Ragan-Kelley",
            "Karima Ma"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  We present a method for automatically producing human-like vocal imitations\nof sounds: the equivalent of \"sketching,\" but for auditory rather than visual\nrepresentation. Starting with a simulated model of the human vocal tract, we\nfirst try generating vocal imitations by tuning the model's control parameters\nto make the synthesized vocalization match the target sound in terms of\nperceptually-salient auditory features. Then, to better match human intuitions,\nwe apply a cognitive theory of communication to take into account how human\nspeakers reason strategically about their listeners. Finally, we show through\nseveral experiments and user studies that when we add this type of\ncommunicative reasoning to our method, it aligns with human intuitions better\nthan matching auditory features alone does. This observation has broad\nimplications for the study of depiction in computer graphics.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.13507v1"
    },
    {
        "title": "Ellipsoidal Density-Equalizing Map for Genus-0 Closed Surfaces",
        "authors": [
            "Zhiyuan Lyu",
            "Lok Ming Lui",
            "Gary P. T. Choi"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Surface parameterization is a fundamental task in geometry processing and\nplays an important role in many science and engineering applications. In recent\nyears, the density-equalizing map, a shape deformation technique based on the\nphysical principle of density diffusion, has been utilized for the\nparameterization of simply connected and multiply connected open surfaces. More\nrecently, a spherical density-equalizing mapping method has been developed for\nthe parameterization of genus-0 closed surfaces. However, for genus-0 closed\nsurfaces with extreme geometry, using a spherical domain for the\nparameterization may induce large geometric distortion. In this work, we\ndevelop a novel method for computing density-equalizing maps of genus-0 closed\nsurfaces onto an ellipsoidal domain. This allows us to achieve ellipsoidal\narea-preserving parameterizations and ellipsoidal parameterizations with\ncontrolled area change. We further propose an energy minimization approach that\ncombines density-equalizing maps and quasi-conformal maps, which allows us to\nproduce ellipsoidal density-equalizing quasi-conformal maps for achieving a\nbalance between density-equalization and quasi-conformality. Using our proposed\nmethods, we can significantly improve the performance of surface remeshing for\ngenus-0 closed surfaces. Experimental results on a large variety of genus-0\nclosed surfaces are presented to demonstrate the effectiveness of our proposed\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12331v1"
    },
    {
        "title": "HoloChrome: Polychromatic Illumination for Speckle Reduction in\n  Holographic Near-Eye Displays",
        "authors": [
            "Florian Schiffers",
            "Grace Kuo",
            "Nathan Matsuda",
            "Douglas Lanman",
            "Oliver Cossairt"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  Holographic displays hold the promise of providing authentic depth cues,\nresulting in enhanced immersive visual experiences for near-eye applications.\nHowever, current holographic displays are hindered by speckle noise, which\nlimits accurate reproduction of color and texture in displayed images. We\npresent HoloChrome, a polychromatic holographic display framework designed to\nmitigate these limitations. HoloChrome utilizes an ultrafast,\nwavelength-adjustable laser and a dual-Spatial Light Modulator (SLM)\narchitecture, enabling the multiplexing of a large set of discrete wavelengths\nacross the visible spectrum. By leveraging spatial separation in our dual-SLM\nsetup, we independently manipulate speckle patterns across multiple\nwavelengths. This novel approach effectively reduces speckle noise through\nincoherent averaging achieved by wavelength multiplexing. Our method is\ncomplementary to existing speckle reduction techniques, offering a new pathway\nto address this challenge. Furthermore, the use of polychromatic illumination\nbroadens the achievable color gamut compared to traditional three-color primary\nholographic displays.\n  Our simulations and tabletop experiments validate that HoloChrome\nsignificantly reduces speckle noise and expands the color gamut. These\nadvancements enhance the performance of holographic near-eye displays, moving\nus closer to practical, immersive next-generation visual experiences.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.24144v1"
    },
    {
        "title": "Time-Dependent 2-D Vector Field Topology: An Approach Inspired by\n  Lagrangian Coherent Structures",
        "authors": [
            "Filip Sadlo",
            "Daniel Weiskopf"
        ],
        "category": "cs.GR",
        "published_year": "2011",
        "summary": "  This paper presents an approach to a time-dependent variant of the concept of\nvector field topology for 2-D vector fields. Vector field topology is defined\nfor steady vector fields and aims at discriminating the domain of a vector\nfield into regions of qualitatively different behaviour. The presented approach\nrepresents a generalization for saddle-type critical points and their\nseparatrices to unsteady vector fields based on generalized streak lines, with\nthe classical vector field topology as its special case for steady vector\nfields. The concept is closely related to that of Lagrangian coherent\nstructures obtained as ridges in the finite-time Lyapunov exponent field. The\nproposed approach is evaluated on both 2-D time-dependent synthetic and vector\nfields from computational fluid dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.5678v1"
    },
    {
        "title": "DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion\n  Model",
        "authors": [
            "Fan Zhang",
            "Naye Ji",
            "Fuxing Gao",
            "Yongping Li"
        ],
        "category": "cs.GR",
        "published_year": "2023",
        "summary": "  Speech-driven gesture synthesis is a field of growing interest in virtual\nhuman creation. However, a critical challenge is the inherent intricate\none-to-many mapping between speech and gestures. Previous studies have explored\nand achieved significant progress with generative models. Notwithstanding, most\nsynthetic gestures are still vastly less natural. This paper presents\nDiffMotion, a novel speech-driven gesture synthesis architecture based on\ndiffusion models. The model comprises an autoregressive temporal encoder and a\ndenoising diffusion probability Module. The encoder extracts the temporal\ncontext of the speech input and historical gestures. The diffusion module\nlearns a parameterized Markov chain to gradually convert a simple distribution\ninto a complex distribution and generates the gestures according to the\naccompanied speech. Compared with baselines, objective and subjective\nevaluations confirm that our approach can produce natural and diverse\ngesticulation and demonstrate the benefits of diffusion-based models on\nspeech-driven gesture synthesis.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.10047v2"
    },
    {
        "title": "QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation",
        "authors": [
            "Zhizhen Zhou",
            "Yejing Huo",
            "Guoheng Huang",
            "An Zeng",
            "Xuhang Chen",
            "Lian Huang",
            "Zinuo Li"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  The study of music-generated dance is a novel and challenging Image\ngeneration task. It aims to input a piece of music and seed motions, then\ngenerate natural dance movements for the subsequent music. Transformer-based\nmethods face challenges in time series prediction tasks related to human\nmovements and music due to their struggle in capturing the nonlinear\nrelationship and temporal aspects. This can lead to issues like joint\ndeformation, role deviation, floating, and inconsistencies in dance movements\ngenerated in response to the music. In this paper, we propose a\nQuaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a\nquaternion perspective, which consists of a Spin Position Embedding (SPE)\nmodule and a Quaternion Rotary Attention (QRA) module. First, SPE embeds\nposition information into self-attention in a rotational manner, leading to\nbetter learning of features of movement sequences and audio sequences, and\nimproved understanding of the connection between music and dance. Second, QRA\nrepresents and fuses 3D motion features and audio features in the form of a\nseries of quaternions, enabling the model to better learn the temporal\ncoordination of music and dance under the complex temporal cycle conditions of\ndance generation. Finally, we conducted experiments on the dataset AIST++, and\nthe results show that our approach achieves better and more robust performance\nin generating accurate, high-quality dance movements. Our source code and\ndataset can be available from https://github.com/MarasyZZ/QEAN and\nhttps://google.github.io/aistplusplus_dataset respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.11626v1"
    },
    {
        "title": "LLM Gesticulator: Leveraging Large Language Models for Scalable and\n  Controllable Co-Speech Gesture Synthesis",
        "authors": [
            "Haozhou Pang",
            "Tianwei Ding",
            "Lanshan He",
            "Ming Tao",
            "Lu Zhang",
            "Qi Gan"
        ],
        "category": "cs.GR",
        "published_year": "2024",
        "summary": "  In this work, we present LLM Gesticulator, an LLM-based audio-driven\nco-speech gesture generation framework that synthesizes full-body animations\nthat are rhythmically aligned with the input audio while exhibiting natural\nmovements and editability. Compared to previous work, our model demonstrates\nsubstantial scalability. As the size of the backbone LLM model increases, our\nframework shows proportional improvements in evaluation metrics (a.k.a. scaling\nlaw). Our method also exhibits strong controllability where the content, style\nof the generated gestures can be controlled by text prompt. To the best of our\nknowledge, LLM gesticulator is the first work that use LLM on the co-speech\ngeneration task. Evaluation with existing objective metrics and user studies\nindicate that our framework outperforms prior works.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.10851v2"
    }
]