[
    {
        "title": "Open Access beyond cable: The case of Interactive TV",
        "authors": [
            "Hernan Galperin",
            "Francois Bar"
        ],
        "category": "cs.MM",
        "published_year": "2001",
        "summary": "  In this paper we analyze the development of interactive TV in the U.S. and\nWestern Europe. We argue that despite the nascent character of the market there\nare important regulatory issues at stake, as exemplified by the AOL/TW merger\nand the British Interactive Broadcasting case. Absent rules that provide for\nnon-discriminatory access to network components (including terminal equipment\nspecifications), dominant platform operators are likely to leverage ownership\nof delivery infrastructure into market power over interactive TV services.\nWhile integration between platform operator, service provider and terminal\nvendor may facilitate the introduction of services in the short-term, the\nlasting result will be a collection of fragmented \"walled gardens\" offering\nlimited content and applications. Would interactive TV develop under such\nmodel, the exciting opportunities for broad-based innovation and extended\naccess to multiple information, entertainment and educational services opened\nby the new generation of broadcasting technologies will be foregone\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0109041v2"
    },
    {
        "title": "Data Visualization on Shared Usage Multi-Screen Environment",
        "authors": [
            "Ph. D. Yuriy A. Chashkov"
        ],
        "category": "cs.MM",
        "published_year": "2005",
        "summary": "  The modern multimedia technologies based on the whole palette of hardware and\nsoftware facilities of real-time high-speed information processing, in a\ncombination with effective facilities of the remote access to information\nresources, allow us to visualize diverse types of information. Data\nvisualization facilities &#8211; is the face of the Automated Control System on\nwhom often judge about their efficiency. They take a special place, providing\nvisualization of the diverse information necessary for decision-making by a\nfinal control link - the person allocated by certain powers.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0506070v1"
    },
    {
        "title": "Digital watermarking in the singular vector domain",
        "authors": [
            "Rashmi Agarwal",
            "M. S. Santhanam"
        ],
        "category": "cs.MM",
        "published_year": "2006",
        "summary": "  Many current watermarking algorithms insert data in the spatial or transform\ndomains like the discrete cosine, the discrete Fourier, and the discrete\nwavelet transforms. In this paper, we present a data-hiding algorithm that\nexploits the singular value decomposition (SVD) representation of the data. We\ncompute the SVD of the host image and the watermark and embed the watermark in\nthe singular vectors of the host image. The proposed method leads to an\nimperceptible scheme for digital images, both in grey scale and color and is\nquite robust against attacks like noise and JPEG compression.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0603130v1"
    },
    {
        "title": "Un filtre temporel crédibiliste pour la reconnaissance d'actions\n  humaines dans les vidéos",
        "authors": [
            "Emmanuel Ramasso",
            "Michèle Rombaut",
            "Denis Pellerin"
        ],
        "category": "cs.MM",
        "published_year": "2006",
        "summary": "  In the context of human action recognition in video sequences, a temporal\nbelief filter is presented. It allows to cope with human action disparity and\nlow quality videos. The whole system of action recognition is based on the\nTransferable Belief Model (TBM) proposed by P. Smets. The TBM allows to\nexplicitly model the doubt between actions. Furthermore, the TBM emphasizes the\nconflict which is exploited for action recognition. The filtering performance\nis assessed on real video sequences acquired by a moving camera and under\nseveral unknown view angles.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0607087v1"
    },
    {
        "title": "A Fast Block Matching Algorithm for Video Motion Estimation Based on\n  Particle Swarm Optimization and Motion Prejudgment",
        "authors": [
            "Ran Ren",
            "Madan mohan Manokar",
            "Yaogang Shi",
            "Baoyu Zheng"
        ],
        "category": "cs.MM",
        "published_year": "2006",
        "summary": "  In this paper, we propose a fast 2-D block-based motion estimation algorithm\ncalled Particle Swarm Optimization - Zero-motion Prejudgment(PSO-ZMP) which\nconsists of three sequential routines: 1)Zero-motion prejudgment. The routine\naims at finding static macroblocks(MB) which do not need to perform remaining\nsearch thus reduces the computational cost; 2)Predictive image coding and 3)PSO\nmatching routine. Simulation results obtained show that the proposed PSO-ZMP\nalgorithm achieves over 10 times of computation less than Diamond Search(DS)\nand 5 times less than the recent proposed Adaptive Rood Pattern\nSearching(ARPS). Meanwhile the PSNR performances using PSO-ZMP are very close\nto that using DS and ARPS in some less-motioned sequences. While in some\nsequences containing dense and complex motion contents, the PSNR performances\nof PSO-ZMP are several dB lower than that using DS and ARPS but in an\nacceptable degree.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0609131v1"
    },
    {
        "title": "A High Quality/Low Computational Cost Technique for Block Matching\n  Motion Estimation",
        "authors": [
            "S. Lopez",
            "G. M. Callico",
            "J. F. Lopez",
            "R. Sarmiento"
        ],
        "category": "cs.MM",
        "published_year": "2007",
        "summary": "  Motion estimation is the most critical process in video coding systems. First\nof all, it has a definitive impact on the rate-distortion performance given by\nthe video encoder. Secondly, it is the most computationally intensive process\nwithin the encoding loop. For these reasons, the design of high-performance\nlow-cost motion estimators is a crucial task in the video compression field. An\nadaptive cost block matching (ACBM) motion estimation technique is presented in\nthis paper, featuring an excellent tradeoff between the quality of the\nreconstructed video sequences and the computational effort. Simulation results\ndemonstrate that the ACBM algorithm achieves a slight better rate-distortion\nperformance than the one given by the well-known full search algorithm block\nmatching algorithm with reductions of up to 95% in the computational load.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4819v1"
    },
    {
        "title": "Multimedia Applications of Multiprocessor Systems-on-Chips",
        "authors": [
            "Wayne Wolf"
        ],
        "category": "cs.MM",
        "published_year": "2007",
        "summary": "  This paper surveys the characteristics of multimedia systems. Multimedia\napplications today are dominated by compression and decompression, but\nmultimedia devices must also implement many other functions such as security\nand file management. We introduce some basic concepts of multimedia algorithms\nand the larger set of functions that multimedia systems-on-chips must\nimplement.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4821v1"
    },
    {
        "title": "A Coprocessor for Accelerating Visual Information Processing",
        "authors": [
            "W. Stechele",
            "L. Alvado Carcel",
            "S. Herrmann",
            "J. Lidon Simon"
        ],
        "category": "cs.MM",
        "published_year": "2007",
        "summary": "  Visual information processing will play an increasingly important role in\nfuture electronics systems. In many applications, e.g. video surveillance\ncameras, data throughput of microprocessors is not sufficient and power\nconsumption is too high. Instruction profiling on a typical test algorithm has\nshown that pixel address calculations are the dominant operations to be\noptimized. Therefore AddressLib, a structured scheme for pixel addressing was\ndeveloped, that can be accelerated by AddressEngine, a coprocessor for visual\ninformation processing. In this paper, the architectural design of\nAddressEngine is described, which in the first step supports a subset of the\nAddressLib. Dataflow and memory organization are optimized during architectural\ndesign. AddressEngine was implemented in a FPGA and was tested with MPEG-7\nGlobal Motion Estimation algorithm. Results on processing speed and circuit\ncomplexity are given and compared to a pure software implementation. The next\nstep will be the support for the full AddressLib, including segment addressing.\nAn outlook on further investigations on dynamic reconfiguration capabilities is\ngiven.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4823v1"
    },
    {
        "title": "SecMon: End-to-End Quality and Security Monitoring System",
        "authors": [
            "Tomasz Ciszkowski",
            "Charlott Eliasson",
            "Markus Fiedler",
            "Zbigniew Kotulski",
            "Radu Lupu",
            "Wojciech Mazurczyk"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  The Voice over Internet Protocol (VoIP) is becoming a more available and\npopular way of communicating for Internet users. This also applies to\nPeer-to-Peer (P2P) systems and merging these two have already proven to be\nsuccessful (e.g. Skype). Even the existing standards of VoIP provide an\nassurance of security and Quality of Service (QoS), however, these features are\nusually optional and supported by limited number of implementations. As a\nresult, the lack of mandatory and widely applicable QoS and security guaranties\nmakes the contemporary VoIP systems vulnerable to attacks and network\ndisturbances. In this paper we are facing these issues and propose the SecMon\nsystem, which simultaneously provides a lightweight security mechanism and\nimproves quality parameters of the call. SecMon is intended specially for VoIP\nservice over P2P networks and its main advantage is that it provides\nauthentication, data integrity services, adaptive QoS and (D)DoS attack\ndetection. Moreover, the SecMon approach represents a low-bandwidth consumption\nsolution that is transparent to the users and possesses a self-organizing\ncapability. The above-mentioned features are accomplished mainly by utilizing\ntwo information hiding techniques: digital audio watermarking and network\nsteganography. These techniques are used to create covert channels that serve\nas transport channels for lightweight QoS measurement's results. Furthermore,\nthese metrics are aggregated in a reputation system that enables best route\npath selection in the P2P network. The reputation system helps also to mitigate\n(D)DoS attacks, maximize performance and increase transmission efficiency in\nthe network.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.0134v1"
    },
    {
        "title": "A Reliable SVD based Watermarking Schem",
        "authors": [
            "Chirag Jain",
            "Siddharth Arora",
            "Prasanta K. Panigrahi"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  We propose a novel scheme for watermarking of digital images based on\nsingular value decomposition (SVD), which makes use of the fact that the SVD\nsubspace preserves significant amount of information of an image, as compared\nto its singular value matrix, Zhang and Li (2005). The principal components of\nthe watermark are embedded in the original image, leaving the detector with a\ncomplimentary set of singular vectors for watermark extraction. The above step\ninvariably ensures that watermark extraction from the embedded watermark image,\nusing a modified matrix, is not possible, thereby removing a major drawback of\nan earlier proposed algorithm by Liu and Tan (2002).\n",
        "pdf_link": "http://arxiv.org/pdf/0808.0309v1"
    },
    {
        "title": "An Export Architecture for a Multimedia Authoring Environment",
        "authors": [
            "Jan Mikác",
            "Cécile Roisin",
            "Bao Le Duc"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  In this paper, we propose an export architecture that provides a clear\nseparation of authoring services from publication services. We illustrate this\narchitecture with the LimSee3 authoring tool and several standard publication\nformats: Timesheets, SMIL, and XHTML.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.5154v1"
    },
    {
        "title": "Initial Offset Placement in p2p Live Streaming Systems",
        "authors": [
            "Chunxi Li",
            "Changjia Chen"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  Initial offset placement in p2p streaming systems is studied in this paper.\nProportional placement (PP) scheme is proposed. In this scheme, peer places the\ninitial offset as the offset reported by other reference peer with a shift\nproportional to the buffer width or offset lag of this reference peer. This\nwill introduce a stable placement that supports larger buffer width for peers\nand small buffer width for tracker. Real deployed placement method in PPLive is\nstudied through measurement. It shows that, instead of based on offset lag, the\nplacement is based on buffer width of the reference peer to facilitate the\ninitial chunk fetching. We will prove that, such a PP scheme may not be stable\nunder arbitrary buffer occupation in the reference peer. The required average\nbuffer width then is derived. A simple good peer selection mechanism to check\nthe buffer occupation of reference peer is proposed for a stable PP scheme\nbased on buffer width\n",
        "pdf_link": "http://arxiv.org/pdf/0810.2063v1"
    },
    {
        "title": "Characterization and collection of information from heterogeneous\n  multimedia sources with users' parameters for decision support",
        "authors": [
            "Charles A. B. Robert"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  No single information source can be good enough to satisfy the divergent and\ndynamic needs of users all the time. Integrating information from divergent\nsources can be a solution to deficiencies in information content. We present\nhow Information from multimedia document can be collected based on associating\na generic database to a federated database. Information collected in this way\nis brought into relevance by integrating the parameters of usage and user's\nparameter for decision making. We identified seven different classifications of\nmultimedia document.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.1959v1"
    },
    {
        "title": "Kalinahia: Considering Quality of Service to Design and Execute\n  Distributed Multimedia Applications",
        "authors": [
            "Sophie Laplace",
            "Marc Dalmau",
            "Philippe Roose"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  One of the current challenges of Information Systems is to ensure\nsemi-structured data transmission, such as multimedia data, in a distributed\nand pervasive environment. Information Sytems must then guarantee users a\nquality of service ensuring data accessibility whatever the hardware and\nnetwork conditions may be. They must also guarantee information coherence and\nparticularly intelligibility that imposes a personalization of the service.\nWithin this framework, we propose a design method based on original models of\nmultimedia applications and quality of service. We also define a supervision\nplatform Kalinahia using a user centered heuristic allowing us to define at any\nmoment which configuration of software components constitutes the best answers\nto users' wishes in terms of service.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2529v1"
    },
    {
        "title": "The Korrontea Data Modeling",
        "authors": [
            "Emmanuel Bouix",
            "Philippe Roose",
            "Marc Dalmau"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  Needs of multimedia systems evolved due to the evolution of their\narchitecture which is now distributed into heterogeneous contexts. A critical\nissue lies in the fact that they handle, process, and transmit multimedia data.\nThis data integrates several properties which should be considered since it\nholds a considerable part of its semantics, for instance the lips\nsynchronization in a video. In this paper, we focus on the definition of a\nmodel as a basic abstraction for describing and modeling media in multimedia\nsystems by taking into account their properties. This model will be used in\nsoftware architecture in order to handle data in efficient way. The provided\nmodel is an interesting solution for the integration of media into\napplications; we propose to consider and to handle them in a uniform way. This\nmodel is proposed with synchronization policies to ensure synchronous transport\nof media. Therefore, we use it in a component model that we develop for the\ndesign and deployment of distributed multimedia systems.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2988v1"
    },
    {
        "title": "Heterogeneous component interactions: Sensors integration into\n  multimedia applications",
        "authors": [
            "Christine Louberry",
            "Philippe Roose",
            "Marc Dalmau"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  Resource-constrained embedded and mobile devices are becoming increasingly\ncommon. Since few years, some mobile and ubiquitous devices such as wireless\nsensor, able to be aware of their physical environment, appeared. Such devices\nenable proposing applications which adapt to user's need according the context\nevolution. It implies the collaboration of sensors and software components\nwhich differ on their nature and their communication mechanisms. This paper\nproposes a unified component model in order to easily design applications based\non software components and sensors without taking care of their nature. Then it\npresents a state of the art of communication problems linked to heterogeneous\ncomponents and proposes an interaction mechanism which ensures information\nexchanges between wireless sensors and software components.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2989v1"
    },
    {
        "title": "Optimal Control of a Single Queue with Retransmissions: Delay-Dropping\n  Tradeoffs",
        "authors": [
            "Anastasios Giovanidis",
            "Gerhard Wunder",
            "Joerg Buehler"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  A single queue incorporating a retransmission protocol is investigated,\nassuming that the sequence of per effort success probabilities in the Automatic\nRetransmission reQuest (ARQ) chain is a priori defined and no channel state\ninformation at the transmitter is available. A Markov Decision Problem with an\naverage cost criterion is formulated where the possible actions are to either\ncontinue the retransmission process of an erroneous packet at the next time\nslot or to drop the packet and move on to the next packet awaiting for\ntransmission. The cost per slot is a linear combination of the current queue\nlength and a penalty term in case dropping is chosen as action. The\ninvestigation seeks policies that provide the best possible average packet\ndelay-dropping trade-off for Quality of Service guarantees. An optimal\ndeterministic stationary policy is shown to exist, several structural\nproperties of which are obtained. Based on that, a class of suboptimal\n<L,K>-policies is introduced. These suggest that it is almost optimal to use a\nK-truncated ARQ protocol as long as the queue length is lower than L, else send\nall packets in one shot. The work concludes with an evaluation of the optimal\ndelay-dropping tradeoff using dynamic programming and a comparison between the\noptimal and suboptimal policies.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.3979v1"
    },
    {
        "title": "A Systematic Framework for Dynamically Optimizing Multi-User Wireless\n  Video Transmission",
        "authors": [
            "Fangwen Fu",
            "Mihaela van der Schaar"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  In this paper, we formulate the collaborative multi-user wireless video\ntransmission problem as a multi-user Markov decision process (MUMDP) by\nexplicitly considering the users' heterogeneous video traffic characteristics,\ntime-varying network conditions and the resulting dynamic coupling between the\nwireless users. These environment dynamics are often ignored in existing\nmulti-user video transmission solutions. To comply with the decentralized\nnature of wireless networks, we propose to decompose the MUMDP into local MDPs\nusing Lagrangian relaxation. Unlike in conventional multi-user video\ntransmission solutions stemming from the network utility maximization\nframework, the proposed decomposition enables each wireless user to\nindividually solve its own dynamic cross-layer optimization (i.e. the local\nMDP) and the network coordinator to update the Lagrangian multipliers (i.e.\nresource prices) based on not only current, but also future resource needs of\nall users, such that the long-term video quality of all users is maximized.\nHowever, solving the MUMDP requires statistical knowledge of the experienced\nenvironment dynamics, which is often unavailable before transmission time. To\novercome this obstacle, we then propose a novel online learning algorithm,\nwhich allows the wireless users to update their policies in multiple states\nduring one time slot. This is different from conventional learning solutions,\nwhich often update one state per time slot. The proposed learning algorithm can\nsignificantly improve the learning performance, thereby dramatically improving\nthe video quality experienced by the wireless users over time. Our simulation\nresults demonstrate the efficiency of the proposed MUMDP framework as compared\nto conventional multi-user video transmission solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.0207v1"
    },
    {
        "title": "A Novel Approach for Compression of Images Captured using Bayer Color\n  Filter Arrays",
        "authors": [
            "Sang-Yong Lee",
            "Antonio Ortega"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  We propose a new approach for image compression in digital cameras, where the\ngoal is to achieve better quality at a given rate by using the characteristics\nof a Bayer color filter array. Most digital cameras produce color images by\nusing a single CCD plate, so that each pixel in an image has only one color\ncomponent and therefore an interpolation method is needed to produce a full\ncolor image. After the image processing stage, in order to reduce the memory\nrequirements of the camera, a lossless or lossy compression stage often\nfollows. But in this scheme, before decreasing redundancy through compression,\nredundancy is increased in an interpolation stage. In order to avoid increasing\nthe redundancy before compression, we propose algorithms for image compression\nin which the order of the compression and interpolation stages is reversed. We\nintroduce image transform algorithms, since non interpolated images cannot be\ndirectly compressed with general image coders. The simulation results show that\nour algorithm outperforms conventional methods with various color interpolation\nmethods in a wide range of compression ratios. Our proposed algorithm provides\nnot only better quality but also lower encoding complexity because the amount\nof luminance data used is only half of that in conventional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.2272v1"
    },
    {
        "title": "Virtual Reality",
        "authors": [
            "Dan L. Lacrama",
            "Dorina Fera"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  This paper is focused on the presentation of Virtual Reality principles\ntogether with the main implementation methods and techniques. An overview of\nthe main development directions is included.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.4314v1"
    },
    {
        "title": "The Multimedia Product - between Design and Information, Design and\n  Utility and Design and Entertainment",
        "authors": [
            "Dieter Penteliuc Cotosman"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  The paper investigates the possible coherent and effective alternatives to\nsolve the problems related to the communication needs of any multimedia\nproduct. In essence, the presentation will focus on identifying the issues and\nprinciples governing three types of the design - in fact, the multimedia design\nin a broader sense - namely the information design - precisely aiming at ways\nof organization and presentation of information in a useful and significant\nform, the graphical user interface design, whose sub-domain consists of the\ninformation displayed on the monitor screen and of interactivity between user,\ncomputer and electronic devices, meaning, in fact, everything the user sees,\ntouches, hears and all the elements with which he interacts, the graphic\ndesign, whose main concern is to create an aesthetic layout arrangement (from\nthe visual and perceptive) information.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.3693v1"
    },
    {
        "title": "The new multimedia educational technologies, used in open and distance\n  learning",
        "authors": [
            "Dieter Penteliuc-Cotosman"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  This paper reviews and refers to the latest telematics technology that has\nturned the open system learning and helped it to become an institutional\nalternative to the face-to-face traditional one. Most technologies, briefly\npresented here, will be implemented in the \"ARTeFACt\" project - telematic\nsystem for vocational education system of open system learning, system which\nwill be officially launched at the end of 2006, in the institutional offer of\nthe Faculty of Arts of the University West of Timisoara. The scientific\ncoordination of the doctoral project \"ARTeFACt\" is done by Mr. Prof. Dr. Eng.\nSavi G. George, representing the Department of Mechatronics Faculty of\nMechanical Engineering from the University \"Politehnica\" of Timisoara, Romania\n",
        "pdf_link": "http://arxiv.org/pdf/0904.3694v1"
    },
    {
        "title": "Development and Optimization of a Multimedia Product",
        "authors": [
            "Cristian Anghel",
            "Vlad Muia",
            "Miodrag Stoianovici"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  This article presents a new concept of a multimedia interactive product. It\nis a multiuser versatile platform that can be used for different purposes. The\nfirst implementation of the platform is a multiplayer game called Texas Hold\n'em, which is a very popular community card game. The paper shows the product's\nmultimedia structure where Hardware and Software work together in creating a\nrealistic feeling for the users.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.4205v1"
    },
    {
        "title": "Web Publishing of the Files Obtained by Flash",
        "authors": [
            "Virgiliu Streian",
            "Adela Ionescu"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  The aim of this article is to familiarize the user with the Web publishing of\nthe files obtained by Flash. The article contains an overview of Macromedia\nFlash 5, as well as the running of a Playing Flash movie, information on Flash\nand Generator, the publishing of Flash movies, a HTLM publishing for Flash\nPlayer files and publishing by Generator templates.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.0866v1"
    },
    {
        "title": "A Bandwidth Characterization Tool For MPEG-2 File",
        "authors": [
            "Sandeep. Kugali",
            "S. S. Manvi",
            "A. V. Sutagundar"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  This paper proposes the design and development of MPEG 2 Video Decoder to\noffer flexible and effective utilization of bandwidth services. The decoder is\ncapable of decoding the MPEG 2 bit stream on a single host machine. The present\ndecoder is designed to be simple, but yet effectively reconstruct the video\nfrom MPEG 2 bit stream.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.4607v1"
    },
    {
        "title": "TTSS Packet Classification Algorithm to enhance Multimedia Applications\n  in Network Processor based Router",
        "authors": [
            "R. Avudaiammal",
            "R. SivaSubramanian",
            "R. Pandian",
            "P. Seethalakshmi"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  The objective of this paper is to implement the Trie based Tuple Space\nSearch(TTSS) packet classification algorithm for Network Processor(NP) based\nrouter to enhance multimedia applications. The performance is evaluated using\nIntel IXP2400 NP Simulator. The results demonstrate that, TTSS has better\nperformance than Tuple Space Search algorithm and is well suited to achieve\nhigh speed packet classification to support multimedia applications.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.5073v1"
    },
    {
        "title": "Enhanced Mode Selection Algorithm for H.264 encoder for Application in\n  Low Computational power devices",
        "authors": [
            "Sourabh Rungta",
            "Kshitij Verma",
            "Neeta Tripathi",
            "Anupam Shukla"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  The intent of the H.264 AVC project was to create a standard capable of\nproviding good video quality at substantially lower bit rates than previous\nstandards without increasing the complexity of design so much that it would be\nimpractical or excessively expensive to implement. An additional goal was to\nprovide enough flexibility to allow the standard to be applied to a wide\nvariety of applications. To achieve better coding efficiency, H.264 AVC uses\nseveral techniques such as inter mode and intra mode prediction with variable\nsize motion compensation, which adopts Rate Distortion Optimization (RDO). This\nincreases the computational complexity of the encoder especially for devices\nwith lower processing capabilities such as mobile and other handheld devices.\nIn this paper, we propose an algorithm to reduce the number of mode and sub\nmode evaluations in inter mode prediction. Experimental results show that this\nfast intra mode selection algorithm can lessen about 75 percent encoding time\nwith little loss of bit rate and visual quality.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.0245v1"
    },
    {
        "title": "Efficient Quality-Based Playout Buffer Algorithm",
        "authors": [
            "Emine Zerrin Sakir",
            "Christian Feldbauer"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  Playout buffers are used in VoIP systems to compensate for network delay\njitter by making a trade-off between delay and loss. In this work we propose a\nplayout buffer algorithm that makes the trade-off based on maximization of\nconversational speech quality, aiming to keep the computational complexity\nlowest possible. We model the network delay using a Pareto distribution and\nshow that it is a good compromise between providing an appropriate fit to the\nnetwork delay characteristics and yielding a low arithmetical complexity. We\nuse the ITU-T E-Model as the quality model and simplify its delay impairment\nfunction. The proposed playout buffer algorithm finds the optimum playout delay\nusing a closed-form solution that minimizes the sum of the simplified delay\nimpairment factor and the loss-dependent equipment impairment factor of the\nE-model. The simulation results show that our proposed algorithm outperforms\nexisting state-of-the-art algorithms with a reduced complexity for a\nquality-based algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.2816v1"
    },
    {
        "title": "Prefetching of VoD Programs Based On ART1 Requesting Clustering",
        "authors": [
            "P. Jayarekha",
            "T. R. GopalaKrishnan Nair"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  In this paper, we propose a novel approach to group users according to the\nVoD user request pattern. We cluster the user requests based on ART1 neural\nnetwork algorithm. The knowledge extracted from the cluster is used to prefetch\nthe multimedia object from each cluster before the users request. We have\ndeveloped an algorithm to cluster users according to the users request patterns\nbased on ART1 neural network algorithm that offers an unsupervised clustering.\nThis approach adapts to changes in user request patterns over period without\nlosing previous information. Each cluster is represented as prototype vector by\ngeneralizing the most frequently used URLs that are accessed by all the cluster\nmembers. The simulation results of our proposed clustering and prefetching\nalgorithm, shows enormous increase in the performance of streaming server. Our\nalgorithm helps the servers agent to learn user preferences and discover the\ninformation about the corresponding sources and other similar interested\nindividuals.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.1468v1"
    },
    {
        "title": "A Wavelet-Based Digital Watermarking for Video",
        "authors": [
            "A. Essaouabi",
            "F. Regragui",
            "E. Ibnelhaj"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  A novel video watermarking system operating in the three dimensional wavelet\ntransform is here presented. Specifically the video sequence is partitioned\ninto spatio temporal units and the single shots are projected onto the 3D\nwavelet domain. First a grayscale watermark image is decomposed into a series\nof bitplanes that are preprocessed with a random location matrix. After that\nthe preprocessed bitplanes are adaptively spread spectrum and added in 3D\nwavelet coefficients of the video shot. Our video watermarking algorithm is\nrobust against the attacks of frame dropping, averaging and swapping.\nFurthermore, it allows blind retrieval of embedded watermark which does not\nneed the original video and the watermark is perceptually invisible. The\nalgorithm design, evaluation, and experimentation of the proposed scheme are\ndescribed in this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.0399v1"
    },
    {
        "title": "How Do Interactive Virtual Operas Shift Relationships between Music,\n  Text and Image?",
        "authors": [
            "Alain Bonardi",
            "Francis Rousseaux"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  In this paper we present the new genre of interactive operas implemented on\npersonal computers. They differ from traditional ones not only because they are\nvirtual, but mainly because they offer to composers and listeners new\nperspectives of combinations and interactions between music, text and visual\naspects.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.4880v1"
    },
    {
        "title": "Modeling and Annotating the Expressive Semantics of Dance Videos",
        "authors": [
            "Rajkumar Kannan",
            "Balakrishnan Ramadoss"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Dance videos are interesting and semantics-intensive. At the same time, they\nare the complex type of videos compared to all other types such as sports, news\nand movie videos. In fact, dance video is the one which is less explored by the\nresearchers across the globe. Dance videos exhibit rich semantics such as macro\nfeatures and micro features and can be classified into several types. Hence,\nthe conceptual modeling of the expressive semantics of the dance videos is very\ncrucial and complex. This paper presents a generic Dance Video Semantics Model\n(DVSM) in order to represent the semantics of the dance videos at different\ngranularity levels, identified by the components of the accompanying song. This\nmodel incorporates both syntactic and semantic features of the videos and\nintroduces a new entity type called, Agent, to specify the micro features of\nthe dance videos. The instantiations of the model are expressed as graphs. The\nmodel is implemented as a tool using J2SE and JMF to annotate the macro and\nmicro features of the dance videos. Finally examples and evaluation results are\nprovided to depict the effectiveness of the proposed dance video model.\nKeywords: Agents, Dance videos, Macro features, Micro features, Video\nannotation, Video semantics.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.0442v1"
    },
    {
        "title": "Discovering Knowledge from Multi-modal Lecture Recordings",
        "authors": [
            "Rajkumar Kannan",
            "Christian Guetl"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Educational media mining is the process of converting raw media data from\neducational systems to useful information that can be used to design learning\nsystems, answer research questions and allow personalized learning experiences.\nKnowledge discovery encompasses a wide range of techniques ranging from\ndatabase queries to more recent developments in machine learning and language\ntechnology. Educational media mining techniques are now being used in IT\nServices research worldwide. Multi-modal Lecture Recordings is one of the\nimportant types of educational media and this paper explores the research\nchallenges for mining lecture recordings for the efficient personalized\nlearning experiences. Keywords: Educational Media Mining; Lecture Recordings,\nMultimodal Information System, Personalized Learning; Online Course Ware;\nSkills and Competences;\n",
        "pdf_link": "http://arxiv.org/pdf/1001.0443v1"
    },
    {
        "title": "Multicast Transmission Prefix and Popularity Aware Interval Caching\n  Based Admission Control Policy",
        "authors": [
            "P. Jayarekha",
            "T. R. Gopalakrishnan Nair"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Admission control is a key component in multimedia servers, which will allow\nthe resources to be used by the client only when they are available. A problem\nfaced by numerous content serving machines is overload, when there are too many\nclients who need to be served, the server tends to slow down. An admission\ncontrol algorithm for a multimedia server is responsible for determining if a\nnew request can be accepted without violating the QoS requirements of the\nexisting requests in the system. By caching and streaming only the data in the\ninterval between two successive requests on the same object, the following\nrequest can be serviced directly from the buffer cache without disk operations\nand within the deadline of the request. An admission control strategy based on\nPopularity-aware interval caching for Prefix [3] scheme extends the interval\ncaching by considering different popularity of multimedia objects. The method\nof Prefix caching with multicast transmission of popular objects utilizes the\nhard disk and network bandwidth efficiently and increases the number of\nrequests being served.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.3744v1"
    },
    {
        "title": "Cooperative Proxy Servers Architecture for VoD to Achieve High QoS with\n  Reduced Transmission Time and Cost",
        "authors": [
            "M. Dakshayini",
            "T. R. Gopalakrishan Nair"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  - The aim of this paper is to propose a novel Voice On Demand (VoD)\narchitecture and implementation of an efficient load sharing algorithm to\nachieve Quality of Service (QoS). This scheme reduces the transmission cost\nfrom the Centralized Multimedia Sever (CMS) to Proxy Servers (PS) by sharing\nthe videos among the proxy servers of the Local Proxy Servers Group [LPSG] and\namong the neighboring LPSGs, which are interconnected in a ring fashion. This\nresults in very low request rejection ratio, reduction in transmission time and\ncost, reduction of load on the CMS and high QoS for the users. Simulation\nresults indicate acceptable initial startup latency, reduced transmission cost\nand time, load sharing among the proxy servers, among the LPSGs and between the\nCMS and the PS.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.3774v1"
    },
    {
        "title": "An Adaptive Dynamic Replacement Approach for a Multicast based\n  Popularity Aware Prefix Cache Memory System",
        "authors": [
            "P. Jayarekha",
            "T. R. Gopalakrishnan Nair"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  In this paper we have proposed an adaptive dynamic cache replacement\nalgorithm for a multimedia servers cache system. The goal is to achieve an\neffective utilization of the cache memory which stores the prefix of popular\nvideos. A replacement policy is usually evaluated using hit ratio, the\nfrequency with which any video is requested. Usually discarding the least\nrecently used page is the policy of choice in cache management. The adaptive\ndynamic replacement approach for prefix cache is a self tuning, low overhead\nalgorithm that responds online to changing access patterns. It constantly\nbalances between lru and lfu to improve combined result. It automatically\nadapts to evolving workloads. Since in our algorithm we have considered a\nprefix caching with multicast transmission of popular objects it utilizes the\nhard disk and network bandwidth efficiently and increases the number of\nrequests being served.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.4135v1"
    },
    {
        "title": "A Strategy to enable Prefix of Multicast VoD through dynamic buffer\n  allocation",
        "authors": [
            "T. R. GopalaKrishnan nair",
            "P. Jayarekha"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  In this paper we have proposed a dynamic buffer allocation algorithm for the\nprefix, based on the popularity of the videos. More cache blocks are allocated\nfor most popular videos and a few cache blocks are allocated for less popular\nvideos. Buffer utilization is also maximized irrespective of the load on the\nVideo-on-Demand system. Overload can lead the server getting slowed down. By\nstoring the first few seconds of popular video clips, a multimedia local server\ncan shield the users from the delay, throughput, and loss properties of the\npath between the local server and the central server. The key idea of\ncontrolled multicast is used to allow clients to share a segment of a video\nstream even when the requests arrive at different times. This dynamic buffer\nallocation algorithm is simulated and its performance is evaluated based on the\nbuffer utilization by multimedia servers and average buffer allocation for the\nmost popular videos. Our simulation results shows efficient utilization of\nnetwork bandwidth and reduced hard disk utilization hence resulting in increase\nin the number of requests being served.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.1166v1"
    },
    {
        "title": "Shape-Adaptive Motion Estimation Algorithm for MPEG-4 Video Coding",
        "authors": [
            "F. Benboubker",
            "F. Abdi",
            "A. Ahaitouf"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  This paper presents a gradient based motion estimation algorithm based on\nshape-motion prediction, which takes advantage of the correlation between\nneighboring Binary Alpha Blocks (BABs), to match with the Mpeg-4 shape coding\ncase and speed up the estimation process. The PSNR and computation time\nachieved by the proposed algorithm seem to be better than those obtained by\nmost popular motion estimation techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.1168v1"
    },
    {
        "title": "Stochastic Model Based Proxy Servers Architecture for VoD to Achieve\n  Reduced Client Waiting Time",
        "authors": [
            "T. R. GopalaKrishnan Nair",
            "M. Dakshayini"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  In a video on demand system, the main video repository may be far away from\nthe user and generally has limited streaming capacities. Since a high quality\nvideo's size is huge, it requires high bandwidth for streaming over the\ninternet. In order to achieve a higher video hit ratio, reduced client waiting\ntime, distributed server's architecture can be used, in which multiple local\nservers are placed close to clients and, based on their regional demands video\ncontents are cached dynamically from the main server. As the cost of proxy\nserver is decreasing and demand for reduced waiting time is increasing day by\nday, newer architectures are explored, innovative schemes are arrived at. In\nthis paper we present novel 3 layer architecture, includes main multimedia\nserver, a Tracker and Proxy servers. This architecture targets to optimize the\nclient waiting time. We also propose an efficient prefix caching and load\nsharing algorithm at the proxy server to allocate the cache according to\nregional popularity of the video. The simulation results demonstrate that it\nachieves significantly lower client's waiting time, when compared to the other\nexisting algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.1195v1"
    },
    {
        "title": "Effect of Embedding Watermark on Compression of the Digital Images",
        "authors": [
            "Deepak Aggarwal",
            "Kanwalvir Singh Dhindsa"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Image Compression plays a very important role in image processing especially\nwhen we are to send the image on the internet. The threat to the information on\nthe internet increases and image is no exception. Generally the image is sent\non the internet as the compressed image to optimally use the bandwidth of the\nnetwork. But as we are on the network, at any intermediate level the image can\nbe changed intentionally or unintentionally. To make sure that the correct\nimage is being delivered at the other end we embed the water mark to the image.\nThe watermarked image is then compressed and sent on the network. When the\nimage is decompressed at the other end we can extract the watermark and make\nsure that the image is the same that was sent by the other end. Though\nwatermarking the image increases the size of the uncompressed image but that\nhas to done to achieve the high degree of robustness i.e. how an image sustains\nthe attacks on it. The present paper is an attempt to make transmission of the\nimages secure from the intermediate attacks by applying the generally used\ncompression transforms.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.3984v1"
    },
    {
        "title": "An Optimal Prefix Replication Strategy for VoD Services",
        "authors": [
            "M Dakshayini",
            "T R GopalaKrishnan Nair"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  In this paper we propose scalable proxy servers cluster architecture of\ninterconnected proxy servers for high quality and high availability services.\nWe also propose an optimal regional popularity based video prefix replication\nstrategy and a scene change based replica caching algorithm that utilizes the\nzipf-like video popularity distribution to maximize the availability of videos\ncloser to the client and request-servicing rate thereby reducing the client\nrejection ratio and the response time for the client. The simulation results of\nour proposed architecture and algorithm show the greater achievement in\nmaximizing the availability of videos, client request-servicing rate and in\nreduction of initial start-up latency and client rejection ratio.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.4049v1"
    },
    {
        "title": "Voice Recognition Algorithms using Mel Frequency Cepstral Coefficient\n  (MFCC) and Dynamic Time Warping (DTW) Techniques",
        "authors": [
            "Lindasalwa Muda",
            "Mumtaj Begam",
            "I. Elamvazuthi"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Digital processing of speech signal and voice recognition algorithm is very\nimportant for fast and accurate automatic voice recognition technology. The\nvoice is a signal of infinite information. A direct analysis and synthesizing\nthe complex voice signal is due to too much information contained in the\nsignal. Therefore the digital signal processes such as Feature Extraction and\nFeature Matching are introduced to represent the voice signal. Several methods\nsuch as Liner Predictive Predictive Coding (LPC), Hidden Markov Model (HMM),\nArtificial Neural Network (ANN) and etc are evaluated with a view to identify a\nstraight forward and effective method for voice signal. The extraction and\nmatching process is implemented right after the Pre Processing or filtering\nsignal is performed. The non-parametric method for modelling the human auditory\nperception system, Mel Frequency Cepstral Coefficients (MFCCs) are utilize as\nextraction techniques. The non linear sequence alignment known as Dynamic Time\nWarping (DTW) introduced by Sakoe Chiba has been used as features matching\ntechniques. Since it's obvious that the voice signal tends to have different\ntemporal rate, the alignment is important to produce the better\nperformance.This paper present the viability of MFCC to extract features and\nDTW to compare the test patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.4083v1"
    },
    {
        "title": "Context-Oriented Web Video Tag Recommendation",
        "authors": [
            "Zhineng Chen",
            "Juan Cao",
            "Yicheng Song",
            "Junbo Guo",
            "Yongdong Zhang",
            "Jintao Li"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Tag recommendation is a common way to enrich the textual annotation of\nmultimedia contents. However, state-of-the-art recommendation methods are built\nupon the pair-wised tag relevance, which hardly capture the context of the web\nvideo, i.e., when who are doing what at where. In this paper we propose the\ncontext-oriented tag recommendation (CtextR) approach, which expands tags for\nweb videos under the context-consistent constraint. Given a web video, CtextR\nfirst collects the multi-form WWW resources describing the same event with the\nvideo, which produce an informative and consistent context; and then, the tag\nrecommendation is conducted based on the obtained context. Experiments on an\n80,031 web video collection show CtextR recommends various relevant tags to web\nvideos. Moreover, the enriched tags improve the performance of web video\ncategorization.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.4637v1"
    },
    {
        "title": "A reversible high embedding capacity data hiding technique for hiding\n  secret data in images",
        "authors": [
            "P. Mohan Kumar",
            "K. L. Shunmuganathan"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  As the multimedia and internet technologies are growing fast, the\ntransmission of digital media plays an important role in communication. The\nvarious digital media like audio, video and images are being transferred\nthrough internet. There are a lot of threats for the digital data that are\ntransferred through internet. Also, a number of security techniques have been\nemployed to protect the data that is transferred through internet. This paper\nproposes a new technique for sending secret messages securely, using\nsteganographic technique. Since the proposed system uses multiple level of\nsecurity for data hiding, where the data is hidden in an image file and the\nstego file is again concealed in another image. Previously, the secret message\nis being encrypted with the encryption algorithm which ensures the achievement\nof high security enabled data transfer through internet.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1676v1"
    },
    {
        "title": "Design And Implementation Of Multilevel Access Control In Medical Image\n  Transmission Using Symmetric Polynomial Based Audio Steganography",
        "authors": [
            "J. Nafeesa Begum",
            "K. Kumar",
            "V. Sumathy"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  ...The steganography scheme makes it possible to hide the medical image in\ndifferent bit locations of host media without inviting suspicion. The Secret\nfile is embedded in a cover media with a key. At the receiving end the key can\nbe derived by all the classes which are higher in the hierarchy using symmetric\npolynomial and the medical image file can be retrieved. The system is\nimplemented and found to be secure, fast and scalable. Simulation results show\nthat the system is dynamic in nature and allows any type of hierarchy. The\nproposed approach performs better even during frequent member joins and leaves.\nThe computation cost is reduced as the same algorithm is used for key\ncomputation and descendant key derivation. Steganographic technique used in\nthis paper does not use the conventional LSB's and uses two bit positions and\nthe hidden data occurs only from a frame which is dictated by the key that is\nused. Hence the quality of stego data is improved.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1682v1"
    },
    {
        "title": "Review of Robust Video Watermarking Algorithms",
        "authors": [
            "Neeta Deshpande",
            "Archana Rajurkar",
            "R Manthalkar"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  There has been a remarkable increase in the data exchange over web and the\nwidespread use of digital media. As a result, multimedia data transfers also\nhad a boost up. The mounting interest with reference to digital watermarking\nthroughout the last decade is certainly due to the increase in the need of\ncopyright protection of digital content. This is also enhanced due to\ncommercial prospective. Applications of video watermarking in copy control,\nbroadcast monitoring, fingerprinting, video authentication, copyright\nprotection etc is immensely rising. The main aspects of information hiding are\ncapacity, security and robustness. Capacity deals with the amount of\ninformation that can be hidden. The skill of anyone detecting the information\nis security and robustness refers to the resistance to modification of the\ncover content before concealed information is destroyed. Video watermarking\nalgorithms normally prefers robustness. In a robust algorithm it is not\npossible to eliminate the watermark without rigorous degradation of the cover\ncontent. In this paper, we introduce the notion of Video Watermarking and the\nfeatures required to design a robust watermarked video for a valuable\napplication. We review several algorithms, and introduce frequently used key\ntechniques. The aim of this paper is to focus on the various domains of video\nwatermarking techniques. The majority of the reviewed methods based on video\nwatermarking emphasize on the notion of robustness of the algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1770v1"
    },
    {
        "title": "Reversible Image data Hiding using Lifting wavelet Transform and\n  Histogram Shifting",
        "authors": [
            "S. Kurshid Jinna",
            "L. Ganesan"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  A method of lossless data hiding in images using integer wavelet transform\nand histogram shifting for gray scale images is proposed. The method shifts\npart of the histogram, to create space for embedding the watermark information\nbits. The method embeds watermark while maintaining the visual quality well.\nThe method is completely reversible. The original image and the watermark data\ncan be recovered without any loss.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1791v1"
    },
    {
        "title": "C Implementation & comparison of companding & silence audio compression\n  techniques",
        "authors": [
            "Kruti Dangarwala",
            "Jigar Shah"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Just about all the newest living room audio-video electronics and PC\nmultimedia products being designed today will incorporate some form of\ncompressed digitized-audio processing capability. Audio compression reduces the\nbit rate required to represent an analog audio signal while maintaining the\nperceived audio quality. Discarding inaudible data reduces the storage,\ntransmission and compute requirements of handling high-quality audio files.\nThis paper covers wave audio file format & algorithm of silence compression\nmethod and companding method to compress and decompress wave audio file. Then\nit compares the result of these two methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.3275v1"
    },
    {
        "title": "Policies and Economics of Digital Multimedia Transmission",
        "authors": [
            "Mohsen Gerami"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  There are different Standards of digital multimedia transmission, for example\nDVB in Europe and ISDB in Japan and DMB in Korea, with different delivery\nsystem (example MPEG-2, MPEG-4).This paper describe an overview of Digital\nMultimedia Transmission (DMT) technologies. The economic aspects of digital\ncontent & software solution industry as a strategic key in the future will be\ndiscussed. The study then focuses on some important policy and technology\nissues, such S-DMB, T-DMB, Digital Video Broadcasting Handheld (DVB-H) and\nconcludes DMT policies for convergence of telecommunications and broadcasting.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.3556v1"
    },
    {
        "title": "Error Concealment in Image Communication Using Edge Map Watermarking and\n  Spatial Smoothing",
        "authors": [
            "Shabnam Sodagari",
            "Peyman Hesami",
            "Alireza Nasiri Avanaki"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  We propose a novel error concealment algorithm to be used at the receiver\nside of a lossy image transmission system. Our algorithm involves hiding the\nedge map of the original image at the transmitter within itself using a robust\nwatermarking scheme. At the receiver, wherever a lost block is detected, the\nextracted edge information is used as border constraint for the spatial\nsmoothing employing the intact neighboring blocks in order to conceal errors.\nSimulation results show the superiority of our technique over existing methods\neven in case of high packet loss ratios in the communication network.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4241v1"
    },
    {
        "title": "Combination of Subtractive Clustering and Radial Basis Function in\n  Speaker Identification",
        "authors": [
            "Ibrahim A. Albidewi",
            "Yap Teck Ann"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Speaker identification is the process of determining which registered speaker\nprovides a given utterance. Speaker identification required to make a claim on\nthe identity of speaker from the Ns trained speaker in its user database. In\nthis study, we propose the combination of clustering algorithm and the\nclassification technique - subtractive and Radial Basis Function (RBF). The\nproposed technique is chosen because RBF is a simpler network structures and\nfaster learning algorithm. RBF finds the input to output map using the local\napproximators which will combine the linear of the approximators and cause the\nlinear combiner have few weights. Besides that, RBF neural network model using\nsubtractive clustering algorithm for selecting the hidden node centers, which\ncan achieve faster training speed. In the meantime, the RBF network was trained\nwith a regularization term so as to minimize the variances of the nodes in the\nhidden layer and perform more accu-rate prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4457v1"
    },
    {
        "title": "Visual Infrared Video Fusion for Night Vision using Background\n  Estimation",
        "authors": [
            "Anjali Malviya",
            "S. G. Bhirud"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Video fusion is a process that combines visual data from different sensors to\nobtain a single composite video preserving the information of the sources. The\navailability of a system, enhancing human ability to perceive the observed\nscenario, is crucial to improve the performance of a surveillance system. The\ninfrared (IR) camera captures thermal image of object in night-time\nenvironment, when only limited visual information can be captured by RGB\ncamera. The fusion of data recorded by an IR sensor and a visible RGB camera\ncan produce information otherwise not obtainable by viewing the sensor outputs\nseparately. In this paper we consider the problem of fusing two video streams\nacquired by an RGB camera and an IR sensor. The pedestrians, distinctly\ncaptured by IR video, are separated and fused with the RGB video. The\nalgorithms implemented involve estimation of the background, followed by\ndetection of object from the IR Video, after necessary denoising. Finally a\nsuitable fusion algorithm is employed to combine the extracted pedestrians with\nthe visual output. The obtained results clearly demonstrate the effectiveness\nof the proposed video fusion scheme, for night vision.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4459v1"
    },
    {
        "title": "Towards Hardware implementation of video applications in new\n  telecommunications devices",
        "authors": [
            "Lamjed Touil",
            "Abdessalem Ben Abdelali",
            "Abdellatif Mtibaa",
            "Elbey Bourennane"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Among the areas, most demanding in terms of calculation is the\ntelecommunication and video applications are now included in several\ntelecommunication devices such as set-top boxes, mobile phones. Embedded videos\napplications in new generations of telecommunication devices need a processing\ncapacity that can not be achieved by the conventional processor, to work around\nthis problem the use of programmable technology has a lot of interest. First,\nField Programmable Gate Arrays (FPGAs) present many performance benefits for\nreal-time image processing applications. The FPGA structure is able to exploit\nspatial and temporal parallelism. In this paper, we present a new method for\nimplementation of the Color Structure Descriptor (CSD) using the FPGA circuit.\nIn fact the (CSD) provides satisfactory image indexing and retrieval results\namong all colorbased descriptors in MPEG-7. But the real time implementation of\nthis descriptor is still having problems. In this paper we propose a method for\nadapting this descriptor for possible implementation under the constraints of\nthe video processing in real time. We have verified the real-time\nimplementation of the (CSD) with an image size of 120*80 pixels.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0771v1"
    },
    {
        "title": "Architecture for Cooperative Prefetching in P2P Video-on- Demand System",
        "authors": [
            "Ubaid Abbasi",
            "Toufik Ahmed"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Most P2P VoD schemes focused on service architectures and overlays\noptimization without considering segments rarity and the performance of\nprefetching strategies. As a result, they cannot better support VCRoriented\nservice in heterogeneous environment having clients using free VCR controls.\nDespite the remarkable popularity in VoD systems, there exist no prior work\nthat studies the performance gap between different prefetching strategies. In\nthis paper, we analyze and understand the performance of different prefetching\nstrategies. Our analytical characterization brings us not only a better\nunderstanding of several fundamental tradeoffs in prefetching strategies, but\nalso important insights on the design of P2P VoD system. On the basis of this\nanalysis, we finally proposed a cooperative prefetching strategy called\n\"cooching\". In this strategy, the requested segments in VCR interactivities are\nprefetched into session beforehand using the information collected through\ngossips. We evaluate our strategy through extensive simulations. The results\nindicate that the proposed strategy outperforms the existing prefetching\nmechanisms.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.1757v1"
    },
    {
        "title": "A Study on Potential of Integrating Multimodal Interaction into Musical\n  Conducting Education",
        "authors": [
            "Gilbert Phuah Leong Siang",
            "Nor Azman Ismail",
            "Pang Yee Yong"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  With the rapid development of computer technology, computer music has begun\nto appear in the laboratory. Many potential utility of computer music is\ngradually increasing. The purpose of this paper is attempted to analyze the\npossibility of integrating multimodal interaction such as vision-based hand\ngesture and speech interaction into musical conducting education. To achieve\nthis purpose, this paper is focus on discuss some related research and the\ntraditional musical conducting education. To do so, six musical conductors had\nbeen interviewed to share their musical conducting learning/ teaching\nexperience. These interviews had been analyzed in this paper to show the\nsyllabus and the focus of musical conducting education for beginners.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.4014v1"
    },
    {
        "title": "Client-to-Client Streaming Scheme for VOD Applications",
        "authors": [
            "M. Dakshayini",
            "T. R. Gopala Krishnan Nair"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  In this paper, we propose an efficient client-to-client streaming approach to\ncooperatively stream the video using chaining technique with unicast\ncommunication among the clients. This approach considers two major issues of\nVoD 1) Prefix caching scheme to accommodate more number of videos closer to\nclient, so that the request-service delay for the user can be minimized. 2)\nCooperative proxy and client chaining scheme for streaming the videos using\nunicasting. This approach minimizes the client rejection rate and bandwidth\nrequirement on server to proxy and proxy to client path. Our simulation results\nshow that the proposed approach achieves reduced client waiting time and\noptimal prefix caching of videos minimizing server to proxy path bandwidth\nusage by utilizing the client to client bandwidth, which is occasionally used\nwhen compared to busy server to proxy path bandwidth.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.5436v1"
    },
    {
        "title": "An Automated Algorithm for Approximation of Temporal Video Data Using\n  Linear B'EZIER Fitting",
        "authors": [
            "Murtaza Ali Khan"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  This paper presents an efficient method for approximation of temporal video\ndata using linear Bezier fitting. For a given sequence of frames, the proposed\nmethod estimates the intensity variations of each pixel in temporal dimension\nusing linear Bezier fitting in Euclidean space. Fitting of each segment ensures\nupper bound of specified mean squared error. Break and fit criteria is employed\nto minimize the number of segments required to fit the data. The proposed\nmethod is well suitable for lossy compression of temporal video data and\nautomates the fitting process of each pixel. Experimental results show that the\nproposed method yields good results both in terms of objective and subjective\nquality measurement parameters without causing any blocking artifacts.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.5613v1"
    },
    {
        "title": "A novel technique for image steganography based on Block-DCT and Huffman\n  Encoding",
        "authors": [
            "A. Nag",
            "S. Biswas",
            "D. Sarkar",
            "P. P. Sarkar"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Image steganography is the art of hiding information into a cover image. This\npaper presents a novel technique for Image steganography based on Block-DCT,\nwhere DCT is used to transform original image (cover image) blocks from spatial\ndomain to frequency domain. Firstly a gray level image of size M x N is divided\ninto no joint 8 x 8 blocks and a two dimensional Discrete Cosine Transform (2-d\nDCT) is performed on each of the P = MN / 64 blocks. Then Huffman encoding is\nalso performed on the secret messages/images before embedding and each bit of\nHuffman code of secret message/image is embedded in the frequency domain by\naltering the least significant bit of each of the DCT coefficients of cover\nimage blocks. The experimental results show that the algorithm has a high\ncapacity and a good invisibility. Moreover PSNR of cover image with stego-image\nshows the better results in comparison with other existing steganography\napproaches. Furthermore, satisfactory security is maintained since the secret\nmessage/image cannot be extracted without knowing decoding rules and Huffman\ntable.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.1186v1"
    },
    {
        "title": "An Alternative Approach of Steganography using Reference Image",
        "authors": [
            "Samir Kumar Bandyopadhyay",
            "Indra Kanta Maitra"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  This paper is to create a practical steganographic implementation for 4-bit\nimages.The proposed technique converts 4 bit image into 4 shaded Gray Scale\nimage. This image will be act as reference image to hide the text. Using this\ngrey scale reference image any text can be hidden. Single character of a text\ncan be represented by 8-bit. The 8-bit character can be split into 4X2 bit\ninformation. If the reference image and the data file are transmitted through\nnetwork separately, we can achieve the effect of Steganography. Here the image\nis not at all distorted because said image is only used for referencing. Any\nhuge mount of text material can be hidden using a very small image. Decipher\nthe text is not possible intercepting the image or data file separately. So, it\nis more secure.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.1233v1"
    },
    {
        "title": "Perceptual Copyright Protection Using Multiresolution Wavelet-Based\n  Watermarking And Fuzzy Logic",
        "authors": [
            "Ming-Shing Hsieh"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  In this paper, an efficiently DWT-based watermarking technique is proposed to\nembed signatures in images to attest the owner identification and discourage\nthe unauthorized copying. This paper deals with a fuzzy inference filter to\nchoose the larger entropy of coefficients to embed watermarks. Unlike most\nprevious watermarking frameworks which embedded watermarks in the larger\ncoefficients of inner coarser subbands, the proposed technique is based on\nutilizing a context model and fuzzy inference filter by embedding watermarks in\nthe larger-entropy coefficients of coarser DWT subbands. The proposed\napproaches allow us to embed adaptive casting degree of watermarks for\ntransparency and robustness to the general image-processing attacks such as\nsmoothing, sharpening, and JPEG compression. The approach has no need the\noriginal host image to extract watermarks. Our schemes have been shown to\nprovide very good results in both image transparency and robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.5136v1"
    },
    {
        "title": "Web Video Categorization based on Wikipedia Categories and\n  Content-Duplicated Open Resources",
        "authors": [
            "Zhineng Chen",
            "Juan Cao",
            "Yicheng Song",
            "Yongdong Zhang",
            "Jintao Li"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  This paper presents a novel approach for web video categorization by\nleveraging Wikipedia categories (WikiCs) and open resources describing the same\ncontent as the video, i.e., content-duplicated open resources (CDORs). Note\nthat current approaches only col-lect CDORs within one or a few media forms and\nignore CDORs of other forms. We explore all these resources by utilizing WikiCs\nand commercial search engines. Given a web video, its discrimin-ative Wikipedia\nconcepts are first identified and classified. Then a textual query is\nconstructed and from which CDORs are collected. Based on these CDORs, we\npropose to categorize web videos in the space spanned by WikiCs rather than\nthat spanned by raw tags. Experimental results demonstrate the effectiveness of\nboth the proposed CDOR collection method and the WikiC voting catego-rization\nalgorithm. In addition, the categorization model built based on both WikiCs and\nCDORs achieves better performance compared with the models built based on only\none of them as well as state-of-the-art approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.0757v1"
    },
    {
        "title": "A Block Based Scheme for Enhancing Low Luminated Images",
        "authors": [
            "A. Saradha Devi",
            "S. Suja Priyadharsini",
            "S. Athinarayanan"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  In this paper the background detection in images in poor lighting can be done\nby the use of morphological filters. Lately contrast image enhancement\ntechnique is used to detect the background in image which uses Weber's Law. The\nproposed technique is more effective one in which the background detection in\nimage can be done in color images. The given image obtained in this method is\nvery effective one. More enhancement can be obtained while comparing the\nresults. In this technique compressed domain enhancement has been used for\nbetter result.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.1478v1"
    },
    {
        "title": "Improved Iterative Techniques to Compensate for Interpolation\n  Distortions",
        "authors": [
            "A. ParandehGheibi",
            "M. A. Akhaee",
            "A. Ayremlou",
            "M. A. Rahimian",
            "F. Marvasti"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  In this paper a novel hybrid approach for compensating the distortion of any\ninterpolation has been proposed. In this hybrid method, a modular approach was\nincorporated in an iterative fashion. By using this approach we can get drastic\nimprovement with less computational complexity. The extension of the proposed\napproach to two dimensions was also studied. Both the simulation results and\nmathematical analyses confirmed the superiority of the hybrid method. The\nproposed method was also shown to be robust against additive noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.3785v1"
    },
    {
        "title": "Transmitting Video-on-Demand Effectively",
        "authors": [
            "Rachit Mohan Garg",
            "Shipra Kapoor",
            "Kapil Kumar",
            "Mohd. Dilshad Ansari"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Now-a-days internet has become a vast source of entertainment & new services\nare available in quick succession which provides entertainment to the users.\nOne of this service i.e. Video-on-Demand is most hyped service in this context.\nTransferring the video over the network with less error is the main objective\nof the service providers. In this paper we present an algorithm for routing the\nvideo to the user in an effective manner along with a method that ensures less\nerror rate than others.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.2432v1"
    },
    {
        "title": "Colour Guided Colour Image Steganography",
        "authors": [
            "R. Amirtharajan",
            "Sandeep Kumar Behera",
            "Motamarri Abhilash Swarup",
            "Mohamed Ashfaaq K",
            "John Bosco Balaguru Rayappan"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Information security has become a cause of concern because of the electronic\neavesdropping. Capacity, robustness and invisibility are important parameters\nin information hiding and are quite difficult to achieve in a single algorithm.\nThis paper proposes a novel steganography technique for digital color image\nwhich achieves the purported targets. The professed methodology employs a\ncomplete random scheme for pixel selection and embedding of data. Of the three\ncolour channels (Red, Green, Blue) in a given colour image, the least two\nsignificant bits of any one of the channels of the color image is used to\nchannelize the embedding capacity of the remaining two channels. We have\ndevised three approaches to achieve various levels of our desired targets. In\nthe first approach, Red is the default guide but it results in localization of\nMSE in the remaining two channels, which makes it slightly vulnerable. In the\nsecond approach, user gets the liberty to select the guiding channel (Red,\nGreen or Blue) to guide the remaining two channels. It will increase the\nrobustness and imperceptibility of the embedded image however the MSE factor\nwill still remain as a drawback. The third approach improves the performance\nfactor as a cyclic methodology is employed and the guiding channel is selected\nin a cyclic fashion. This ensures the uniform distribution of MSE, which gives\nbetter robustness and imperceptibility along with enhanced embedding capacity.\nThe imperceptibility has been enhanced by suitably adapting optimal pixel\nadjustment process (OPAP) on the stego covers.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.4007v1"
    },
    {
        "title": "Haar Wavelet Based Approach for Image Compression and Quality Assessment\n  of Compressed Image",
        "authors": [
            "Kamrul Hasan Talukder",
            "Koichi Harada"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  With the increasing growth of technology and the entrance into the digital\nage, we have to handle a vast amount of information every time which often\npresents difficulties. So, the digital information must be stored and retrieved\nin an efficient and effective manner, in order for it to be put to practical\nuse. Wavelets provide a mathematical way of encoding information in such a way\nthat it is layered according to level of detail. This layering facilitates\napproximations at various intermediate stages. These approximations can be\nstored using a lot less space than the original data. Here a low complex 2D\nimage compression method using wavelets as the basis functions and the approach\nto measure the quality of the compressed image are presented. The particular\nwavelet chosen and used here is the simplest wavelet form namely the Haar\nWavelet. The 2D discret wavelet transform (DWT) has been applied and the detail\nmatrices from the information matrix of the image have been estimated. The\nreconstructed image is synthesized using the estimated detail matrices and\ninformation matrix provided by the Wavelet transform. The quality of the\ncompressed images has been evaluated using some factors like Compression Ratio\n(CR), Peak Signal to Noise Ratio (PSNR), Mean Opinion Score (MOS), Picture\nQuality Scale (PQS) etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.4084v1"
    },
    {
        "title": "Compensating Interpolation Distortion by New Optimized Modular Method",
        "authors": [
            "Ali Ayremlou",
            "Mohammad Tofighi",
            "Farokh Marvasti"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  A modular method was suggested before to recover a band limited signal from\nthe sample and hold and linearly interpolated (or, in general, an\nnth-order-hold) version of the regular samples. In this paper a novel approach\nfor compensating the distortion of any interpolation based on modular method\nhas been proposed. In this method the performance of the modular method is\noptimized by adding only some simply calculated coefficients. This approach\ncauses drastic improvement in terms of SNRs with fewer modules compared to the\nclassical modular method. Simulation results clearly confirm the improvement of\nthe proposed method and also its superior robustness against additive noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.2753v1"
    },
    {
        "title": "Image Inpainting Using Sparsity of the Transform Domain",
        "authors": [
            "H. Hosseini",
            "N. B. Marvasti",
            "F. Marvasti"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  In this paper, we propose a new image inpainting method based on the property\nthat much of the image information in the transform domain is sparse. We add a\nredundancy to the original image by mapping the transform coefficients with\nsmall amplitudes to zero and the resultant sparsity pattern is used as the side\ninformation in the recovery stage. If the side information is not available,\nthe receiver has to estimate the sparsity pattern. At the end, the recovery is\ndone by consecutive projecting between two spatial and transform sets.\nExperimental results show that our method works well for both structural and\ntexture images and outperforms other techniques in objective and subjective\nperformance measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.5458v1"
    },
    {
        "title": "A proposed Optimized Spline Interpolation",
        "authors": [
            "Ramtin Madani",
            "Ali Ayremlou",
            "Arash Amini",
            "Farrokh Marvasti"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  The goal of this paper is to design compact support basis spline functions\nthat best approximate a given filter (e.g., an ideal Lowpass filter). The\noptimum function is found by minimizing the least square problem ($\\ell$2 norm\nof the difference between the desired and the approximated filters) by means of\nthe calculus of variation; more precisely, the introduced splines give optimal\nfiltering properties with respect to their time support interval. Both\nmathematical analysis and simulation results confirm the superiority of these\nsplines.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.0397v2"
    },
    {
        "title": "Evaluating Modelling Approaches for Medical Image Annotations",
        "authors": [
            "Jasmin Opitz",
            "Bijan Parsia",
            "Ulrike Sattler"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Information system designers face many challenges w.r.t. selecting\nappropriate semantic technologies and deciding on a modelling approach for\ntheir system. However, there is no clear methodology yet to evaluate\n\"semantically enriched\" information systems. In this paper we present a case\nstudy on different modelling approaches for annotating medical images and\nintroduce a conceptual framework that can be used to analyse the fitness of\ninformation systems and help designers to spot the strengths and weaknesses of\nvarious modelling approaches as well as managing trade-offs between modelling\neffort and their potential benefits.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.1882v1"
    },
    {
        "title": "Digital watermarking : An approach based on Hilbert transform",
        "authors": [
            "Rashmi Agarwal",
            "R. Krishnan",
            "M. S. Santhanam",
            "K. Srinivas",
            "K. Venugopalan"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Most of the well known algorithms for watermarking of digital images involve\ntransformation of the image data to Fourier or singular vector space. In this\npaper, we introduce watermarking in Hilbert transform domain for digital media.\nGenerally, if the image is a matrix of order $m$ by $n$, then the transformed\nspace is also an image of the same order. However, with Hilbert transforms, the\ntransformed space is of order $2m$ by $2n$. This allows for more latitude in\nstoring the watermark in the host image. Based on this idea, we propose an\nalgorithm for embedding and extracting watermark in a host image and\nanalytically obtain a parameter related to this procedure. Using extensive\nsimulations, we show that the algorithm performs well even if the host image is\ncorrupted by various attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.2965v1"
    },
    {
        "title": "Image Sterilization to Prevent LSB-based Steganographic Transmission",
        "authors": [
            "Goutam Paul",
            "Imon Mukherjee"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Sterilization is a very popular word used in biomedical testing (like removal\nof all microorganisms on surface of an article or in fluid using appropriate\nchemical products). Motivated by this biological analogy, we, for the first\ntime, introduce the concept of sterilization of an image, i.e., removing any\nsteganographic information embedded in the image. Experimental results show\nthat our technique succeeded in sterilizing around 76% to 91% of stego pixels\nin an image on average, where data is embedded using LSB-based steganography.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.5573v1"
    },
    {
        "title": "Alchymical Mirror: Real-time Interactive Sound- and Simple\n  Motion-Tracking Set of Jitter/Max/MSP Patches",
        "authors": [
            "Elizaveta Eidelman",
            "Serguei A. Mokhov"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  This document supplements an experimental Jitter / Max/MSP collection of\nimplementation patches that set its goal to simulate an alchemical process for\na person standing in front of a mirror-like screen while interacting with it.\nThe work involved takes some patience and has three stages to go through. At\nthe final stage the \"alchemist\" in the mirror wearing sharp-colored gloves (for\nmotion tracking) is to extract the final ultimate shining sparkle (FFT-based\nvisualization) in the nexus of the hands. The more the hands are apart, the\nlarge the sparkle should be. Moving hands around should make the sparkle\nfollow. To achieve the desired visual effect and the feedback mechanism, the\nJitter lattice-based intensional programming model is used to work on\n4-dimensional (A+R+G+B) video matrices and sound signals in order to apply some\nwell-known alchemical techniques to the video at real-time to get a mirror\neffect and accompanying transmutation and transformation stages of the video\nbased on the stability of the sound produced for some duration of time in\nreal-time. There is an accompanying video of the result with the interaction\nwith the tool and the corresponding programming patches.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.2219v1"
    },
    {
        "title": "Transmitting important bits and sailing high radio waves: a\n  decentralized cross-layer approach to cooperative video transmission",
        "authors": [
            "Nicholas Mastronarde",
            "Francesco Verde",
            "Donatella Darsena",
            "Anna Scaglione",
            "Mihaela van der Schaar"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  We investigate the impact of cooperative relaying on uplink and downlink\nmulti-user (MU) wireless video transmissions. The objective is to maximize the\nlong-term sum of utilities across the video terminals in a decentralized\nfashion, by jointly optimizing the packet scheduling, the resource allocation,\nand the cooperation decisions, under the assumption that some nodes are willing\nto act as cooperative relays. A pricing-based distributed resource allocation\nframework is adopted, where the price reflects the expected future congestion\nin the network. Specifically, we formulate the wireless video transmission\nproblem as an MU Markov decision process (MDP) that explicitly considers the\ncooperation at the physical layer and the medium access control sublayer, the\nvideo users' heterogeneous traffic characteristics, the dynamically varying\nnetwork conditions, and the coupling among the users' transmission strategies\nacross time due to the shared wireless resource. Although MDPs notoriously\nsuffer from the curse of dimensionality, our study shows that, with appropriate\nsimplications and approximations, the complexity of the MU-MDP can be\nsignificantly mitigated. Our simulation results demonstrate that integrating\ncooperative decisions into the MU-MDP optimization can increase the resource\nprice in networks that only support low transmission rates and can decrease the\nprice in networks that support high transmission rates. Additionally, our\nresults show that cooperation allows users with feeble direct signals to\nachieve improvements in video quality on the order of 5-10 dB peak\nsignal-to-noise ratio (PSNR), with less than 0.8 dB quality loss by users with\nstrong direct signals, and with a moderate increase in total network energy\nconsumption that is significantly less than the energy that a distant node\nwould require to achieve an equivalent PSNR without exploiting cooperative\ndiversity.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.5437v2"
    },
    {
        "title": "Multimedia Database Applications: Issues and Concerns for Classroom\n  Teaching",
        "authors": [
            "Chien Yu",
            "Teri Brandenburg"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  The abundance of multimedia data and information is challenging educators to\neffectively search, browse, access, use, and store the data for their classroom\nteaching. However, many educators could still be accustomed to teaching or\nsearching for information using conventional methods, but often the\nconventional methods may not function well with multimedia data. Educators need\nto efficiently interact and manage a variety of digital media files too. The\npurpose of this study is to review current multimedia database applications in\nteaching and learning, and further discuss some of the issues or concerns that\neducators may have while incorporating multimedia data into their classrooms.\nSome strategies and recommendations are also provided in order for educators to\nbe able to use multimedia data more effectively in their teaching environments.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.5769v1"
    },
    {
        "title": "Interdisciplinary Collaboration through Designing 3D Simulation Case\n  Studies",
        "authors": [
            "Xin Bai",
            "Dana Fusco"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Interdisciplinary collaboration is essential for the advance of research. As\ndomain subjects become more and more specialized, researchers need to cross\ndisciplines for insights from peers in other areas to have a broader and deeper\nunderstand of a topic at micro- and macro-levels. We developed a 3D virtual\nlearning environment that served as a platform for faculty to plan curriculum,\nshare educational beliefs, and conduct cross-discipline research for effective\nlearning. Based upon the scripts designed by faculty from five disciplines,\nvirtual doctors, nurses, or patients interact in a 3D virtual hospital. The\nteaching vignettes were then converted to video clips, allowing users to view,\npause, replay, or comment on the videos individually or in groups. Unlike many\nexisting platforms, we anticipated a value-added by adding a social networking\ncapacity to this virtual environment. The focus of this paper is on the\ncost-efficiency and system design of the virtual learning environment.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.0065v1"
    },
    {
        "title": "Stage Staffing Scheme for Copyright Protection in Multimedia",
        "authors": [
            "Sumit Kumar",
            "Santosh Kumar",
            "Sukumar Nandi"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Copyright protection has become a need in today's world. To achieve a secure\ncopyright protection we embedded some information in images and videos and that\nimage or video is called copyright protected. The embedded information can't be\ndetected by human eye but some attacks and operations can tamper that\ninformation to breach protection. So in order to find a secure technique of\ncopyright protection, we have analyzed image processing techniques i.e. Spatial\nDomain (Least Significant Bit (LSB)), Transform Domain (Discrete Cosine\nTransform (DCT)), Discrete Wavelet Transform (DWT) and there are numerous\nalgorithm for watermarking using them. After having a good understanding of the\nsame we have proposed a novel algorithm named as Stage Staffing Algorithm that\ngenerates results with high effectiveness, additionally we can use self\nextracted-watermark technique to increase the security and automate the process\nof watermark image. The proposed algorithm provides protection in three stages.\nWe have implemented the algorithm and results of the simulations are shown. The\nvarious factors affecting spatial domain watermarking are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.3802v1"
    },
    {
        "title": "Distributed Video Coding: Codec Architecture and Implementation",
        "authors": [
            "Vijay Kumar Kodavalla",
            "Dr. P. G. Krishna Mohan"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Distributed Video Coding (DVC) is a new coding paradigm for video\ncompression, based on Slepian- Wolf (lossless coding) and Wyner-Ziv (lossy\ncoding) information theoretic results. DVC is useful for emerging applications\nsuch as wireless video cameras, wireless low-power surveillance networks and\ndisposable video cameras for medical applications etc. The primary objective of\nDVC is low-complexity video encoding, where bulk of computation is shifted to\nthe decoder, as opposed to low-complexity decoder in conventional video\ncompression standards such as H.264 and MPEG etc. There are couple of early\narchitectures and implementations of DVC from Stanford University[2][3] in\n2002, Berkeley University PRISM (Power-efficient, Robust, hIgh-compression,\nSyndrome-based Multimedia coding)[4][5] in 2002 and European project DISCOVER\n(DIStributed COding for Video SERvices)[6] in 2007. Primarily there are two\ntypes of DVC techniques namely pixel domain and transform domain based.\nTransform domain design will have better rate-distortion (RD) performance as it\nexploits spatial correlation between neighbouring samples and compacts the\nblock energy into as few transform coefficients as possible (aka energy\ncompaction). In this paper, architecture, implementation details and \"C\" model\nresults of our transform domain DVC are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.4712v1"
    },
    {
        "title": "SLDs for Visualizing Multicolor Elevation Contour Lines in Geo-Spatial\n  Web Applications",
        "authors": [
            "B. G. Kodge",
            "P. S. Hiremath"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  This paper addresses the need for geospatial consumers (either humans or\nmachines) to visualize multicolored elevation contour poly lines with respect\ntheir different contour intervals and control the visual portrayal of the data\nwith which they work. The current OpenGIS Web Map Service (WMS) specification\nsupports the ability for an information provider to specify very basic styling\noptions by advertising a preset collection of visual portrayals for each\navailable data set. However, while a WMS currently can provide the user with a\nchoice of style options, the WMS can only tell the user the name of each style.\nIt cannot tell the user what portrayal will look like on the map. More\nimportantly, the user has no way of defining their own styling rules. The\nability for a human or machine client to define these rules requires a styling\nlanguage that the client and server can both understand. Defining this\nlanguage, called the StyledLayerDescriptor (SLD), is the main focus of this\npaper, and it can be used to portray the output of Web Map Servers, Web Feature\nServers and Web Coverage Servers.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.0809v1"
    },
    {
        "title": "Optimized Spline Interpolation",
        "authors": [
            "Ramtin Madani",
            "Ali Ayremlou",
            "Arash Amini",
            "Farrokh Marvasti"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  In this paper, we investigate the problem of designing compact support\ninterpolation kernels for a given class of signals. By using calculus of\nvariations, we simplify the optimization problem from an infinite nonlinear\nproblem to a finite dimensional linear case, and then find the optimum compact\nsupport function that best approximates a given filter in the least square\nsense (l2 norm). The benefit of compact support interpolants is the low\ncomputational complexity in the interpolation process while the optimum compact\nsupport interpolant gaurantees the highest achivable Signal to Noise Ratio\n(SNR). Our simulation results confirm the superior performance of the proposed\nsplines compared to other conventional compact support interpolants such as\ncubic spline.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.0011v1"
    },
    {
        "title": "Survey of Cognitive Radio Techniques in Wireless Network",
        "authors": [
            "Lu Lu"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  In this report, I surveyed the cognitive radio technique in wireless\nnetworks. Researched several kinds of cognitive techniques about their\nadvantages and disadvantages.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.0023v1"
    },
    {
        "title": "Robust Sign Language Recognition System Using ToF Depth Cameras",
        "authors": [
            "Morteza Zahedi",
            "Ali Reza Manashty"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Sign language recognition is a difficult task, yet required for many\napplications in real-time speed. Using RGB cameras for recognition of sign\nlanguages is not very successful in practical situations and accurate 3D\nimaging requires expensive and complex instruments. With introduction of\nTime-of-Flight (ToF) depth cameras in recent years, it has become easier to\nscan the environment for accurate, yet fast depth images of the objects without\nthe need of any extra calibrating object. In this paper, a robust system for\nsign language recognition using ToF depth cameras is presented for converting\nthe recorded signs to a standard and portable XML sign language named SiGML for\neasy transferring and converting to real-time 3D virtual characters animations.\nFeature extraction using moments and classification using nearest neighbor\nclassifier are used to track hand gestures and significant result of 100% is\nachieved for the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.0699v1"
    },
    {
        "title": "Efficient Image Transmission Through Analog Error Correction",
        "authors": [
            "Yang Liu",
            " Jing",
            " Li",
            "Kai Xie"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  This paper presents a new paradigm for image transmission through analog\nerror correction codes. Conventional schemes rely on digitizing images through\nquantization (which inevitably causes significant bandwidth expansion) and\ntransmitting binary bit-streams through digital error correction codes (which\ndo not automatically differentiate the different levels of significance among\nthe bits). To strike a better overall performance in terms of transmission\nefficiency and quality, we propose to use a single analog error correction code\nin lieu of digital quantization, digital code and digital modulation. The key\nis to get analog coding right. We show that this can be achieved by cleverly\nexploiting an elegant \"butterfly\" property of chaotic systems. Specifically, we\ndemonstrate a tail-biting triple-branch baker's map code and its\nmaximum-likelihood decoding algorithm. Simulations show that the proposed\nanalog code can actually outperform digital turbo code, one of the best codes\nknown to date. The results and findings discussed in this paper speak volume\nfor the promising potential of analog codes, in spite of their rather short\nhistory.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.1561v2"
    },
    {
        "title": "Analytical Classification of Multimedia Index Structures by Using a\n  Partitioning Method-Based Framework",
        "authors": [
            "Mohammadreza keyvanpour",
            "Najva Izadpanah"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Due to the advances in hardware technology and increase in production of\nmultimedia data in many applications, during the last decades, multimedia\ndatabases have become increasingly important. Contentbased multimedia retrieval\nis one of an important research area in the field of multimedia databases. Lots\nof research on this field has led to proposition of different kinds of index\nstructures to support fast and efficient similarity search to retrieve\nmultimedia data from these databases. Due to variety and plenty of proposed\nindex structures, we suggest a systematic framework based on partitioning\nmethod used in these structures to classify multimedia index structures, and\nthen we evaluated these structures based on important functional measures. We\nhope this proposed framework will lead to empirical and technical comparison of\nmultimedia index structures and development of more efficient structures at\nfuture.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.1948v1"
    },
    {
        "title": "Learning content similarity for music recommendation",
        "authors": [
            "Brian McFee",
            "Luke Barrington",
            "Gert Lanckriet"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Many tasks in music information retrieval, such as recommendation, and\nplaylist generation for online radio, fall naturally into the query-by-example\nsetting, wherein a user queries the system by providing a song, and the system\nresponds with a list of relevant or similar song recommendations. Such\napplications ultimately depend on the notion of similarity between items to\nproduce high-quality results. Current state-of-the-art systems employ\ncollaborative filter methods to represent musical items, effectively comparing\nitems in terms of their constituent users. While collaborative filter\ntechniques perform well when historical data is available for each item, their\nreliance on historical data impedes performance on novel or unpopular items. To\ncombat this problem, practitioners rely on content-based similarity, which\nnaturally extends to novel items, but is typically out-performed by\ncollaborative filter methods.\n  In this article, we propose a method for optimizing contentbased similarity\nby learning from a sample of collaborative filter data. The optimized\ncontent-based similarity metric can then be applied to answer queries on novel\nand unpopular items, while still maintaining high recommendation accuracy. The\nproposed system yields accurate and efficient representations of audio content,\nand experimental results show significant improvements in accuracy over\ncompeting content-based recommendation techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.2344v1"
    },
    {
        "title": "Fast restoration of natural images corrupted by high-density impulse\n  noise",
        "authors": [
            "Hossein Hosseini",
            "Farokh Marvasti"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  In this paper, we suggest a general model for the fixed-valued impulse noise\nand propose a two-stage method for high density noise suppression while\npreserving the image details. In the first stage, we apply an iterative impulse\ndetector, exploiting the image entropy, to identify the corrupted pixels and\nthen employ an Adaptive Iterative Mean filter to restore them. The filter is\nadaptive in terms of the number of iterations, which is different for each\nnoisy pixel, according to the Euclidean distance from the nearest uncorrupted\npixel. Experimental results show that the proposed filter is fast and\noutperforms the best existing techniques in both objective and subjective\nperformance measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.2899v2"
    },
    {
        "title": "Activities of Daily Living Indexing by Hierarchical HMM for Dementia\n  Diagnostics",
        "authors": [
            "Svebor Karaman",
            "Jenny Benois-Pineau",
            "Jean-François Dartigues",
            "Yann Gaëstel",
            "Rémi Mégret",
            "Julien Pinquier"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  This paper presents a method for indexing human ac- tivities in videos\ncaptured from a wearable camera being worn by patients, for studies of\nprogression of the dementia diseases. Our method aims to produce indexes to\nfacilitate the navigation throughout the individual video recordings, which\ncould help doctors search for early signs of the dis- ease in the activities of\ndaily living. The recorded videos have strong motion and sharp lighting\nchanges, inducing noise for the analysis. The proposed approach is based on a\ntwo steps analysis. First, we propose a new approach to segment this type of\nvideo, based on apparent motion. Each segment is characterized by two original\nmotion de- scriptors, as well as color, and audio descriptors. Second, a\nHidden-Markov Model formulation is used to merge the multimodal audio and video\nfeatures, and classify the test segments. Experiments show the good properties\nof the ap- proach on real data.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.4451v1"
    },
    {
        "title": "Celerity: A Low-Delay Multi-Party Conferencing Solution",
        "authors": [
            "X. Chen",
            "M. Chen",
            "B. Li",
            "Y. Zhao",
            "Y. Wu",
            "J. Li"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  In this paper, we attempt to revisit the problem of multi-party conferencing\nfrom a practical perspective, and to rethink the design space involved in this\nproblem. We believe that an emphasis on low end-to-end delays between any two\nparties in the conference is a must, and the source sending rate in a session\nshould adapt to bandwidth availability and congestion. We present Celerity, a\nmulti-party conferencing solution specifically designed to achieve our\nobjectives. It is entirely Peer-to-Peer (P2P), and as such eliminating the cost\nof maintaining centrally administered servers. It is designed to deliver video\nwith low end-to-end delays, at quality levels commensurate with available\nnetwork resources over arbitrary network topologies where bottlenecks can be\nanywhere in the network. This is in contrast to commonly assumed P2P scenarios\nwhere bandwidth bottlenecks reside only at the edge of the network. The\nhighlight in our design is a distributed and adaptive rate control protocol,\nthat can discover and adapt to arbitrary topologies and network conditions\nquickly, converging to efficient link rate allocations allowed by the\nunderlying network. In accordance with adaptive link rate control, source video\nencoding rates are also dynamically controlled to optimize video quality in\narbitrary and unpredictable network conditions. We have implemented Celerity in\na prototype system, and demonstrate its superior performance over existing\nsolutions in a local experimental testbed and over the Internet.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.1138v4"
    },
    {
        "title": "MediaWiki Grammar Recovery",
        "authors": [
            "Vadim Zaytsev"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  The paper describes in detail the recovery effort of one of the official\nMediaWiki grammars. Over two hundred grammar transformation steps are reported\nand annotated, leading to delivery of a level 2 grammar, semi-automatically\nextracted from a community created semi-formal text using at least five\ndifferent syntactic notations, several non-enforced naming conventions,\nmultiple misspellings, obsolete parsing technology idiosyncrasies and other\nproblems commonly encountered in grammars that were not engineered properly.\nHaving a quality grammar will allow to test and validate it further, without\nalienating the community with a separately developed grammar.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4661v1"
    },
    {
        "title": "Multi-Layer Local Graph Words for Object Recognition",
        "authors": [
            "Svebor Karaman",
            "Jenny Benois-Pineau",
            "Rémi Mégret",
            "Aurélie Bugeau"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  In this paper, we propose a new multi-layer structural approach for the task\nof object based image retrieval. In our work we tackle the problem of\nstructural organization of local features. The structural features we propose\nare nested multi-layered local graphs built upon sets of SURF feature points\nwith Delaunay triangulation. A Bag-of-Visual-Words (BoVW) framework is applied\non these graphs, giving birth to a Bag-of-Graph-Words representation. The\nmulti-layer nature of the descriptors consists in scaling from trivial Delaunay\ngraphs - isolated feature points - by increasing the number of nodes layer by\nlayer up to graphs with maximal number of nodes. For each layer of graphs its\nown visual dictionary is built. The experiments conducted on the SIVAL and\nCaltech-101 data sets reveal that the graph features at different layers\nexhibit complementary performances on the same content and perform better than\nbaseline BoVW approach. The combination of all existing layers, yields\nsignificant improvement of the object recognition performance compared to\nsingle level approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.6895v1"
    },
    {
        "title": "Hierarchical Hidden Markov Model in Detecting Activities of Daily Living\n  in Wearable Videos for Studies of Dementia",
        "authors": [
            "Svebor Karaman",
            "Jenny Benois-Pineau",
            "Vladislavs Dovgalecs",
            "Rémi Mégret",
            "Julien Pinquier",
            "Régine André-Obrecht",
            "Yann Gaëstel",
            "Jean-François Dartigues"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  This paper presents a method for indexing activities of daily living in\nvideos obtained from wearable cameras. In the context of dementia diagnosis by\ndoctors, the videos are recorded at patients' houses and later visualized by\nthe medical practitioners. The videos may last up to two hours, therefore a\ntool for an efficient navigation in terms of activities of interest is crucial\nfor the doctors. The specific recording mode provides video data which are\nreally difficult, being a single sequence shot where strong motion and sharp\nlighting changes often appear. Our work introduces an automatic motion based\nsegmentation of the video and a video structuring approach in terms of\nactivities by a hierarchical two-level Hidden Markov Model. We define our\ndescription space over motion and visual characteristics of video and audio\nchannels. Experiments on real data obtained from the recording at home of\nseveral patients show the difficulty of the task and the promising results of\nour approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.1817v2"
    },
    {
        "title": "A Survey on Web-based AR Applications",
        "authors": [
            "Behrang Parhizkar",
            "Ashraf Abbas M. Al-Modwahi",
            "Arash Habibi Lashkari",
            "Mohammad Mehdi Bartaripou",
            "Hossein Reza Babae"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Due to the increase of interest in Augmented Reality (AR), the potential uses\nof AR are increasing also. It can benefit the user in various fields such as\neducation, business, medicine, and other. Augmented Reality supports the real\nenvironment with synthetic environment to give more details and meaning to the\nobjects in the real word. AR refers to a situation in which the goal is to\nsupplement a user's perception of the real-world through the addition of\nvirtual objects. This paper is an attempt to make a survey of web-based\nAugmented Reality applications and make a comparison among them.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.2993v1"
    },
    {
        "title": "A Scalable Video Search Engine Based on Audio Content Indexing and Topic\n  Segmentation",
        "authors": [
            "Julien Lawto",
            "Jean-Luc Gauvain",
            "Lori Lamel",
            "Gregory Grefenstete",
            "Guillaume Gravier",
            "Julien Despres",
            "Camille Guinaudeau",
            "Pascale Sébillot"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  One important class of online videos is that of news broadcasts. Most news\norganisations provide near-immediate access to topical news broadcasts over the\nInternet, through RSS streams or podcasts. Until lately, technology has not\nmade it possible for a user to automatically go to the smaller parts, within a\nlonger broadcast, that might interest them. Recent advances in both speech\nrecognition systems and natural language processing have led to a number of\nrobust tools that allow us to provide users with quicker, more focussed access\nto relevant segments of one or more news broadcast videos. Here we present our\nnew interface for browsing or searching news broadcasts (video/audio) that\nexploits these new language processing tools to (i) provide immediate access to\ntopical passages within news broadcasts, (ii) browse news broadcasts by events\nas well as by people, places and organisations, (iii) perform cross lingual\nsearch of news broadcasts, (iv) search for news through a map interface, (v)\nbrowse news by trending topics, and (vi) see automatically-generated textual\nclues for news segments, before listening. Our publicly searchable demonstrator\ncurrently indexes daily broadcast news content from 50 sources in English,\nFrench, Chinese, Arabic, Spanish, Dutch and Russian.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.6265v1"
    },
    {
        "title": "A New Digital Watermarking Algorithm Using Combination of Least\n  Significant Bit (LSB) and Inverse Bit",
        "authors": [
            "Abdullah Bamatraf",
            "Rosziati Ibrahim",
            "Mohd. Najib Mohd. Salleh"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  In this paper, we introduce a new digital watermarking algorithm using least\nsignificant bit (LSB). LSB is used because of its little effect on the image.\nThis new algorithm is using LSB by inversing the binary values of the watermark\ntext and shifting the watermark according to the odd or even number of pixel\ncoordinates of image before embedding the watermark. The proposed algorithm is\nflexible depending on the length of the watermark text. If the length of the\nwatermark text is more than ((MxN)/8)-2 the proposed algorithm will also embed\nthe extra of the watermark text in the second LSB. We compare our proposed\nalgorithm with the 1-LSB algorithm and Lee's algorithm using Peak\nsignal-to-noise ratio (PSNR). This new algorithm improved its quality of the\nwatermarked image. We also attack the watermarked image by using cropping and\nadding noise and we got good results as well.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.6727v1"
    },
    {
        "title": "Automatic Classification of X-rated Videos using Obscene Sound Analysis\n  based on a Repeated Curve-like Spectrum Feature",
        "authors": [
            "JaeDeok Lim",
            "ByeongCheol Choi",
            "SeungWan Han",
            "ChoelHoon Lee"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  This paper addresses the automatic classification of X-rated videos by\nanalyzing its obscene sounds. In this paper, obscene sounds refer to audio\nsignals generated from sexual moans and screams during sexual scenes. By\nanalyzing various sound samples, we determined the distinguishable\ncharacteristics of obscene sounds and propose a repeated curve-like spectrum\nfeature that represents the characteristics of such sounds. We constructed\n6,269 audio clips to evaluate the proposed feature, and separately constructed\n1,200 X-rated and general videos for classification. The proposed feature has\nan F1-score, precision, and recall rate of 96.6%, 98.2%, and 95.2%,\nrespectively, for the original dataset, and 92.6%, 97.6%, and 88.0% for a noisy\ndataset of 5dB SNR. And, in classifying videos, the feature has more than a 90%\nF1-score, 97% precision, and an 84% recall rate. From the measured performance,\nX-rated videos can be classified with only the audio features and the repeated\ncurve-like spectrum feature is suitable to detect obscene sounds.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2027v1"
    },
    {
        "title": "Statistical Information of the Increased Demand for Watch the VOD with\n  the Increased Sophistication in the Mobile Devices,Communications and\n  Internet Penetration in Asia",
        "authors": [
            "Saleh Ali Alomari",
            "Putra Sumari"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  As the rapid progress of the media streaming applications such as video\nstreaming can be classified into two types of streaming, Live video streaming,\nVideo on Demand (VoD). Live video streaming is a service which allows the\nclients to watch many TV channels over the internet and the clients able to use\none operation to perform is to switch the channels. Video on Demand (VoD) is\none of the most important applications for the internet of the future and has\nbecome an interactive multimedia service which allows the users to start\nwatching the video of their choice at anytime and anywhere, especially after\nthe rapid deployment of the wireless networks and mobile devices. In this paper\nprovide statistical information about the Internet, communications and mobile\ndevices etc. This has led to an increased demand for the development,\ncommunication and computational powers of many of the mobile wireless\nsubscribers/mobile devices such as laptops, PDAs, smart phones and notebook.\nThese techniques are utilized to obtain a video on demand service with higher\nresolution and quality. Another objective in this paper is to see Malaysia\nranked as a fully developed country by the year 2020.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2042v1"
    },
    {
        "title": "Modelling Gesture Based Ubiquitous Applications",
        "authors": [
            "Kurien Zacharia",
            "Eldo P. Elias",
            "Surekha Mariam Varghese"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  A cost effective, gesture based modelling technique called Virtual\nInteractive Prototyping (VIP) is described in this paper. Prototyping is\nimplemented by projecting a virtual model of the equipment to be prototyped.\nUsers can interact with the virtual model like the original working equipment.\nFor capturing and tracking the user interactions with the model image and sound\nprocessing techniques are used. VIP is a flexible and interactive prototyping\nmethod that has much application in ubiquitous computing environments.\nDifferent commercial as well as socio-economic applications and extension to\ninteractive advertising of VIP are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2044v1"
    },
    {
        "title": "Lossless Digital Image Compression Method for Bitmap Images",
        "authors": [
            "Dr. T. Meyyappan",
            "SM. Thamarai",
            "N. M. Jeya Nachiaban"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  In this research paper, the authors propose a new approach to digital image\ncompression using crack coding This method starts with the original image and\ndevelop crack codes in a recursive manner, marking the pixels visited earlier\nand expanding the entropy in four directions. The proposed method is\nexperimented with sample bitmap images and results are tabulated. The method is\nimplemented in uni-processor machine using C language source code.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2261v1"
    },
    {
        "title": "Markov Decision Process Based Energy-Efficient On-Line Scheduling for\n  Slice-Parallel Video Decoders on Multicore Systems",
        "authors": [
            "Nicholas Mastronarde",
            "Karim Kanoun",
            "David Atienza",
            "Pascal Frossard",
            "Mihaela van der Schaar"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  We consider the problem of energy-efficient on-line scheduling for\nslice-parallel video decoders on multicore systems. We assume that each of the\nprocessors are Dynamic Voltage Frequency Scaling (DVFS) enabled such that they\ncan independently trade off performance for power, while taking the video\ndecoding workload into account. In the past, scheduling and DVFS policies in\nmulti-core systems have been formulated heuristically due to the inherent\ncomplexity of the on-line multicore scheduling problem. The key contribution of\nthis report is that we rigorously formulate the problem as a Markov decision\nprocess (MDP), which simultaneously takes into account the on-line scheduling\nand per-core DVFS capabilities; the power consumption of the processor cores\nand caches; and the loss tolerant and dynamic nature of the video decoder's\ntraffic. In particular, we model the video traffic using a Direct Acyclic Graph\n(DAG) to capture the precedence constraints among frames in a Group of Pictures\n(GOP) structure, while also accounting for the fact that frames have different\ndisplay/decoding deadlines and non-deterministic decoding complexities. The\nobjective of the MDP is to minimize long-term power consumption subject to a\nminimum Quality of Service (QoS) constraint related to the decoder's\nthroughput. Although MDPs notoriously suffer from the curse of dimensionality,\nwe show that, with appropriate simplifications and approximations, the\ncomplexity of the MDP can be mitigated. We implement a slice-parallel version\nof H.264 on a multiprocessor ARM (MPARM) virtual platform simulator, which\nprovides cycle-accurate and bus signal-accurate simulation for different\nprocessors. We use this platform to generate realistic video decoding traces\nwith which we evaluate the proposed on-line scheduling algorithm in Matlab.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.4084v2"
    },
    {
        "title": "Development Trends in Steganography",
        "authors": [
            "Elzbieta Zielinska",
            "Wojciech Mazurczyk",
            "Krzysztof Szczypiorski"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Steganography is a general term referring to all methods for the embedding of\nadditional secret content into some form of carrier, with the aim of\nconcealment of the introduced alterations. The choice of the carrier is nearly\nunlimited, it may be an ancient piece of parchment, as well as a network\nprotocol header. Inspired by biological phenomena, adopted by man in the\nancient times, it has been developed over the ages. Present day steganographic\nmethods are far more sophisticated than their ancient predecessors, but the\nmain principles have remained unchanged. They typically rely on the utilization\nof digital media files or network protocols as a carrier, in which secret data\nis embedded. This paper presents the evolution of the hidden data carrier from\nthe ancient times till the present day and pinpoints the observed development\ntrends, with special emphasis on network steganography.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.5289v2"
    },
    {
        "title": "Blind 3D Model Watermarking Based on Multi-Resolution Representation and\n  Fuzzy Logic",
        "authors": [
            "Sharvari C. Tamane",
            "Ratnadeep R. Deshmukh"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Insertion of a text message, audio data or/and an image into another image or\n3D model is called as a watermarking process. Watermarking has variety of\napplications like: Copyright Protection, Owner Identification, Copy Protection\nand Data Hiding etc., depending upon the type of watermark insertion algorithm.\nWatermark remains in the content after applying various attacks without any\ndistortions. The blind watermarking method used in the system is based on a\nwavelet transform, a fuzzy inference system and a multi-resolution\nrepresentation (MRR) of the 3d model. The watermark scrambled by Arnold\nTransform is embedded in the wavelet coefficients at third resolution level of\nthe MRR. Fuzzy logic approach used in the method makes it to approximate the\nbest possible gain with an accurate scaling factor so that the watermark\nremains invisible. The fuzzy input variables are computed for each wavelet\ncoefficient in the 3D model. The output of the fuzzy system is a single value\nwhich is a perceptual value for each corresponding wavelet coefficient. Thus,\nthe fuzzy perceptual mask combines all these non-linear variables to build a\nsimple, easy to use HVS model. Results shows that the system is robust against\naffine transformations, smoothing, cropping and noise attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.2485v1"
    },
    {
        "title": "Quantitative Multiscale Analysis using Different Wavelets in 1D Voice\n  Signal and 2D Image",
        "authors": [
            "Niraj Shakhakarmi"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Mutiscale analysis represents multiresolution scrutiny of a signal to improve\nits signal quality. Multiresolution analysis of 1D voice signal and 2D image is\nconducted using DCT, FFT and different wavelets such as Haar, Deubachies,\nMorlet, Cauchy, Shannon, Biorthogonal, Symmlet and Coiflet deploying the\ncascaded filter banks based decomposition and reconstruction. The outstanding\nquantitative analysis of the specified wavelets is done to investigate the\nsignal quality, mean square error, entropy and peak-to-peak SNR at multiscale\nstage-4 for both 1D voice signal and 2D image. In addition, the 2D image\ncompression performance is significantly found 93.00% in DB-4, 93.68% in\nbior-4.4, 93.18% in Sym-4 and 92.20% in Coif-2 during the multiscale analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.4035v1"
    },
    {
        "title": "I-SolFramework: An Integrated Solution Framework Six Layers Assessment\n  on Multimedia Information Security Architecture Policy Compliance",
        "authors": [
            "Heru Susanto",
            "Mohammad Nabil Almunawar",
            "Yong Chee Tuan",
            "Mehmet Sabih Aksoy"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Multimedia Information security becomes a important part for the\norganization's intangible assets. Level of confidence and stakeholder trusted\nare performance indicator as successes organization, it is imperative for\norganizations to use Information Security Management System (ISMS) to\neffectively manage their multimedia information assets. The main objective of\nthis paper is to Provide a novel practical framework approach to the\ndevelopment of ISMS, Called by the I-SolFramework, implemented in multimedia\ninformation security architecture (MISA), it divides a problem into six object\ndomains or six layers, namely organization,stakeholders, tool & technology,\npolicy, knowledge, and culture. In addition, this framework also introduced\nnovelty algorithm and mathematic models as measurement and assessment tools of\nMISA parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.0056v1"
    },
    {
        "title": "An Overview of Video Allocation Algorithms for Flash-based SSD Storage\n  Systems",
        "authors": [
            "Jaafer Al-Sabateen",
            "Saleh Ali Alomari",
            "Putra Sumari"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Despite the fact that Solid State Disk (SSD) data storage media had offered a\nrevolutionary property storages community, but the unavailability of a\ncomprehensive allocation strategy in SSDs storage media, leads to consuming the\navailable space, random writing processes, time-consuming reading processes,\nand system resources consumption. In order to overcome these challenges, an\nefficient allocation algorithm is a desirable option. In this paper, we had\nexecuted an intensive investigation on the SSD-based allocation algorithms that\nhad been proposed by the knowledge community. An explanatory comparison had\nbeen made between these algorithms. We reviewed these algorithms in order to\nbuilding advanced knowledge armature that would help in inventing new\nallocation algorithms for this type of storage media.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.2359v1"
    },
    {
        "title": "Content based video retrieval systems",
        "authors": [
            "B V Patel",
            "B B Meshram"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  With the development of multimedia data types and available bandwidth there\nis huge demand of video retrieval systems, as users shift from text based\nretrieval systems to content based retrieval systems. Selection of extracted\nfeatures play an important role in content based video retrieval regardless of\nvideo attributes being under consideration. These features are intended for\nselecting, indexing and ranking according to their potential interest to the\nuser. Good features selection also allows the time and space costs of the\nretrieval process to be reduced. This survey reviews the interesting features\nthat can be extracted from video data for indexing and retrieval along with\nsimilarity measurement methods. We also identify present research issues in\narea of content based video retrieval systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.1641v1"
    },
    {
        "title": "Text Steganography using LSB insertion method along with Chaos Theory",
        "authors": [
            "Bhavana S.",
            "K. L. Sudha"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  The art of information hiding has been around nearly as long as the need for\ncovert communication. Steganography, the concealing of information, arose early\non as an extremely useful method for covert information transmission.\nSteganography is the art of hiding secret message within a larger image or\nmessage such that the hidden message or an image is undetectable; this is in\ncontrast to cryptography, where the existence of the message itself is not\ndisguised, but the content is obscure. The goal of a steganographic method is\nto minimize the visually apparent and statistical differences between the cover\ndata and a steganogram while maximizing the size of the payload. Current\ndigital image steganography presents the challenge of hiding message in a\ndigital image in a way that is robust to image manipulation and attack. This\npaper explains about how a secret message can be hidden into an image using\nleast significant bit insertion method along with chaos.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.1859v1"
    },
    {
        "title": "An Adaptive Watermarking Technique for the copyright of digital images\n  and Digital Image Protection",
        "authors": [
            "Yusuf Perwej",
            "Firoj Parwej",
            "Asif Perwej"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  The Internet as a whole does not use secure links, thus information in\ntransit may be vulnerable to interruption as well. The important of reducing a\nchance of the information being detected during the transmission is being an\nissue in the real world now days. The Digital watermarking method provides for\nthe quick and inexpensive distribution of digital information over the\nInternet. This method provides new ways of ensuring the sufficient protection\nof copyright holders in the intellectual property dispersion process. The\nproperty of digital watermarking images allows insertion of additional data in\nthe image without altering the value of the image.In this paper investigate the\nfollowing relevant concepts and terminology, history of watermarks and the\nproperties of a watermarking system and applications. We are proposing edge\ndetection using Gabor Filters. In this paper we are proposed least significant\nbit (LSB) substitution method to encrypt the message in the watermark image\nfile. The benefits of the LSB are its simplicity to embed the bits of the\nmessage directly into the LSB plane of cover-image and many techniques using\nthese methods. The LSB does not result in a human perceptible difference\nbecause the amplitude of the change is little therefore the human eye the\nresulting stego image will look identical to the cover image and this allows\nhigh perceptual transparency of the LSB. The spatial domain technique LSB\nsubstitution it would be able to use a pseudo-random number generator to\ndetermine the pixels to be used for embedding based on a given key. We are\nusing DCT transform watermark algorithms based on robustness. The watermarking\nrobustness have been calculated by the Peak Signal to Noise Ratio (PSNR) and\nNormalized cross correlation (NC) is used to quantify by the similarity between\nthe real watermark and after extracting watermark.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2800v1"
    },
    {
        "title": "A Novel Video Compression Approach Based on Underdetermined Blind Source\n  Separation",
        "authors": [
            "Jing Liu",
            "Fei Qiao",
            "Qi Wei",
            "Huazhong Yang"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  This paper develops a new video compression approach based on underdetermined\nblind source separation. Underdetermined blind source separation, which can be\nused to efficiently enhance the video compression ratio, is combined with\nvarious off-the-shelf codecs in this paper. Combining with MPEG-2, video\ncompression ratio could be improved slightly more than 33%. As for combing with\nH.264, 4X~12X more compression ratio could be achieved with acceptable PSNR,\naccording to different kinds of video sequences.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.4572v1"
    },
    {
        "title": "Perceptual quality comparison between single-layer and scalable videos\n  at the same spatial, temporal and amplitude resolutions",
        "authors": [
            "Yuanyi Xue",
            "Yao Wang"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  In this paper, the perceptual quality difference between scalable and\nsingle-layer videos coded at the same spatial, temporal and amplitude\nresolution (STAR) is investigated through a subjective test using a mobile\nplatform. Three source videos are considered and for each source video\nsingle-layer and scalable video are compared at 9 different STARs. We utilize\npaired comparison methods with and without tie option. Results collected from\n10 subjects in the without \"tie\" option and 6 subjects in the with \"tie\" option\nshow that there is no significant quality difference between scalable and\nsinglelayer video when coded at the same STAR. An analysis of variance (ANOVA)\ntest is also performed to further confirm the finding.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.1866v1"
    },
    {
        "title": "Q-STAR:A Perceptual Video Quality Model Considering Impact of Spatial,\n  Temporal, and Amplitude Resolutions",
        "authors": [
            "Yen-Fu Ou",
            "Yuanyi Xue",
            "Yao Wang"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  In this paper, we investigate the impact of spatial, temporal and amplitude\nresolution (STAR) on the perceptual quality of a compressed video. Subjective\nquality tests were carried out on a mobile device. Seven source sequences are\nincluded in the tests and for each source sequence we have 27 test\nconfigurations generated by JSVM encoder (3 QP levels, 3 spatial resolutions,\nand 3 temporal resolutions), resulting a total of 189 processed video sequences\n(PVSs). Videos coded at different spatial resolutions are displayed at the full\nscreen size of the mobile platform. Subjective data reveal that the impact of\nspatial resolution (SR), temporal resolution (TR) and quantization stepsize\n(QS) can each be captured by a function with a single content-dependent\nparameter. The joint impact of SR, TR and QS can be accurately modeled by the\nproduct of these three functions with only three parameters. We further find\nthat the quality decay rates with SR and QS, respectively are independent of\nTR, and likewise, the decay rate with TR is independent of SR and QS,\nrespectively. However, there is a significant interaction between the effects\nof SR and QS. The overall quality model is further validated on five other\ndatasets with very high accuracy. The complete model correlates well with the\nsubjective ratings with a Pearson Correlation Coefficient (PCC) of 0.991.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2320v1"
    },
    {
        "title": "Rate Model for Compressed Video Considering Impacts Of Spatial, Temporal\n  and Amplitude Resolutions and Its Applications for Video Coding and\n  Adaptation",
        "authors": [
            "Zhan Ma",
            "Hao Hu",
            "Meng Xu",
            "Yao Wang"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  In this paper, we investigate the impacts of spatial, temporal and amplitude\nresolution (STAR) on the bit rate of a compressed video. We propose an\nanalytical rate model in terms of the quantization stepsize, frame size and\nframe rate. Experimental results reveal that the increase of the video rate as\nthe individual resolution increases follows a power function. Hence, the\nproposed model expresses the rate as the product of power functions of the\nquantization stepsize, frame size and frame rate, respectively. The proposed\nrate model is analytically tractable, requiring only four content dependent\nparameters. We also propose methods for predicting the model parameters from\ncontent features that can be computed from original video. Simulation results\nshow that model predicted rates fit the measured data very well with high\nPearson correlation (PC) and small relative root mean square error (RRMSE). The\nsame model function works for different coding scenarios (including scalable\nand non-scalable video, temporal prediction using either hierarchical B or IPPP\nstructure, etc.) with very high accuracy (average PC $>$ 0.99), but the values\nof model parameters differ. Using the proposed rate model and the quality model\nintroduced in a separate work, we show how to optimize the STAR for a given\nrate constraint, which is important for both encoder rate control and scalable\nvideo adaptation. Furthermore, we demonstrate how to order the spatial,\ntemporal and amplitude layers of a scalable video in a rate-quality optimized\nway.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2625v1"
    },
    {
        "title": "Exploiting Image Local And Nonlocal Consistency For Mixed\n  Gaussian-Impulse Noise Removal",
        "authors": [
            "Jian Zhang",
            "Ruiqin Xiong",
            "Chen Zhao",
            "Siwei Ma",
            "Debin Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Most existing image denoising algorithms can only deal with a single type of\nnoise, which violates the fact that the noisy observed images in practice are\noften suffered from more than one type of noise during the process of\nacquisition and transmission. In this paper, we propose a new variational\nalgorithm for mixed Gaussian-impulse noise removal by exploiting image local\nconsistency and nonlocal consistency simultaneously. Specifically, the local\nconsistency is measured by a hyper-Laplace prior, enforcing the local\nsmoothness of images, while the nonlocal consistency is measured by\nthree-dimensional sparsity of similar blocks, enforcing the nonlocal\nself-similarity of natural images. Moreover, a Split-Bregman based technique is\ndeveloped to solve the above optimization problem efficiently. Extensive\nexperiments for mixed Gaussian plus impulse noise show that significant\nperformance improvements over the current state-of-the-art schemes have been\nachieved, which substantiates the effectiveness of the proposed algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.3718v1"
    },
    {
        "title": "Behavioral Systel Level Power Consumption Modeling of Mobile Video\n  Streaming applications",
        "authors": [
            "Yahia Benmoussa",
            "Jalil Boukhobza",
            "Yassine Hadjadj Aoul",
            "Loïc Lagadec",
            "Djamel Benazzouz"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Nowadays, the use of mobile applications and terminals faces fundamental\nchallenges related to energy constraint. This is due to the limited battery\nlifetime as compared to the increasing hardware evolution. Video streaming is\none of the most energy consuming applications in a mobile system because of its\nintensive use of bandwidth, memory and processing power. In this work, we aim\nto propose a methodology for building and validating a high level global power\nconsumption model including a hardware and software elements. Our approach is\nbased on exploiting the interactions between power consumption sub-models of\nstandalone systems in the perspective to build more accurate global model. The\ninteractions are studied within the exclusive context of video streaming\napplications that are one of the most used mobile applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.6389v1"
    },
    {
        "title": "Minimum Distortion Variance Concatenated Block Codes for Embedded Source\n  Transmission",
        "authors": [
            "Suayb S. Arslan"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Some state-of-art multimedia source encoders produce embedded source bit\nstreams that upon the reliable reception of only a fraction of the total bit\nstream, the decoder is able reconstruct the source up to a basic quality.\nReliable reception of later source bits gradually improve the reconstruction\nquality. Examples include scalable extensions of H.264/AVC and progressive\nimage coders such as JPEG2000. To provide an efficient protection for embedded\nsource bit streams, a concatenated block coding scheme using a minimum mean\ndistortion criterion was considered in the past. Although, the original design\nwas shown to achieve better mean distortion characteristics than previous\nstudies, the proposed coding structure was leading to dramatic quality\nfluctuations. In this paper, a modification of the original design is first\npresented and then the second order statistics of the distortion is taken into\naccount in the optimization. More specifically, an extension scheme is proposed\nusing a minimum distortion variance optimization criterion. This robust system\ndesign is tested for an image transmission scenario. Numerical results show\nthat the proposed extension achieves significantly lower variance than the\noriginal design, while showing similar mean distortion performance using both\nconvolutional codes and low density parity check codes.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.2815v2"
    },
    {
        "title": "Non-uniform Quantization of Detail Components in Wavelet Transformed\n  Image for Lossy JPEG2000 Compression",
        "authors": [
            "Madhur Srivastava",
            "Prasanta K. Panigrahi"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  The paper introduces the idea of non-uniform quantization in the detail\ncomponents of wavelet transformed image. It argues that most of the\ncoefficients of horizontal, vertical and diagonal components lie near to zeros\nand the coefficients representing large differences are few at the extreme ends\nof histogram. Therefore, this paper advocates need for variable step size\nquantization scheme which preserves the edge information at the edge of\nhistogram and removes redundancy with the minimal number of quantized values.\nTo support the idea, preliminary results are provided using a non-uniform\nquantization algorithm. We believe that successful implementation of\nnon-uniform quantization in detail components in JPEG-2000 still image standard\nwill improve image quality and compression efficiency with lesser number of\nquantized values.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.8165v1"
    },
    {
        "title": "Collaborative P2P Streaming of Interactive Live Free Viewpoint Video",
        "authors": [
            "Dongni Ren",
            "S. -H. Gary Chan",
            "Gene Cheung",
            "Vicky Zhao",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  We study an interactive live streaming scenario where multiple peers pull\nstreams of the same free viewpoint video that are synchronized in time but not\nnecessarily in view. In free viewpoint video, each user can periodically select\na virtual view between two anchor camera views for display. The virtual view is\nsynthesized using texture and depth videos of the anchor views via\ndepth-image-based rendering (DIBR). In general, the distortion of the virtual\nview increases with the distance to the anchor views, and hence it is\nbeneficial for a peer to select the closest anchor views for synthesis. On the\nother hand, if peers interested in different virtual views are willing to\ntolerate larger distortion in using more distant anchor views, they can\ncollectively share the access cost of common anchor views.\n  Given anchor view access cost and synthesized distortion of virtual views\nbetween anchor views, we study the optimization of anchor view allocation for\ncollaborative peers. We first show that, if the network reconfiguration costs\ndue to view-switching are negligible, the problem can be optimally solved in\npolynomial time using dynamic programming. We then consider the case of\nnon-negligible reconfiguration costs (e.g., large or frequent view-switching\nleading to anchor-view changes). In this case, the view allocation problem\nbecomes NP-hard. We thus present a locally optimal and centralized allocation\nalgorithm inspired by Lloyd's algorithm in non-uniform scalar quantization. We\nalso propose a distributed algorithm with guaranteed convergence where each\npeer group independently make merge-and-split decisions with a well-defined\nfairness criteria. The results show that depending on the problem settings, our\nproposed algorithms achieve respective optimal and close-to-optimal performance\nin terms of total cost, and outperform a P2P scheme without collaborative\nanchor selection.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.4767v1"
    },
    {
        "title": "A Modified LSB Technique of Digital Watermarking in Spatial Domain",
        "authors": [
            "Nisha Sharma",
            "Kamlesh Sharma"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Digital watermarking is a technique of embedding pieces of information into\ndigital data such as text, audio, video, and still images that can be detected\nor extracted later to show authentication about the data. Watermark is hidden\ninformation in the image(s) and is so designed that it does not degrade/distort\nthe quality of the image and still keeps the information. Digital watermarking\nis basically to protect ownership rights and to control of making illicit\ncopies of digital data. In this paper, we have discussed various watermarking\ntechniques and properties and have proposed a modified LSB technique. We have\nimplemented the proposed technique by following: 2-bits of 8-bit gray image is\nreplaced by luminance part, next 2-bits by red component, next 2-bits by green\ncomponent and next 2-bits by blue component of 32-bit image using secret key.\nThe advantage is that watermarking capacity has been increased and unaffected\nby various attacks e.g. zero out LSB bits, cropping etc. Watermark image is\nimperceptible in resultant image. We have tested this technique on several\nimages and found that it is quite satisfactory. This technique is secured as\nunauthorized user can not extract the watermarked contents easily from the\noriginal image and works well in adverse situations. We have implemented this\ntechnique on platform java 1.5.0.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.7353v1"
    },
    {
        "title": "A local fingerprinting approach for audio copy detection",
        "authors": [
            "Mani Malekesmaeili",
            "Rabab K. Ward"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  This study proposes an audio copy detection system that is robust to various\nattacks. These include the severe pitch shift and tempo change attacks which\nexisting systems fail to detect. First, we propose a novel two dimensional\nrepresentation for audio signals called the time-chroma image. This image is\nbased on a modification of the concept of chroma in the music literature and is\nshown to achieve better performance in song identification. Then, we propose a\nnovel fingerprinting algorithm that extracts local fingerprints from the\ntime-chroma image. The proposed local fingerprinting algorithm is invariant to\ntime/frequency scale changes in audio signals. It also outperforms existing\nmethods like SIFT by a great extent. Finally, we introduce a song\nidentification algorithm that uses the proposed fingerprints. The resulting\ncopy detection system is shown to significantly outperform existing methods.\nBesides being able to detect whether a song (or a part of it) has been copied,\nthe proposed system can accurately estimate the amount of pitch shift and/or\ntempo change that might have been applied to a song.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.0793v1"
    },
    {
        "title": "Genetic Soundtracks: Creative Matching of Audio to Video",
        "authors": [
            "Jorge Gomes",
            "Fernando Silva",
            "Teresa Chambel"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  The matching of the soundtrack in a movie or a video can have an enormous\ninfluence in the message being conveyed and its impact, in the sense of\ninvolvement and engagement, and ultimately in their aesthetic and entertainment\nqualities. Art is often associated with creativity, implying the presence of\ninspiration, originality and appropriateness. Evolutionary systems provides us\nwith the novelty, showing us new and subtly different solutions in every\ngeneration, possibly stimulating the creativity of the human using the system.\nIn this paper, we present Genetic Soundtracks, an evolutionary approach to the\ncreative matching of audio to a video. It analyzes both media to extract\nfeatures based on their content, and adopts genetic algorithms, with the\npurpose of truncating, combining and adjusting audio clips, to align and match\nthem with the video scenes.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.2671v1"
    },
    {
        "title": "A Secure And High Capacity Image Steganography Technique",
        "authors": [
            "Hemalatha S",
            "U Dinesh Acharya",
            "Renuka A",
            "Priya R. Kamath"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Steganography is the science of invisible communication. The purpose of\nSteganography is to maintain secret communication between two parties. The\nsecret information can be concealed in content such as image, audio, or video.\nThis paper provides a novel image steganography technique to hide multiple\nsecret images and keys in color cover image using Integer Wavelet Transform\n(IWT). There is no visual difference between the stego image and the cover\nimage. The extracted secret images are also similar to the original secret\nimages. Very good PSNR (Peak Signal to Noise Ratio) values are obtained for\nboth stego and extracted secret images. The results are compared with the\nresults of other techniques, where single image is hidden and it is found that\nthe proposed technique is simple and gives better PSNR values than others.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3629v1"
    },
    {
        "title": "Joint On-the-Fly Network Coding/Video Quality Adaptation for Real-Time\n  Delivery",
        "authors": [
            "Tuan Tran Thai",
            "Jérôme Lacan",
            "Emmanuel Lochin"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  This paper introduces a redundancy adaptation algorithm for an on-the-fly\nerasure network coding scheme called Tetrys in the context of real-time video\ntransmission. The algorithm exploits the relationship between the redundancy\nratio used by Tetrys and the gain or loss in encoding bit rate from changing a\nvideo quality parameter called the Quantization Parameter (QP). Our evaluations\nshow that with equal or less bandwidth occupation, the video protected by\nTetrys with redundancy adaptation algorithm obtains a PSNR gain up to or more 4\ndB compared to the video without Tetrys protection. We demonstrate that the\nTetrys redundancy adaptation algorithm performs well with the variations of\nboth loss pattern and delay induced by the networks. We also show that Tetrys\nwith the redundancy adaptation algorithm outperforms FEC with and without\nredundancy adaptation.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.5068v1"
    },
    {
        "title": "Adaptive Software Radio Steganography",
        "authors": [
            "David E. Robillard"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  This paper presents an adaptable steganography (information hiding) method\nfor digital radio communication. Many radio steganography methods exist, but\nmost are defined at higher levels of the protocol stack and are thus protocol\ndependent. In contrast, this method is defined at the physical layer, which\nmakes it widely applicable regardless of the protocols used at higher layers.\nThis approach is also adaptive; the covertness of the hidden channel is simple\nto control via a single continuous parameter either manually or automatically.\nSeveral variations are introduced, each with performance evaluated by\nsimulation. Results show this to be a feasible method with a reasonable\ntrade-off between performance and covertness.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7324v1"
    },
    {
        "title": "Model and Performance of a No-Reference Quality Assessment Metric for\n  Video Streaming",
        "authors": [
            "Mirghiasaldin Seyedebrahimi",
            "Colin Bailey",
            "Xiao-Hong Peng"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Video streaming via TCP networks has become a popular and highly demanded\nservice, but its quality assessment in both objective and subjective terms has\nnot been properly addressed. In this paper, based on statistical analysis a\nfull analytic model of a no-reference objective metric, namely Pause Intensity,\nfor video quality assessment is presented. The model characterizes the video\nplayout buffer behavior in connection with the network performance (throughput)\nand the video playout rate. This allows for instant quality measurement and\ncontrol without requiring a reference video. Pause intensity specifically\naddresses the need for assessing the quality issue in terms of the continuity\nin the playout of TCP streaming videos, which cannot be properly measured by\nother objective metrics such as PSNR, SSIM and buffer underrun or pause\nfrequency. The performance of the analytical model is rigidly verified by\nsimulation results and subjective tests using a range of video clips. It is\ndemonstrated that pause intensity is closely correlated with viewer opinion\nscores regardless of the vastly different composition of individual elements,\nsuch as pause duration and pause frequency which jointly constitute this new\nquality metric. It is also shown that the correlation performance of pause\nintensity is consistent and content independent.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.1839v1"
    },
    {
        "title": "Compressive Sampling for the Packet Loss Recovery in Audio Multimedia\n  Streaming",
        "authors": [
            "Angelo Ciaramella",
            "Giulio Giunta"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  The aim of this paper is to introduce a new schema, based on a Compressive\nSampling technique, for the recovery of lost data in multimedia streaming. The\naudio streaming data are encapsuled in different packets by using an\ninterleaving technique. The Compressive Sampling technique is used to recover\naudio information in case of lost packets. Experimental results are presented\non speech and musical audio signals to illustrate the performances and the\ncapabilities of the proposed methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.4263v1"
    },
    {
        "title": "Coded Acquisition of High Frame Rate Video",
        "authors": [
            "Reza Pournaghi",
            "Xiaolin Wu"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  High frame video (HFV) is an important investigational tool in sciences,\nengineering and military. In ultra-high speed imaging, the obtainable temporal,\nspatial and spectral resolutions are limited by the sustainable throughput of\nin-camera mass memory, the lower bound of exposure time, and illumination\nconditions. In order to break these bottlenecks, we propose a new coded video\nacquisition framework that employs K > 2 conventional cameras, each of which\nmakes random measurements of the 3D video signal in both temporal and spatial\ndomains. For each of the K cameras, this multi-camera strategy greatly relaxes\nthe stringent requirements in memory speed, shutter speed, and illumination\nstrength. The recovery of HFV from these random measurements is posed and\nsolved as a large scale l1 minimization problem by exploiting joint temporal\nand spatial sparsities of the 3D signal. Three coded video acquisition\ntechniques of varied trade offs between performance and hardware complexity are\ndeveloped: frame-wise coded acquisition, pixel-wise coded acquisition, and\ncolumn-row-wise coded acquisition. The performances of these techniques are\nanalyzed in relation to the sparsity of the underlying video signal.\nSimulations of these new HFV capture techniques are carried out and\nexperimental results are reported.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.4458v1"
    },
    {
        "title": "Blind and robust images watermarking based on wavelet and edge insertion",
        "authors": [
            "Henri Bruno Razafindradina",
            "Attoumani Mohamed Karim"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  This paper gives a new scheme of watermarking technique related to insert the\nmark by adding edge in HH sub-band of the host image after wavelet\ndecomposition. Contrary to most of the watermarking algorithms in wavelet\ndomain, our method is blind and results show that it is robust against the JPEG\nand GIF compression, histogram and spectrum spreading, noise adding and small\nrotation. Its robustness against compression is better than others watermarking\nalgorithms reported in the literature. The algorithm is flexible because its\ncapacity or robustness can be improved by modifying some parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.5653v1"
    },
    {
        "title": "Multiview Navigation based on Extended Layered Depth Image\n  Representation",
        "authors": [
            "Uday Takyar",
            "Thomas Maugey",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Emerging applications in multiview streaming look for providing interactive\nnavigation services to video players. The user can ask for information from any\nviewpoint with a minimum transmission delay. The purpose is to provide user\nwith as much information as possible with least number of redundancies. The\nrecent concept of navigation segment representation consists of regrouping a\ngiven number of viewpoints in one signal and transmitting them to the users\naccording to their navigation path. The question of the best description\nstrategy of these navigation segments is however still open. In this paper, we\npropose to represent and code navigation segments by a method that extends the\nrecent layered depth image (LDI) format. It consists of describing the scene\nfrom a viewpoint with multiple images organized in layers corresponding to the\ndifferent levels of occluded objects. The notion of extended LDI comes from the\nfact that the size of this image is adapted to take into account the sides of\nthe scene also, in contrary to classical LDI. The obtained results show a\nsignificant rate-distortion gain compared to classical multiview compression\napproaches in navigation scenario.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.6377v1"
    },
    {
        "title": "Increasing Compression Ratio of Low Complexity Compressive Sensing Video\n  Encoder with Application-Aware Configurable Mechanism",
        "authors": [
            "Shuang Yu",
            "Fei Qiao",
            "Li Luo",
            "Huazhong Yang"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  With the development of embedded video acquisition nodes and wireless video\nsurveillance systems, traditional video coding methods could not meet the needs\nof less computing complexity any more, as well as the urgent power consumption.\nSo, a low-complexity compressive sensing video encoder framework with\napplication-aware configurable mechanism is proposed in this paper, where novel\nencoding methods are exploited based on the practical purposes of the real\napplications to reduce the coding complexity effectively and improve the\ncompression ratio (CR). Moreover, the group of processing (GOP) size and the\nmeasurement matrix size can be configured on the encoder side according to the\npost-analysis requirements of an application example of object tracking to\nincrease the CR of encoder as best as possible. Simulations show the proposed\nframework of encoder could achieve 60X of CR when the tracking successful rate\n(SR) is still keeping above 90%.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.1419v1"
    },
    {
        "title": "Image Steganography using Karhunen-Loeve Transform and Least Bit\n  Substitution",
        "authors": [
            "Ankit Chadha",
            "Neha Satam",
            "Rakshak Sood",
            "Dattatray Bade"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  As communication channels are increasing in number, reliability of faithful\ncommunication is reducing. Hacking and tempering of data are two major issues\nfor which security should be provided by channel. This raises the importance of\nsteganography. In this paper, a novel method to encode the message information\ninside a carrier image has been described. It uses Karhunen-Lo\\`eve Transform\nfor compression of data and Least Bit Substitution for data encryption.\nCompression removes redundancy and thus also provides encoding to a level. It\nis taken further by means of Least Bit Substitution. The algorithm used for\nthis purpose uses pixel matrix which serves as a best tool to work on. Three\ndifferent sets of images were used with three different numbers of bits to be\nsubstituted by message information. The experimental results show that\nalgorithm is time efficient and provides high data capacity. Further, it can\ndecrypt the original data effectively. Parameters such as carrier error and\nmessage error were calculated for each set and were compared for performance\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.1700v1"
    },
    {
        "title": "Optimal Foresighted Multi-User Wireless Video",
        "authors": [
            "Yuanzhang Xiao",
            "Mihaela van der Schaar"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Recent years have seen an explosion in wireless video communication systems.\nOptimization in such systems is crucial - but most existing methods intended to\noptimize the performance of multi-user wireless video transmission are\ninefficient. Some works (e.g. Network Utility Maximization (NUM)) are myopic:\nthey choose actions to maximize instantaneous video quality while ignoring the\nfuture impact of these actions. Such myopic solutions are known to be inferior\nto foresighted solutions that optimize the long-term video quality.\nAlternatively, foresighted solutions such as rate-distortion optimized packet\nscheduling focus on single-user wireless video transmission, while ignoring the\nresource allocation among the users.\n  In this paper, we propose an optimal solution for performing joint\nforesighted resource allocation and packet scheduling among multiple users\ntransmitting video over a shared wireless network. A key challenge in\ndeveloping foresighted solutions for multiple video users is that the users'\ndecisions are coupled. To decouple the users' decisions, we adopt a novel dual\ndecomposition approach, which differs from the conventional optimization\nsolutions such as NUM, and determines foresighted policies. Specifically, we\npropose an informationally-decentralized algorithm in which the network manager\nupdates resource \"prices\" (i.e. the dual variables associated with the resource\nconstraints), and the users make individual video packet scheduling decisions\nbased on these prices. Because a priori knowledge of the system dynamics is\nalmost never available at run-time, the proposed solution can learn online,\nconcurrently with performing the foresighted optimization. Simulation results\nshow 7 dB and 3 dB improvements in Peak Signal-to-Noise Ratio (PSNR) over\nmyopic solutions and existing foresighted solutions, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.4227v1"
    },
    {
        "title": "Traffic and Statistical Multiplexing Characterization of 3D Video\n  Representation Formats (Extended Version)",
        "authors": [
            "Akshay Pulipaka",
            "Patrick Seeling",
            "Martin Reisslein",
            "Lina J. Karam"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  The network transport of 3D video, which contains two views of a video scene,\nposes significant challenges due to the increased video data compared to\nconventional single-view video. Addressing these challenges requires a thorough\nunderstanding of the traffic and multiplexing characteristics of the different\nrepresentation formats of 3D video. We examine the average bitrate-distortion\n(RD) and bitrate variability-distortion (VD) characteristics of three main\nrepresentation formats. Specifically, we compare multiview video (MV)\nrepresentation and encoding, frame sequential (FS) representation, and\nside-by-side (SBS) representation, whereby conventional single-view encoding is\nemployed for the FS and SBS representations. Our results for long 3D videos in\nfull HD format indicate that the MV representation and encoding achieves the\nhighest RD efficiency, while exhibiting the highest bitrate variabilities. We\nexamine the impact of these bitrate variabilities on network transport through\nextensive statistical multiplexing simulations. We find that when multiplexing\na small number of streams, the MV and FS representations require the same\nbandwidth. However, when multiplexing a large number of streams or smoothing\ntraffic, the MV representation and encoding reduces the bandwidth requirement\nrelative to the FS representation.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.5834v1"
    },
    {
        "title": "Modeling the Time-varying Subjective Quality of HTTP Video Streams with\n  Rate Adaptations",
        "authors": [
            "Chao Chen",
            "Lark Kwon Choi",
            "Gustavo de Veciana",
            "Constantine Caramanis",
            "Robert W. Heath Jr.",
            "Alan C. Bovik"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Newly developed HTTP-based video streaming technologies enable flexible\nrate-adaptation under varying channel conditions. Accurately predicting the\nusers' Quality of Experience (QoE) for rate-adaptive HTTP video streams is thus\ncritical to achieve efficiency. An important aspect of understanding and\nmodeling QoE is predicting the up-to-the-moment subjective quality of a video\nas it is played, which is difficult due to hysteresis effects and\nnonlinearities in human behavioral responses. This paper presents a\nHammerstein-Wiener model for predicting the time-varying subjective quality\n(TVSQ) of rate-adaptive videos. To collect data for model parameterization and\nvalidation, a database of longer-duration videos with time-varying distortions\nwas built and the TVSQs of the videos were measured in a large-scale subjective\nstudy. The proposed method is able to reliably predict the TVSQ of rate\nadaptive videos. Since the Hammerstein-Wiener model has a very simple\nstructure, the proposed method is suitable for on-line TVSQ prediction in HTTP\nbased streaming.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.6441v1"
    },
    {
        "title": "Rate Adaptation and Admission Control for Video Transmission with\n  Subjective Quality Constraints",
        "authors": [
            "Chao Chen",
            "Xiaoqing Zhu",
            "Gustavo de Veciana",
            "Alan C. Bovik",
            "Robert W. Heath Jr"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Adapting video data rate during streaming can effectively reduce the risk of\nplayback interruptions caused by channel throughput fluctuations. The\nvariations in rate, however, also introduce video quality fluctuations and thus\npotentially affects viewers' Quality of Experience (QoE). We show how the QoE\nof video users can be improved by rate adaptation and admission control. We\nconducted a subjective study wherein we found that viewers' QoE was strongly\ncorrelated with the empirical cumulative distribution function (eCDF) of the\npredicted video quality. Based on this observation, we propose a\nrate-adaptation algorithm that can incorporate QoE constraints on the empirical\ncumulative quality distribution per user. We then propose a threshold-based\nadmission control policy to block users whose empirical cumulative quality\ndistribution is not likely to satisfy their QoE constraint. We further devise\nan online adaptation algorithm to automatically optimize the threshold.\nExtensive simulation results show that the proposed scheme can reduce network\nresource consumption by $40\\%$ over conventional average-quality maximized\nrate-adaptation algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.6453v1"
    },
    {
        "title": "Graph-based representation for multiview image coding",
        "authors": [
            "Thomas Maugey",
            "Antonio Ortega",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  In this paper, we propose a new representation for multiview image sets. Our\napproach relies on graphs to describe geometry information in a compact and\ncontrollable way. The links of the graph connect pixels in different images and\ndescribe the proximity between pixels in the 3D space. These connections are\ndependent on the geometry of the scene and provide the right amount of\ninformation that is necessary for coding and reconstructing multiple views.\nThis multiview image representation is very compact and adapts the transmitted\ngeometry information as a function of the complexity of the prediction\nperformed at the decoder side. To achieve this, our GBR adapts the accuracy of\nthe geometry representation, in contrast with depth coding, which directly\ncompresses with losses the original geometry signal. We present the principles\nof this graph-based representation (GBR) and we build a complete prototype\ncoding scheme for multiview images. Experimental results demonstrate the\npotential of this new representation as compared to a depth-based approach. GBR\ncan achieve a gain of 2 dB in reconstructed quality over depth-based schemes\noperating at similar rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.6090v1"
    },
    {
        "title": "State-of-the Art Motion Estimation in the Context of 3D TV",
        "authors": [
            "Vania V. Estrela",
            "Alessandra M. Coelho"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Progress in image sensors and computation power has fueled studies to improve\nacquisition, processing, and analysis of 3D streams along with 3D\nscenes/objects reconstruction. The role of motion compensation/motion\nestimation (MCME) in 3D TV from end-to-end user is investigated in this\nchapter. Motion vectors (MVs) are closely related to the concept of\ndisparities, and they can help improving dynamic scene acquisition, content\ncreation, 2D to 3D conversion, compression coding, decompression/decoding,\nscene rendering, error concealment, virtual/augmented reality handling,\nintelligent content retrieval, and displaying. Although there are different 3D\nshape extraction methods, this chapter focuses mostly on shape-from-motion\n(SfM) techniques due to their relevance to 3D TV. SfM extraction can restore 3D\nshape information from a single camera data.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.6497v1"
    },
    {
        "title": "A Study on the Optimal Implementation of Statistical Multiplexing in DVB\n  Distribution Systems",
        "authors": [
            "Alexandru Florin Antone",
            "Radu Arsinte"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  The paper presents an overview of the main methods used to improve the\nefficiency of DVB systems, based on multiplexing, through a study on the impact\nof the multiplexing methods used in DVB, having as a final goal a better usage\nof the data capacity and the possibility to insert new services into the\noriginal DVB Transport Stream. This study revealed that not all DVB providers\nare using statistical multiplexing. Based on this study, we were able to\npropose a method to improve the original DVB stream, originated from DVB-S or\nDVB-T providers. This method is proposing the detection of null packets,\nremoval and reinserting a new service, with a VBR content. The method developed\nin this research can be implemented even in optimized statistical multiplexing\nsystems, due to a residual use of null packets for data rate adjustment. There\nis no need to have access in the original stream multiplexer, since the method\nallows the implementation on the fly, near to the end user. The proposed method\nis proposed to be applied in DVB-S to DVB-C translation, using the computing\npower of a PC or in a FPGA implementation.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.0812v1"
    },
    {
        "title": "Twofold Video Hashing with Automatic Synchronization",
        "authors": [
            "Mu Li",
            "Vishal Monga"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Video hashing finds a wide array of applications in content authentication,\nrobust retrieval and anti-piracy search. While much of the existing research\nhas focused on extracting robust and secure content descriptors, a significant\nopen challenge still remains: Most existing video hashing methods are fallible\nto temporal desynchronization. That is, when the query video results by\ndeleting or inserting some frames from the reference video, most existing\nmethods assume the positions of the deleted (or inserted) frames are either\nperfectly known or reliably estimated. This assumption may be okay under\ntypical transcoding and frame-rate changes but is highly inappropriate in\nadversarial scenarios such as anti-piracy video search. For example, an illegal\nuploader will try to bypass the 'piracy check' mechanism of YouTube/Dailymotion\netc by performing a cleverly designed non-uniform resampling of the video. We\npresent a new solution based on dynamic time warping (DTW), which can implement\nautomatic synchronization and can be used together with existing video hashing\nmethods. The second contribution of this paper is to propose a new robust\nfeature extraction method called flow hashing (FH), based on frame averaging\nand optical flow descriptors. Finally, a fusion mechanism called distance\nboosting is proposed to combine the information extracted by DTW and FH.\nExperiments on real video collections show that such a hash extraction and\ncomparison enables unprecedented robustness under both spatial and temporal\nattacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.5422v1"
    },
    {
        "title": "A Methodology for Implementation of MMS Client on Embedded Platforms",
        "authors": [
            "A. A. Milani",
            "Reza Rahimi"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  MMS (Multimedia Messaging Service) is the next generation of messaging\nservices in multimedia mobile communications. MMS enables messaging with full\nmultimedia content including images, audios, videos, texts and data, from\nclient to client or e-mail. MMS is based on WAP technology, so it is technology\nindependent. This means that enabling messages from a GSM/GPRS network to be\nsent to a TDMA or WCDMA network. In this paper a methodology for implementing\nMMS client on embedded platforms especially on Wince OS is described.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.4158v1"
    },
    {
        "title": "Perceptual Quality of Video with Periodic Frame Rate and Quantization\n  Variation-Subjective Studies and Analytical Modeling",
        "authors": [
            "Yen-Fu Ou",
            "Wenzhi Lin",
            "Huiqi Zeng",
            "Yao Wang"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  In networked video applications, the frame rate (FR) and quantization\nstepsize (QS) of a compressed video are often adapted in response to the\nchanges of the available bandwidth. It is important to understand how do the\nvariation of FR and QS and their variation pattern affect the video quality. In\nthis paper, we investigate the impact of temporal variation of FR and QS on the\nperceptual video quality. Among all possible variation patterns, we focus on\nvideos in which two FR's (or QS's) alternate over a fixed interval. We explore\nthe human responses to such variation by conducting subjective evaluation of\ntest videos with different variation magnitudes and frequencies. We further\nanalyze statistical significance of the impact of variation magnitude,\nvariation frequency, video content, and their interactions. By analyzing the\nsubjective ratings, we propose two models for predicting the quality of video\nwith alternating FR and QS, respectively, The proposed models have simple\nmathematical forms with a few content-dependent parameters. The models fit the\nmeasured data very well using parameters determined by least square fitting\nwith the measured data. We further propose some guidelines for adaptation of FR\nand QS based on trends observed from subjective test results.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.2018v1"
    },
    {
        "title": "real-time audio translation module between iax and rsw",
        "authors": [
            "Hadeel Saleh Haj Aliwi",
            "Putra Sumari"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  At the last few years, multimedia communication has been developed and\nimproved rapidly in order to enable users to communicate between each other\nover the internet. Generally, multimedia communication consists of audio and\nvideo communication. However, this research concentrates on audio conferencing\nonly. The audio translation between protocols is a very critical issue, because\nit solves the communication problems between any two protocols. So, it enables\npeople around the world to talk with each other even they use different\nprotocols. In this research, a real time audio translation module between two\nprotocols has been done. These two protocols are: InterAsterisk eXchange\nProtocol (IAX) and Real-Time Switching Control Protocol (RSW), which they are\nwidely used to provide two ways audio transfer feature. The solution here is to\nprovide inter-working between the two protocols which they have different media\ntransports, audio codecs, header formats and different transport protocols for\nthe audio transmission. This translation will help bridging the gap between the\ntwo protocols by providing inter-working capability between the two audio\nstreams of IAX and RSW. Some related works have been done to provide\ntranslation between IAX and RSW control signalling messages. But, this research\npaper concentrates on the translation that depends on the media transfer. The\nproposed translation module was tested and evaluated in different scenarios in\norder to examine its performance. The obtained results showed that the\nReal-Time Audio Translation Module produces lower rates of packet delay and\njitter than the acceptance values for each of the mentioned performance\nmetrics.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.2779v1"
    },
    {
        "title": "Optimized Adaptive Streaming Representations based on System Dynamics",
        "authors": [
            "Laura Toni",
            "Ramon Aparicio-Pardo",
            "Karine Pires",
            "Gwendal Simon",
            "Alberto Blanc",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Adaptive streaming addresses the increasing and heterogenous demand of\nmultimedia content over the Internet by offering several encoded versions for\neach video sequence. Each version (or representation) has a different\nresolution and bit rate, aimed at a specific set of users, like TV or mobile\nphone clients. While most existing works on adaptive streaming deal with\neffective playout-control strategies at the client side, we take in this paper\na providers' perspective and propose solutions to improve user satisfaction by\noptimizing the encoding rates of the video sequences. We formulate an integer\nlinear program that maximizes users' average satisfaction, taking into account\nthe network dynamics, the video content information, and the user population\ncharacteristics. The solution of the optimization is a set of encoding\nparameters that permit to create different streams to robustly satisfy users'\nrequests over time. We simulate multiple adaptive streaming sessions\ncharacterized by realistic network connections models, where the proposed\nsolution outperforms commonly used vendor recommendations, in terms of user\nsatisfaction but also in terms of fairness and outage probability. The\nsimulation results further show that video content information as well as\nnetwork constraints and users' statistics play a crucial role in selecting\nproper encoding parameters to provide fairness a mong users and to reduce\nnetwork resource usage. We finally propose a few practical guidelines that can\nbe used to choose the encoding parameters based on the user base\ncharacteristics, the network capacity and the type of video content.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.3161v2"
    },
    {
        "title": "Securing Medical Images by Watermarking Using DWT DCT and SVD",
        "authors": [
            "Nilesh Rathi",
            "Ganga Holi"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Telemedicine is well known application where enormous amount of medical data\nneed to be transferred securely over network and manipulate effectively.\nSecurity of digital data, especially medical images, becomes important for many\nreasons such as confidentiality, authentication and integrity. Digital\nwatermarking has emerged as a advanced technology to enhance the security of\ndigital images. The insertion of watermark in medical images can authenticate\nit and guarantee its integrity. The watermark must be generally hidden does not\naffect the quality of the medical image. In this paper, we propose blind\nwatermarking based on Discrete Wavelet Transform (DWT), Discrete Cosine\nTransform (DCT) and Singular Value Decomposition (SVD), we compare the\nperformance of this technique with watermarking based DWT and SVD. The proposed\nmethod DWT, DCT and SVD comparatively better than DWT and SVD method.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.7226v1"
    },
    {
        "title": "ITEM: Immersive Telepresence for Entertainment and Meetings - A\n  Practical Approach",
        "authors": [
            "Viet-Anh Nguyen",
            "Jiangbo Lu",
            "Shengkui Zhao",
            "Tien Dung Vu",
            "Hongsheng Yang",
            "Jones L. Douglas",
            "Minh N. Do"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  This paper presents an Immersive Telepresence system for Entertainment and\nMeetings (ITEM). The system aims to provide a radically new video communication\nexperience by seamlessly merging participants into the same virtual space to\nallow a natural interaction among them and shared collaborative contents. With\nthe goal to make a scalable, flexible system for various business solutions as\nwell as easily accessible by massive consumers, we address the challenges in\nthe whole pipeline of media processing, communication, and displaying in our\ndesign and realization of such a system. Particularly, in this paper we focus\non the system aspects that maximize the end-user experience, optimize the\nsystem and network resources, and enable various teleimmersive application\nscenarios. In addition, we also present a few key technologies, i.e. fast\nobject-based video coding for real world data and spatialized audio capture and\n3D sound localization for group teleconferencing. Our effort is to investigate\nand optimize the key system components and provide an efficient end-to-end\noptimization and integration by considering user needs and preferences.\nExtensive experiments show the developed system runs reliably and comfortably\nin real time with a minimal setup requirement (e.g. a webcam and/or a depth\ncamera, an optional microphone array, a laptop/desktop connected to the public\nInternet) for teleimmersive communication. With such a really minimal\ndeployment requirement, we present a variety of interesting applications and\nuser experiences created by ITEM.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.0605v1"
    },
    {
        "title": "Digital Image Data Hiding Techniques: A Comparative Study",
        "authors": [
            "Minati Mishra",
            "Priyadarsini Mishra",
            "M. C. Adhikary"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  With the advancements in the field of digital image processing during the\nlast decade, digital image data hiding techniques such as watermarking,\nSteganography have gained wide popularity. Digital image watermarking\ntechniques hide a small amount of data into a digital image which, later can be\nretrieved using some specific retrieval algorithms to prove the copyright of a\npiece of digital information whereas, Steganographic techniques are used to\nhide a large amount of data secretly into some innocuous looking digital\nmedium. In this paper we are providing an up-to-date review of these data\nhiding techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.3564v1"
    },
    {
        "title": "A Novel No-reference Video Quality Metric for Evaluating Temporal\n  Jerkiness due to Frame Freezing",
        "authors": [
            "Yuanyi Xue",
            "Beril Erkin",
            "Yao Wang"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  In this work, we propose a novel no-reference (NR) video quality metric that\nevaluates the impact of frame freezing due to either packet loss or late\narrival. Our metric uses a trained neural network acting on features that are\nchosen to capture the impact of frame freezing on the perceived quality. The\nconsidered features include the number of freezes, freeze duration statistics,\ninter-freeze distance statistics, frame difference before and after the freeze,\nnormal frame difference, and the ratio of them. We use the neural network to\nfind the mapping between features and subjective test scores. We optimize the\nnetwork structure and the feature selection through a cross validation\nprocedure, using training samples extracted from both VQEG and LIVE video\ndatabases. The resulting feature set and network structure yields accurate\nquality prediction for both the training data containing 54 test videos and a\nseparate testing dataset including 14 videos, with Pearson Correlation\nCoefficients greater than 0.9 and 0.8 for the training set and the testing set,\nrespectively. Our proposed metric has low complexity and could be utilized in a\nsystem with realtime processing constraint.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1705v1"
    },
    {
        "title": "Maximizing compression efficiency through block rotation",
        "authors": [
            "Rui F. C. Guerreiro",
            "Pedro M. Q. Aguiar"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  The Discrete Cosine Transform (DCT) is widely used in lossy image and video\ncompression schemes, e.g., JPEG and MPEG. In this paper, we show that the\ncompression efficiency of the DCT is dependent on the edge directions within a\nblock. In particular, higher compression ratios are achieved when edges are\naligned with the image axes. To maximize compression for general images, we\npropose a rotated block DCT method. It consists of rotating each block, before\napplying the DCT, by an angle that aligns the edges, and rotating back the\nblock in the decompression stage. We show how to compute the rotation angle and\nanalyze two alternative block rotation approaches. Our experiments show that\nour method enables both a perceptual improvement and a PSNR increase of up to\n2dB, compared with the standard DCT, for low and medium bit rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.4290v1"
    },
    {
        "title": "A Tag Identification Approach Based On Fragile Watermark",
        "authors": [
            "Jianbiao Lin",
            "Ke Ji",
            "Hui Lin",
            "Enyan Wu",
            "Xin Gao"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  This paper proposes a tag identify approach based on fragile Watermark that\nbased on Least significant bit of the replacement that we first use a special\nway to initialize the cover to ensure that we can use random positions to embed\nthe information of tag. Using this way enhance the security of other to get the\nright information of this tag. Finally as long as the covered information can\nbe decoded, the completeness and accuracy of the tag information can be\nguaranteed. the result of simulation experiment show that this approach has\nhigh sensitivity and security .\n",
        "pdf_link": "http://arxiv.org/pdf/1411.6928v1"
    },
    {
        "title": "Optimized Packet Scheduling in Multiview Video Navigation Systems",
        "authors": [
            "Laura Toni",
            "Thomas Maugey",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  In multiview video systems, multiple cameras generally acquire the same scene\nfrom different perspectives, such that users have the possibility to select\ntheir preferred viewpoint. This results in large amounts of highly redundant\ndata, which needs to be properly handled during encoding and transmission over\nresource-constrained channels. In this work, we study coding and transmission\nstrategies in multicamera systems, where correlated sources send data through a\nbottleneck channel to a central server, which eventually transmits views to\ndifferent interactive users. We propose a dynamic correlation-aware packet\nscheduling optimization under delay, bandwidth, and interactivity constraints.\nThe optimization relies both on a novel rate-distortion model, which captures\nthe importance of each view in the 3D scene reconstruction, and on an objective\nfunction that optimizes resources based on a client navigation model. The\nlatter takes into account the distortion experienced by interactive clients as\nwell as the distortion variations that might be observed by clients during\nmultiview navigation. We solve the scheduling problem with a novel\ntrellis-based solution, which permits to formally decompose the multivariate\noptimization problem thereby significantly reducing the computation complexity.\nSimulation results show the gain of the proposed algorithm compared to baseline\nscheduling policies. More in details, we show the gain offered by our dynamic\nscheduling policy compared to static camera allocation strategies and to\nschemes with constant coding strategies. Finally, we show that the best\nscheduling policy consistently adapts to the most likely user navigation path\nand that it minimizes distortion variations that can be very disturbing for\nusers in traditional navigation systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.0954v1"
    },
    {
        "title": "Multi-Hypothesis Compressed Video Sensing Technique",
        "authors": [
            "Masoumeh Azghani",
            "Mostafa Karimi",
            "Farokh Marvasti"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  In this paper, we present a compressive sampling and Multi-Hypothesis (MH)\nreconstruction strategy for video sequences which has a rather simple encoder,\nwhile the decoding system is not that complex. We introduce a convex cost\nfunction that incorporates the MH technique with the sparsity constraint and\nthe Tikhonov regularization. Consequently, we derive a new iterative algorithm\nbased on these criteria. This algorithm surpasses its counterparts (Elasticnet\nand Tikhonov) in the recovery performance. Besides it is computationally much\nfaster than the Elasticnet and comparable to the Tikhonov. Our extensive\nsimulation results confirm these claims.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.4576v1"
    },
    {
        "title": "Mobile Instant Video Clip Sharing: Modeling and Enhancing View\n  Experience",
        "authors": [
            "Lei Zhang",
            "Feng Wang",
            "Jiangchuan Liu"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  With the rapid development of wireless networking and mobile devices, anytime\nand anywhere data access becomes readily available nowadays. Given the\ncrowdsourced content capturing and sharing, the preferred content length\nbecomes shorter and shorter, even for such multimedia data as video. A\nrepresentative is Twitter's Vine service, which, mainly targeting mobile users,\nenables them to create ultra-short video clips and instantly post and share\nwith their followers. In this paper, we present an initial study on this new\ngeneration of instant video clip sharing service enabled by mobile platforms\nand explore the potentials towards its further enhancement. We closely\ninvestigate its unique mobile interface, revealing the key differences between\nVine-enabled anytime anywhere data access patterns and that of traditional\ncounterparts. We then examine the scheduling policy to maximize the user\nwatching experience as well as the efficiency on the monetary and energy costs.\nWe show that the generic scheduling problem involves two subproblems, namely,\npre-fetching scheduling and watch-time download scheduling, and develop\neffective solutions towards both of them. The superiority of our solution is\ndemonstrated by extensive trace-driven simulations. To the best of our\nknowledge, this is the first work on modeling and optimizing the instant video\nclip sharing on mobile devices.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.7595v3"
    },
    {
        "title": "Wavelet based Watermarking approach in the Compressive Sensing Scenario",
        "authors": [
            "Jelena Music",
            "Ivan Knezevic",
            "Edis Franca"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Due to the wide distribution and usage of digital media, an important issue\nis protection of the digital content. There is a number of algorithms and\ntechniques developed for the digital watermarking.In this paper, the invisible\nimage watermark procedure is considered. Watermark is created as a pseudo\nrandom sequence, embedded in the certain region of the image, obtained using\nHaar wavelet decomposition. Generally, the watermarking procedure should be\nrobust to the various attacks-filtering, noise etc. Here we assume the\nCompressive sensing scenario as a new signal processing technique that may\ninfluence the robustness. The focus of this paper was the possibility of the\nwatermark detection under Compressive Sensing attack with different number of\navailable image coefficients. The quality of the reconstructed images has been\nevaluated using Peak Signal to Noise Ratio (PSNR).The theory is supported with\nexperimental results.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.01996v1"
    },
    {
        "title": "A DCT And SVD based Watermarking Technique To Identify Tag",
        "authors": [
            "Ke Ji",
            "Jianbiao Lin",
            "Hui Li",
            "Ao Wang",
            "Tianjing Tang"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  With the rapid development of the multimedia,the secure of the multimedia is\nget more concerned. as far as we know , Digital watermarking is an effective\nway to protect copyright. The watermark must be generally hidden does not\naffect the quality of the original image. In this paper,a novel way based on\ndiscrete cosine transform(DCT) and singular value decomposition(SVD) .In the\nproposed way,we decomposition the image into 8*8 blocks, next we use the DCT to\nget the transformed block,then we choose the diagonal to embed the information,\nafter we do this, we recover the image and then we decomposition the image to\n8*8 blocks,we use the SVD way to get the diagonal matrix and embed the\ninformation in the matrix. next we extract the information use both inverse of\nDCT and SVD, as we all know,after we embed the information seconded time , the\ninformation we first information we embed must be changed, we choose a measure\nway called Peak Signal to Noise Ratio(PSNR) to estimate the similarity of the\ntwo image, and set a threshold to ensure whether the information is same or\nnot.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.02969v1"
    },
    {
        "title": "A two-stage video coding framework with both self-adaptive redundant\n  dictionary and adaptively orthonormalized DCT basis",
        "authors": [
            "Yuanyi Xue",
            "Yi Zhou",
            "Yao Wang"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  In this work, we propose a two-stage video coding framework, as an extension\nof our previous one-stage framework in [1]. The two-stage frameworks consists\ntwo different dictionaries. Specifically, the first stage directly finds the\nsparse representation of a block with a self-adaptive dictionary consisting of\nall possible inter-prediction candidates by solving an L0-norm minimization\nproblem using an improved orthogonal matching pursuit with embedded\northonormalization (eOMP) algorithm, and the second stage codes the residual\nusing DCT dictionary adaptively orthonormalized to the subspace spanned by the\nfirst stage atoms. The transition of the first stage and the second stage is\ndetermined based on both stages' quantization stepsizes and a threshold. We\nfurther propose a complete context adaptive entropy coder to efficiently code\nthe locations and the coefficients of chosen first stage atoms. Simulation\nresults show that the proposed coder significantly improves the RD performance\nover our previous one-stage coder. More importantly, the two-stage coder, using\na fixed block size and inter-prediction only, outperforms the H.264 coder\n(x264) and is competitive with the HEVC reference coder (HM) over a large rate\nrange.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.03802v1"
    },
    {
        "title": "On Crowdsourced Interactive Live Streaming: A Twitch.TV-Based\n  Measurement Study",
        "authors": [
            "Cong Zhang",
            "Jiangchuan Liu"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Empowered by today's rich tools for media generation and collaborative\nproduction, the multimedia service paradigm is shifting from the conventional\nsingle source, to multi-source, to many sources, and now toward {\\em\ncrowdsource}. Such crowdsourced live streaming platforms as Twitch.tv allow\ngeneral users to broadcast their content to massive viewers, thereby greatly\nexpanding the content and user bases. The resources available for these\nnon-professional broadcasters however are limited and unstable, which\npotentially impair the streaming quality and viewers' experience. The diverse\nlive interactions among the broadcasters and viewers can further aggravate the\nproblem.\n  In this paper, we present an initial investigation on the modern crowdsourced\nlive streaming systems. Taking Twitch as a representative, we outline their\ninside architecture using both crawled data and captured traffic of local\nbroadcasters/viewers. Closely examining the access data collected in a\ntwo-month period, we reveal that the view patterns are determined by both\nevents and broadcasters' sources. Our measurements explore the unique source-\nand event-driven views, showing that the current delay strategy on the viewer's\nside substantially impacts the viewers' interactive experience, and there is\nsignificant disparity between the long broadcast latency and the short live\nmessaging latency. On the broadcaster's side, the dynamic uploading capacity is\na critical challenge, which noticeably affects the smoothness of live streaming\nfor viewers.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.04666v2"
    },
    {
        "title": "Evaluating QoS Parameters for IPTV Distribution in Heterogeneous\n  Networks",
        "authors": [
            "Ioan Sorin Comsa",
            "Radu Arsinte"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The present work presents an architecture developed to evaluate the QoS\nparameters for the IPTV heterogeneous network. At its very basic level lie two\nsoftware technologies: Video LAN and Windows Media Services with two operating\nsystems: Windows and Linux. Three types of streams are analyzed, which will be\ntransmitted to a Linux VLC client through means of the aggregation and access\nservers. The first stream is generated in real time by a capture camera,\nprocessed by the encapsulated VC-1 encoder and sent to the Media Server, while\nthe second one is of VoD(Video on Demand) type and the third one will be\nhandled by DVBViewer through the MPEG TS form. The first stream is transcoded\nin H.264-AAC such that the Linux stations will recognize its format. Through\nthe simultaneous transmission of the three streams, we are analyzing their\nperformance from a QoS parameters point of view by means of an application\nimplemented in C programming language. The stream transporting the DVB-S\ntelevision content was proven to ensure the best performance regarding loss of\npackets, delays and jitter.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.06078v1"
    },
    {
        "title": "Compressive sensing based velocity estimation in video data",
        "authors": [
            "Ana Miletic",
            "Nemanja Ivanovic"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This paper considers the use of compressive sensing based algorithms for\nvelocity estimation of moving vehicles. The procedure is based on sparse\nreconstruction algorithms combined with time-frequency analysis applied to\nvideo data. This algorithm provides an accurate estimation of object's velocity\neven in the case of a very reduced number of available video frames. The\ninfluence of crucial parameters is analysed for different types of moving\nvehicles.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.06103v1"
    },
    {
        "title": "A Computation Control Motion Estimation Method for Complexity-Scalable\n  Video Coding",
        "authors": [
            "Weiyao Lin",
            "Krit Panusopone",
            "David M. Baylon",
            "Ming-Ting Sun"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  In this paper, a new Computation-Control Motion Estimation (CCME) method is\nproposed which can perform Motion Estimation (ME) adaptively under different\ncomputation or power budgets while keeping high coding performance. We first\npropose a new class-based method to measure the Macroblock (MB) importance\nwhere MBs are classified into different classes and their importance is\nmeasured by combining their class information as well as their initial matching\ncost information. Based on the new MB importance measure, a complete CCME\nframework is then proposed to allocate computation for ME. The proposed method\nperforms ME in a one-pass flow. Experimental results demonstrate that the\nproposed method can allocate computation more accurately than previous methods\nand thus has better performance under the same computation budget.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00083v1"
    },
    {
        "title": "A Fast Sub-Pixel Motion Estimation Algorithm for H.264/AVC Video Coding",
        "authors": [
            "Weiyao Lin",
            "Krit Panusopone",
            "David M. Baylon",
            "Ming-Ting Sun",
            "Zhenzhong Chen",
            "Hongxiang Li"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Motion Estimation (ME) is one of the most time-consuming parts in video\ncoding. The use of multiple partition sizes in H.264/AVC makes it even more\ncomplicated when compared to ME in conventional video coding standards. It is\nimportant to develop fast and effective sub-pixel ME algorithms since (a) The\ncomputation overhead by sub-pixel ME has become relatively significant while\nthe complexity of integer-pixel search has been greatly reduced by fast\nalgorithms, and (b) Reducing sub-pixel search points can greatly save the\ncomputation for sub-pixel interpolation. In this paper, a novel fast sub-pixel\nME algorithm is proposed which performs a 'rough' sub-pixel search before the\npartition selection, and performs a 'precise' sub-pixel search for the best\npartition. By reducing the searching load for the large number of non-best\npartitions, the computation complexity for sub-pixel search can be greatly\ndecreased. Experimental results show that our method can reduce the sub-pixel\nsearch points by more than 50% compared to existing fast sub-pixel ME methods\nwith negligible quality degradation.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00085v1"
    },
    {
        "title": "An Efficient Coding Method for Coding Region-of-Interest Locations in\n  AVS2",
        "authors": [
            "Mingliang Chen",
            "Weiyao Lin",
            "Xiaozhen Zheng"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Region-of-Interest (ROI) location information in videos has many practical\nusages in video coding field, such as video content analysis and user\nexperience improvement. Although ROI-based coding has been studied widely by\nmany researchers to improve coding efficiency for video contents, the ROI\nlocation information itself is seldom coded in video bitstream. In this paper,\nwe will introduce our proposed ROI location coding tool which has been adopted\nin surveillance profile of AVS2 video coding standard (surveillance profile).\nOur tool includes three schemes: direct-coding scheme, differential- coding\nscheme, and reconstructed-coding scheme. We will illustrate the details of\nthese schemes, and perform analysis of their advantages and disadvantages,\nrespectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00118v1"
    },
    {
        "title": "Region-Based Rate-Control for H.264/AVC for Low Bit-Rate Applications",
        "authors": [
            "Hai-Miao Hu",
            "Bo Li",
            "Weiyao Lin",
            "Wei Li",
            "Ming-Ting Sun"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Rate-control plays an important role in video coding. However, in the\nconventional rate-control algorithms, the number and position of Macroblocks\n(MBs) inside one basic unit for rate-control is inflexible and predetermined.\nThe different characteristics of the MBs are not fully considered. Also, there\nis no overall optimization of the coding of basic units. This paper proposes a\nnew region-based rate-control scheme for H.264/AVC to improve the coding\nefficiency. The inter-frame information is explored to objectively divide one\nframe into multiple regions based on their rate-distortion behaviors. The MBs\nwith the similar characteristics are classified into the same region, and the\nentire region instead of a single MB or a group of contiguous MBs is treated as\na basic unit for rate-control. A linear rate-quantization stepsize model and a\nlinear distortion-quantization stepsize model are proposed to accurately\ndescribe the rate-distortion characteristics for the region-based basic units.\nMoreover, based on the above linear models, an overall optimization model is\nproposed to obtain suitable Quantization Parameters (QPs) for the region-based\nbasic units. Experimental results demonstrate that the proposed region-based\nrate-control approach can achieve both better subjective and objective quality\nby performing the rate-control adaptively with the content, compared to the\nconventional rate-control approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00121v1"
    },
    {
        "title": "A Novel Image Steganographic Approach for Hiding Text in Color Images\n  using HSI Color Model",
        "authors": [
            "Khan Muhammad",
            "Jamil Ahmad",
            "Haleem Farman",
            "Muhammad Zubair"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Image Steganography is the process of embedding text in images such that its\nexistence cannot be detected by Human Visual System (HVS) and is known only to\nsender and receiver. This paper presents a novel approach for image\nsteganography using Hue-Saturation-Intensity (HSI) color space based on Least\nSignificant Bit (LSB). The proposed method transforms the image from RGB color\nspace to Hue-Saturation-Intensity (HSI) color space and then embeds secret data\ninside the Intensity Plane (I-Plane) and transforms it back to RGB color model\nafter embedding. The said technique is evaluated by both subjective and\nObjective Analysis. Experimentally it is found that the proposed method have\nlarger Peak Signal-to Noise Ratio (PSNR) values, good imperceptibility and\nmultiple security levels which shows its superiority as compared to several\nexisting methods\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00388v1"
    },
    {
        "title": "Gaussian Mixture Model Based Contrast Enhancement",
        "authors": [
            "Mohsen Abdoli",
            "Hossein Sarikhani",
            "Mohammad Ghanbari",
            "Patrice Brault"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  In this paper, a method for enhancing low contrast images is proposed. This\nmethod, called Gaussian Mixture Model based Contrast Enhancement (GMMCE),\nbrings into play the Gaussian mixture modeling of histograms to model the\ncontent of the images. Based on the fact that each homogeneous area in natural\nimages has a Gaussian-shaped histogram, it decomposes the narrow histogram of\nlow contrast images into a set of scaled and shifted Gaussians. The individual\nhistograms are then stretched by increasing their variance parameters, and are\ndiffused on the entire histogram by scattering their mean parameters, to build\na broad version of the histogram. The number of Gaussians as well as their\nparameters are optimized to set up a GMM with lowest approximation error and\nhighest similarity to the original histogram. Compared to the existing\nhistogram-based methods, the experimental results show that the quality of\nGMMCE enhanced pictures are mostly consistent and outperform other benchmark\nmethods. Additionally, the computational complexity analysis show that GMMCE is\na low complexity method.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.01620v2"
    },
    {
        "title": "Reliable SVD based Semi-blind and Invisible Watermarking Schemes",
        "authors": [
            "Subhayan Roy Moulick",
            "Siddharth Arora",
            "Chirag Jain",
            "Prasanta K. Panigrahi"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  A semi-blind watermarking scheme is presented based on Singular Value\nDecomposition (SVD), which makes essential use of the fact that, the SVD\nsubspace preserves significant amount of information of an image and is a one\nway decomposition. The principal components are used, along with the\ncorresponding singular vectors of the watermark image to watermark the target\nimage. For further security, the semi-blind scheme is extended to an invisible\nhash based watermarking scheme. The hash based scheme commits a watermark with\na key such that, it is incoherent with the actual watermark, and can only be\nextracted using the key. Its security is analyzed in the random oracle model\nand shown to be unforgeable, invisible and satisfying the property of\nnon-repudiation.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.01934v1"
    },
    {
        "title": "A novel hash based least significant bit (2-3-3) image steganography in\n  spatial domain",
        "authors": [
            "G. R. Manjula",
            "Ajit Danti"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This paper presents a novel 2-3-3 LSB insertion method. The image\nsteganography takes the advantage of human eye limitation. It uses color image\nas cover media for embedding secret message.The important quality of a\nsteganographic system is to be less distortive while increasing the size of the\nsecret message. In this paper a method is proposed to embed a color secret\nimage into a color cover image. A 2-3-3 LSB insertion method has been used for\nimage steganography. Experimental results show an improvement in the Mean\nsquared error (MSE) and Peak Signal to Noise Ratio (PSNR) values of the\nproposed technique over the base technique of hash based 3-3-2 LSB insertion.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.03674v1"
    },
    {
        "title": "User Centric Content Management System for Open IPTV Over SNS (ICTC2012)",
        "authors": [
            "Seung Hyun Jeon",
            "Sanghong An",
            "Changwoo Yoon",
            "Hyun-woo Lee",
            "Junkyun Choi"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Coupled schemes between service-oriented architecture (SOA) and Web 2.0 have\nrecently been researched. Web-based content providers and telecommunications\ncompany (Telecom) based Internet protocol television (IPTV) providers have\nstruggled against each other to accommodate more three-screen service\nsubscribers. Since the advent of Web 2.0, more abundant reproduced content can\nbe circulated. However, because according to increasing device's resolution and\ncontent formats IPTV providers transcode content in advance, network bandwidth,\nstorage and operation costs for content management systems (CMSs) are wasted.\nIn this paper, we present a user centric CMS for open IPTV, which integrates\nSOA and Web 2.0. Considering content popularity based on a Zipf-like\ndistribution to solve these problems, we analyze the performance between the\nuser centric CMS and the conventional Web syndication system for normalized\ncosts. Based on the user centric CMS, we implement a social Web TV with\ndevice-aware function, which can aggregate, transcode, and deploy content over\nsocial networking service (SNS) independently.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.04263v1"
    },
    {
        "title": "Identification of Image Operations Based on Steganalytic Features",
        "authors": [
            "Haodong Li",
            "Weiqi Luo",
            "Xiaoqing Qiu",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Image forensics have attracted wide attention during the past decade. Though\nmany forensic methods have been proposed to identify image forgeries, most of\nthem are targeted ones, since their proposed features are highly dependent on\nthe image operation under investigation. The performance of the well-designed\nfeatures for detecting the targeted operation usually degrades significantly\nfor other operations. On the other hand, a wise attacker can perform\nanti-forensics to fool the existing forensic methods, making countering\nanti-forensics become an urgent need. In this paper, we try to find a universal\nfeature to detect various image processing and anti-forensic operations. Based\non our extensive experiments and analysis, we find that any image\nprocessing/anti-forensic operations would inevitably modify many image pixels.\nThis would change some inherent statistics within original images, which is\nsimilar to the case of steganography. Therefore, we model image\nprocessing/anti-forensic operations as steganography problems, and propose a\ndetection strategy by applying steganalytic features. With some advanced\nsteganalytic features, we are able to detect various image operations and\nfurther identify their types. In our experiments, we have tested several\nsteganalytic features on 11 different kinds of typical image processing\noperations and 4 kinds of anti-forensic operations. The experimental results\nhave shown that the proposed strategy significantly outperforms the existing\nforensic methods in both effectiveness and universality.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.04718v2"
    },
    {
        "title": "The blind detection for palette image watermarking without changing the\n  color",
        "authors": [
            "V. N. Gorbachev",
            "E. M. Kaynarova",
            "I. K. Metelev",
            "O. V. Pavlovskaya"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  To hide a binary pattern in the palette image a steganographic scheme with\nblind detection is considered. The embedding algorithm uses the Lehmer code by\npalette color permutations for which the cover image palette is generally\nrequired. The found transformation between the palette and RGB images allows to\nextract the hidden data without any cover work.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.04958v1"
    },
    {
        "title": "Error-Resilient Multicasting for Multi-View 3D Videos in Wireless\n  Networks",
        "authors": [
            "Chi-Heng Lin",
            "De-Nian Yang",
            "Ji-Tang Lee",
            "Wanjiun Liao"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  With the emergence of naked-eye 3D mobile devices, mobile 3D video services\nare becoming increasingly important for video service providers, such as\nYoutube and Netflix, while multi-view 3D videos have the potential to inspire a\nvariety of innovative applications. However, enabling multi-view 3D video\nservices may overwhelm WiFi networks when every view of a video are\nmulticasted. In this paper, therefore, we propose to incorporate\ndepth-image-based rendering (DIBR), which allows each mobile client to\nsynthesize the desired view from nearby left and right views, in order to\neffectively reduce the bandwidth consumption. Moreover, when each client\nsuffers from packet losses, retransmissions incur additional bandwidth\nconsumption and excess delay, which in turn undermines the quality of\nexperience in video applications. To address the above issue, we first discover\nthe merit of view protection via DIBR for multi-view video multicast using a\nmathematical analysis and then design a new protocol, named Multi-View Group\nManagement Protocol (MVGMP), to support the dynamic join and leave of users and\nthe change of desired views. The simulation results demonstrate that our\nprotocol effectively reduces bandwidth consumption and increases the\nprobability for each client to successfully playback the desired views in a\nmulti-view 3D video.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08726v4"
    },
    {
        "title": "SLRMA: Sparse Low-Rank Matrix Approximation for Data Compression",
        "authors": [
            "Junhui Hou",
            "Lap-Pui Chau",
            "Nadia Magnenat-Thalmann",
            "Ying He"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Low-rank matrix approximation (LRMA) is a powerful technique for signal\nprocessing and pattern analysis. However, its potential for data compression\nhas not yet been fully investigated in the literature. In this paper, we\npropose sparse low-rank matrix approximation (SLRMA), an effective\ncomputational tool for data compression. SLRMA extends the conventional LRMA by\nexploring both the intra- and inter-coherence of data samples simultaneously.\nWith the aid of prescribed orthogonal transforms (e.g., discrete cosine/wavelet\ntransform and graph transform), SLRMA decomposes a matrix into a product of two\nsmaller matrices, where one matrix is made of extremely sparse and orthogonal\ncolumn vectors, and the other consists of the transform coefficients.\nTechnically, we formulate SLRMA as a constrained optimization problem, i.e.,\nminimizing the approximation error in the least-squares sense regularized by\n$\\ell_0$-norm and orthogonality, and solve it using the inexact augmented\nLagrangian multiplier method. Through extensive tests on real-world data, such\nas 2D image sets and 3D dynamic meshes, we observe that (i) SLRMA empirically\nconverges well; (ii) SLRMA can produce approximation error comparable to LRMA\nbut in a much sparse form; (iii) SLRMA-based compression schemes significantly\noutperform the state-of-the-art in terms of rate-distortion performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.01673v2"
    },
    {
        "title": "Data Hiding in Video using Triangularization LSB Technique",
        "authors": [
            "Subhashri Acharya",
            "Pramita Srimany",
            "Sanchari Kundu",
            "JayatiGhosh Dastidar"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The importance of data hiding in the field of Information Technology is a\nwidely accepted. The challenge is to be able to pass information in a manner\nthat the very existence of the message is unknown in order to repel attention\nof the potential attacker. Steganography is a technique that has been widely\nused to achieve this objective. However Steganography is often found to be\nlacking when it comes to hiding bulk data. Attempting to hide data in a video\novercomes this problem because of the large sized cover object (video) as\ncompared to an image in the case of steganography. This paper attempts to\npropose a scheme using which data can be hidden in a video. We focus on the\nTriangularization method and make use of the Least Significant Bit (LSB)\ntechnique in hiding messages in a video.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.05242v1"
    },
    {
        "title": "Low Bit-Rate and High Fidelity Reversible Data Hiding",
        "authors": [
            "Xiaochao Qu",
            "Suah Kim",
            "Run Cui",
            "Hyoung Joong Kim"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  An accurate predictor is crucial for histogram-shifting (HS) based reversible\ndata hiding methods. The embedding capacity is increased and the embedding\ndistortion is decreased simultaneously if the predictor can generate accurate\npredictions. In this paper, we propose an accurate linear predictor based on\nweighted least squares (WLS) estimation. The robustness of WLS helps the\nproposed predictor generate accurate predictions, especially in complex texture\nareas of an image, where other predictors usually fail. To further reduce the\nembedding distortion, we propose a new embedding method called dynamic\nhistogram shifting with pixel selection (DHS-PS) that selects not only the\nproper histogram bins but also the proper pixel locations to embed the given\ndata. As a result, the proposed method can obtain very high fidelity marked\nimages with low bit-rate data embedded. The experimental results show that the\nproposed method outperforms the state-of-the-art low bit-rate reversible data\nhiding method.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.08075v1"
    },
    {
        "title": "Arabic Text Watermarking: A Review",
        "authors": [
            "Reem Ahmed Alotaibi",
            "Lamiaa A. Elrefaei"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The using of the internet with its technologies and applications have been\nincreased rapidly. So, protecting the text from illegal use is too needed .\nText watermarking is used for this purpose. Arabic text has many\ncharacteristics such existing of diacritics , kashida (extension character) and\npoints above or under its letters .Each of Arabic letters can take different\nshapes with different Unicode. These characteristics are utilized in the\nwatermarking process. In this paper, several methods are discussed in the area\nof Arabic text watermarking with its advantages and disadvantages .Comparison\nof these methods is done in term of capacity, robustness and Imperceptibility.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.01485v1"
    },
    {
        "title": "Compressive Video Sensing via Dictionary Learning and Forward Prediction",
        "authors": [
            "Nasser Eslahi",
            "Ali Aghagolzadeh",
            "Seyed Mehdi Hosseini Andargoli"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  In this paper, we propose a new framework for compressive video sensing (CVS)\nthat exploits the inherent spatial and temporal redundancies of a video\nsequence, effectively. The proposed method splits the video sequence into the\nkey and non-key frames followed by dividing each frame into small\nnon-overlapping blocks of equal sizes. At the decoder side, the key frames are\nreconstructed using adaptively learned sparsifying (ALS) basis via $\\ell_0$\nminimization, in order to exploit the spatial redundancy. Also, the\neffectiveness of three well-known dictionary learning algorithms is\ninvestigated in our method. For recovery of the non-key frames, a prediction of\nthe current frame is initialized, by using the previous reconstructed frame, in\norder to exploit the temporal redundancy. The prediction is employed in a\nproper optimization problem to recover the current non-key frame. To compare\nour experimental results with the results of some other methods, we employ peak\nsignal to noise ratio (PSNR) and structural similarity (SSIM) index as the\nquality assessor. The numerical results show the adequacy of our proposed\nmethod in CVS.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.07640v1"
    },
    {
        "title": "In-Network View Synthesis for Interactive Multiview Video Systems",
        "authors": [
            "Laura Toni",
            "Gene Cheung",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  To enable Interactive multiview video systems with a minimum view-switching\ndelay, multiple camera views are sent to the users, which are used as reference\nimages to synthesize additional virtual views via depth-image-based rendering.\nIn practice, bandwidth constraints may however restrict the number of reference\nviews sent to clients per time unit, which may in turn limit the quality of the\nsynthesized viewpoints. We argue that the reference view selection should\nideally be performed close to the users, and we study the problem of in-network\nreference view synthesis such that the navigation quality is maximized at the\nclients. We consider a distributed cloud network architecture where data stored\nin a main cloud is delivered to end users with the help of cloudlets, i.e.,\nresource-rich proxies close to the users. In order to satisfy last-hop\nbandwidth constraints from the cloudlet to the users, a cloudlet re-samples\nviewpoints of the 3D scene into a discrete set of views (combination of\nreceived camera views and virtual views synthesized) to be used as reference\nfor the synthesis of additional virtual views at the client. This in-network\nsynthesis leads to better viewpoint sampling given a bandwidth constraint\ncompared to simple selection of camera views, but it may however carry a\ndistortion penalty in the cloudlet-synthesized reference views. We therefore\ncast a new reference view selection problem where the best subset of views is\ndefined as the one minimizing the distortion over a view navigation window\ndefined by the user under some transmission bandwidth constraints. We show that\nthe view selection problem is NP-hard, and propose an effective polynomial time\nalgorithm using dynamic programming to solve the optimization problem.\nSimulation results finally confirm the performance gain offered by virtual view\nsynthesis in the network.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.00464v1"
    },
    {
        "title": "Audio Steganography: LSB Technique Using a Pyramid Structure and Range\n  of Bytes",
        "authors": [
            "Satish Bhalshankar",
            "Avinash K. Gulve"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The demand for keeping the information secure and confidential simultaneously\nhas been progressively increasing. Among various techniques- Audio\nSteganography, a technique of embedding information transparently in a digital\nmedia thereby restricting the access to such information has been prominently\ndeveloped. Imperceptibility, robustness, and payload or hiding capacity are the\nmain character for it. In earlier, LSB techniques increased payload capacity\nwould hamper robustness as well as imperceptibility of the cover media and vice\nversa. The proposed technique overcomes the problem. It provides relatively\ngood improvement in the payload capacity by dividing the bytes of cover media\ninto ranges to hide the bits of secret message appropriately. As well as due to\nthe use of ranges of bytes the robustness of cover media has maintained and\nimperceptibility preserved by using a pyramid structure.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.02630v1"
    },
    {
        "title": "Merge Frame Design for Video Stream Switching using Piecewise Constant\n  Functions",
        "authors": [
            "Wei Dai",
            "Gene Cheung",
            "Ngai-Man Cheung",
            "Antonio Ortega",
            "Oscar C. Au"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The ability to efficiently switch from one pre-encoded video stream to\nanother (e.g., for bitrate adaptation or view switching) is important for many\ninteractive streaming applications. Recently, stream-switching mechanisms based\non distributed source coding (DSC) have been proposed. In order to reduce the\noverall transmission rate, these approaches provide a \"merge\" mechanism, where\ninformation is sent to the decoder such that the exact same frame can be\nreconstructed given that any one of a known set of side information (SI) frames\nis available at the decoder (e.g., each SI frame may correspond to a different\nstream from which we are switching). However, the use of bit-plane coding and\nchannel coding in many DSC approaches leads to complex coding and decoding. In\nthis paper, we propose an alternative approach for merging multiple SI frames,\nusing a piecewise constant (PWC) function as the merge operator. In our\napproach, for each block to be reconstructed, a series of parameters of these\nPWC merge functions are transmitted in order to guarantee identical\nreconstruction given the known side information blocks. We consider two\ndifferent scenarios. In the first case, a target frame is first given, and then\nmerge parameters are chosen so that this frame can be reconstructed exactly at\nthe decoder. In contrast, in the second scenario, the reconstructed frame and\nmerge parameters are jointly optimized to meet a rate-distortion criteria.\nExperiments show that for both scenarios, our proposed merge techniques can\noutperform both a recent approach based on DSC and the SP-frame approach in\nH.264, in terms of compression efficiency and decoder complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.02995v1"
    },
    {
        "title": "A New Method For Digital Watermarking Based on Combination of DCT and\n  PCA",
        "authors": [
            "Arash Saboori",
            "S. Abolfazl Hosseini"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  In the digital watermarking with DCT method,the watermark is located within a\nrange of DCT coefficients of the cover image. In this paper to use the\nlow-frequency band, a new method is proposed by using a combination of the DCT\nand PCA transform. The proposed method is compared to other DCT methods, our\nmethod is robust and keeps the quality of cover image, also increases capacity\nof the watermarking.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.03278v1"
    },
    {
        "title": "Hardware Implementation of Compressed Sensing based Low Complex Video\n  Encoder",
        "authors": [
            "Batta Kota Naga Srinivasarao",
            "Indrajit Chakrabarti"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This paper presents a memory efficient VLSI architecture of low complex video\nencoder using three dimensional (3-D) wavelet and Compressed Sensing (CS) is\nproposed for space and low power video applications. Majority of the\nconventional video coding schemes are based on hybrid model, which requires\ncomplex operations like transform coding (DCT), motion estimation and\ndeblocking filter at the encoder. Complexity of the proposed encoder is reduced\nby replacing those complex operations by 3-D DWT and CS at the encoder. The\nproposed architecture uses 3-D DWT to enable the scalability with levels of\nwavelet decomposition and also to exploit the spatial and the temporal\nredundancies. CS provides the good error resilience and coding efficiency. At\nthe first stage of the proposed architecture for encoder, 3-D DWT has been\napplied (Lifting based 2-D DWT in spatial domain and Haar wavelet in temporal\ndomain) on each frame of the group of frames (GOF), and in the second stage CS\nmodule exploits the sparsity of the wavelet coefficients. Small set of linear\nmeasurements are extracted by projecting the sparse 3-D wavelet coefficients\nonto random Bernoulli matrix at the encoder. Compared with the best existing\n3-D DWT architectures, the proposed architecture for 3-D DWT requires less\nmemory and provide high throughput. For an N?N image, the proposed 3-D DWT\narchitecture consumes a total of only 2?(3N +40P) words of on-chip memory for\nthe one level of decomposition. The proposed architecture for an encoder is\nfirst of its kind and to the best of my knowledge, no architecture is noted for\ncomparison. The proposed VLSI architecture of the encoder has been synthesized\non 90-nm CMOS process technology and results show that it consumes 90.08 mW\npower and occupies an area equivalent to 416.799 K equivalent gate at frequency\nof 158 MHz.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.03836v1"
    },
    {
        "title": "A Resource Allocation Mechanism for Video Mixing as a Cloud Computing\n  Service in Multimedia Conferencing Applications",
        "authors": [
            "Abbas Soltanian",
            "Mohammad A. Salahuddin",
            "Halima Elbiaze",
            "Roch Glitho"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Multimedia conferencing is the conversational exchange of multimedia content\nbetween multiple parties. It has a wide range of applications (e.g. Massively\nMultiplayer Online Games (MMOGs) and distance learning). Many multimedia\nconferencing applications use video extensively, thus video mixing in\nconferencing settings is of critical importance. Cloud computing is a\ntechnology that can solve the scalability issue in multimedia conferencing,\nwhile bringing other benefits, such as, elasticity, efficient use of resources,\nrapid development, and introduction of new applications. However, proposed\ncloud-based multimedia conferencing approaches so far have several deficiencies\nwhen it comes to efficient resource usage while meeting Quality of Service\n(QoS) requirements. We propose a solution to optimize resource allocation for\ncloud-based video mixing service in multimedia conferencing applications, which\ncan support scalability in terms of number of users, while guaranteeing QoS. We\nformulate the resource allocation problem mathematically as an Integer Linear\nProgramming (ILP) problem and design a heuristic for it. Simulation results\nshow that our resource allocation model can support more participants compared\nto the state-of-the-art, while honoring QoS, with respect to end-to-end delay.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.06792v1"
    },
    {
        "title": "CVC: The Contourlet Video Compression algorithm for real-time\n  applications",
        "authors": [
            "Stamos Katsigiannis",
            "Georgios Papaioannou",
            "Dimitris Maroulis"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Nowadays, real-time video communication over the internet through video\nconferencing applications has become an invaluable tool in everyone's\nprofessional and personal life. This trend underlines the need for video coding\nalgorithms that provide acceptable quality on low bitrates and can support\nvarious resolutions inside the same stream in order to cope with limitations on\ncomputational resources and network bandwidth. In this work, a novel scalable\nvideo coding algorithm based on the contourlet transform is presented. The\nalgorithm utilizes both lossy and lossless methods in order to achieve\ncompression. One of its most notable features is that due to the transform\nutilised, it does not suffer from blocking artifacts that occur with many\nwidely adopted compression algorithms. The proposed algorithm takes advantage\nof the vast computational capabilities of modern GPUs, in order to achieve\nreal-time performance and provide satisfactory encoding and decoding times at\nrelatively low cost, making it suitable for applications like video\nconferencing. Experiments show that the proposed algorithm performs\nsatisfactorily in terms of compression ratio and speed, while it outperforms\nstandard methods in terms of perceptual quality on lower bitrates.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.00561v1"
    },
    {
        "title": "A System for Precise End-to-End Delay Measurements in Video\n  Communication",
        "authors": [
            "Christoph Bachhuber",
            "Eckehard Steinbach"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Low delay video transmission is becoming increasingly important. Delay\ncritical, video enabled applications range from teleoperation scenarios such as\ncontrolling drones or telesurgery to autonomous control through computer vision\nalgorithms applied on real-time video. To judge the quality of the video\ntransmission in such a system, it is important to be able to precisely measure\nthe end-to-end (E2E) delay of the transmitted video. We present a\nlow-complexity system that automatically takes pairwise independent\nmeasurements of E2E delay. The precision can be far below the millisecond\norder, mainly limited by the sampling rate of the measurement system. In our\nimplementation, we achieve a precision of 0.5 milliseconds with a sampling rate\nof 2kHz.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.01134v2"
    },
    {
        "title": "The Virtual Splitter: Refactoring Web Applications for the Multiscreen\n  Environment",
        "authors": [
            "Mira Sarkis",
            "Cyril Concolato",
            "Jean-Claude Dufourd"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Creating web applications for the multiscreen environment is still a\nchallenge. One approach is to transform existing single-screen applications but\nthis has not been done yet automatically or generically. This paper proposes a\nrefactor-ing system. It consists of a generic and extensible mapping phase that\nautomatically analyzes the application content based on a semantic or a visual\ncriterion determined by the author or the user, and prepares it for the\nsplitting process. The system then splits the application and as a result\ndelivers two instrumented applications ready for distribution across devices.\nDuring runtime, the system uses a mirroring phase to maintain the functionality\nof the distributed application and to support a dynamic splitting process.\nDeveloped as a Chrome extension, our approach is validated on several web\napplications, including a YouTube page and a video application from Mozilla.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.05405v1"
    },
    {
        "title": "GECKA3D: A 3D Game Engine for Commonsense Knowledge Acquisition",
        "authors": [
            "Erik Cambria",
            "Tam V. Nguyen",
            "Brian Cheng",
            "Kenneth Kwok",
            "Jose Sepulveda"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Commonsense knowledge representation and reasoning is key for tasks such as\nartificial intelligence and natural language understanding. Since commonsense\nconsists of information that humans take for granted, gathering it is an\nextremely difficult task. In this paper, we introduce a novel 3D game engine\nfor commonsense knowledge acquisition (GECKA3D) which aims to collect\ncommonsense from game designers through the development of serious games.\nGECKA3D integrates the potential of serious games and games with a purpose.\nThis provides a platform for the acquisition of re-usable and multi-purpose\nknowledge, and also enables the development of games that can provide\nentertainment value and teach players something meaningful about the actual\nworld they live in.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.01178v1"
    },
    {
        "title": "Adaptation Logic for HTTP Dynamic Adaptive Streaming using\n  Geo-Predictive Crowdsourcing",
        "authors": [
            "Ran Dubin",
            "Amit Dvir",
            "Ofir Pele",
            "Ofer Hadar",
            "Itay Katz",
            "Ori Mashiach"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The increasing demand for video streaming services with high Quality of\nExperience (QoE) has prompted a lot of research on client-side adaptation logic\napproaches. However, most algorithms use the client's previous download\nexperience and do not use a crowd knowledge database generated by users of a\nprofessional service. We propose a new crowd algorithm that maximizes the QoE.\nAdditionally, we show how crowd information can be integrated into existing\nalgorithms and illustrate this with two state-of-the-art algorithms. We\nevaluate our algorithm and state-of-the-art algorithms (including our modified\nalgorithms) on a large, real-life crowdsourcing dataset that contains 336,551\nsamples on network performance. The dataset was provided by WeFi LTD. Our new\nalgorithm outperforms all other methods in terms of QoS (eMOS).\n",
        "pdf_link": "http://arxiv.org/pdf/1602.02030v1"
    },
    {
        "title": "Perceptual Vector Quantization For Video Coding",
        "authors": [
            "Jean-Marc Valin",
            "Timothy B. Terriberry"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This paper applies energy conservation principles to the Daala video codec\nusing gain-shape vector quantization to encode a vector of AC coefficients as a\nlength (gain) and direction (shape). The technique originates from the CELT\nmode of the Opus audio codec, where it is used to conserve the spectral\nenvelope of an audio signal. Conserving energy in video has the potential to\npreserve textures rather than low-passing them. Explicitly quantizing a gain\nallows a simple contrast masking model with no signaling cost. Vector\nquantizing the shape keeps the number of degrees of freedom the same as scalar\nquantization, avoiding redundancy in the representation. We demonstrate how to\npredict the vector by transforming the space it is encoded in, rather than\nsubtracting off the predictor, which would make energy conservation impossible.\nWe also derive an encoding of the vector-quantized codewords that takes\nadvantage of their non-uniform distribution. We show that the resulting\ntechnique outperforms scalar quantization by an average of 0.90 dB on still\nimages, equivalent to a 24.8% reduction in bitrate at equal quality, while for\nvideos, the improvement averages 0.83 dB, equivalent to a 13.7% reduction in\nbitrate.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05209v1"
    },
    {
        "title": "The AV1 Constrained Directional Enhancement Filter (CDEF)",
        "authors": [
            "Steinar Midtskogen",
            "Jean-Marc Valin"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This paper presents the constrained directional enhancement filter designed\nfor the AV1 royalty-free video codec. The in-loop filter is based on a\nnon-linear low-pass filter and is designed for vectorization efficiency. It\ntakes into account the direction of edges and patterns being filtered. The\nfilter works by identifying the direction of each block and then adaptively\nfiltering with a high degree of control over the filter strength along the\ndirection and across it. The proposed enhancement filter is shown to improve\nthe quality of the Alliance for Open Media (AOM) AV1 and Thor video codecs in\nparticular in low complexity configurations.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05975v3"
    },
    {
        "title": "A Practical Approach to Spatiotemporal Data Compression",
        "authors": [
            "Niall H. Robinson",
            "Rachel Prudden",
            "Alberto Arribas"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Datasets representing the world around us are becoming ever more unwieldy as\ndata volumes grow. This is largely due to increased measurement and modelling\nresolution, but the problem is often exacerbated when data are stored at\nspuriously high precisions. In an effort to facilitate analysis of these\ndatasets, computationally intensive calculations are increasingly being\nperformed on specialised remote servers before the reduced data are transferred\nto the consumer. Due to bandwidth limitations, this often means data are\ndisplayed as simple 2D data visualisations, such as scatter plots or images. We\npresent here a novel way to efficiently encode and transmit 4D data fields\non-demand so that they can be locally visualised and interrogated. This nascent\n\"4D video\" format allows us to more flexibly move the boundary between data\nserver and consumer client. However, it has applications beyond purely\nscientific visualisation, in the transmission of data to virtual and augmented\nreality.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.03688v2"
    },
    {
        "title": "Prediction-error of Prediction Error (PPE)-based Reversible Data Hiding",
        "authors": [
            "Han-Zhou Wu",
            "Hong-Xia Wang",
            "Yun-Qing Shi"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This paper presents a novel reversible data hiding (RDH) algorithm for\ngray-scaled images, in which the prediction-error of prediction error (PPE) of\na pixel is used to carry the secret data. In the proposed method, the pixels to\nbe embedded are firstly predicted with their neighboring pixels to obtain the\ncorresponding prediction errors (PEs). Then, by exploiting the PEs of the\nneighboring pixels, the prediction of the PEs of the pixels can be determined.\nAnd, a sorting technique based on the local complexity of a pixel is used to\ncollect the PPEs to generate an ordered PPE sequence so that, smaller PPEs will\nbe processed first for data embedding. By reversibly shifting the PPE histogram\n(PPEH) with optimized parameters, the pixels corresponding to the altered PPEH\nbins can be finally modified to carry the secret data. Experimental results\nhave implied that the proposed method can benefit from the prediction procedure\nof the PEs, sorting technique as well as parameters selection, and therefore\noutperform some state-of-the-art works in terms of payload-distortion\nperformance when applied to different images.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.04984v5"
    },
    {
        "title": "Lossless Intra Coding in HEVC with Adaptive 3-tap Filters",
        "authors": [
            "Saeed Ranjbar Alvar",
            "Fatih Kamisli"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  In pixel-by-pixel spatial prediction methods for lossless intra coding, the\nprediction is obtained by a weighted sum of neighbouring pixels. The proposed\nprediction approach in this paper uses a weighted sum of three neighbor pixels\naccording to a two-dimensional correlation model. The weights are obtained\nafter a three step optimization procedure. The first two stages are offline\nprocedures where the computed prediction weights are obtained offline from\ntraining sequences. The third stage is an online optimization procedure where\nthe offline obtained prediction weights are further fine-tuned and adapted to\neach encoded block during encoding using a rate-distortion optimized method and\nthe modification in this third stage is transmitted to the decoder as side\ninformation. The results of the simulations show average bit rate reductions of\n12.02% and 3.28% over the default lossless intra coding in HEVC and the\nwell-known Sample-based Angular Prediction (SAP) method, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.07051v1"
    },
    {
        "title": "Compressed-domain visual saliency models: A comparative study",
        "authors": [
            "Sayed Hossein Khatoonabadi",
            "Ivan V. Bajic",
            "Yufeng Shan"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Computational modeling of visual saliency has become an important research\nproblem in recent years, with applications in video quality estimation, video\ncompression, object tracking, retargeting, summarization, and so on. While most\nvisual saliency models for dynamic scenes operate on raw video, several models\nhave been developed for use with compressed-domain information such as motion\nvectors and transform coefficients. This paper presents a comparative study of\neleven such models as well as two high-performing pixel-domain saliency models\non two eye-tracking datasets using several comparison metrics. The results\nindicate that highly accurate saliency estimation is possible based only on a\npartially decoded video bitstream. The strategies that have shown success in\ncompressed-domain saliency modeling are highlighted, and certain challenges are\nidentified as potential avenues for further improvement.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.07339v1"
    },
    {
        "title": "Subjective Assessment of H.264 Compressed Stereoscopic Video",
        "authors": [
            "Manasa K",
            "Balasubramanyam Appina",
            "Sumohana S. Channappayya"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The tremendous growth in 3D (stereo) imaging and display technologies has led\nto stereoscopic content (video and image) becoming increasingly popular.\nHowever, both the subjective and the objective evaluation of stereoscopic video\ncontent has not kept pace with the rapid growth of the content. Further, the\navailability of standard stereoscopic video databases is also quite limited. In\nthis work, we attempt to alleviate these shortcomings. We present a\nstereoscopic video database and its subjective evaluation. We have created a\ndatabase containing a set of 144 distorted videos. We limit our attention to\nH.264 compression artifacts. The distorted videos were generated using 6\nuncompressed pristine videos of left and right views originally created by\nGoldmann et al. at EPFL [1]. Further, 19 subjects participated in the\nsubjective assessment task. Based on the subjective study, we have formulated a\nrelation between the 2D and stereoscopic subjective scores as a function of\ncompression rate and depth range. We have also evaluated the performance of\npopular 2D and 3D image/video quality assessment (I/VQA) algorithms on our\ndatabase.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.07519v1"
    },
    {
        "title": "Compress Voice Transference over low Signal Strength in Satellite\n  Communication",
        "authors": [
            "Saira Beg",
            "M. Fahad Khan",
            "Faisal Baig"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This paper presents the comparison of compression algorithms for voice\ntransferring method over SMS in satellite communication. Voice transferring\nmethod over SMS is useful in situations when signal strength is low and due to\npoor signal strength voice call connection is not possible to initiate or\nsignal dropped during voice call. This method has one serious flaw that it\nproduces large number of SMS while converting voice into SMS. Such issue is\ncatered to some extend by employing any compression algorithm. In this paper\nour major aim is to find best compression scheme for said method, for that\npurpose we compare 6 different types of compression algorithms which are; LZW\n(Lempel-Ziv-Welch), Huffman coding, PPM (Prediction by partial matching),\nArithmetic Coding (AC), BWT (Burrows-Wheeler-Transform), LZMA\n(Lempel-Ziv-Markov chain). This comparison shows that PPM compression method\noffers better compression ratio and produce small number of SMS. For\nexperimentation we use Thuraya SG-2520 satellite phone. Moreover, we develop an\napplication using J2ME platform[Ref:a]. We tested that application more than\n100 times and then we compare the result in terms of compression ratio of each\nalgorithm and number of connected SMS produce after each compression method.\nThe result of this study will help developers to choose better compression\nscheme for their respective applications.\nhttp://www.learnrnd.com/news.php?id=ISSUES_IN_MOLECULAR_COMMUNICATIONS\n",
        "pdf_link": "http://arxiv.org/pdf/1604.07593v1"
    },
    {
        "title": "Context Tree based Image Contour Coding using A Geometric Prior",
        "authors": [
            "Amin Zheng",
            "Gene Cheung",
            "Dinei Florencio"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  If object contours in images are coded efficiently as side information, then\nthey can facilitate advanced image / video coding techniques, such as graph\nFourier transform coding or motion prediction of arbitrarily shaped pixel\nblocks. In this paper, we study the problem of lossless and lossy compression\nof detected contours in images. Specifically, we first convert a detected\nobject contour composed of contiguous between-pixel edges to a sequence of\ndirectional symbols drawn from a small alphabet. To encode the symbol sequence\nusing arithmetic coding, we compute an optimal variable-length context tree\n(VCT) $\\mathcal{T}$ via a maximum a posterior (MAP) formulation to estimate\nsymbols' conditional probabilities. MAP prevents us from overfitting given a\nsmall training set $\\mathcal{X}$ of past symbol sequences by identifying a VCT\n$\\mathcal{T}$ that achieves a high likelihood $P(\\mathcal{X}|\\mathcal{T})$ of\nobserving $\\mathcal{X}$ given $\\mathcal{T}$, and a large geometric prior\n$P(\\mathcal{T})$ stating that image contours are more often straight than\ncurvy. For the lossy case, we design efficient dynamic programming (DP)\nalgorithms that optimally trade off coding rate of an approximate contour\n$\\hat{\\mathbf{x}}$ given a VCT $\\mathcal{T}$ with two notions of distortion of\n$\\hat{\\mathbf{x}}$ with respect to the original contour $\\mathbf{x}$. To reduce\nthe size of the DP tables, a total suffix tree is derived from a given VCT\n$\\mathcal{T}$ for compact table entry indexing, reducing complexity.\nExperimental results show that for lossless contour coding, our proposed\nalgorithm outperforms state-of-the-art context-based schemes consistently for\nboth small and large training datasets. For lossy contour coding, our\nalgorithms outperform comparable schemes in the literature in rate-distortion\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.08001v1"
    },
    {
        "title": "Text writing in the air",
        "authors": [
            "Saira Beg",
            "M. Fahad Khan",
            "Faisal Baig"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This paper presents a real time video based pointing method which allows\nsketching and writing of English text over air in front of mobile camera.\nProposed method have two main tasks: first it track the colored finger tip in\nthe video frames and then apply English OCR over plotted images in order to\nrecognize the written characters. Moreover, proposed method provides a natural\nhuman-system interaction in such way that it do not require keypad, stylus, pen\nor glove etc for character input. For the experiments, we have developed an\napplication using OpenCv with JAVA language. We tested the proposed method on\nSamsung Galaxy3 android mobile. Results show that proposed algorithm gains the\naverage accuracy of 92.083% when tested for different shaped alphabets. Here,\nmore than 3000 different Magnetic 3D shaped characters were used [Ref:\nhttp://learnrnd.com/news.php?id=Magnetic_3D_Bio_Printing]. Our proposed system\nis the software based approach and relevantly very simple, fast and easy. It\ndoes not require sensors or any hardware rather than camera and red tape.\nMoreover, proposed methodology can be applicable for all disconnected languages\nbut having one issue that it is color sensitive in such a way that existence of\nany red color in the background before starting the character writing can lead\nto false results.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.08245v1"
    },
    {
        "title": "Which Adaptation Logic? An Objective and Subjective Performance\n  Evaluation of HTTP-based Adaptive Media Streaming Systems",
        "authors": [
            "Christian Timmerer",
            "Matteo Maiero",
            "Benjamin Rainer"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Multimedia content delivery over the Internet is predominantly using the\nHypertext Transfer Protocol (HTTP) as its primary protocol and multiple\nproprietary solutions exits. The MPEG standard Dynamic Adaptive Streaming over\nHTTP (DASH) provides an interoperable solution and in recent years various\nadaptation logics/algorithms have been proposed. However, to the best of our\nknowledge, there is no comprehensive evaluation of the various\nlogics/algorithms. Therefore, this paper provides a comprehensive evaluation of\nten different adaptation logics/algorithms, which have been proposed in the\npast years. The evaluation is done both objectively and subjectively. The\nformer is using a predefined bandwidth trajectory within a controlled\nenvironment and the latter is done in a real-world environment adopting\ncrowdsourcing. The results shall provide insights about which strategy can be\nadopted in actual deployment scenarios. Additionally, the evaluation\nmethodology described in this paper can be used to evaluate any other/new\nadaptation logic and to compare it directly with the results reported here.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00341v1"
    },
    {
        "title": "Can Machine Learn Steganography? - Implementing LSB Substitution and\n  Matrix Coding Steganography with Feed-Forward Neural Networks",
        "authors": [
            "Han-Zhou Wu",
            "Hong-Xia Wang",
            "Yun-Qing Shi"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  In recent years, due to the powerful abilities to deal with highly complex\ntasks, the artificial neural networks (ANNs) have been studied in the hope of\nachieving human-like performance in many applications. Since the ANNs have the\nability to approximate complex functions from observations, it is\nstraightforward to consider the ANNs for steganography. In this paper, we aim\nto implement the well-known LSB substitution and matrix coding steganography\nwith the feed-forward neural networks (FNNs). Our experimental results have\nshown that, the used FNNs can achieve the data embedding operation of the LSB\nsubstitution and matrix coding steganography. For steganography with the ANNs,\nthough there may be some challenges to us, it would be very promising and\nvaluable to pay attention to the ANNs for steganography, which may be a new\ndirection for steganography.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.05294v1"
    },
    {
        "title": "A Note on Efficiency of Downsampling and Color Transformation in Image\n  Quality Assessment",
        "authors": [
            "Hossein Ziaei Nafchi",
            "Mohamed Cheriet"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Several existing and successful full reference image quality assessment (IQA)\nmodels use linear color transformation and downsampling before measuring\nsimilarity or quality of images. This paper indicates to the right order of\nthese two procedures and that the existing models have not chosen the more\nefficient approach. In addition, efficiency of these metrics is not compared in\na fair basis in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.06152v1"
    },
    {
        "title": "Leveraging Contextual Cues for Generating Basketball Highlights",
        "authors": [
            "Vinay Bettadapura",
            "Caroline Pantofaru",
            "Irfan Essa"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The massive growth of sports videos has resulted in a need for automatic\ngeneration of sports highlights that are comparable in quality to the\nhand-edited highlights produced by broadcasters such as ESPN. Unlike previous\nworks that mostly use audio-visual cues derived from the video, we propose an\napproach that additionally leverages contextual cues derived from the\nenvironment that the game is being played in. The contextual cues provide\ninformation about the excitement levels in the game, which can be ranked and\nselected to automatically produce high-quality basketball highlights. We\nintroduce a new dataset of 25 NCAA games along with their play-by-play stats\nand the ground-truth excitement data for each basket. We explore the\ninformativeness of five different cues derived from the video and from the\nenvironment through user studies. Our experiments show that for our study\nparticipants, the highlights produced by our system are comparable to the ones\nproduced by ESPN for the same games.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.08955v1"
    },
    {
        "title": "Formal Definition of QoE Metrics",
        "authors": [
            "Tobias Hossfeld",
            "Poul E. Heegaard",
            "Martin Varela",
            "Sebastian Möller"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This technical report formally defines the QoE metrics which are introduced\nand discussed in the article \"QoE Beyond the MOS: An In-Depth Look at QoE via\nBetter Metrics and their Relation to MOS\" by Tobias Ho{\\ss}feld, Poul E.\nHeegaard, Martin Varela, Sebastian M\\\"oller, accepted for publication in the\nSpringer journal \"Quality and User Experience\". Matlab scripts for computing\nthe QoE metrics for given data sets are available in GitHub.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.00321v1"
    },
    {
        "title": "Dynamic Flow Scheduling Strategy in Multihoming Video CDNs",
        "authors": [
            "Ming Ma",
            "Zhi Wang",
            "Yankai Zhang",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Multihoming for a video Content Delivery Network (CDN) allows edge peering\nservers to deliver video chunks through different Internet Service Providers\n(ISPs), to achieve an improved quality of service (QoS) for video streaming\nusers. However, since traditional strategies for a multihoming video CDN are\nsimply designed according to static rules, e.g., simply sending traffic via a\nISP which is the same as the ISP of client, they fail to dynamically allocate\nresources among different ISPs over time. In this paper, we perform measurement\nstudies to demonstrate that such static allocation mechanism is inefficient to\nmake full utilization of multiple ISPs' resources. To address this problem, we\npropose a dynamic flow scheduling strategy for multihoming video CDN. The\nchallenge is to find the control parameters that can guide the ISP selection\nwhen performing flow scheduling. Using a data-driven approach, we find factors\nthat have a major impact on the performance improvement in the dynamic flow\nscheduling. We further utilize an information gain approach to generate\nparameter combinations that can be used to guide the flow scheduling, i.e., to\ndetermine the ISP each request should be responded by. Our evaluation results\ndemonstrate that our design effectively performs the flow scheduling. In\nparticular, our design yields near optimal performance in a simulation of\nreal-world multihoming setup.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.01261v1"
    },
    {
        "title": "Two RPG Flow-graphs for Software Watermarking using Bitonic Sequences of\n  Self-inverting Permutations",
        "authors": [
            "Anna Mpanti",
            "Stavros D. Nikolopoulos"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Software watermarking has received considerable attention and was adopted by\nthe software development community as a technique to prevent or discourage\nsoftware piracy and copyright infringement. A wide range of software\nwatermarking techniques has been proposed among which the graph-based methods\nthat encode watermarks as graph structures. Following up on our recently\nproposed methods for encoding watermark numbers $w$ as reducible permutation\nflow-graphs $F[\\pi^*]$ through the use of self-inverting permutations $\\pi^*$,\nin this paper, we extend the types of flow-graphs available for software\nwatermarking by proposing two different reducible permutation flow-graphs\n$F_1[\\pi^*]$ and $F_2[\\pi^*]$ incorporating important properties which are\nderived from the bitonic subsequences composing the self-inverting permutation\n$\\pi^*$. We show that a self-inverting permutation $\\pi^*$ can be efficiently\nencoded into either $F_1[\\pi^*]$ or $F_2[\\pi^*]$ and also efficiently decoded\nfrom theses graph structures. The proposed flow-graphs $F_1[\\pi^*]$ and\n$F_2[\\pi^*]$ enrich the repository of graphs which can encode the same\nwatermark number $w$ and, thus, enable us to embed multiple copies of the same\nwatermark $w$ into an application program $P$. Moreover, the enrichment of that\nrepository with new flow-graphs increases our ability to select a graph\nstructure more similar to the structure of a given application program $P$\nthereby enhancing the resilience of our codec system to attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02281v2"
    },
    {
        "title": "Hybrid Video Signal Coding Technologies: Past, Current and Future",
        "authors": [
            "Miaohui Wang",
            "Ngan King Ngi"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The growing needs for high-quality video applications have resulted in a lot\nof studies and developments in video signal coding. This chapter presents some\nadvanced techniques in enhancing the rate-distortion performance of the\nblock-based hybrid video coding systems. Additionally, as can be seen from the\ndevelopments of H.264/AVC and HEVC, most of the current coding tools, such as\nprediction, transformation and entropy coding, have less room to improve in the\ncompression performance. On the other hand, loop filer in the modern video\nstandards shows the promising results. Thus, we believe that loop filter can be\nthe candidate in contributing to higher video compression for the\nnext-generation video coding. Specifically, improvements on ALF and SAO are\nalso introduced, and the simulation results show that the proposed methods\noutperform the existing method, which offer new degrees of freedom to improve\nthe overall rate-distortion performance. As a result, they can be the candidate\ncoding tools for the next-generation video codec.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.05808v1"
    },
    {
        "title": "Restoring highly corrupted images by impulse noise using radial basis\n  functions interpolation",
        "authors": [
            "Fariborz Taherkhani",
            "Mansour Jamzad"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Preserving details in restoring images highly corrupted by impulse noise\nremains a challenging problem. We proposed an algorithm based on radial basis\nfunctions (RBF) interpolation which estimates the intensities of corrupted\npixels by their neighbors. In this algorithm, first intensity values of noisy\npixels in the corrupted image are estimated using RBFs. Next, the image is\nsmoothed. The proposed algorithm can effectively remove the highly dense\nimpulse noise. Experimental results show the superiority of the proposed\nalgorithm in comparison to the recent similar methods both in noise suppression\nand detail preservation. Extensive simulations show better results in measure\nof peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM),\nespecially when the image is corrupted by very highly dense impulse noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.06803v3"
    },
    {
        "title": "Skipping Selected Steps of DWT Computation in Lossless JPEG 2000 for\n  Improved Bitrates",
        "authors": [
            "Roman Starosolski"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  In order to improve bitrates of lossless JPEG 2000, we propose to modify the\ndiscrete wavelet transform (DWT) by skipping selected steps of its computation.\nWe employ a heuristic to construct the skipped steps DWT (SS-DWT) in an\nimage-adaptive way and define fixed SS-DWT variants. For a large and diverse\nset of images, we find that SS-DWT significantly improves bitrates of\nnon-photographic images. From a practical standpoint, the most interesting\nresults are obtained by applying entropy estimation of coding effects for\nselecting among the fixed SS-DWT variants. This way we get the compression\nscheme that, as opposed to the general SS-DWT case, is compliant with the JPEG\n2000 part 2 standard. It provides average bitrate improvement of roughly 5% for\nthe entire test-set, whereas the overall compression time becomes only 3%\ngreater than that of the unmodified JPEG 2000. Bitrates of photographic and\nnon-photographic images are improved by roughly 0.5% and 14%, respectively. At\na significantly increased cost of exploiting a heuristic, selecting the steps\nto be skipped based on the actual bitrate instead of an estimated one, and by\napplying reversible denoising and lifting steps to SS-DWT, we have attained\ngreater bitrate improvements of up to about 17.5% for non-photographic images.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.00613v2"
    },
    {
        "title": "Daala: Building A Next-Generation Video Codec From Unconventional\n  Technology",
        "authors": [
            "Jean-Marc Valin",
            "Timothy B. Terriberry",
            "Nathan E. Egge",
            "Thomas Daede",
            "Yushin Cho",
            "Christopher Montgomery",
            "Michael Bebenita"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Daala is a new royalty-free video codec that attempts to compete with\nstate-of-the-art royalty-bearing codecs. To do so, it must achieve good\ncompression while avoiding all of their patented techniques. We use technology\nthat is as different as possible from traditional approaches to achieve this.\nThis paper describes the technology behind Daala and discusses where it fits in\nthe newly created AV1 codec from the Alliance for Open Media. We show that\nDaala is approaching the performance level of more mature, state-of-the art\nvideo codecs and can contribute to improving AV1.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.01947v1"
    },
    {
        "title": "MT3S: Mobile Turkish Scene Text-to-Speech System for the Visually\n  Impaired",
        "authors": [
            "Muhammet Bastan",
            "Hilal Kandemir",
            "Busra Canturk"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Reading text is one of the essential needs of the visually impaired people.\nWe developed a mobile system that can read Turkish scene and book text, using a\nfast gradient-based multi-scale text detection algorithm for real-time\noperation and Tesseract OCR engine for character recognition. We evaluated the\nOCR accuracy and running time of our system on a new, publicly available mobile\nTurkish scene text dataset we constructed and also compared with\nstate-of-the-art systems. Our system proved to be much faster, able to run on a\nmobile device, with OCR accuracy comparable to the state-of-the-art.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05054v1"
    },
    {
        "title": "A Convolutional Neural Network Approach for Post-Processing in HEVC\n  Intra Coding",
        "authors": [
            "Yuanying Dai",
            "Dong Liu",
            "Feng Wu"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Lossy image and video compression algorithms yield visually annoying\nartifacts including blocking, blurring, and ringing, especially at low\nbit-rates. To reduce these artifacts, post-processing techniques have been\nextensively studied. Recently, inspired by the great success of convolutional\nneural network (CNN) in computer vision, some researches were performed on\nadopting CNN in post-processing, mostly for JPEG compressed images. In this\npaper, we present a CNN-based post-processing algorithm for High Efficiency\nVideo Coding (HEVC), the state-of-the-art video coding standard. We redesign a\nVariable-filter-size Residue-learning CNN (VRCNN) to improve the performance\nand to accelerate network training. Experimental results show that using our\nVRCNN as post-processing leads to on average 4.6% bit-rate reduction compared\nto HEVC baseline. The VRCNN outperforms previously studied networks in\nachieving higher bit-rate reduction, lower memory cost, and multiplied\ncomputational speedup.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.06690v2"
    },
    {
        "title": "Towards Hybrid Cloud-assisted Crowdsourced Live Streaming: Measurement\n  and Analysis",
        "authors": [
            "Cong Zhang",
            "Jiangchuan Liu",
            "Haiyang Wang"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Crowdsourced Live Streaming (CLS), most notably Twitch.tv, has seen explosive\ngrowth in its popularity in the past few years. In such systems, any user can\nlively broadcast video content of interest to others, e.g., from a game player\nto many online viewers. To fulfill the demands from both massive and\nheterogeneous broadcasters and viewers, expensive server clusters have been\ndeployed to provide video ingesting and transcoding services. Despite the\nexistence of highly popular channels, a significant portion of the channels is\nindeed unpopular. Yet as our measurement shows, these broadcasters are\nconsuming considerable system resources; in particular, 25% (resp. 30%) of\nbandwidth (resp. computation) resources are used by the broadcasters who do not\nhave any viewers at all. In this paper, we closely examine the challenge of\nhandling unpopular live-broadcasting channels in CLS systems and present a\ncomprehensive solution for service partitioning on hybrid cloud. The\ntrace-driven evaluation shows that our hybrid cloud-assisted design can smartly\nassign ingesting and transcoding tasks to the elastic cloud virtual machines,\nproviding flexible system deployment cost-effectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.00045v1"
    },
    {
        "title": "To Click or Not To Click: Automatic Selection of Beautiful Thumbnails\n  from Videos",
        "authors": [
            "Yale Song",
            "Miriam Redi",
            "Jordi Vallmitjana",
            "Alejandro Jaimes"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Thumbnails play such an important role in online videos. As the most\nrepresentative snapshot, they capture the essence of a video and provide the\nfirst impression to the viewers; ultimately, a great thumbnail makes a video\nmore attractive to click and watch. We present an automatic thumbnail selection\nsystem that exploits two important characteristics commonly associated with\nmeaningful and attractive thumbnails: high relevance to video content and\nsuperior visual aesthetic quality. Our system selects attractive thumbnails by\nanalyzing various visual quality and aesthetic metrics of video frames, and\nperforms a clustering analysis to determine the relevance to video content,\nthus making the resulting thumbnails more representative of the video. On the\ntask of predicting thumbnails chosen by professional video editors, we\ndemonstrate the effectiveness of our system against six baseline methods, using\na real-world dataset of 1,118 videos collected from Yahoo Screen. In addition,\nwe study what makes a frame a good thumbnail by analyzing the statistical\nrelationship between thumbnail frames and non-thumbnail frames in terms of\nvarious image quality features. Our study suggests that the selection of a good\nthumbnail is highly correlated with objective visual quality metrics, such as\nthe frame texture and sharpness, implying the possibility of building an\nautomatic thumbnail selection system based on visual aesthetics.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.01388v1"
    },
    {
        "title": "Optimal Representations for Adaptive Streaming in Interactive Multi-View\n  Video Systems",
        "authors": [
            "Laura Toni",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Interactive multi-view video streaming (IMVS) services permit to remotely\nimmerse within a 3D scene. This is possible by transmitting a set of reference\ncamera views (anchor views), which are used by the clients to freely navigate\nin the scene and possibly synthesize additional viewpoints of interest. From a\nnetworking perspective, the big challenge in IMVS systems is to deliver to each\nclient the best set of anchor views that maximizes the navigation quality,\nminimizes the view-switching delay and yet satisfies the network constraints.\nIntegrating adaptive streaming solutions in free-viewpoint systems offers a\npromising solution to deploy IMVS in large and heterogeneous scenarios, as long\nas the multi-view video representations on the server are properly selected. We\ntherefore propose to optimize the multi-view data at the server by minimizing\nthe overall resource requirements, yet offering a good navigation quality to\nthe different users. We propose a video representation set optimization for\nmultiview adaptive streaming systems and we show that it is NP-hard. We\ntherefore introduce the concept of multi-view navigation segment that permits\nto cast the video representation set selection as an integer linear programming\nproblem with a bounded computational complexity. We then show that the proposed\nsolution reduces the computational complexity while preserving optimality in\nmost of the 3D scenes. We then provide simulation results for different classes\nof users and show the gain offered by an optimal multi-view video\nrepresentation selection compared to recommended representation sets (e.g.,\nNetflix and Apple ones) or to a baseline representation selection algorithm\nwhere the encoding parameters are decided a priori for all the views.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.04196v1"
    },
    {
        "title": "Color-Based Coding Unit Level Adaptive Quantization for HEVC",
        "authors": [
            "Lee Prangnell",
            "Victor Sanchez"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  HEVC HM 16 includes a Coding Unit (CU) level perceptual quantization\ntechnique named AdaptiveQP. AdaptiveQP adjusts the Quantization Parameter (QP)\nat the CU level based on the spatial activity of samples in the four\nconstituent NxN sub-blocks of the luma Coding Block (CB), which is contained\nwithin a 2Nx2N CU. In this paper, we propose C-BAQ, which, in contrast to\nAdaptiveQP, adjusts the CU level QP according to the spatial activity of\nsamples in the four constituent NxN sub-blocks of both the luma and chroma CBs.\nBy computing the sum of luma, chroma Cb and chroma Cr spatial activity in a CU,\na richer reflection of spatial activity in the CU is attained. Therefore, a\nmore appropriate CU level QP can be selected, thus leading to important\nimprovements in terms of coding efficiency. We evaluate the proposed technique\nin HEVC HM 16.7 using 4:4:4, 4:2:2 and 4:2:0 YCbCr sequences. Both subjective\nand objective evaluations are undertaken during which we compare C-BAQ with\nAdaptiveQP. The objective evaluation reveals that C-BAQ attains a maximum\nBD-Rate reduction of 15.9% (Y), 13.1% (Cr) and 16.1% (Cb) in addition to a\nmaximum decoding time reduction of 11.0%.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.06302v2"
    },
    {
        "title": "Minimizing Compression Artifacts for High Resolutions with Adaptive\n  Quantization Matrices for HEVC",
        "authors": [
            "Lee Prangnell",
            "Victor Sanchez"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Visual Display Units (VDUs), capable of displaying video data at High\nDefinition (HD) and Ultra HD (UHD) resolutions, are frequently employed in a\nvariety of technological domains. Quantization-induced video compression\nartifacts, which are usually unnoticeable in low resolution environments, are\ntypically conspicuous on high resolution VDUs and video data. The default\nquantization matrices (QMs) in HEVC do not take into account specific display\nresolutions of VDUs or video data to determine the appropriate levels of\nquantization required to reduce unwanted compression artifacts. Therefore, we\npropose a novel, adaptive quantization matrix technique for the HEVC standard\nincluding Scalable HEVC (SHVC). Our technique, which is based on a refinement\nof the current QM technique in HEVC, takes into consideration specific display\nresolutions of the target VDUs in order to minimize compression artifacts. We\nundertake a thorough evaluation of the proposed technique by utilizing SHVC SHM\n9.0 (two-layered bit-stream) and the BD-Rate and SSIM metrics. For the BD-Rate\nevaluation, the proposed method achieves maximum BD-Rate reductions of 56.5% in\nthe enhancement layer. For the SSIM evaluation, our technique achieves a\nmaximum structural improvement of 0.8660 vs. 0.8538.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.06442v1"
    },
    {
        "title": "Multimedia Communication Quality Assessment Testbeds",
        "authors": [
            "Edip Demirbilek",
            "Jean-Charles Grégoire"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  We make an intensive use of multimedia frameworks in our research on modeling\nthe perceived quality estimation in streaming services and real-time\ncommunications. In our preliminary work, we have used the VLC VOD software to\ngenerate reference audiovisual files with various degree of coding and network\ndegradations. We have successfully built machine learning based models on the\nsubjective quality dataset we have generated using these files. However,\nimperfections in the dataset introduced by the multimedia framework we have\nused prevented us from achieving the full potential of these models.\n  In order to develop better models, we have re-created our end-to-end\nmultimedia pipeline using the GStreamer framework for audio and video\nstreaming. A GStreamer based pipeline proved to be significantly more robust to\nnetwork degradations than the VLC VOD framework and allowed us to stream a\nvideo flow at a loss rate up to 5\\% packet very easily. GStreamer has also\nenabled us to collect the relevant RTCP statistics that proved to be more\naccurate than network-deduced information. This dataset is free to the public.\nThe accuracy of the statistics eventually helped us to generate better\nperforming perceived quality estimation models.\n  In this paper, we present the implementation of these VLC and GStreamer-based\nmultimedia communication quality assessment testbeds with the references to\ntheir publicly available code bases.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.06612v1"
    },
    {
        "title": "Location-Based and Audience-Aware Storytelling",
        "authors": [
            "Jeff Burke",
            "Jared J. Stein"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  While the daily user of digital, Internet-enabled devices has some explicit\ncontrol over what they read and see, the providers fulfilling searches,\noffering options, and presenting material are using increasingly sophisticated\nreal-time algorithms that tune and target content for the particular user. They\nredefine the historical relationships between tellers and users, providing a\nresponsiveness paralleled only by forms of live performance incorporating\nelements of improvisation and audience interaction. The general accessibility\nof algorithmically driven content delivery techniques suggests significant\nuntapped potential for new approaches to narrative beyond advertising and\ncommercially orientated customization.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.07848v1"
    },
    {
        "title": "Viewport-Adaptive Navigable 360-Degree Video Delivery",
        "authors": [
            "Xavier Corbillon",
            "Gwendal Simon",
            "Alisa Devlic",
            "Jacob Chakareski"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The delivery and display of 360-degree videos on Head-Mounted Displays (HMDs)\npresents many technical challenges. 360-degree videos are ultra high resolution\nspherical videos, which contain an omnidirectional view of the scene. However\nonly a portion of this scene is displayed on the HMD. Moreover, HMD need to\nrespond in 10 ms to head movements, which prevents the server to send only the\ndisplayed video part based on client feedback. To reduce the bandwidth waste,\nwhile still providing an immersive experience, a viewport-adaptive 360-degree\nvideo streaming system is proposed. The server prepares multiple video\nrepresentations, which differ not only by their bit-rate, but also by the\nqualities of different scene regions. The client chooses a representation for\nthe next segment such that its bit-rate fits the available throughput and a\nfull quality region matches its viewing. We investigate the impact of various\nspherical-to-plane projections and quality arrangements on the video quality\ndisplayed to the user, showing that the cube map layout offers the best quality\nfor the given bit-rate budget. An evaluation with a dataset of users navigating\n360-degree videos demonstrates that segments need to be short enough to enable\nfrequent view switches.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.08042v2"
    },
    {
        "title": "Backward-Shifted Coding (BSC) based on Scalable Video Coding for HAS",
        "authors": [
            "Zakaria Ye",
            "Rachid El-Azouzi",
            "Tania Jimenez",
            "Francesco De Pellegrini"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The main task of HTTP Adaptive Streaming is to adapt video quality\ndynamically under variable network conditions. This is a key feature for\nmultimedia delivery especially when quality of service cannot be granted\nnetwork-wide and, e.g., throughput may suffer short term fluctuations.\n  Hence, robust bitrate adaptation schemes become crucial in order to improve\nvideo quality. The objective, in this context, is to control the filling level\nof the playback buffer and maximize the quality of the video, while avoiding\nunnecessary video quality variations.\n  In this paper we study bitrate adaptation algorithms based on\nBackward-Shifted Coding (BSC), a scalable video coding scheme able to greatly\nimprove video quality. We design bitrate adaptation algorithms that balance\nvideo rate smoothness and high network capacity utilization, leveraging both on\nthroughput-based and buffer-based adaptation mechanisms.\n  Extensive simulations using synthetic and real-world video traffic traces\nshow that the proposed scheme performs remarkably well even under challenging\nnetwork conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.02263v1"
    },
    {
        "title": "mPDF: Framework for Watermarking PDF Files using Image Watermarking\n  Algorithms",
        "authors": [
            "Sachin Mehta",
            "Balakrishnan Prabhakaran",
            "Rajarathnam Nallusamy",
            "Derrick Newton"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The advancement in digital technologies have made it possible to produce\nperfect copies of digital content. In this environment, malicious users\nreproduce the digital content and share it without compensation to the content\nowner. Content owners are concerned about the potential loss of revenue and\nreputation from piracy, especially when the content is available over the\nInternet. Digital watermarking has emerged as a deterrent measure towards such\nmalicious activities. Several methods have been proposed for copyright\nprotection and fingerprinting of digital images. However, these methods are not\napplicable to text documents as these documents lack rich texture information\nwhich is abundantly available in digital images. In this paper, a framework\n(mPDF) is proposed which facilitates the usage of digital image watermarking\nalgorithms on text documents. The proposed method divides a text document into\ntexture and non-texture blocks using an energy-based approach. After\nclassification, a watermark is embedded inside the texture blocks in a content\nadaptive manner. The proposed method is integrated with five known image\nwatermarking methods and its performance is studied in terms of quality and\nrobustness. Experiments are conducted on documents in 11 different languages.\nExperimental results clearly show that the proposed method facilitates the\nusage of image watermarking algorithms on text documents and is robust against\nattacks such as print & scan, print screen, and skew. Also, the proposed method\novercomes the drawbacks of existing text watermarking methods such as manual\ninspection and language dependency.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.02443v1"
    },
    {
        "title": "Perceptually-Driven Video Coding with the Daala Video Codec",
        "authors": [
            "Yushin Cho",
            "Thomas J. Daede",
            "Nathan E. Egge",
            "Guillaume Martres",
            "Tristan Matthews",
            "Christopher Montgomery",
            "Timothy B. Terriberry",
            "Jean-Marc Valin"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The Daala project is a royalty-free video codec that attempts to compete with\nthe best patent-encumbered codecs. Part of our strategy is to replace core\ntools of traditional video codecs with alternative approaches, many of them\ndesigned to take perceptual aspects into account, rather than optimizing for\nsimple metrics like PSNR. This paper documents some of our experiences with\nthese tools, which ones worked and which did not. We evaluate which tools are\neasy to integrate into a more traditional codec design, and show results in the\ncontext of the codec being developed by the Alliance for Open Media.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.02488v1"
    },
    {
        "title": "Saliency-Guided Complexity Control for HEVC Decoding",
        "authors": [
            "Ren Yang",
            "Mai Xu",
            "Zulin Wang",
            "Yiping Duan",
            "Xiaoming Tao"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The latest High Efficiency Video Coding (HEVC) standard significantly\nimproves coding efficiency over its previous video coding standards. The\nexpense of such improvement is enormous computational complexity, from both\nencoding and decoding sides. Since computational capability and power capacity\nare diverse across portable devices, it is necessary to reduce decoding\ncomplexity to a target with tolerable quality loss, so called complexity\ncontrol. This paper proposes a Saliency-Guided Complexity Control (SGCC)\napproach for HEVC decoding, which reduces the decoding complexity to the target\nwith minimal perceptual quality loss. First, we establish the SGCC formulation\nto minimize perceptual quality loss at the constraint on reduced decoding\ncomplexity, which is achieved via disabling Deblocking Filter (DF) and\nsimplifying Motion Compensation (MC) of some non-salient Coding Tree Units\n(CTUs). One important component in this formulation is the modelled\nrelationship between decoding complexity reduction and DF disabling/MC\nsimplification, which determines the control accuracy of our approach. Another\ncomponent is the modelled relationship between quality loss and DF disabling/MC\nsimplification, responsible for optimizing perceptual quality. By solving the\nSGCC formulation for a given target complexity, we can obtain the DF and MC\nsettings of each CTU, and then decoding complexity can be reduced to the\ntarget. Finally, the experimental results validate the effectiveness of our\nSGCC approach, from the aspects of control performance, complexity-distortion\nperformance, fluctuation of quality loss and subjective quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.02516v5"
    },
    {
        "title": "An Efficient Adaptive Boundary Matching Algorithm for Video Error\n  Concealment",
        "authors": [
            "Seyed Mojtaba Marvasti-Zadeh",
            "Hossein Ghanei-Yakhdan",
            "Shohreh Kasaei"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Sending compressed video data in error-prone environments (like the Internet\nand wireless networks) might cause data degradation. Error concealment\ntechniques try to conceal the received data in the decoder side. In this paper,\nan adaptive boundary matching algorithm is presented for recovering the damaged\nmotion vectors (MVs). This algorithm uses an outer boundary matching or\ndirectional temporal boundary matching method to compare every boundary of\ncandidate macroblocks (MBs), adaptively. It gives a specific weight according\nto the accuracy of each boundary of the damaged MB. Moreover, if each of the\nadjacent MBs is already concealed, different weights are given to the\nboundaries. Finally, the MV with minimum adaptive boundary distortion is\nselected as the MV of the damaged MB. Experimental results show that the\nproposed algorithm can improve both objective and subjective quality of\nreconstructed frames without any considerable computational complexity. The\naverage PSNR in some frames of test sequences increases about 5.20, 5.78, 5.88,\n4.37, 4.41, and 3.50 dB compared to average MV, classic boundary matching,\ndirectional boundary matching, directional temporal boundary matching, outer\nboundary matching, and dynamical temporal error concealment algorithm,\nrespectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.07386v1"
    },
    {
        "title": "Video transrating in AVC and HEVC transcoding",
        "authors": [
            "Krzysztof Wegner",
            "Tomasz Grajek",
            "Jakub Stankowski",
            "Marek Domanski"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  HEVC (MPEG-H Part 2 and H.265) is a new coding technology which is expected\nto be deployed on the market along with new video services in the near future.\nHEVC is a successor of currently widely used AVC (MPEG-4 Part 10 and H.264). In\nthis paper, the quality coding gains obtained for the Cascaded Pixel Domain\nTranscoder of AVC-coded material to HEVC standard are reported. Extensive\nexperiments showed that transcoding with bitrate reduction allows the\nachievement of better rate-distortion performance than by compressing an\noriginal video sequence with the use of AVC at the same (reduced) bitrate.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00190v1"
    },
    {
        "title": "Learning to Predict Streaming Video QoE: Distortions, Rebuffering and\n  Memory",
        "authors": [
            "Christos G. Bampis",
            "Alan C. Bovik"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Mobile streaming video data accounts for a large and increasing percentage of\nwireless network traffic. The available bandwidths of modern wireless networks\nare often unstable, leading to difficulties in delivering smooth, high-quality\nvideo. Streaming service providers such as Netflix and YouTube attempt to adapt\ntheir systems to adjust in response to these bandwidth limitations by changing\nthe video bitrate or, failing that, allowing playback interruptions\n(rebuffering). Being able to predict end user' quality of experience (QoE)\nresulting from these adjustments could lead to perceptually-driven network\nresource allocation strategies that would deliver streaming content of higher\nquality to clients, while being cost effective for providers. Existing\nobjective QoE models only consider the effects on user QoE of video quality\nchanges or playback interruptions. For streaming applications, adaptive network\nstrategies may involve a combination of dynamic bitrate allocation along with\nplayback interruptions when the available bandwidth reaches a very low value.\nTowards effectively predicting user QoE, we propose Video Assessment of\nTemporaL Artifacts and Stalls (Video ATLAS): a machine learning framework where\nwe combine a number of QoE-related features, including objective quality\nfeatures, rebuffering-aware features and memory-driven features to make QoE\npredictions. We evaluated our learning-based QoE prediction model on the\nrecently designed LIVE-Netflix Video QoE Database which consists of practical\nplayout patterns, where the videos are afflicted by both quality changes and\nrebuffering events, and found that it provides improved performance over\nstate-of-the-art video quality metrics while generalizing well on different\ndatasets. The proposed algorithm is made publicly available at\nhttp://live.ece.utexas.edu/research/Quality/VideoATLAS release_v2.rar.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00633v1"
    },
    {
        "title": "LSB Matching Steganalysis Based on Patterns of Pixel Differences and\n  Random Embedding",
        "authors": [
            "Daniel Lerch-Hostalot",
            "David Megías"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  This paper presents a novel method for detection of LSB matching steganogra-\nphy in grayscale images. This method is based on the analysis of the\ndifferences between neighboring pixels before and after random data embedding.\nIn natu- ral images, there is a strong correlation between adjacent pixels.\nThis correla- tion is disturbed by LSB matching generating new types of\ncorrelations. The pre- sented method generates patterns from these correlations\nand analyzes their varia- tion when random data are hidden. The experiments\nperformed for two different image databases show that the method yields better\nclassification accuracy com- pared to prior art for both LSB matching and HUGO\nsteganography. In addition, although the method is designed for the spatial\ndomain, some experiments show its applicability also for detecting JPEG\nsteganography.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00817v1"
    },
    {
        "title": "Learning from Experience: A Dynamic Closed-Loop QoE Optimization for\n  Video Adaptation and Delivery",
        "authors": [
            "Imen Triki",
            "Quanyan Zhu",
            "Rachid Elazouzi",
            "Majed Haddad",
            "Zhiheng Xu"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The quality of experience (QoE) is known to be subjective and\ncontext-dependent. Identifying and calculating the factors that affect QoE is\nindeed a difficult task. Recently, a lot of effort has been devoted to estimate\nthe users QoE in order to improve video delivery. In the literature, most of\nthe QoE-driven optimization schemes that realize trade-offs among different\nquality metrics have been addressed under the assumption of homogenous\npopulations. Nevertheless, people perceptions on a given video quality may not\nbe the same, which makes the QoE optimization harder. This paper aims at taking\na step further in order to address this limitation and meet users profiles. To\ndo so, we propose a closed-loop control framework based on the\nusers(subjective) feedbacks to learn the QoE function and optimize it at the\nsame time. Our simulation results show that our system converges to a steady\nstate, where the resulting QoE function noticeably improves the users\nfeedbacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.01986v3"
    },
    {
        "title": "A Convolutional Neural Network Approach for Half-Pel Interpolation in\n  Video Coding",
        "authors": [
            "Ning Yan",
            "Dong Liu",
            "Houqiang Li",
            "Feng Wu"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Motion compensation is a fundamental technology in video coding to remove the\ntemporal redundancy between video frames. To further improve the coding\nefficiency, sub-pel motion compensation has been utilized, which requires\ninterpolation of fractional samples. The video coding standards usually adopt\nfixed interpolation filters that are derived from the signal processing theory.\nHowever, as video signal is not stationary, the fixed interpolation filters may\nturn out less efficient. Inspired by the great success of convolutional neural\nnetwork (CNN) in computer vision, we propose to design a CNN-based\ninterpolation filter (CNNIF) for video coding. Different from previous studies,\none difficulty for training CNNIF is the lack of ground-truth since the\nfractional samples are actually not available. Our solution for this problem is\nto derive the \"ground-truth\" of fractional samples by smoothing high-resolution\nimages, which is verified to be effective by the conducted experiments.\nCompared to the fixed half-pel interpolation filter for luma in High Efficiency\nVideo Coding (HEVC), our proposed CNNIF achieves up to 3.2% and on average 0.9%\nBD-rate reduction under low-delay P configuration.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.03502v1"
    },
    {
        "title": "Towards Wi-Fi AP-Assisted Content Prefetching for On-Demand TV Series: A\n  Reinforcement Learning Approach",
        "authors": [
            "Wen Hu",
            "Yichao Jin",
            "Yonggang Wen",
            "Zhi Wang",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The emergence of smart Wi-Fi APs (Access Point), which are equipped with huge\nstorage space, opens a new research area on how to utilize these resources at\nthe edge network to improve users' quality of experience (QoE) (e.g., a short\nstartup delay and smooth playback). One important research interest in this\narea is content prefetching, which predicts and accurately fetches contents\nahead of users' requests to shift the traffic away during peak periods.\nHowever, in practice, the different video watching patterns among users, and\nthe varying network connection status lead to the time-varying server load,\nwhich eventually makes the content prefetching problem challenging. To\nunderstand this challenge, this paper first performs a large-scale measurement\nstudy on users' AP connection and TV series watching patterns using\nreal-traces. Then, based on the obtained insights, we formulate the content\nprefetching problem as a Markov Decision Process (MDP). The objective is to\nstrike a balance between the increased prefetching&storage cost incurred by\nincorrect prediction and the reduced content download delay because of\nsuccessful prediction. A learning-based approach is proposed to solve this\nproblem and another three algorithms are adopted as baselines. In particular,\nfirst, we investigate the performance lower bound by using a random algorithm,\nand the upper bound by using an ideal offline approach. Then, we present a\nheuristic algorithm as another baseline. Finally, we design a reinforcement\nlearning algorithm that is more practical to work in the online manner. Through\nextensive trace-based experiments, we demonstrate the performance gain of our\ndesign. Remarkably, our learning-based algorithm achieves a better precision\nand hit ratio (e.g., 80%) with about 70% (resp. 50%) cost saving compared to\nthe random (resp. heuristic) algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.03530v1"
    },
    {
        "title": "Image denoising by median filter in wavelet domain",
        "authors": [
            "Afrah Ramadhan",
            "Firas Mahmood",
            "Atilla Elci"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The details of an image with noise may be restored by removing noise through\na suitable image de-noising method. In this research, a new method of image\nde-noising based on using median filter (MF) in the wavelet domain is proposed\nand tested. Various types of wavelet transform filters are used in conjunction\nwith median filter in experimenting with the proposed approach in order to\nobtain better results for image de-noising process, and, consequently to select\nthe best suited filter. Wavelet transform working on the frequencies of\nsub-bands split from an image is a powerful method for analysis of images.\nAccording to this experimental work, the proposed method presents better\nresults than using only wavelet transform or median filter alone. The MSE and\nPSNR values are used for measuring the improvement in de-noised images.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.06499v1"
    },
    {
        "title": "Theoretical Evaluation of Li et al.'s Approach for Improving a Binary\n  Watermark-Based Scheme in Remote Sensing Data Communications",
        "authors": [
            "Mohammad Reza Khosravi",
            "Mohammad Kazem Moghimi"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  This letter is about a principal weakness of the published article by Li et\nal. in 2014. It seems that the mentioned work has a terrible conceptual mistake\nwhile presenting its theoretical approach. In fact, the work has tried to\ndesign a new attack and its effective solution for a basic watermarking\nalgorithm by Zhu et al. published in 2013, however in practice, we show the Li\net al.'s approach is not correct to obtain the aim. For disproof of the\nincorrect approach, we only apply a numerical example as the counterexample of\nthe Li et al.'s approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.09103v1"
    },
    {
        "title": "Detection of Copy-move Image forgery using SVD and Cuckoo Search\n  Algorithm",
        "authors": [
            "Abhishek Kashyap",
            "Megha Agarwal",
            "Hariom Gupta"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Copy-move forgery is one of the simple and effective operations to create\nforged images. Recently, techniques based on singular value decomposition (SVD)\nare widely used to detect copy-move forgery (CMF). Some approaches based on SVD\nare most acceptable to detect copy-move forgery but some copy-move forgery\ndetection approaches can not produce satisfactory detection results. Sometimes\nthese approaches may even produce error results. According to our observation,\ndetection result produced using SVD depend highly on those parameters whose\nvalues are often determined with experiences. These values are only applicable\nto a few images, which limit their application. To solve this problem, a novel\napproach named as copy-move forgery detection using Cuckoo search algorithm\n(CMFD-CS) is proposed in this paper. CMFD-CS integrates the CS algorithm into\nSVD. It utilizes the CS algorithm to generate customized parameter values for\nimages, which are used CMFD under block-based framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.00631v1"
    },
    {
        "title": "CCL: Cross-modal Correlation Learning with Multi-grained Fusion by\n  Hierarchical Network",
        "authors": [
            "Yuxin Peng",
            "Jinwei Qi",
            "Xin Huang",
            "Yuxin Yuan"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Cross-modal retrieval has become a highlighted research topic for retrieval\nacross multimedia data such as image and text. A two-stage learning framework\nis widely adopted by most existing methods based on Deep Neural Network (DNN):\nThe first learning stage is to generate separate representation for each\nmodality, and the second learning stage is to get the cross-modal common\nrepresentation. However, the existing methods have three limitations: (1) In\nthe first learning stage, they only model intra-modality correlation, but\nignore inter-modality correlation with rich complementary context. (2) In the\nsecond learning stage, they only adopt shallow networks with single-loss\nregularization, but ignore the intrinsic relevance of intra-modality and\ninter-modality correlation. (3) Only original instances are considered while\nthe complementary fine-grained clues provided by their patches are ignored. For\naddressing the above problems, this paper proposes a cross-modal correlation\nlearning (CCL) approach with multi-grained fusion by hierarchical network, and\nthe contributions are as follows: (1) In the first learning stage, CCL exploits\nmulti-level association with joint optimization to preserve the complementary\ncontext from intra-modality and inter-modality correlation simultaneously. (2)\nIn the second learning stage, a multi-task learning strategy is designed to\nadaptively balance the intra-modality semantic category constraints and\ninter-modality pairwise similarity constraints. (3) CCL adopts multi-grained\nmodeling, which fuses the coarse-grained instances and fine-grained patches to\nmake cross-modal correlation more precise. Comparing with 13 state-of-the-art\nmethods on 6 widely-used cross-modal datasets, the experimental results show\nour CCL approach achieves the best performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.02116v4"
    },
    {
        "title": "An Overview of Cross-media Retrieval: Concepts, Methodologies,\n  Benchmarks and Challenges",
        "authors": [
            "Yuxin Peng",
            "Xin Huang",
            "Yunzhen Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Multimedia retrieval plays an indispensable role in big data utilization.\nPast efforts mainly focused on single-media retrieval. However, the\nrequirements of users are highly flexible, such as retrieving the relevant\naudio clips with one query of image. So challenges stemming from the \"media\ngap\", which means that representations of different media types are\ninconsistent, have attracted increasing attention. Cross-media retrieval is\ndesigned for the scenarios where the queries and retrieval results are of\ndifferent media types. As a relatively new research topic, its concepts,\nmethodologies and benchmarks are still not clear in the literatures. To address\nthese issues, we review more than 100 references, give an overview including\nthe concepts, methodologies, major challenges and open issues, as well as build\nup the benchmarks including datasets and experimental results. Researchers can\ndirectly adopt the benchmarks to promptly evaluate their proposed methods. This\nwill help them to focus on algorithm design, rather than the time-consuming\ncompared methods and results. It is noted that we have constructed a new\ndataset XMedia, which is the first publicly available dataset with up to five\nmedia types (text, image, video, audio and 3D model). We believe this overview\nwill attract more researchers to focus on cross-media retrieval and be helpful\nto them.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.02223v4"
    },
    {
        "title": "A Synchronization Algorithm Based on Moving Average for Robust Audio\n  Watermarking Scheme",
        "authors": [
            "Zhang Jin-quan",
            "Han Bin"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  A synchronization code scheme based on moving average is proposed for robust\naudio watermarking in the paper. Two proper positive integers are chosen to\ncompute the moving average sequence by sliding one sample every time. The\nsynchronization bits are embedded at crosses of the two moving average\nsequences with the quantization index modulation. The experimental results show\nthat the proposed watermarking scheme maintains high audio quality and is\nrobust to common attacks such as additive white Gaussian noise, re-sampling,\nlow-pass filtering, random cropping, MP3 compression, jitter attack and time\nscale modification. Simultaneously, the algorithm has high search efficiency\nand low false alarm rate.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.02754v1"
    },
    {
        "title": "Robust Audio Watermarking Algorithm Based on Moving Average and DCT",
        "authors": [
            "Jinquan Zhang",
            "Bin Han"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Noise is often brought to host audio by common signal processing operation,\nand it usually changes the high-frequency component of an audio signal. So\nembedding watermark by adjusting low-frequency coefficient can improve the\nrobustness of a watermark scheme. Moving Average sequence is a low-frequency\nfeature of an audio signal. This work proposed a method which embedding\nwatermark into the maximal coefficient in discrete cosine transform domain of a\nmoving average sequence. Subjective and objective tests reveal that the\nproposed watermarking scheme maintains highly audio quality, and\nsimultaneously, the algorithm is highly robust to common digital signal\nprocessing operations, including additive noise, sampling rate change, bit\nresolution transformation, MP3 compression, and random cropping, especially\nlow-pass filtering.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.02755v1"
    },
    {
        "title": "A Robust Blind Watermarking Using Convolutional Neural Network",
        "authors": [
            "Seung-Min Mun",
            "Seung-Hun Nam",
            "Han-Ul Jang",
            "Dongkyu Kim",
            "Heung-Kyu Lee"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  This paper introduces a blind watermarking based on a convolutional neural\nnetwork (CNN). We propose an iterative learning framework to secure robustness\nof watermarking. One loop of learning process consists of the following three\nstages: Watermark embedding, attack simulation, and weight update. We have\nlearned a network that can detect a 1-bit message from a image sub-block.\nExperimental results show that this learned network is an extension of the\nfrequency domain that is widely used in existing watermarking scheme. The\nproposed scheme achieved robustness against geometric and signal processing\nattacks with a learning time of one day.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.03248v1"
    },
    {
        "title": "FISF: Better User Experience using Smaller Bandwidth for Panoramic\n  Virtual Reality Video",
        "authors": [
            "Lun Wang",
            "Damai Dai",
            "Jie Jiang",
            "Tong Yang",
            "Xiaoke Jiang",
            "Zekun Cai",
            "Yang Li",
            "Xiaoming Li"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The panoramic video is widely used to build virtual reality (VR) and is\nexpected to be one of the next generation Killer-Apps. Transmitting panoramic\nVR videos is a challenging task because of two problems: 1) panoramic VR videos\nare typically much larger than normal videos but they need to be transmitted\nwith limited bandwidth in mobile networks. 2) high-resolution and fluent views\nshould be provided to guarantee a superior user experience and avoid\nside-effects such as dizziness and nausea. To address these two problems, we\npropose a novel interactive streaming technology, namely Focus-based\nInteractive Streaming Framework (FISF). FISF consists of three parts: 1) we use\nthe classic clustering algorithm DBSCAN to analyze real user data for Video\nFocus Detection (VFD); 2) we propose a Focus-based Interactive Streaming\nTechnology (FIST), including a static version and a dynamic version; 3) we\npropose two optimization methods: focus merging and prefetch strategy.\nExperimental results show that FISF significantly outperforms the\nstate-of-the-art. The paper is submitted to Sigcomm 2017, VR/AR Network on 31\nMar 2017 at 10:44:04am EDT.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.06444v1"
    },
    {
        "title": "Deep Convolutional Neural Network to Detect J-UNIWARD",
        "authors": [
            "Guanshuo Xu"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  This paper presents an empirical study on applying convolutional neural\nnetworks (CNNs) to detecting J-UNIWARD, one of the most secure JPEG\nsteganographic method. Experiments guiding the architectural design of the CNNs\nhave been conducted on the JPEG compressed BOSSBase containing 10,000 covers of\nsize 512x512. Results have verified that both the pooling method and the depth\nof the CNNs are critical for performance. Results have also proved that a\n20-layer CNN, in general, outperforms the most sophisticated feature-based\nmethods, but its advantage gradually diminishes on hard-to-detect cases. To\nshow that the performance generalizes to large-scale databases and to different\ncover sizes, one experiment has been conducted on the CLS-LOC dataset of\nImageNet containing more than one million covers cropped to unified size of\n256x256. The proposed 20-layer CNN has cut the error achieved by a CNN recently\nproposed for large-scale JPEG steganalysis by 35%. Source code is available via\nGitHub: https://github.com/GuanshuoXu/deep_cnn_jpeg_steganalysis\n",
        "pdf_link": "http://arxiv.org/pdf/1704.08378v1"
    },
    {
        "title": "Co-projection-plane based 3-D padding for polyhedron projection for\n  360-degree video",
        "authors": [
            "Li Li",
            "Zhu Li",
            "Xiang Ma",
            "Haitao Yang",
            "Houqiang Li"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The polyhedron projection for 360-degree video is becoming more and more\npopular since it can lead to much less geometry distortion compared with the\nequirectangular projection. However, in the polyhedron projection, we can\nobserve very obvious texture discontinuity in the area near the face boundary.\nSuch a texture discontinuity may lead to serious quality degradation when\nmotion compensation crosses the discontinuous face boundary. To solve this\nproblem, in this paper, we first propose to fill the corresponding neighboring\nfaces in the suitable positions as the extension of the current face to keep\napproximated texture continuity. Then a co-projection-plane based 3-D padding\nmethod is proposed to project the reference pixels in the neighboring face to\nthe current face to guarantee exact texture continuity. Under the proposed\nscheme, the reference pixel is always projected to the same plane with the\ncurrent pixel when performing motion compensation so that the texture\ndiscontinuity problem can be solved. The proposed scheme is implemented in the\nreference software of High Efficiency Video Coding. Compared with the existing\nmethod, the proposed algorithm can significantly improve the rate-distortion\nperformance. The experimental results obviously demonstrate that the texture\ndiscontinuity in the face boundary can be well handled by the proposed\nalgorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.08768v1"
    },
    {
        "title": "Optimum Decoder for Multiplicative Spread Spectrum Image Watermarking\n  with Laplacian Modeling",
        "authors": [
            "Nematollah Zarmehi",
            "Mohammad Reza Aref"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  This paper investigates the multiplicative spread spectrum watermarking\nmethod for the image. The information bit is spreaded into middle-frequency\nDiscrete Cosine Transform (DCT) coefficients of each block of an image using a\ngenerated pseudo-random sequence. Unlike the conventional signal modeling, we\nsuppose that both signal and noise are distributed with Laplacian distribution\nbecause the sample loss of digital media can be better modeled with this\ndistribution than the Gaussian one. We derive the optimum decoder for the\nproposed embedding method thanks to the maximum likelihood decoding scheme. We\nalso analyze our watermarking system in the presence of noise and provide\nanalytical evaluations and several simulations. The results show that it has\nthe suitable performance and transparency required for watermarking\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.00726v1"
    },
    {
        "title": "Towards Predictions of the Image Quality of Experience for Augmented\n  Reality Scenarios",
        "authors": [
            "Brian Bauman",
            "Patrick Seeling"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Augmented Reality (AR) devices are commonly head-worn to overlay\ncontext-dependent information into the field of view of the device operators.\nOne particular scenario is the overlay of still images, either in a traditional\nfashion, or as spherical, i.e., immersive, content. For both media types, we\nevaluate the interplay of user ratings as Quality of Experience (QoE) with (i)\nthe non-referential BRISQUE objective image quality metric and (ii) human\nsubject dry electrode EEG signals gathered with a commercial device.\nAdditionally, we employ basic machine learning approaches to assess the\npossibility of QoE predictions based on rudimentary subject data. Corroborating\nprior research for the overall scenario, we find strong correlations for both\napproaches with user ratings as Mean Opinion Scores, which we consider as QoE\nmetric. In prediction scenarios based on data subsets, we find good performance\nfor the objective metric as well as the EEG-based approach. While the objective\nmetric can yield high QoE prediction accuracies overall, it is limited i its\napplication for individual subjects. The subject-based EEG approach, on the\nother hand, enables good predictability of the QoE for both media types, but\nwith better performance for regular content. Our results can be employed in\npractical scenarios by content and network service providers to optimize the\nuser experience in augmented reality scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01123v1"
    },
    {
        "title": "A Comparative Case Study of HTTP Adaptive Streaming Algorithms in Mobile\n  Networks",
        "authors": [
            "Theodoros Karagkioules",
            "Dimitrios Tsilimantos",
            "Cyril Concolato",
            "Stefan Valentin"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  HTTP Adaptive Streaming (HAS) techniques are now the dominant solution for\nvideo delivery in mobile networks. Over the past few years, several HAS\nalgorithms have been introduced in order to improve user quality-of-experience\n(QoE) by bit-rate adaptation. Their difference is mainly the required input\ninformation, ranging from network characteristics to application-layer\nparameters such as the playback buffer. Interestingly, despite the recent\noutburst in scientific papers on the topic, a comprehensive comparative study\nof the main algorithm classes is still missing. In this paper we provide such\ncomparison by evaluating the performance of the state-of-the-art HAS algorithms\nper class, based on data from field measurements. We provide a systematic study\nof the main QoE factors and the impact of the target buffer level. We conclude\nthat this target buffer level is a critical classifier for the studied HAS\nalgorithms. While buffer-based algorithms show superior QoE in most of the\ncases, their performance may differ at the low target buffer levels of live\nstreaming services. Overall, we believe that our findings provide valuable\ninsight for the design and choice of HAS algorithms according to networks\nconditions and service requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01762v1"
    },
    {
        "title": "A Hybrid Approach to Video Source Identification",
        "authors": [
            "Massimo Iuliani",
            "Marco Fontani",
            "Dasara Shullani",
            "Alessandro Piva"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Multimedia Forensics allows to determine whether videos or images have been\ncaptured with the same device, and thus, eventually, by the same person.\nCurrently, the most promising technology to achieve this task, exploits the\nunique traces left by the camera sensor into the visual content. Anyway, image\nand video source identification are still treated separately from one another.\nThis approach is limited and anachronistic if we consider that most of the\nvisual media are today acquired using smartphones, that capture both images and\nvideos. In this paper we overcome this limitation by exploring a new approach\nthat allows to synergistically exploit images and videos to study the device\nfrom which they both come. Indeed, we prove it is possible to identify the\nsource of a digital video by exploiting a reference sensor pattern noise\ngenerated from still images taken by the same device of the query video. The\nproposed method provides comparable or even better performance, when compared\nto the current video identification strategies, where a reference pattern is\nestimated from video frames. We also show how this strategy can be effective\neven in case of in-camera digitally stabilized videos, where a non-stabilized\nreference is not available, by solving some state-of-the-art limitations. We\nexplore a possible direct application of this result, that is social media\nprofile linking, i.e. discovering relationships between two or more social\nmedia profiles by comparing the visual contents - images or videos - shared\ntherein.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01854v1"
    },
    {
        "title": "Optimized Data Representation for Interactive Multiview Navigation",
        "authors": [
            "Rui Ma",
            "Thomas Maugey",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In contrary to traditional media streaming services where a unique media\ncontent is delivered to different users, interactive multiview navigation\napplications enable users to choose their own viewpoints and freely navigate in\na 3-D scene. The interactivity brings new challenges in addition to the\nclassical rate-distortion trade-off, which considers only the compression\nperformance and viewing quality. On the one hand, interactivity necessitates\nsufficient viewpoints for richer navigation; on the other hand, it requires to\nprovide low bandwidth and delay costs for smooth navigation during view\ntransitions. In this paper, we formally describe the novel trade-offs posed by\nthe navigation interactivity and classical rate-distortion criterion. Based on\nan original formulation, we look for the optimal design of the data\nrepresentation by introducing novel rate and distortion models and practical\nsolving algorithms. Experiments show that the proposed data representation\nmethod outperforms the baseline solution by providing lower resource\nconsumptions and higher visual quality in all navigation configurations, which\ncertainly confirms the potential of the proposed data representation in\npractical interactive navigation systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.02940v3"
    },
    {
        "title": "Spatiotemporal Rate Adaptive Tiled Scheme for 360 Sports Events",
        "authors": [
            "Tarek El-Ganainy"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The recent rise of interest in Virtual Reality (VR) came with the\navailability of commodity commercial VR products, such as the Head Mounted\nDisplays (HMD) created by Oculus and other vendors. One of the main\napplications of virtual reality that has been recently adopted is streaming\nsports events. For instance, the last olympics held in Rio De Janeiro was\nstreamed over the Internet for users to view on VR headsets or using 360 video\nplayers. A big challenge for streaming VR sports events is the users limited\nbandwidth and the amount of data required to transmit 360 videos. While 360\nvideo demands high bandwidth, at any time instant users are only viewing a\nsmall portion of the video according to the HMD field of view (FOV). Many\napproaches have been proposed in the literature such as proposing new\nrepresentations (e.g. pyramid and offset-cubemap) and tiling the video and\nstreaming the tiles currently being viewed. In this paper, we propose a tiled\nstreaming framework, where we provide a degrading quality model similar to the\nstate-of-the-art offset-cubemap while minimizing its storage requirements at\nthe server side. We conduct objective studies showing the effectiveness of our\napproach providing smooth degradation of quality from the user FOV to the back\nof the 360 space. In addition, we conduct subjective studies showing that users\ntend to prefer our proposed scheme over offset-cubemap in low bandwidth\nconnections, and they don't feel difference for higher bandwidth connections.\nThat is, we achieve better perceived quality with huge storage savings up to\n670%.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.04911v1"
    },
    {
        "title": "StegIbiza: Steganography in Club Music Implemented in Python",
        "authors": [
            "Krzysztof Szczypiorski",
            "Wojciech Zydecki"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  This paper introduces the implementation of steganography method called\nStegIbiza, which uses tempo modulation as hidden message carrier. With the use\nof Python scripting language, a bit string was encoded and decoded using WAV\nand MP3 files. Once the message was hidden into a music files, an internet\nradio was created to evaluate broadcast possibilities. No dedicated music or\nsignal processing equipment was used in this StegIbiza implementation\n",
        "pdf_link": "http://arxiv.org/pdf/1705.07788v1"
    },
    {
        "title": "Traffic Profiling for Mobile Video Streaming",
        "authors": [
            "Dimitrios Tsilimantos",
            "Theodoros Karagkioules",
            "Amaya Nogales-Gómez",
            "Stefan Valentin"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  This paper describes a novel system that provides key parameters of HTTP\nAdaptive Streaming (HAS) sessions to the lower layers of the protocol stack. A\nnon-intrusive traffic profiling solution is proposed that observes packet flows\nat the transmit queue of base stations, edge-routers, or gateways. By analyzing\nIP flows in real time, the presented scheme identifies different phases of an\nHAS session and estimates important application-layer parameters, such as\nplay-back buffer state and video encoding rate. The introduced estimators only\nuse IP-layer information, do not require standardization and work even with\ntraffic that is encrypted via Transport Layer Security (TLS). Experimental\nresults for a popular video streaming service clearly verify the high accuracy\nof the proposed solution. Traffic profiling, thus, provides a valuable\nalternative to cross-layer signaling and Deep Packet Inspection (DPI) in order\nto perform efficient network optimization for video streaming.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.08733v1"
    },
    {
        "title": "Fast MPEG-CDVS Encoder with GPU-CPU Hybrid Computing",
        "authors": [
            "Lingyu Duan",
            "Wei Sun",
            "Xinfeng Zhang",
            "Shiqi Wang",
            "Jie Chen",
            "Jianxiong Yin",
            "Simon See",
            "Tiejun Huang",
            "Alex C. Kot",
            "Wen Gao"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The compact descriptors for visual search (CDVS) standard from ISO/IEC moving\npictures experts group (MPEG) has succeeded in enabling the interoperability\nfor efficient and effective image retrieval by standardizing the bitstream\nsyntax of compact feature descriptors. However, the intensive computation of\nCDVS encoder unfortunately hinders its widely deployment in industry for\nlarge-scale visual search. In this paper, we revisit the merits of low\ncomplexity design of CDVS core techniques and present a very fast CDVS encoder\nby leveraging the massive parallel execution resources of GPU. We elegantly\nshift the computation-intensive and parallel-friendly modules to the\nstate-of-the-arts GPU platforms, in which the thread block allocation and the\nmemory access are jointly optimized to eliminate performance loss. In addition,\nthose operations with heavy data dependence are allocated to CPU to resolve the\nextra but non-necessary computation burden for GPU. Furthermore, we have\ndemonstrated the proposed fast CDVS encoder can work well with those\nconvolution neural network approaches which has harmoniously leveraged the\nadvantages of GPU platforms, and yielded significant performance improvements.\nComprehensive experimental results over benchmarks are evaluated, which has\nshown that the fast CDVS encoder using GPU-CPU hybrid computing is promising\nfor scalable visual search.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.09776v2"
    },
    {
        "title": "MM2RTB: Bringing Multimedia Metrics to Real-Time Bidding",
        "authors": [
            "Xiang Chen",
            "Bowei Chen",
            "Mohan Kankanhalli"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In display advertising, users' online ad experiences are important for the\nadvertising effectiveness. However, users have not been well accommodated in\nreal-time bidding (RTB). This further influences their site visits and\nperception of the displayed banner ads. In this paper, we propose a novel\ncomputational framework which brings multimedia metrics, like the contextual\nrelevance, the visual saliency and the ad memorability into RTB to improve the\nusers' ad experiences as well as maintain the benefits of the publisher and the\nadvertiser. We aim at developing a vigorous ecosystem by optimizing the\ntrade-offs among all stakeholders. The framework considers the scenario of a\nwebpage with multiple ad slots. Our experimental results show that the benefits\nof the advertiser and the user can be significantly improved if the publisher\nwould slightly sacrifice his short-term revenue. The improved benefits will\nincrease the advertising requests (demand) and the site visits (supply), which\ncan further boost the publisher's revenue in the long run.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.00255v1"
    },
    {
        "title": "Aktuelle Entwicklungen in der Automatischen Musikverfolgung",
        "authors": [
            "Andreas Arzt",
            "Matthias Dorfer"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In this paper we present current trends in real-time music tracking (a.k.a.\nscore following). Casually speaking, these algorithms \"listen\" to a live\nperformance of music, compare the audio signal to an abstract representation of\nthe score, and \"read\" along in the sheet music. In this way at any given time\nthe exact position of the musician(s) in the sheet music is computed. Here, we\nfocus on the aspects of flexibility and usability of these algorithms. This\ncomprises work on automatic identification and flexible tracking of the piece\nbeing played as well as current approaches based on Deep Learning. The latter\nenables direct learning of correspondences between complex audio data and\nimages of the sheet music, avoiding the complicated and time-consuming\ndefinition of a mid-level representation.\n  -----\n  Diese Arbeit befasst sich mit aktuellen Entwicklungen in der automatischen\nMusikverfolgung durch den Computer. Es handelt sich dabei um Algorithmen, die\neiner musikalischen Auff\\\"uhrung \"zuh\\\"oren\", das aufgenommene Audiosignal mit\neiner (abstrakten) Repr\\\"asentation des Notentextes vergleichen und sozusagen\nin diesem mitlesen. Der Algorithmus kennt also zu jedem Zeitpunkt die Position\nder Musiker im Notentext. Neben der Vermittlung eines generellen \\\"Uberblicks,\nliegt der Schwerpunkt dieser Arbeit auf der Beleuchtung des Aspekts der\nFlexibilit\\\"at und der einfacheren Nutzbarkeit dieser Algorithmen. Es wird\ndargelegt, welche Schritte get\\\"atigt wurden (und aktuell get\\\"atigt werden) um\nden Prozess der automatischen Musikverfolgung einfacher zug\\\"anglich zu machen.\nDies umfasst Arbeiten zur automatischen Identifikation von gespielten St\\\"ucken\nund deren flexible Verfolgung ebenso wie aktuelle Ans\\\"atze mithilfe von Deep\nLearning, die es erlauben Bild und Ton direkt zu verbinden, ohne Umwege \\\"uber\nabstrakte und nur unter gro{\\ss}em Zeitaufwand zu erstellende\nZwischenrepr\\\"asentationen.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.02100v1"
    },
    {
        "title": "Joint Optimization of QoE and Fairness Through Network Assisted Adaptive\n  Mobile Video Streaming",
        "authors": [
            "Abbas Mehrabi",
            "Matti Siekkinen",
            "Antti Ylä-Jääski"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  MPEG has recently proposed Server and Network Assisted Dynamic Adaptive\nStreaming over HTTP (SAND-DASH) for video streaming over the Internet. In\ncontrast to the purely client-based video streaming in which each client makes\nits own decision to adjust its bitrate, SAND-DASH enables a group of\nsimultaneous clients to select their bitrates in a coordinated fashion in order\nto improve resource utilization and quality of experience. In this paper, we\nstudy the performance of such an adaptation strategy compared to the\ntraditional approach with large number of clients having mobile Internet\naccess. We propose a multi-servers multi-coordinators (MSs-MCs) framework to\nmodel groups of remote clients accessing video content replicated to spatially\ndistributed edge servers. We then formulate an optimization problem to maximize\njointly the QoE of individual clients, proportional fairness in allocating the\nlimited resources of base stations as well as balancing the utilized resources\namong multiple serves. We then present an efficient heuristic-based solution to\nthe problem and perform simulations in order to explore parameter space of the\nscheme as well as to compare the performance to purely client-based DASH.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.02859v1"
    },
    {
        "title": "Robust Video Watermarking against H.264 and H.265 Compression Attacks",
        "authors": [
            "Nematollah Zarmehi",
            "Mohammad Javad Barikbin"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  This paper proposes a robust watermarking method for uncompressed video data\nagainst H.264/AVC and H.265/HEVC compression standards. We embed the watermark\ndata in the mid-range transform coefficients of a block that is less similar to\nits corresponding block in the previous and next frames. This idea makes the\nwatermark robust against the compression standards that use the inter\nprediction technique. The last two video compression standards also use inter\nprediction for motion compensation like previous video compression standards.\nTherefore, the proposed method is also well suited with these standards.\nSimulation results show the adequate robustness and transparency of our\nwatermarking scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.02991v1"
    },
    {
        "title": "360-degree Video Stitching for Dual-fisheye Lens Cameras Based On Rigid\n  Moving Least Squares",
        "authors": [
            "Tuan Ho",
            "Ioannis Schizas",
            "K. R. Rao",
            "Madhukar Budagavi"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Dual-fisheye lens cameras are becoming popular for 360-degree video capture,\nespecially for User-generated content (UGC), since they are affordable and\nportable. Images generated by the dual-fisheye cameras have limited overlap and\nhence require non-conventional stitching techniques to produce high-quality\n360x180-degree panoramas. This paper introduces a novel method to align these\nimages using interpolation grids based on rigid moving least squares.\nFurthermore, jitter is the critical issue arising when one applies the\nimage-based stitching algorithms to video. It stems from the unconstrained\nmovement of stitching boundary from one frame to another. Therefore, we also\npropose a new algorithm to maintain the temporal coherence of stitching\nboundary to provide jitter-free 360-degree videos. Results show that the method\nproposed in this paper can produce higher quality stitched images and videos\nthan prior work.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.05922v1"
    },
    {
        "title": "Visually Lossless Coding in HEVC: A High Bit Depth and 4:4:4 Capable\n  JND-Based Perceptual Quantisation Technique for HEVC",
        "authors": [
            "Lee Prangnell"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Due to the increasing prevalence of high bit depth and YCbCr 4:4:4 video\ndata, it is desirable to develop a JND-based visually lossless coding technique\nwhich can account for high bit depth 4:4:4 data in addition to standard 8-bit\nprecision chroma subsampled data. In this paper, we propose a Coding Block\n(CB)-level JND-based luma and chroma perceptual quantisation technique for HEVC\nnamed Pixel-PAQ. Pixel-PAQ exploits both luminance masking and chrominance\nmasking to achieve JND-based visually lossless coding; the proposed method is\ncompatible with high bit depth YCbCr 4:4:4 video data of any resolution. When\napplied to YCbCr 4:4:4 high bit depth video data, Pixel-PAQ can achieve vast\nbitrate reductions, of up to 75% (68.6% over four QP data points), compared\nwith a state-of-the-art luma-based JND method for HEVC named IDSQ. Moreover,\nthe participants in the subjective evaluations confirm that visually lossless\ncoding is successfully achieved by Pixel-PAQ (at a PSNR value of 28.04 dB in\none test).\n",
        "pdf_link": "http://arxiv.org/pdf/1708.06417v5"
    },
    {
        "title": "Hierarchical Watermarking Framework Based on Analysis of Local\n  Complexity Variations",
        "authors": [
            "Majid Mohrekesh",
            "Shekoofeh Azizi",
            "Shahram Shirani",
            "Nader Karimi",
            "Shadrokh Samavi"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Increasing production and exchange of multimedia content has increased the\nneed for better protection of copyright by means of watermarking. Different\nmethods have been proposed to satisfy the tradeoff between imperceptibility and\nrobustness as two important characteristics in watermarking while maintaining\nproper data-embedding capacity. Many watermarking methods use image independent\nset of parameters. Different images possess different potentials for robust and\ntransparent hosting of watermark data. To overcome this deficiency, in this\npaper we have proposed a new hierarchical adaptive watermarking framework. At\nthe higher level of hierarchy, complexity of an image is ranked in comparison\nwith complexities of images of a dataset. For a typical dataset of images, the\nstatistical distribution of block complexities is found. At the lower level of\nthe hierarchy, for a single cover image that is to be watermarked, complexities\nof blocks can be found. Local complexity variation (LCV) among a block and its\nneighbors is used to adaptively control the watermark strength factor of each\nblock. Such local complexity analysis creates an adaptive embedding scheme,\nwhich results in higher transparency by reducing blockiness effects. This two\nlevel hierarchy has enabled our method to take advantage of all image blocks to\nelevate the embedding capacity while preserving imperceptibility. For testing\nthe effectiveness of the proposed framework, contourlet transform (CT) in\nconjunction with discrete cosine transform (DCT) is used to embed pseudo-random\nbinary sequences as watermark. Experimental results show that the proposed\nframework elevates the performance the watermarking routine in terms of both\nrobustness and transparency.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.03020v1"
    },
    {
        "title": "Neural network-based arithmetic coding of intra prediction modes in HEVC",
        "authors": [
            "Rui Song",
            "Dong Liu",
            "Houqiang Li",
            "Feng Wu"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In both H.264 and HEVC, context-adaptive binary arithmetic coding (CABAC) is\nadopted as the entropy coding method. CABAC relies on manually designed\nbinarization processes as well as handcrafted context models, which may\nrestrict the compression efficiency. In this paper, we propose an arithmetic\ncoding strategy by training neural networks, and make preliminary studies on\ncoding of the intra prediction modes in HEVC. Instead of binarization, we\npropose to directly estimate the probability distribution of the 35 intra\nprediction modes with the adoption of a multi-level arithmetic codec. Instead\nof handcrafted context models, we utilize convolutional neural network (CNN) to\nperform the probability estimation. Simulation results show that our proposed\narithmetic coding leads to as high as 9.9% bits saving compared with CABAC.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.05737v1"
    },
    {
        "title": "Adaptive Blind Image Watermarking Using Fuzzy Inference System Based on\n  Human Visual Perception",
        "authors": [
            "Maedeh Jamali",
            "Shima Rafiei",
            "S. M. Reza Soroushmehr",
            "Nader Karimi",
            "Shahram Shirani",
            "Kayvan Najarian",
            "Shadrokh Samavi"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Development of digital content has increased the necessity of copyright\nprotection by means of watermarking. Imperceptibility and robustness are two\nimportant features of watermarking algorithms. The goal of watermarking methods\nis to satisfy the tradeoff between these two contradicting characteristics.\nRecently watermarking methods in transform domains have displayed favorable\nresults. In this paper, we present an adaptive blind watermarking method which\nhas high transparency in areas that are important to human visual system. We\npropose a fuzzy system for adaptive control of the embedding strength factor.\nFeatures such as saliency, intensity, and edge-concentration, are used as fuzzy\nattributes. Redundant embedding in discrete cosine transform (DCT) of wavelet\ndomain has increased the robustness of our method. Experimental results show\nthe efficiency of the proposed method and better results are obtained as\ncompared to comparable methods with same size of watermark logo.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.06536v3"
    },
    {
        "title": "Enhancing Quality for HEVC Compressed Videos",
        "authors": [
            "Ren Yang",
            "Mai Xu",
            "Tie Liu",
            "Zulin Wang",
            "Zhenyu Guan"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The latest High Efficiency Video Coding (HEVC) standard has been increasingly\napplied to generate video streams over the Internet. However, HEVC compressed\nvideos may incur severe quality degradation, particularly at low bit-rates.\nThus, it is necessary to enhance the visual quality of HEVC videos at the\ndecoder side. To this end, this paper proposes a Quality Enhancement\nConvolutional Neural Network (QE-CNN) method that does not require any\nmodification of the encoder to achieve quality enhancement for HEVC. In\nparticular, our QE-CNN method learns QE-CNN-I and QE-CNN-P models to reduce the\ndistortion of HEVC I and P frames, respectively. The proposed method differs\nfrom the existing CNN-based quality enhancement approaches, which only handle\nintra-coding distortion and are thus not suitable for P frames. Our\nexperimental results validate that our QE-CNN method is effective in enhancing\nquality for both I and P frames of HEVC videos. To apply our QE-CNN method in\ntime-constrained scenarios, we further propose a Time-constrained Quality\nEnhancement Optimization (TQEO) scheme. Our TQEO scheme controls the\ncomputational time of QE-CNN to meet a target, meanwhile maximizing the quality\nenhancement. Next, the experimental results demonstrate the effectiveness of\nour TQEO scheme from the aspects of time control accuracy and quality\nenhancement under different time constraints. Finally, we design a prototype to\nimplement our TQEO scheme in a real-time scenario.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.06734v2"
    },
    {
        "title": "Spatial-Temporal Residue Network Based In-Loop Filter for Video Coding",
        "authors": [
            "Chuanmin Jia",
            "Shiqi Wang",
            "Xinfeng Zhang",
            "Shanshe Wang",
            "Siwei Ma"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Deep learning has demonstrated tremendous break through in the area of\nimage/video processing. In this paper, a spatial-temporal residue network\n(STResNet) based in-loop filter is proposed to suppress visual artifacts such\nas blocking, ringing in video coding. Specifically, the spatial and temporal\ninformation is jointly exploited by taking both current block and co-located\nblock in reference frame into consideration during the processing of in-loop\nfilter. The architecture of STResNet only consists of four convolution layers\nwhich shows hospitality to memory and coding complexity. Moreover, to fully\nadapt the input content and improve the performance of the proposed in-loop\nfilter, coding tree unit (CTU) level control flag is applied in the sense of\nrate-distortion optimization. Extensive experimental results show that our\nscheme provides up to 5.1% bit-rate reduction compared to the state-of-the-art\nvideo coding standard.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08462v1"
    },
    {
        "title": "Encoding Bitrate Optimization Using Playback Statistics for HTTP-based\n  Adaptive Video Streaming",
        "authors": [
            "Chao Chen",
            "Yao-Chung Lin",
            "Anil Kokaram",
            "Steve Benting"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  HTTP video streaming is in wide use to deliver video over the Internet. With\nHTTP adaptive steaming, a video playback dynamically selects a video stream\nfrom a pre-encoded representation based on available bandwidth and viewport\n(screen) size. The viewer's video quality is therefore influenced by the\nencoded bitrates. We minimize the average delivered bitrate subject to a\nquality lower bound on a per-chunk basis by modeling the probability that a\nplayer selects a particular encoding. Through simulation and real-world\nexperiments, the proposed method saves 9.6% of bandwidth while average\ndelivered video quality comparing with state of the art while keeping average\ndelivered video quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08763v2"
    },
    {
        "title": "Impact of Three-Dimensional Video Scalability on Multi-View Activity\n  Recognition using Deep Learning",
        "authors": [
            "Jun-Ho Choi",
            "Manri Cheon",
            "Min-Su Choi",
            "Jong-Seok Lee"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Human activity recognition is one of the important research topics in\ncomputer vision and video understanding. It is often assumed that high quality\nvideo sequences are available for recognition. However, relaxing such a\nrequirement and implementing robust recognition using videos having reduced\ndata rates can achieve efficiency in storing and transmitting video data.\nThree-dimensional video scalability, which refers to the possibility of\nreducing spatial, temporal, and quality resolutions of videos, is an effective\nway for flexible representation and management of video data. In this paper, we\ninvestigate the impact of the video scalability on multi-view activity\nrecognition. We employ both a spatiotemporal feature extraction-based method\nand a deep learning-based method using convolutional and recurrent neural\nnetworks. The recognition performance of the two methods is examined, along\nwith in-depth analysis regarding how their performance vary with respect to\nvarious scalability combinations. In particular, we demonstrate that the deep\nlearning-based method can achieve significantly improved robustness in\ncomparison to the feature-based method. Furthermore, we investigate optimal\nscalability combinations with respect to bitrate in order to provide useful\nguidelines for an optimal operation policy in resource-constrained activity\nrecognition systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10206v1"
    },
    {
        "title": "ADS: Adaptive and Dynamic Scaling Mechanism for Multimedia Conferencing\n  Services in the Cloud",
        "authors": [
            "Abbas Soltanian",
            "Diala Naboulsi",
            "Mohammad A. Salahuddin",
            "Roch Glitho",
            "Halima Elbiaze",
            "Constant Wette"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Multimedia conferencing is used extensively in a wide range of applications,\nsuch as online games and distance learning. These applications need to\nefficiently scale the conference size as the number of participants fluctuates.\nCloud is a technology that addresses the scalability issue. However, the\nproposed cloud-based solutions have several shortcomings in considering the\nfuture demand of applications while meeting both Quality of Service (QoS)\nrequirements and efficiency in resource usage. In this paper, we propose an\nAdaptive and Dynamic Scaling mechanism (ADS) for multimedia conferencing\nservices in the cloud. This mechanism enables scalable and elastic resource\nallocation with respect to the number of participants. ADS produces a\ncost-efficient scaling schedule while considering the QoS requirements and the\nfuture demand of the conferencing service. We formulate the problem using\nInteger Linear Programming (ILP) and design a heuristic for it. Simulation\nresults show that ADS mechanism elastically scales conferencing services.\nMoreover, the ADS heuristic is shown to outperform a greedy algorithm from a\nresource-efficiency perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02150v1"
    },
    {
        "title": "Viewport-aware adaptive 360° video streaming using tiles for\n  virtual reality",
        "authors": [
            "Cagri Ozcinar",
            "Ana De Abreu",
            "Aljosa Smolic"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  360{\\deg} video is attracting an increasing amount of attention in the\ncontext of Virtual Reality (VR). Owing to its very high-resolution\nrequirements, existing professional streaming services for 360{\\deg} video\nsuffer from severe drawbacks. This paper introduces a novel end-to-end\nstreaming system from encoding to displaying, to transmit 8K resolution\n360{\\deg} video and to provide an enhanced VR experience using Head Mounted\nDisplays (HMDs). The main contributions of the proposed system are about\ntiling, integration of the MPEG-Dynamic Adaptive Streaming over HTTP (DASH)\nstandard, and viewport-aware bitrate level selection. Tiling and adaptive\nstreaming enable the proposed system to deliver very high-resolution 360{\\deg}\nvideo at good visual quality. Further, the proposed viewport-aware bitrate\nassignment selects an optimum DASH representation for each tile in a\nviewport-aware manner. The quality performance of the proposed system is\nverified in simulations with varying network bandwidth using realistic view\ntrajectories recorded from user experiments. Our results show that the proposed\nstreaming system compares favorably compared to existing methods in terms of\nPSNR and SSIM inside the viewport.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02386v1"
    },
    {
        "title": "Convolutional Neural Network Steganalysis's Application to Steganography",
        "authors": [
            "Mehdi Sharifzadeh",
            "Chirag Agarwal",
            "Mohammed Aloraini",
            "Dan Schonfeld"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  This paper presents a novel approach to increase the performance bounds of\nimage steganography under the criteria of minimizing distortion. The proposed\napproach utilizes a steganalysis convolutional neural network (CNN) framework\nto understand an image's model and embed in less detectable regions to preserve\nthe model. In other word, the trained steganalysis CNN is used to calculate\nderivatives of the statistical model of an image with respect to embedding\nchanges. The experimental results show that the proposed algorithm outperforms\nprevious state-of-the-art methods in a wide range of low relative payloads when\ncompared with HUGO, S-UNIWARD, and HILL by the state-of-the-art steganalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02581v1"
    },
    {
        "title": "Estimation of optimal encoding ladders for tiled 360° VR video in\n  adaptive streaming systems",
        "authors": [
            "Cagri Ozcinar",
            "Ana De Abreu",
            "Sebastian Knorr",
            "Aljosa Smolic"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Given the significant industrial growth of demand for virtual reality (VR),\n360{\\deg} video streaming is one of the most important VR applications that\nrequire cost-optimal solutions to achieve widespread proliferation of VR\ntechnology. Because of its inherent variability of data-intensive content types\nand its tiled-based encoding and streaming, 360{\\deg} video requires new\nencoding ladders in adaptive streaming systems to achieve cost-optimal and\nimmersive streaming experiences. In this context, this paper targets both the\nprovider's and client's perspectives and introduces a new content-aware\nencoding ladder estimation method for tiled 360{\\deg} VR video in adaptive\nstreaming systems. The proposed method first categories a given 360{\\deg} video\nusing its features of encoding complexity and estimates the visual distortion\nand resource cost of each bitrate level based on the proposed distortion and\nresource cost models. An optimal encoding ladder is then formed using the\nproposed integer linear programming (ILP) algorithm by considering practical\nconstraints. Experimental results of the proposed method are compared with the\nrecommended encoding ladders of professional streaming service providers.\nEvaluations show that the proposed encoding ladders deliver better results\ncompared to the recommended encoding ladders in terms of objective quality for\n360{\\deg} video, providing optimal encoding ladders using a set of service\nprovider's constraint parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.03362v1"
    },
    {
        "title": "Predicting Chroma from Luma in AV1",
        "authors": [
            "Luc N. Trudeau",
            "Nathan E. Egge",
            "David Barr"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Chroma from luma (CfL) prediction is a new and promising chroma-only intra\npredictor that models chroma pixels as a linear function of the coincident\nreconstructed luma pixels. In this paper, we present the CfL predictor adopted\nin Alliance Video 1 (AV1), a royalty-free video codec developed by the Alliance\nfor Open Media (AOM). The proposed CfL distinguishes itself from prior art not\nonly by reducing decoder complexity, but also by producing more accurate\npredictions. On average, CfL reduces the BD-rate, when measured with CIEDE2000,\nby 5% for still images and 2% for video sequences.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.03951v2"
    },
    {
        "title": "Generative Steganography with Kerckhoffs' Principle",
        "authors": [
            "Yan Ke",
            "Minqing Zhang",
            "Jia Liu",
            "Tingting Su",
            "Xiaoyuan Yang"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The distortion in steganography that usually comes from the modification or\nrecoding on the cover image during the embedding process leaves the\nsteganalyzer with possibility of discriminating. Faced with such a risk, we\npropose generative steganography with Kerckhoffs' principle (GSK) in this\nletter. In GSK, the secret messages are generated by a cover image using a\ngenerator rather than embedded into the cover, thus resulting in no\nmodifications in the cover. To ensure the security, the generators are trained\nto meet Kerckhoffs' principle based on generative adversarial networks (GAN).\nEverything about the GSK system, except the extraction key, is public knowledge\nfor the receivers. The secret messages can be outputted by the generator if and\nonly if the extraction key and the cover image are both inputted. In the\ngenerator training procedures, there are two GANs, Message- GAN and Cover-GAN,\ndesigned to work jointly making the generated results under the control of the\nextraction key and the cover image. We provide experimental results on the\ntraining process and give an example of the working process by adopting a\ngenerator trained on MNIST, which demonstrate that GSK can use a cover image\nwithout any modification to generate messages, and without the extraction key\nor the cover image, only meaningless results would be obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.04916v3"
    },
    {
        "title": "A Novel Convolutional Neural Network for Image Steganalysis with Shared\n  Normalization",
        "authors": [
            "Songtao Wu",
            "Sheng-hua Zhong",
            "Yan Liu"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Deep learning based image steganalysis has attracted increasing attentions in\nrecent years. Several Convolutional Neural Network (CNN) models have been\nproposed and achieved state-of-the-art performances on detecting steganography.\nIn this paper, we explore an important technique in deep learning, the batch\nnormalization, for the task of image steganalysis. Different from natural image\nclassification, steganalysis is to discriminate cover images and stego images\nwhich are the result of adding weak stego signals into covers. This\ncharacteristic makes a cover image is more statistically similar to its stego\nthan other cover images, requiring steganalytic methods to use paired learning\nto extract effective features for image steganalysis. Our theoretical analysis\nshows that a CNN model with multiple normalization layers is hard to be\ngeneralized to new data in the test set when it is well trained with paired\nlearning. To hand this difficulty, we propose a novel normalization technique\ncalled Shared Normalization (SN) in this paper. Unlike the batch normalization\nlayer utilizing the mini-batch mean and standard deviation to normalize each\ninput batch, SN shares same statistics for all training and test batches. Based\non the proposed SN layer, we further propose a novel neural network model for\nimage steganalysis. Extensive experiments demonstrate that the proposed network\nwith SN layers is stable and can detect the state of the art steganography with\nbetter performances than previous methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.07306v2"
    },
    {
        "title": "Optimized Pre-Compensating Compression",
        "authors": [
            "Yehuda Dar",
            "Michael Elad",
            "Alfred M. Bruckstein"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In imaging systems, following acquisition, an image/video is transmitted or\nstored and eventually presented to human observers using different and often\nimperfect display devices. While the resulting quality of the output image may\nseverely be affected by the display, this degradation is usually ignored in the\npreceding compression. In this paper we model the sub-optimality of the display\ndevice as a known degradation operator applied on the decompressed image/video.\nWe assume the use of a standard compression path, and augment it with a\nsuitable pre-processing procedure, providing a compressed signal intended to\ncompensate the degradation without any post-filtering. Our approach originates\nfrom an intricate rate-distortion problem, optimizing the modifications to the\ninput image/video for reaching best end-to-end performance. We address this\nseemingly computationally intractable problem using the alternating direction\nmethod of multipliers (ADMM) approach, leading to a procedure in which a\nstandard compression technique is iteratively applied. We demonstrate the\nproposed method for adjusting HEVC image/video compression to compensate\npost-decompression visual effects due to a common type of displays.\nParticularly, we use our method to reduce motion-blur perceived while viewing\nvideo on LCD devices. The experiments establish our method as a leading\napproach for preprocessing high bit-rate compression to counterbalance a\npost-decompression degradation.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.07901v2"
    },
    {
        "title": "Channel Transition Invariant Fast Broadcasting Scheme",
        "authors": [
            "Mohammad Saidur Rahman",
            "Ashfaqur Rahman"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Fast broadcasting (FB) is a popular near video-on-demand system where a video\nis divided into equal size segments those are repeatedly transmitted over a\nnumber of channels following a pattern. For user satisfaction, it is required\nto reduce the initial user waiting time and client side buffer requirement at\nstreaming. Use of additional channels can achieve the objective. However, some\naugmentation is required to the basic FB scheme as it lacks any mechanism to\nrealise a well defined relationship among the segment sizes at channel\ntransition. Lack of correspondence between the segments causes intermediate\nwaiting for the clients while watching videos. Use of additional channel\nrequires additional bandwidth. In this paper, we propose a modified FB scheme\nthat achieves zero initial clients waiting time and provides a mechanism to\ncontrol client side buffer requirement at streaming without requiring\nadditional channels. We present several results to demonstrate the\neffectiveness of the proposed FB scheme over the existing ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08118v1"
    },
    {
        "title": "Calibrated Audio Steganalysis",
        "authors": [
            "Hamzeh Ghasemzadeh",
            "Mohammad H. Kayvanrad"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Calibration is a common practice in image steganalysis for extracting\nprominent features. Based on the idea of reembedding, a new set of calibrated\nfeatures for audio steganalysis applications are proposed. These features are\nextracted from a model that has maximum deviation from human auditory system\nand had been specifically designed for audio steganalysis. Ability of the\nproposed system is tested extensively. Simulations demonstrate that the\nproposed method can accurately detect the presence of hidden messages even in\nvery low embedding rates. Proposed method achieves an accuracy of 99.3%\n(StegHide@0.76% BPB) which is 9.5% higher than the previous R-MFCC based\nsteganalysis method.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08571v2"
    },
    {
        "title": "JPEG Steganalysis Based on DenseNet",
        "authors": [
            "Jianhua Yang",
            "Yun-Qing Shi",
            "Edward K. Wong",
            "Xiangui Kang"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Different from the conventional deep learning work based on an images content\nin computer vision, deep steganalysis is an art to detect the secret\ninformation embedded in an image via deep learning, pose challenge of detection\nweak information invisible hidden in a host image thus learning in a very low\nsignal-to-noise (SNR) case. In this paper, we propose a 32- layer convolutional\nneural Networks (CNNs) in to improve the efficiency of preprocess and reuse the\nfeatures by concatenating all features from the previous layers with the same\nfeature- map size, thus improve the flow of information and gradient. The\nshared features and bottleneck layers further improve the feature propagation\nand reduce the CNN model parameters dramatically. Experimental results on the\nBOSSbase, BOWS2 and ImageNet datasets have showed that the proposed CNN\narchitecture can improve the performance and enhance the robustness. To further\nboost the detection accuracy, an ensemble architecture called as CNN-SCA-GFR is\nproposed, CNN-SCA- GFR is also the first work to combine the CNN architecture\nand conventional method in the JPEG domain. Experiments show that it can\nfurther lower detection errors. Compared with the state-of-the-art method XuNet\n[1] on BOSSbase, the proposed CNN-SCA-GFR architecture can reduce detection\nerror rate by 5.67% for 0.1 bpnzAC and by 4.41% for 0.4 bpnzAC while the number\nof training parameters in CNN is only 17% of what used by XuNet. It also\ndecreases the detection errors from the conventional method SCA-GFR by 7.89%\nfor 0.1 bpnzAC and 8.06% for 0.4 bpnzAC, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.09335v3"
    },
    {
        "title": "A Color Intensity Invariant Low Level Feature Optimization Framework for\n  Image Quality Assessment",
        "authors": [
            "Navaneeth K. Kottayil",
            "Irene Cheng",
            "Frederic Dufaux",
            "Anup Basu"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Image Quality Assessment (IQA) algorithms evaluate the perceptual quality of\nan image using evaluation scores that assess the similarity or difference\nbetween two images. We propose a new low-level feature based IQA technique,\nwhich applies filter-bank decomposition and center-surround methodology.\nDiffering from existing methods, our model incorporates color intensity\nadaptation and frequency scaling optimization at each filter-bank level and\nspatial orientation to extract and enhance perceptually significant features.\nOur computational model exploits the concept of object detection and\nencapsulates characteristics proposed in other IQA algorithms in a unified\narchitecture. We also propose a systematic approach to review the evolution of\nIQA algorithms using unbiased test datasets, instead of looking at individual\nscores in isolation. Experimental results demonstrate the feasibility of our\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.00043v1"
    },
    {
        "title": "DCT-domain Deep Convolutional Neural Networks for Multiple JPEG\n  Compression Classification",
        "authors": [
            "Vinay Verma",
            "Nikita Agarwal",
            "Nitin Khanna"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  With the rapid advancements in digital imaging systems and networking,\nlow-cost hand-held image capture devices equipped with network connectivity are\nbecoming ubiquitous. This ease of digital image capture and sharing is also\naccompanied by widespread usage of user-friendly image editing software. Thus,\nwe are in an era where digital images can be very easily used for the massive\nspread of false information and their integrity need to be seriously\nquestioned. Application of multiple lossy compressions on images is an\nessential part of any image editing pipeline involving lossy compressed images.\nThis paper aims to address the problem of classifying images based on the\nnumber of JPEG compressions they have undergone, by utilizing deep\nconvolutional neural networks in DCT domain. The proposed system incorporates a\nwell designed pre-processing step before feeding the image data to CNN to\ncapture essential characteristics of compression artifacts and make the system\nimage content independent. Detailed experiments are performed to optimize\ndifferent aspects of the system, such as depth of CNN, number of DCT\nfrequencies, and execution time. Results on the standard UCID dataset\ndemonstrate that the proposed system outperforms existing systems for multiple\nJPEG compression detection and is capable of classifying more number of\nre-compression cycles then existing systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.02313v1"
    },
    {
        "title": "Real-time Video Processing in Web Applications",
        "authors": [
            "Cristian Ionita",
            "Alexandru Barbulescu"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The OpenGL ES standard is implemented in modern desktop and mobile browsers\nthrough the WebGL API. This paper explores the potential for using OpenGL ES\nhardware acceleration for real time video processing in standard HTML5\napplications. It analyses the WebGL performance across device types and\ncompares it with the standard JavaScript and canvas performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.02438v1"
    },
    {
        "title": "A Graph-theoretic Model to Steganography on Social Networks",
        "authors": [
            "Hanzhou Wu",
            "Wei Wang",
            "Jing Dong",
            "Yiliang Xiong",
            "Hongxia Wang"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Steganography aims to conceal the very fact that the communication takes\nplace, by embedding a message into a digit object such as image without\nintroducing noticeable artifacts. A number of steganographic systems have been\ndeveloped in past years, most of which, however, are confined to the laboratory\nconditions where the real-world use of steganography are rarely concerned. In\nthis paper, we introduce an alternative perspective to steganography. A\ngraph-theoretic model to steganography on social networks is presented to\nanalyze real-world steganographic scenarios. In the graph, steganographic\nparticipants are corresponding to the vertices with meaningless unique\nidentifiers. Each edge allows the two vertices to communicate with each other\nby any steganographic algorithm. Meanwhile, the edges are associated with\nweights to quantize the corresponding communication risk (or say cost). The\noptimization task is to minimize the overall risk, which is modeled as additive\nover the social network. We analyze different scenarios on a social network,\nand provide the suited solutions to the corresponding optimization tasks. We\nprove that a multiplicative probabilistic graph is equivalent to an additive\nweighted graph. From the viewpoint of an attacker, he may hope to detect\nsuspicious communication channels, the data encoder(s) and the data decoder(s).\nWe present limited detection analysis to steganographic communication on a\nnetwork.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.03621v5"
    },
    {
        "title": "Minimizing Embedding Distortion with Weighted Bigraph Matching in\n  Reversible Data Hiding",
        "authors": [
            "Hanzhou Wu"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  For a required payload, the existing reversible data hiding (RDH) methods\nalways expect to reduce the embedding distortion as much as possible, such as\nby utilizing a well-designed predictor, taking into account the carrier-content\ncharacteristics, and/or improving modification efficiency etc. However, due to\nthe diversity of natural images, it is actually very hard to accurately model\nthe statistical characteristics of natural images, which has limited the\npractical use of traditional RDH methods that rely heavily on the content\ncharacteristics. Based on this perspective, instead of directly exploiting the\ncontent characteristics, in this paper, we model the embedding operation on a\nweighted bipartite graph to reduce the introduced distortion due to data\nembedding, which is proved to be equivalent to a graph problem called as\n\\emph{minimum weight maximum matching (MWMM)}. By solving the MWMM problem, we\ncan find the optimal histogram shifting strategy under the given condition.\nSince the proposed method is essentially a general embedding model for the RDH,\nit can be utilized for designing an RDH scheme. In our experiments, we\nincorporate the proposed method into some related works, and, our experimental\nresults have shown that the proposed method can significantly improve the\npayload-distortion performance, indicating that the proposed method could be\ndesirable and promising for practical use and the design of RDH schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.06240v1"
    },
    {
        "title": "Blind High Dynamic Range Quality estimation by disentangling perceptual\n  and noise features in images",
        "authors": [
            "Navaneeth Kamballur Kottayil",
            "Giuseppe Valenzise",
            "Frederic Dufaux",
            "Irene Cheng"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Assessing the visual quality of High Dynamic Range (HDR) images is an\nunexplored and an interesting research topic that has become relevant with the\ncurrent boom in HDR technology. We propose a new convolutional neural network\nbased model for No reference image quality assessment(NR-IQA) on HDR data. This\nmodel predicts the amount and location of noise, perceptual influence of image\npixels on the noise, and the perceived quality, of a distorted image without\nany reference image. The proposed model extracts numerical values corresponding\nto the noise present in any given distorted image, and the perceptual effects\nexhibited by a human eye when presented with the same. These two measures are\nextracted separately yet sequentially and combined in a mixing function to\ncompute the quality of the distorted image perceived by a human eye. Our\ntraining process derives the the component that computes perceptual effects\nfrom a real world image quality dataset, rather than using results of\npsycovisual experiments. With the proposed model, we demonstrate state of the\nart performance for HDR NR-IQA and our results show performance similar to HDR\nFull Reference Image Quality Assessment algorithms (FR-IQA).\n",
        "pdf_link": "http://arxiv.org/pdf/1712.07269v1"
    },
    {
        "title": "Robust and discriminative zero-watermark scheme based on invariant\n  feature and similarity-based retrieval for protecting large-scale DIBR 3D\n  videos",
        "authors": [
            "Xiyao Liu",
            "Yifang Wang",
            "Ziqiang Sun",
            "Lei Wang",
            "Rongchang Zhao",
            "Yuesheng Zhu",
            "Beiji Zou"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Digital rights management (DRM) of depth-image-based rendering (DIBR) 3D\nvideo is an emerging area of research. Existing schemes for DIBR 3D video cause\nvideo distortions, are vulnerable to severe signal and geometric attacks,\ncannot protect 2D frame and depth map independently or can hardly deal with\nlarge-scale videos. To address these issues, a novel zero-watermark scheme\nbased on invariant feature and similarity-based retrieval for protecting DIBR\n3D video (RZW-SR3D) is proposed in this study. In RZW-SR3D, invariant features\nare extracted to generate master and ownership shares for providing\ndistortion-free, robust and discriminative copyright identification under\nvarious attacks. Different from traditional zero-watermark schemes, features\nand ownership shares are stored correlatively, and a similarity-based retrieval\nphase is designed to provide effective solutions for large-scale videos. In\naddition, flexible mechanisms based on attention-based fusion are designed to\nprotect 2D frame and depth map independently and simultaneously. Experimental\nresults demonstrate that RZW-SR3D have superior DRM performances than existing\nschemes. First, RZW-SR3D can extracted the ownership shares relevant to a\nparticular 3D video precisely and reliably for effective copyright\nidentification of large-scale videos. Second, RZW-SR3D ensures lossless,\nprecise, reliable and flexible copyright identification for 2D frame and depth\nmap of 3D videos.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09480v2"
    },
    {
        "title": "Field Studies with Multimedia Big Data: Opportunities and Challenges\n  (Extended Version)",
        "authors": [
            "Mario Michael Krell",
            "Julia Bernd",
            "Yifan Li",
            "Daniel Ma",
            "Jaeyoung Choi",
            "Michael Ellsworth",
            "Damian Borth",
            "Gerald Friedland"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Social multimedia users are increasingly sharing all kinds of data about the\nworld. They do this for their own reasons, not to provide data for field\nstudies-but the trend presents a great opportunity for scientists. The Yahoo\nFlickr Creative Commons 100 Million (YFCC100M) dataset comprises 99 million\nimages and nearly 800 thousand videos from Flickr, all shared under Creative\nCommons licenses. To enable scientists to leverage these media records for\nfield studies, we propose a new framework that extracts targeted subcorpora\nfrom the YFCC100M, in a format usable by researchers who are not experts in big\ndata retrieval and processing.\n  This paper discusses a number of examples from the literature-as well as some\nentirely new ideas-of natural and social science field studies that could be\npiloted, supplemented, replicated, or conducted using YFCC100M data. These\nexamples illustrate the need for a general new open-source framework for\nMultimedia Big Data Field Studies. There is currently a gap between the\nseparate aspects of what multimedia researchers have shown to be possible with\nconsumer-produced big data and the follow-through of creating a comprehensive\nfield study framework that supports scientists across other disciplines.\n  To bridge this gap, we must meet several challenges. For example, the\nframework must handle unlabeled and noisily labeled data to produce a filtered\ndataset for a scientist-who naturally wants it to be both as large and as clean\nas possible. This requires an iterative approach that provides access to\nstatistical summaries and refines the search by constructing new classifiers.\nThe first phase of our framework is available as Multimedia Commons Search, an\nintuitive interface that enables complex search queries at a large scale...\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09915v1"
    },
    {
        "title": "Fake Colorized Image Detection",
        "authors": [
            "Yuanfang Guo",
            "Xiaochun Cao",
            "Wei Zhang",
            "Rui Wang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Image forensics aims to detect the manipulation of digital images. Currently,\nsplicing detection, copy-move detection and image retouching detection are\ndrawing much attentions from researchers. However, image editing techniques\ndevelop with time goes by. One emerging image editing technique is\ncolorization, which can colorize grayscale images with realistic colors.\nUnfortunately, this technique may also be intentionally applied to certain\nimages to confound object recognition algorithms. To the best of our knowledge,\nno forensic technique has yet been invented to identify whether an image is\ncolorized. We observed that, compared to natural images, colorized images,\nwhich are generated by three state-of-the-art methods, possess statistical\ndifferences for the hue and saturation channels. Besides, we also observe\nstatistical inconsistencies in the dark and bright channels, because the\ncolorization process will inevitably affect the dark and bright channel values.\nBased on our observations, i.e., potential traces in the hue, saturation, dark\nand bright channels, we propose two simple yet effective detection methods for\nfake colorized images: Histogram based Fake Colorized Image Detection\n(FCID-HIST) and Feature Encoding based Fake Colorized Image Detection\n(FCID-FE). Experimental results demonstrate that both proposed methods exhibit\na decent performance against multiple state-of-the-art colorization approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02768v2"
    },
    {
        "title": "Ensemble Reversible Data Hiding",
        "authors": [
            "Hanzhou Wu",
            "Wei Wang",
            "Jing Dong",
            "Hongxia Wang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The conventional reversible data hiding (RDH) algorithms often consider the\nhost as a whole to embed a secret payload. In order to achieve satisfactory\nrate-distortion performance, the secret bits are embedded into the noise-like\ncomponent of the host such as prediction errors. From the rate-distortion\noptimization view, it may be not optimal since the data embedding units use the\nidentical parameters. This motivates us to present a segmented data embedding\nstrategy for efficient RDH in this paper, in which the raw host could be\npartitioned into multiple subhosts such that each one can freely optimize and\nuse the data embedding parameters. Moreover, it enables us to apply different\nRDH algorithms within different subhosts, which is defined as ensemble. Notice\nthat, the ensemble defined here is different from that in machine learning.\nAccordingly, the conventional operation corresponds to a special case of the\nproposed work. Since it is a general strategy, we combine some state-of-the-art\nalgorithms to construct a new system using the proposed embedding strategy to\nevaluate the rate-distortion performance. Experimental results have shown that,\nthe ensemble RDH system could outperform the original versions in most cases,\nwhich has shown the superiority and applicability.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04747v4"
    },
    {
        "title": "Reversible Embedding to Covers Full of Boundaries",
        "authors": [
            "Hanzhou Wu",
            "Wei Wang",
            "Jing Dong",
            "Yanli Chen",
            "Hongxia Wang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In reversible data embedding, to avoid overflow and underflow problem, before\ndata embedding, boundary pixels are recorded as side information, which may be\nlosslessly compressed. The existing algorithms often assume that a natural\nimage has little boundary pixels so that the size of side information is small.\nAccordingly, a relatively high pure payload could be achieved. However, there\nactually may exist a lot of boundary pixels in a natural image, implying that,\nthe size of side information could be very large. Therefore, when to directly\nuse the existing algorithms, the pure embedding capacity may be not sufficient.\nIn order to address this problem, in this paper, we present a new and efficient\nframework to reversible data embedding in images that have lots of boundary\npixels. The core idea is to losslessly preprocess boundary pixels so that it\ncan significantly reduce the side information. Experimental results have shown\nthe superiority and applicability of our work.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04752v1"
    },
    {
        "title": "Perceived Audiovisual Quality Modelling based on Decison Trees, Genetic\n  Programming and Neural Networks",
        "authors": [
            "Edip Demirbilek",
            "Jean-Charles Grégoire"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Our objective is to build machine learning based models that predict\naudiovisual quality directly from a set of correlated parameters that are\nextracted from a target quality dataset. We have used the bitstream version of\nthe INRS audiovisual quality dataset that reflects contemporary real-time\nconfigurations for video frame rate, video quantization, noise reduction\nparameters and network packet loss rate. We have utilized this dataset to build\nbitstream perceived quality estimation models based on the Random Forests,\nBagging, Deep Learning and Genetic Programming methods.\n  We have taken an empirical approach and have generated models varying from\nvery simple to the most complex depending on the number of features used from\nthe quality dataset. Random Forests and Bagging models have overall generated\nthe most accurate results in terms of RMSE and Pearson correlation coefficient\nvalues. Deep Learning and Genetic Programming based bitstream models have also\nachieved good results but that high performance was observed only with a\nlimited range of features. We have also obtained the epsilon-insensitive RMSE\nvalues for each model and have computed the significance of the difference\nbetween the correlation coefficients.\n  Overall we conclude that computing the bitstream information is worth the\neffort it takes to generate and helps to build more accurate models for\nreal-time communications. However, it is useful only for the deployment of the\nright algorithms with the carefully selected subset of the features. The\ndataset and tools that have been developed during this research are publicly\navailable for research and development purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.05889v1"
    },
    {
        "title": "Multi-measures fusion based on multi-objective genetic programming for\n  full-reference image quality assessment",
        "authors": [
            "Naima Merzougui",
            "Naima Merzougui"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In this paper, we exploit the flexibility of multi-objective fitness\nfunctions, and the efficiency of the model structure selection ability of a\nstandard genetic programming (GP) with the parameter estimation power of\nclassical regression via multi-gene genetic programming (MGGP), to propose a\nnew fusion technique for image quality assessment (IQA) that is called\nMulti-measures Fusion based on Multi-Objective Genetic Programming (MFMOGP).\nThis technique can automatically select the most significant suitable measures,\nfrom 16 full-reference IQA measures, used in aggregation and finds weights in a\nweighted sum of their outputs while simultaneously optimizing for both accuracy\nand complexity. The obtained well-performing fusion of IQA measures are\nevaluated on four largest publicly available image databases and compared\nagainst state-of-the-art full-reference IQA approaches. Results of comparison\nreveal that the proposed approach outperforms other state-of-the-art recently\ndeveloped fusion approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06030v1"
    },
    {
        "title": "Computer-Aided Annotation for Video Tampering Dataset of Forensic\n  Research",
        "authors": [
            "Ye Yao"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The annotation of video tampering dataset is a boring task that takes a lot\nof manpower and financial resources. At present, there is no published\nliterature which is capable to improve the annotation efficiency of forged\nvideos. We presented a computer-aided annotation method for video tampering\ndataset in this paper. This annotation method can be utilized to label the\nframes of forged video sequences. By means of comparing the original video\nframes with the forged video frames, we can locate the position and the\ntrajectory of the forged areas of the forged video frames. Then, we select\nseveral key points on the temporal domain according to the trajectory of the\nforged areas, and mark the forged area of the forged frames in the key point\nwith a mouse. Finally, we use the linear prediction algorithm based on the\ncoordinates of the key positions in the temporal domain to generate the\nannotation information of forged areas in other video frames which without\nmanually labeled. If the bounding box generated by the computer-aided algorithm\ndeviates from the actual location of the forged area, we can use the mouse to\nchange the position of the bounding box during the preview period. This method\ncombines the manual annotation with computer-aided annotation. It solves the\nproblems of the inaccuracy of annotation by computer-aided as well as the low\nefficiency of annotation manually, and meet the needs of annotation for an\nenormous amount of forged videos in the research of video passive forensics.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.02308v1"
    },
    {
        "title": "Compression for Multiple Reconstructions",
        "authors": [
            "Yehuda Dar",
            "Michael Elad",
            "Alfred M. Bruckstein"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this work we propose a method for optimizing the lossy compression for a\nnetwork of diverse reconstruction systems. We focus on adapting a standard\nimage compression method to a set of candidate displays, presenting the\ndecompressed signals to viewers. Each display is modeled as a linear operator\napplied after decompression, and its probability to serve a network user. We\nformulate a complicated operational rate-distortion optimization trading-off\nthe network's expected mean-squared reconstruction error and the compression\nbit-cost. Using the alternating direction method of multipliers (ADMM) we\ndevelop an iterative procedure where the network structure is separated from\nthe compression method, enabling the reliance on standard compression\ntechniques. We present experimental results showing our method to be the best\napproach for adjusting high bit-rate image compression (using the\nstate-of-the-art HEVC standard) to a set of displays modeled as blur\ndegradations.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.03937v1"
    },
    {
        "title": "Coding Block-Level Perceptual Video Coding for 4:4:4 Data in HEVC",
        "authors": [
            "Lee Prangnell",
            "Miguel Hernández-Cabronero",
            "Victor Sanchez"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  There is an increasing consumer demand for high bit-depth 4:4:4 HD video data\nplayback due to its superior perceptual visual quality compared with standard\n8-bit subsampled 4:2:0 video data. Due to vast file sizes and associated\nbitrates, it is desirable to compress raw high bit-depth 4:4:4 HD video\nsequences as much as possible without incurring a discernible decrease in\nvisual quality. In this paper, we propose a Coding Block (CB)-level perceptual\nvideo coding technique for HEVC named Full Color Perceptual Quantization\n(FCPQ). FCPQ is designed to adjust the Quantization Parameter (QP) at the CB\nlevel (i.e., the luma CB and the chroma Cb and Cr CBs) according to the\nvariances of pixel data in each CB. FCPQ is based on the default perceptual\nquantization method in HEVC called AdaptiveQP. AdaptiveQP adjusts the QP of an\nentire CU based only on the spatial activity of the constituent luma CB. As\ndemonstrated in this paper, by not accounting for the spatial activity of the\nconstituent chroma CBs, as is the case with AdaptiveQP, coding performance can\nbe significantly affected; this is because the variance of pixel data in a luma\nCB is notably different from the variances of pixel data in chroma Cb and Cr\nCBs. FCPQ, therefore, addresses this problem. In terms of coding performance,\nFCPQ achieves BD-Rate improvements of up to 39.5% (Y), 16% (Cb) and 29.9% (Cr)\ncompared with AdaptiveQP.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.05884v1"
    },
    {
        "title": "Viewport Adaptation-Based Immersive Video Streaming: Perceptual Modeling\n  and Applications",
        "authors": [
            "Shaowei Xie",
            "Qiu Shen",
            "Yiling Xu",
            "Qiaojian Qian",
            "Shaowei Wang",
            "Zhan Ma",
            "Wenjun Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Immersive video offers the freedom to navigate inside virtualized\nenvironment. Instead of streaming the bulky immersive videos entirely, a\nviewport (also referred to as field of view, FoV) adaptive streaming is\npreferred. We often stream the high-quality content within current viewport,\nwhile reducing the quality of representation elsewhere to save the network\nbandwidth consumption. Consider that we could refine the quality when focusing\non a new FoV, in this paper, we model the perceptual impact of the quality\nvariations (through adapting the quantization stepsize and spatial resolution)\nwith respect to the refinement duration, and yield a product of two closed-form\nexponential functions that well explain the joint quantization and resolution\ninduced quality impact. Analytical model is cross-validated using another set\nof data, where both Pearson and Spearman's rank correlation coefficients are\nclose to 0.98. Our work is devised to optimize the adaptive FoV streaming of\nthe immersive video under limited network resource. Numerical results show that\nour proposed model significantly improves the quality of experience of users,\nwith about 9.36\\% BD-Rate (Bjontegaard Delta Rate) improvement on average as\ncompared to other representative methods, particularly under the limited\nbandwidth.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.06057v1"
    },
    {
        "title": "Adaptive Streaming in Interactive Multiview Video Systems",
        "authors": [
            "Xue Zhang",
            "Laura Toni",
            "Pascal Frossard",
            "Yao Zhao",
            "Chunyu Lin"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Multiview applications endow final users with the possibility to freely\nnavigate within 3D scenes with minimum-delay. A real feeling of scene\nnavigation is enabled by transmitting multiple high-quality camera views, which\ncan be used to synthesize additional virtual views to offer a smooth\nnavigation. However, when network resources are limited, not all camera views\ncan be sent at high quality. It is therefore important, yet challenging, to\nfind the right tradeoff between coding artifacts (reducing the quality of\ncamera views) and virtual synthesis artifacts (reducing the number of camera\nviews sent to users). To this aim, we propose an optimal transmission strategy\nfor interactive multiview HTTP adaptive streaming (HAS). We propose a problem\nformulation to select the optimal set of camera views that the client requests\nfor downloading, such that the navigation quality experienced by the user is\noptimized while the bandwidth constraints are satisfied. We show that our\noptimization problem is NP-hard, and we therefore develop an optimal solution\nbased on the dynamic programming algorithm with polynomial time complexity. To\nfurther simplify the deployment, we present a suboptimal greedy algorithm with\neffective performance and lower complexity. The proposed controller is\nevaluated in theoretical and realistic settings characterized by realistic\nnetwork statistics estimation, buffer management and server-side representation\noptimization. Simulation results show significant improvement in terms of\nnavigation quality compared with alternative baseline multiview adaptation\nlogic solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.08994v4"
    },
    {
        "title": "Perceptual Quality Assessment of Immersive Images Considering Peripheral\n  Vision Impact",
        "authors": [
            "Peiyao Guo",
            "Qiu Shen",
            "Zhan Ma",
            "David J. Brady",
            "Yao Wang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Conventional images/videos are often rendered within the central vision area\nof the human visual system (HVS) with uniform quality. Recent virtual reality\n(VR) device with head mounted display (HMD) extends the field of view (FoV)\nsignificantly to include both central and peripheral vision areas. It exhibits\nthe unequal image quality sensation among these areas because of the\nnon-uniform distribution of photoreceptors on our retina. We propose to study\nthe sensation impact on the image subjective quality with respect to the\neccentric angle $\\theta$ across different vision areas. Often times, image\nquality is controlled by the quantization stepsize $q$ and spatial resolution\n$s$, separately and jointly. Therefore, the sensation impact can be understood\nby exploring the $q$ and/or $s$ in terms of the $\\theta$, resulting in\nself-adaptive analytical models that have shown quite impressive accuracy\nthrough independent cross validations. These models can further be applied to\ngive different quality weights at different regions, so as to significantly\nreduce the transmission data size but without subjective quality loss. As\ndemonstrated in a gigapixel imaging system, we have shown that the image\nrendering can be speed up about 10$\\times$ with the model guided unequal\nquality scales, in comparison to the the legacy scheme with uniform quality\nscales everywhere.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.09065v1"
    },
    {
        "title": "User Satisfaction-Driven Bandwidth Allocation for Image Transmission in\n  a Crowded Environment",
        "authors": [
            "Sandipan Choudhuri",
            "Kaustav Basu",
            "Arunabha Sen"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  A major portion of postings on social networking sites constitute high\nquality digital images and videos. These images and videos require a fairly\nlarge amount of bandwidth during transmission. Accordingly, high quality image\nand video postings become a challenge for the network service provider,\nespecially in a crowded environment where bandwidth is in high demand. In this\npaper we present a user satisfaction driven bandwidth allocation scheme for\nimage transmission in such environments. In an image, there are always objects\nthat stand out more than others. The reason behind some set of objects being\nmore important in a scene is based on a number of visual, as well as, cognitive\nfactors. Being motivated by the fact that user satisfaction is more dependent\non the quality of these salient objects in an image than non-salient ones, we\npropose a quantifiable metric for measuring user-satisfiability (based on image\nquality and delay of transmission). The bandwidth allocation technique proposed\nthereafter, ensures that this user-satisfiability is maximized. Unlike the\nexisting approaches that utilize some fixed set of non-linear functions for\nframing the user-satisfiability index, our metric is modelled over customer\nsurvey data, where the unknown parameters are trained with machine learning\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.09079v1"
    },
    {
        "title": "Image Compression Using Proposed Enhanced Run Length Encoding Algorithm",
        "authors": [
            "Ali H. Husseen Al-nuaimi",
            "Shyamaa Shakir Al-juboori",
            "R. J. Mohammed"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this paper, we will present p roposed enhance process of image compression\nby using RLE algorithm. This proposed yield to decrease the size of compressing\nimage, but the original method used primarily for compressing a binary images\n[1].Which will yield increasing the size of an original image mostly when used\nfor color images. The test of an enhanced algorithm is performed on sample\nconsists of ten BMP 24-bit true color images, building an application by using\nvisual basic 6.0 to show the size after and before compression process and\ncomputing the compression ratio for RLE and for the enhanced RLE algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.00589v1"
    },
    {
        "title": "Intra-Frame Error Concealment Scheme using 3D Reversible Data Hiding in\n  Mobile Cloud Environment",
        "authors": [
            "Yanli Chen",
            "Hongxia Wang",
            "Hanzhou Wu",
            "Yi Chen",
            "Asad Malik"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Data in mobile cloud environment are mainly transmitted via wireless noisy\nchannels, which may result in transmission errors with a high probability due\nto its unreliable connectivity. For video transmission, unreliable connectivity\nmay cause significant degradation of the content. Improving or keeping video\nquality over lossy channel is therefore a very important research topic. Error\nconcealment with data hiding (ECDH) is an effective way to conceal the errors\nintroduced by channels. It can reduce error propagation between neighbor\nblocks/frames comparing with the methods exploiting temporal/spatial\ncorrelations. The existing video ECDH methods often embed the motion vectors\n(MVs) into the specific locations. Nevertheless, specific embedding locations\ncannot resist against random errors. To compensate the unreliable connectivity\nin mobile cloud environment, in this paper, we present a video ECDH scheme\nusing 3D reversible data hiding (RDH), in which each MV is repeated multiple\ntimes, and the repeated MVs are embedded into different macroblocks (MBs)\nrandomly. Though the multiple embedding requires more embedding space,\nsatisfactory trade-off between the introduced distortion and the reconstructed\nvideo quality can be achieved by tuning the repeating times of the MVs. For\nrandom embedding, the lost probability of the MVs decreases rapidly, resulting\nin better error concealment performance. Experimental results show that the\nPSNR values gain about 5dB at least comparing with the existing ECDH methods.\nMeanwhile, the proposed method improves the video quality significantly.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.00935v1"
    },
    {
        "title": "The diveXplore System at the Video Browser Showdown 2018 - Final Notes",
        "authors": [
            "Klaus Schoeffmann",
            "Bernd Münzer",
            "Jürgen Primus",
            "Andreas Leibetseder"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  This short paper provides further details of the diveXplore system (formerly\nknown as CoViSS), which has been used by team ITEC1 for the Video Browser\nShowdown (VBS) 2018. In particular, it gives a short overview of search\nfeatures and some details of final system changes, not included in the\ncorresponding VBS2018 paper, as well as a basic analysis of how the system has\nbeen used for VBS2018 (from a user perspective).\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01863v1"
    },
    {
        "title": "Semi-fragile Tamper Detection and Recovery based on Region\n  Categorization and Two-Sided Circular Block Dependency",
        "authors": [
            "Seyyed Hossein Soleymani"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  This paper presents a new semi-fragile algorithm for image tamper detection\nand recovery, which is based on region attention and two-sided circular block\ndependency. This method categorizes the image blocks into three categories\naccording to their texture. In this method, less information is extracted from\nareas with the smooth texture, and more information is extracted from areas\nwith the rough texture. Also, the extracted information for each type of blocks\nis embedded in another block with the same type. So, changes in the smooth\nareas are invisible to Human Visual System. To increase the localization power\na two-sided circular block dependency is proposed, which is able to distinguish\npartially destroyed blocks. Pairwise block dependency and circular block\ndependency, which are common methods in the block-based tamper detection, are\nnot able to distinguish the partially destroyed blocks. Cubic interpolation is\nused in order to decrease the blocking effects in the recovery phase. The\nresults of the proposed method for regions with different texture show that the\nproposed method is superior to non-region-attention based methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.02680v1"
    },
    {
        "title": "Adaptive Spatial Steganography Based on Probability-Controlled\n  Adversarial Examples",
        "authors": [
            "Sai Ma",
            "Qingxiao Guan",
            "Xianfeng Zhao",
            "Yaqi Liu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Explanation from Sai Ma: The experiments in this paper are conducted on Caffe\nframework. In Caffe, there is an API to directly set the gradient in Matlab. I\nwrongly use it to control the 'probability', in fact, I modify the gradient\ndirectly. The misusage of API leads to wrong experiment results, and wrong\ntheoretical analysis.\n  Apologize to readers who have read this paper. We have submitted a correct\nversion of this paper to Multimedia Tools and Applications and it is under\nrevision.\n  Thanks to Dr. Patrick Bas, who is the Associate Editor of TIFS and the\nanonymous reviewers of this paper.\n  Thanks to Tingting Song from Sun Yat-sen University. We discussed some\nproblems of this paper. Her advice helps me to improve the submitted paper to\nMultimedia Tools and Applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.02691v3"
    },
    {
        "title": "DeepQoE: A unified Framework for Learning to Predict Video QoE",
        "authors": [
            "Huaizheng Zhang",
            "Han Hu",
            "Guanyu Gao",
            "Yonggang Wen",
            "Kyle Guan"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Motivated by the prowess of deep learning (DL) based techniques in\nprediction, generalization, and representation learning, we develop a novel\nframework called DeepQoE to predict video quality of experience (QoE). The\nend-to-end framework first uses a combination of DL techniques (e.g., word\nembeddings) to extract generalized features. Next, these features are combined\nand fed into a neural network for representation learning. Such representations\nserve as inputs for classification or regression tasks. Evaluating the\nperformance of DeepQoE with two datasets, we show that for the small dataset,\nthe accuracy of all shallow learning algorithm is improved by using the\nrepresentation derived from DeepQoE. For the large dataset, our DeepQoE\nframework achieves significant performance improvement in comparison to the\nbest baseline method (90.94% vs. 82.84%). Moreover, DeepQoE, also released as\nan open source tool, provides video QoE research much-needed flexibility in\nfitting different datasets, extracting generalized features, and learning\nrepresentations.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03481v1"
    },
    {
        "title": "The PS-Battles Dataset - an Image Collection for Image Manipulation\n  Detection",
        "authors": [
            "Silvan Heller",
            "Luca Rossetto",
            "Heiko Schuldt"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The boost of available digital media has led to a significant increase in\nderivative work. With tools for manipulating objects becoming more and more\nmature, it can be very difficult to determine whether one piece of media was\nderived from another one or tampered with. As derivations can be done with\nmalicious intent, there is an urgent need for reliable and easily usable\ntampering detection methods. However, even media considered semantically\nuntampered by humans might have already undergone compression steps or light\npost-processing, making automated detection of tampering susceptible to false\npositives. In this paper, we present the PS-Battles dataset which is gathered\nfrom a large community of image manipulation enthusiasts and provides a basis\nfor media derivation and manipulation detection in the visual domain. The\ndataset consists of 102'028 images grouped into 11'142 subsets, each containing\nthe original image as well as a varying number of manipulated derivatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.04866v1"
    },
    {
        "title": "A survey of comics research in computer science",
        "authors": [
            "Olivier Augereau",
            "Motoi Iwata",
            "Koichi Kise"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Graphical novels such as comics and mangas are well known all over the world.\nThe digital transition started to change the way people are reading comics,\nmore and more on smartphones and tablets and less and less on paper. In the\nrecent years, a wide variety of research about comics has been proposed and\nmight change the way comics are created, distributed and read in future years.\nEarly work focuses on low level document image analysis: indeed comic books are\ncomplex, they contains text, drawings, balloon, panels, onomatopoeia, etc.\nDifferent fields of computer science covered research about user interaction\nand content generation such as multimedia, artificial intelligence,\nhuman-computer interaction, etc. with different sets of values. We propose in\nthis paper to review the previous research about comics in computer science, to\nstate what have been done and to give some insights about the main outlooks.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.05490v1"
    },
    {
        "title": "Multimodal Co-Training for Selecting Good Examples from Webly Labeled\n  Video",
        "authors": [
            "Ryota Hinami",
            "Junwei Liang",
            "Shin'ichi Satoh",
            "Alexander Hauptmann"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  We tackle the problem of learning concept classifiers from videos on the web\nwithout using manually labeled data. Although metadata attached to videos\n(e.g., video titles, descriptions) can be of help collecting training data for\nthe target concept, the collected data is often very noisy. The main challenge\nis therefore how to select good examples from noisy training data. Previous\napproaches firstly learn easy examples that are unlikely to be noise and then\ngradually learn more complex examples. However, hard examples that are much\ndifferent from easy ones are never learned. In this paper, we propose an\napproach called multimodal co-training (MMCo) for selecting good examples from\nnoisy training data. MMCo jointly learns classifiers for multiple modalities\nthat complement each other to select good examples. Since MMCo selects examples\nby consensus of multimodal classifiers, a hard example for one modality can\nstill be used as a training example by exploiting the power of the other\nmodalities. The algorithm is very simple and easily implemented but yields\nconsistent and significant boosts in example selection and classification\nperformance on the FCVID and YouTube8M benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06057v1"
    },
    {
        "title": "Reversible Video Data Hiding Using Zero QDCT Coefficient-Pairs",
        "authors": [
            "Yi Chen",
            "Hongxia Wang",
            "Hanzhou Wu",
            "Yong Liu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  H.264/Advanced Video Coding (AVC) is one of the most commonly used video\ncompression standard currently. In this paper, we propose a Reversible Data\nHiding (RDH) method based on H.264/AVC videos. In the proposed method, the\nmacroblocks with intra-frame $4\\times 4$ prediction modes in intra frames are\nfirst selected as embeddable blocks. Then, the last zero Quantized Discrete\nCosine Transform (QDCT) coefficients in all $4\\times 4$ blocks of the\nembeddable macroblocks are paired. In the following, a modification mapping\nrule based on making full use of modification directions are given. Finally,\neach zero coefficient-pair is changed by combining the given mapping rule with\nthe to-be-embedded information bits. Since most of last QDCT coefficients in\nall $4\\times 4$ blocks are zero and they are located in high frequency area.\nTherefore, the proposed method can obtain high embedding capacity and low\ndistortion.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06628v2"
    },
    {
        "title": "An Improved Reversible Data Hiding Scheme by Changing Modification\n  Direction of Partial Coefficients in JPEG Images",
        "authors": [
            "Yi Chen",
            "Hongxia Wang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  This paper first reviews the reversible data hiding scheme, of Liu et al. in\n2018, for JPEG images. After that, a novel reversible data hiding scheme, in\nwhich modification directions of partial nonzero quantized alternating current\n(AC) coefficients are utilized to decrease distortion and file size increase\ncaused by data hiding, is proposed. Experimental results have shown that the\nproposed scheme has indeed advantages in visual quality and smaller increase in\nfile size of marked JPEG images while compared to the state-of-the-art scheme\nwith the same embedding payload so far.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06645v3"
    },
    {
        "title": "Simple Yet Efficient Content Based Video Copy Detection",
        "authors": [
            "Jörg P. Bachmann",
            "Benjamin Hauskeller"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Given a collection of videos, how to detect content-based copies efficiently\nwith high accuracy? Detecting copies in large video collections still remains\none of the major challenges of multimedia retrieval. While many video copy\ndetection approaches show high computation times and insufficient quality, we\npropose a new efficient content-based video copy detection algorithm improving\nboth aspects. The idea of our approach consists in utilizing self-similarity\nmatrices as video descriptors in order to capture different visual properties.\nWe benchmark our algorithm on the MuscleVCD ST1 benchmark dataset and show that\nour approach is able to achieve a score of 100\\% and a score of at least 93\\%\nin a wide range of parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.07019v1"
    },
    {
        "title": "Spatial Image Steganography Based on Generative Adversarial Network",
        "authors": [
            "Jianhua Yang",
            "Kai Liu",
            "Xiangui Kang",
            "Edward K. Wong",
            "Yun-Qing Shi"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  With the recent development of deep learning on steganalysis, embedding\nsecret information into digital images faces great challenges. In this paper, a\nsecure steganography algorithm by using adversarial training is proposed. The\narchitecture contain three component modules: a generator, an embedding\nsimulator and a discriminator. A generator based on U-NET to translate a cover\nimage into an embedding change probability is proposed. To fit the optimal\nembedding simulator and propagate the gradient, a function called\nTanh-simulator is proposed. As for the discriminator, the selection-channel\nawareness (SCA) is incorporated to resist the SCA based steganalytic methods.\nExperimental results have shown that the proposed framework can increase the\nsecurity performance dramatically over the recently reported method ASDL-GAN,\nwhile the training time is only 30% of that used by ASDL-GAN. Furthermore, it\nalso performs better than the hand-crafted steganographic algorithm S-UNIWARD.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.07939v1"
    },
    {
        "title": "Rate-Utility Optimized Streaming of Volumetric Media for Augmented\n  Reality",
        "authors": [
            "Jounsup Park",
            "Philip A. Chou",
            "Jenq-Neng Hwang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Volumetric media, popularly known as holograms, need to be delivered to users\nusing both on-demand and live streaming, for new augmented reality (AR) and\nvirtual reality (VR) experiences. As in video streaming, hologram streaming\nmust support network adaptivity and fast startup, but must also moderate large\nbandwidths, multiple simultaneously streaming objects, and frequent user\ninteraction, which requires low delay. In this paper, we introduce the first\nsystem to our knowledge designed specifically for streaming volumetric media.\nThe system reduces bandwidth by introducing 3D tiles, and culling them or\nreducing their level of detail depending on their relation to the user's view\nfrustum and distance to the user. Our system reduces latency by introducing a\nwindow-based buffer, which in contrast to a queue-based buffer allows\ninsertions near the head of the buffer rather than only at the tail of the\nbuffer, to respond quickly to user interaction. To allocate bits between\ndifferent tiles across multiple objects, we introduce a simple greedy yet\nprovably optimal algorithm for rate-utility optimization. We introduce utility\nmeasures based not only on the underlying quality of the representation, but on\nthe level of detail relative to the user's viewpoint and device resolution.\nSimulation results show that the proposed algorithm provides superior quality\ncompared to existing video-streaming approaches adapted to hologram streaming,\nin terms of utility and user experience over variable, throughput-constrained\nnetworks.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.09864v1"
    },
    {
        "title": "Hybrid Point Cloud Attribute Compression Using Slice-based Layered\n  Structure and Block-based Intra Prediction",
        "authors": [
            "Yiting Shao",
            "Qi Zhang",
            "Ge Li",
            "Zhu Li"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Point cloud compression is a key enabler for the emerging applications of\nimmersive visual communication, autonomous driving and smart cities, etc. In\nthis paper, we propose a hybrid point cloud attribute compression scheme built\non an original layered data structure. First, a slice-partition scheme and\ngeometry-adaptive k dimensional-tree (kd-tree) method are devised to generate\nthe four-layer structure. Second, we introduce an efficient block-based intra\nprediction scheme containing a DC prediction mode and several angular modes, in\norder to exploit the spatial correlation between adjacent points. Third, an\nadaptive transform scheme based on Graph Fourier Transform (GFT) is Lagrangian\noptimized to achieve better transform efficiency. The Lagrange multiplier is\noff-line derived based on the statistics of color attribute coding. Last but\nnot least, multiple reordering scan modes are dedicated to improve coding\nefficiency for entropy coding. In intra-frame compression of point cloud color\nattributes, results demonstrate that our method performs better than the\nstate-of-the-art region-adaptive hierarchical transform (RAHT) system, and on\naverage a 29.37$\\%$ BD-rate gain is achieved. Comparing with the test model for\ncategory 1 (TMC1) anchor's coding results, which were recently published by\nMPEG-3DG group on 121st meeting, a 16.37$\\%$ BD-rate gain is obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10783v1"
    },
    {
        "title": "Dynamic Adaptive Point Cloud Streaming",
        "authors": [
            "Mohammad Hosseini",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  High-quality point clouds have recently gained interest as an emerging form\nof representing immersive 3D graphics. Unfortunately, these 3D media are bulky\nand severely bandwidth intensive, which makes it difficult for streaming to\nresource-limited and mobile devices. This has called researchers to propose\nefficient and adaptive approaches for streaming of high-quality point clouds.\n  In this paper, we run a pilot study towards dynamic adaptive point cloud\nstreaming, and extend the concept of dynamic adaptive streaming over HTTP\n(DASH) towards DASH-PC, a dynamic adaptive bandwidth-efficient and view-aware\npoint cloud streaming system. DASH-PC can tackle the huge bandwidth demands of\ndense point cloud streaming while at the same time can semantically link to\nhuman visual acuity to maintain high visual quality when needed. In order to\ndescribe the various quality representations, we propose multiple thinning\napproaches to spatially sub-sample point clouds in the 3D space, and design a\nDASH Media Presentation Description manifest specific for point cloud\nstreaming. Our initial evaluations show that we can achieve significant\nbandwidth and performance improvement on dense point cloud streaming with minor\nnegative quality impacts compared to the baseline scenario when no adaptations\nis applied.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10878v2"
    },
    {
        "title": "A blind robust watermarking method based on Arnold Cat map and amplified\n  pseudo-noise strings with weak correlation",
        "authors": [
            "Seyyed Hossein Soleymani",
            "Amir Hossein Taherinia",
            "Amir Hossein Mohajerzadeh"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this paper, a robust and blind watermarking method is proposed, which is\nhighly resistant to the common image watermarking attacks, such as noises,\ncompression, and image quality enhancement processing. In this method, Arnold\nCat map is used as a pre-processing on the host image, which increases the\nsecurity and imperceptibility of embedding watermark bits with a strong gain\nfactor. Moreover, two pseudo-noise strings with weak correlation are used as\nthe symbol of each 0 or 1 bit of the watermark, which increases the accuracy in\ndetecting the state of watermark bits at extraction phase in comparison to\nusing two random pseudo-noise strings. In this method, to increase the\nrobustness and further imperceptibility of the embedding, the Arnold Cat mapped\nimage is subjected to non-overlapping blocking, and then the high frequency\ncoefficients of the approximation sub-band of the FDCuT transform are used as\nthe embedding location for each block. Comparison of the proposed method with\nrecent robust methods under the same experimental conditions indicates the\nsuperiority of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.11240v1"
    },
    {
        "title": "Realistic Multimedia Tools based on Physical Models: I. The Spectrum\n  Analyzer and Animator (SA2)",
        "authors": [
            "I. Pachoulakis"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The present sequence of two articles reports on a custom-built toolkit\nimplementing a technique similar to multi-directional medical tomography to\nsimulate and visualize the composite 3D structure of winds from hot close\ndouble stars. In such hot binaries, the light sources scanning and probing the\ncomposite wind volume are the bright \"surfaces\" (photospheres) of the\nindividual stars. Then, as the Keplerian orbit is traced out and the geometry\npresented to the observer varies, each star constitutes an analyzer upon its\ncompanion's wind. In contrast to medical tomography, however, these targets are\ntoo far to be resolved spatially so we resort to modeling the ultraviolet (UV)\nspectral lines of certain wind ions (e.g., N+4, Si+3, C+3) whose shapes vary\nwith Keplerian phase as the stars revolve around their common centre of mass.\nThe flagships of the toolkit are the Spectrum Analyzer and Animator (SA 2 ) and\nthe Binary 3D Renderer (B3dR). The SA 2 is the subject of the present article\n(paper I). It automates (a) the derivation of light curves from the observed\nspectra and (b) the generation of synthetic binary wind-line profiles which\nreproduce the morphologies and variabilities of the observed wind profiles. The\nsecond tool, the B3dR is discussed in paper II.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.00207v1"
    },
    {
        "title": "Analysis of Problem Tokens to Rank Factors Impacting Quality in VoIP\n  Applications",
        "authors": [
            "Jayant Gupchup",
            "Yasaman Hosseinkashi",
            "Martin Ellis",
            "Sam Johnson",
            "Ross Cutler"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  User-perceived quality-of-experience (QoE) in internet telephony systems is\ncommonly evaluated using subjective ratings computed as a Mean Opinion Score\n(MOS). In such systems, while user MOS can be tracked on an ongoing basis, it\ndoes not give insight into which factors of a call induced any perceived\ndegradation in QoE -- it does not tell us what caused a user to have a\nsub-optimal experience. For effective planning of product improvements, we are\ninterested in understanding the impact of each of these degrading factors,\nallowing the estimation of the return (i.e., the improvement in user QoE) for a\ngiven investment. To obtain such insights, we advocate the use of an\nend-of-call \"problem token questionnaire\" (PTQ) which probes the user about\ncommon call quality issues (e.g., distorted audio or frozen video) which they\nmay have experienced. In this paper, we show the efficacy of this questionnaire\nusing data gathered from over 700,000 end-of-call surveys gathered from Skype\n(a large commercial VoIP application). We present a method to rank call quality\nand reliability issues and address the challenge of isolating independent\nfactors impacting the QoE. Finally, we present representative examples of how\nthese problem tokens have proven to be useful in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.00373v1"
    },
    {
        "title": "Delay-Constrained Rate Control for Real-Time Video Streaming with\n  Bounded Neural Network",
        "authors": [
            "Tianchi Huang",
            "Rui-Xiao Zhang",
            "Chao Zhou",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Rate control is widely adopted during video streaming to provide both high\nvideo qualities and low latency under various network conditions. However,\ndespite that many work have been proposed, they fail to tackle one major\nproblem: previous methods determine a future transmission rate as a single for\nvalue which will be used in an entire time-slot, while real-world network\nconditions, unlike lab setup, often suffer from rapid and stochastic changes,\nresulting in the failures of predictions.\n  In this paper, we propose a delay-constrained rate control approach based on\nend-to-end deep learning. The proposed model predicts future bit rate not as a\nsingle value, but as possible bit rate ranges using target delay gradient, with\nwhich the transmission delay is guaranteed. We collect a large scale of\nreal-world live streaming data to train our model, and as a result, it\nautomatically learns the correlation between throughput and target delay\ngradient. We build a testbed to evaluate our approach. Compared with the\nstate-of-the-art methods, our approach demonstrates a better performance in\nbandwidth utilization. In all considered scenarios, a range based rate control\napproach outperforms the one without range by 19% to 35% in average QoE\nimprovement.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.00619v1"
    },
    {
        "title": "Competitive Video Retrieval with vitrivr at the Video Browser Showdown\n  2018 - Final Notes",
        "authors": [
            "Luca Rossetto",
            "Ivan Giangreco",
            "Ralph Gasser",
            "Heiko Schuldt"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  This paper presents an after-the-fact summary of the participation of the\nvitrivr system to the 2018 Video Browser Showdown. A particular focus is on\nadditions made since the original publication and the systems performance\nduring the competition.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.02371v1"
    },
    {
        "title": "QARC: Video Quality Aware Rate Control for Real-Time Video Streaming via\n  Deep Reinforcement Learning",
        "authors": [
            "Tianchi Huang",
            "Rui-Xiao Zhang",
            "Chao Zhou",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Due to the fluctuation of throughput under various network conditions, how to\nchoose a proper bitrate adaptively for real-time video streaming has become an\nupcoming and interesting issue. Recent work focuses on providing high video\nbitrates instead of video qualities. Nevertheless, we notice that there exists\na trade-off between sending bitrate and video quality, which motivates us to\nfocus on how to get a balance between them. In this paper, we propose QARC\n(video Quality Awareness Rate Control), a rate control algorithm that aims to\nhave a higher perceptual video quality with possibly lower sending rate and\ntransmission latency. Starting from scratch, QARC uses deep reinforcement\nlearning(DRL) algorithm to train a neural network to select future bitrates\nbased on previously observed network status and past video frames, and we\ndesign a neural network to predict future perceptual video quality as a vector\nfor taking the place of the raw picture in the DRL's inputs. We evaluate QARC\nover a trace-driven emulation. As excepted, QARC betters existing approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.02482v3"
    },
    {
        "title": "CloudAR: A Cloud-based Framework for Mobile Augmented Reality",
        "authors": [
            "Wenxiao Zhang",
            "Sikun Lin",
            "Farshid Hassani Bijarbooneh",
            "Hao Fei Cheng",
            "And Pan Hui"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Computation capabilities of recent mobile devices enable natural feature\nprocessing for Augmented Reality (AR). However, mobile AR applications are\nstill faced with scalability and performance challenges. In this paper, we\npropose CloudAR, a mobile AR framework utilizing the advantages of cloud and\nedge computing through recognition task offloading. We explore the design space\nof cloud-based AR exhaustively and optimize the offloading pipeline to minimize\nthe time and energy consumption. We design an innovative tracking system for\nmobile devices which provides lightweight tracking in 6 degree of freedom\n(6DoF) and hides the offloading latency from users' perception. We also design\na multi-object image retrieval pipeline that executes fast and accurate image\nrecognition tasks on servers. In our evaluations, the mobile AR application\nbuilt with the CloudAR framework runs at 30 frames per second (FPS) on average\nwith precise tracking of only 1~2 pixel errors and image recognition of at\nleast 97% accuracy. Our results also show that CloudAR outperforms one of the\nleading commercial AR framework in several performance metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03060v1"
    },
    {
        "title": "Optimization of Occlusion-Inducing Depth Pixels in 3-D Video Coding",
        "authors": [
            "Pan Gao",
            "Cagri Ozcinar",
            "Aljosa Smolic"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The optimization of occlusion-inducing depth pixels in depth map coding has\nreceived little attention in the literature, since their associated texture\npixels are occluded in the synthesized view and their effect on the synthesized\nview is considered negligible. However, the occlusion-inducing depth pixels\nstill need to consume the bits to be transmitted, and will induce geometry\ndistortion that inherently exists in the synthesized view. In this paper, we\npropose an efficient depth map coding scheme specifically for the\nocclusion-inducing depth pixels by using allowable depth distortions. Firstly,\nwe formulate a problem of minimizing the overall geometry distortion in the\nocclusion subject to the bit rate constraint, for which the depth distortion is\nproperly adjusted within the set of allowable depth distortions that introduce\nthe same disparity error as the initial depth distortion. Then, we propose a\ndynamic programming solution to find the optimal depth distortion vector for\nthe occlusion. The proposed algorithm can improve the coding efficiency without\nalteration of the occlusion order. Simulation results confirm the performance\nimprovement compared to other existing algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03105v1"
    },
    {
        "title": "Enhancing HEVC Compressed Videos with a Partition-masked Convolutional\n  Neural Network",
        "authors": [
            "Xiaoyi He",
            "Qiang Hu",
            "Xintong Han",
            "Xiaoyun Zhang",
            "Chongyang Zhang",
            "Weiyao Lin"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this paper, we propose a partition-masked Convolution Neural Network (CNN)\nto achieve compressed-video enhancement for the state-of-the-art coding\nstandard, High Efficiency Video Coding (HECV). More precisely, our method\nutilizes the partition information produced by the encoder to guide the quality\nenhancement process. In contrast to existing CNN-based approaches, which only\ntake the decoded frame as the input to the CNN, the proposed approach considers\nthe coding unit (CU) size information and combines it with the distorted\ndecoded frame such that the degradation introduced by HEVC is reduced more\nefficiently. Experimental results show that our approach leads to over 9.76%\nBD-rate saving on benchmark sequences, which achieves the state-of-the-art\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03894v2"
    },
    {
        "title": "Video Processing on the Edge for Multimedia IoT Systems",
        "authors": [
            "Yang Cao",
            "Zeyu Xu",
            "Peng Qin",
            "Tao Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this article, we first survey the current situation of video processing on\nthe edge for multimedia Internet-of-Things (M-IoT) systems in three typical\nscenarios, i.e., smart cities, satellite networks, and Internet-of-Vehicles. By\nsummarizing a general model of the edge video processing, the importance of\ndeveloping an edge computing platform is highlighted. Then, we give a method of\nimplementing cooperative video processing on an edge computing platform based\non light-weighted virtualization technologies. Performance evaluation is\nconducted and some insightful observations can be obtained. Moreover, we\nsummarize challenges and opportunities of realizing effective edge video\nprocessing for M-IoT systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.04837v1"
    },
    {
        "title": "Visual Comfort Assessment for Stereoscopic Image Retargeting",
        "authors": [
            "Ya Zhou",
            "Wei Zhou",
            "Ping An",
            "Zhibo Chen"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In recent years, visual comfort assessment (VCA) for 3D/stereoscopic content\nhas aroused extensive attention. However, much less work has been done on the\nperceptual evaluation of stereoscopic image retargeting. In this paper, we\nfirst build a Stereoscopic Image Retargeting Database (SIRD), which contains\nsource images and retargeted images produced by four typical stereoscopic\nretargeting methods. Then, the subjective experiment is conducted to assess\nfour aspects of visual distortion, i.e. visual comfort, image quality, depth\nquality and the overall quality. Furthermore, we propose a Visual Comfort\nAssessment metric for Stereoscopic Image Retargeting (VCA-SIR). Based on the\ncharacteristics of stereoscopic retargeted images, the proposed model\nintroduces novel features like disparity range, boundary disparity as well as\ndisparity intensity distribution into the assessment model. Experimental\nresults demonstrate that VCA-SIR can achieve high consistency with subjective\nperception.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.05575v1"
    },
    {
        "title": "A practical convolutional neural network as loop filter for intra frame",
        "authors": [
            "Xiaodan Song",
            "Jiabao Yao",
            "Lulu Zhou",
            "Li Wang",
            "Xiaoyang Wu",
            "Di Xie",
            "Shiliang Pu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Loop filters are used in video coding to remove artifacts or improve\nperformance. Recent advances in deploying convolutional neural network (CNN) to\nreplace traditional loop filters show large gains but with problems for\npractical application. First, different model is used for frames encoded with\ndifferent quantization parameter (QP), respectively. It is expensive for\nhardware. Second, float points operation in CNN leads to inconsistency between\nencoding and decoding across different platforms. Third, redundancy within CNN\nmodel consumes precious computational resources.\n  This paper proposes a CNN as the loop filter for intra frames and proposes a\nscheme to solve the above problems. It aims to design a single CNN model with\nlow redundancy to adapt to decoded frames with different qualities and ensure\nconsistency. To adapt to reconstructions with different qualities, both\nreconstruction and QP are taken as inputs. After training, the obtained model\nis compressed to reduce redundancy. To ensure consistency, dynamic fixed points\n(DFP) are adopted in testing CNN. Parameters in the compressed model are first\nquantized to DFP and then used for inference of CNN. Outputs of each layer in\nCNN are computed by DFP operations. Experimental results on JEM 7.0 report\n3.14%, 5.21%, 6.28% BD-rate savings for luma and two chroma components with all\nintra configuration when replacing all traditional filters.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06121v1"
    },
    {
        "title": "Robust curvelet domain watermarking technique that preserves cleanness\n  of high quality images",
        "authors": [
            "Wook-Hyung Kim",
            "Seung-Hun Nam",
            "Ji-Hyeon Kang",
            "Heung-Kyu Lee"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Watermarking inserts invisible data into content to protect copyright. The\nembedded information provides proof of authorship and facilitates tracking\nillegal distribution, etc. Current robust watermarking techniques have been\nproposed to preserve inserted copyright information from various attacks, such\nas content modification and watermark removal attack. However, since the\nwatermark is inserted in the form of noise, there is an inevitable effect of\nreducing content visual quality. In general, more robust watermarking\ntechniques tend to have larger effect on the quality, and content creators and\nusers are often reluctant to insert watermarks. Thus, there is a demand for a\nwatermark that maintains maximum image quality, even if the watermark\nperformance is slightly inferior. Therefore, we propose a watermarking\ntechnique that maximizes invisibility while maintaining sufficient robustness\nand data capacity enough to be applied for real situations. The proposed method\nminimizes watermarking energy by adopting curvelet domain multi-directional\ndecomposition to maximize invisibility, and maximizes robustness against signal\nprocessing attack by watermarking pattern suitable for curvelet transformation.\nThe method is also robust against geometric attack by employing watermark\ndetection method utilizing curvelet characteristics. The proposed method showed\nvery good results of 57.65 dB peak signal-to-noise ratio in fidelity tests, and\nmean opinion score showed that images treated with the proposed method were\nhardly distinguishable from the originals. The proposed technique also showed\ngood robustness against signal processing and geometric attacks compared with\nexisting techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06181v1"
    },
    {
        "title": "Convolutional Neural Network Architecture for Recovering Watermark\n  Synchronization",
        "authors": [
            "Wook-Hyung Kim",
            "Jong-Uk Hou",
            "Seung-Min Mun",
            "Heung-Kyu Lee"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Since real-time contents can be captured and downloaded very easily,\ncopyright infringement has become a serious problem. In order to reduce the\nloss caused by copyright infringement, copyright owners insert a watermark in\nthe content to protect the copyright using illegal distribution route tracking\nand copyright authentication. However, whereas many existing watermarking\ntechniques are robust to signal distortion such as compression, they are\nvulnerable to geometric distortion that causes synchronization errors. In\nparticular, capturing real-time content in Internet browsers and smartphone\napplications is problematic because geometric distortion such as scaling and\ntranslation frequently occurs. In this paper, we propose a convolutional neural\nnetwork-based template architecture that compensates for the disadvantages of\nexisting watermarking techniques that are vulnerable to geometric distortion.\nThe proposed template consists of a template generation network, a template\nextraction network, and a template matching network. The template generation\nnetwork generates a template in the form of noise and the template is inserted\ninto certain pre-defined spatial locations of the image. The extraction network\ndetects spatial locations where the template is inserted in the image. Finally,\nthe template matching network estimates the parameters of the geometric\ndistortion by comparing the shape of spatial locations where the template was\ninserted with the locations where the template was detected. It is possible to\nrecover an image in its original geometrical form using the estimated\nparameters, and as a result, watermarks applied using existing watermarking\ntechniques that are vulnerable to geometric distortion can be decoded normally.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06199v1"
    },
    {
        "title": "Modeling Continuous Video QoE Evolution: A State Space Approach",
        "authors": [
            "Nagabhushan Eswara",
            "Hemanth P. Sethuram",
            "Soumen Chakraborty",
            "Kiran Kuchi",
            "Abhinav Kumar",
            "Sumohana S. Channappayya"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  A rapid increase in the video traffic together with an increasing demand for\nhigher quality videos has put a significant load on content delivery networks\nin the recent years. Due to the relatively limited delivery infrastructure, the\nvideo users in HTTP streaming often encounter dynamically varying quality over\ntime due to rate adaptation, while the delays in video packet arrivals result\nin rebuffering events. The user quality-of-experience (QoE) degrades and varies\nwith time because of these factors. Thus, it is imperative to monitor the QoE\ncontinuously in order to minimize these degradations and deliver an optimized\nQoE to the users. Towards this end, we propose a nonlinear state space model\nfor efficiently and effectively predicting the user QoE on a continuous time\nbasis. The QoE prediction using the proposed approach relies on a state space\nthat is defined by a set of carefully chosen time varying QoE determining\nfeatures. An evaluation of the proposed approach conducted on two publicly\navailable continuous QoE databases shows a superior QoE prediction performance\nover the state-of-the-art QoE modeling approaches. The evaluation results also\ndemonstrate the efficacy of the selected features and the model order employed\nfor predicting the QoE. Finally, we show that the proposed model is completely\nstate controllable and observable, so that the potential of state space\nmodeling approaches can be exploited for further improving QoE prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06335v1"
    },
    {
        "title": "Extraction and Analysis of Dynamic Conversational Networks from TV\n  Series",
        "authors": [
            "Xavier Bost",
            "Vincent Labatut",
            "Serigne Gueye",
            "Georges Linarès"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Identifying and characterizing the dynamics of modern tv series subplots is\nan open problem. One way is to study the underlying social network of\ninteractions between the characters. Standard dynamic network extraction\nmethods rely on temporal integration, either over the whole considered period,\nor as a sequence of several time-slices. However, they turn out to be\ninappropriate in the case of tv series, because the scenes shown onscreen\nalternatively focus on parallel storylines, and do not necessarily respect a\ntraditional chronology. In this article, we introduce Narrative Smoothing, a\nnovel network extraction method taking advantage of the plot properties to\nsolve some of their limitations. We apply our method to a corpus of 3 popular\nseries, and compare it to both standard approaches. Narrative smoothing leads\nto more relevant observations when it comes to the characterization of the\nprotagonists and their relationships, confirming its appropriateness to model\nthe intertwined storylines constituting the plots.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06782v1"
    },
    {
        "title": "Surface Light Field Compression using a Point Cloud Codec",
        "authors": [
            "Xiang Zhang",
            "Philip A. Chou",
            "Ming-Ting Sun",
            "Maolong Tang",
            "Shanshe Wang",
            "Siwei Ma",
            "Wen Gao"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Light field (LF) representations aim to provide photo-realistic,\nfree-viewpoint viewing experiences. However, the most popular LF\nrepresentations are images from multiple views. Multi-view image-based\nrepresentations generally need to restrict the range or degrees of freedom of\nthe viewing experience to what can be interpolated in the image domain,\nessentially because they lack explicit geometry information. We present a new\nsurface light field (SLF) representation based on explicit geometry, and a\nmethod for SLF compression. First, we map the multi-view images of a scene onto\na 3D geometric point cloud. The color of each point in the point cloud is a\nfunction of viewing direction known as a view map. We represent each view map\nefficiently in a B-Spline wavelet basis. This representation is capable of\nmodeling diverse surface materials and complex lighting conditions in a highly\nscalable and adaptive manner. The coefficients of the B-Spline wavelet\nrepresentation are then compressed spatially. To increase the spatial\ncorrelation and thus improve compression efficiency, we introduce a smoothing\nterm to make the coefficients more similar across the 3D space. We compress the\ncoefficients spatially using existing point cloud compression (PCC) methods. On\nthe decoder side, the scene is rendered efficiently from any viewing direction\nby reconstructing the view map at each point. In contrast to multi-view\nimage-based LF approaches, our method supports photo-realistic rendering of\nreal-world scenes from arbitrary viewpoints, i.e., with an unlimited six\ndegrees of freedom (6DOF). In terms of rate and distortion, experimental\nresults show that our method achieves superior performance with lighter decoder\ncomplexity compared with a reference image-plus-geometry compression (IGC)\nscheme, indicating its potential in practical virtual and augmented reality\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.11203v1"
    },
    {
        "title": "SVS-JOIN: Efficient Spatial Visual Similarity Join over Multimedia Data",
        "authors": [
            "Chengyuan Zhang",
            "Ruipeng Chen",
            "Lei Zhu",
            "Zuping Zhang",
            "Fang Huang",
            "Yunwu Lin"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In the big data era, massive amount of multimedia data with geo-tags has been\ngenerated and collected by mobile smart devices equipped with mobile\ncommunications module and position sensor module. This trend has put forward\nhigher request on large-scale of geo-multimedia data retrieval. Spatial\nsimilarity join is one of the important problem in the area of spatial\ndatabase. Previous works focused on textual document with geo-tags, rather than\ngeo-multimedia data such as geo-images. In this paper, we study a novel search\nproblem named spatial visual similarity join (SVS-JOIN for short), which aims\nto find similar geo-image pairs in both the aspects of geo-location and visual\ncontent. We propose the definition of SVS-JOIN at the first time and present\nhow to measure geographical similarity and visual similarity. Then we introduce\na baseline inspired by the method for textual similarity join and a extension\nnamed SVS-JOIN$_G$ which applies spatial grid strategy to improve the\nefficiency. To further improve the performance of search, we develop a novel\napproach called SVS-JOIN$_Q$ which utilizes a quadtree and a global inverted\nindex. Experimental evaluations on real geo-image datasets demonstrate that our\nsolution has a really high performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.00549v1"
    },
    {
        "title": "3D Holoscopic Imaging for Cultural Heritage Digitalisation",
        "authors": [
            "Taha Alfaqheri",
            "Seif Allah El Mesloul Nasri",
            "Abdul Hamid Sadka"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The growing interest in archaeology has enabled the discovery of an immense\nnumber of cultural heritage assets and historical sites. Hence, preservation of\nCH through digitalisation is becoming a primordial requirement for many\ncountries as a part of national cultural programs. However, CH digitalisation\nis still posing serious challenges such as cost and time-consumption. In this\nmanuscript, 3D holoscopic (H3D) technology is applied to capture small sized CH\nassets. The H3D camera utilises micro lens array within a single aperture lens\nand typical 2D sensor to acquire 3D information. This technology allows 3D\nautostereoscopic visualisation with full motion parallax if convenient\nMicrolens Array (MLA)is used on the display side. Experimental works have shown\neasiness and simplicity of H3D acquisition compared to existing technologies.\nIn fact, H3D capture process took an equal time of shooting a standard 2D\nimage. These advantages qualify H3D technology to be cost effective and\ntime-saving technology for cultural heritage 3D digitisation.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.03916v1"
    },
    {
        "title": "Inferring User Gender from User Generated Visual Content on a Deep\n  Semantic Space",
        "authors": [
            "David Semedo",
            "João Magalhães",
            "Flávio Martins"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this paper we address the task of gender classification on picture sharing\nsocial media networks such as Instagram and Flickr. We aim to infer the gender\nof an user given only a small set of the images shared in its profile. We make\nthe assumption that user's images contain a collection of visual elements that\nimplicitly encode discriminative patterns that allow inferring its gender, in a\nlanguage independent way. This information can then be used in personalisation\nand recommendation. Our main hypothesis is that semantic visual features are\nmore adequate for discriminating high-level classes.\n  The gender detection task is formalised as: given an user's profile,\nrepresented as a bag of images, we want to infer the gender of the user. Social\nmedia profiles can be noisy and contain confounding factors, therefore we\nclassify bags of user-profile's images to provide a more robust prediction.\nExperiments using a dataset from the picture sharing social network Instagram\nshow that the use of multiple images is key to improve detection performance.\nMoreover, we verify that deep semantic features are more suited for gender\ndetection than low-level image representations. The methods proposed can infer\nthe gender with precision scores higher than 0.825, and the best performing\nmethod achieving 0.911 precision.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04531v1"
    },
    {
        "title": "Temporal Cross-Media Retrieval with Soft-Smoothing",
        "authors": [
            "David Semedo",
            "João Magalhães"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Multimedia information have strong temporal correlations that shape the way\nmodalities co-occur over time. In this paper we study the dynamic nature of\nmultimedia and social-media information, where the temporal dimension emerges\nas a strong source of evidence for learning the temporal correlations across\nvisual and textual modalities. So far, cross-media retrieval models, explored\nthe correlations between different modalities (e.g. text and image) to learn a\ncommon subspace, in which semantically similar instances lie in the same\nneighbourhood. Building on such knowledge, we propose a novel temporal\ncross-media neural architecture, that departs from standard cross-media\nmethods, by explicitly accounting for the temporal dimension through temporal\nsubspace learning. The model is softly-constrained with temporal and\ninter-modality constraints that guide the new subspace learning task by\nfavouring temporal correlations between semantically similar and temporally\nclose instances. Experiments on three distinct datasets show that accounting\nfor time turns out to be important for cross-media retrieval. Namely, the\nproposed method outperforms a set of baselines on the task of temporal\ncross-media retrieval, demonstrating its effectiveness for performing temporal\nsubspace learning.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04547v1"
    },
    {
        "title": "CNN-VWII: An Efficient Approach for Large-Scale Video Retrieval by Image\n  Queries",
        "authors": [
            "Chengyuan Zhang",
            "Yunwu Lin",
            "Lei Zhu",
            "Anfeng Liu",
            "Zuping Zhang",
            "Fang Huang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  This paper aims to solve the problem of large-scale video retrieval by a\nquery image. Firstly, we define the problem of top-$k$ image to video query.\nThen, we combine the merits of convolutional neural networks(CNN for short) and\nBag of Visual Word(BoVW for short) module to design a model for video frames\ninformation extraction and representation. In order to meet the requirements of\nlarge-scale video retrieval, we proposed a visual weighted inverted index(VWII\nfor short) and related algorithm to improve the efficiency and accuracy of\nretrieval process. Comprehensive experiments show that our proposed technique\nachieves substantial improvements (up to an order of magnitude speed up) over\nthe state-of-the-art techniques with similar accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.06030v1"
    },
    {
        "title": "Quality Assessment for Tone-Mapped HDR Images Using Multi-Scale and\n  Multi-Layer Information",
        "authors": [
            "Qin He",
            "Dingquan Li",
            "Tingting Jiang",
            "Ming Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Tone mapping operators and multi-exposure fusion methods allow us to enjoy\nthe informative contents of high dynamic range (HDR) images with standard\ndynamic range devices, but also introduce distortions into HDR contents.\nTherefore methods are needed to evaluate tone-mapped image quality. Due to the\ncomplexity of possible distortions in a tone-mapped image, information from\ndifferent scales and different levels should be considered when predicting\ntone-mapped image quality. So we propose a new no-reference method of\ntone-mapped image quality assessment based on multi-scale and multi-layer\nfeatures that are extracted from a pre-trained deep convolutional neural\nnetwork model. After being aggregated, the extracted features are mapped to\nquality predictions by regression. The proposed method is tested on the largest\npublic database for TMIQA and compared to existing no-reference methods. The\nexperimental results show that the proposed method achieves better performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.08339v2"
    },
    {
        "title": "Hierarchy-Dependent Cross-Platform Multi-View Feature Learning for Venue\n  Category Prediction",
        "authors": [
            "Shuqiang Jiang",
            "Weiqing Min",
            "Shuhuan Mei"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this work, we focus on visual venue category prediction, which can\nfacilitate various applications for location-based service and personalization.\nConsidering that the complementarity of different media platforms, it is\nreasonable to leverage venue-relevant media data from different platforms to\nboost the prediction performance. Intuitively, recognizing one venue category\ninvolves multiple semantic cues, especially objects and scenes, and thus they\nshould contribute together to venue category prediction. In addition, these\nvenues can be organized in a natural hierarchical structure, which provides\nprior knowledge to guide venue category estimation. Taking these aspects into\naccount, we propose a Hierarchy-dependent Cross-platform Multi-view Feature\nLearning (HCM-FL) framework for venue category prediction from videos by\nleveraging images from other platforms. HCM-FL includes two major components,\nnamely Cross-Platform Transfer Deep Learning (CPTDL) and Multi-View Feature\nLearning with the Hierarchical Venue Structure (MVFL-HVS). CPTDL is capable of\nreinforcing the learned deep network from videos using images from other\nplatforms. Specifically, CPTDL first trained a deep network using videos. These\nimages from other platforms are filtered by the learnt network and these\nselected images are then fed into this learnt network to enhance it. Two kinds\nof pre-trained networks on the ImageNet and Places dataset are employed.\nMVFL-HVS is then developed to enable multi-view feature fusion. It is capable\nof embedding the hierarchical structure ontology to support more discriminative\njoint feature learning. We conduct the experiment on videos from Vine and\nimages from Foursqure. These experimental results demonstrate the advantage of\nour proposed framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.09833v1"
    },
    {
        "title": "Image Super-Resolution Using TV Priori Guided Convolutional Network",
        "authors": [
            "Bo Fu",
            "Yi Li",
            "Xianghai Wang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  We proposed a TV priori information guided deep learning method for single\nimage super-resolution(SR). The new alogorithm up-sample method based on TV\npriori, new learning method and neural networks architecture are embraced in\nour TV guided priori Convolutional Neural Network which diretcly learns an end\nto end mapping between the low level to high level images.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.11801v1"
    },
    {
        "title": "Feature Bagging for Steganographer Identification",
        "authors": [
            "Hanzhou Wu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Traditional steganalysis algorithms focus on detecting the existence of\nsteganography in a single object. In practice, one may face a complex scenario\nwhere one or some of multiple users also called actors are guilty of using\nsteganography, which is defined as the steganographer identification problem\n(SIP). This requires steganalysis experts to design effective and robust\ndetection algorithms to identify the guilty actor(s). The mainstream works use\nclustering, ensemble and anomaly detection, where distances in high dimensional\nspace between features of actors are determined to find out the outlier(s)\ncorresponding to steganographer(s). However, in high dimensional space, feature\npoints could be sparse such that distances between feature points may become\nrelatively similar to each other, which cannot benefit the detection. Moreover,\nit is well-known in machine learning that combining techniques such as boosting\nand bagging can be effective in improving detection performance. This motivates\nthe authors in this paper to present a feature bagging approach to SIP. The\nproposed work merges results from multiple detection sub-models, each of which\nfeature space is randomly sampled from the raw full dimensional space. We\ncreate a new dataset called ImgNetEase including 5108 images downloaded from a\nsocial website to mimic the real-world scenario. We extract PEV-274 features\nfrom images, and take nsF5 as the steganographic algorithm for evaluation.\nExperiments have shown that our work improves the detection accuracy\nsignificantly on created dataset in most cases, which has shown the superiority\nand applicability.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.11973v1"
    },
    {
        "title": "Semantic Modeling of Textual Relationships in Cross-Modal Retrieval",
        "authors": [
            "Jing Yu",
            "Chenghao Yang",
            "Zengchang Qin",
            "Zhuoqian Yang",
            "Yue Hu",
            "Weifeng Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Feature modeling of different modalities is a basic problem in current\nresearch of cross-modal information retrieval. Existing models typically\nproject texts and images into one embedding space, in which semantically\nsimilar information will have a shorter distance. Semantic modeling of textural\nrelationships is notoriously difficult. In this paper, we propose an approach\nto model texts using a featured graph by integrating multi-view textual\nrelationships including semantic relations, statistical co-occurrence, and\nprior relations in the knowledge base. A dual-path neural network is adopted to\nlearn multi-modal representations of information and cross-modal similarity\nmeasure jointly. We use a Graph Convolutional Network (GCN) for generating\nrelation-aware text representations, and use a Convolutional Neural Network\n(CNN) with non-linearities for image representations. The cross-modal\nsimilarity measure is learned by distance metric learning. Experimental results\nshow that, by leveraging the rich relational semantics in texts, our model can\noutperform the state-of-the-art models by 3.4% and 6.3% on accuracy on two\nbenchmark datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.13151v3"
    },
    {
        "title": "Music Popularity: Metrics, Characteristics, and Audio-Based Prediction",
        "authors": [
            "Junghyuk Lee",
            "Jong-Seok Lee"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Understanding music popularity is important not only for the artists who\ncreate and perform music but also for the music-related industry. It has not\nbeen studied well how music popularity can be defined, what its characteristics\nare, and whether it can be predicted, which are addressed in this paper. We\nfirst define eight popularity metrics to cover multiple aspects of popularity.\nThen, the analysis of each popularity metric is conducted with long-term\nreal-world chart data to deeply understand the characteristics of music\npopularity in the real world. We also build classification models for\npredicting popularity metrics using acoustic data. In particular, we focus on\nevaluating features describing music complexity together with other\nconventional acoustic features including MPEG-7 and Mel-frequency cepstral\ncoefficient (MFCC) features. The results show that, although room still exists\nfor improvement, it is feasible to predict the popularity metrics of a song\nsignificantly better than random chance based on its audio signal, particularly\nusing both the complexity and MFCC features.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.00551v1"
    },
    {
        "title": "Learned Scalable Image Compression with Bidirectional Context\n  Disentanglement Network",
        "authors": [
            "Zhizheng Zhang",
            "Zhibo Chen",
            "Jianxin Lin",
            "Weiping Li"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this paper, we propose a learned scalable/progressive image compression\nscheme based on deep neural networks (DNN), named Bidirectional Context\nDisentanglement Network (BCD-Net). For learning hierarchical representations,\nwe first adopt bit-plane decomposition to decompose the information coarsely\nbefore the deep-learning-based transformation. However, the information carried\nby different bit-planes is not only unequal in entropy but also of different\nimportance for reconstruction. We thus take the hidden features corresponding\nto different bit-planes as the context and design a network topology with\nbidirectional flows to disentangle the contextual information for more\neffective compressed representations. Our proposed scheme enables us to obtain\nthe compressed codes with scalable rates via a one-pass encoding-decoding.\nExperiment results demonstrate that our proposed model outperforms the\nstate-of-the-art DNN-based scalable image compression methods in both PSNR and\nMS-SSIM metrics. In addition, our proposed model achieves higher performance in\nMS-SSIM metric than conventional scalable image codecs. Effectiveness of our\ntechnical components is also verified through sufficient ablation experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09443v2"
    },
    {
        "title": "Reversible Data Hiding in Encrypted Images based on MSB Prediction and\n  Huffman Coding",
        "authors": [
            "Youzhi Xiang",
            "Zhaoxia Yin",
            "Xinpeng Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  With the development of cloud storage and privacy protection, reversible data\nhiding in encrypted images (RDHEI) has attracted increasing attention as a\ntechnology that can embed additional data in the encryption domain. In general,\nan RDHEI method embeds secret data in an encrypted image while ensuring that\nthe embedded data can be extracted error-free and the original image can be\nrestored lossless. In this paper, A high-capacity RDHEI algorithm is proposed.\nAt first, the Most Significant Bits (MSB) of each pixel was predicted\nadaptively and marked by Huffman coding in the original image. Then, the image\nwas encrypted by a stream cipher method. At last, the vacated space can be used\nto embed additional data. Experimental results show that our method achieved\nhigher embedding capacity while comparing with the state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09499v1"
    },
    {
        "title": "HTML5 MSE Playback of MPEG 360 VR Tiled Streaming",
        "authors": [
            "Dimitri Podborski",
            "Jangwoo Son",
            "Gurdeep Singh Bhullar",
            "Robert Skupin",
            "Yago Sanchez",
            "Cornelius Hellge",
            "Thomas Schierl"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Virtual Reality (VR) and 360-degree video streaming have gained significant\nattention in recent years. First standards have been published in order to\navoid market fragmentation. For instance, 3GPP released its first VR\nspecification to enable 360-degree video streaming over 5G networks which\nrelies on several technologies specified in ISO/IEC 23090-2, also known as\nMPEG-OMAF. While some implementations of OMAF-compatible players have already\nbeen demonstrated at several trade shows, so far, no web browser-based\nimplementations have been presented. In this demo paper we describe a\nbrowser-based JavaScript player implementation of the most advanced media\nprofile of OMAF: HEVC-based viewport-dependent OMAF video profile, also known\nas tile-based streaming, with multi-resolution HEVC tiles. We also describe the\napplied workarounds for the implementation challenges we encountered with\nstate-of-the-art HTML5 browsers. The presented implementation was tested in the\nSafari browser with support of HEVC video through the HTML5 Media Source\nExtensions API. In addition, the WebGL API was used for rendering, using\nregion-wise packing metadata as defined in OMAF.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.02971v2"
    },
    {
        "title": "Notation for Subject Answer Analysis",
        "authors": [
            "Lucjan Janowski",
            "Jakub Nawała",
            "Werner Robitza",
            "Zhi Li",
            "Lukáš Kasula",
            "Krzysztof Rusek"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  It is believed that consistent notation helps the research community in many\nways. First and foremost, it provides a consistent interface of communication.\nSubjective experiments described according to uniform rules are easier to\nunderstand and analyze. Additionally, a comparison of various results is less\ncomplicated. In this publication we describe notation proposed by VQEG (Video\nQuality Expert Group) working group SAM (Statistical Analysis and Methods).\n",
        "pdf_link": "http://arxiv.org/pdf/1903.05940v1"
    },
    {
        "title": "Scene Segmentation-Based Luminance Adjustment for Multi-Exposure Image\n  Fusion",
        "authors": [
            "Yuma Kinoshita",
            "Hitoshi Kiya"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  We propose a novel method for adjusting luminance for multi-exposure image\nfusion. For the adjustment, two novel scene segmentation approaches based on\nluminance distribution are also proposed. Multi-exposure image fusion is a\nmethod for producing images that are expected to be more informative and\nperceptually appealing than any of the input ones, by directly fusing photos\ntaken with different exposures. However, existing fusion methods often produce\nunclear fused images when input images do not have a sufficient number of\ndifferent exposure levels. In this paper, we point out that adjusting the\nluminance of input images makes it possible to improve the quality of the final\nfused images. This insight is the basis of the proposed method. The proposed\nmethod enables us to produce high-quality images, even when undesirable inputs\nare given. Visual comparison results show that the proposed method can produce\nimages that clearly represent a whole scene. In addition, multi-exposure image\nfusion with the proposed method outperforms state-of-the-art fusion methods in\nterms of MEF-SSIM, discrete entropy, tone mapped image quality index, and\nstatistical naturalness.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.07428v1"
    },
    {
        "title": "Audio Watermarking over the Air With Modulated Self-Correlation",
        "authors": [
            "Yuan-Yen Tai",
            "Mohamed F. Mansour"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  We propose a novel audio watermarking system that is robust to the distortion\ndue to the indoor acoustic propagation channel between the loudspeaker and the\nreceiving microphone. The system utilizes a set of new algorithms that\neffectively mitigate the impact of room reverberation and interfering sound\nsources without using dereverberation procedures. The decoder has low-latency\nand it operates asynchronously, which alleviates the need for explicit\nsynchronization with the encoder. It is also robust to standard audio\nprocessing operations in legacy watermarking systems, e.g., compression and\nvolume change. The effectiveness of the system is established with a real-time\nsystem under general room conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.08238v1"
    },
    {
        "title": "Detecting the Presence of ENF Signal in Digital Videos: a Superpixel\n  based Approach",
        "authors": [
            "Saffet Vatansever",
            "Ahmet Emir Dirik",
            "Nasir Memon"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  ENF (Electrical Network Frequency) instantaneously fluctuates around its\nnominal value (50/60 Hz) due to a continuous disparity between generated power\nand consumed power. Consequently, luminous intensity of a mains-powered light\nsource varies depending on ENF fluctuations in the grid network. Variations in\nthe luminance over time can be captured from video recordings and ENF can be\nestimated through content analysis of these recordings. In ENF based video\nforensics, it is critical to check whether a given video file is appropriate\nfor this type of analysis. That is, if ENF signal is not present in a given\nvideo, it would be useless to apply ENF based forensic analysis. In this work,\nan ENF signal presence detection method is introduced for videos. The proposed\nmethod is based on multiple ENF signal estimations from steady superpixels,\ni.e. pixels that are most likely uniform in color, brightness, and texture, and\nintraclass similarity of the estimated signals. Subsequently, consistency among\nthese estimates is then used to determine the presence or absence of an ENF\nsignal in a given video. The proposed technique can operate on video clips as\nshort as 2 minutes and is independent of the camera sensor type, i.e. CCD or\nCMOS.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.09884v1"
    },
    {
        "title": "Analysis of Rolling Shutter Effect on ENF based Video Forensics",
        "authors": [
            "Saffet Vatansever",
            "Ahmet Emir Dirik",
            "Nasir Memon"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  ENF is a time-varying signal of the frequency of mains electricity in a power\ngrid. It continuously fluctuates around a nominal value (50/60 Hz) due to\nchanges in supply and demand of power over time. Depending on these ENF\nvariations, the luminous intensity of a mains-powered light source also\nfluctuates. These fluctuations in luminance can be captured by video\nrecordings. Accordingly, ENF can be estimated from such videos by analysis of\nsteady content in the video scene. When videos are captured by using a rolling\nshutter sampling mechanism, as is done mostly with CMOS cameras, there is an\nidle period between successive frames. Consequently, a number of illumination\nsamples of the scene are effectively lost due to the idle period. These missing\nsamples affect ENF estimation, in the sense of the frequency shift caused and\nthe power attenuation that results. This work develops an analytical model for\nvideos captured using a rolling shutter mechanism. The model illustrates how\nthe frequency of the main ENF harmonic varies depending on the idle period\nlength, and how the power of the captured ENF attenuates as idle period\nincreases. Based on this, a novel idle period estimation method for potential\nuse in camera forensics that is able to operate independently of video frame\nrate is proposed. Finally, a novel time-of-recording verification approach\nbased on use of multiple ENF components, idle period assumptions and\ninterpolation of missing ENF samples is also proposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.09889v1"
    },
    {
        "title": "SRDGAN: learning the noise prior for Super Resolution with Dual\n  Generative Adversarial Networks",
        "authors": [
            "Jingwei Guan",
            "Cheng Pan",
            "Songnan Li",
            "Dahai Yu"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Single Image Super Resolution (SISR) is the task of producing a high\nresolution (HR) image from a given low-resolution (LR) image. It is a well\nresearched problem with extensive commercial applications such as digital\ncamera, video compression, medical imaging and so on. Most super resolution\nworks focus on the features learning architecture, which can recover the\ntexture details as close as possible. However, these works suffer from the\nfollowing challenges: (1) The low-resolution (LR) training images are\nartificially synthesized using HR images with bicubic downsampling, which have\nmuch richer-information than real demosaic-upscaled mobile images. The mismatch\nbetween training and inference mobile data heavily blocks the improvement of\npractical super resolution algorithms. (2) These methods cannot effectively\nhandle the blind distortions during super resolution in practical applications.\nIn this work, an end-to-end novel framework, including high-to-low network and\nlow-to-high network, is proposed to solve the above problems with dual\nGenerative Adversarial Networks (GAN). First, the above mismatch problems are\nwell explored with the high-to-low network, where clear high-resolution image\nand the corresponding realistic low-resolution image pairs can be generated.\nMoreover, a large-scale General Mobile Super Resolution Dataset, GMSR, is\nproposed, which can be utilized for training or as a fair comparison benchmark\nfor super resolution methods. Second, an effective low-to-high network (super\nresolution network) is proposed in the framework. Benefiting from the GMSR\ndataset and novel training strategies, the super resolution model can\neffectively handle detail recovery and denoising at the same time.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.11821v1"
    },
    {
        "title": "Universal chosen-ciphertext attack for a family of image encryption\n  schemes",
        "authors": [
            "Junxin Chen",
            "Lei Chen",
            "Yicong Zhou"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  During the past decades, there is a great popularity employing nonlinear\ndynamics and permutation-substitution architecture for image encryption. There\nare three primary procedures in such encryption schemes, the key schedule\nmodule for producing encryption factors, permutation for image scrambling and\nsubstitution for pixel modification. Under the assumption of chosen-ciphertext\nattack, we evaluate the security of a class of image ciphers which adopts\npixel-level permutation and modular addition for substitution. It is\nmathematically revealed that the mapping from differentials of ciphertexts to\nthose of plaintexts are linear and has nothing to do with the key schedules,\npermutation techniques and encryption rounds. Moreover, a universal\nchosen-ciphertext attack is proposed and validated. Experimental results\ndemonstrate that the plaintexts can be directly reconstructed without any\nsecurity key or encryption elements. Related cryptographic discussions are also\ngiven.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.11987v1"
    },
    {
        "title": "Quality Assessment of Free-viewpoint Videos by Quantifying the Elastic\n  Changes of Multi-Scale Motion Trajectories",
        "authors": [
            "Suiyi Ling",
            "Jing Li",
            "Zhaohui Che",
            "Xiongkuo Min",
            "Guangtao Zhai",
            "Patrick Le Callet"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Virtual viewpoints synthesis is an essential process for many immersive\napplications including Free-viewpoint TV (FTV). A widely used technique for\nviewpoints synthesis is Depth-Image-Based-Rendering (DIBR) technique. However,\nsuch techniques may introduce challenging non-uniform spatial-temporal\nstructure-related distortions. Most of the existing state-of-the-art quality\nmetrics fail to handle these distortions, especially the temporal structure\ninconsistencies observed during the switch of different viewpoints. To tackle\nthis problem, an elastic metric and multi-scale trajectory based video quality\nmetric (EM-VQM) is proposed in this paper. Dense motion trajectory is first\nused as a proxy for selecting temporal sensitive regions, where local geometric\ndistortions might significantly diminish the perceived quality. Afterwards, the\namount of temporal structure inconsistencies and unsmooth viewpoints\ntransitions are quantified by calculating 1) the amount of motion trajectory\ndeformations with elastic metric and, 2) the spatial-temporal structural\ndissimilarity. According to the comprehensive experimental results on two FTV\nvideo datasets, the proposed metric outperforms the state-of-the-art metrics\ndesigned for free-viewpoint videos significantly and achieves a gain of 12.86%\nand 16.75% in terms of median Pearson linear correlation coefficient values on\nthe two datasets compared to the best one, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.12107v1"
    },
    {
        "title": "A Study on the Characteristics of Douyin Short Videos and Implications\n  for Edge Caching",
        "authors": [
            "Zhuang Chen",
            "Qian He",
            "Zhifei Mao",
            "Hwei-Ming Chung",
            "Sabita Maharjan"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Douyin, internationally known as TikTok, has become one of the most\nsuccessful short-video platforms. To maintain its popularity, Douyin has to\nprovide better Quality of Experience (QoE) to its growing user base.\nUnderstanding the characteristics of Douyin videos is thus critical to its\nservice improvement and system design. In this paper, we present an initial\nstudy on the fundamental characteristics of Douyin videos based on a dataset of\nover 260 thousand short videos collected across three months. The\ncharacteristics of Douyin videos are found to be significantly different from\ntraditional online videos, ranging from video bitrate, size, to popularity. In\nparticular, the distributions of the bitrate and size of videos follow Weibull\ndistribution. We further observe that the most popular Douyin videos follow\nZifp's law on video popularity, but the rest of the videos do not. We also\ninvestigate the correlation between popularity metrics used for Douyin videos.\nIt is found that the correlation between the number of views and the number of\nlikes are strong, while other correlations are relatively low. Finally, by\nusing a case study, we demonstrate that the above findings can provide\nimportant guidance on designing an efficient edge caching system.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.12399v1"
    },
    {
        "title": "Content Format and Quality of Experience in Virtual Reality",
        "authors": [
            "Henrique Galvan Debarba",
            "Mario Montagud",
            "Sylvain Chagué",
            "Javier Lajara",
            "Ignacio Lacosta",
            "Sergi Fernandez Langa",
            "Caecilia Charbonnier"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In this paper, we investigate three forms of virtual reality content\nproduction and consumption. Namely, 360 stereoscopic video, the combination of\na 3D environment with a video billboard for dynamic elements, and a full 3D\nrendered scene. On one hand, video based techniques facilitate the acquisition\nof content, but they can limit the experience of the user since the content is\ncaptured from a fixed point of view. On the other hand, 3D content allows for\npoint of view translation, but real-time photorealistic rendering is not\ntrivial and comes at high production and processing costs. We also compare the\ntwo extremes with an approach that combines dynamic video elements with a 3D\nvirtual environment. We discuss the advantages and disadvantages of these\nsystems, and present the result of a user study with 24 participants. In the\nstudy, we evaluated the quality of experience, including presence, simulation\nsickness and participants' assessment of content quality, of three versions of\na cinematic segment with two actors. We found that, in this context, mixing\nvideo and 3D content produced the best experience.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04511v1"
    },
    {
        "title": "ByeGlassesGAN: Identity Preserving Eyeglasses Removal for Face Images",
        "authors": [
            "Yu-Hui Lee",
            "Shang-Hong Lai"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In this paper, we propose a novel image-to-image GAN framework for eyeglasses\nremoval, called ByeGlassesGAN, which is used to automatically detect the\nposition of eyeglasses and then remove them from face images. Our ByeGlassesGAN\nconsists of an encoder, a face decoder, and a segmentation decoder. The encoder\nis responsible for extracting information from the source face image, and the\nface decoder utilizes this information to generate glasses-removed images. The\nsegmentation decoder is included to predict the segmentation mask of eyeglasses\nand completed face region. The feature vectors generated by the segmentation\ndecoder are shared with the face decoder, which facilitates better\nreconstruction results. Our experiments show that ByeGlassesGAN can provide\nvisually appealing results in the eyeglasses-removed face images even for\nsemi-transparent color eyeglasses or glasses with glare. Furthermore, we\ndemonstrate significant improvement in face recognition accuracy for face\nimages with glasses by applying our method as a pre-processing step in our face\nrecognition experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.11042v1"
    },
    {
        "title": "Low Complexity Trellis-Coded Quantization in Versatile Video Coding",
        "authors": [
            "Meng Wang",
            "Shiqi Wang",
            "Junru Li",
            "Li Zhang",
            "Yue Wang",
            "Siwei Ma",
            "Sam Kwong"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The forthcoming Versatile Video Coding (VVC) standard adopts the\ntrellis-coded quantization, which leverages the delicate trellis graph to map\nthe quantization candidates within one block into the optimal path. Despite the\nhigh compression efficiency, the complex trellis search with soft decision\nquantization may hinder the applications due to high complexity and low\nthroughput capacity. To reduce the complexity, in this paper, we propose a low\ncomplexity trellis-coded quantization scheme in a scientifically sound way with\ntheoretical modeling of the rate and distortion. As such, the trellis departure\npoint can be adaptively adjusted, and unnecessarily visited branches are\naccordingly pruned, leading to the shrink of total trellis stages and\nsimplification of transition branches. Extensive experimental results on the\nVVC test model show that the proposed scheme is effective in reducing the\nencoding complexity by 11% and 5% with all intra and random access\nconfigurations, respectively, at the cost of only 0.11% and 0.05% BD-Rate\nincrease. Meanwhile, on average 24% and 27% quantization time savings can be\nachieved under all intra and random access configurations. Due to the excellent\nperformance, the VVC test model has adopted one implementation of the proposed\nscheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.11420v1"
    },
    {
        "title": "High Efficiency Rate Control for Versatile Video Coding Based on\n  Composite Cauchy Distribution",
        "authors": [
            "Yunhao Mao",
            "Meng Wang",
            "Shiqi Wang",
            "Sam Kwong"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In this work, we propose a novel rate control algorithm for Versatile Video\nCoding (VVC) standard based on its distinct rate-distortion characteristics. By\nmodelling the transform coefficients with the composite Cauchy distribution,\nhigher accuracy compared with traditional distributions has been achieved.\nBased on the transform coefficient modelling, the theoretically derived R-Q and\nD-Q models which have been shown to deliver higher accuracy in characterizing\nRD characteristics for sequences with different content are incorporated into\nthe rate control process. Furthermore, to establish an adaptive bit allocation\nscheme, the dependency between different levels of frames is modelled by a\ndependency factor to describe relationship between the reference and\nto-be-coded frames. Given the derived R-Q and D-Q relationships, as well as the\ndependency factor, an adaptive bit allocation scheme is developed for optimal\nbits allocation. We implement the proposed algorithm on VVC Test Model (VTM)\n3.0. Experiments show that due to proper bit allocation, for low delay\nconfiguration the proposed algorithm can achieve 1.03% BD-Rate saving compared\nwith the default rate control algorithm and 2.96% BD-Rate saving compared with\nfixed QP scheme. Moreover, 1.29% BD-Rate saving and higher control accuracy\nhave also been observed under the random access configuration.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.11455v2"
    },
    {
        "title": "Semantics Preserving Hierarchy based Retrieval of Indian heritage\n  monuments",
        "authors": [
            "Ronak Gupta",
            "Prerana Mukherjee",
            "Brejesh Lall",
            "Varshul Gupta"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Monument classification can be performed on the basis of their appearance and\nshape from coarse to fine categories. Although there is much semantic\ninformation present in the monuments which is reflected in the eras they were\nbuilt, its type or purpose, the dynasty which established it, etc.\nParticularly, Indian subcontinent exhibits a huge deal of variation in terms of\narchitectural styles owing to its rich cultural heritage. In this paper, we\npropose a framework that utilizes hierarchy to preserve semantic information\nwhile performing image classification or image retrieval. We encode the learnt\ndeep embeddings to construct a dictionary of images and then utilize a\nre-ranking framework on the the retrieved results using DeLF features. The\nsemantic information preserved in these embeddings helps to classify unknown\nmonuments at higher level of granularity in hierarchy. We have curated a large,\nnovel Indian heritage monuments dataset comprising of images of historical,\ncultural and religious importance with subtypes of eras, dynasties and\narchitectural styles. We demonstrate the performance of the proposed framework\nin image classification and retrieval tasks and compare it with other competing\nmethods on this dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.12832v1"
    },
    {
        "title": "Developing a Video Steganography Toolkit",
        "authors": [
            "James Ridgway",
            "Mike Stannett"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Although techniques for separate image and audio steganography are widely\nknown, relatively little has been described concerning the hiding of\ninformation within video streams (\"video steganography\"). In this paper we\nreview the current state of the art in this field, and describe the key issues\nwe have encountered in developing a practical video steganography system. A\nsupporting video is also available online at\nhttp://www.youtube.com/watch?v=YhnlHmZolRM\n",
        "pdf_link": "http://arxiv.org/pdf/1409.4883v1"
    },
    {
        "title": "Web Video in Numbers - An Analysis of Web-Video Metadata",
        "authors": [
            "Luca Rossetto",
            "Heiko Schuldt"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Web video is often used as a source of data in various fields of study. While\nspecialized subsets of web video, mainly earmarked for dedicated purposes, are\noften analyzed in detail, there is little information available about the\nproperties of web video as a whole. In this paper we present insights gained\nfrom the analysis of the metadata associated with more than 120 million videos\nharvested from two popular web video platforms, vimeo and YouTube, in 2016 and\ncompare their properties with the ones found in commonly used video\ncollections. This comparison has revealed that existing collections do not (or\nno longer) properly reflect the properties of web video \"in the wild\".\n",
        "pdf_link": "http://arxiv.org/pdf/1707.01340v1"
    },
    {
        "title": "On the steganographic image based approach to PDF files protection",
        "authors": [
            "V. N. Gorbachev",
            "L. A. Denisov",
            "E. M. Kaynarova",
            "I. K. Metelev"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Digital images can be copied without authorization and have to be protected.\nTwo schemes for watermarking images in PDF document were considered. Both\nschemes include a converter to extract images from PDF pages and return the\nprotected images back. Frequency and spatial domain embedding were used for\nhiding a message presented by a binary pattern. We considered visible and\ninvisible watermarking and found that spatial domain LSB technique can be more\npreferable than frequency embedding using DWT.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.01513v1"
    },
    {
        "title": "High Resilience Diverse Domain Multilevel Audio Watermarking with\n  Adaptive Threshold",
        "authors": [
            "Jerrin Thomas Panachakel",
            "Anurenjan P. R"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  A novel diverse domain (DCT-SVD & DWT-SVD) watermarking scheme is proposed in\nthis paper. Here, the watermark is embedded simultaneously onto the two\ndomains. It is shown that an audio signal watermarked using this scheme has\nbetter subjective and objective quality when compared with other watermarking\nschemes. Also proposed are two novel watermark detection algorithms viz., AOT\n(Adaptively Optimised Threshold) and AOTx (AOT eXtended). The fundamental idea\nbehind both is finding an optimum threshold for detecting a known character\nembedded along with the actual watermarks in a known location, with the\nconstraint that the Bit Error Rate (BER) is minimum. This optimum threshold is\nused for detecting the other characters in the watermarks. This approach is\nshown to make the watermarking scheme less susceptible to various signal\nprocessing attacks, thus making the watermarks more robust.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.01742v1"
    },
    {
        "title": "An Augmented Autoregressive Approach to HTTP Video Stream Quality\n  Prediction",
        "authors": [
            "Christos G. Bampis",
            "Alan C. Bovik"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  HTTP-based video streaming technologies allow for flexible rate selection\nstrategies that account for time-varying network conditions. Such rate changes\nmay adversely affect the user's Quality of Experience; hence online prediction\nof the time varying subjective quality can lead to perceptually optimised\nbitrate allocation policies. Recent studies have proposed to use dynamic\nnetwork approaches for continuous-time prediction; yet they do not consider\nmultiple video quality models as inputs nor consider forecasting ensembles.\nHere we address the problem of predicting continuous-time subjective quality\nusing multiple inputs fed to a non-linear autoregressive network. By\nconsidering multiple network configurations and by applying simple averaging\nforecasting techniques, we are able to considerably improve prediction\nperformance and decrease forecasting errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02709v1"
    },
    {
        "title": "Multichannel Attention Network for Analyzing Visual Behavior in Public\n  Speaking",
        "authors": [
            "Rahul Sharma",
            "Tanaya Guha",
            "Gaurav Sharma"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Public speaking is an important aspect of human communication and\ninteraction. The majority of computational work on public speaking concentrates\non analyzing the spoken content, and the verbal behavior of the speakers. While\nthe success of public speaking largely depends on the content of the talk, and\nthe verbal behavior, non-verbal (visual) cues, such as gestures and physical\nappearance also play a significant role. This paper investigates the importance\nof visual cues by estimating their contribution towards predicting the\npopularity of a public lecture. For this purpose, we constructed a large\ndatabase of more than $1800$ TED talk videos. As a measure of popularity of the\nTED talks, we leverage the corresponding (online) viewers' ratings from\nYouTube. Visual cues related to facial and physical appearance, facial\nexpressions, and pose variations are extracted from the video frames using\nconvolutional neural network (CNN) models. Thereafter, an attention-based long\nshort-term memory (LSTM) network is proposed to predict the video popularity\nfrom the sequence of visual features. The proposed network achieves\nstate-of-the-art prediction accuracy indicating that visual cues alone contain\nhighly predictive information about the popularity of a talk. Furthermore, our\nnetwork learns a human-like attention mechanism, which is particularly useful\nfor interpretability, i.e. how attention varies with time, and across different\nvisual cues by indicating their relative importance.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.06830v1"
    },
    {
        "title": "Anti-Forensics of Camera Identification and the Triangle Test by\n  Improved Fingerprint-Copy Attack",
        "authors": [
            "Haodong Li",
            "Weiqi Luo",
            "Quanquan Rao",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The fingerprint-copy attack aims to confuse camera identification based on\nsensor pattern noise. However, the triangle test shows that the forged images\nundergone fingerprint-copy attack would share a non-PRNU (Photo-response\nnonuniformity) component with every stolen image, and thus can detect\nfingerprint-copy attack. In this paper, we propose an improved fingerprint-copy\nattack scheme. Our main idea is to superimpose the estimated fingerprint into\nthe target image dispersedly, via employing a block-wise method and using the\nstolen images randomly and partly. We also develop a practical method to\ndetermine the strength of the superimposed fingerprint based on objective image\nquality. In such a way, the impact of non-PRNU component on the triangle test\nis reduced, and our improved fingerprint-copy attack is difficultly detected.\nThe experiments evaluated on 2,900 images from 4 cameras show that our scheme\ncan effectively fool camera identification, and significantly degrade the\nperformance of the triangle test simultaneously.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.07795v1"
    },
    {
        "title": "Intra Prediction Using In-Loop Residual Coding for the post-HEVC\n  Standard",
        "authors": [
            "Mohsen Abdoli",
            "Félix Henry",
            "Patric Brault",
            "Pierre Duhamel",
            "Frédéric Dufaux"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  A few years after standardization of the High Efficiency Video Coding (HEVC),\nnow the Joint Video Exploration Team (JVET) group is exploring post-HEVC video\ncompression technologies. In the intra prediction domain, this effort has\nresulted in an algorithm with 67 internal modes, new filters and tools which\nsignificantly improve HEVC. However, the improved algorithm still suffers from\nthe long distance prediction inaccuracy problem. In this paper, we propose an\nIn-Loop Residual coding Intra Prediction (ILR-IP) algorithm which utilizes\ninner-block reconstructed pixels as references to reduce the distance from\npredicted pixels. This is done by using the ILR signal for partially\nreconstructing each pixel, right after its prediction and before its\nblock-level out-loop residual calculation. The ILR signal is decided in the\nrate-distortion sense, by a brute-force search on a QP-dependent finite\ncodebook that is known to the decoder. Experiments show that the proposed\nILR-IP algorithm improves the existing method in the Joint Exploration Model\n(JEM) up to 0.45% in terms of bit rate saving, without complexity overhead at\nthe decoder side.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09791v1"
    },
    {
        "title": "Spatially Variant Laplacian Pyramids for Multi-Frame Exposure Fusion",
        "authors": [
            "Anmol Biswas",
            "Green Rosh K S",
            "Sachin Deepak Lomte"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Laplacian Pyramid Blending is a commonly used method for several seamless\nimage blending tasks. While the method works well for images with comparable\nintensity levels, it is often unable to produce artifact free images for\napplications which handle images with large intensity variation such as\nexposure fusion. This paper proposes a spatially varying Laplacian Pyramid\nBlending to blend images with large intensity differences. The proposed method\ndynamically alters the blending levels during the final stage of Pyramid\nReconstruction based on the amount of local intensity variation. The proposed\nalgorithm out performs state-of-the-art methods for image blending both\nqualitatively as well as quantitatively on publicly available High Dynamic\nRange (HDR) imaging dataset. Qualitative improvements are demonstrated in terms\nof details, halos and dark halos. For quantitative comparison, the no-reference\nperceptual metric MEF-SSIM was used.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.01425v1"
    },
    {
        "title": "Data hiding in speech signal using steganography and encryption",
        "authors": [
            "Hanisha Chowdary N",
            "Karan K",
            "Bharath K P",
            "Rajesh Kumar M"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Data privacy and data security are always on highest priority in the world.\nWe need a reliable method to encrypt the data so that it reaches the\ndestination safely. Encryption is a simple yet effective way to protect our\ndata while transmitting it to a destination. The proposed method has state of\nart technology of steganography and encryption. This paper puts forward a\ndifferent approach for data hiding in speech signals. A ten-digit number within\nspeech signal using audio steganography and encrypting it with a unique key for\nbetter security. At the receiver end the same unique key is used to decrypt the\nreceived signal and then hidden numbers are extracted. The proposed approach\nperformance can be evaluated by PSNR, MSE, SSIM and bit-error rate. The\nsimulation results give better performance compared to existing approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.02370v1"
    },
    {
        "title": "A dwt, dct and svd based watermarking technique to protect the image\n  piracy",
        "authors": [
            "Md. Maklachur Rahman"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  With the rapid development of information technology and multimedia, the use\nof digital data is increasing day by day. So it becomes very essential to\nprotect multimedia information from piracy and also it is challenging. A great\ndeal of Copyright owners is worried about protecting any kind of illegal\nrepetition of their information. Hence, facing all these kinds of problems\ndevelopment of the techniques is very important. Digital watermarking\nconsidered as a solution to prevent the multimedia data. In this paper, an idea\nof watermarking is proposed and implemented. In proposed watermarking method,\nthe original image is rearranged using zigzag sequence and DWT is applied on\nrearranged image. Then DCT and SVD are applied on all high bands LH, HL and HH.\nWatermark is then embedded by modifying the singular values of these bands.\nExtraction of watermark is performed by the inversion of watermark embedding\nprocess. For choosing of these three bands it gives facility of mid-band and\npure high band that ensures good imperceptibility and more robustness against\ndifferent kinds of attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.3294v1"
    },
    {
        "title": "Digital Watermarking for Image AuthenticationBased on Combined DCT, DWT\n  and SVD Transformation",
        "authors": [
            "Mohammad Ibrahim Khan",
            "Md. Maklachur Rahman",
            "Md. Iqbal Hasan Sarker"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  This paper presents a hybrid digital image watermarking based on Discrete\nWavelet Transform (DWT), Discrete Cosine Transform (DCT) and Singular Value\nDecomposition (SVD) in a zigzag order. From DWT we choose the high band to\nembed the watermark that facilities to add more information, gives more\ninvisibility and robustness against some attacks. Such as geometric attack.\nZigzag method is applied to map DCT coefficients into four quadrants that\nrepresent low, mid and high bands. Finally, SVD is applied to each quadrant.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.6328v1"
    },
    {
        "title": "A simple technique for steganography",
        "authors": [
            "Adity Sharma",
            "Anoo Agarwal",
            "Vinay Kumar"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  A new technique for data hiding in digital image is proposed in this paper.\nSteganography is a well known technique for hiding data in an image, but\ngenerally the format of image plays a pivotal role in it, and the scheme is\nformat dependent. In this paper we will discuss a new technique where\nirrespective of the format of image, we can easily hide a large amount of data\nwithout deteriorating the quality of the image. The data to be hidden is\nenciphered with the help of a secret key. This enciphered data is then embedded\nat the end of the image. The enciphered data bits are extracted and then\ndeciphered with the help of same key used for encryption. Simulation results\nshow that Image Quality Measures of this proposed scheme are better than the\nconventional LSB replacing technique. The proposed method is simple and is easy\nto implement.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.8385v1"
    },
    {
        "title": "Color to Gray and Back transformation for distributing color digital\n  images",
        "authors": [
            "V. N. Gorbachev",
            "E. M. Kaynarova",
            "I. K. Metelev",
            "E. S. Yakovleva"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  The Color to Gray and Back transformation watermarking with a secrete key is\nconsidered. Color is embedded into the bit planes of the luminosity component\nof the YUV color space with the help of a block algorithm that allows using not\nonly the least significant bits. An application of the problem of distributing\ncolor digital images from a data base among legitimate users is discussed. The\nproposed protocol can protect original images from unauthorized copying.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.1313v1"
    },
    {
        "title": "Analysis of Computer Hardware Affecting Video Transmission via IEEE\n  1394a connection",
        "authors": [
            "Dr. Timur Mirzoev"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  When 60 de-interlaced fields per second digital uncompressed video is\nstreamed to a computer, some video fields are lost and not able to be stored on\na computer s hard drive successfully. Additionally, this problem amplifies once\nmultiple video sources are deployed. If it is possible to stream digital\nuncompressed video without dropped video fields, then a sophisticated computer\nanalysis of the transmitted via IEEE 1394a connection video is possible. Such\nprocess is used in biomechanics when it is important to analyze athletes\nperformance via streaming digital uncompressed video to a computer and then\nanalyzing it. If a loss of video fields occurs, then a quality analysis of\nvideo is not possible.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.2344v1"
    },
    {
        "title": "Enhancing User Experience for Multi-Screen Social TV Streaming over\n  Wireless Networks",
        "authors": [
            "Huazi Zhang",
            "Yichao Jin",
            "Weiwen Zhang",
            "Yonggang Wen"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Recently, multi-screen cloud social TV is invented to transform TV into\nsocial experience. People watching the same content on social TV may come from\ndifferent locations, while freely interact with each other through text, image,\naudio and video. This crucial virtual living-room experience adds social\naspects into existing performance metrics. In this paper, we parse social TV\nuser experience into three elements (i.e., inter-user delay, video quality of\nexperience (QoE), and resource efficiency), and provide a joint analytical\nframework to enhance user experience. Specifically, we propose a cloud-based\noptimal playback rate allocation scheme to maximize the overall QoE while upper\nbounding inter-user delay. Experiment results show that our algorithm achieves\nnear-optimal tradeoff between inter-user delay and video quality, and\ndemonstrates resilient performance even under very fast wireless channel\nfading.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.3018v1"
    },
    {
        "title": "Improving Low Bit-Rate Video Coding using Spatio-Temporal Down-Scaling",
        "authors": [
            "Yehuda Dar",
            "Alfred M. Bruckstein"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Good quality video coding for low bit-rate applications is important for\ntransmission over narrow-bandwidth channels and for storage with limited memory\ncapacity. In this work, we develop a previous analysis for image compression at\nlow bit-rates to adapt it to video signals. Improving compression using\ndown-scaling in the spatial and temporal dimensions is examined. We show, both\ntheoretically and experimentally, that at low bit-rates, we benefit from\napplying spatio-temporal scaling. The proposed method includes down-scaling\nbefore the compression and a corresponding up-scaling afterwards, while the\ncodec itself is left unmodified. We propose analytic models for low bit-rate\ncompression and spatio-temporal scaling operations. Specifically, we use\ntheoretic models of motion-compensated prediction of available and absent\nframes as in coding and frame-rate up-conversion (FRUC) applications,\nrespectively. The proposed models are designed for multi-resolution analysis.\nIn addition, we formulate a bit-allocation procedure and propose a method for\nestimating good down-scaling factors of a given video based on its second-order\nstatistics and the given bit-budget. We validate our model with experimental\nresults of H.264 compression.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.4026v2"
    },
    {
        "title": "A Novel Approach for Video Temporal Annotation",
        "authors": [
            "Hadi Restgou Haghi",
            "Mohammadreza Kangavari",
            "Behrang QasemiZadeh"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Recent advances in computing, communication, and data storage have led to an\nincreasing number of large digital libraries publicly available on the\nInternet. Main problem of content-based video retrieval is inferring semantics\nfrom raw video data. Video data play an important role in these libraries.\nInstead of words, a video retrieval system deals with collections of video\nrecords. Therefore, the system is confronted with the problem of video\nunderstanding. Because machine understanding of the video data is still an\nunsolved research problem, text annotations are usually used to describe the\ncontent of video data according to the annotator's understanding and the\npurpose of that video data. Most of proposed systems for video annotation are\ndomain dependent. In addition, in many of these systems, an important feature\nof video data, temporality, is disregarded. In this paper, we proposed a\nframework for video temporal annotation. The proposed system uses domain\nknowledge and a time ontology to perform temporal annotation of input video.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.4543v1"
    },
    {
        "title": "A Smart Intelligent Way of Video Authentication Using Classification and\n  Decomposition of Watermarking Methods",
        "authors": [
            "T. Srinivasa Rao",
            "Rajasekhar R. Kurra"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Video Watermarking serves as a new technology mainly used to provide security\nto the illegal distribution of digital video over the web. The purpose of any\nvideo watermarking scheme is to embed extra information into video in such a\nway that must be perceptually undetectable while still holding enough\ninformation in order to extract the watermark beginning with the resultant\nvideo. Information which is embedded within the original image is a Digital\nWatermark, which could be visible or invisible. To improved more security,\nembedding and extraction Watermark process should be complex against attackers.\nRecent research indicates SVD (Singular Value Decomposition) algorithms are\nemployed owing to their simple scheme with mathematical function. In this\nproposed work an advanced SVD transformation algorithm is used for embedding\nand extraction process. Experimental results show proposed watermarking process\nis more secured than existing SVD approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.7237v1"
    },
    {
        "title": "Optimum Decoder for an Additive Video Watermarking with Laplacian Noise\n  in H.264",
        "authors": [
            "Nematollah Zarmehi",
            "Morteza Banagar",
            "Mohammad Ali Akhaee"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  In this paper, we investigate an additive video watermarking method in H.264\nstandard in presence of the Laplacian noise. In some applications, due to the\nloss of some pixels or a region of a frame, we resort to Laplacian noise rather\nthan Gaussian one. The embedding is performed in the transform domain; while an\noptimum and a sub-optimum decoder are derived for the proposed Laplacian model.\nSimulation results show that the proposed watermarking scheme has suitable\nperformance with enough transparency required for watermarking applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.01501v1"
    },
    {
        "title": "A novel magic LSB substitution method (M-LSB-SM) using multi-level\n  encryption and achromatic component of an image",
        "authors": [
            "Khan Muhammad",
            "Muhammad Sajjad",
            "Irfan Mehmood",
            "Seungmin Rho",
            "Sung Wook Baik"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Image Steganography is a thriving research area of information security where\nsecret data is embedded in images to hide its existence while getting the\nminimum possible statistical detectability. This paper proposes a novel magic\nleast significant bit substitution method (M-LSB-SM) for RGB images. The\nproposed method is based on the achromatic component (I-plane) of the\nhue-saturation-intensity (HSI) color model and multi-level encryption (MLE) in\nthe spatial domain. The input image is transposed and converted into an HSI\ncolor space. The I-plane is divided into four sub-images of equal size,\nrotating each sub-image with a different angle using a secret key. The secret\ninformation is divided into four blocks, which are then encrypted using an MLE\nalgorithm (MLEA). Each sub-block of the message is embedded into one of the\nrotated sub-images based on a specific pattern using magic LSB substitution.\nExperimental results validate that the proposed method not only enhances the\nvisual quality of stego images but also provides good imperceptibility and\nmultiple security levels as compared to several existing prominent methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.02100v1"
    },
    {
        "title": "Optimal Layered Representation for Adaptive Interactive Multiview Video\n  Streaming",
        "authors": [
            "Ana De Abreu",
            "Laura Toni",
            "Nikolaos Thomos",
            "Thomas Maugey",
            "Fernando Pereira",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  We consider an interactive multiview video streaming (IMVS) system where\nclients select their preferred viewpoint in a given navigation window. To\nprovide high quality IMVS, many high quality views should be transmitted to the\nclients. However, this is not always possible due to the limited and\nheterogeneous capabilities of the clients. In this paper, we propose a novel\nadaptive IMVS solution based on a layered multiview representation where camera\nviews are organized into layered subsets to match the different clients\nconstraints. We formulate an optimization problem for the joint selection of\nthe views subsets and their encoding rates. Then, we propose an optimal and a\nreduced computational complexity greedy algorithms, both based on\ndynamic-programming. Simulation results show the good performance of our novel\nalgorithms compared to a baseline algorithm, proving that an effective IMVS\nadaptive solution should consider the scene content and the client capabilities\nand their preferences in navigation.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.07823v1"
    },
    {
        "title": "Low-latency compression of mocap data using learned spatial\n  decorrelation transform",
        "authors": [
            "Junhui Hou",
            "Lap-Pui Chau",
            "Nadia Magnenat-Thalmann",
            "Ying He"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Due to the growing needs of human motion capture (mocap) in movie, video\ngames, sports, etc., it is highly desired to compress mocap data for efficient\nstorage and transmission. This paper presents two efficient frameworks for\ncompressing human mocap data with low latency. The first framework processes\nthe data in a frame-by-frame manner so that it is ideal for mocap data\nstreaming and time critical applications. The second one is clip-based and\nprovides a flexible tradeoff between latency and compression performance. Since\nmocap data exhibits some unique spatial characteristics, we propose a very\neffective transform, namely learned orthogonal transform (LOT), for reducing\nthe spatial redundancy. The LOT problem is formulated as minimizing square\nerror regularized by orthogonality and sparsity and solved via alternating\niteration. We also adopt a predictive coding and temporal DCT for temporal\ndecorrelation in the frame- and clip-based frameworks, respectively.\nExperimental results show that the proposed frameworks can produce higher\ncompression performance at lower computational cost and latency than the\nstate-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.08898v3"
    },
    {
        "title": "An Overview of Emerging Technologies for High Efficiency 3D Video Coding",
        "authors": [
            "Qifei Wang"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  3D video coding is one of the most popular research area in multimedia. This\npaper reviews the recent progress of the coding technologies for multiview\nvideo (MVV) and free view-point video (FVV) which is represented by MVV and\ndepth maps. We first discuss the traditional multiview video coding (MVC)\nframework with different prediction structures. The rate-distortion performance\nand the view switching delay of the three main coding prediction structures are\nanalyzed. We further introduce the joint coding technologies for MVV and depth\nmaps and evaluate the rate-distortion performance of them. The scalable 3D\nvideo coding technologies are reviewed by the quality and view scalability,\nrespectively. Finally, we summarize the bit allocation work of 3D video coding.\nThis paper also points out some future research problems in high efficiency 3D\nvideo coding such as the view switching latency optimization in coding\nstructure and bit allocation.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.08854v1"
    },
    {
        "title": "Duplicate matching and estimating features for detection of copy-move\n  images forgery",
        "authors": [
            "Ghassem Alikhajeh",
            "Abdolreza Mirzaei",
            "Mehran Safayani",
            "Meysam Ghaffari"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Copy-move forgery is the most popular and simplest image manipulation method.\nIn this type of forgery, an area from the image copied, then after post\nprocessing such as rotation and scaling, placed on the destination. The goal of\nCopy-move forgery is to hide or duplicate one or more objects in the image.\nKey-point based Copy-move forgery detection methods have five main steps:\npreprocessing, feature extraction, matching, transform estimation and post\nprocessing that matching and transform estimation have important effect on the\ndetection. More over the error could happens in some steps due to the noise.\nThe existing methods process these steps separately and in case of having an\nerror in a step, this error could be propagated to the following steps and\naffects the detection. To solve the above mentioned problem, in this paper the\nsteps of the detection system interact with each other and if an error happens\nin a step, following steps are trying to detect and solve it. We formulate this\ninteraction by defining and optimizing a cost function. This function includes\nmatching and transform estimation steps. Then in an iterative procedure the\nsteps are executed and in case of detecting error, the error will be corrected.\nThe efficiency of the proposed method analyzed in diverse cases such as pixel\nimage precision level on the simple forgery images, robustness to the rotation\nand scaling, detecting professional forgery images and the precision of the\ntransformation matrix. The results indicate the better efficiency of the\nproposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.00474v1"
    },
    {
        "title": "VideoSet: A Large-Scale Compressed Video Quality Dataset Based on JND\n  Measurement",
        "authors": [
            "Haiqiang Wang",
            "Ioannis Katsavounidis",
            "Jiantong Zhou",
            "Jeonghoon Park",
            "Shawmin Lei",
            "Xin Zhou",
            "Man-On Pun",
            "Xin Jin",
            "Ronggang Wang",
            "Xu Wang",
            "Yun Zhang",
            "Jiwu Huang",
            "Sam Kwong",
            "C. -C. Jay Kuo"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  A new methodology to measure coded image/video quality using the\njust-noticeable-difference (JND) idea was proposed. Several small JND-based\nimage/video quality datasets were released by the Media Communications Lab at\nthe University of Southern California. In this work, we present an effort to\nbuild a large-scale JND-based coded video quality dataset. The dataset consists\nof 220 5-second sequences in four resolutions (i.e., $1920 \\times 1080$, $1280\n\\times 720$, $960 \\times 540$ and $640 \\times 360$). For each of the 880 video\nclips, we encode it using the H.264 codec with $QP=1, \\cdots, 51$ and measure\nthe first three JND points with 30+ subjects. The dataset is called the\n\"VideoSet\", which is an acronym for \"Video Subject Evaluation Test (SET)\". This\nwork describes the subjective test procedure, detection and removal of outlying\nmeasured data, and the properties of collected JND data. Finally, the\nsignificance and implications of the VideoSet to future video coding research\nand standardization efforts are pointed out. All source/coded video clips as\nwell as measured JND data included in the VideoSet are available to the public\nin the IEEE DataPort.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.01500v2"
    },
    {
        "title": "Comprehensive Review of Audio Steganalysis Methods",
        "authors": [
            "Hamzeh Ghasemzadeh",
            "Mohammad H. Kayvanrad"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Recently, merging signal processing techniques with information security\nservices has found a lot of attention. Steganography and steganalysis are among\nthose trends. Like their counterparts in cryptology, steganography and\nsteganalysis are in a constant battle. Steganography methods try to hide the\npresence of covert messages in innocuous-looking data, whereas steganalysis\nmethods try to reveal existence of such messages and to break steganography\nmethods. The stream nature of audio signals, their popularity, and their wide\nspread usage make them very suitable media for steganography. This has led to a\nvery rich literature on both steganography and steganalysis of audio signals.\nThis paper intends to conduct a comprehensive review of audio steganalysis\nmethods aggregated over near fifteen years. Furthermore, we implement some of\nthe most recent audio steganalysis methods and conduct a comparative analysis\non their performances. Finally, the paper provides some possible directions for\nfuture researches on audio steganalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.05611v3"
    },
    {
        "title": "Universal Audio Steganalysis Based on Calibration and Reversed Frequency\n  Resolution of Human Auditory System",
        "authors": [
            "Hamzeh Ghasemzadeh",
            "Meisam Khalil Arjmandi"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Calibration and higher order statistics (HOS) are standard components of many\nimage steganalysis systems. These techniques have not yet found adequate\nattention in audio steganalysis context. Specifically, most of current works\nare either non-calibrated or only based on noise removal approach. This paper\naims to fill these gaps by proposing a new set of calibrated features based on\nre-embedding technique. Additionally, we show that least significant bit (LSB)\nis the most sensitive bit-plane to data hiding algorithms and therefore it can\nbe employed as a universal embedding method. Furthermore, the proposed features\nare based on a model that has the maximum deviation from human auditory system\n(HAS), and therefore are more suitable for the purpose of steganalysis.\nPerformance of the proposed method is evaluated on a wide range of data hiding\nalgorithms in both targeted and universal paradigms. Simulation results show\nthat the proposed method can detect the finest traces of data hiding algorithms\nand in very low embedding rates. The system detects steghide at capacity of\n0.06 bit per symbol (BPS) with sensitivity of 98.6% (music) and 78.5% (speech).\nThese figures are respectively 7.1% and 27.5% higher than state-of-the-art\nresults based on RMFCC.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.05614v2"
    },
    {
        "title": "Adaptive 360 VR Video Streaming based on MPEG-DASH SRD",
        "authors": [
            "Mohammad Hosseini",
            "Viswanathan Swaminathan"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  We demonstrate an adaptive bandwidth-efficient 360 VR video streaming system\nbased on MPEG-DASH SRD. We extend MPEG-DASH SRD to the 3D space of 360 VR\nvideos, and showcase a dynamic view-aware adaptation technique to tackle the\nhigh bandwidth demands of streaming 360 VR videos to wireless VR headsets. We\nspatially partition the underlying 3D mesh into multiple 3D sub-meshes, and\nconstruct an efficient 3D geometry mesh called hexaface sphere to optimally\nrepresent tiled 360 VR videos in the 3D space. We then spatially divide the 360\nvideos into multiple tiles while encoding and packaging, use MPEG-DASH SRD to\ndescribe the spatial relationship of tiles in the 3D space, and prioritize the\ntiles in the Field of View (FoV) for view-aware adaptation. Our initial\nevaluation results show that we can save up to 72% of the required bandwidth on\n360 VR video streaming with minor negative quality impacts compared to the\nbaseline scenario when no adaptations is applied.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06509v1"
    },
    {
        "title": "Analysis of challenges faced by WebRTC videoconferencing and a remedial\n  architecture",
        "authors": [
            "Maruf Pasha",
            "Furrakh Shahzad",
            "Arslan Ahmad"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Lately, World Wide Web came up with an evolution in the niche of\nvideoconference applications. Latest technologies give browsers a capacity to\ninitiate real-time communications. WebRTC is one of the free and open source\nprojects that aim at providing the users freedom to enjoy real-time\ncommunications, and it does so by following and redefining the standards.\nHowever, WebRTC is still a new project and it lacks some high-end\nvideoconferencing features such as media mixing, recording of a session and\ndifferent network conditions adaptation. This paper is an attempt at analyzing\nthe shortcomings and challenges faced by WebRTC and proposing a Multipoint\nControl Unit or traditional communications entity based architecture as a\nsolution.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.09182v2"
    },
    {
        "title": "Combining and Steganography of 3D Face Textures",
        "authors": [
            "Mohsen Moradi",
            "Mohammad-Reza Sadeghi"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  One of the serious issues in communication between people is hiding\ninformation from others, and the best way for this, is deceiving them. Since\nnowadays face images are mostly used in three dimensional format, in this paper\nwe are going to steganography 3D face images, detecting which by curious people\nwill be impossible. As in detecting face only its texture is important, we\nseparate texture from shape matrices, for eliminating half of the extra\ninformation, steganography is done only for face texture, and for\nreconstructing 3D face, we can use any other shape. Moreover, we will indicate\nthat, by using two textures, how two 3D faces can be combined. For a complete\ndescription of the process, first, 2D faces are used as an input for building\n3D faces, and then 3D textures are hidden within other images.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.01325v2"
    },
    {
        "title": "Perceptual Compressive Sensing based on Contrast Sensitivity Function:\n  Can we avoid non-visible redundancies acquisition?",
        "authors": [
            "Seyed Hamid Safavi",
            "Farah Torkamani-Azar"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In this paper, we propose a novel CS approach in which the acquisition of\nnon-visible information is also avoided.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.05718v3"
    },
    {
        "title": "An Efficient Four-Parameter Affine Motion Model for Video Coding",
        "authors": [
            "Li Li",
            "Houqiang Li",
            "Dong Liu",
            "Haitao Yang",
            "Sixin Lin",
            "Huanbang Chen",
            "Feng Wu"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In this paper, we study a simplified affine motion model based coding\nframework to overcome the limitation of translational motion model and maintain\nlow computational complexity. The proposed framework mainly has three key\ncontributions. First, we propose to reduce the number of affine motion\nparameters from 6 to 4. The proposed four-parameter affine motion model can not\nonly handle most of the complex motions in natural videos but also save the\nbits for two parameters. Second, to efficiently encode the affine motion\nparameters, we propose two motion prediction modes, i.e., advanced affine\nmotion vector prediction combined with a gradient-based fast affine motion\nestimation algorithm and affine model merge, where the latter attempts to reuse\nthe affine motion parameters (instead of the motion vectors) of neighboring\nblocks. Third, we propose two fast affine motion compensation algorithms. One\nis the one-step sub-pixel interpolation, which reduces the computations of each\ninterpolation. The other is the interpolation-precision-based adaptive block\nsize motion compensation, which performs motion compensation at the block level\nrather than the pixel level to reduce the interpolation times. Our proposed\ntechniques have been implemented based on the state-of-the-art high efficiency\nvideo coding standard, and the experimental results show that the proposed\ntechniques altogether achieve on average 11.1% and 19.3% bits saving for random\naccess and low delay configurations, respectively, on typical video sequences\nthat have rich rotation or zooming motions. Meanwhile, the computational\ncomplexity increases of both encoder and decoder are within an acceptable\nrange.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.06297v1"
    },
    {
        "title": "Convolutional Neural Network-Based Block Up-sampling for Intra Frame\n  Coding",
        "authors": [
            "Yue Li",
            "Dong Liu",
            "Houqiang Li",
            "Li Li",
            "Feng Wu",
            "Hong Zhang",
            "Haitao Yang"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Inspired by the recent advances of image super-resolution using convolutional\nneural network (CNN), we propose a CNN-based block up-sampling scheme for intra\nframe coding. A block can be down-sampled before being compressed by normal\nintra coding, and then up-sampled to its original resolution. Different from\nprevious studies on down/up-sampling-based coding, the up-sampling methods in\nour scheme have been designed by training CNN instead of hand-crafted. We\nexplore a new CNN structure for up-sampling, which features deconvolution of\nfeature maps, multi-scale fusion, and residue learning, making the network both\ncompact and efficient. We also design different networks for the up-sampling of\nluma and chroma components, respectively, where the chroma up-sampling CNN\nutilizes the luma information to boost its performance. In addition, we design\na two-stage up-sampling process, the first stage being within the\nblock-by-block coding loop, and the second stage being performed on the entire\nframe, so as to refine block boundaries. We also empirically study how to set\nthe coding parameters of down-sampled blocks for pursuing the frame-level\nrate-distortion optimization. Our proposed scheme is implemented into the High\nEfficiency Video Coding (HEVC) reference software, and a comprehensive set of\nexperiments have been performed to evaluate our methods. Experimental results\nshow that our scheme achieves significant bits saving compared with HEVC anchor\nespecially at low bit rates, leading to on average 5.5% BD-rate reduction on\ncommon test sequences and on average 9.0% BD-rate reduction on ultra high\ndefinition (UHD) test sequences.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.06728v3"
    },
    {
        "title": "Analysis of video quality losses in the homogenous HEVC video\n  transcoding",
        "authors": [
            "Tomasz Grajek",
            "Jakub Stankowski",
            "Damian Karwowski",
            "Krzysztof Klimaszewski",
            "Olgierd Stankiewicz",
            "Krzysztof Wegner"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The paper presents quantitative analysis of the video quality losses in the\nhomogenous HEVC video transcoder. With the use of HM15.0 reference software and\na set of test video sequences, cascaded pixel domain video transcoder (CPDT)\nconcept has been used to gather all the necessary data needed for the analysis.\nThis experiment was done for wide range of source and target bitrates. The\nessential result of the work is extensive evaluation of CPDT, commonly used as\na reference in works on effective video transcoding. Until now no such\nextensively performed study have been made available in the literature. Quality\ndegradation between transcoded video and the video that would be result of\ndirect compression of the original video at the same bitrate as the transcoded\none have been reported. The dependency between quality degradation caused by\ntranscoding and the bitrate changes of the transcoded data stream are clearly\npresented on graphs.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.07548v1"
    },
    {
        "title": "Understanding Performance of Edge Content Caching for Mobile Video\n  Streaming",
        "authors": [
            "Ge Ma",
            "Zhi Wang",
            "Miao Zhang",
            "Jiahui Ye",
            "Minghua Chen",
            "Wenwu Zhu"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Today's Internet has witnessed an increase in the popularity of mobile video\nstreaming, which is expected to exceed 3/4 of the global mobile data traffic by\n2019. To satisfy the considerable amount of mobile video requests, video\nservice providers have been pushing their content delivery infrastructure to\nedge networks--from regional CDN servers to peer CDN servers (e.g.,\nsmartrouters in users' homes)--to cache content and serve users with storage\nand network resources nearby. Among the edge network content caching paradigms,\nWi-Fi access point caching and cellular base station caching have become two\nmainstream solutions. Thus, understanding the effectiveness and performance of\nthese solutions for large-scale mobile video delivery is important. However,\nthe characteristics and request patterns of mobile video streaming are unclear\nin practical wireless network. In this paper, we use real-world datasets\ncontaining 50 million trace items of nearly 2 million users viewing more than\n0.3 million unique videos using mobile devices in a metropolis in China over 2\nweeks, not only to understand the request patterns and user behaviors in mobile\nvideo streaming, but also to evaluate the effectiveness of Wi-Fi and\ncellular-based edge content caching solutions. To understand performance of\nedge content caching for mobile video streaming, we first present temporal and\nspatial video request patterns, and we analyze their impacts on caching\nperformance using frequency-domain and entropy analysis approaches. We then\nstudy the behaviors of mobile video users, including their mobility and\ngeographical migration behaviors. Using trace-driven experiments, we compare\nstrategies for edge content caching including LRU and LFU, in terms of\nsupporting mobile video requests. Moreover, we design an efficient caching\nstrategy based on the measurement insights and experimentally evaluate its\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.07627v1"
    },
    {
        "title": "Video Generation From Text",
        "authors": [
            "Yitong Li",
            "Martin Renqiang Min",
            "Dinghan Shen",
            "David Carlson",
            "Lawrence Carin"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Generating videos from text has proven to be a significant challenge for\nexisting generative models. We tackle this problem by training a conditional\ngenerative model to extract both static and dynamic information from text. This\nis manifested in a hybrid framework, employing a Variational Autoencoder (VAE)\nand a Generative Adversarial Network (GAN). The static features, called \"gist,\"\nare used to sketch text-conditioned background color and object layout\nstructure. Dynamic features are considered by transforming input text into an\nimage filter. To obtain a large amount of data for training the deep-learning\nmodel, we develop a method to automatically create a matched text-video corpus\nfrom publicly available online videos. Experimental results show that the\nproposed framework generates plausible and diverse videos, while accurately\nreflecting the input text information. It significantly outperforms baseline\nmodels that directly adapt text-to-image generation procedures to produce\nvideos. Performance is evaluated both visually and by adapting the inception\nscore used to evaluate image generation in GANs.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.00421v1"
    },
    {
        "title": "Attribute Compression of 3D Point Clouds Using Laplacian Sparsity\n  Optimized Graph Transform",
        "authors": [
            "Yiting Shao",
            "Zhaobin Zhang",
            "Zhu Li",
            "Kui Fan",
            "Ge Li"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  3D sensing and content capture have made significant progress in recent years\nand the MPEG standardization organization is launching a new project on\nimmersive media with point cloud compression (PCC) as one key corner stone. In\nthis work, we introduce a new binary tree based point cloud content partition\nand explore the graph signal processing tools, especially the graph transform\nwith optimized Laplacian sparsity, to achieve better energy compaction and\ncompression efficiency. The resulting rate-distortion operating points are\nconvex-hull optimized over the existing Lagrangian solutions. Simulation\nresults with the latest high quality point cloud content captured from the MPEG\nPCC demonstrated the transform efficiency and rate-distortion (R-D) optimal\npotential of the proposed solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.03532v1"
    },
    {
        "title": "JND-Based Perceptual Video Coding for 4:4:4 Screen Content Data in HEVC",
        "authors": [
            "Lee Prangnell",
            "Victor Sanchez"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The JCT-VC standardized Screen Content Coding (SCC) extension in the HEVC HM\nRExt + SCM reference codec offers an impressive coding efficiency performance\nwhen compared with HM RExt alone; however, it is not significantly perceptually\noptimized. For instance, it does not include advanced HVS-based perceptual\ncoding methods, such as JND-based spatiotemporal masking schemes. In this\npaper, we propose a novel JND-based perceptual video coding technique for HM\nRExt + SCM. The proposed method is designed to further improve the compression\nperformance of HM RExt + SCM when applied to YCbCr 4:4:4 SC video data. In the\nproposed technique, luminance masking and chrominance masking are exploited to\nperceptually adjust the Quantization Step Size (QStep) at the Coding Block (CB)\nlevel. Compared with HM RExt 16.10 + SCM 8.0, the proposed method considerably\nreduces bitrates (Kbps), with a maximum reduction of 48.3%. In addition to\nthis, the subjective evaluations reveal that SC-PAQ achieves visually lossless\ncoding at very low bitrates.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.09919v2"
    },
    {
        "title": "Watching Videos with Certain and Constant Quality: PID-based Quality\n  Control Method",
        "authors": [
            "Yuhang Song",
            "Mai Xu",
            "Shengxi Li"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In video coding, compressed videos with certain and constant quality can\nensure quality of experience (QoE). To this end, we propose in this paper a\nnovel PID-based quality control (PQC) method for video coding. Specifically, a\nformulation is modelled to control quality of video coding with two objectives:\nminimizing control error and quality fluctuation. Then, we apply the Laplace\ndomain analysis to model the relationship between quantization parameter (QP)\nand control error in this formulation. Given the relationship between QP and\ncontrol error, we propose a solution to the PQC formulation, such that videos\ncan be compressed at certain and constant quality. Finally, experimental\nresults show that our PQC method is effective in both control accuracy and\nquality fluctuation.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.09980v1"
    },
    {
        "title": "Prediction of Satisfied User Ratio for Compressed Video",
        "authors": [
            "Haiqiang Wang",
            "Ioannis Katsavounidis",
            "Qin Huang",
            "Xin Zhou",
            "C. -C. Jay Kuo"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  A large-scale video quality dataset called the VideoSet has been constructed\nrecently to measure human subjective experience of H.264 coded video in terms\nof the just-noticeable-difference (JND). It measures the first three JND points\nof 5-second video of resolution 1080p, 720p, 540p and 360p. Based on the\nVideoSet, we propose a method to predict the satisfied-user-ratio (SUR) curves\nusing a machine learning framework. First, we partition a video clip into local\nspatial-temporal segments and evaluate the quality of each segment using the\nVMAF quality index. Then, we aggregate these local VMAF measures to derive a\nglobal one. Finally, the masking effect is incorporated and the support vector\nregression (SVR) is used to predict the SUR curves, from which the JND points\ncan be derived. Experimental results are given to demonstrate the performance\nof the proposed SUR prediction method.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.11090v1"
    },
    {
        "title": "Image Encryption Algorithm Based on Facebook Social Network",
        "authors": [
            "Xiaoqing Liu",
            "Yinyin Peng",
            "Jie Wang",
            "Zhaoxia Yin"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Facebook is the online social networks (OSNs) platform with the largest\nnumber of users in the world today, information protection based on Facebook\nsocial network platform have important practical significance. Since the\ninformation users share on social networks is often based on images, this paper\nproposes a more secure image encryption algorithm based on Facebook social\nnetwork platform to ensure the loss of information as much as possible. When\nthe sender encrypts the image for uploading, it can first resist the third\nparty's attack on the encrypted image and prevent the image data from leaking,\nsimultaneously processed by some unknown processing such as compression and\nfiltering of the image on the Facebook platform, the receiver can still decrypt\nthe corresponding image data.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.03175v1"
    },
    {
        "title": "Frequency-Dependent Perceptual Quantisation for Visually Lossless\n  Compression Applications",
        "authors": [
            "Lee Prangnell"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The default quantisation algorithms in the state-of-the-art High Efficiency\nVideo Coding (HEVC) standard, namely Uniform Reconstruction Quantisation (URQ)\nand Rate-Distortion Optimised Quantisation (RDOQ), do not take into account the\nperceptual relevance of individual transform coefficients. In this paper, a\nFrequency-Dependent Perceptual Quantisation (FDPQ) technique for HEVC is\nproposed. FDPQ exploits the well-established Modulation Transfer Function (MTF)\ncharacteristics of the linear transformation basis functions by taking into\naccount the Euclidean distance of an AC transform coefficient from the DC\ncoefficient. As such, in luma and chroma Cb and Cr Transform Blocks (TBs), FDPQ\nquantises more coarsely the least perceptually relevant transform coefficients\n(i.e., the high frequency AC coefficients). Conversely, FDPQ preserves the\nintegrity of the DC coefficient and the very low frequency AC coefficients.\nCompared with RDOQ, which is the most widely used transform coefficient-level\nquantisation technique in video coding, FDPQ successfully achieves bitrate\nreductions of up to 41%. Furthermore, the subjective evaluations confirm that\nthe FDPQ-coded video data is perceptually indistinguishable (i.e., visually\nlossless) from the raw video data for a given Quantisation Parameter (QP).\n",
        "pdf_link": "http://arxiv.org/pdf/1906.03395v1"
    },
    {
        "title": "Key-Point Sequence Lossless Compression for Intelligent Video Analysis",
        "authors": [
            "Weiyao Lin",
            "Xiaoyi He",
            "Wenrui Dai",
            "John See",
            "Tushar Shinde",
            "Hongkai Xiong",
            "Lingyu Duan"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Feature coding has been recently considered to facilitate intelligent video\nanalysis for urban computing. Instead of raw videos, extracted features in the\nfront-end are encoded and transmitted to the back-end for further processing.\nIn this article, we present a lossless key-point sequence compression approach\nfor efficient feature coding. The essence of this predict-and-encode strategy\nis to eliminate the spatial and temporal redundancies of key points in videos.\nMultiple prediction modes with an adaptive mode selection method are proposed\nto handle key-point sequences with various structures and motion. Experimental\nresults validate the effectiveness of the proposed scheme on four types of\nwidely used key-point sequences in video analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.04646v1"
    },
    {
        "title": "DualLip: A System for Joint Lip Reading and Generation",
        "authors": [
            "Weicong Chen",
            "Xu Tan",
            "Yingce Xia",
            "Tao Qin",
            "Yu Wang",
            "Tie-Yan Liu"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Lip reading aims to recognize text from talking lip, while lip generation\naims to synthesize talking lip according to text, which is a key component in\ntalking face generation and is a dual task of lip reading. In this paper, we\ndevelop DualLip, a system that jointly improves lip reading and generation by\nleveraging the task duality and using unlabeled text and lip video data. The\nkey ideas of the DualLip include: 1) Generate lip video from unlabeled text\nwith a lip generation model, and use the pseudo pairs to improve lip reading;\n2) Generate text from unlabeled lip video with a lip reading model, and use the\npseudo pairs to improve lip generation. We further extend DualLip to talking\nface generation with two additionally introduced components: lip to face\ngeneration and text to speech generation. Experiments on GRID and TCD-TIMIT\ndemonstrate the effectiveness of DualLip on improving lip reading, lip\ngeneration, and talking face generation by utilizing unlabeled data.\nSpecifically, the lip generation model in our DualLip system trained with\nonly10% paired data surpasses the performance of that trained with the whole\npaired data. And on the GRID benchmark of lip reading, we achieve 1.16%\ncharacter error rate and 2.71% word error rate, outperforming the\nstate-of-the-art models using the same amount of paired data.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.05784v1"
    },
    {
        "title": "H.264/SVC Mode Decision Based on Mode Correlation and Desired Mode List",
        "authors": [
            "L. Balaji",
            "K. K. Thyagharajan"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The design of video encoders involves the implementation of fast mode\ndecision (FMD) algorithm to reduce computation complexity while maintaining the\nperformance of the coding. Although H.264/scalable video coding (SVC) achieves\nhigh scalability and coding efficiency, it also has high complexity in\nimplementing its exhaustive computation. In this paper, a novel algorithm is\nproposed to reduce the redundant candidate modes by making use of the\ncorrelation among layers. The desired mode list is created based on the\nprobability to be the best mode for each block in the base layer and a\ncandidate mode selection in the enhancement layer by the correlations of modes\namong the reference frame and current frame. Our algorithm is implemented in\njoint scalable video model (JSVM) 9.19.15 reference software and the\nperformance is evaluated based on the average encoding time, peak signal to\nnoise ratio (PSNR) and bit rate. The experimental results show 41.89%\nimprovement in encoding time with minimal loss of 0.02dB in PSNR and 0.05%\nincrease in bit rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.10708v1"
    },
    {
        "title": "Adaptive Multi-modal Fusion Hashing via Hadamard Matrix",
        "authors": [
            "Jun Yu",
            "Donglin Zhang",
            "Zhenqiu Shu",
            "Feng Chen"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Hashing plays an important role in information retrieval, due to its low\nstorage and high speed of processing. Among the techniques available in the\nliterature, multi-modal hashing, which can encode heterogeneous multi-modal\nfeatures into compact hash codes, has received particular attention. Most of\nthe existing multi-modal hashing methods adopt the fixed weighting factors to\nfuse multiple modalities for any query data, which cannot capture the variation\nof different queries. Besides, many methods introduce hyper-parameters to\nbalance many regularization terms that make the optimization harder. Meanwhile,\nit is time-consuming and labor-intensive to set proper parameter values. The\nlimitations may significantly hinder their promotion in real applications. In\nthis paper, we propose a simple, yet effective method that is inspired by the\nHadamard matrix. The proposed method captures the multi-modal feature\ninformation in an adaptive manner and preserves the discriminative semantic\ninformation in the hash codes. Our framework is flexible and involves a very\nfew hyper-parameters. Extensive experimental results show the method is\neffective and achieves superior performance compared to state-of-the-art\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.12148v4"
    },
    {
        "title": "Traffic model of LTE using maximum flow algorithm with binary search\n  technique",
        "authors": [
            "Md. Zahurul Haque",
            "Md. Rafiqul Isla"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Inrecent time a rapid increase in the number of smart devices and user\napplications have generated an intensity volume of data traffic from/to a\ncellular network. So the Long Term Evaluation(LTE)network is facing some\nissuesdifficulties ofthebase station and infrastructure in terms of upgrade and\nconfiguration becausethere is no concept of BSC (Base Station Controller) of 2G\nand RNC (Radio Network Controller) of 3G to control several BTS/NB. Only 4G\n(LTE) all the eNBs areinterconnected for traffic flow from UE (user equipment)\nto core switch. Determination of capacity of a linkof such a network is a\nchallenging job since each node offers its own traffic andat the same time\nconveys traffic of other nodes.In this paper, we apply maximum flow algorithm\nincluding the binary search techniqueto solve the traffic flow of radio\nnetworkandinterconnected eNBs of the LTE network. The throughput of the LTE\nnetwork shown graphically under the QPSK and 16-QAM\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13216v1"
    },
    {
        "title": "Describing Subjective Experiment Consistency by $p$-Value P-P Plot",
        "authors": [
            "Jakub Nawała",
            "Lucjan Janowski",
            "Bogdan Ćmiel",
            "Krzysztof Rusek"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  There are phenomena that cannot be measured without subjective testing.\nHowever, subjective testing is a complex issue with many influencing factors.\nThese interplay to yield either precise or incorrect results. Researchers\nrequire a tool to classify results of subjective experiment as either\nconsistent or inconsistent. This is necessary in order to decide whether to\ntreat the gathered scores as quality ground truth data. Knowing if subjective\nscores can be trusted is key to drawing valid conclusions and building\nfunctional tools based on those scores (e.g., algorithms assessing the\nperceived quality of multimedia materials). We provide a tool to classify\nsubjective experiment (and all its results) as either consistent or\ninconsistent. Additionally, the tool identifies stimuli having irregular score\ndistribution. The approach is based on treating subjective scores as a random\nvariable coming from the discrete Generalized Score Distribution (GSD). The\nGSD, in combination with a bootstrapped G-test of goodness-of-fit, allows to\nconstruct $p$-value P-P plot that visualizes experiment's consistency. The tool\nsafeguards researchers from using inconsistent subjective data. In this way, it\nmakes sure that conclusions they draw and tools they build are more precise and\ntrustworthy. The proposed approach works in line with expectations drawn solely\non experiment design descriptions of 21 real-life multimedia quality subjective\nexperiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13372v1"
    },
    {
        "title": "Sequential Reinforced 360-Degree Video Adaptive Streaming with\n  Cross-user Attentive Network",
        "authors": [
            "Jun Fu",
            "Zhibo Chen",
            "Xiaoming Chen",
            "Weiping Li"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In the tile-based 360-degree video streaming, predicting user's future\nviewpoints and developing adaptive bitrate (ABR) algorithms are essential for\noptimizing user's quality of experience (QoE). Traditional single-user based\nviewpoint prediction methods fail to achieve good performance in long-term\nprediction, and the recently proposed reinforcement learning (RL) based ABR\nschemes applied in traditional video streaming can not be directly applied in\nthe tile-based 360-degree video streaming due to the exponential action space.\nTherefore, we propose a sequential reinforced 360-degree video streaming scheme\nwith cross-user attentive network. Firstly, considering different users may\nhave the similar viewing preference on the same video, we propose a cross-user\nattentive network (CUAN), boosting the performance of long-term viewpoint\nprediction by selectively utilizing cross-user information. Secondly, we\npropose a sequential RL-based (360SRL) ABR approach, transforming action space\nsize of each decision step from exponential to linear via introducing a\nsequential decision structure. We evaluate the proposed CUAN and 360SRL using\ntrace-driven experiments and experimental results demonstrate that CUAN and\n360SRL outperform existing viewpoint prediction and ABR approaches with a\nnoticeable margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13737v1"
    },
    {
        "title": "Performance of AV1 Real-Time Mode",
        "authors": [
            "Ludovic Roux",
            "Alexandre Gouaillard"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  With COVID-19, the interest for digital interactions has raised, putting in\nturn real-time (or low-latency) codecs into a new light. Most of the codec\nresearch has been traditionally focusing on coding efficiency, while very\nlittle literature exist on real-time codecs. It is shown how the speed at which\ncontent is made available impacts both latency and throughput. The authors\nintroduce a new test set up, integrating a paced reader, which allows to run\ncodec in the same condition as real-time media capture. Quality measurements\nusing VMAF, as well as multiple speed measurements are made on encoding of HD\nand full HD video sequences, both at 25 fps and 50 fps to compare the\nrespective performances of several implementations of the H.264, H.265, VP8,\nVP9 and AV1 codecs.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.14165v2"
    },
    {
        "title": "Optimal Frame Transmission for Scalable Video with Hierarchical\n  Prediction Structure",
        "authors": [
            "Saied Mehdian",
            "Ben Liang"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  An optimal frame transmission scheme is presented for streaming scalable\nvideo over a link with limited capacity. The objective is to select a\ntransmission sequence of frames and their transmission schedule such that the\noverall video quality is maximized. The problem is solved for two general\nclasses of hierarchical prediction structures, which include as a special case\nthe popular dyadic structure. Based on a new characterization of the\ninterdependence among frames in terms of trees, structural properties of an\noptimal transmission schedule are derived. These properties lead to the\ndevelopment of a jointly optimal frame selection and scheduling algorithm,\nwhich has computational complexity that is quadratic in the number of frames.\nSimulation results show that the optimal scheme substantially outperforms three\nexisting alternatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.4999v2"
    },
    {
        "title": "Loss-resilient Coding of Texture and Depth for Free-viewpoint Video\n  Conferencing",
        "authors": [
            "Bruno Macchiavello",
            "Camilo Dorea",
            "Edson M. Hung",
            "Gene Cheung",
            "Wai-tian Tan"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Free-viewpoint video conferencing allows a participant to observe the remote\n3D scene from any freely chosen viewpoint. An intermediate virtual viewpoint\nimage is commonly synthesized using two pairs of transmitted texture and depth\nmaps from two neighboring captured viewpoints via depth-image-based rendering\n(DIBR). To maintain high quality of synthesized images, it is imperative to\ncontain the adverse effects of network packet losses that may arise during\ntexture and depth video transmission. Towards this end, we develop an\nintegrated approach that exploits the representation redundancy inherent in the\nmultiple streamed videos a voxel in the 3D scene visible to two captured views\nis sampled and coded twice in the two views. In particular, at the receiver we\nfirst develop an error concealment strategy that adaptively blends\ncorresponding pixels in the two captured views during DIBR, so that pixels from\nthe more reliable transmitted view are weighted more heavily. We then couple it\nwith a sender-side optimization of reference picture selection (RPS) during\nreal-time video coding, so that blocks containing samples of voxels that are\nvisible in both views are more error-resiliently coded in one view only, given\nadaptive blending will erase errors in the other view. Further, synthesized\nview distortion sensitivities to texture versus depth errors are analyzed, so\nthat relative importance of texture and depth code blocks can be computed for\nsystem-wide RPS optimization. Experimental results show that the proposed\nscheme can outperform the use of a traditional feedback channel by up to 0.82\ndB on average at 8% packet loss rate, and by as much as 3 dB for particular\nframes.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.5464v1"
    },
    {
        "title": "Recover Subjective Quality Scores from Noisy Measurements",
        "authors": [
            "Zhi Li",
            "Christos G. Bampis"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Simple quality metrics such as PSNR are known to not correlate well with\nsubjective quality when tested across a wide spectrum of video content or\nquality regime. Recently, efforts have been made in designing objective quality\nmetrics trained on subjective data (e.g. VMAF), demonstrating better\ncorrelation with video quality perceived by human. Clearly, the accuracy of\nsuch a metric heavily depends on the quality of the subjective data that it is\ntrained on. In this paper, we propose a new approach to recover subjective\nquality scores from noisy raw measurements, using maximum likelihood\nestimation, by jointly estimating the subjective quality of impaired videos,\nthe bias and consistency of test subjects, and the ambiguity of video contents\nall together. We also derive closed-from expression for the confidence interval\nof each estimate. Compared to previous methods which partially exploit the\nsubjective information, our approach is able to exploit the information in\nfull, yielding tighter confidence interval and better handling of outliers\nwithout the need for z-scoring or subject rejection. It also handles missing\ndata more gracefully. Finally, as side information, it provides interesting\ninsights on the test subjects and video contents.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.01715v3"
    },
    {
        "title": "Large-scale JPEG steganalysis using hybrid deep-learning framework",
        "authors": [
            "Jishen Zeng",
            "Shunquan Tan",
            "Bin Li",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Adoption of deep learning in image steganalysis is still in its initial\nstage. In this paper we propose a generic hybrid deep-learning framework for\nJPEG steganalysis incorporating the domain knowledge behind rich steganalytic\nmodels. Our proposed framework involves two main stages. The first stage is\nhand-crafted, corresponding to the convolution phase and the quantization &\ntruncation phase of the rich models. The second stage is a compound deep neural\nnetwork containing multiple deep subnets in which the model parameters are\nlearned in the training procedure. We provided experimental evidences and\ntheoretical reflections to argue that the introduction of threshold quantizers,\nthough disable the gradient-descent-based learning of the bottom convolution\nphase, is indeed cost-effective. We have conducted extensive experiments on a\nlarge-scale dataset extracted from ImageNet. The primary dataset used in our\nexperiments contains 500,000 cover images, while our largest dataset contains\nfive million cover images. Our experiments show that the integration of\nquantization and truncation into deep-learning steganalyzers do boost the\ndetection performance by a clear margin. Furthermore, we demonstrate that our\nframework is insensitive to JPEG blocking artifact alterations, and the learned\nmodel can be easily transferred to a different attacking target and even a\ndifferent dataset. These properties are of critical importance in practical\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.03233v3"
    },
    {
        "title": "Columbia MVSO Image Sentiment Dataset",
        "authors": [
            "Vaidehi Dalmia",
            "Hongyi Liu",
            "Shih-Fu Chang"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The Multilingual Visual Sentiment Ontology (MVSO) consists of 15,600 concepts\nin 12 different languages that are strongly related to emotions and sentiments\nexpressed in images. These concepts are defined in the form of Adjective-Noun\nPair (ANP), which are crawled and discovered from online image forum Flickr. In\nthis work, we used Amazon Mechanical Turk as a crowd-sourcing platform to\ncollect human judgments on sentiments expressed in images that are uniformly\nsampled over 3,911 English ANPs extracted from a tag-restricted subset of MVSO.\nOur goal is to use the dataset as a benchmark for the evaluation of systems\nthat automatically predict sentiments in images or ANPs.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.04455v1"
    },
    {
        "title": "A Second Order Derivatives based Approach for Steganography",
        "authors": [
            "Jean-François Couchot",
            "Raphaël Couturier",
            "Yousra Ahmed Fadil",
            "Christophe Guyeux"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Steganography schemes are designed with the objective of minimizing a defined\ndistortion function. In most existing state of the art approaches, this\ndistortion function is based on image feature preservation. Since smooth\nregions or clean edges define image core, even a small modification in these\nareas largely modifies image features and is thus easily detectable. On the\ncontrary, textures, noisy or chaotic regions are so difficult to model that the\nfeatures having been modified inside these areas are similar to the initial\nones. These regions are characterized by disturbed level curves. This work\npresents a new distortion function for steganography that is based on second\norder derivatives, which are mathematical tools that usually evaluate level\ncurves. Two methods are explained to compute these partial derivatives and have\nbeen completely implemented. The first experiments show that these approaches\nare promising.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.08397v1"
    },
    {
        "title": "A novel Adaptive weighted Kronecker Compressive Sensing",
        "authors": [
            "Seyed Hamid Safavi",
            "Farah Torkamani-Azar"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Recently, multidimensional signal reconstruction using a low number of\nmeasurements is of great interest. Therefore, an effective sampling scheme\nwhich should acquire the most information of signal using a low number of\nmeasurements is required. In this paper, we study a novel cube-based method for\nsampling and reconstruction of multidimensional signals. First, inspired by the\nblock-based compressive sensing (BCS), we divide a group of pictures (GoP) in a\nvideo sequence into cubes. By this way, we can easily store the measurement\nmatrix and also easily can generate the sparsifying basis. The reconstruction\nprocess also can be done in parallel. Second, along with the Kronecker\nstructure of the sampling matrix, we design a weight matrix based on the human\nvisuality system, i.e. perceptually. We will also benefit from different\nweighted $\\ell_1$-minimization methods for reconstruction. Furthermore,\nconventional methods for BCS consider an equal number of samples for all\nblocks. However, the sparsity order of blocks in natural images could be\ndifferent and, therefore, a various number of samples could be required for\ntheir reconstruction. Motivated by this point, we will adaptively allocate the\nsamples for each cube in a video sequence. Our aim is to show that our simple\nlinear sampling approach can be competitive with the other state-of-the-art\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.01113v2"
    },
    {
        "title": "Pseudo Sequence based 2-D hierarchical reference structure for\n  Light-Field Image Compression",
        "authors": [
            "Li Li",
            "Zhu Li",
            "Bin Li",
            "Dong Liu",
            "Houqiang Li"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  In this paper, we present a novel pseudo sequence based 2-D hierarchical\nreference structure for light-field image compression. In the proposed scheme,\nwe first decompose the light-field image into multiple views and organize them\ninto a 2-D coding structure according to the spatial coordinates of the\ncorresponding microlens. Then we mainly develop three technologies to optimize\nthe 2-D coding structure. First, we divide all the views into four quadrants,\nand all the views are encoded one quadrant after another to reduce the\nreference buffer size as much as possible. Inside each quadrant, all the views\nare encoded hierarchically to fully exploit the correlations between different\nviews. Second, we propose to use the distance between the current view and its\nreference views as the criteria for selecting better reference frames for each\ninter view. Third, we propose to use the spatial relative positions between\ndifferent views to achieve more accurate motion vector scaling. The whole\nscheme is implemented in the reference software of High Efficiency Video\nCoding. The experimental results demonstrate that the proposed novel\npseudo-sequence based 2-D hierarchical structure can achieve maximum 14.2%\nbit-rate savings compared with the state-of-the-art light-field image\ncompression method.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07309v1"
    },
    {
        "title": "Object Shape Approximation & Contour Adaptive Depth Image Coding for\n  Virtual View Synthesis",
        "authors": [
            "Yuan Yuan",
            "Gene Cheung",
            "Patrick Le Callet",
            "Pascal Frossard",
            "Hong Vicky Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  A depth image provides partial geometric information of a 3D scene, namely\nthe shapes of physical objects as observed from a particular viewpoint. This\ninformation is important when synthesizing images of different virtual camera\nviewpoints via depth-image-based rendering (DIBR). It has been shown that depth\nimages can be efficiently coded using contour-adaptive codecs that preserve\nedge sharpness, resulting in visually pleasing DIBR-synthesized images.\nHowever, contours are typically losslessly coded as side information (SI),\nwhich is expensive if the object shapes are complex.\n  In this paper, we pursue a new paradigm in depth image coding for\ncolor-plus-depth representation of a 3D scene: we pro-actively simplify object\nshapes in a depth and color image pair to reduce depth coding cost, at a\npenalty of a slight increase in synthesized view distortion. Specifically, we\nfirst mathematically derive a distortion upper-bound proxy for 3DSwIM---a\nquality metric tailored for DIBR-synthesized images. This proxy reduces\ninterdependency among pixel rows in a block to ease optimization. We then\napproximate object contours via a dynamic programming (DP) algorithm to\noptimally trade off coding cost of contours using arithmetic edge coding (AEC)\nwith our proposed view synthesis distortion proxy. We modify the depth and\ncolor images according to the approximated object contours in an inter-view\nconsistent manner. These are then coded respectively using a contour-adaptive\nimage codec based on graph Fourier transform (GFT) for edge preservation and\nHEVC intra. Experimental results show that by maintaining sharp but simplified\nobject contours during contour-adaptive coding, for the same visual quality of\nDIBR-synthesized virtual views, our proposal can reduce depth image coding rate\nby up to 22% compared to alternative coding strategies such as HEVC intra.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07872v1"
    },
    {
        "title": "Cross-Color Channel Perceptually Adaptive Quantization for HEVC",
        "authors": [
            "Lee Prangnell",
            "Miguel Hernández-Cabronero",
            "Victor Sanchez"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  HEVC includes a Coding Unit (CU) level luminance-based perceptual\nquantization technique known as AdaptiveQP. AdaptiveQP perceptually adjusts the\nQuantization Parameter (QP) at the CU level based on the spatial activity of\nraw input video data in a luma Coding Block (CB). In this paper, we propose a\nnovel cross-color channel adaptive quantization scheme which perceptually\nadjusts the CU level QP according to the spatial activity of raw input video\ndata in the constituent luma and chroma CBs; i.e., the combined spatial\nactivity across all three color channels (the Y, Cb and Cr channels). Our\ntechnique is evaluated in HM 16 with 4:4:4, 4:2:2 and 4:2:0 YCbCr JCT-VC test\nsequences. Both subjective and objective visual quality evaluations are\nundertaken during which we compare our method with AdaptiveQP. Our technique\nachieves considerable coding efficiency improvements, with maximum BD-Rate\nreductions of 15.9% (Y), 13.1% (Cr) and 16.1% (Cb) in addition to a maximum\ndecoding time reduction of 11.0%.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07893v4"
    },
    {
        "title": "Streaming Virtual Reality Content",
        "authors": [
            "Tarek El-Ganainy",
            "Mohamed Hefeeda"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The recent rise of interest in Virtual Reality (VR) came with the\navailability of commodity commercial VR prod- ucts, such as the Head Mounted\nDisplays (HMD) created by Oculus and other vendors. To accelerate the user\nadoption of VR headsets, content providers should focus on producing high\nquality immersive content for these devices. Similarly, multimedia streaming\nservice providers should enable the means to stream 360 VR content on their\nplatforms. In this study, we try to cover different aspects related to VR\ncontent representation, streaming, and quality assessment that will help\nestablishing the basic knowledge of how to build a VR streaming system.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08350v1"
    },
    {
        "title": "Improving Blind Steganalysis in Spatial Domain using a Criterion to\n  Choose the Appropriate Steganalyzer between CNN and SRM+EC",
        "authors": [
            "Jean-Francois Couchot",
            "Raphaël Couturier",
            "Michel Salomon"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Conventional state-of-the-art image steganalysis approaches usually consist\nof a classifier trained with features provided by rich image models. As both\nfeatures extraction and classification steps are perfectly embodied in the deep\nlearning architecture called Convolutional Neural Network (CNN), different\nstudies have tried to design a CNN-based steganalyzer. The network designed by\nXu et al. is the first competitive CNN with the combination Spatial Rich Models\n(SRM) and Ensemble Classifier (EC) providing detection performances of the same\norder. In this work we propose a criterion to choose either the CNN or the\nSRM+EC method for a given input image. Our approach is studied with three\ndifferent steganographic spatial domain algorithms: S-UNIWARD, MiPOD, and HILL,\nusing the Tensorflow computing platform, and exhibits detection capabilities\nbetter than each method alone. Furthermore, as SRM+EC and the CNN are both only\ntrained with a single embedding algorithm, namely MiPOD, the proposed method\ncan be seen as an approach for blind steganalysis. In blind detection, error\nrates are respectively of 16% for S-UNIWARD, 16% for MiPOD, and 17% for HILL on\nthe BOSSBase with a payload of 0.4 bpp. For 0.1 bpp, the respective\ncorresponding error rates are of 39%, 38%, and 41%, and are always better than\nthe ones provided by SRM+EC.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08882v2"
    },
    {
        "title": "Synchronous Prediction of Arousal and Valence Using LSTM Network for\n  Affective Video Content Analysis",
        "authors": [
            "Ligang Zhang",
            "Jiulong Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The affect embedded in video data conveys high-level semantic information\nabout the content and has direct impact on the understanding and perception of\nreviewers, as well as their emotional responses. Affective Video Content\nAnalysis (AVCA) attempts to generate a direct mapping between video content and\nthe corresponding affective states such as arousal and valence dimensions. Most\nexisting studies establish the mapping for each dimension separately using\nknowledge-based rules or traditional classifiers such as Support Vector Machine\n(SVM). The inherent correlations between affective dimensions have largely been\nunexploited, which are anticipated to include important information for\naccurate prediction of affective dimensions. To address this issue, this paper\npresents an approach to predict arousal and valance dimensions synchronously\nusing the Long Short Term Memory (LSTM) network. The approach extracts a set of\nlow-level audio and visual features from video data and projects them\nsynchronously into pairs of arousal and valence values using the LSTM network\nwhich automatically incorporates the correlations between arousal and valance\ndimensions. We evaluate the performance of the proposed approach on a dataset\ncomprising video clips segmented from real-world resources such as film, drama,\nand news, and demonstrate its superior performance over the traditional SVM\nbased method. The results provide one of the earliest preliminary evidence to\nthe benefit of considering correlations between affective dimensions towards\naccurate AVCA.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.00257v1"
    },
    {
        "title": "A Revision Control System for Image Editing in Collaborative Multimedia\n  Design",
        "authors": [
            "Fabio Calefato",
            "Giovanna Castellano",
            "Veronica Rossano"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Revision control is a vital component in the collaborative development of\nartifacts such as software code and multimedia. While revision control has been\nwidely deployed for text files, very few attempts to control the versioning of\nbinary files can be found in the literature. This can be inconvenient for\ngraphics applications that use a significant amount of binary data, such as\nimages, videos, meshes, and animations. Existing strategies such as storing\nwhole files for individual revisions or simple binary deltas, respectively\nconsume significant storage and obscure semantic information. To overcome these\nlimitations, in this paper we present a revision control system for digital\nimages that stores revisions in form of graphs. Besides, being integrated with\nGit, our revision control system also facilitates artistic creation processes\nin common image editing and digital painting workflows. A preliminary user\nstudy demonstrates the usability of the proposed system.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.00263v2"
    },
    {
        "title": "Sloth Search System at the Video Browser Showdown 2018 - Final Notes",
        "authors": [
            "Nattachai Watcharapinchai",
            "Sitapa Rujikietgumjorn",
            "Sanparith Marukatat"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  This short paper provides further details of the Sloth Search System, which\nwas developed by the NECTEC team for the Video Browser Showdown (VBS) 2018.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01081v1"
    },
    {
        "title": "Double JPEG Compression Detection by Exploring the Correlations in DCT\n  Domain",
        "authors": [
            "Pengpeng Yang",
            "Rongrong Ni",
            "Yao Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In the field of digital image processing, JPEG image compression technique\nhas been widely applied. And numerous image processing software suppose this.\nIt is likely for the images undergoing double JPEG compression to be tampered.\nTherefore, double JPEG compression detection schemes can provide an important\nclue for image forgery detection. In this paper, we propose an effective\nalgorithm to detect double JPEG compression with different quality factors.\nFirstly, the quantized DCT coefficients with same frequency are extracted to\nbuild the new data matrices. Then, considering the direction effect on the\ncorrelation between the adjacent positions in DCT domain, twelve kinds of\nhigh-pass filter templates with different directions are executed and the\ntranslation probability matrix is calculated for each filtered data.\nFurthermore, principal component analysis and support vector machine technique\nare applied to reduce the feature dimension and train a classifier,\nrespectively. Experimental results have demonstrated that the proposed method\nis effective and has comparable performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01571v1"
    },
    {
        "title": "Convolutional Video Steganography with Temporal Residual Modeling",
        "authors": [
            "Xinyu Weng",
            "Yongzhi Li",
            "Lu Chi",
            "Yadong Mu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Steganography represents the art of unobtrusively concealing a secrete\nmessage within some cover data. The key scope of this work is about visual\nsteganography techniques that hide a full-sized color image / video within\nanother. A majority of existing works are devoted to the image case, where both\nsecret and cover data are images. We empirically validate that image\nsteganography model does not naturally extend to the video case (i.e., hiding a\nvideo into another video), mainly because it completely ignores the temporal\nredundancy within consecutive video frames. Our work proposes a novel solution\nto the problem of video steganography. The technical contributions are\ntwo-fold: first, the residual between two consecutive frames tends to zero at\nmost pixels. Hiding such highly-sparse data is significantly easier than hiding\nthe original frames. Motivated by this fact, we propose to explicitly consider\ninter-frame residuals rather than blindly applying image steganography model on\nevery video frame. Specifically, our model contains two branches, one of which\nis specially designed for hiding inter-frame difference into a cover video\nframe and the other instead hides the original secret frame. A simple\nthresholding method determines which branch a secret video frame shall choose.\nWhen revealing the concealed secret video, two decoders are devised, revealing\ndifference or frame respectively. Second, we develop the model based on deep\nconvolutional neural networks, which is the first of its kind in the literature\nof video steganography. In experiments, comprehensive evaluations are conducted\nto compare our model with both classic least significant bit (LSB) method and\npure image steganography models. All results strongly suggest that the proposed\nmodel enjoys advantages over previous methods. We also carefully investigate\nkey factors in the success of our deep video steganography model.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.02941v1"
    },
    {
        "title": "StegNet: Mega Image Steganography Capacity with Deep Convolutional\n  Network",
        "authors": [
            "Pin Wu",
            "Yang Yang",
            "Xiaoqiang Li"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Traditional image steganography often leans interests towards safely\nembedding hidden information into cover images with payload capacity almost\nneglected. This paper combines recent deep convolutional neural network methods\nwith image-into-image steganography. It successfully hides the same size images\nwith a decoding rate of 98.2% or bpp (bits per pixel) of 23.57 by changing only\n0.76% of the cover image on average. Our method directly learns end-to-end\nmappings between the cover image and the embedded image and between the hidden\nimage and the decoded image. We~further show that our embedded image, while\nwith mega payload capacity, is still robust to statistical analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.06357v1"
    },
    {
        "title": "Source Printer Classification using Printer Specific Local Texture\n  Descriptor",
        "authors": [
            "Sharad Joshi",
            "Nitin Khanna"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The knowledge of source printer can help in printed text document\nauthentication, copyright ownership, and provide important clues about the\nauthor of a fraudulent document along with his/her potential means and motives.\nDevelopment of automated systems for classifying printed documents based on\ntheir source printer, using image processing techniques, is gaining a lot of\nattention in multimedia forensics. Currently, state-of-the-art systems require\nthat the font of letters present in test documents of unknown origin must be\navailable in those used for training the classifier. In this work, we attempt\nto take the first step towards overcoming this limitation. Specifically, we\nintroduce a novel printer specific local texture descriptor. The highlight of\nour technique is the use of encoding and regrouping strategy based on small\nlinear-shaped structures composed of pixels having similar intensity and\ngradient. The results of experiments performed on two separate datasets show\nthat: 1) on a publicly available dataset, the proposed method outperforms\nstate-of-the-art algorithms for characters printed in the same font, and 2) on\nanother dataset\\footnote{Code and dataset will be made publicly available with\npublished version of this paper.} having documents printed in four different\nfonts, the proposed method correctly classifies all test samples when\nsufficient training data is available in same font setup. In addition, it\noutperforms state-of-the-art methods for cross font experiments. Moreover, it\nreduces the confusion between the printers of same brand and model.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.06650v1"
    },
    {
        "title": "A Group Variational Transformation Neural Network for Fractional\n  Interpolation of Video Coding",
        "authors": [
            "Sifeng Xia",
            "Wenhan Yang",
            "Yueyu Hu",
            "Siwei Ma",
            "Jiaying Liu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Motion compensation is an important technology in video coding to remove the\ntemporal redundancy between coded video frames. In motion compensation,\nfractional interpolation is used to obtain more reference blocks at sub-pixel\nlevel. Existing video coding standards commonly use fixed interpolation filters\nfor fractional interpolation, which are not efficient enough to handle diverse\nvideo signals well. In this paper, we design a group variational transformation\nconvolutional neural network (GVTCNN) to improve the fractional interpolation\nperformance of the luma component in motion compensation. GVTCNN infers samples\nat different sub-pixel positions from the input integer-position sample. It\nfirst extracts a shared feature map from the integer-position sample to infer\nvarious sub-pixel position samples. Then a group variational transformation\ntechnique is used to transform a group of copied shared feature maps to samples\nat different sub-pixel positions. Experimental results have identified the\ninterpolation efficiency of our GVTCNN. Compared with the interpolation method\nof High Efficiency Video Coding, our method achieves 1.9% bit saving on average\nand up to 5.6% bit saving under low-delay P configuration.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07008v1"
    },
    {
        "title": "Occupancy-map-based rate distortion optimization for video-based point\n  cloud compression",
        "authors": [
            "Li Li",
            "Zhu Li",
            "Shan Liu",
            "Houqiang Li"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The state-of-the-art video-based point cloud compression scheme projects the\n3D point cloud to 2D patch by patch and organizes the patches into frames to\ncompress them using the efficient video compression scheme. Such a scheme shows\na good trade-off between the number of points projected and the video\ncontinuity to utilize the video compression scheme. However, some unoccupied\npixels between different patches are compressed using almost the same quality\nwith the occupied pixels, which will lead to the waste of lots of bits since\nthe unoccupied pixels are useless for the reconstructed point cloud. In this\npaper, we propose to consider only the rate instead of the rate distortion cost\nfor the unoccupied pixels during the rate distortion optimization process. The\nproposed scheme can be applied to both the geometry and attribute frames. The\nexperimental results show that the proposed algorithm can achieve an average of\n11.9% and 15.4% bitrate savings for the geometry and attribute, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.04169v1"
    },
    {
        "title": "A block-based inter-band predictor using multilayer propagation neural\n  network for hyperspectral image compression",
        "authors": [
            "Rui Dusselaar",
            "Manoranjan Paul"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this paper, a block-based inter-band predictor (BIP) with multilayer\npropagation neural network model (MLPNN) is presented by a completely new\nframework. This predictor can combine with diversity entropy coding methods.\nHyperspectral (HS) images are composed by a series high similarity spectral\nbands. Our assumption is to use trained MLPNN predict the succeeding bands\nbased on current band information. The purpose is to explore whether BIP-MLPNN\ncan provide better image predictive results with high efficiency. The algorithm\nalso changed from the traditional compression methods encoding images pixel by\npixel, the compression process only encodes the weights and the biases vectors\nof BIP-MLPNN which require few bits to transfer. The decoder will reconstruct a\nband by using the same structure of the network at the encoder side. The\nBIP-MLPNN decoder does not need to be trained as the weights and biases have\nalready been transmitted. We can easily reconstruct the succeeding bands by\nusing the BIP-MLPNN decoder. The experimental results indicate that BIP-MLPNN\npredictor outperforms the CCSDS-123 HS image coding standard. Due to a good\napproximation of the target band, the proposed method outperforms the CCSDS-123\nby more than 2.0dB PSNR image quality in the predicted bands. Moreover, the\nproposed method provides high quality image e.g., 30 to 40dB PSNR at very low\nbit rate (less than 0.1 bpppb) and outperforms the existing methods e.g., JPEG,\n3DSPECK, 3DSPIHT and in terms of rate-distortion performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.04191v1"
    },
    {
        "title": "Multi-task learning with compressible features for Collaborative\n  Intelligence",
        "authors": [
            "Saeed Ranjbar Alvar",
            "Ivan V. Bajić"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  A promising way to deploy Artificial Intelligence (AI)-based services on\nmobile devices is to run a part of the AI model (a deep neural network) on the\nmobile itself, and the rest in the cloud. This is sometimes referred to as\ncollaborative intelligence. In this framework, intermediate features from the\ndeep network need to be transmitted to the cloud for further processing. We\nstudy the case where such features are used for multiple purposes in the cloud\n(multi-tasking) and where they need to be compressible in order to allow\nefficient transmission to the cloud. To this end, we introduce a new loss\nfunction that encourages feature compressibility while improving system\nperformance on multiple tasks. Experimental results show that with the\ncompression-friendly loss, one can achieve around 20% bitrate reduction without\nsacrificing the performance on several vision-related tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.05179v2"
    },
    {
        "title": "Effectiveness of Crypto-Transcoding for H.264/AVC and HEVC Video\n  Bit-streams",
        "authors": [
            "Rizwan A. Shah",
            "Mamoona N. Asghar",
            "Saima Abdullah",
            "Martin Fleury",
            "Neelam Gohar"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  To avoid delays arising from a need to decrypt a video prior to transcoding\nand then re-encrypt it afterwards, this paper assesses a selective encryption\n(SE) content protection scheme. The scheme is suited to both recent\nstandardized codecs, namely H.264/Advanced Video Coding (AVC) and High\nEfficiency Video Coding (HEVC). Specifically, the paper outlines a joint\ncrypto-transcoding scheme for secure transrating of a video bitstream. That is\nto say it generates new video bitrates, possibly as part of an HTTP Adaptive\nStreaming (HAS) content delivery network. The scheme will reduce the bitrate to\none or more lower desired bit-rate without consuming time in the\nencryption/decryption process, which would be the case when full encryption is\nused. In addition, the decryption key no longer needs to be exposed at\nintermediate middleboxes, including when transrating is performed in a cloud\ndatacenter. The effectiveness of the scheme is variously evaluated: by\nexamination of the SE generated visual distortion; by the extent of\ncomputational and bitrate overheads; and by choice of cipher when encrypting\nthe selected elements within the bitstream. Results indicate that there\nremains: a content; quantization level (after transrating of an encrypted\nvideo); and codec-type dependency to any distortion introduced. A further\nrecommendation is that the Advanced Encryption Standard (AES) is preferred for\nSE to lightweight XOR encryption, despite it being taken up elsewhere as a\nreal-time encryption method.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.06990v1"
    },
    {
        "title": "Deep Learning-based Concept Detection in vitrivr at the Video Browser\n  Showdown 2019 - Final Notes",
        "authors": [
            "Luca Rossetto",
            "Mahnaz Amiri Parian",
            "Ralph Gasser",
            "Ivan Giangreco",
            "Silvan Heller",
            "Heiko Schuldt"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  This paper presents an after-the-fact summary of the participation of the\nvitrivr system to the 2019 Video Browser Showdown. Analogously to last year's\nreport, the focus of this paper lies on additions made since the original\npublication and the system's performance during the competition.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10647v1"
    },
    {
        "title": "Game-theoretic Analysis to Content-adaptive Reversible Watermarking",
        "authors": [
            "Hanzhou Wu",
            "Xinpeng Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  While many games were designed for steganography and robust watermarking, few\nfocused on reversible watermarking. We present a two-encoder game related to\nthe rate-distortion optimization of content-adaptive reversible watermarking.\nIn the game, Alice first hides a payload into a cover. Then, Bob hides another\npayload into the modified cover. The embedding strategy of Alice affects the\nembedding capacity of Bob. The embedding strategy of Bob may produce\ndata-extraction errors to Alice. Both want to embed as many pure secret bits as\npossible, subjected to an upper-bounded distortion. We investigate\nnon-cooperative game and cooperative game between Alice and Bob. When they\ncooperate with each other, one may consider them as a whole, i.e., an encoder\nuses a cover for data embedding with two times. When they do not cooperate with\neach other, the game corresponds to a separable system, i.e., both want to\nindependently hide a payload within the cover, but recovering the cover may\nneed cooperation. We find equilibrium strategies for both players under\nconstraints.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.01674v1"
    },
    {
        "title": "A multimodal lossless coding method for skeletons in videos",
        "authors": [
            "Mingzhou Liu",
            "Xiaoyi He",
            "Weiyao Lin",
            "Xintong Han",
            "Yanmin Zhu",
            "Hongtao Lu",
            "Hongkai Xiong"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Nowadays, skeleton information in videos plays an important role in\nhuman-centric video analysis but effective coding such massive skeleton\ninformation has never been addressed in previous work. In this paper, we make\nthe first attempt to solve this problem by proposing a multimodal skeleton\ncoding tool containing three different coding schemes, namely, spatial\ndifferential-coding scheme, motionvector-based differential-coding scheme and\ninter prediction scheme, thus utilizing both spatial and temporal redundancy to\nlosslessly compress skeleton data. More importantly, these schemes are switched\nproperly for different types of skeletons in video frames, hence achieving\nfurther improvement of compression rate. Experimental results show that our\napproach leads to 74.4% and 54.7% size reduction on our surveillance sequences\nand overall test sequences respectively, which demonstrates the effectiveness\nof our skeleton coding tool.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.01790v2"
    },
    {
        "title": "Methodology for accurately assessing the quality perceived by users on\n  360VR contents",
        "authors": [
            "Lara Muñoz",
            "César Díaz",
            "Marta Orduna",
            "José Ignacio Ronda",
            "Pablo Pérez",
            "Ignacio Benito",
            "Narciso García"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  To properly evaluate the performance of 360VR-specific encoding and\ntransmission schemes, and particularly of the solutions based on viewport\nadaptation, it is necessary to consider not only the bandwidth saved, but also\nthe quality of the portion of the scene actually seen by users over time. With\nthis motivation, we propose a robust, yet flexible methodology for accurately\nassessing the quality within the viewport along the visualization session. This\nprocedure is based on a complete analysis of the geometric relations involved.\nMoreover, the designed methodology allows for both offline and online usage\nthanks to the use of different approximations. In this way, our methodology can\nbe used regardless of the approach to properly evaluate the implemented\nstrategy, obtaining a fairer comparison between them.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03508v1"
    },
    {
        "title": "Reversible Data Hiding in JPEG Images with Multi-objective Optimization",
        "authors": [
            "Zhaoxia Yin",
            "Yuan Ji",
            "Bin Luo"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Among various methods of reversible data hiding (RDH) in JPEG images, the\nconsideration in designing is only the image quality, but the image quality and\nthe file size expansion are equally important in JPEG images. Based on this\nsituation, we propose a RDH scheme in JPEG images considering both the image\nquality and the file size expansion while designing the algorithm. The\nmulti-objective optimization strategy is utilized to realize the balance of the\ntwo objectives. Specifically, the cover is divided into several non-overlapping\nsignals firstly, and after that, the embedding costs of signals are calculated\nusing the knowledge of the JPEG compression. Next, the optimized combination of\nsignals for embedding data is gained by the multi-objective optimization.\nExperimental results show the better performance of our proposed RDH compared\nwith state-of-the-art RDH in JPEG images.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03533v1"
    },
    {
        "title": "A Taxonomy and Dataset for 360° Videos",
        "authors": [
            "Afshin Taghavi Nasrabadi",
            "Aliehsan Samiei",
            "Anahita Mahzari",
            "Mylene C. Q. Farias",
            "Marcelo M. Carvalho",
            "Ryan P. McMahan",
            "Ravi Prakash"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this paper, we propose a taxonomy for 360{\\deg} videos that categorizes\nvideos based on moving objects and camera motion. We gathered and produced 28\nvideos based on the taxonomy, and recorded viewport traces from 60 participants\nwatching the videos. In addition to the viewport traces, we provide the\nviewers' feedback on their experience watching the videos, and we also analyze\nviewport patterns on each category.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03823v1"
    },
    {
        "title": "Reversible data hiding based on reducing invalid shifting of pixels in\n  histogram shifting",
        "authors": [
            "Yujie Jia",
            "Zhaoxia Yin",
            "Xinpeng Zhang",
            "Yonglong Luo"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In recent years, reversible data hiding (RDH), a new research hotspot in the\nfield of information security, has been paid more and more attention by\nresearchers. Most of the existing RDH schemes do not fully take it into account\nthat natural image's texture has influence on embedding distortion. The image\ndistortion caused by embedding data in the image's smooth region is much\nsmaller than that in the unsmooth region, essentially, it is because embedding\nadditional data in the smooth region corresponds to fewer invalid shifting\npixels (ISPs) in histogram shifting. Thus, we propose a RDH scheme based on the\nimages texture to reduce invalid shifting of pixels in histogram shifting.\nSpecifically, first, a cover image is divided into two sub-images by the\ncheckerboard pattern, and then each sub-image's fluctuation values are\ncalculated. Finally, additional data can be embedded into the region of\nsub-images with smaller fluctuation value preferentially. The experimental\nresults demonstrate that the proposed method has higher capacity and better\nstego-image quality than some existing RDH schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.05365v1"
    },
    {
        "title": "High Capacity Lossless Data Hiding in JPEG Bitstream Based on General\n  VLC Mapping",
        "authors": [
            "Yang Du",
            "Zhaoxia Yin",
            "Xinpeng Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  JPEG is the most popular image format, which is widely used in our daily\nlife. Therefore, reversible data hiding (RDH) for JPEG images is important.\nMost of the RDH schemes for JPEG images will cause significant distortions and\nlarge file size increments in the marked JPEG image. As a special case of RDH,\nthe lossless data hiding (LDH) technique can keep the visual quality of the\nmarked images no degradation. In this paper, a novel high capacity LDH scheme\nis proposed. In the JPEG bitstream, not all the variable length codes (VLC) are\nused to encode image data. By constructing the mapping between the used and\nunused VLCs, the secret data can be embedded by replacing the used VLC with the\nunused VLC. Different from the previous schemes, our mapping strategy allows\nthe lengths of unused and used VLCs in a mapping set to be unequal. We present\nsome basic insights into the construction of the mapping relationship.\nExperimental results show that most of the JPEG images using the proposed\nscheme obtain smaller file size increments than previous RDH schemes.\nFurthermore, the proposed scheme can obtain high embedding capacity while\nkeeping the marked JPEG image with no distortion.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.05627v2"
    },
    {
        "title": "EVSO: Environment-aware Video Streaming Optimization of Power\n  Consumption",
        "authors": [
            "Kyoungjun Park",
            "Myungchul Kim"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Streaming services gradually support high-quality videos for better user\nexperience. However, streaming high-quality video on mobile devices consumes a\nconsiderable amount of energy. This paper presents the design and prototype of\nEVSO, which achieves power saving by applying adaptive frame rates to parts of\nvideos with a little degradation of the user experience. EVSO utilizes a novel\nperceptual similarity measurement method based on human visual perception\nspecialized for a video encoder. We also extend the media presentation\ndescription, in which the video content is selected based only on the network\nbandwidth, to allow for additional consideration of the user's battery status.\nEVSO's streaming server preprocesses the video into several processed videos\naccording to the similarity intensity of each part of the video and then\nprovides the client with the processed video suitable for the network bandwidth\nand the battery status of the client's mobile device. The EVSO system was\nimplemented on the commonly used H.264/AVC encoder. We conduct various\nexperiments and a user study with nine videos. Our experimental results show\nthat EVSO effectively reduces energy consumption when mobile devices use\nstreaming services by 22% on average and up to 27% while maintaining the\nquality of the user experience.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.06500v1"
    },
    {
        "title": "An Improved Reversible Data Hiding in Encrypted Images using Parametric\n  Binary Tree Labeling",
        "authors": [
            "Youqing Wu",
            "Youzhi Xiang",
            "Yutang Guo",
            "Jin Tang",
            "Zhaoxia Yin"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  This work proposes an improved reversible data hiding scheme in encrypted\nimages using parametric binary tree labeling(IPBTL-RDHEI), which takes\nadvantage of the spatial correlation in the entire original image but not in\nsmall image blocks to reserve room for hiding data. Then the original image is\nencrypted with an encryption key and the parametric binary tree is used to\nlabel encrypted pixels into two different categories. Finally, one of the two\ncategories of encrypted pixels can embed secret information by bit replacement.\nAccording to the experimental results, compared with several state-of-the-art\nmethods, the proposed IPBTL-RDHEI method achieves higher embedding rate and\noutperforms the competitors. Due to the reversibility of IPBTL-RDHEI, the\noriginal plaintext image and the secret information can be restored and\nextracted losslessly and separately.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.09625v2"
    },
    {
        "title": "Automatic Realistic Music Video Generation from Segments of Youtube\n  Videos",
        "authors": [
            "Sarah Gross",
            "Xingxing Wei",
            "Jun Zhu"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  A Music Video (MV) is a video aiming at visually illustrating or extending\nthe meaning of its background music. This paper proposes a novel method to\nautomatically generate, from an input music track, a music video made of\nsegments of Youtube music videos which would fit this music. The system\nanalyzes the input music to find its genre (pop, rock, ...) and finds segmented\nMVs with the same genre in the database. Then, a K-Means clustering is done to\ngroup video segments by color histogram, meaning segments of MVs having the\nsame global distribution of colors. A few clusters are randomly selected, then\nare assembled around music boundaries, which are moments where a significant\nchange in the music occurs (for instance, transitioning from verse to chorus).\nThis way, when the music changes, the video color mood changes as well. This\nwork aims at generating high-quality realistic MVs, which could be mistaken for\nman-made MVs. By asking users to identify, in a batch of music videos\ncontaining professional MVs, amateur-made MVs and generated MVs by our\nalgorithm, we show that our algorithm gives satisfying results, as 45% of\ngenerated videos are mistaken for professional MVs and 21.6% are mistaken for\namateur-made MVs. More information can be found in the project website:\nhttp://ml.cs.tsinghua.edu.cn/~sarah/\n",
        "pdf_link": "http://arxiv.org/pdf/1905.12245v1"
    },
    {
        "title": "Universal Stego Post-processing for Enhancing Image Steganography",
        "authors": [
            "Bolin Chen",
            "Weiqi Luo",
            "Peijia Zheng",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  It is well known that the designing or improving embedding cost becomes a key\nissue for current steganographic methods. Unlike existing works, we propose a\nnovel framework to enhance the steganography security via post-processing on\nthe embedding units (i.e., pixel values and DCT coefficients) of stego\ndirectly. In this paper, we firstly analyze the characteristics of STCs\n(Syndrome-Trellis Codes), and then design the rule for post-processing to\nensure the correct extraction of hidden message. Since the steganography\nartifacts are typically reflected on image residuals, we try to reduce the\nresidual distance between cover and the modified stego in order to enhance\nsteganography security. To this end, we model the post-processing as a\nnon-linear integer programming, and implement it via heuristic search. In\naddition, we carefully determine several important issues in the proposed\npost-processing, such as the candidate embedding units to be modified, the\ndirection and amplitude of post-modification, the adaptive filters for getting\nresiduals, and the distance measure of residuals. Extensive experimental\nresults evaluated on both hand-crafted steganalytic features and deep learning\nbased ones demonstrate that the proposed method can effectively enhance the\nsecurity of most modern steganographic methods both in spatial and JPEG\ndomains.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.03878v2"
    },
    {
        "title": "An Efficient Coding Method for Spike Camera using Inter-Spike Intervals",
        "authors": [
            "Siwei Dong",
            "Lin Zhu",
            "Daoyuan Xu",
            "Yonghong Tian",
            "Tiejun Huang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Recently, a novel bio-inspired spike camera has been proposed, which\ncontinuously accumulates luminance intensity and fires spikes while the\ndispatch threshold is reached. Compared to the conventional frame-based cameras\nand the emerging dynamic vision sensors, the spike camera has shown great\nadvantages in capturing fast-moving scene in a frame-free manner with full\ntexture reconstruction capabilities. However, it is difficult to transmit or\nstore the large amount of spike data. To address this problem, we first\ninvestigate the spatiotemporal distribution of inter-spike intervals and\npropose an intensity-based measurement of spike train distance. Then, we design\nan efficient spike coding method, which integrates the techniques of adaptive\ntemporal partitioning, intra-/inter-pixel prediction, quantization and entropy\ncoding into a unified lossy coding framework. Finally, we construct a PKU-Spike\ndataset captured by the spike camera to evaluate the compression performance.\nThe experimental results on the dataset demonstrate that the proposed approach\nis effective in compressing such spike data while maintaining the fidelity.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.09669v1"
    },
    {
        "title": "JPEG Image Compression using the Discrete Cosine Transform: An Overview,\n  Applications, and Hardware Implementation",
        "authors": [
            "Ahmad Shawahna",
            "Md. Enamul Haque",
            "Alaaeldin Amin"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Digital images are becoming large in size containing more information day by\nday to represent the as is state of the original one due to the availability of\nhigh resolution digital cameras, smartphones, and medical tests images.\nTherefore, we need to come up with some technique to convert these images into\nsmaller size without loosing much information from the actual. There are both\nlossy and lossless image compression format available and JPEG is one of the\npopular lossy compression among them. In this paper, we present the\narchitecture and implementation of JPEG compression using VHDL (VHSIC Hardware\nDescription Language) and compare the performance with some contemporary\nimplementation. JPEG compression takes place in five steps with color space\nconversion, down sampling, discrete cosine transformation (DCT), quantization,\nand entropy encoding. The five steps cover for the compression purpose only.\nAdditionally, we implement the reverse order in VHDL to get the original image\nback. We use optimized matrix multiplication and quantization for DCT to\nachieve better performance. Our experimental results show that significant\namount of compression ratio has been achieved with very little change in the\nimages, which is barely noticeable to human eye.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10789v1"
    },
    {
        "title": "Quality of Experience for Streaming Services: Measurements, Challenges\n  and Insights",
        "authors": [
            "Khadija Bouraqia",
            "Essaid Sabir",
            "Mohamed Sadik",
            "Latif Ladid"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Over the last few years, the evolution of network and user handsets'\ntechnologies, have challenged the telecom industry and the Internet ecosystem.\nEspecially, the unprecedented progress of multimedia streaming services like\nYouTube, Vimeo and DailyMotion resulted in an impressive demand growth and a\nsignificant need of Quality of Service (QoS) (e.g., high data rate, low\nlatency/jitter, etc.). Mainly, numerous difficulties are to be considered while\ndelivering a specific service, such as a strict QoS, human-centric features,\nmassive number of devices, heterogeneous devices and networks, and\nuncontrollable environments. Thenceforth, the concept of Quality of Experience\n(QoE) is gaining visibility, and tremendous research efforts have been spent on\nimproving and/or delivering reliable and addedvalue services, at a high user\nexperience. In this paper, we present the importance of QoE in wireless and\nmobile networks (4G, 5G, and beyond), by providing standard definitions and the\nmost important measurement methods developed. Moreover, we exhibit notable\nenhancements and controlling approaches proposed by researchers to meet the\nuser expectation in terms of service experience.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11318v1"
    },
    {
        "title": "Text Steganalysis with Attentional LSTM-CNN",
        "authors": [
            "YongJian Bao",
            "Hao Yang",
            "Zhongliang Yang",
            "Sheng Liu",
            "Yongfeng Huang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  With the rapid development of Natural Language Processing (NLP) technologies,\ntext steganography methods have been significantly innovated recently, which\nposes a great threat to cybersecurity. In this paper, we propose a novel\nattentional LSTM-CNN model to tackle the text steganalysis problem. The\nproposed method firstly maps words into semantic space for better exploitation\nof the semantic feature in texts and then utilizes a combination of\nConvolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM)\nrecurrent neural networks to capture both local and long-distance contextual\ninformation in steganography texts. In addition, we apply attention mechanism\nto recognize and attend to important clues within suspicious sentences. After\nmerge feature clues from Convolutional Neural Networks (CNNs) and Recurrent\nNeural Networks (RNNs), we use a softmax layer to categorize the input text as\ncover or stego. Experiments showed that our model can achieve the state-of-art\nresult in the text steganalysis task.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12871v2"
    },
    {
        "title": "Joint Communication and Computational Resource Allocation for QoE-driven\n  Point Cloud Video Streaming",
        "authors": [
            "Jie Li",
            "Cong Zhang",
            "Zhi Liu",
            "Wei Sun",
            "Qiyue Li"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Point cloud video is the most popular representation of hologram, which is\nthe medium to precedent natural content in VR/AR/MR and is expected to be the\nnext generation video. Point cloud video system provides users immersive\nviewing experience with six degrees of freedom and has wide applications in\nmany fields such as online education, entertainment. To further enhance these\napplications, point cloud video streaming is in critical demand. The inherent\nchallenges lie in the large size by the necessity of recording the\nthree-dimensional coordinates besides color information, and the associated\nhigh computation complexity of encoding. To this end, this paper proposes a\ncommunication and computation resource allocation scheme for QoE-driven point\ncloud video streaming. In particular, we maximize system resource utilization\nby selecting different quantities, transmission forms and quality level tiles\nto maximize the quality of experience. Extensive simulations are conducted and\nthe simulation results show the superior performance over the existing schemes\n",
        "pdf_link": "http://arxiv.org/pdf/2001.01403v2"
    },
    {
        "title": "QoE-driven Coupled Uplink and Downlink Rate Adaptation for 360-degree\n  Video Live Streaming",
        "authors": [
            "Jie Li",
            "Ransheng Feng",
            "Zhi Liu",
            "Wei Sun",
            "Qiyue Li"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  360-degree video provides an immersive 360-degree viewing experience and has\nbeen widely used in many areas. The 360-degree video live streaming systems\ninvolve capturing, compression, uplink (camera to video server) and downlink\n(video server to user) transmissions. However, few studies have jointly\ninvestigated such complex systems, especially the rate adaptation for the\ncoupled uplink and downlink in the 360-degree video streaming under limited\nbandwidth constraints. In this letter, we propose a quality of experience\n(QoE)-driven 360-degree video live streaming system, in which a video server\nperforms rate adaptation based on the uplink and downlink bandwidths and\ninformation concerning each user's real-time field-of-view (FOV). We formulate\nit as a nonlinear integer programming problem and propose an algorithm, which\ncombines the Karush-Kuhn-Tucker (KKT) condition and branch and bound method, to\nsolve it. The numerical results show that the proposed optimization model can\nimprove users' QoE significantly in comparison with other baseline schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.03536v1"
    },
    {
        "title": "Evaluation of a course mediatised with Xerte",
        "authors": [
            "Ghalia Merzougui",
            "Roumaissa Dehkal",
            "Maheiddine Djoudi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Interactive multimedia educational content has recently been of interest to\nattract attention on the learner and increase understanding by the latter. In\nparallel several open source authoring tools offer a quick and easy production\nof this type of content. As such, our contribution is to mediatize a course\ni.e. 'English' with the authoring system 'Xerte' which is intended both for\nsimple users and developers in ActionScript. An experiment of course is\nconducted on a sample of a private school's students. At the end of this\nexperience, we administered a questionnaire to evaluate the device, the results\nobtained, evidenced by the favorable reception of interactive multimedia\nintegration in educational content.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.07494v1"
    },
    {
        "title": "Accessibility in 360-degree video players",
        "authors": [
            "Chris Hughes",
            "Mario Montagud"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Any media experience must be fully inclusive and accessible to all users\nregardless of their ability. With the current trend towards immersive\nexperiences, such as Virtual Reality (VR) and 360-degree video, it becomes key\nthat these environments are adapted to be fully accessible. However, until\nrecently the focus has been mostly on adapting the existing techniques to fit\nimmersive displays, rather than considering new approaches for accessibility\ndesigned specifically for these increasingly relevant media experiences. This\npaper surveys a wide range of 360-degree video players and examines the\nfeatures they include for dealing with accessibility, such as Subtitles, Audio\nDescription, Sign Language, User Interfaces, and other interaction features,\nlike voice control and support for multi-screen scenarios. These features have\nbeen chosen based on guidelines from standardization contributions, like in the\nWorld Wide Web Consortium (W3C) and the International Communication Union\n(ITU), and from research contributions for making 360-degree video consumption\nexperiences accessible. The in-depth analysis has been part of a research\neffort towards the development of a fully inclusive and accessible 360-degree\nvideo player. The paper concludes by discussing how the newly developed player\nhas gone above and beyond the existing solutions and guidelines, by providing\naccessibility features that meet the expectations for a widely used immersive\nmedium, like 360-degree video.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03373v1"
    },
    {
        "title": "White Paper: Recommendations for immersive accessibility services",
        "authors": [
            "Peter tho Pesch",
            "Romain Bouqueau",
            "Mario Montagud"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper provides recommendations on how to integrate accessibility\nsolutions, like subtitling, audio description and sign language, with immersive\nmedia services, with a focus on 360-degree video and spatial audio. It provides\nan in-depth analysis of the features provided by state-of-the-art standard\nsolutions to achieve this goal, and elaborates on the finding and proposed\nsolutions from the EU H2020 ImAc project to address existing gaps. The proposed\nsolutions are described qualitatively and technically, including example\nimplementations. The document is intended to serve as a valuable information\nsource for early adopters who plan to provide accessibility services to their\nportfolio with standard-compliant solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03756v1"
    },
    {
        "title": "Hardware Implementation of Adaptive Watermarking Based on Local Spatial\n  Disorder Analysis",
        "authors": [
            "Mohsen Hajabdolahi",
            "Nader Karimi",
            "Shahram Shirani",
            "Shadrokh Samavi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  With the increasing use of the internet and the ease of exchange of\nmultimedia content, the protection of ownership rights has become a significant\nconcern. Watermarking is an efficient means for this purpose. In many\napplications, real-time watermarking is required, which demands hardware\nimplementation of low complexity and robust algorithm. In this paper, an\nadaptive watermarking is presented, which uses embedding in different\nbit-planes to achieve transparency and robustness. Local disorder of pixels is\nanalyzed to control the strength of the watermark. A new low complexity method\nfor disorder analysis is proposed, and its hardware implantation is presented.\nAn embedding method is proposed, which causes lower degradation in the\nwatermarked image. Also, the performance of proposed watermarking architecture\nis improved by a pipe-line structure and is tested on an FPGA device. Results\nshow that the algorithm produces transparent and robust watermarked images. The\nsynthesis report from FPGA implementation illustrates a low complexity hardware\nstructure.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.05319v1"
    },
    {
        "title": "The Hyper360 toolset for enriched 360$^\\circ$ video",
        "authors": [
            "Hannes Fassold",
            "Antonis Karakottas",
            "Dorothea Tsatsou",
            "Dimitrios Zarpalas",
            "Barnabas Takacs",
            "Christian Fuhrhop",
            "Angelo Manfredi",
            "Nicolas Patz",
            "Simona Tonoli",
            "Iana Dulskaia"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  360$^\\circ$ video is a novel media format, rapidly becoming adopted in media\nproduction and consumption as part of todays ongoing virtual reality\nrevolution. Due to its novelty, there is a lack of tools for producing highly\nengaging 360$^\\circ$ video for consumption on a multitude of platforms. In this\nwork, we describe the work done so far in the Hyper360 project on tools for\n360$^\\circ$ video. Furthermore, the first pilots which have been produced with\nthe Hyper360 tools are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.06944v1"
    },
    {
        "title": "Towards 5G: Joint Optimization of Video Segment Cache, Transcoding and\n  Resource Allocation for Adaptive Video Streaming in a Muti-access Edge\n  Computing Network",
        "authors": [
            "Xinyu Huang",
            "Lijun He",
            "Liejun Wang",
            "Fan Li"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The cache and transcoding of the multi-access edge computing (MEC) server and\nwireless resource allocation in eNodeB interact and determine the quality of\nexperience (QoE) of dynamic adaptive streaming over HTTP (DASH) clients in MEC\nnetworks. However, the relationship among the three factors has not been\nexplored, which has led to limited improvement in clients' QoE. Therefore, we\npropose a joint optimization framework of video segment cache and transcoding\nin MEC servers and resource allocation to improve the QoE of DASH clients.\nBased on the established framework, we develop a MEC cache management mechanism\nthat consists of the MEC cache partition, video segment deletion, and MEC cache\nspace transfer. Then, a joint optimization algorithm that combines video\nsegment cache and transcoding in the MEC server and resource allocation is\nproposed. In the algorithm, the clients' channel state and the playback status\nand cooperation among MEC servers are employed to estimate the client's\npriority, video segment presentation switch and continuous playback time.\nConsidering the above four factors, we develop a utility function model of\nclients' QoE. Then, we formulate a mixed-integer nonlinear programming\nmathematical model to maximize the total utility of DASH clients, where the\nvideo segment cache and transcoding strategy and resource allocation strategy\nare jointly optimized. To solve this problem, we propose a low-complexity\nheuristic algorithm that decomposes the original problem into multiple\nsubproblems. The simulation results show that our proposed algorithms\nefficiently improve client's throughput, received video quality and hit ratio\nof video segments while decreasing the playback rebuffering time, video segment\npresentation switch and system backhaul traffic.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.07384v1"
    },
    {
        "title": "Spatiotemporal Adaptive Quantization for Video Compression Applications",
        "authors": [
            "Lee Prangnell"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  JCT-VC HEVC HM 16 includes a Coding Unit (CU) level adaptive Quantization\nParameter (QP) technique named AdaptiveQP. It is designed to perceptually\nadjust the QP in Y, Cb and Cr Coding Blocks (CBs) based only on the variance of\nsamples in a luma CB. In this paper, we propose an adaptive quantisation\ntechnique that consists of two contributions. The first contribution relates to\naccounting for the variance of chroma samples, in addition to luma samples, in\na CU. The second contribution relates to accounting for CU temporal information\nas well as CU spatial information. Moreover, we integrate into our method a\nlambda refined QP technique to reduce complexity associated multiple QP\noptimizations in the Rate Distortion Optimization process. We evaluate the\nproposed technique on 4:4:4, 4:2:2, 4:2:0 and 4:0:0 YCbCr test sequences, for\nwhich we quantify the results using the Bjontegaard Delta Rate (BD-Rate)\nmetric. Our method achieves a maximum BD-Rate reduction of 23.1% (Y), 26.7%\n(Cr) and 25.2% (Cb). Furthermore, a maximum encoding time reduction of 4.4% is\nachieved.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.07925v1"
    },
    {
        "title": "Towards a Perceived Audiovisual Quality Model for Immersive Content",
        "authors": [
            "Randy Frans Fela",
            "Nick Zacharov",
            "Søren Forchhammer"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper studies the quality of multimedia content focusing on 360 video\nand ambisonic spatial audio reproduced using a head-mounted display and a\nmultichannel loudspeaker setup. Encoding parameters following basic video\nquality test conditions for 360 videos were selected and a low-bitrate codec\nwas used for the audio encoder. Three subjective experiments were performed for\nthe audio, video, and audiovisual respectively. Peak signal-to-noise ratio\n(PSNR) and its variants for 360 videos were computed to obtain objective\nquality metrics and subsequently correlated with the subjective video scores.\nThis study shows that a Cross-Format SPSNR-NN has a slightly higher linear and\nmonotonic correlation over all video sequences. Based on the audiovisual model,\na power model shows a highest correlation between test data and predicted\nscores. We concluded that to enable the development of superior predictive\nmodel, a high quality, critical, synchronized audiovisual database is required.\nFurthermore, comprehensive assessor training may be beneficial prior to the\ntesting to improve the assessors' discrimination ability particularly with\nrespect to multichannel audio reproduction. In order to further improve the\nperformance of audiovisual quality models for immersive content, in addition to\ndeveloping broader and critical audiovisual databases, the subjective testing\nmethodology needs to be evolved to provide greater resolution and robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09309v1"
    },
    {
        "title": "Self-play Reinforcement Learning for Video Transmission",
        "authors": [
            "Tianchi Huang",
            "Rui-Xiao Zhang",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Video transmission services adopt adaptive algorithms to ensure users'\ndemands. Existing techniques are often optimized and evaluated by a function\nthat linearly combines several weighted metrics. Nevertheless, we observe that\nthe given function fails to describe the requirement accurately. Thus, such\nproposed methods might eventually violate the original needs. To eliminate this\nconcern, we propose \\emph{Zwei}, a self-play reinforcement learning algorithm\nfor video transmission tasks. Zwei aims to update the policy by\nstraightforwardly utilizing the actual requirement. Technically, Zwei samples a\nnumber of trajectories from the same starting point and instantly estimates the\nwin rate w.r.t the competition outcome. Here the competition result represents\nwhich trajectory is closer to the assigned requirement. Subsequently, Zwei\noptimizes the strategy by maximizing the win rate. To build Zwei, we develop\nsimulation environments, design adequate neural network models, and invent\ntraining methods for dealing with different requirements on various video\ntransmission scenarios. Trace-driven analysis over two representative tasks\ndemonstrates that Zwei optimizes itself according to the assigned requirement\nfaithfully, outperforming the state-of-the-art methods under all considered\nscenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.12788v1"
    },
    {
        "title": "Investigating Correlations of Automatically Extracted Multimodal\n  Features and Lecture Video Quality",
        "authors": [
            "Jianwei Shi",
            "Christian Otto",
            "Anett Hoppe",
            "Peter Holtz",
            "Ralph Ewerth"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Ranking and recommendation of multimedia content such as videos is usually\nrealized with respect to the relevance to a user query. However, for lecture\nvideos and MOOCs (Massive Open Online Courses) it is not only required to\nretrieve relevant videos, but particularly to find lecture videos of high\nquality that facilitate learning, for instance, independent of the video's or\nspeaker's popularity. Thus, metadata about a lecture video's quality are\ncrucial features for learning contexts, e.g., lecture video recommendation in\nsearch as learning scenarios. In this paper, we investigate whether\nautomatically extracted features are correlated to quality aspects of a video.\nA set of scholarly videos from a Mass Open Online Course (MOOC) is analyzed\nregarding audio, linguistic, and visual features. Furthermore, a set of\ncross-modal features is proposed which are derived by combining transcripts,\naudio, video, and slide content. A user study is conducted to investigate the\ncorrelations between the automatically collected features and human ratings of\nquality aspects of a lecture video. Finally, the impact of our features on the\nknowledge gain of the participants is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.13876v1"
    },
    {
        "title": "Augmenting reality: On the shared history of perceptual illusion and\n  video projection mapping",
        "authors": [
            "Alvaro Pastor"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Perceptual illusions based on the spatial correspondence between objects and\ndisplayed images have been pursued by artists and scientists since the 15th\ncentury, mastering optics to create crucial techniques as the linear\nperspective and devices as the Magic Lantern. Contemporary video projection\nmapping inherits and further extends this drive to produce perceptual illusions\nin space by incorporating the required real time capabilities for dynamically\nsuperposing the imaginary onto physical objects under fluid real world\nconditions. A critical milestone has been reached in the creation of the\ntechnical possibilities for all encompassing, untethered synthetic reality\nexperiences available to the plain senses, where every surface may act as a\nscreen and the relation to everyday objects is open to alterations.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.14317v1"
    },
    {
        "title": "Transmission of Successful Route Error Message(RERR) in Routing Aware\n  Multiple Description Video Coding over Mobile Ad-Hoc Network",
        "authors": [
            "Kinjal Shah",
            "Gagan Dua",
            "Dharmendar Sharma",
            "Priyanka Mishra",
            "Nitin Rakesh"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Video transmission over mobile ad-hoc networks is becoming important as these\nnetworks become more widely used in the wireless networks. We propose a\nrouting-aware multiple description video coding approach to support video\ntransmission over mobile ad-hoc networks with single and multiple path\ntransport. We build a model to estimate the packet loss probability of each\npacket transmitted over the network based on the standard ad-hoc routing\nmessages and network parameters without losing the RERR message. We then\ncalculate the frame loss probability in order to eliminate error without any\nloss of data.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.0753v1"
    },
    {
        "title": "A Survey on Web Multimedia Mining",
        "authors": [
            "Pravin M. Kamde",
            "Dr. Siddu. P. Algur"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Modern developments in digital media technologies has made transmitting and\nstoring large amounts of multi/rich media data (e.g. text, images, music, video\nand their combination) more feasible and affordable than ever before. However,\nthe state of the art techniques to process, mining and manage those rich media\nare still in their infancy. Advances developments in multimedia acquisition and\nstorage technology the rapid progress has led to the fast growing incredible\namount of data stored in databases. Useful information to users can be revealed\nif these multimedia files are analyzed. Multimedia mining deals with the\nextraction of implicit knowledge, multimedia data relationships, or other\npatterns not explicitly stored in multimedia files. Also in retrieval, indexing\nand classification of multimedia data with efficient information fusion of the\ndifferent modalities is essential for the system's overall performance. The\npurpose of this paper is to provide a systematic overview of multimedia mining.\nThis article is also represents the issues in the application process component\nfor multimedia mining followed by the multimedia mining models.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.1145v1"
    },
    {
        "title": "Secured color image watermarking technique in DWT-DCT domain",
        "authors": [
            "Baisa L. Gunjal",
            "Suresh N. Mali"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  The multilayer secured DWT-DCT and YIQ color space based image watermarking\ntechnique with robustness and better correlation is presented here. The\nsecurity levels are increased by using multiple pn sequences, Arnold\nscrambling, DWT domain, DCT domain and color space conversions. Peak signal to\nnoise ratio and Normalized correlations are used as measurement metrics. The\n512x512 sized color images with different histograms are used for testing and\nwatermark of size 64x64 is embedded in HL region of DWT and 4x4 DCT is used.\n'Haar' wavelet is used for decomposition and direct flexing factor is used. We\ngot PSNR value is 63.9988 for flexing factor k=1 for Lena image and the maximum\nNC 0.9781 for flexing factor k=4 in Q color space. The comparative performance\nin Y, I and Q color space is presented. The technique is robust for different\nattacks like scaling, compression, rotation etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.2325v1"
    },
    {
        "title": "A Markov Decision Model for Adaptive Scheduling of Stored Scalable\n  Videos",
        "authors": [
            "Chao Chen",
            "Robert W. Heath Jr",
            "Alan C. Bovik",
            "Gustavo de Veciana"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  We propose two scheduling algorithms that seek to optimize the quality of\nscalably coded videos that have been stored at a video server before\ntransmission.} The first scheduling algorithm is derived from a Markov Decision\nProcess (MDP) formulation developed here. We model the dynamics of the channel\nas a Markov chain and reduce the problem of dynamic video scheduling to a\ntractable Markov decision problem over a finite state space. Based on the MDP\nformulation, a near-optimal scheduling policy is computed that minimize the\nmean square error. Using insights taken from the development of the optimal\nMDP-based scheduling policy, the second proposed scheduling algorithm is an\nonline scheduling method that only requires easily measurable knowledge of the\nchannel dynamics, and is thus viable in practice. Simulation results show that\nthe performance of both scheduling algorithms is close to a performance upper\nbound also derived in this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.2067v2"
    },
    {
        "title": "Surveying the Social, Smart and Converged TV Landscape: Where is\n  Television Research Headed?",
        "authors": [
            "Marie-Jose Montpetit",
            "Pablo Cesar",
            "Maja Matijasevic",
            "Zhu Liu",
            "John Crowcroft",
            "Oscar Martinez-Bonastre"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  The TV is dead motto of just a few years ago has been replaced by the\nprospect of Internet Protocol (IP) television experiences over converged\nnetworks to become one of the great technology opportunities in the next few\nyears. As an introduction to the Special Issue on Smart, Social and Converged\nTelevision, this extended editorial intends to review the current IP television\nlandscape in its many realizations: operator-based, over-the-top, and user\ngenerated. We will address new services like social TV and recommendation\nengines, dissemination including new paradigms built on peer to peer and\ncontent centric networks, as well as the all important quality of experience\nthat challenges services and networks alike. But we intend to go further than\njust review the existing work by proposing areas for the future of television\nresearch. These include strategies to provide services that are more efficient\nin network and energy usage while being socially engaging, novel services that\nwill provide consumers with a broader choice of content and devices, and\nmetrics that will enable operators and users alike to define the level of\nservice they require or that they are ready to provide. These topics are\naddressed in this survey paper that attempts to create a unifying framework to\nlink them all together. Not only is television not dead, it is well alive,\nthriving and fostering innovation and this paper will hopefully prove it.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.2905v1"
    },
    {
        "title": "A new chaos-based watermarking algorithm",
        "authors": [
            "Jacques M. Bahi",
            "Christophe Guyeux"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This paper introduces a new watermarking algorithm based on discrete chaotic\niterations. After defining some coefficients deduced from the description of\nthe carrier medium, chaotic discrete iterations are used to mix the watermark\nand to embed it in the carrier medium. It can be proved that this procedure\ngenerates topological chaos, which ensures that desired properties of a\nwatermarking algorithm are satisfied.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.00118v1"
    },
    {
        "title": "A Novel Adaptation Method for HTTP Streaming of VBR Videos over Mobile\n  Networks",
        "authors": [
            "Hung. T Le",
            "Hai N. Nguyen",
            "Nam Pham Ngoc",
            "Anh T. Pham",
            "Truong Cong Thang"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Recently, HTTP streaming has become very popular for delivering video over\nthe Internet. For adaptivity, a provider should generate multiple versions of a\nvideo as well as the related metadata. Various adaptation methods have been\nproposed to support a streaming client in coping with strong bandwidth\nvariations. However, most of existing methods target at constant bitrate (CBR)\nvideos only. In this paper, we present a new method for quality adaptation in\non-demand streaming of variable bitrate (VBR) videos. To cope with strong\nvariations of VBR bitrate, we use a local average bitrate as the representative\nbitrate of a version. A buffer-based algorithm is then proposed to\nconservatively adapt video quality. Through experiments, we show that our\nmethod can provide quality stability as well as buffer stability even under\nvery strong variations of bandwidth and video bitrates.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.02656v1"
    },
    {
        "title": "Optimization of the Block-level Bit Allocation in Perceptual Video\n  Coding based on MINMAX",
        "authors": [
            "Chao Wang",
            "Xuanqin Mou",
            "Lei Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  In video coding, it is expected that the encoder could adaptively select the\nencoding parameters (e.g., quantization parameter) to optimize the bit\nallocation to different sources under the given constraint. However, in hybrid\nvideo coding, the dependency between sources brings high complexity for the bit\nallocation optimization, especially in the block-level, and existing\noptimization methods mostly focus on frame-level bit allocation. In this paper,\nwe propose a macroblock (MB) level bit allocation method based on the minimum\nmaximum (MINMAX) criterion, which has acceptable encoding complexity for\noffline applications. An iterative-based algorithm, namely maximum distortion\ndescend (MDD), is developed to reduce quality fluctuation among MBs within a\nframe, where the Structure SIMilarity (SSIM) index is used to measure the\nperceptual distortion of MBs. Our extensive experimental results on benchmark\nvideo sequences show that the proposed method can greatly enhance the encoding\nperformance in terms of both bits saving and perceptual quality improvement.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.04691v1"
    },
    {
        "title": "Steganography: A Secure way for Transmission in Wireless Sensor Networks",
        "authors": [
            "Khan Muhammad"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Addressing the security concerns in wireless sensor networks (WSN) is a\nchallenging task, which has attracted the attention of many researchers from\nthe last few decades. Researchers have presented various schemes in WSN,\naddressing the problems of processing, bandwidth, load balancing, and efficient\nrouting. However, little work has been done on security aspects of WSN. In a\ntypical WSN network, the tiny nodes installed on different locations sense the\nsurrounding environment, send the collected data to their neighbors, which in\nturn is forwarded to a sink node. The sink node aggregate the data received\nfrom different sensors and send it to the base station for further processing\nand necessary actions. In highly critical sensor networks such as military and\nlaw enforcement agencies networks, the transmission of such aggregated data via\nthe public network Internet is very sensitive and vulnerable to various attacks\nand risks. Therefore, this paper provides a solution for addressing these\nsecurity issues based on steganography, where the aggregated data can be\nembedded as a secret message inside an innocent-looking cover image. The stego\nimage containing the embedded data can be then sent to fusion center using\nInternet. At the fusion center, the hidden data is extracted from the image,\nthe required processing is performed and decision is taken accordingly.\nExperimentally, the proposed method is evaluated by objective analysis using\npeak signal-to-noise ratio (PSNR), mean square error (MSE), normalized cross\ncorrelation (NCC), and structural similarity index metric (SSIM), providing\npromising results in terms of security and image quality, thus validating its\nsuperiority.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.08865v1"
    },
    {
        "title": "Classifying flows and buffer state for YouTube's HTTP adaptive streaming\n  service in mobile networks",
        "authors": [
            "Dimitrios Tsilimantos",
            "Theodoros Karagkioules",
            "Stefan Valentin"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Accurate cross-layer information is very useful to optimize mobile networks\nfor specific applications. However, providing application-layer information to\nlower protocol layers has become very difficult due to the wide adoption of\nend-to-end encryption and due to the absence of cross-layer signaling\nstandards. As an alternative, this paper presents a traffic profiling solution\nto passively estimate parameters of HTTP Adaptive Streaming (HAS) applications\nat the lower layers. By observing IP packet arrivals, our machine learning\nsystem identifies video flows and detects the state of an HAS client's\nplay-back buffer in real time. Our experiments with YouTube's mobile client\nshow that Random Forests achieve very high accuracy even with a strong\nvariation of link quality. Since this high performance is achieved at IP level\nwith a small, generic feature set, our approach requires no Deep Packet\nInspection (DPI), comes at low complexity, and does not interfere with\nend-to-end encryption. Traffic profiling is, thus, a powerful new tool for\nmonitoring and managing even encrypted HAS traffic in mobile networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00303v2"
    },
    {
        "title": "Stylize Aesthetic QR Code",
        "authors": [
            "Mingliang Xu",
            "Hao Su",
            "Yafei Li",
            "Xi Li",
            "Jing Liao",
            "Jianwei Niu",
            "Pei Lv",
            "Bing Zhou"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  With the continued proliferation of smart mobile devices, Quick Response (QR)\ncode has become one of the most-used types of two-dimensional code in the\nworld. Aiming at beautifying the visual-unpleasant appearance of QR codes,\nexisting works have developed a series of techniques. However, these works\nstill leave much to be desired, such as personalization, artistry, and\nrobustness. To address these issues, in this paper, we propose a novel type of\naesthetic QR codes, SEE (Stylize aEsthEtic) QR code, and a three-stage approach\nto automatically produce such robust style-oriented codes. Specifically, in the\nfirst stage, we propose a method to generate an optimized baseline aesthetic QR\ncode, which reduces the visual contrast between the noise-like black/white\nmodules and the blended image. In the second stage, to obtain art style QR\ncode, we tailor an appropriate neural style transformation network to endow the\nbaseline aesthetic QR code with artistic elements. In the third stage, we\ndesign an error-correction mechanism by balancing two competing terms, visual\nquality and readability, to ensure the performance robust. Extensive\nexperiments demonstrate that SEE QR code has high quality in terms of both\nvisual appearance and robustness, and also offers a greater variety of\npersonalized choices to users.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.01146v2"
    },
    {
        "title": "ART-UP: A Novel Method for Generating Scanning-robust Aesthetic QR codes",
        "authors": [
            "Mingliang Xu",
            "Qingfeng Li",
            "Jianwei Niu",
            "Xiting Liu",
            "Weiwei Xu",
            "Pei Lv",
            "Bing Zhou"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  QR codes are usually scanned in different environments, so they must be\nrobust to variations in illumination, scale, coverage, and camera angles.\nAesthetic QR codes improve the visual quality, but subtle changes in their\nappearance may cause scanning failure. In this paper, a new method to generate\nscanning-robust aesthetic QR codes is proposed, which is based on a\nmodule-based scanning probability estimation model that can effectively balance\nthe tradeoff between visual quality and scanning robustness. Our method locally\nadjusts the luminance of each module by estimating the probability of\nsuccessful sampling. The approach adopts the hierarchical, coarse-to-fine\nstrategy to enhance the visual quality of aesthetic QR codes, which\nsequentially generate the following three codes: a binary aesthetic QR code, a\ngrayscale aesthetic QR code, and the final color aesthetic QR code. Our\napproach also can be used to create QR codes with different visual styles by\nadjusting some initialization parameters. User surveys and decoding experiments\nwere adopted for evaluating our method compared with state-of-the-art\nalgorithms, which indicates that the proposed approach has excellent\nperformance in terms of both visual quality and scanning robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.02280v1"
    },
    {
        "title": "Deep Cross-media Knowledge Transfer",
        "authors": [
            "Xin Huang",
            "Yuxin Peng"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Cross-media retrieval is a research hotspot in multimedia area, which aims to\nperform retrieval across different media types such as image and text. The\nperformance of existing methods usually relies on labeled data for model\ntraining. However, cross-media data is very labor consuming to collect and\nlabel, so how to transfer valuable knowledge in existing data to new data is a\nkey problem towards application. For achieving the goal, this paper proposes\ndeep cross-media knowledge transfer (DCKT) approach, which transfers knowledge\nfrom a large-scale cross-media dataset to promote the model training on another\nsmall-scale cross-media dataset. The main contributions of DCKT are: (1)\nTwo-level transfer architecture is proposed to jointly minimize the media-level\nand correlation-level domain discrepancies, which allows two important and\ncomplementary aspects of knowledge to be transferred: intra-media semantic and\ninter-media correlation knowledge. It can enrich the training information and\nboost the retrieval accuracy. (2) Progressive transfer mechanism is proposed to\niteratively select training samples with ascending transfer difficulties, via\nthe metric of cross-media domain consistency with adaptive feedback. It can\ndrive the transfer process to gradually reduce vast cross-media domain\ndiscrepancy, so as to enhance the robustness of model training. For verifying\nthe effectiveness of DCKT, we take the largescale dataset XMediaNet as source\ndomain, and 3 widelyused datasets as target domain for cross-media retrieval.\nExperimental results show that DCKT achieves promising improvement on retrieval\naccuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.03777v1"
    },
    {
        "title": "Robust Contrast Enhancement Forensics Using Pixel and Histogram Domain\n  CNNs",
        "authors": [
            "Pengpeng Yang",
            "Rongrong Ni",
            "Yao Zhao",
            "Gang Cao",
            "Wei Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Contrast enhancement (CE) forensics has always been ofconcern to image\nforensics community. It can provide aneffective tool for recovering image\nhistory and identifyingtampered images. Although several CE forensic\nalgorithmshave been proposed, their robustness against some processingis still\nunsatisfactory, such as JPEG compression and anti-forensic attacks. In order to\nattenuate such deficiency, inthis paper we first present a discriminability\nanalysis of CEforensics in pixel and gray level histogram domains. Then, insuch\ntwo domains, two end-to-end methods based on convo-lutional neural networks\n(P-CNN, H-CNN) are proposed toachieve robust CE forensics against pre-JPEG\ncompressionand anti-forensics attacks. Experimental results show that\ntheproposed methods achieve much better performance than thestate-of-the-art\nschemes for CE detection in the case of noother operation and comparable\nperformance when pre-JPEGcompression and anti-foresics attacks is used.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04749v3"
    },
    {
        "title": "WISERNet: Wider Separate-then-reunion Network for Steganalysis of Color\n  Images",
        "authors": [
            "Jishen Zeng",
            "Shunquan Tan",
            "Guangqing Liu",
            "Bin Li",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Until recently, deep steganalyzers in spatial domain have been all designed\nfor gray-scale images. In this paper, we propose WISERNet (the wider\nseparate-then-reunion network) for steganalysis of color images. We provide\ntheoretical rationale to claim that the summation in normal convolution is one\nsort of \"linear collusion attack\" which reserves strong correlated patterns\nwhile impairs uncorrelated noises. Therefore in the bottom convolutional layer\nwhich aims at suppressing correlated image contents, we adopt separate\nchannel-wise convolution without summation instead. Conversely, in the upper\nconvolutional layers we believe that the summation in normal convolution is\nbeneficial. Therefore we adopt united normal convolution in those layers and\nmake them remarkably wider to reinforce the effect of \"linear collusion\nattack\". As a result, our proposed wide-and-shallow, separate-then-reunion\nnetwork structure is specifically suitable for color image steganalysis. We\nhave conducted extensive experiments on color image datasets generated from\nBOSSBase raw images and another large-scale dataset which contains 100,000 raw\nimages, with different demosaicking algorithms and down-sampling algorithms.\nThe experimental results show that our proposed network outperforms other\nstate-of-the-art color image steganalytic models either hand-crafted or learned\nusing deep networks in the literature by a clear margin. Specifically, it is\nnoted that the detection performance gain is achieved with less than half the\ncomplexity compared to the most advanced deep-learning steganalyzer as far as\nwe know, which is scarce in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04805v2"
    },
    {
        "title": "A nonlinear transform based analog video transmission framework",
        "authors": [
            "Yongtao Liu",
            "Xiaopeng Fan",
            "Yang Wang",
            "Debin Zhao",
            "Wen Gao"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Soft-cast, a cross-layer design for wireless video transmission, is proposed\nto solve the drawbacks of digital video transmission: threshold transmission\nframework achieving the same effect. Specifically, in encoder, we carry out\npower allocation on the transformed coefficients and encode the coefficients\nbased on the new formulation of power distortion. In decoder, the process of\nLLSE estimator is also improved. Accompanied with the inverse nonlinear\ntransform, DCT coefficients can be recovered depending on the scaling factors ,\nLLSE estimator coefficients and metadata. Experiment results show that our\nproposed framework outperforms the Soft-cast in PSNR 1.08 dB and the MSSIM gain\nreaches to 2.35% when transmitting under the same bandwidth and total power.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.05596v1"
    },
    {
        "title": "Joint Rate Allocation with Both Look-ahead And Feedback Model For High\n  Efficiency Video Coding",
        "authors": [
            "Hongfei Fan",
            "Lin Ding",
            "Xiaodong Xie",
            "Huizhu Jia",
            "Wen Gao"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The objective of joint rate allocation among multiple coded video streams is\nto share the bandwidth to meet the demands of minimum average distortion\n(minAVE) or minimum distortion variance (minVAR). In previous works on minVAR\nproblems, bits are directly assigned in proportion to their complexity measures\nand we call it look-ahead allocation model (LAM), which leads to the fact that\nthe performance will totally depend on the accuracy of the complexity measures.\nThis paper proposes a look-ahead and feedback allocation model (LFAM) for joint\nrate allocation for High Efficiency Video Coding (HEVC) platform which requires\nnegligible computational cost. We derive the model from the target function of\nminVAR theoretically. The bits are assigned according to the complexity\nmeasures, the distortion and bitrate values fed back by the encoder together.\nWe integrated the proposed allocation model in HEVC reference software HM16.0\nand several complexity measures were applied to our allocation model. Results\ndemonstrate that our proposed LFAM performs better than LAM, and an average of\n65.94% variance of mean square error (MSE) is saved with different complexity\nmeasures.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.05747v1"
    },
    {
        "title": "Multi-Codec DASH Dataset",
        "authors": [
            "Anatoliy Zabrovskiy",
            "Christian Feldmann",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The number of bandwidth-hungry applications and services is constantly\ngrowing. HTTP adaptive streaming of audio-visual content accounts for the\nmajority of today's internet traffic. Although the internet bandwidth increases\nalso constantly, audio-visual compression technology is inevitable and we are\ncurrently facing the challenge to be confronted with multiple video codecs.\nThis paper proposes a multi-codec DASH dataset comprising AVC, HEVC, VP9, and\nAV1 in order to enable interoperability testing and streaming experiments for\nthe efficient usage of these codecs under various conditions. We adopt state of\nthe art encoding and packaging options and also provide basic quality metrics\nalong with the DASH segments. Additionally, we briefly introduce a multi-codec\nDASH scheme and possible usage scenarios. Finally, we provide a preliminary\nevaluation of the encoding efficiency in the context of HTTP adaptive streaming\nservices and applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.06874v1"
    },
    {
        "title": "Viewport-Driven Rate-Distortion Optimized 360° Video Streaming",
        "authors": [
            "Jacob Chakareski",
            "Ridvan Aksu",
            "Xavier Corbillon",
            "Gwendal Simon",
            "Viswanathan Swaminathan"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The growing popularity of virtual and augmented reality communications and\n360{\\deg} video streaming is moving video communication systems into much more\ndynamic and resource-limited operating settings. The enormous data volume of\n360{\\deg} videos requires an efficient use of network bandwidth to maintain the\ndesired quality of experience for the end user. To this end, we propose a\nframework for viewport-driven rate-distortion optimized 360{\\deg} video\nstreaming that integrates the user view navigation pattern and the\nspatiotemporal rate-distortion characteristics of the 360{\\deg} video content\nto maximize the delivered user quality of experience for the given\nnetwork/system resources. The framework comprises a methodology for\nconstructing dynamic heat maps that capture the likelihood of navigating\ndifferent spatial segments of a 360{\\deg} video over time by the user, an\nanalysis and characterization of its spatiotemporal rate-distortion\ncharacteristics that leverage preprocessed spatial tilling of the 360{\\deg}\nview sphere, and an optimization problem formulation that characterizes the\ndelivered user quality of experience given the user navigation patterns,\n360{\\deg} video encoding decisions, and the available system/network resources.\nOur experimental results demonstrate the advantages of our framework over the\nconventional approach of streaming a monolithic uniformly encoded 360{\\deg}\nvideo and a state-of-the-art reference method. Considerable video quality gains\nof 4 - 5 dB are demonstrated in the case of two popular 4K 360{\\deg} videos.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08177v1"
    },
    {
        "title": "CNN Based Adversarial Embedding with Minimum Alteration for Image\n  Steganography",
        "authors": [
            "Weixuan Tang",
            "Bin Li",
            "Shunquan Tan",
            "Mauro Barni",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Historically, steganographic schemes were designed in a way to preserve image\nstatistics or steganalytic features. Since most of the state-of-the-art\nsteganalytic methods employ a machine learning (ML) based classifier, it is\nreasonable to consider countering steganalysis by trying to fool the ML\nclassifiers. However, simply applying perturbations on stego images as\nadversarial examples may lead to the failure of data extraction and introduce\nunexpected artefacts detectable by other classifiers. In this paper, we present\na steganographic scheme with a novel operation called adversarial embedding,\nwhich achieves the goal of hiding a stego message while at the same time\nfooling a convolutional neural network (CNN) based steganalyzer. The proposed\nmethod works under the conventional framework of distortion minimization.\nAdversarial embedding is achieved by adjusting the costs of image element\nmodifications according to the gradients backpropagated from the CNN classifier\ntargeted by the attack. Therefore, modification direction has a higher\nprobability to be the same as the sign of the gradient. In this way, the so\ncalled adversarial stego images are generated. Experiments demonstrate that the\nproposed steganographic scheme is secure against the targeted adversary-unaware\nsteganalyzer. In addition, it deteriorates the performance of other\nadversary-aware steganalyzers opening the way to a new class of modern\nsteganographic schemes capable to overcome powerful CNN-based steganalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09043v1"
    },
    {
        "title": "Distinguishing Computer-generated Graphics from Natural Images Based on\n  Sensor Pattern Noise and Deep Learning",
        "authors": [
            "Ye Yao",
            "Weitong Hu",
            "Wei Zhang",
            "Ting Wu",
            "Yun-Qing Shi"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Computer-generated graphics (CGs) are images generated by computer software.\nThe~rapid development of computer graphics technologies has made it easier to\ngenerate photorealistic computer graphics, and these graphics are quite\ndifficult to distinguish from natural images (NIs) with the naked eye. In this\npaper, we propose a method based on sensor pattern noise (SPN) and deep\nlearning to distinguish CGs from NIs. Before being fed into our convolutional\nneural network (CNN)-based model, these images---CGs and NIs---are clipped into\nimage patches. Furthermore, three high-pass filters (HPFs) are used to remove\nlow-frequency signals, which represent the image content. These filters are\nalso used to reveal the residual signal as well as SPN introduced by the\ndigital camera device. Different from the traditional methods of distinguishing\nCGs from NIs, the proposed method utilizes a five-layer CNN to classify the\ninput image patches. Based on the classification results of the image patches,\nwe deploy a majority vote scheme to obtain the classification results for the\nfull-size images. The~experiments have demonstrated that (1) the proposed\nmethod with three HPFs can achieve better results than that with only one HPF\nor no HPF and that (2) the proposed method with three HPFs achieves 100\\%\naccuracy, although the NIs undergo a JPEG compression with a quality factor of\n75.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09403v2"
    },
    {
        "title": "Weakening the Detecting Capability of CNN-based Steganalysis",
        "authors": [
            "Sai Ma",
            "Qingxiao Guan",
            "Xianfeng Zhao",
            "Yaqi Liu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Recently, the application of deep learning in steganalysis has drawn many\nresearchers' attention. Most of the proposed steganalytic deep learning models\nare derived from neural networks applied in computer vision. These kinds of\nneural networks have distinguished performance. However, all these kinds of\nback-propagation based neural networks may be cheated by forging input named\nthe adversarial example. In this paper we propose a method to generate\nsteganographic adversarial example in order to enhance the steganographic\nsecurity of existing algorithms. These adversarial examples can increase the\ndetection error of steganalytic CNN. The experiments prove the effectiveness of\nthe proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.10889v1"
    },
    {
        "title": "High Capacity Image Data Hiding of Scanned Text Documents Using Improved\n  Quadtree",
        "authors": [
            "Seyyed Hossein Soleymani",
            "Amir Hossein Taherinia"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this paper, an effective method was introduced to steganography of text\ndocument in the host image. In the available steganography methods, the message\nhas a random form. Therefore, the embedding capacity is generally low. In the\nproposed method, the main underlying idea was the sparse property of scanned\ndocuments. The scanned documents were converted from gray-level form to binary\nvalues by halftoning idea and then the information-included parts were\nextracted using the improved quadtree and separated from document context.\nNext, in order to compress the extracted parts, an algorithm was proposed based\non reading the binary string bits, ignoring the zero behind the number, and\nconverting them to decimal values. Embedding capacity of the proposed method is\nhigher than that of other available methods with a random-based message.\nTherefore, the proposed method can be used in the secure and intangible\ntransfer of text documents in the host image.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.11286v1"
    },
    {
        "title": "Analysis and prediction of JND-based video quality model",
        "authors": [
            "Haiqiang Wang",
            "Xinfeng Zhang",
            "Chao Yang",
            "C. -C. Jay Kuo"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The just-noticeable-difference (JND) visual perception property has received\nmuch attention in characterizing human subjective viewing experience of\ncompressed video. In this work, we quantify the JND-based video quality\nassessment model using the satisfied user ratio (SUR) curve, and show that the\nSUR model can be greatly simplified since the JND points of multiple subjects\nfor the same content in the VideoSet can be well modeled by the normal\ndistribution. Then, we design an SUR prediction method with video quality\ndegradation features and masking features and use them to predict the first,\nsecond and the third JND points and their corresponding SUR curves. Finally, we\nverify the performance of the proposed SUR prediction method with different\nconfigurations on the VideoSet. The experimental results demonstrate that the\nproposed SUR prediction method achieves good performance in various resolutions\nwith the mean absolute error (MAE) of the SUR smaller than 0.05 on average.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00681v1"
    },
    {
        "title": "A JND-based Video Quality Assessment Model and Its Application",
        "authors": [
            "Haiqiang Wang",
            "Xinfeng Zhang",
            "Chao Yang",
            "C. -C. Jay Kuo"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Based on the Just-Noticeable-Difference (JND) criterion, a subjective video\nquality assessment (VQA) dataset, called the VideoSet, was constructed\nrecently. In this work, we propose a JND-based VQA model using a probabilistic\nframework to analyze and clean collected subjective test data. While most\ntraditional VQA models focus on content variability, our proposed VQA model\ntakes both subject and content variabilities into account. The model parameters\nused to describe subject and content variabilities are jointly optimized by\nsolving a maximum likelihood estimation (MLE) problem. As an application, the\nnew subjective VQA model is used to filter out unreliable video quality scores\ncollected in the VideoSet. Experiments are conducted to demonstrate the\neffectiveness of the proposed model.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00920v1"
    },
    {
        "title": "A Bayesian Approach to Block Structure Inference in AV1-based Multi-rate\n  Video Encoding",
        "authors": [
            "Bichuan Guo",
            "Xinyao Chen",
            "Jiawen Gu",
            "Yuxing Han",
            "Jiangtao Wen"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Due to differences in frame structure, existing multi-rate video encoding\nalgorithms cannot be directly adapted to encoders utilizing special reference\nframes such as AV1 without introducing substantial rate-distortion loss. To\ntackle this problem, we propose a novel bayesian block structure inference\nmodel inspired by a modification to an HEVC-based algorithm. It estimates the\nposterior probabilistic distributions of block partitioning, and adapts early\nterminations in the RDO procedure accordingly. Experimental results show that\nthe proposed method provides flexibility for controlling the tradeoff between\nspeed and coding efficiency, and can achieve an average time saving of 36.1%\n(up to 50.6%) with negligible bitrate cost.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.05323v1"
    },
    {
        "title": "Convex Optimization Based Bit Allocation for Light Field Compression\n  under Weighting and Consistency Constraints",
        "authors": [
            "Bichuan Guo",
            "Yuxing Han",
            "Jiangtao Wen"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Compared with conventional image and video, light field images introduce the\nweight channel, as well as the visual consistency of rendered view, information\nthat has to be taken into account when compressing the pseudo-temporal-sequence\n(PTS) created from light field images. In this paper, we propose a novel frame\nlevel bit allocation framework for PTS coding. A joint model that measures\nweighted distortion and visual consistency, combined with an iterative encoding\nsystem, yields the optimal bit allocation for each frame by solving a convex\noptimization problem. Experimental results show that the proposed framework is\neffective in producing desired distortion distribution based on weights, and\nachieves up to 24.7% BD-rate reduction comparing to the default rate control\nalgorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.05364v1"
    },
    {
        "title": "Fast Block Structure Determination in AV1-based Multiple Resolutions\n  Video Encoding",
        "authors": [
            "Bichuan Guo",
            "Yuxing Han",
            "Jiangtao Wen"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The widely used adaptive HTTP streaming requires an efficient algorithm to\nencode the same video to different resolutions. In this paper, we propose a\nfast block structure determination algorithm based on the AV1 codec that\naccelerates high resolution encoding, which is the bottle-neck of multiple\nresolutions encoding. The block structure similarity across resolutions is\nmodeled by the fineness of frame detail and scale of object motions, this\nenables us to accelerate high resolution encoding based on low resolution\nencoding results. The average depth of a block's co-located neighborhood is\nused to decide early termination in the RDO process. Encoding results show that\nour proposed algorithm reduces encoding time by 30.1%-36.8%, while keeping\nBD-rate low at 0.71%-1.04%. Comparing to the state-of-the-art, our method\nhalves performance loss without sacrificing time savings.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.05365v1"
    },
    {
        "title": "Streaming Video QoE Modeling and Prediction: A Long Short-Term Memory\n  Approach",
        "authors": [
            "Nagabhushan Eswara",
            "S Ashique",
            "Anand Panchbhai",
            "Soumen Chakraborty",
            "Hemanth P. Sethuram",
            "Kiran Kuchi",
            "Abhinav Kumar",
            "Sumohana S. Channappayya"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  HTTP based adaptive video streaming has become a popular choice of streaming\ndue to the reliable transmission and the flexibility offered to adapt to\nvarying network conditions. However, due to rate adaptation in adaptive\nstreaming, the quality of the videos at the client keeps varying with time\ndepending on the end-to-end network conditions. Further, varying network\nconditions can lead to the video client running out of playback content\nresulting in rebuffering events. These factors affect the user satisfaction and\ncause degradation of the user quality of experience (QoE). It is important to\nquantify the perceptual QoE of the streaming video users and monitor the same\nin a continuous manner so that the QoE degradation can be minimized. However,\nthe continuous evaluation of QoE is challenging as it is determined by complex\ndynamic interactions among the QoE influencing factors. Towards this end, we\npresent LSTM-QoE, a recurrent neural network based QoE prediction model using a\nLong Short-Term Memory (LSTM) network. The LSTM-QoE is a network of cascaded\nLSTM blocks to capture the nonlinearities and the complex temporal dependencies\ninvolved in the time varying QoE. Based on an evaluation over several publicly\navailable continuous QoE databases, we demonstrate that the LSTM-QoE has the\ncapability to model the QoE dynamics effectively. We compare the proposed model\nwith the state-of-the-art QoE prediction models and show that it provides\nsuperior performance across these databases. Further, we discuss the state\nspace perspective for the LSTM-QoE and show the efficacy of the state space\nmodeling approaches for QoE prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07126v1"
    },
    {
        "title": "A Convolutional Neural Networks Denoising Approach for Salt and Pepper\n  Noise",
        "authors": [
            "Bo Fu",
            "Xiao-Yang Zhao",
            "Yi Li",
            "Xiang-Hai Wang",
            "Yong-Gong Ren"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The salt and pepper noise, especially the one with extremely high percentage\nof impulses, brings a significant challenge to image denoising. In this paper,\nwe propose a non-local switching filter convolutional neural network denoising\nalgorithm, named NLSF-CNN, for salt and pepper noise. As its name suggested,\nour NLSF-CNN consists of two steps, i.e., a NLSF processing step and a CNN\ntraining step. First, we develop a NLSF pre-processing step for noisy images\nusing non-local information. Then, the pre-processed images are divided into\npatches and used for CNN training, leading to a CNN denoising model for future\nnoisy images. We conduct a number of experiments to evaluate the effectiveness\nof NLSF-CNN. Experimental results show that NLSF-CNN outperforms the\nstate-of-the-art denoising algorithms with a few training images.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.08176v1"
    },
    {
        "title": "Who is the director of this movie? Automatic style recognition based on\n  shot features",
        "authors": [
            "Michele Svanera",
            "Mattia Savardi",
            "Alberto Signoroni",
            "András Bálint Kovács",
            "Sergio Benini"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  We show how low-level formal features, such as shot duration, meant as length\nof camera takes, and shot scale, i.e. the distance between the camera and the\nsubject, are distinctive of a director's style in art movies. So far such\nfeatures were thought of not having enough varieties to become distinctive of\nan author. However our investigation on the full filmographies of six different\nauthors (Scorsese, Godard, Tarr, Fellini, Antonioni, and Bergman) for a total\nnumber of 120 movies analysed second by second, confirms that these\nshot-related features do not appear as random patterns in movies from the same\ndirector. For feature extraction we adopt methods based on both conventional\nand deep learning techniques. Our findings suggest that feature sequential\npatterns, i.e. how features evolve in time, are at least as important as the\nrelated feature distributions. To the best of our knowledge this is the first\nstudy dealing with automatic attribution of movie authorship, which opens up\ninteresting lines of cross-disciplinary research on the impact of style on the\naesthetic and emotional effects on the viewers.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.09560v1"
    },
    {
        "title": "A user model for JND-based video quality assessment: theory and\n  applications",
        "authors": [
            "Haiqiang Wang",
            "Ioannis Katsavounidis",
            "Xinfeng Zhang",
            "Chao Yang",
            "C. -C. Jay Kuo"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The video quality assessment (VQA) technology has attracted a lot of\nattention in recent years due to an increasing demand of video streaming\nservices. Existing VQA methods are designed to predict video quality in terms\nof the mean opinion score (MOS) calibrated by humans in subjective experiments.\nHowever, they cannot predict the satisfied user ratio (SUR) of an aggregated\nviewer group. Furthermore, they provide little guidance to video coding\nparameter selection, e.g. the Quantization Parameter (QP) of a set of\nconsecutive frames, in practical video streaming services. To overcome these\nshortcomings, the just-noticeable-difference (JND) based VQA methodology has\nbeen proposed as an alternative. It is observed experimentally that the JND\nlocation is a normally distributed random variable. In this work, we explain\nthis distribution by proposing a user model that takes both subject\nvariabilities and content variabilities into account. This model is built upon\nuser's capability to discern the quality difference between video clips encoded\nwith different QPs. Moreover, it analyzes video content characteristics to\naccount for inter-content variability. The proposed user model is validated on\nthe data collected in the VideoSet. It is demonstrated that the model is\nflexible to predict SUR distribution of a specific user group.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.10894v1"
    },
    {
        "title": "Efficient feature learning and multi-size image steganalysis based on\n  CNN",
        "authors": [
            "Ru Zhang",
            "Feng Zhu",
            "Jianyi Liu",
            "Gongshen Liu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  For steganalysis, many studies showed that convolutional neural network has\nbetter performances than the two-part structure of traditional machine learning\nmethods. However, there are still two problems to be resolved: cutting down\nsignal to noise ratio of the steganalysis feature map and steganalyzing images\nof arbitrary size. Some algorithms required fixed size images as the input and\nhad low accuracy due to the underutilization of the noise residuals obtained by\nvarious types of filters. In this paper, we focus on designing an improved\nnetwork structure based on CNN to resolve the above problems. First, we use 3x3\nkernels instead of the traditional 5x5 kernels and optimize convolution kernels\nin the preprocessing layer. The smaller convolution kernels are used to reduce\nthe number of parameters and model the features in a small local region. Next,\nwe use separable convolutions to utilize channel correlation of the residuals,\ncompress the image content and increase the signal-to-noise ratio (between the\nstego signal and the image signal). Then, we use spatial pyramid pooling (SPP)\nto aggregate the local features, enhance the representation ability of\nfeatures, and steganalyze arbitrary size image. Finally, data augmentation is\nadopted to further improve network performance. The experimental results show\nthat the proposed CNN structure is significantly better than other four methods\nsuch as SRM, Ye-Net, Xu-Net, and Yedroudj-Net, when it is used to detect two\nspatial algorithms such as WOW and S-UNIWARAD with a wide variety of datasets\nand payloads.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.11428v1"
    },
    {
        "title": "Layered Image Compression using Scalable Auto-encoder",
        "authors": [
            "Chuanmin Jia",
            "Zhaoyi Liu",
            "Yao Wang",
            "Siwei Ma",
            "Wen Gao"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  This paper presents a novel convolutional neural network (CNN) based image\ncompression framework via scalable auto-encoder (SAE). Specifically, our SAE\nbased deep image codec consists of hierarchical coding layers, each of which is\nan end-to-end optimized auto-encoder. The coarse image content and texture are\nencoded through the first (base) layer while the consecutive (enhance) layers\niteratively code the pixel-level reconstruction errors between the original and\nformer reconstructed images. The proposed SAE structure alleviates the need to\ntrain multiple models for different bit-rate points by recently proposed\nauto-encoder based codecs. The SAE layers can be combined to realize multiple\nrate points, or to produce a scalable stream. The proposed method has similar\nrate-distortion performance in the low-to-medium rate range as the\nstate-of-the-art CNN based image codec (which uses different optimized networks\nto realize different bit rates) over a standard public image dataset.\nFurthermore, the proposed codec generates better perceptual quality in this bit\nrate range.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00553v1"
    },
    {
        "title": "The bilateral solver for quality estimation based multi-focus image\n  fusion",
        "authors": [
            "Jingwei Guan",
            "Yibo Chen",
            "Wai-kuen Cham"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this work, a fast Bilateral Solver for Quality Estimation Based\nmulti-focus Image Fusion method (BS-QEBIF) is proposed. The all-in-focus image\nis generated by pixel-wise summing up the multi-focus source images with their\nfocus-levels maps as weights. Since the visual quality of an image patch is\nhighly correlated with its focus level, the focus-level maps are preliminarily\nobtained based on visual quality scores, as pre-estimations. However, the\npre-estimations are not ideal. Thus the fast bilateral solver is then adopted\nto smooth the pre-estimations, and edges in the multi-focus source images can\nbe preserved simultaneously. The edge-preserving smoothed results are utilized\nas final focus-level maps. Moreover, this work provides a confidence-map\nsolution for the unstable fusion in the focus-level-changed boundary regions.\nExperiments were conducted on $25$ pairs of source images. The proposed\nBS-QEBIF outperforms the other $13$ fusion methods objectively and\nsubjectively. The all-in-focus image produced by the proposed method can well\nmaintain the details in the multi-focus source images and does not suffer from\nany residual errors. Experimental results show that BS-QEBIF can handle the\nfocus-level-changed boundary regions without any blocking, ringing and blurring\nartifacts.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.01417v1"
    },
    {
        "title": "Source Camera Attribution of Multi-Format Devices",
        "authors": [
            "Samet Taspinar",
            "Manoranjan Mohanty",
            "Nasir Memon"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Photo Response Non-Uniformity (PRNU) based source camera attribution is an\neffective method to determine the origin camera of visual media (an image or a\nvideo). However, given that modern devices, especially smartphones, capture\nimages, and videos at different resolutions using the same sensor array, PRNU\nattribution can become ineffective as the camera fingerprint and query visual\nmedia can be misaligned. We examine different resizing techniques such as\nbinning, line-skipping, cropping and scaling that cameras use to downsize the\nraw sensor image to different media. Taking such techniques into account, this\npaper studies the problem of source camera attribution. We define the notion of\nRatio of Alignment, which is a measure of shared sensor elements among\nspatially corresponding pixels within two media objects resized with different\ntechniques. We then compute the Ratio of Alignment between the different\ncombinations of three common resizing methods under simplified conditions and\nexperimentally validate our analysis. Based on the insights drawn from the\ndifferent techniques used by cameras and the RoA analysis, the paper proposes\nan algorithm for matching the source of a video with an image and vice versa.\nWe also present an efficient search method resulting in significantly improved\nperformance in matching as well as computation time.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.01533v2"
    },
    {
        "title": "Système d'indexation et de recherche de vidéo intégrant un\n  système gestuel pour les personnes handicapées",
        "authors": [
            "Mohamed Hamroun",
            "Mohamed Salim Bouhlel"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The amount of audio-visual information has increased dramatically with the\nadvent of High Speed Internet. Furthermore, technological advances in recent\nyears in the field of information technology, have simplified the use of video\ndata in various fields by the general public. This made it possible to store\nlarge collections of video documents into computer systems. To enable efficient\nuse of these collections, it is necessary to develop tools to facilitate access\nto these documents and handling them. In this paper we propose a method for\nindexing and retrieval of video sequences in a video database of large\ndimension, based on a weighting technique to calculate the degree of membership\nof a concept in a video also a structuring of the data of the audio-visual\n(context / concept / video). Finally, we decided to create a search system,\noffering in addition to the usual commands, different types of access to the\nsystem, depending on the disability of the person. Indeed, the application\nconsists of a search system but offers access to commands through voice or\ngestures. Our contribution at the experimental level consists with the\nimplementation of prototype. We integrated the techniques proposed in system to\nevaluate it contributions in terms of effectiveness and precision.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.10715v1"
    },
    {
        "title": "A Noise-aware Enhancement Method for Underexposed Images",
        "authors": [
            "Chien-Cheng Chien",
            "Yuma Kinoshita",
            "Hitoshi Kiya"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  A novel method of contrast enhancement is proposed for underexposed images,\nin which heavy noise is hidden. Under low light conditions, images taken by\ndigital cameras have low contrast in dark or bright regions. This is due to a\nlimited dynamic range that imaging sensors have. For these reasons, various\ncontrast enhancement methods have been proposed so far. These methods, however,\nhave two problems: (1) The loss of details in bright regions due to\nover-enhancement of contrast. (2) The noise is amplified in dark regions\nbecause conventional enhancement methods do not consider noise included in\nimages. The proposed method aims to overcome these problems. In the proposed\nmethod, a shadow-up function is applied to adaptive gamma correction with\nweighting distribution, and a denoising filter is also used to avoid noise\nbeing amplified in dark regions. As a result, the proposed method allows us not\nonly to enhance contrast of dark regions, but also to avoid amplifying noise,\neven under strong noise environments.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.10961v1"
    },
    {
        "title": "New Design Paradigm of Distortion Cost Function for Efficient JPEG\n  Steganography",
        "authors": [
            "Wenkang Su",
            "Jiangqun Ni",
            "Xianglei Hu",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Recently, with the introduction of JPEG phase-aware steganalysis features,\ne.g., GFR, the design of JPEG steganographic distortion cost function turns to\nmaintain not only the statistical undetectability in DCT domain but also in\nspatial domain. To tackle this issue, this paper presents a novel paradigm for\nthe design of JPEG steganographic distortion cost function, which calculates\nthe distortion cost via a generalized Distortion Cost Domain Transformation\n(DCDT) function. The proposed function comprises the decompressed pixel block\nembedding changes and their corresponding embedding distortion costs for unit\nchange, where the pixel embedding distortion costs are represented in a more\ngeneral exponential model, aiming to flexibly allocate the embedding data. In\nthis way, the JPEG steganography could be formulated as the optimization\nproblem of minimizing the overall distortion cost in its decompressed spatial\ndomain, which is equivalent to maximizing its statistical undetectability\nagainst JPEG phase-aware steganalysis features. Experimental results show that\nthe proposed DCDT equipped with HiLL (a spatial steganographic distortion cost\nfunction) is superior to other state-of-the-art JPEG steganographic schemes,\ne.g., UERD, J-UNIWARD, and GUED in resisting the detection of JPEG phase-aware\nfeature-based steganalyzers GFR and SCA-GFR, and rivals BET-HiLL with one order\nof magnitude lower computational complexity, along with the possibility of\nbeing further improved by considering the mutually dependent embedding\ninteractions. In addition, the proposed DCDT is also verified to be effective\nfor different image databases and quality factors.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.01947v3"
    },
    {
        "title": "Report of 2017 NSF Workshop on Multimedia Challenges, Opportunities and\n  Research Roadmaps",
        "authors": [
            "Shih-Fu Chang",
            "Alex Hauptmann",
            "Louis-Philippe Morency",
            "Sameer Antani",
            "Dick Bulterman",
            "Carlos Busso",
            "Joyce Chai",
            "Julia Hirschberg",
            "Ramesh Jain",
            "Ketan Mayer-Patel",
            "Reuven Meth",
            "Raymond Mooney",
            "Klara Nahrstedt",
            "Shri Narayanan",
            "Prem Natarajan",
            "Sharon Oviatt",
            "Balakrishnan Prabhakaran",
            "Arnold Smeulders",
            "Hari Sundaram",
            "Zhengyou Zhang",
            "Michelle Zhou"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  With the transformative technologies and the rapidly changing global R&D\nlandscape, the multimedia and multimodal community is now faced with many new\nopportunities and uncertainties. With the open source dissemination platform\nand pervasive computing resources, new research results are being discovered at\nan unprecedented pace. In addition, the rapid exchange and influence of ideas\nacross traditional discipline boundaries have made the emphasis on multimedia\nmultimodal research even more important than before. To seize these\nopportunities and respond to the challenges, we have organized a workshop to\nspecifically address and brainstorm the challenges, opportunities, and research\nroadmaps for MM research. The two-day workshop, held on March 30 and 31, 2017\nin Washington DC, was sponsored by the Information and Intelligent Systems\nDivision of the National Science Foundation of the United States. Twenty-three\n(23) invited participants were asked to review and identify research areas in\nthe MM field that are most important over the next 10-15 year timeframe.\nImportant topics were selected through discussion and consensus, and then\ndiscussed in depth in breakout groups. Breakout groups reported initial\ndiscussion results to the whole group, who continued with further extensive\ndeliberation. For each identified topic, a summary was produced after the\nworkshop to describe the main findings, including the state of the art,\nchallenges, and research roadmaps planned for the next 5, 10, and 15 years in\nthe identified area.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02308v1"
    },
    {
        "title": "Separable Reversible Data Hiding Based on Integer Mapping and Multi-MSB\n  Prediction for Encrypted 3D Mesh Models",
        "authors": [
            "Zhaoxia Yin",
            "Na Xu",
            "Feng Wang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Reversible data hiding in encrypted domain (RDH-ED) has received tremendous\nattention from the research community because data can be embedded into cover\nmedia without exposing it to the third party data hider and the cover media can\nbe losslessly recovered after the extraction of the embedded data. Although, in\nrecent years, extensive studies have been carried out about images based\nRDH-ED, little attention is paid to RDH-ED in 3D meshes due to its complex data\nstructure and irregular geometry. In this paper, we propose a separable RDH-ED\nmethod for 3D meshes based on integer mapping and Multi-MSB (multiplication\nmost significant bit) prediction. The proposed method divides all the vertices\nof the mesh into the \"embedded\" set and \"reference\" set, and maps decimals of\nthe vertex into integers. Then, we calculate the Multi-MSB prediction errors\nfor the vertices of the \"embedded\" set and a bit-stream encryption technique\nwill be executed. Finally, additional data is embedded by replacing the\nMulti-MSB of the encrypted vertex coordinates. According to different\npermissions, recipient can obtain the original plaintext meshes, additional\ndata or both. Experimental results show that the proposed method has higher\nembedding capacity and higher quality of the recovered meshes compared to the\nstate-of-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02473v2"
    },
    {
        "title": "Adaptive Embedding Pattern for Grayscale-Invariance Reversible Data\n  Hiding",
        "authors": [
            "Erdun Gao",
            "Zhibin Pan",
            "Xinyi Gao"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In traditional reversible data hiding (RDH) methods, researchers pay\nattention to enlarge the embedding capacity (EC) and to reduce the embedding\ndistortion (ED). Recently, a completely novel RDH algorithm was developed to\nembed secret data into color image without changing the corresponding grayscale\n[1], which largely expands the applications of RDH. In [1], for color image,\nchannel R and channel B are exploited to carry secret information, channel G is\nadjusted for balancing the modifications of channel R and channel B to keep the\ninvariance of grayscale. However, we found that the embedding performance (EP)\nof that method is still unsatisfied and could be further enhanced. To improve\nthe EP, an adaptive embedding pattern is introduced to enhance the competence\nof algorithm for selectively embedding different bits of secret data into\npixels according to context information. Moreover, a novel two-level predictor\nis designed by uniting two normal predictors for reducing the ED for embedding\nmore bits. Experimental results demonstrate that, compared to the previous\nmethod, our scheme could significantly enhance the image fidelity while keeping\nthe grayscale invariant.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.05965v1"
    },
    {
        "title": "Cumulative Quality Modeling for HTTP Adaptive Streaming",
        "authors": [
            "Huyen T. T. Tran",
            "Nam Pham Ngoc",
            "Tobias Hoßfeld",
            "Michael Seufert",
            "Truong Cong Thang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Thanks to the abundance of Web platforms and broadband connections, HTTP\nAdaptive Streaming has become the de facto choice for multimedia delivery\nnowadays. However, the visual quality of adaptive video streaming may fluctuate\nstrongly during a session due to bandwidth fluctuations. So, it is important to\nevaluate the quality of a streaming session over time. In this paper, we\npropose a model to estimate the cumulative quality for HTTP Adaptive Streaming.\nIn the model, a sliding window of video segments is employed as the basic\nbuilding block. Through statistical analysis using a subjective dataset, we\nidentify three important components of the cumulative quality model, namely the\nminimum window quality, the last window quality, and the average window\nquality. Experiment results show that the proposed model achieves high\nprediction performance and outperforms related quality models. In addition,\nanother advantage of the proposed model is its simplicity and effectiveness for\ndeployment in real-time estimation. The source code of the proposed model has\nbeen made available to the public at https://github.com/TranHuyen1191/CQM.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.02772v3"
    },
    {
        "title": "Hit Ratio Driven Mobile Edge Caching Scheme for Video on Demand Services",
        "authors": [
            "Xing Chen",
            "Lijun He",
            "Shang Xu",
            "Shibo Hu",
            "Qingzhou Li",
            "Guizhong Liu"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  More and more scholars focus on mobile edge computing (MEC) technology,\nbecause the strong storage and computing capabilities of MEC servers can reduce\nthe long transmission delay, bandwidth waste, energy consumption, and privacy\nleaks in the data transmission process. In this paper, we study the cache\nplacement problem to determine how to cache videos and which videos to be\ncached in a mobile edge computing system. First, we derive the video request\nprobability by taking into account video popularity, user preference and the\ncharacteristic of video representations. Second, based on the acquired request\nprobability, we formulate a cache placement problem with the objective to\nmaximize the cache hit ratio subject to the storage capacity constraints.\nFinally, in order to solve the formulated problem, we transform it into a\ngrouping knapsack problem and develop a dynamic programming algorithm to obtain\nthe optimal caching strategy. Simulation results show that the proposed\nalgorithm can greatly improve the cache hit ratio.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03766v1"
    },
    {
        "title": "Enhancing JPEG Steganography using Iterative Adversarial Examples",
        "authors": [
            "Huaxiao Mo",
            "Tingting Song",
            "Bolin Chen",
            "Weiqi Luo",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Convolutional Neural Networks (CNN) based methods have significantly improved\nthe performance of image steganalysis compared with conventional ones based on\nhand-crafted features. However, many existing literatures on computer vision\nhave pointed out that those effective CNN-based methods can be easily fooled by\nadversarial examples. In this paper, we propose a novel steganography framework\nbased on adversarial example in an iterative manner. The proposed framework\nfirst starts from an existing embedding cost, such as J-UNIWARD in this work,\nand then updates the cost iteratively based on adversarial examples derived\nfrom a series of steganalytic networks until achieving satisfactory results. We\ncarefully analyze two important factors that would affect the security\nperformance of the proposed framework, i.e. the percentage of selected\ngradients with larger amplitude and the adversarial intensity to modify\nembedding cost. The experimental results evaluated on three modern steganalytic\nmodels, including GFR, SCA-GFR and SRNet, show that the proposed framework is\nvery promising to enhance the security performances of JPEG steganography.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.07556v3"
    },
    {
        "title": "Multi-Task Music Representation Learning from Multi-Label Embeddings",
        "authors": [
            "Alexander Schindler",
            "Peter Knees"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  This paper presents a novel approach to music representation learning.\nTriplet loss based networks have become popular for representation learning in\nvarious multimedia retrieval domains. Yet, one of the most crucial parts of\nthis approach is the appropriate selection of triplets, which is indispensable,\nconsidering that the number of possible triplets grows cubically. We present an\napproach to harness multi-tag annotations for triplet selection, by using\nLatent Semantic Indexing to project the tags onto a high-dimensional space.\nFrom this we estimate tag-relatedness to select hard triplets. The approach is\nevaluated in a multi-task scenario for which we introduce four large multi-tag\nannotations for the Million Song Dataset for the music properties genres,\nstyles, moods, and themes.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.07730v1"
    },
    {
        "title": "Focus Your Attention: A Bidirectional Focal Attention Network for\n  Image-Text Matching",
        "authors": [
            "Chunxiao Liu",
            "Zhendong Mao",
            "An-An Liu",
            "Tianzhu Zhang",
            "Bin Wang",
            "Yongdong Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Learning semantic correspondence between image and text is significant as it\nbridges the semantic gap between vision and language. The key challenge is to\naccurately find and correlate shared semantics in image and text. Most existing\nmethods achieve this goal by representing the shared semantic as a weighted\ncombination of all the fragments (image regions or text words), where fragments\nrelevant to the shared semantic obtain more attention, otherwise less. However,\ndespite relevant ones contribute more to the shared semantic, irrelevant ones\nwill more or less disturb it, and thus will lead to semantic misalignment in\nthe correlation phase. To address this issue, we present a novel Bidirectional\nFocal Attention Network (BFAN), which not only allows to attend to relevant\nfragments but also diverts all the attention into these relevant fragments to\nconcentrate on them. The main difference with existing works is they mostly\nfocus on learning attention weight while our BFAN focus on eliminating\nirrelevant fragments from the shared semantic. The focal attention is achieved\nby pre-assigning attention based on inter-modality relation, identifying\nrelevant fragments based on intra-modality relation and reassigning attention.\nFurthermore, the focal attention is jointly applied in both image-to-text and\ntext-to-image directions, which enables to avoid preference to long text or\ncomplex image. Experiments show our simple but effective framework\nsignificantly outperforms state-of-the-art, with relative Recall@1 gains of\n2.2% on both Flicr30K and MSCOCO benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.11416v1"
    },
    {
        "title": "Diachronic Cross-modal Embeddings",
        "authors": [
            "David Semedo",
            "João Magalhães"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Understanding the semantic shifts of multimodal information is only possible\nwith models that capture cross-modal interactions over time. Under this\nparadigm, a new embedding is needed that structures visual-textual interactions\naccording to the temporal dimension, thus, preserving data's original temporal\norganisation. This paper introduces a novel diachronic cross-modal embedding\n(DCM), where cross-modal correlations are represented in embedding space,\nthroughout the temporal dimension, preserving semantic similarity at each\ninstant t. To achieve this, we trained a neural cross-modal architecture, under\na novel ranking loss strategy, that for each multimodal instance, enforces\nneighbour instances' temporal alignment, through subspace structuring\nconstraints based on a temporal alignment window. Experimental results show\nthat our DCM embedding successfully organises instances over time. Quantitative\nexperiments, confirm that DCM is able to preserve semantic cross-modal\ncorrelations at each instant t while also providing better alignment\ncapabilities. Qualitative experiments unveil new ways to browse multimodal\ncontent and hint that multimodal understanding tasks can benefit from this new\nembedding.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.13689v1"
    },
    {
        "title": "BlessMark: A Blind Diagnostically-Lossless Watermarking Framework for\n  Medical Applications Based on Deep Neural Networks",
        "authors": [
            "Hamidreza Zarrabi",
            "Ali Emami",
            "Pejman Khadivi",
            "Nader Karimi",
            "Shadrokh Samavi"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Nowadays, with the development of public network usage, medical information\nis transmitted throughout the hospitals. The watermarking system can help for\nthe confidentiality of medical information distributed over the internet. In\nmedical images, regions-of-interest (ROI) contain diagnostic information. The\nwatermark should be embedded only into non-regions-of-interest (NROI) to keep\ndiagnostic information without distortion. Recently, ROI based watermarking has\nattracted the attention of the medical research community. The ROI map can be\nused as an embedding key for improving confidentiality protection purposes.\nHowever, in most existing works, the ROI map that is used for the embedding\nprocess must be sent as side-information along with the watermarked image. This\nside information is a disadvantage and makes the extraction process non-blind.\nAlso, most existing algorithms do not recover NROI of the original cover image\nafter the extraction of the watermark. In this paper, we propose a framework\nfor blind diagnostically-lossless watermarking, which iteratively embeds only\ninto NROI. The significance of the proposed framework is in satisfying the\nconfidentiality of the patient information through a blind watermarking system,\nwhile it preserves diagnostic/medical information of the image throughout the\nwatermarking process. A deep neural network is used to recognize the ROI map in\nthe embedding, extraction, and recovery processes. In the extraction process,\nthe same ROI map of the embedding process is recognized without requiring any\nadditional information. Hence, the watermark is blindly extracted from the\nNROI.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00382v2"
    },
    {
        "title": "A Generalized Rate-Distortion-$λ$ Model Based HEVC Rate Control\n  Algorithm",
        "authors": [
            "Minhao Tang",
            "Jiangtao Wen",
            "Yuxing Han"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The High Efficiency Video Coding (HEVC/H.265) standard doubles the\ncompression efficiency of the widely used H.264/AVC standard. For practical\napplications, rate control (RC) algorithms for HEVC need to be developed. Based\non the R-Q, R-${\\rho}$ or R-${\\lambda}$ models, rate control algorithms aim at\nencoding a video clip/segment to a target bit rate accurately with high video\nquality after compression. Among the various models used by HEVC rate control\nalgorithms, the R-${\\lambda}$ model performs the best in both coding efficiency\nand rate control accuracy. However, compared with encoding with a fixed\nquantization parameter (QP), even the best rate control algorithm [1] still\nunder-performs when comparing the video quality achieved at identical average\nbit rates. In this paper, we propose a novel generalized\nrate-distortion-${\\lambda}$ (R-D-${\\lambda}$) model for the relationship\nbetween rate (R), distortion (D) and the Lagrangian multiplier (${\\lambda}$) in\nrate-distortion (RD) optimized encoding. In addition to the well designed\nhierarchical initialization and coefficient update scheme, a new model based\nrate allocation scheme composed of amortization, smooth window and consistency\ncontrol is proposed for a better rate allocation. Experimental results\nimplementing the proposed algorithm in the HEVC reference software HM-16.9 show\nthat the proposed rate control algorithm is able to achieve an average of BDBR\nsaving of 6.09%, 3.15% and 4.03% for random access (RA), low delay P (LDP) and\nlow delay B (LDB) configurations respectively as compared with the\nR-${\\lambda}$ model based RC algorithm [1] implemented in HM. The proposed\nalgorithm also outperforms the state-of-the-art algorithms, while rate control\naccuracy and encoding speed are hardly impacted.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00639v1"
    },
    {
        "title": "FCEM: A Novel Fast Correlation Extract Model For Real Time Steganalysis\n  of VoIP Stream via Multi-head Attention",
        "authors": [
            "Hao Yang",
            "ZhongLiang Yang",
            "YongJian Bao",
            "Sheng Liu",
            "YongFeng Huang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Extracting correlation features between codes-words with high computational\nefficiency is crucial to steganalysis of Voice over IP (VoIP) streams. In this\npaper, we utilized attention mechanisms, which have recently attracted enormous\ninterests due to their highly parallelizable computation and flexibility in\nmodeling correlation in sequence, to tackle steganalysis problem of\nQuantization Index Modulation (QIM) based steganography in compressed VoIP\nstream. We design a light-weight neural network named Fast Correlation Extract\nModel (FCEM) only based on a variant of attention called multi-head attention\nto extract correlation features from VoIP frames. Despite its simple form, FCEM\noutperforms complicated Recurrent Neural Networks (RNNs) and Convolutional\nNeural Networks (CNNs) models on both prediction accuracy and time efficiency.\nIt significantly improves the best result in detecting both low embedded rates\nand short samples recently. Besides, the proposed model accelerates the\ndetection speed as twice as before when the sample length is as short as 0.1s,\nmaking it a excellent method for online services.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00682v2"
    },
    {
        "title": "Adaptive Rate Allocation for View-Aware Point-Cloud Streaming",
        "authors": [
            "Mohammad Hosseini"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In the context of view-dependent point-cloud streaming in a scene, our rate\nallocation is \"adaptive\" in the sense that it priorities the point-cloud models\ndepending on the camera view and the visibility of the objects and their\ndistance as described. The algorithm delivers higher bitrate to the point-cloud\nmodels which are inside user's viewport, more likely for the user to look at,\nor are closer to the view camera or, while delivers lower quality level to the\npoint-cloud models outside of a user's immediate viewport or farther away from\nthe camera. For that purpose, we hereby explain the rate allocation problem\nwithin the context of multi-point-cloud streaming where multiple point-cloud\nmodels are aimed to be streamed to the target device, and propose a rate\nallocation heuristic algorithm to enable the adaptations within this context.\nTo the best of our knowledge, this is the first work to mathematically model,\nand propose a rate allocation heuristic algorithm within the context of\npoint-cloud streaming.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00812v1"
    },
    {
        "title": "Reversible Data Hiding in Encrypted Images based on Pixel Prediction and\n  Bit-plane Compression",
        "authors": [
            "Zhaoxia Yin",
            "Yinyin Peng",
            "Youzhi Xiang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Reversible data hiding in encrypted images (RDHEI) receives growing attention\nbecause it protects the content of the original image while the embedded data\ncan be accurately extracted and the original image can be reconstructed\nlossless. To make full use of the correlation of the adjacent pixels, this\npaper proposes an RDHEI scheme based on pixel prediction and bit-plane\ncompression. Firstly, to vacate room for data embedding, the prediction error\nof the original image is calculated and used for bit-plane rearrangement and\ncompression. Then, the image after vacating room is encrypted by a stream\ncipher. Finally, the additional data is embedded in the vacated room by\nmulti-LSB substitution. Experimental results show that the embedding capacity\nof the proposed method outperforms the state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.01699v1"
    },
    {
        "title": "A Robust Blind 3-D Mesh Watermarking based on Wavelet Transform for\n  Copyright Protection",
        "authors": [
            "Mohamed Hamidi",
            "Mohamed El Haziti",
            "Hocine Cherifi",
            "Driss Aboutajdine"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Nowadays, three-dimensional meshes have been extensively used in several\napplications such as, industrial, medical, computer-aided design (CAD) and\nentertainment due to the processing capability improvement of computers and the\ndevelopment of the network infrastructure. Unfortunately, like digital images\nand videos, 3-D meshes can be easily modified, duplicated and redistributed by\nunauthorized users. Digital watermarking came up while trying to solve this\nproblem. In this paper, we propose a blind robust watermarking scheme for\nthree-dimensional semiregular meshes for Copyright protection. The watermark is\nembedded by modifying the norm of the wavelet coefficient vectors associated\nwith the lowest resolution level using the edge normal norms as synchronizing\nprimitives. The experimental results show that in comparison with alternative\n3-D mesh watermarking approaches, the proposed method can resist to a wide\nrange of common attacks, such as similarity transformations including\ntranslation, rotation, uniform scaling and their combination, noise addition,\nLaplacian smoothing, quantization, while preserving high imperceptibility.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.03793v1"
    },
    {
        "title": "Pano: Optimizing 360° Video Streaming with a Better Understanding\n  of Quality Perception",
        "authors": [
            "Yu Guan",
            "Chengyuan Zheng",
            "Zongming Guo",
            "Xinggong Zhang",
            "Junchen Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Streaming 360{\\deg} videos requires more bandwidth than non-360{\\deg} videos.\nThis is because current solutions assume that users perceive the quality of\n360{\\deg} videos in the same way they perceive the quality of non-360{\\deg}\nvideos. This means the bandwidth demand must be proportional to the size of the\nuser's field of view. However, we found several qualitydetermining factors\nunique to 360{\\deg}videos, which can help reduce the bandwidth demand. They\ninclude the moving speed of a user's viewpoint (center of the user's field of\nview), the recent change of video luminance, and the difference in\ndepth-of-fields of visual objects around the viewpoint. This paper presents\nPano, a 360{\\deg} video streaming system that leverages the 360{\\deg}\nvideo-specific factors. We make three contributions. (1) We build a new quality\nmodel for 360{\\deg} videos that captures the impact of the 360{\\deg}\nvideo-specific factors. (2) Pano proposes a variable-sized tiling scheme in\norder to strike a balance between the perceived quality and video encoding\nefficiency. (3) Pano proposes a new qualityadaptation logic that maximizes\n360{\\deg} video user-perceived quality and is readily deployable. Our\nevaluation (based on user study and trace analysis) shows that compared with\nstate-of-the-art techniques, Pano can save 41-46% bandwidth without any drop in\nthe perceived quality, or it can raise the perceived quality (user rating) by\n25%-142% without using more bandwidth.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.04139v1"
    },
    {
        "title": "Are Social Networks Watermarking Us or Are We (Unawarely) Watermarking\n  Ourself?",
        "authors": [
            "Flavio Bertini",
            "Rajesh Sharma",
            "Danilo Montesi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In the last decade, Social Networks (SNs) have deeply changed many aspects of\nsociety, and one of the most widespread behaviours is the sharing of pictures.\nHowever, malicious users often exploit shared pictures to create fake profiles\nleading to the growth of cybercrime. Thus, keeping in mind this scenario,\nauthorship attribution and verification through image watermarking techniques\nare becoming more and more important. In this paper, firstly, we investigate\nhow 13 most popular SNs treat the uploaded pictures, in order to identify a\npossible implementation of image watermarking techniques by respective SNs.\nSecondly, on these 13 SNs, we test the robustness of several image watermarking\nalgorithms. Finally, we verify whether a method based on the Photo-Response\nNon-Uniformity (PRNU) technique can be successfully used as a watermarking\napproach for authorship attribution and verification of pictures on SNs. The\nproposed method is robust enough in spite of the fact that the pictures get\ndowngraded during the uploading process by SNs. The results of our analysis on\na real dataset of 8,400 pictures show that the proposed method is more\neffective than other watermarking techniques and can help to address serious\nquestions about privacy and security on SNs.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.03903v1"
    },
    {
        "title": "A Study on Impacts of Multiple Factors on Video Qualify of Experience",
        "authors": [
            "Huyen T. T. Tran",
            "Nam Pham Ngoc",
            "Truong Cong Thang"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  HTTP Adaptive Streaming (HAS) has become a cost-effective means for\nmultimedia delivery nowadays. However, how the quality of experience (QoE) is\njointly affected by 1) varying perceptual quality and 2) interruptions is not\nwell-understood. In this paper, we present the first attempt to quantitatively\nquantify the relative impacts of these factors on the QoE of streaming\nsessions. To achieve this purpose, we first model the impacts of the factors\nusing histograms, which represent the frequency distributions of the individual\nfactors in a session. By using a large dataset, various insights into the\nrelative impacts of these factors are then provided, serving as suggestions to\nimprove the QoE of streaming sessions.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.12697v1"
    },
    {
        "title": "New Framework for Code-Mapping-based Reversible Data Hiding in JPEG\n  Images",
        "authors": [
            "Yang Du",
            "Zhaoxia Yin"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Code mapping (CM) is an efficient technique for reversible data hiding (RDH)\nin JPEG images, which embeds data by constructing a mapping relationship\nbetween the used and unused codes in the JPEG bitstream. This study presents a\nnew framework for designing a CM-based RDH method. First, a new code mapping\nstrategy is proposed to suppress file size expansion and improve applicability.\nBased on our proposed strategy, the mapped codes are redefined by creating a\nnew Huffman table rather than selecting them from the unused codes in the\noriginal Huffman table. The critical issue of designing the CM-based RDH\nmethod, that is, constructing code mapping, is converted into a combinatorial\noptimization problem. This study proposes a novel CM-based RDH method that\nutilizes a genetic algorithm (GA). The experimental results demonstrate that\nthe proposed method achieves a high embedding capacity with no signal\ndistortion while suppressing file size expansion.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.15984v6"
    },
    {
        "title": "Cost Efficient Repository Management for Cloud-Based On-Demand Video\n  Streaming",
        "authors": [
            "Mahmoud Darwich",
            "Ege Beyazit",
            "Mohsen Amini Salehiy",
            "Magdy Bayoumi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Video transcoding is the process of converting a video to the format\nsupported by the viewer's device. Video transcoding requires huge storage and\ncomputational resources, thus, many video stream providers choose to carry it\nout on the cloud. Video streaming providers generally need to prepare several\nformats of the same video (termed pre-transcoding) and stream the appropriate\nformat to the viewer. However, pre-transcoding requires enormous storage space\nand imposes a significant cost to the stream provider. More importantly,\npre-transcoding proven to be inefficient due to the long-tail access pattern to\nvideo streams in a repository. To reduce the incurred cost, in this research,\nwe propose a method to partially pre-transcode video streams and re-transcode\nthe rest of it in an on-demand manner. We will develop a method to strike a\ntrade-off between pre-transcoding and on-demand transcoding of video streams to\nreduce the overall cost. Experimental results show the efficiency of our\napproach, particularly, when a high percentage of videos are accessed\nfrequently. In such repositories, the proposed approach reduces the incurred\ncost by up to 70\\%.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.00597v1"
    },
    {
        "title": "LAMP: Label Augmented Multimodal Pretraining",
        "authors": [
            "Jia Guo",
            "Chen Zhu",
            "Yilun Zhao",
            "Heda Wang",
            "Yao Hu",
            "Xiaofei He",
            "Deng Cai"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Multi-modal representation learning by pretraining has become an increasing\ninterest due to its easy-to-use and potential benefit for various\nVisual-and-Language~(V-L) tasks. However its requirement of large volume and\nhigh-quality vision-language pairs highly hinders its values in practice. In\nthis paper, we proposed a novel label-augmented V-L pretraining model, named\nLAMP, to address this problem. Specifically, we leveraged auto-generated labels\nof visual objects to enrich vision-language pairs with fine-grained alignment\nand correspondingly designed a novel pretraining task. Besides, we also found\nsuch label augmentation in second-stage pretraining would further universally\nbenefit various downstream tasks. To evaluate LAMP, we compared it with some\nstate-of-the-art models on four downstream tasks. The quantitative results and\nanalysis have well proven the value of labels in V-L pretraining and the\neffectiveness of LAMP.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.04446v1"
    },
    {
        "title": "A User-experience Driven SSIM-Aware Adaptation Approach for DASH Video\n  Streaming",
        "authors": [
            "Mustafa Othman",
            "Ken Chen",
            "Anissa Mokraoui"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Dynamic Adaptive Streaming over HTTP (DASH) is a video streaming technique\nlargely used. One key point is the adaptation mechanism which resides at the\nclient's side. This mechanism impacts greatly on the overall Quality of\nExperience (QoE) of the video streaming. In this paper, we propose a new\nadaptation algorithm for DASH, namely SSIM Based Adaptation (SBA). This\nmechanism is user-experience driven: it uses the Structural Similarity Index\nMeasurement (SSIM) as main video perceptual quality indicator; moreover, the\nadaptation is based on a joint consideration of SSIM indicator and the physical\nresources (buffer occupancy, bandwidth) in order to minimize the buffer\nstarvation (rebuffering) and video quality instability, as well as to maximize\nthe overall video quality (through SSIM). To evaluate the performance of our\nproposal, we carried out trace-driven emulation with real traffic traces\n(captured in real mobile network). Comparisons with some representative\nalgorithms (BBA, FESTIVE, OSMF) through major QoE metrics show that our\nadaptation algorithm SBA achieves an efficient adaptation minimizing both the\nrebuffering and instability, whereas the displayed video is maintained at a\nhigh level of bitrate.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.05696v1"
    },
    {
        "title": "An Artistic Visualization of Music Modeling a Synesthetic Experience",
        "authors": [
            "Matthew Joseph Adiletta",
            "Oliver Thomas"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This project brings music to sight. Music can be a visual masterpiece. Some\npeople naturally experience a visualization of audio - a condition called\nsynesthesia. The type of synesthesia explored is when sounds create colors in\nthe 'mind's eye.' Project included interviews with people who experience\nsynesthesia, examination of prior art, and topic research to inform project\ndesign. Audio input, digital signal processing (including Fast Fourier\nTransforms (FFTs)) and data manipulation produce arguments required for our\nvisualization. Arguments are then applied to a physics particle simulator which\nis re-purposed to model a synesthetic experience. The result of the project is\na simulator in MAX 8, which generates a visual performance using particles by\nvarying each particle's position, velocity, and color based on parameters\nextracted via digital processing of input audio.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08034v1"
    },
    {
        "title": "A Deep Multi-Level Attentive network for Multimodal Sentiment Analysis",
        "authors": [
            "Ashima Yadav",
            "Dinesh Kumar Vishwakarma"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Multimodal sentiment analysis has attracted increasing attention with broad\napplication prospects. The existing methods focuses on single modality, which\nfails to capture the social media content for multiple modalities. Moreover, in\nmulti-modal learning, most of the works have focused on simply combining the\ntwo modalities, without exploring the complicated correlations between them.\nThis resulted in dissatisfying performance for multimodal sentiment\nclassification. Motivated by the status quo, we propose a Deep Multi-Level\nAttentive network, which exploits the correlation between image and text\nmodalities to improve multimodal learning. Specifically, we generate the\nbi-attentive visual map along the spatial and channel dimensions to magnify\nCNNs representation power. Then we model the correlation between the image\nregions and semantics of the word by extracting the textual features related to\nthe bi-attentive visual features by applying semantic attention. Finally,\nself-attention is employed to automatically fetch the sentiment-rich multimodal\nfeatures for the classification. We conduct extensive evaluations on four\nreal-world datasets, namely, MVSA-Single, MVSA-Multiple, Flickr, and Getty\nImages, which verifies the superiority of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08256v1"
    },
    {
        "title": "UAV-Assisted Image Acquisition: 3D UAV Trajectory Design and Camera\n  Control",
        "authors": [
            "Xiao-Wei Tang",
            "Shuowen Zhang",
            "Changsheng You",
            "Xin-Lin Huang",
            "Rui Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In this paper, we consider a new unmanned aerial vehicle (UAV)-assisted\noblique image acquisition system where a UAV is dispatched to take images of\nmultiple ground targets (GTs). To study the three-dimensional (3D) UAV\ntrajectory design for image acquisition, we first propose a novel UAV-assisted\noblique photography model, which characterizes the image resolution with\nrespect to the UAV's 3D image-taking location. Then, we formulate a 3D UAV\ntrajectory optimization problem to minimize the UAV's traveling distance\nsubject to the image resolution constraints. The formulated problem is shown to\nbe equivalent to a modified 3D traveling salesman problem with neighbourhoods,\nwhich is NP-hard in general. To tackle this difficult problem, we propose an\niterative algorithm to obtain a high-quality suboptimal solution efficiently,\nby alternately optimizing the UAV's 3D image-taking waypoints and its visiting\norder for the GTs. Numerical results show that the proposed algorithm\nsignificantly reduces the UAV's traveling distance as compared to various\nbenchmark schemes, while meeting the image resolution requirement.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08865v2"
    },
    {
        "title": "Digital Reconstruction of Elmina Castle for Mobile Virtual Reality via\n  Point-based Detail Transfer",
        "authors": [
            "Sifan Ye",
            "Ting Wu",
            "Michael Jarvis",
            "Yuhao Zhu"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Reconstructing 3D models from large, dense point clouds is critical to enable\nVirtual Reality (VR) as a platform for entertainment, education, and heritage\npreservation. Existing 3D reconstruction systems inevitably make trade-offs\nbetween three conflicting goals: the efficiency of reconstruction (e.g., time\nand memory requirements), the visual quality of the constructed scene, and the\nrendering speed on the VR device. This paper proposes a reconstruction system\nthat simultaneously meets all three goals. The key idea is to avoid the\nresource-demanding process of reconstructing a high-polygon mesh altogether.\nInstead, we propose to directly transfer details from the original point cloud\nto a low polygon mesh, which significantly reduces the reconstruction time and\ncost, preserves the scene details, and enables real-time rendering on mobile VR\ndevices.\n  While our technique is general, we demonstrate it in reconstructing cultural\nheritage sites. We for the first time digitally reconstruct the Elmina Castle,\na UNESCO world heritage site at Ghana, from billions of laser-scanned points.\nThe reconstruction process executes on low-end desktop systems without\nrequiring high processing power, making it accessible to the broad community.\nThe reconstructed scenes render on Oculus Go in 60 FPS, providing a real-time\nVR experience with high visual quality. Our project is part of the Digital\nElmina effort (http://digitalelmina.org/) between University of Rochester and\nUniversity of Ghana.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.10739v3"
    },
    {
        "title": "Study On Coding Tools Beyond Av1",
        "authors": [
            "Xin Zhao",
            "Liang Zhao",
            "Madhu Krishnan",
            "Yixin Du",
            "Shan Liu",
            "Debargha Mukherjee",
            "Yaowu Xu",
            "Adrian Grange"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The Alliance for Open Media has recently initiated coding tool exploration\nactivities towards the next-generation video coding beyond AV1. With this\nregard, this paper presents a package of coding tools that have been\ninvestigated, implemented and tested on top of the codebase, known as libaom,\nwhich is used for the exploration of next-generation video compression tools.\nThe proposed tools cover several technical areas based on a traditional hybrid\nvideo coding structure, including block partitioning, prediction, transform and\nloop filtering. The proposed coding tools are integrated as a package, and a\ncombined coding gain over AV1 is demonstrated in this paper. Furthermore, to\nbetter understand the behavior of each tool, besides the combined coding gain,\nthe tool-on and tool-off tests are also simulated and reported for each\nindividual coding tool. Experimental results show that, compared to libaom, the\nproposed methods achieve an average 8.0% (up to 22.0%) overall BD-rate\nreduction for All Intra coding configuration a wide range of image and video\ncontent.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.13491v1"
    },
    {
        "title": "Detecting Medical Misinformation on Social Media Using Multimodal Deep\n  Learning",
        "authors": [
            "Zuhui Wang",
            "Zhaozheng Yin",
            "Young Anna Argyris"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In 2019, outbreaks of vaccine-preventable diseases reached the highest number\nin the US since 1992. Medical misinformation, such as antivaccine content\npropagating through social media, is associated with increases in vaccine delay\nand refusal. Our overall goal is to develop an automatic detector for\nantivaccine messages to counteract the negative impact that antivaccine\nmessages have on the public health. Very few extant detection systems have\nconsidered multimodality of social media posts (images, texts, and hashtags),\nand instead focus on textual components, despite the rapid growth of\nphoto-sharing applications (e.g., Instagram). As a result, existing systems are\nnot sufficient for detecting antivaccine messages with heavy visual components\n(e.g., images) posted on these newer platforms. To solve this problem, we\npropose a deep learning network that leverages both visual and textual\ninformation. A new semantic- and task-level attention mechanism was created to\nhelp our model to focus on the essential contents of a post that signal\nantivaccine messages. The proposed model, which consists of three branches, can\ngenerate comprehensive fused features for predictions. Moreover, an ensemble\nmethod is proposed to further improve the final prediction accuracy. To\nevaluate the proposed model's performance, a real-world social media dataset\nthat consists of more than 30,000 samples was collected from Instagram between\nJanuary 2016 and October 2019. Our 30 experiment results demonstrate that the\nfinal network achieves above 97% testing accuracy and outperforms other\nrelevant models, demonstrating that it can detect a large amount of antivaccine\nmessages posted daily. The implementation code is available at\nhttps://github.com/wzhings/antivaccine_detection.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.13968v1"
    },
    {
        "title": "An Efficient QP Variable Convolutional Neural Network Based In-loop\n  Filter for Intra Coding",
        "authors": [
            "Zhijie Huang",
            "Xiaopeng Guo",
            "Mingyu Shang",
            "Jie Gao",
            "Jun Sun"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In this paper, a novel QP variable convolutional neural network based in-loop\nfilter is proposed for VVC intra coding. To avoid training and deploying\nmultiple networks, we develop an efficient QP attention module (QPAM) which can\ncapture compression noise levels for different QPs and emphasize meaningful\nfeatures along channel dimension. Then we embed QPAM into the residual block,\nand based on it, we design a network architecture that is equipped with\ncontrollability for different QPs. To make the proposed model focus more on\nexamples that have more compression artifacts or is hard to restore, a focal\nmean square error (MSE) loss function is employed to fine tune the network.\nExperimental results show that our approach achieves 4.03\\% BD-Rate saving on\naverage for all intra configuration, which is even better than QP-separate CNN\nmodels while having less model parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.15003v1"
    },
    {
        "title": "Deep Learning-based Forgery Attack on Document Images",
        "authors": [
            "Lin Zhao",
            "Changsheng Chen",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  With the ongoing popularization of online services, the digital document\nimages have been used in various applications. Meanwhile, there have emerged\nsome deep learning-based text editing algorithms which alter the textual\ninformation of an image . In this work, we present a document forgery algorithm\nto edit practical document images. To achieve this goal, the limitations of\nexisting text editing algorithms towards complicated characters and complex\nbackground are addressed by a set of network design strategies. First, the\nunnecessary confusion in the supervision data is avoided by disentangling the\ntextual and background information in the source images. Second, to capture the\nstructure of some complicated components, the text skeleton is provided as\nauxiliary information and the continuity in texture is considered explicitly in\nthe loss function. Third, the forgery traces induced by the text editing\noperation are mitigated by some post-processing operations which consider the\ndistortions from the print-and-scan channel. Quantitative comparisons of the\nproposed method and the exiting approach have shown the advantages of our\ndesign by reducing the about 2/3 reconstruction error measured in MSE,\nimproving reconstruction quality measured in PSNR and in SSIM by 4 dB and 0.21,\nrespectively. Qualitative experiments have confirmed that the reconstruction\nresults of the proposed method are visually better than the existing approach.\nMore importantly, we have demonstrated the performance of the proposed document\nforgery algorithm under a practical scenario where an attacker is able to alter\nthe textual information in an identity document using only one sample in the\ntarget domain. The forged-and-recaptured samples created by the proposed text\nediting attack and recapturing operation have successfully fooled some existing\ndocument authentication systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.00653v1"
    },
    {
        "title": "Efficient Compressed Sensing Based Image Coding by Using Gray\n  Transformation",
        "authors": [
            "Bo Zhang",
            "Di Xiao",
            "Lan Wang",
            "Sen Bai",
            "Lei Yang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In recent years, compressed sensing (CS) based image coding has become a hot\ntopic in image processing field. However, since the bit depth required for\nencoding each CS sample is too large, the compression performance of this\nparadigm is unattractive. To address this issue, a novel CS-based image coding\nsystem by using gray transformation is proposed. In the proposed system, we use\na gray transformation to preprocess the original image firstly and then use CS\nto sample the transformed image. Since gray transformation makes the\nprobability distribution of CS samples centralized, the bit depth required for\nencoding each CS sample is reduced significantly. Consequently, the proposed\nsystem can considerably improve the compression performance of CS-based image\ncoding. Simulation results show that the proposed system outperforms the\ntraditional one without using gray transformation in terms of compression\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.01272v1"
    },
    {
        "title": "Multi-color balancing for correctly adjusting the intensity of target\n  colors",
        "authors": [
            "Teruaki Akazawa",
            "Yuma Kinoshita",
            "Hitoshi Kiya"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In this paper, we propose a novel multi-color balance method for reducing\ncolor distortions caused by lighting effects. The proposed method allows us to\nadjust three target-colors chosen by a user in an input image so that each\ntarget color is the same as the corresponding destination (benchmark) one. In\ncontrast, white balancing is a typical technique for reducing the color\ndistortions, however, they cannot remove lighting effects on colors other than\nwhite. In an experiment, the proposed method is demonstrated to be able to\nremove lighting effects on selected three colors, and is compared with existing\nwhite balance adjustments.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.01893v2"
    },
    {
        "title": "High-Capacity Framework for Reversible Data Hiding in Encrypted Image\n  Using Pixel Predictions and Entropy Encoding",
        "authors": [
            "Yingqiang Qiu",
            "Qichao Ying",
            "Yuyan Yang",
            "Huanqiang Zeng",
            "Sheng Li",
            "Zhenxing Qian"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  While the existing vacating room before encryption (VRBE) based schemes can\nachieve decent embedding rate, the payloads of the existing vacating room after\nencryption (VRAE) based schemes are relatively low. To address this issue, this\npaper proposes a generalized framework for high-capacity RDHEI for both VRBE\nand VRAE cases. First, an efficient embedding room generation algorithm (ERGA)\nis designed to produce large embedding room by using pixel prediction and\nentropy encoding. Then, we propose two RDHEI schemes, one for VRBE, another for\nVRAE. In the VRBE scenario, the image owner generates the embedding room with\nERGA and encrypts the preprocessed image by using the stream cipher with two\nencryption keys. Then, the data hider locates the embedding room and embeds the\nencrypted additional data. In the VRAE scenario, the cover image is encrypted\nby an improved block modulation and permutation encryption algorithm, where the\nspatial redundancy in the plain-text image is largely preserved. Then, the data\nhider applies ERGA on the encrypted image to generate the embedding room and\nconducts data embedding. For both schemes, the receivers with different\nauthentication keys can respectively conduct error-free data extraction and/or\nerror-free image recovery. The experimental results show that the two proposed\nschemes outperform many state-of-the-art RDHEI arts. Besides, the schemes can\nensure high security level, where the original image can be hardly discovered\nfrom the encrypted version before and after data hiding by the unauthorized\nuser.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12613v4"
    },
    {
        "title": "High-Capacity Reversible Data Hiding in Encrypted Images using Adaptive\n  Encoding",
        "authors": [
            "Wenjing Ma",
            "Youqing Wu",
            "Zhaoxia Yin"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  With the popularization of digital information technology, the reversible\ndata hiding in encrypted images (RDHEI) has gradually become the research\nhotspot of privacy protection in cloud storage. As a technology which can embed\nadditional information in encrypted domain, extract the embedded information\ncorrectly and recover the original image without loss, RDHEI has been widely\npaid attention by researchers. To embed sufficient additional information in\nthe encrypted image, a high-capacity RDHEI method using adaptive encoding is\nproposed in this paper. Firstly, the occurrence frequency of different\nprediction errors of the original image is calculated and the corresponding\nadaptive Huffman coding is generated. Then, the original image is encrypted\nwith stream cipher and the encrypted pixels are marked with different Huffman\ncodewords according to the prediction errors. Finally, additional information\nis embedded in the reserved room of marked pixels by bit substitution. The\nexperimental results show that the proposed algorithm can extract the embedded\ninformation correctly and recover the original image losslessly. Compared with\nsimilar algorithms, the proposed algorithm makes full use of the\ncharacteristics of the image itself and greatly improves the embedding rate of\nthe image. On UCID, BOSSBase, and BOWS-2 datasets, the average embedding rate\nof the proposed algorithm reaches 3.162 bpp, 3.917 bpp, and 3.775 bpp, which is\nhigher than the state-of-the-art algorithm of 0.263 bpp, 0.292 bpp, and 0.280\nbpp, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12620v2"
    },
    {
        "title": "PARIMA: Viewport Adaptive 360-Degree Video Streaming",
        "authors": [
            "Lovish Chopra",
            "Sarthak Chakraborty",
            "Abhijit Mondal",
            "Sandip Chakraborty"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  With increasing advancements in technologies for capturing 360{\\deg} videos,\nadvances in streaming such videos have become a popular research topic.\nHowever, streaming 360{\\deg} videos require high bandwidth, thus escalating the\nneed for developing optimized streaming algorithms. Researchers have proposed\nvarious methods to tackle the problem, considering the network bandwidth or\nattempt to predict future viewports in advance. However, most of the existing\nworks either (1) do not consider video contents to predict user viewport, or\n(2) do not adapt to user preferences dynamically, or (3) require a lot of\ntraining data for new videos, thus making them potentially unfit for video\nstreaming purposes. We develop PARIMA, a fast and efficient online viewport\nprediction model that uses past viewports of users along with the trajectories\nof prime objects as a representative of video content to predict future\nviewports. We claim that the head movement of a user majorly depends upon the\ntrajectories of the prime objects in the video. We employ a pyramid-based\nbitrate allocation scheme and perform a comprehensive evaluation of the\nperformance of PARIMA. In our evaluation, we show that PARIMA outperforms\nstate-of-the-art approaches, improving the Quality of Experience by over 30\\%\nwhile maintaining a short response time.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.00981v3"
    },
    {
        "title": "User Generated HDR Gaming Video Streaming: Dataset, Codec Comparison and\n  Challenges",
        "authors": [
            "Nabajeet Barman",
            "Maria G Martini"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Gaming video streaming services have grown tremendously in the past few\nyears, with higher resolutions, higher frame rates and HDR gaming videos\ngetting increasingly adopted among the gaming community. Since gaming content\nas such is different from non-gaming content, it is imperative to evaluate the\nperformance of the existing encoders to help understand the bandwidth\nrequirements of such services, as well as further improve the compression\nefficiency of such encoders. Towards this end, we present in this paper\nGamingHDRVideoSET, a dataset consisting of eighteen 10-bit UHD-HDR gaming\nvideos and encoded video sequences using four different codecs, together with\ntheir objective evaluation results. The dataset is available online at [to be\nadded after paper acceptance]. Additionally, the paper discusses the codec\ncompression efficiency of most widely used practical encoders, i.e., x264\n(H.264/AVC), x265 (H.265/HEVC) and libvpx (VP9), as well the recently proposed\nencoder libaom (AV1), on 10-bit, UHD-HDR content gaming content. Our results\nshow that the latest compression standard AV1 results in the best compression\nefficiency, followed by HEVC, H.264, and VP9.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.02189v1"
    },
    {
        "title": "Reversible Data Hiding Associated with Digital Halftoning That Allows\n  Printing with Special Color Ink by Using Single Color Layer",
        "authors": [
            "Minagi Ueda",
            "Shoko Imaizumi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We propose an efficient framework of reversible data hiding to preserve\ncompatibility between normal printing and printing with a special color ink by\nusing a single common image. The special color layer is converted to a binary\nimage by digital halftoning and losslessly compressed using JBIG2. Then, the\ncompressed information of the binarized special color layer is reversibly\nembedded into the general color layer without significant distortion. Our\nexperimental results show the availability of the proposed method in terms of\nthe marked image quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.02453v1"
    },
    {
        "title": "Application of Reversible Data Hiding for Printing with Special Color\n  Inks to Preserve Compatibility with Normal Printing",
        "authors": [
            "Kotoko Hiraoka",
            "Kensuke Fukumoto",
            "Takashi Yamazoe",
            "Norimichi Tsumura",
            "Satoshi Kaneko",
            "Wataru Arai",
            "Shoko Imaizumi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We propose an efficient framework with compatibility between normal printing\nand printing with special color inks in this paper. Special color inks can be\nused for printing to represent some particular colors and specific optical\nproperties, which are difficult to express using only CMYK inks. Special color\nlayers are required in addition to the general color layer for printing with\nspecial color inks. We introduce a reversible data hiding (RDH) method to embed\nthe special color layers into the general color layer without visible\nartifacts. The proposed method can realize both normal printing and printing\nwith special color inks by using a single layer. Our experimental results show\nthat the quality of the marked image is virtually identical to that of the\noriginal image, i.e., the general color layer.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.02777v1"
    },
    {
        "title": "Semantics-Consistent Representation Learning for Remote Sensing\n  Image-Voice Retrieval",
        "authors": [
            "Hailong Ning",
            "Bin Zhao",
            "Yuan Yuan"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  With the development of earth observation technology, massive amounts of\nremote sensing (RS) images are acquired. To find useful information from these\nimages, cross-modal RS image-voice retrieval provides a new insight. This paper\naims to study the task of RS image-voice retrieval so as to search effective\ninformation from massive amounts of RS data. Existing methods for RS\nimage-voice retrieval rely primarily on the pairwise relationship to narrow the\nheterogeneous semantic gap between images and voices. However, apart from the\npairwise relationship included in the datasets, the intra-modality and\nnon-paired inter-modality relationships should also be taken into account\nsimultaneously, since the semantic consistency among non-paired representations\nplays an important role in the RS image-voice retrieval task. Inspired by this,\na semantics-consistent representation learning (SCRL) method is proposed for RS\nimage-voice retrieval. The main novelty is that the proposed method takes the\npairwise, intra-modality, and non-paired inter-modality relationships into\naccount simultaneously, thereby improving the semantic consistency of the\nlearned representations for the RS image-voice retrieval. The proposed SCRL\nmethod consists of two main steps: 1) semantics encoding and 2)\nsemantics-consistent representation learning. Firstly, an image encoding\nnetwork is adopted to extract high-level image features with a transfer\nlearning strategy, and a voice encoding network with dilated convolution is\ndevised to obtain high-level voice features. Secondly, a consistent\nrepresentation space is conducted by modeling the three kinds of relationships\nto narrow the heterogeneous semantic gap and learn semantics-consistent\nrepresentations across two modalities. Extensive experimental results on three\nchallenging RS image-voice datasets show the effectiveness of the proposed\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.05302v1"
    },
    {
        "title": "GraphIQA: Learning Distortion Graph Representations for Blind Image\n  Quality Assessment",
        "authors": [
            "Simeng Sun",
            "Tao Yu",
            "Jiahua Xu",
            "Wei Zhou",
            "Zhibo Chen"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  A good distortion representation is crucial for the success of deep blind\nimage quality assessment (BIQA). However, most previous methods do not\neffectively model the relationship between distortions or the distribution of\nsamples with the same distortion type but different distortion levels. In this\nwork, we start from the analysis of the relationship between perceptual image\nquality and distortion-related factors, such as distortion types and levels.\nThen, we propose a Distortion Graph Representation (DGR) learning framework for\nIQA, named GraphIQA, in which each distortion is represented as a graph, i.e.,\nDGR. One can distinguish distortion types by learning the contrast relationship\nbetween these different DGRs, and infer the ranking distribution of samples\nfrom different levels in a DGR. Specifically, we develop two sub-networks to\nlearn the DGRs: a) Type Discrimination Network (TDN) that aims to embed DGR\ninto a compact code for better discriminating distortion types and learning the\nrelationship between types; b) Fuzzy Prediction Network (FPN) that aims to\nextract the distributional characteristics of the samples in a DGR and predicts\nfuzzy degrees based on a Gaussian prior. Experiments show that our GraphIQA\nachieves the state-of-the-art performance on many benchmark datasets of both\nsynthetic and authentic distortions.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.07666v4"
    },
    {
        "title": "Quantum-inspired Multimodal Fusion for Video Sentiment Analysis",
        "authors": [
            "Qiuchi Li",
            "Dimitris Gkoumas",
            "Christina Lioma",
            "Massimo Melucci"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We tackle the crucial challenge of fusing different modalities of features\nfor multimodal sentiment analysis. Mainly based on neural networks, existing\napproaches largely model multimodal interactions in an implicit and\nhard-to-understand manner. We address this limitation with inspirations from\nquantum theory, which contains principled methods for modeling complicated\ninteractions and correlations. In our quantum-inspired framework, the word\ninteraction within a single modality and the interaction across modalities are\nformulated with superposition and entanglement respectively at different\nstages. The complex-valued neural network implementation of the framework\nachieves comparable results to state-of-the-art systems on two benchmarking\nvideo sentiment analysis datasets. In the meantime, we produce the unimodal and\nbimodal sentiment directly from the model to interpret the entangled decision.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.10572v2"
    },
    {
        "title": "Edge-Cloud Collaboration Enabled Video Service Enhancement: A Hybrid\n  Human-Artificial Intelligence Scheme",
        "authors": [
            "Dapeng Wu",
            "Ruili Bao",
            "Zhidu Li",
            "Honggang Wang",
            "Ruyan Wang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In this paper, a video service enhancement strategy is investigated under an\nedge-cloud collaboration framework, where video caching and delivery decisions\nare made in the cloud and edge respectively. We aim to guarantee the user\nfairness in terms of video coding rate under statistical delay constraint and\nedge caching capacity constraint. A hybrid human-artificial intelligence\napproach is developed to improve the user hit rate for video caching.\nSpecifically, individual user interest is first characterized by merging\nfactorization machine (FM) model and multi-layer perceptron (MLP) model, where\nboth low-order and high-order features can be well learned simultaneously.\nThereafter, a social aware similarity model is constructed to transferred\nindividual user interest to group interest, based on which, videos can be\nselected to cache. Furthermore, a double bisection exploration scheme is\nproposed to optimize wireless resource allocation and video coding rate. The\neffectiveness of the proposed video caching scheme and video delivery scheme is\nfinally validated by extensive experiments with a real-world data set.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.12516v1"
    },
    {
        "title": "3-D Markerless Tracking of Human Gait by Geometric Trilateration of\n  Multiple Kinects",
        "authors": [
            "Lin Yang",
            "Bowen Yang",
            "Haiwei Dong",
            "Abdulmotaleb El Saddik"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this paper, we develop an integrated markerless gait tracking system with\nthree Kinect v2 sensors. A geometric principle-based trilateration method is\nproposed for optimizing the accuracy of the measured gait data. To tackle the\ndata synchronization problem among the Kinect clients and the server, a\nsynchronization mechanism based on NTP (Network Time Protocol) is designed for\nsynchronizing the server and Kinect clients' clocks. Furthermore, a time\nschedule is designed for timing each Kinect client's data transmission. In the\nexperiment, participants are asked to perform a 60 s walk while the proposed\ntracking system obtains the participant's gait data. Six joints (including left\nhip, right hip, left knee, right knee, left ankle and right ankle) of the\nparticipants are tracked where the obtained gait data are described as 6000\n{movements} of joint positions (1000 {movements} for each joint). The results\nshow that the trilateration tracking result by the three Kinect sensors has a\nmuch higher accuracy compared with the accuracy measured by a single Kinect\nsensor. Within a randomly sampled time period (67.726 s in the experiment),\n98.37% of the frames generated by the gait tracking system have timing errors\nless than 1 ms, which is much better than the default NTP service embedded in\nthe Windows 8.1 operating system. The accuracy of the proposed system is\nquantitatively evaluated and verified by a comparison with a commercial medical\nsystem (Delsys Trigno Smart Sensor System).\n",
        "pdf_link": "http://arxiv.org/pdf/2301.00726v1"
    },
    {
        "title": "From Ember to Blaze: Swift Interactive Video Adaptation via\n  Meta-Reinforcement Learning",
        "authors": [
            "Xuedou Xiao",
            "Mingxuan Yan",
            "Yingying Zuo",
            "Boxi Liu",
            "Paul Ruan",
            "Yang Cao",
            "Wei Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Maximizing quality of experience (QoE) for interactive video streaming has\nbeen a long-standing challenge, as its delay-sensitive nature makes it more\nvulnerable to bandwidth fluctuations. While reinforcement learning (RL) has\ndemonstrated great potential, existing works are either limited by fixed models\nor require enormous data/time for online adaptation, which struggle to fit\ntime-varying and diverse network states. Driven by these practical concerns, we\nperform large-scale measurements on WeChat for Business's interactive video\nservice to study real-world network fluctuations. Surprisingly, our analysis\nshows that, compared to time-varying network metrics, network sequences exhibit\nnoticeable short-term continuity, sufficient for few-shot learning\nrequirements. We thus propose Fiammetta, the first meta-RL-based bitrate\nadaptation algorithm for interactive video streaming. Building on the\nshort-term continuity, Fiammetta accumulates learning experiences through\noffline meta-training and enables fast online adaptation to changing network\nstates through a few gradient updates. Moreover, Fiammetta innovatively\nincorporates a probing mechanism for real-time monitoring of network states,\nand proposes an adaptive meta-testing mechanism for seamless adaptation. We\nimplement Fiammetta on a testbed whose end-to-end network follows the\nreal-world WeChat for Business traces. The results show that Fiammetta\noutperforms prior algorithms significantly, improving video bitrate by\n3.6%-16.2% without increasing stalling rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.05541v1"
    },
    {
        "title": "Top-down and bottom-up approaches to video Quality of Experience\n  studies; overview and proposal of a new model",
        "authors": [
            "Kamil Koniuch",
            "Sabina Baraković",
            "Jasmina Baraković Husić",
            "Katrien De Moor",
            "Lucjan Janowski",
            "Michał Wierzchoń"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Modern video streaming services require quality assurance of the presented\naudiovisual material. Quality assurance mechanisms allow streaming platforms to\nprovide quality levels that are considered sufficient to yield user\nsatisfaction, with the least possible amount of data transferred. A variety of\nmeasures and approaches have been developed to control video quality, e.g., by\nadapting it to network conditions. These include objective matrices of the\nquality and thresholds identified by means of subjective perceptual judgments.\nThe former group of matrices has recently gained the attention of (multi)media\nresearchers. They call this area of study ``Quality of Experience'' (QoE). In\nthis paper, we present a review of QoE's theoretical models together with a\ndiscussion of their properties and implications for the field. We argue that\nmost of them represent the bottom-up approach to modeling. Such models focus on\ndescribing as many variables as possible, but with a limited ability to\ninvestigate the causal relationship between them; therefore, the applicability\nof the findings in practice is limited. To advance the field, we therefore\npropose a structural, top-down model of video QoE that describes causal\nrelationships among variables. We hope that our framework will facilitate\ndesigning comparable experiments in the domain.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.11648v1"
    },
    {
        "title": "Multi-resolution encoding and optimization for next generation video\n  compression",
        "authors": [
            "Vignesh V Menon"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Multi-encoding implies encoding the same content in multiple spatial\nresolutions and multiple bitrates. This work evaluates the encoder analysis\ncorrelations across 2160p, 1080p, and 540p encodings of the same video for\nconventional ABR bitrates. A multi-resolution tier multi-ABR encoding scheme is\nmodeled and evaluated, which significantly improves the computational\nefficiency of conventional ABR encoding. Video content is first encoded at the\nlower resolution with the median bitrate. Encoder analysis decisions, such as\nmotion vectors and CU block structure, are then used in the other encodes in\nthe same resolution tier. The analysis is then extrapolated and refined to be\nused in higher-resolution encodes. The scheme is validated using x265 HEVC\nvideo encoder. The proposed multi-resolution tier multi-bitrate encoding scheme\nachieves overall speed-ups of up to 2.5x, compared to the conventional\nsingle-instance encoding approach. Furthermore, this speed-up is achieved\nwithout substantial losses in coding efficiency. SIMD Vector units in CPUs have\nbecome the de-facto standard for accelerating media and other kernels that\nexhibit parallelism. This work also demonstrates the impact of hardware-aware\noptimizations on the encoding speeds of the next-generation video codecs. The\nwork is evaluated using the Arowana XVC encoder.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.12191v1"
    },
    {
        "title": "Towards Better Quality of Experience in HTTP Adaptive Streaming",
        "authors": [
            "Babak Taraghi",
            "Selina Zoë Haack",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  HTTP Adaptive Streaming (HAS) is nowadays a popular solution for multimedia\ndelivery. The novelty of HAS lies in the possibility of continuously adapting\nthe streaming session to current network conditions, facilitated by Adaptive\nBitrate (ABR) algorithms. Various popular streaming and Video on Demand\nservices such as Netflix, Amazon Prime Video, and Twitch use this method. Given\nthis broad consumer base, ABR algorithms continuously improve to increase user\nsatisfaction. The insights for these improvements are, among others, gathered\nwithin the research area of Quality of Experience (QoE). Within this field,\nvarious researchers have dedicated their works to identifying potential\nimpairments and testing their impact on viewers' QoE. Two frequently discussed\nvisual impairments influencing QoE are stalling events and quality switches. So\nfar, it is commonly assumed that those stalling events have the worst impact on\nQoE. This paper challenged this belief and reviewed this assumption by\ncomparing stalling events with multiple quality and high amplitude quality\nswitches. Two subjective studies were conducted. During the first subjective\nstudy, participants received a monetary incentive, while the second subjective\nstudy was carried out with volunteers. The statistical analysis demonstrated\nthat stalling events do not result in the worst degradation of QoE. These\nfindings suggest that a reevaluation of the effect of stalling events in QoE\nresearch is needed. Therefore, these findings may be used for further research\nand to improve current adaptation strategies in ABR algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.13523v2"
    },
    {
        "title": "A Closer Look into Recent Video-based Learning Research: A Comprehensive\n  Review of Video Characteristics, Tools, Technologies, and Learning\n  Effectiveness",
        "authors": [
            "Evelyn Navarrete",
            "Andreas Nehring",
            "Sascha Schanze",
            "Ralph Ewerth",
            "Anett Hoppe"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  People increasingly use videos on the Web as a source for learning. To\nsupport this way of learning, researchers and developers are continuously\ndeveloping tools, proposing guidelines, analyzing data, and conducting\nexperiments. However, it is still not clear what characteristics a video should\nhave to be an effective learning medium. In this paper, we present a\ncomprehensive review of 257 articles on video-based learning for the period\nfrom 2016 to 2021. One of the aims of the review is to identify the video\ncharacteristics that have been explored by previous work. Based on our\nanalysis, we suggest a taxonomy which organizes the video characteristics and\ncontextual aspects into eight categories: (1) audio features, (2) visual\nfeatures, (3) textual features, (4) instructor behavior, (5) learners\nactivities, (6) interactive features (quizzes, etc.), (7) production style, and\n(8) instructional design. Also, we identify four representative research\ndirections: (1) proposals of tools to support video-based learning, (2) studies\nwith controlled experiments, (3) data analysis studies, and (4) proposals of\ndesign guidelines for learning videos. We find that the most explored\ncharacteristics are textual features followed by visual features, learner\nactivities, and interactive features. Text of transcripts, video frames, and\nimages (figures and illustrations) are most frequently used by tools that\nsupport learning through videos. The learner activity is heavily explored\nthrough log files in data analysis studies, and interactive features have been\nfrequently scrutinized in controlled experiments. We complement our review by\ncontrasting research findings that investigate the impact of video\ncharacteristics on the learning effectiveness, report on tasks and technologies\nused to develop tools that support learning, and summarize trends of design\nguidelines to produce learning videos\n",
        "pdf_link": "http://arxiv.org/pdf/2301.13617v2"
    },
    {
        "title": "Interactive multiview video system with non-complex navigation at the\n  decoder",
        "authors": [
            "Thomas Maugey",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Multiview video with interactive and smooth view switching at the receiver is\na challenging application with several issues in terms of effective use of\nstorage and bandwidth resources, reactivity of the system, quality of the\nviewing experience and system complexity. The classical decoding system for\ngenerating virtual views first projects a reference or encoded frame to a given\nviewpoint and then fills in the holes due to potential occlusions. This last\nstep still constitutes a complex operation with specific software or hardware\nat the receiver and requires a certain quantity of information from the\nneighboring frames for insuring consistency between the virtual images. In this\nwork we propose a new approach that shifts most of the burden due to\ninteractivity from the decoder to the encoder, by anticipating the navigation\nof the decoder and sending auxiliary information that guarantees temporal and\ninterview consistency. This leads to an additional cost in terms of\ntransmission rate and storage, which we minimize by using optimization\ntechniques based on the user behavior modeling. We show by experiments that the\nproposed system represents a valid solution for interactive multiview systems\nwith classical decoders.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.0598v1"
    },
    {
        "title": "Stereo image Transference & Retrieval over SMS",
        "authors": [
            "Muhammad Fahad Khan",
            "Saira Beg"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Paper presents the way of transferring stereo images using SMS over GSM\nnetwork. Generally, Stereo image is composed of two stereoscopic images in such\nway that gives three dimensional affect when viewed. GSM have two short\nmessaging services, which can transfer images and sounds etc. Such services are\nknown as; MMS (Multimedia Messaging Service) and EMS (Extended Messaging\nService). EMS can send Predefined sounds, animation and images but have\nlimitation that it does not support widely. MMS can send much higher contents\nthan EMS but need 3G and other network capability in order to send large size\ndata up to 1000 bytes. Other limitations are Portability, content adaption etc.\nOur major aim in this paper is to provide an alternative way of sending stereo\nimages over SMS which is widely supported than EMS. We develop an application\nusing J2ME Platform.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.1383v1"
    },
    {
        "title": "Identifying and Analysis of Scene Mining Methods Beased on Scenes\n  Extracted Features",
        "authors": [
            "Ashraf Sadat Jabari",
            "Mohammadreza Keyvanpour"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Scene mining is a subset of image mining in which scenes are classified to a\ndistinct set of classes based on analysis of their content. In other word in\nscene mining, a label is given to visual content of scene, for example,\nmountain, beach. Scene mining is used in applications such as medicine, movie,\ninformation retrieval, computer vision, recognition of traffic scene. Reviewing\nof represented methods shows there are various methods in scene mining. Scene\nmining applications extension and existence of various scenes, make comparison\nof methods hard. Scene mining can be followed by identifying scene mining\ncomponents and representing a framework to analyzing and evaluating methods. In\nthis paper, at first, components of scene mining are introduced, then a\nframework based on extracted features of scene is represented to classify scene\nmining methods. Finally, these methods are analyzed and evaluated via a\nproposal framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.1668v1"
    },
    {
        "title": "Throughput Scaling Of Convolution For Error-Tolerant Multimedia\n  Applications",
        "authors": [
            "Mohammad Ashraful Anam",
            "Yiannis Andreopoulos"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Convolution and cross-correlation are the basis of filtering and pattern or\ntemplate matching in multimedia signal processing. We propose two throughput\nscaling options for any one-dimensional convolution kernel in programmable\nprocessors by adjusting the imprecision (distortion) of computation. Our\napproach is based on scalar quantization, followed by two forms of tight\npacking in floating-point (one of which is proposed in this paper) that allow\nfor concurrent calculation of multiple results. We illustrate how our approach\ncan operate as an optional pre- and post-processing layer for off-the-shelf\noptimized convolution routines. This is useful for multimedia applications that\nare tolerant to processing imprecision and for cases where the input signals\nare inherently noisy (error tolerant multimedia applications). Indicative\nexperimental results with a digital music matching system and an MPEG-7 audio\ndescriptor system demonstrate that the proposed approach offers up to 175%\nincrease in processing throughput against optimized (full-precision)\nconvolution with virtually no effect in the accuracy of the results. Based on\nmarginal statistics of the input data, it is also shown how the throughput and\ndistortion can be adjusted per input block of samples under constraints on the\nsignal-to-noise ratio against the full-precision convolution.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3018v1"
    },
    {
        "title": "An Authoring System for Editing Lessons in Phonetic English in SMIL3.0",
        "authors": [
            "G. Merzougui",
            "M. Djoudi"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  One of the difficulties of teaching English is the prosody, including the\nstress. French learners have difficulties to encode this information about the\nword because it is irrelevant for them. Therefore, they have difficulty to\nproduce this stress when they speak that language. Studies in this area have\nconcluded that the dual-coding approach (auditory and visual) of a phonetic\nphenomenon helps a lot to improve its perception and memorization for novice\nlearners. The aim of our work is to provide English teachers with an authoring\nnamed SaCoPh for editing multimedia courses that support this approach. This\ncourse is based on a template that fits the educational aspects of phonetics,\nexploiting the features of version 3.0 of the standard SMIL (Synchronized\nMultimedia Integration Language) for the publication of this course on the web.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.5285v1"
    },
    {
        "title": "A Study of Various Steganographic Techniques Used for Information Hiding",
        "authors": [
            "C. P. Sumathi",
            "T. Santanam",
            "G. Umamaheswari"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Steganography derives from the Greek word steganos, meaning covered or\nsecret, and graphy (writing or drawing). Steganography is a technology where\nmodern data compression, information theory, spread spectrum, and cryptography\ntechnologies are brought together to satisfy the need for privacy on the\nInternet. This paper is an attempt to analyse the various techniques used in\nsteganography and to identify areas in which this technique can be applied, so\nthat the human race can be benefited at large.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.5561v1"
    },
    {
        "title": "Control of Multiple Remote Servers for Quality-Fair Delivery of\n  Multimedia Contents",
        "authors": [
            "Nesrine Changuel",
            "Bessem Sayadi",
            "Michel Kieffer"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  This paper proposes a control scheme for the quality-fair delivery of several\nencoded video streams to mobile users sharing a common wireless resource. Video\nquality fairness, as well as similar delivery delays are targeted among\nstreams. The proposed controller is implemented within some aggregator located\nnear the bottleneck of the network. The transmission rate among streams is\nadapted based on the quality of the already encoded and buffered packets in the\naggregator. Encoding rate targets are evaluated by the aggregator and fed back\nto each remote video server (fully centralized solution), or directly evaluated\nby each server in a distributed way (partially distributed solution). Each\nencoding rate target is adjusted for each stream independently based on the\ncorresponding buffer level or buffering delay in the aggregator. Communication\ndelays between the servers and the aggregator are taken into account. The\ntransmission and encoding rate control problems are studied with a\ncontrol-theoretic perspective. The system is described with a multi-input\nmulti-output model. Proportional Integral (PI) controllers are used to adjust\nthe video quality and control the aggregator buffer levels. The system\nequilibrium and stability properties are studied. This provides guidelines for\nchoosing the parameters of the PI controllers. Experimental results show the\nconvergence of the proposed control system and demonstrate the improvement in\nvideo quality fairness compared to a classical transmission rate fair streaming\nsolution and to a utility max-min fair approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.6361v1"
    },
    {
        "title": "An Adaptive Watermarking Process in Hadamard Transform",
        "authors": [
            "Parvathavarthini S.",
            "Shanthakumari R"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  An adaptive visible/invisible watermarking scheme is done to prevent the\nprivacy and preserving copyright protection of digital data using Hadamard\ntransform based on the scaling factor of the image. The value of scaling factor\ndepends on the control parameter. The scaling factor is calculated to embedded\nthe watermark. Depend upon the control parameter the visible and invisible\nwatermarking is determined. The proposed Hadamard transform domain method is\nmore robust again image/signal processing attacks. Furthermore, it also shows\nthat the proposed method confirm the efficiency through various performance\nanalysis and experimental results.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.3207v1"
    },
    {
        "title": "YouTube QoE Evaluation Tool for Android Wireless Terminals",
        "authors": [
            "Gerardo Gomez",
            "Lorenzo Hortiguela",
            "Quiliano Perez",
            "Javier Lorca",
            "Raquel Garcia",
            "Mari Carmen Aguayo-Torres"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  In this paper, we present an Android application which is able to evaluate\nand analyze the perceived Quality of Experience (QoE) for YouTube service in\nwireless terminals. To achieve this goal, the application carries out\nmeasurements of objective Quality of Service (QoS) parameters, which are then\nmapped onto subjective QoE (in terms of Mean Opinion Score, MOS) by means of a\nutility function. Our application also informs the user about potential causes\nthat lead to a low MOS as well as provides some hints to improve it. After each\nYouTube session, the users may optionally qualify the session through an online\nopinion survey. This information has been used in a pilot experience to\ncorrelate the theoretical QoE model with real user feedback. Results from such\nan experience have shown that the theoretical model (taken from the literature)\nprovides slightly more pessimistic results compared to user feedback. Users\nseem to be more indulgent with wireless connections, increasing the MOS from\nthe opinion survey in about 20% compared to the theoretical model, which was\nobtained from wired scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.4709v1"
    },
    {
        "title": "Block matching algorithm based on Differential Evolution for motion\n  estimation",
        "authors": [
            "Erik Cuevas",
            "Daniel Zaldivar",
            "Marco Perez-Cisneros",
            "Diego Oliva"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Motion estimation is one of the major problems in developing video coding\napplications. Among all motion estimation approaches, Block matching (BM)\nalgorithms are the most popular methods due to their effectiveness and\nsimplicity for both software and hardware implementations. A BM approach\nassumes that the movement of pixels within a defined region of the current\nframe (Macro-Block, MB) can be modeled as a translation of pixels contained in\nthe previous frame. In this procedure, the motion vector is obtained by\nminimizing the sum of absolute differences (SAD) produced by the MB of the\ncurrent frame over a determined search window from the previous frame. The SAD\nevaluation is computationally expensive and represents the most consuming\noperation in the BM process. The most straightforward BM method is the full\nsearch algorithm (FSA) which finds the most accurate motion vector, calculating\nexhaustively the SAD values for all elements of the search window. Over this\ndecade, several fast BM algorithms have been proposed to reduce the number of\nSAD operations by calculating only a fixed subset of search locations at the\nprice of a poor accuracy. In this paper, a new algorithm based on Differential\nEvolution (DE) is proposed to reduce the number of search locations in the BM\nprocess. In order to avoid computing several search locations, the algorithm\nestimates the SAD values (fitness) for some locations using the SAD values of\npreviously calculated neighboring positions. Since the proposed algorithm does\nnot consider any fixed search pattern or other different assumption, a high\nprobability for finding the true minimum (accurate motion vector) is expected.\nIn comparison to other fast BM algorithms, the proposed method deploys more\naccurate motion vectors yet delivering competitive time rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.4721v1"
    },
    {
        "title": "A hybrid video quality metric for analyzing quality degradation due to\n  frame drop",
        "authors": [
            "Manish K Thakur",
            "Vikas Saxena",
            "J P Gupta"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  In last decade, ever growing internet technologies provided platform to share\nthe multimedia data among different communities. As the ultimate users are\nhuman subjects who are concerned about quality of visual information, it is\noften desired to have good resumed perceptual quality of videos, thus arises\nthe need of quality assessment. This paper presents a full reference hybrid\nvideo quality metric which is capable to analyse the video quality for\nspatially or temporally (frame drop) or spatio-temporally distorted video\nsequences. Simulated results show that the metric efficiently analyses the\nquality degradation and more closer to the developed human visual system\n",
        "pdf_link": "http://arxiv.org/pdf/1405.5340v1"
    },
    {
        "title": "Low-complexity video encoder for smart eyes based on underdetermined\n  blind signal separation",
        "authors": [
            "Jing Liu",
            "Fei Qiao",
            "Zhijian Ou",
            "Huazhong Yang"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  This paper presents a low complexity video coding method based on\nUnderdetermined Blind Signal Separation (UBSS). The detailed coding framework\nis designed. Three key techniques are proposed to enhance the compression ratio\nand the quality of the decoded frames. The experiments validate that the\nproposed method costs 30ms encoding time less than DISCOVER. The simulation\nshows that this new method can save 50% energy compared with H.264.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.5948v1"
    },
    {
        "title": "Jpeg Image Compression Using Discrete Cosine Transform - A Survey",
        "authors": [
            "A. M. Raid",
            "W. M. Khedr",
            "M. A. El-dosuky",
            "Wesam Ahmed"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Due to the increasing requirements for transmission of images in computer,\nmobile environments, the research in the field of image compression has\nincreased significantly. Image compression plays a crucial role in digital\nimage processing, it is also very important for efficient transmission and\nstorage of images. When we compute the number of bits per image resulting from\ntypical sampling rates and quantization methods, we find that Image compression\nis needed. Therefore development of efficient techniques for image compression\nhas become necessary .This paper is a survey for lossy image compression using\nDiscrete Cosine Transform, it covers JPEG compression algorithm which is used\nfor full-colour still image applications and describes all the components of\nit.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.6147v1"
    },
    {
        "title": "A scenario based approach for dealing with challenges in a pervasive\n  computing environment",
        "authors": [
            "Divyajyothi M G",
            " Rachappa",
            "D H Rao"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  With the surge in modern research focus towards Pervasive Computing, lot of\ntechniques and challenges needs to be addressed so as to effectively create\nsmart spaces and achieve miniaturization. In the process of scaling down to\ncompact devices, the real things to ponder upon are the Information Retrieval\nchallenges. In this work, we discuss the aspects of multimedia which makes\ninformation access challenging. An Example Pattern Recognition scenario is\npresented and the mathematical techniques that can be used to model uncertainty\nare also presented for developing a system that can sense, compute and\ncommunicate in a way that can make human life easy with smart objects assisting\nfrom around his surroundings.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.6661v1"
    },
    {
        "title": "JPEG Noises beyond the First Compression Cycle",
        "authors": [
            "Bin Li",
            "Tian-Tsong Ng",
            "Xiaolong Li",
            "Shunquan Tan",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  This paper focuses on the JPEG noises, which include the quantization noise\nand the rounding noise, during a JPEG compression cycle. The JPEG noises in the\nfirst compression cycle have been well studied; however, so far less attention\nhas been paid on the JPEG noises in higher compression cycles. In this work, we\npresent a statistical analysis on JPEG noises beyond the first compression\ncycle. To our knowledge, this is the first work on this topic. We find that the\nnoise distributions in higher compression cycles are different from those in\nthe first compression cycle, and they are dependent on the quantization\nparameters used between two successive cycles. To demonstrate the benefits from\nthe statistical analysis, we provide two applications that can employ the\nderived noise distributions to uncover JPEG compression history with\nstate-of-the-art performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.7571v1"
    },
    {
        "title": "QoE assessment for SVC streaming in ENVISION",
        "authors": [
            "Abbas Bradai",
            "Toufik Ahmed",
            "Samir Medjiah"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Scalable video coding has drawn great interest in content delivery in many\nmultimedia services thanks to its capability to handle terminal heterogeneity\nand network conditions variation. In our previous work, and under the umbrella\nof ENVISION, we have proposed a playout smoothing mechanism to ensure the\nuniform delivery of the layered stream, by reducing the quality changes that\nthe stream undergoes when adapting to changing network conditions. In this\npaper we study the resulting video quality, from the final user perception\nunder different network conditions of loss and delays. For that we have adopted\nthe Double Stimulus Impairment Scale (DSIS) method. The results show that the\nMean Opinion Score for the smoothed video clips was higher under different\nnetwork configuration. This confirms the effectiveness of the proposed\nsmoothing mechanism.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.7629v1"
    },
    {
        "title": "Genetic Algorithm in Audio Steganography",
        "authors": [
            "Manisha Rana",
            "Rohit Tanwar"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  With the advancement of communication technology,data is exchanged digitally\nover the network. At the other side the technology is also proven as a tool for\nunauthorized access to attackers. Thus the security of data to be transmitted\ndigitally should get prime focus. Data hiding is the common approach to secure\ndata. In steganography technique, the existence of data is concealed. GA is an\nemerging component of AI to provide suboptimal solutions. In this paper the use\nof GA in Steganography is explored to find future scope of research.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.2729v1"
    },
    {
        "title": "A Survey of Digital Watermarking Techniques and its Applications",
        "authors": [
            "Lalit Kumar Saini",
            "Vishal Shrivastava"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Digital media is the need of a people now a day as the alternate of paper\nmedia.As the technology grown up digital media required protection while\ntransferring through internet or others mediums.Watermarking techniques have\nbeen developed to fulfill this requirement.This paper aims to provide a\ndetailed survey of all watermarking techniques specially focuses on image\nwatermarking types and its applications in today world.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.4735v1"
    },
    {
        "title": "Robust Lossless Semi Fragile Information Protection in Images",
        "authors": [
            "Pushkar Dixit",
            "Nishant Singh",
            "Jay Prakash Gupta"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Internet security finds it difficult to keep the information secure and to\nmaintain the integrity of the data. Sending messages over the internet secretly\nis one of the major tasks as it is widely used for passing the message.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.4865v4"
    },
    {
        "title": "An Easy yet Effective Method for Detecting Spatial Domain LSB\n  Steganography",
        "authors": [
            "Minati Mishra",
            "M. C. Adhikary"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Digitization of image was a revolutionary step for the fields of photography\nand Image processing as this made the editing of images much effortless and\neasier. Image editing was not an issue until it was limited to corrective\nediting procedures used to enhance the quality of an image such as, contrast\nstretching, noise filtering, sharpening etc. But, it became a headache for many\nfields when image editing became manipulative. Digital images have become an\neasier source of tampering and forgery during last few decades. Today users and\nediting specialists, equipped with easily available image editing software,\nmanipulate digital images with varied goals. Photo journalists often tamper\nphotographs to give dramatic effect to their stories. Scientists and\nresearchers use this trick to get theirs works published. Patients' diagnoses\nare misrepresented by manipulating medical imageries. Lawyers and Politicians\nuse tampered images to direct the opinion of people or court to their favor.\nTerrorists, anti-social groups use manipulated Stego images for secret\ncommunication. In this paper we present an effective method for detecting\nspatial domain Steganography.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.6877v1"
    },
    {
        "title": "Detection of Clones in Digital Images",
        "authors": [
            "Minati Mishra",
            "M. C. Adhikary"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  During the recent years, tampering of digital images has become a general\nhabit among people and professionals. As a result, establishment of image\nauthenticity has become a key issue in fields those make use of digital images.\nAuthentication of an image involves separation of original camera outputs from\ntheir tampered or Stego counterparts. Digital image cloning being a popular\ntype of image tampering, in this paper we have experimentally analyzed seven\ndifferent algorithms of cloning detection such as the simple overlapped block\nmatching with lexicographic sorting (SOBMwLS) algorithm, block matching with\ndiscrete cosine transformation, principal component analysis, discrete wavelet\ntransformation and singular value decomposition performed on the blocks (DCT,\nDWT, PCA, SVD), two combination models where, DCT and DWT are combined with\nsingular value decomposition (DCTSVD and DWTSVD. A comparative study of all\nthese techniques with respect to their time complexities and robustness of\ndetection against various post processing operations such as cropping,\nbrightness and contrast adjustments are presented in the paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.6879v1"
    },
    {
        "title": "Fast Adaptive Algorithm for Robust Evaluation of Quality of Experience",
        "authors": [
            "Qianqian Xu",
            "Ming Yan",
            "Yuan Yao"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Outlier detection is an integral part of robust evaluation for\ncrowdsourceable Quality of Experience (QoE) and has attracted much attention in\nrecent years. In QoE for multimedia, outliers happen because of different test\nconditions, human errors, abnormal variations in context, {etc}. In this paper,\nwe propose a simple yet effective algorithm for outlier detection and robust\nQoE evaluation named iterative Least Trimmed Squares (iLTS). The algorithm\nassigns binary weights to samples, i.e., 0 or 1 indicating if a sample is an\noutlier, then the outlier-trimmed subset least squares solutions give robust\nranking scores. An iterative optimization is carried alternatively between\nupdating weights and ranking scores which converges to a local optimizer in\nfinite steps. In our test setting, iLTS is up to 190 times faster than\nLASSO-based methods with a comparable performance. Moreover, a varied version\nof this method shows adaptation in outlier detection, which provides an\nautomatic detection to determine whether a data sample is an outlier without\n\\emph{a priori} knowledge about the amount of the outliers. The effectiveness\nand efficiency of iLTS are demonstrated on both simulated examples and\nreal-world applications. A Matlab package is provided to researchers exploiting\ncrowdsourcing paired comparison data for robust ranking.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.7636v2"
    },
    {
        "title": "Impact of video quality and wireless network interface on power\n  consumption of mobile devices",
        "authors": [
            "Norbert Zsak",
            "Christian Wolff"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  During the last years, many improvements were made to the hardware capability\nof mobile devices. As mobile software also became more interactive and data\nprocessing intensive, the increased power demand could not be compensated by\nthe improvements on battery technology. Adaptive systems can help to balance\nthe demand of applications with the limitations of battery resources. For\neffective systems, the influence of multimedia quality on power consumption of\nthe components of mobile devices needs to be better understood. In this paper,\nwe analyze the impact of video quality and wireless network type on the energy\nconsumption of a mobile device. We have found that the additional power\nconsumption is up to 38% higher when a movie is played over a WiFi network\ninstead from internal memory and 64% higher in case of a mobile network (3G).\nWe have also discovered that a higher movie quality not only affects the power\nconsumption of the CPU but also the power consumption of the WiFi unit by up to\n58% and up to 72% respectively on mobile networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.7667v2"
    },
    {
        "title": "A New Method for Estimating the Widths of JPEG Images",
        "authors": [
            "Wu Xianyan",
            "Han Qi",
            "Le Dan",
            "Niu Xiamu"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Image width is important for image understanding. We propose a novel method\nto estimate widths for JPEG images when their widths are not available. The key\nidea is that the distance between two decoded MCUs (Minimum Coded Unit)\nadjacent in the vertical direction is usually small, which is measured by the\naverage Euclidean distance between the pixels from the bottom row of the top\nMCU and the top row of the bottom MCU. On PASCAL VOC 2010 challenge dataset and\nUSC-SIPI image database, experimental results show the high performance of the\nproposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.2100v1"
    },
    {
        "title": "An Efficient Bit Plane X-OR Algorithm for Irreversible Image\n  Steganography",
        "authors": [
            "Soumendu Chakraborty",
            "Anand Singh Jalal",
            "Charul Bhatnagar"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  The science of hiding secret information in another message is known as\nSteganography; hence the presence of secret information is concealed. It is the\nmethod of hiding cognitive content in same or another media to avoid\nrecognition by the intruders. This paper introduces new method wherein\nirreversible steganography is used to hide an image in the same medium so that\nthe secret data is masked. The secret image is known as payload and the carrier\nis known as cover image. X-OR operation is used amongst mid level bit planes of\ncarrier image and high level bit planes of data image to generate new low level\nbit planes of the stego image. Recovery process includes the X-ORing of low\nlevel bit planes and mid level bit planes of the stego image. Based on the\nresult of the recovery, subsequent data image is generated. A RGB color image\nis used as carrier and the data image is a grayscale image of dimensions less\nthan or equal to the dimensions of the carrier image. The proposed method\ngreatly increases the embedding capacity without significantly decreasing the\nPSNR value.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.3117v1"
    },
    {
        "title": "Secret Image Sharing Using Grayscale Payload Decomposition and\n  Irreversible Image Steganography",
        "authors": [
            "Soumendu Chakraborty",
            "Anand Singh Jalal",
            "Charul Bhatnagar"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  To provide an added security level most of the existing reversible as well as\nirreversible image steganography schemes emphasize on encrypting the secret\nimage (payload) before embedding it to the cover image. The complexity of\nencryption for a large payload where the embedding algorithm itself is complex\nmay adversely affect the steganographic system. Schemes that can induce same\nlevel of distortion, as any standard encryption technique with lower\ncomputational complexity, can improve the performance of stego systems. In this\npaper we propose a secure secret image sharing scheme, which bears minimal\ncomputational complexity. The proposed scheme, as a replacement for encryption,\ndiversifies the payload into different matrices which are embedded into carrier\nimage (cover image) using bit X-OR operation. A payload is a grayscale image\nwhich is divided into frequency matrix, error matrix, and sign matrix. The\nfrequency matrix is scaled down using a mapping algorithm to produce Down\nScaled Frequency (DSF) matrix. The DSF matrix, error matrix, and sign matrix\nare then embedded in different cover images using bit X-OR operation between\nthe bit planes of the matrices and respective cover images. Analysis of the\nproposed scheme shows that it effectively camouflages the payload with minimum\ncomputation time.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.3122v1"
    },
    {
        "title": "Multi-View 3D Video Multicast for Broadband IP Networks",
        "authors": [
            "Ting-Yu Ho",
            "Yi-Nung Yeh",
            "De-Nian Yang"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  With the recent emergence of 3D-supported TVs, video service providers now\nface an opportunity to provide high resolution multi-view 3D videos over IP\nnetworks. One simple way to support efficient communications between a video\nserver and multiple clients is to deliver each desired view in a multicast\nstream. Nevertheless, it is expected that significantly increased bandwidth\nwill be required to support the transmission of all views in multi-view 3D\nvideos. However, the recent emergence of a new video synthesis technique called\nDepth-Image-Based Rendering (DIBR) suggests that multi-view 3D video does not\nnecessarily require the transmission of all views. Therefore, we formulate a\nnew problem, named Multi-view and Multicast Delivery Selection Problem (MMDS),\nand design an algorithm, called MMDEA, to find the optimal solution. Simulation\nresults manifest that using DIBR can effectively reduce bandwidth consumption\nby 35% compared to the original multicast delivery scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.3977v1"
    },
    {
        "title": "Human Motion Capture Data Tailored Transform Coding",
        "authors": [
            "Junhui Hou",
            "Lap-Pui Chau",
            "Nadia Magnenat-Thalmann",
            "Ying He"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Human motion capture (mocap) is a widely used technique for digitalizing\nhuman movements. With growing usage, compressing mocap data has received\nincreasing attention, since compact data size enables efficient storage and\ntransmission. Our analysis shows that mocap data have some unique\ncharacteristics that distinguish themselves from images and videos. Therefore,\ndirectly borrowing image or video compression techniques, such as discrete\ncosine transform, does not work well. In this paper, we propose a novel\nmocap-tailored transform coding algorithm that takes advantage of these\nfeatures. Our algorithm segments the input mocap sequences into clips, which\nare represented in 2D matrices. Then it computes a set of data-dependent\northogonal bases to transform the matrices to frequency domain, in which the\ntransform coefficients have significantly less dependency. Finally, the\ncompression is obtained by entropy coding of the quantized coefficients and the\nbases. Our method has low computational cost and can be easily extended to\ncompress mocap databases. It also requires neither training nor complicated\nparameter setting. Experimental results demonstrate that the proposed scheme\nsignificantly outperforms state-of-the-art algorithms in terms of compression\nperformance and speed.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.4730v1"
    },
    {
        "title": "Comparing CSI and PCA in Amalgamation with JPEG for Spectral Image\n  Compression",
        "authors": [
            "Muhammad Safdar",
            "Ming Ronnier Luo",
            "Xiaoyu Liu"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Continuing our previous research on color image compression, we move towards\nspectral image compression. This enormous amount of data needs more space to\nstore and more time to transmit. To manage this sheer amount of data,\nresearchers have investigated different techniques so that image quality can be\nconserved and compressibility can be improved. The principle component analysis\n(PCA) can be employed to reduce the dimensions of spectral images to achieve\nhigh compressibility and performance. Due to processing complexity of PCA, a\nsimple interpolation technique called cubic spline interpolation (CSI) was\nconsidered to reduce the dimensionality of spectral domain of spectral images.\nThe CSI and PCA were employed one by one in the spectral domain and were\namalgamated with the JPEG, which was employed in spatial domain. Three measures\nincluding compression rate (CR), processing time (Tp) and color difference\nCIEDE2000 were used for performance analysis. Test results showed that for a\nfixed value of compression rate, CSI based algorithm performed poor in terms of\ndE00, in comparison with PCA, but is still reliable because of small color\ndifference. On the other hand it has lower complexity and is computationally\nmuch better as compared to PCA based algorithm, especially for spectral images\nwith large size.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.5092v4"
    },
    {
        "title": "A Systematic Scheme for Measuring the Performance of the Display-Camera\n  Channel",
        "authors": [
            "Changsheng Chen",
            "Wai Ho Mow"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Display-camera communication has become a promising direction in both\ncomputer vision and wireless communication communities. However, the\nconsistency of the channel measurement is an open issue since precise\ncalibration of the experimental setting has not been fully studied in the\nliteratures. This paper focuses on establishing a scheme for precise\ncalibration of the display-camera channel performance. To guarantee high\nconsistency of the experiment, we propose an accurate measurement scheme for\nthe geometric parameters, and identify some unstable channel factors, e.g.,\nMoire effect, rolling shutter effect, blocking artifacts, inconsistency in\nauto-focus, trembling and vibration. In the experiment, we first define the\nconsistency criteria according to the error-prone region in bit error rate\n(BER) plots of the channel measurements. It is demonstrated that the\nconsistency of the experimental result can be improved by the proposed precise\ncalibration scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.02528v1"
    },
    {
        "title": "PacMap: Transferring PacMan to the Physical Realm",
        "authors": [
            "Thomas Chatzidimitris",
            "Damianos Gavalas",
            "Vlasios Kasapakis"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This paper discusses the implementation of the pervasive game PacMap.\nOpenness and portability have been the main design objectives for PacMap. We\nelaborate on programming techniques which may be applicable to a broad range of\nlocation-based games that involve the movement of virtual characters over map\ninterfaces. In particular, we present techniques to execute shortest path\nalgorithms on spatial environments bypassing the restrictions imposed by\ncommercial mapping services. Last, we present ways to improve the movement and\nenhance the intelligence of virtual characters taking into consideration the\nactions and position of players in location-based games.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.02659v1"
    },
    {
        "title": "Watermarking PDF Documents using Various Representations of\n  Self-inverting Permutations",
        "authors": [
            "Maria Chroni",
            "Stavros D. Nikolopoulos"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This work provides to web users copyright protection of their Portable\nDocument Format (PDF) documents by proposing efficient and easily implementable\ntechniques for PDF watermarking; our techniques are based on the ideas of our\nrecently proposed watermarking techniques for software, image, and audio,\nexpanding thus the digital objects that can be efficiently watermarked through\nthe use of self-inverting permutations. In particular, we present various\nrepresentations of a self-inverting permutation $\\pi^*$ namely\n1D-representation, 2D-representation, and RPG-representation, and show that\ntheses representations can be efficiently applied to PDF watermarking. Indeed,\nwe first present an audio-based technique for marking a PDF document $T$ by\nexploiting the 1D-representation of a permutation $\\pi^*$, and then, since\npages of a PDF document $T$ are 2D objects, we present an image-based algorithm\nfor encoding $\\pi^*$ into $T$ by first mapping the elements of $\\pi^*$ into a\nmatrix $A^*$ and then using the information stored in $A^*$ to mark invisibly\nspecific areas of PDF document $T$. Finally, we describe a graph-based\nwatermarking algorithm for embedding a self-inverting permutation $\\pi^*$ into\nthe document structure of a PDF file $T$ by exploiting the RPG-representation\nof $\\pi^*$ and the structure of a PDF document. We have evaluated the embedding\nand extracting algorithms by testing them on various and different in\ncharacteristics PDF documents.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.02686v1"
    },
    {
        "title": "Embedding of binary image in the Gray planes",
        "authors": [
            "V. N. Gorbachev",
            "L. A. Denisov",
            "E. M. Kainarova"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  For watermarking of the digital grayscale image its Gray planes have been\nused. With the help of the introduced representation over Gray planes the LSB\nembedding method and detection have been discussed. It found that data, a\nbinary image, hidden in the Gray planes is more robust to JPEG lossy\ncompression than in the bit planes.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.07034v1"
    },
    {
        "title": "Capacity Enlargement Of The PVD Steganography Method Using The GLM\n  Technique",
        "authors": [
            "Mehdi Safarpour",
            "Mostafa Charmi"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  In most steganographic methods, increasing in the capacity leads to decrease\nin the quality of the stego-image, so in this paper, we propose to combine two\nexisting techniques, Pixel value differencing and Gray Level Modification, to\ncome up with a hybrid steganography scheme which can hide more information\nwithout having to compromise much on the quality of the stego-image.\nExperimental results demonstrate that the proposed approach has larger capacity\nwhile its results are imperceptible. In comparison with original PVD method\ncriterion of the quality is declined by 2% dB averagely while the capacity is\nincreased around 25%.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.00299v1"
    },
    {
        "title": "A New Image Steganographic Technique using Pattern based Bits Shuffling\n  and Magic LSB for Grayscale Images",
        "authors": [
            "Khan Muhammad",
            "Jamil Ahmad",
            "Haleem Farman",
            "Zahoor Jan"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Image Steganography is a growing research area of information security where\nsecret information is embedded in innocent-looking public communication. This\npaper proposes a novel crystographic technique for grayscale images in spatial\ndomain. The secret data is encrypted and shuffled using pattern based bits\nshuffling algorithm (PBSA) and a secret key. The encrypted data is then\nembedded in the cover image using magic least significant bit (M-LSB) method.\nExperimentally, the proposed method is evaluated by qualitative and\nquantitative analysis which validates the effectiveness of the proposed method\nin contrast to several state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.01386v1"
    },
    {
        "title": "Comparison of cinepak, intel, microsoft video and indeo codec for video\n  compression",
        "authors": [
            "Suleiman Mustafa",
            "Hannan Xiao"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The file size and picture quality are factors to be considered for streaming,\nstorage and transmitting videos over networks. This work compares Cinepak,\nIntel, Microsoft Video and Indeo Codec for video compression. The peak signal\nto noise ratio is used to compare the quality of such video compressed using\nAVI codecs. The most widely used objective measurement by developers of video\nprocessing systems is Peak Signal-to-Noise Ratio (PSNR). Peak Signal to Noise\nRation is measured on a logarithmic scale and depends on the mean squared error\n(MSE) between an original and an impaired image or video, relative to (2n-1)2.\n  Previous research done regarding assessing of video quality has been mainly\nby the use of subjective methods, and there is still no standard method for\nobjective assessments. Although it has been considered that compression might\nnot be significant in future as storage and transmission capabilities improve,\nbut at low bandwidths compression makes communication possible.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.01408v1"
    },
    {
        "title": "An Enhanced Edge Adaptive Steganography Approach Using Threshold Value\n  for Region Selection",
        "authors": [
            "Sachin Mungmode",
            "R. R. Sedamkar",
            "Niranjan Kulkarni"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This paper attempts to improve the quality and the modification rate of a\nStego Image. The input image provided for estimating the quality of an image\nand the modified rate is a bitmap image. The threshold value is used as a\nparameter for selecting the high frequency pixels from the Cover Image. The\ndata embedding process are performed on the pixels that are found with the help\nof Threshold value by using LSBMR. The quality of an image is estimated by the\nvalue of PSNR and the modification rate of an image is estimated by the value\nof MSE. The proposed approach achieves about 0.2 to 0.6 % of improvement in the\nquality of an image and about 4 to 10 % of improvement in the modification rate\nof an image compared to the edge detection techniques such as Sobel and Canny.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.02076v1"
    },
    {
        "title": "Lossless Intra Coding in HEVC with 3-tap Filters",
        "authors": [
            "Saeed R. Alvar",
            "Fatih Kamisli"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This paper presents a pixel-by-pixel spatial prediction method for lossless\nintra coding within High Efficiency Video Coding (HEVC). A well-known previous\npixel-by-pixel spatial prediction method uses only two neighboring pixels for\nprediction, based on the angular projection idea borrowed from block-based\nintra prediction in lossy coding. This paper explores a method which uses three\nneighboring pixels for prediction according to a two-dimensional correlation\nmodel, and the used neighbor pixels and prediction weights change depending on\nintra mode. To find the best prediction weights for each intra mode, a\ntwo-stage offline optimization algorithm is used and a number of implementation\naspects are discussed to simplify the proposed prediction method. The proposed\nmethod is implemented in the HEVC reference software and experimental results\nshow that the explored 3-tap filtering method can achieve an average 11.34%\nbitrate reduction over the default lossless intra coding in HEVC. The proposed\nmethod also decreases average decoding time by 12.7% while it increases average\nencoding time by 9.7%\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04473v1"
    },
    {
        "title": "Multiple Watermarking Algorithm Based on Spread Transform Dither\n  Modulation",
        "authors": [
            "Xinchao Li",
            "Ju Liu",
            "Jiande Sun",
            "Xiaohui Yang",
            "Wei Liu"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Multiple watermarking technique, embedding several watermarks in one carrier,\nhas enabled many interesting applications. In this study, a novel multiple\nwatermarking algorithm is proposed based on the spirit of spread transform\ndither modulation (STDM). It can embed multiple watermarks into the same region\nand the same transform domain of one image; meanwhile, the embedded watermarks\ncan be extracted independently and blindly in the detector without any\ninterference. Furthermore, to improve the fidelity of the watermarked image,\nthe properties of the dither modulation quantizer and the proposed multiple\nwatermarks embedding strategy are investigated, and two practical optimization\nmethods are proposed. Finally, to enhance the application flexibility, an\nextension of the proposed algorithm is proposed which can sequentially embeds\ndifferent watermarks into one image during each stage of its circulation.\nCompared with the pioneering multiple watermarking algorithms, the proposed one\nowns more flexibility in practical application and is more robust against\ndistortion due to basic operations such as random noise, JPEG compression and\nvolumetric scaling.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04522v1"
    },
    {
        "title": "Revisiting copy-move forgery detection by considering realistic image\n  with similar but genuine objects",
        "authors": [
            "Ye Zhu",
            "Tian-Tsong Ng",
            "Xuanjing Shen",
            "Bihan Wen"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Many images, of natural or man-made scenes often contain Similar but Genuine\nObjects (SGO). This poses a challenge to existing Copy-Move Forgery Detection\n(CMFD) methods which match the key points / blocks, solely based on the pair\nsimilarity in the scene. To address such issue, we propose a novel CMFD method\nusing Scaled Harris Feature Descriptors (SHFD) that preform consistently well\non forged images with SGO. It involves the following main steps: (i) Pyramid\nscale space and orientation assignment are used to keep scaling and rotation\ninvariance; (ii) Combined features are applied for precise texture description;\n(iii) Similar features of two points are matched and RANSAC is used to remove\nthe false matches. The experimental results indicate that the proposed\nalgorithm is effective in detecting SGO and copy-move forgery, which compares\nfavorably to existing methods. Our method exhibits high robustness even when an\nimage is operated by geometric transformation and post-processing\n",
        "pdf_link": "http://arxiv.org/pdf/1601.07262v1"
    },
    {
        "title": "Daala: A Perceptually-Driven Next Generation Video Codec",
        "authors": [
            "Thomas J. Daede",
            "Nathan E. Egge",
            "Jean-Marc Valin",
            "Guillaume Martres",
            "Timothy B. Terriberry"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The Daala project is a royalty-free video codec that attempts to compete with\nthe best patent-encumbered codecs. Part of our strategy is to replace core\ntools of traditional video codecs with alternative approaches, many of them\ndesigned to take perceptual aspects into account, rather than optimizing for\nsimple metrics like PSNR. This paper documents some of our experiences with\nthese tools, which ones worked and which did not, and what we've learned from\nthem. The result is a codec which compares favorably with HEVC on still images,\nand is on a path to do so for video as well.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.03129v1"
    },
    {
        "title": "Towards Coordinated Bandwidth Adaptations for Hundred-Scale 3D\n  Tele-Immersive Systems",
        "authors": [
            "Mohammad Hosseini",
            "Gregorij Kurillo",
            "Seyed Rasoul Etesami",
            "Jiang Yu"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  3D tele-immersion improves the state of collaboration among geographically\ndistributed participants. Unlike the traditional 2D videos, a 3D tele-immersive\nsystem employs multiple 3D cameras based in each physical site to cover a much\nlarger field of view, generating a very large amount of stream data. One of the\nmajor challenges is how to efficiently transmit these bulky 3D streaming data\nto bandwidth-constrained sites. In this paper, we study an adaptive Human\nVisual System (HVS) -compliant bandwidth management framework for efficient\ndelivery of hundred-scale streams produced from distributed 3D tele-immersive\nsites to a receiver site with limited bandwidth budget. Our adaptation\nframework exploits the semantics link of HVS with multiple 3D streams in the 3D\ntele-immersive environment. We developed TELEVIS, a visual simulation tool to\nshowcase a HVS-aware tele-immersive system for realistic cases. Our evaluation\nresults show that the proposed adaptation can improve the total quality per\nunit of bandwidth used to deliver streams in 3D tele-immersive systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06083v2"
    },
    {
        "title": "Optimal Lagrange Multipliers for Dependent Rate Allocation in Video\n  Coding",
        "authors": [
            "Ana De Abreu",
            "Gene Cheung",
            "Pascal Frossard",
            "Fernando Pereira"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  In a typical video rate allocation problem, the objective is to optimally\ndistribute a source rate budget among a set of (in)dependently coded data units\nto minimize the total distortion of all units. Conventional Lagrangian\napproaches convert the lone rate constraint to a linear rate penalty scaled by\na multiplier in the objective, resulting in a simpler unconstrained\nformulation. However, the search for the \"optimal\" multiplier, one that results\nin a distortion-minimizing solution among all Lagrangian solutions that satisfy\nthe original rate constraint, remains an elusive open problem in the general\nsetting. To address this problem, we propose a computation-efficient search\nstrategy to identify this optimal multiplier numerically. Specifically, we\nfirst formulate a general rate allocation problem where each data unit can be\ndependently coded at different quantization parameters (QP) using a previous\nunit as predictor, or left uncoded at the encoder and subsequently interpolated\nat the decoder using neighboring coded units. After converting the original\nrate constrained problem to the unconstrained Lagrangian counterpart, we design\nan efficient dynamic programming (DP) algorithm that finds the optimal\nLagrangian solution for a fixed multiplier. Finally, within the DP framework,\nwe iteratively compute neighboring singular multiplier values, each resulting\nin multiple simultaneously optimal Lagrangian solutions, to drive the rates of\nthe computed Lagrangian solutions towards the bit budget. We terminate when a\nsingular multiplier value results in two Lagrangian solutions with rates below\nand above the bit budget. In extensive monoview and multiview video coding\nexperiments, we show that our DP algorithm and selection of optimal multipliers\non average outperform comparable rate control solutions used in video\ncompression standards such as HEVC that do not skip frames in Y-PSNR.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06123v1"
    },
    {
        "title": "A framework for event co-occurrence detection in event streams",
        "authors": [
            "Laleh Jalali",
            "Ramesh Jain"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This paper shows that characterizing co-occurrence between events is an\nimportant but non-trivial and neglected aspect of discovering potential causal\nrelationships in multimedia event streams. First an introduction to the notion\nof event co-occurrence and its relation to co-occurrence pattern detection is\ngiven. Then a finite state automaton extended with a time model and event\nparameterization is introduced to convert high level co-occurrence pattern\ndefinition to its corresponding pattern matching automaton. Finally a\nprocessing algorithm is applied to count the occurrence frequency of a\ncollection of patterns with only one pass through input event streams. The\nmethod proposed in this paper can be used for detecting co-occurrences between\nboth events of one event stream (Auto co-occurrence), and events from multiple\nevent streams (Cross co-occurrence). Some fundamental results concerning the\ncharacterization of event co-occurrence are presented in form of a visual co-\noccurrence matrix. Reusable causality rules can be extracted easily from\nco-occurrence matrix and fed into various analysis tools, such as\nrecommendation systems and complex event processing systems for further\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.09012v1"
    },
    {
        "title": "Robust Hybrid Image Watermarking based on Discrete Wavelet and Shearlet\n  Transforms",
        "authors": [
            "Malihe Mardanpour",
            "Mohammad Ali Zare Chahooki"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  With the growth of digital networks such as the Internet, digital media have\nbeen explosively developed in e-commerce and online services. This causes\nproblems such as illegal copy and fake ownership. Watermarking is proposed as\none of the solutions to such cases. Among different watermarking techniques,\nthe wavelet transform has been used more because of its good ability in\nmodeling the human visual system. Recently, Shearlet transform as an extension\nof Wavelet transform which is based on multi-resolution and multi-directional\nanalysis is introduced. The most important feature of this transform is the\nappropriate representation of image edges. In this paper a hybrid scheme using\nDiscrete Wavelet Transform (DWT) and Discrete Shearlet Transform (DST) is\npresented. In this way, the host image is decomposed using DWT, and then its\nlow frequency sub-band is decomposed by DST. After that, the bidiagonal\nsingular value decomposition (BSVD) is applied on the selected sub-band from\nShearlet transform and the gray-scale watermark image is embedded into its\nbidiagonal singular values. The proposed method is examined on the images with\ndifferent textures and resistance is evaluated against various attacks like\nimage processing and geometric attacks. The results show good transparency and\nhigh robustness in proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.09396v1"
    },
    {
        "title": "Efficient Reversible Data Hiding Algorithms Based on Dual Prediction",
        "authors": [
            "Enas N. Jaara",
            "Iyad F. Jafar"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  In this paper, a new reversible data hiding (RDH) algorithm that is based on\nthe concept of shifting of prediction error histograms is proposed. The\nalgorithm extends the efficient modification of prediction errors (MPE)\nalgorithm by incorporating two predictors and using one prediction error value\nfor data embedding. The motivation behind using two predictors is driven by the\nfact that predictors have different prediction accuracy which is directly\nrelated to the embedding capacity and quality of the stego image. The key\nfeature of the proposed algorithm lies in using two predictors without the need\nto communicate additional overhead with the stego image. Basically, the\nidentification of the predictor that is used during embedding is done through a\nset of rules. The proposed algorithm is further extended to use two and three\nbins in the prediction errors histogram in order to increase the embedding\ncapacity. Performance evaluation of the proposed algorithm and its extensions\nshowed the advantage of using two predictors in boosting the embedding capacity\nwhile providing competitive quality for the stego image.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.02605v1"
    },
    {
        "title": "Frame-level quality and memory traffic allocation for lossy embedded\n  compression in video codec systems",
        "authors": [
            "Li Guo",
            "Dajiang Zhou",
            "Shinji Kimura",
            "Satoshi Goto"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  For mobile video codecs, the huge energy dissipation for external memory\ntraffic is a critical challenge under the battery power constraint. Lossy\nembedded compression (EC), as a solution to this challenge, is considered in\nthis paper. While previous studies in EC mostly focused on compression\nalgorithms at the block level, this work, to the best of our knowledge, is the\nfirst one that addresses the allocation of video quality and memory traffic at\nthe frame level. For lossy EC, a main difficulty of its application lies in the\nerror propagation from quality degradation of reference frames. Instinctively,\nit is preferred to perform more lossy EC in non-reference frames to minimize\nthe quality loss. The analysis and experiments in this paper, however, will\nshow lossy EC should actually be distributed to more frames. Correspondingly,\nfor hierarchical-B GOPs, we developed an efficient allocation that outperforms\nthe non-reference-only allocation by up to 4.5 dB in PSNR. In comparison, the\nproposed allocation also delivers more consistent quality between frames by\nhaving lower PSNR fluctuation.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.02976v1"
    },
    {
        "title": "Regression-based Intra-prediction for Image and Video Coding",
        "authors": [
            "Carlo Noel Ochotorena",
            "Yukihiko Yamashita"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  By utilizing previously known areas in an image, intra-prediction techniques\ncan find a good estimate of the current block. This allows the encoder to store\nonly the error between the original block and the generated estimate, thus\nleading to an improvement in coding efficiency. Standards such as AVC and HEVC\ndescribe expert-designed prediction modes operating in certain angular\norientations alongside separate DC and planar prediction modes. Being designed\npredictors, while these techniques have been demonstrated to perform well in\nimage and video coding applications, they do not necessarily fully utilize\nnatural image structures. In this paper, we describe a novel system for\ndeveloping predictors derived from natural image blocks. The proposed algorithm\nis seeded with designed predictors (e.g. HEVC-style prediction) and allowed to\niteratively refine these predictors through regularized regression. The\nresulting prediction models show significant improvements in estimation quality\nover their designed counterparts across all conditions while maintaining\nreasonable computational complexity. We also demonstrate how the proposed\nalgorithm handles the worst-case scenario of intra-prediction with no error\nreporting.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.03754v1"
    },
    {
        "title": "Backward-Shifted Strategies Based on SVC for HTTP Adaptive Video\n  Streaming",
        "authors": [
            "Zakaria Ye",
            "Rachid El-Azouzi",
            "Tania Jimenez",
            "Eitan Altman",
            "Stefan Valentin"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Although HTTP-based video streaming can easily penetrate firewalls and profit\nfrom Web caches, the underlying TCP may introduce large delays in case of a\nsudden capacity loss. To avoid an interruption of the video stream in such\ncases we propose the Backward-Shifted Coding (BSC). Based on Scalable Video\nCoding (SVC), BSC adds a time-shifted layer of redundancy to the video stream\nsuch that future frames are downloaded at any instant. This pre-fetched content\nmaintains a fluent video stream even under highly variant network conditions\nand leads to high Quality of Experience (QoE). We characterize this QoE gain by\nanalyzing initial buffering time, re-buffering time and content resolution\nusing the Ballot theorem. The probability generating functions of the playback\ninterruption and of the initial buffering latency are provided in closed form.\nWe further compute the quasi-stationary distribution of the video quality, in\norder to compute the average quality, as well as temporal variability in video\nquality. Employing these analytic results to optimize QoE shows interesting\ntrade-offs and video streaming at outstanding fluency.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.03815v1"
    },
    {
        "title": "Daala: A Perceptually-Driven Still Picture Codec",
        "authors": [
            "Jean-Marc Valin",
            "Nathan E. Egge",
            "Thomas Daede",
            "Timothy B. Terriberry",
            "Christopher Montgomery"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Daala is a new royalty-free video codec based on perceptually-driven coding\ntechniques. We explore using its keyframe format for still picture coding and\nshow how it has improved over the past year. We believe the technology used in\nDaala could be the basis of an excellent, royalty-free image format.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.04930v1"
    },
    {
        "title": "Lossless Compression in HEVC with Integer-to-Integer Transforms",
        "authors": [
            "Fatih Kamisli"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Many approaches have been proposed to support lossless coding within video\ncoding standards that are primarily designed for lossy coding. The simplest\napproach is to just skip transform and quantization and directly entropy code\nthe prediction residual, which is used in HEVC version 1. However, this simple\napproach is inefficient for compression. More efficient approaches include\nprocessing the residual with DPCM prior to entropy coding. This paper explores\nan alternative approach based on processing the residual with\ninteger-to-integer (i2i) transforms. I2i transforms map integers to integers,\nhowever, unlike the integer transforms used in HEVC for lossy coding, they do\nnot increase the dynamic range at the output and can be used in lossless\ncoding. Experiments with the HEVC reference software show competitive results.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05118v1"
    },
    {
        "title": "Lossless Intra Coding in HEVC with Integer-to-Integer DST",
        "authors": [
            "Fatih Kamisli"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  It is desirable to support efficient lossless coding within video coding\nstandards, which are primarily designed for lossy coding, with as little\nmodification as possible. A simple approach is to skip transform and\nquantization, and directly entropy code the prediction residual, but this is\ninefficient for compression. A more efficient and popular approach is to\nprocess the residual block with DPCM prior to entropy coding. This paper\nexplores an alternative approach based on processing the residual block with\ninteger-to-integer (i2i) transforms. I2i transforms map integers to integers,\nhowever, unlike the integer transforms used in HEVC for lossy coding, they do\nnot increase the dynamic range at the output and can be used in lossless\ncoding. We use both an i2i DCT from the literature and a novel i2i\napproximation of the DST. Experiments with the HEVC reference software show\ncompetitive results.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05319v1"
    },
    {
        "title": "Steganalysis via a Convolutional Neural Network using Large Convolution\n  Filters for Embedding Process with Same Stego Key",
        "authors": [
            "Jean-François Couchot",
            "Raphaël Couturier",
            "Christophe Guyeux",
            "Michel Salomon"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  For the past few years, in the race between image steganography and\nsteganalysis, deep learning has emerged as a very promising alternative to\nsteganalyzer approaches based on rich image models combined with ensemble\nclassifiers. A key knowledge of image steganalyzer, which combines relevant\nimage features and innovative classification procedures, can be deduced by a\ndeep learning approach called Convolutional Neural Networks (CNN). These kind\nof deep learning networks is so well-suited for classification tasks based on\nthe detection of variations in 2D shapes that it is the state-of-the-art in\nmany image recognition problems. In this article, we design a CNN-based\nsteganalyzer for images obtained by applying steganography with a unique\nembedding key. This one is quite different from the previous study of {\\em Qian\net al.} and its successor, namely {\\em Pibre et al.} The proposed architecture\nembeds less convolutions, with much larger filters in the final convolutional\nlayer, and is more general: it is able to deal with larger images and lower\npayloads. For the \"same embedding key\" scenario, our proposal outperforms all\nother steganalyzers, in particular the existing CNN-based ones, and defeats\nmany state-of-the-art image steganography schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.07946v3"
    },
    {
        "title": "Efficient Multiple Line-Based Intra Prediction for HEVC",
        "authors": [
            "Jiahao Li",
            "Bin Li",
            "Jizheng Xu",
            "Ruiqin Xiong"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Traditional intra prediction usually utilizes the nearest reference line to\ngenerate the predicted block when considering strong spatial correlation.\nHowever, this kind of single line-based method does not always work well due to\nat least two issues. One is the incoherence caused by the signal noise or the\ntexture of other object, where this texture deviates from the inherent texture\nof the current block. The other reason is that the nearest reference line\nusually has worse reconstruction quality in block-based video coding. Due to\nthese two issues, this paper proposes an efficient multiple line-based intra\nprediction scheme to improve coding efficiency. Besides the nearest reference\nline, further reference lines are also utilized. The further reference lines\nwith relatively higher quality can provide potential better prediction. At the\nsame time, the residue compensation is introduced to calibrate the prediction\nof boundary regions in a block when we utilize further reference lines. To\nspeed up the encoding process, this paper designs several fast algorithms.\nExperimental results show that, compared with HM-16.9, the proposed fast search\nmethod achieves 2.0% bit saving on average and up to 3.7%, with increasing the\nencoding time by 112%.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.08308v2"
    },
    {
        "title": "Improving Crowdsourced Live Streaming with Aggregated Edge Networks",
        "authors": [
            "Chenglei Wu",
            "Zhi Wang",
            "Jiangchuan Liu",
            "Shiqiang Yang"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Recent years have witnessed a dramatic increase of user-generated video\nservices. In such user-generated video services, crowdsourced live streaming\n(e.g., Periscope, Twitch) has significantly challenged today's edge network\ninfrastructure: today's edge networks (e.g., 4G, Wi-Fi) have limited uplink\ncapacity support, making high-bitrate live streaming over such links\nfundamentally impossible. In this paper, we propose to let broadcasters (i.e.,\nusers who generate the video) upload crowdsourced video streams using\naggregated network resources from multiple edge networks. There are several\nchallenges in the proposal: First, how to design a framework that aggregates\nbandwidth from multiple edge networks? Second, how to make this framework\ntransparent to today's crowdsourced live streaming services? Third, how to\nmaximize the streaming quality for the whole system? We design a\nmulti-objective and deployable bandwidth aggregation system BASS to address\nthese challenges: (1) We propose an aggregation framework transparent to\ntoday's crowdsourced live streaming services, using an edge proxy box and\naggregation cloud paradigm; (2) We dynamically allocate geo-distributed cloud\naggregation servers to enable MPTCP (i.e., multi-path TCP), according to\nlocation and network characteristics of both broadcasters and the original\nstreaming servers; (3) We maximize the overall performance gain for the whole\nsystem, by matching streams with the best aggregation paths.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.08969v1"
    },
    {
        "title": "Drone Streaming with Wi-Fi Grid Aggregation for Virtual Tour",
        "authors": [
            "Chenglei Wu",
            "Zhi Wang",
            "Shiqiang Yang"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  To provide a live, active and high-quality virtual touring streaming\nexperience, we propose an unmanned drone stereoscopic streaming paradigm using\na control and streaming infrastructure of a 2.4GHz Wi-Fi grid. Our system\nallows users to actively control the streaming captured by a drone, receive and\nwatch the streaming using a head mount display (HMD); a Wi-Fi grid is deployed\nacross the remote scene with multi-channel support to enable high-bitrate\nstream- ing broadcast from the drones. The system adopt a joint view adaptation\nand drone control scheme to enable fast viewer movement including both head\nrotation and touring. We implement the prototype on Dji M100 quadcopter and HTC\nVive in a demo scene.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.09486v1"
    },
    {
        "title": "Foveated Video Streaming for Cloud Gaming",
        "authors": [
            "Gazi Illahi",
            "Matti Siekkinen",
            "Enrico Masala"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Good user experience with interactive cloud-based multimedia applications,\nsuch as cloud gaming and cloud-based VR, requires low end-to-end latency and\nlarge amounts of downstream network bandwidth at the same time. In this paper,\nwe present a foveated video streaming system for cloud gaming. The system\nadapts video stream quality by adjusting the encoding parameters on the fly to\nmatch the player's gaze position. We conduct measurements with a prototype that\nwe developed for a cloud gaming system in conjunction with eye tracker\nhardware. Evaluation results suggest that such foveated streaming can reduce\nbandwidth requirements by even more than 50% depending on parametrization of\nthe foveated video coding and that it is feasible from the latency perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.04804v1"
    },
    {
        "title": "Further Study on GFR Features for JPEG Steganalysis",
        "authors": [
            "Xia Chao",
            "Guan Qingxiao",
            "Zhao Xianfeng"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The GFR (Gabor Filter Residual) features, built as histograms of quantized\nresiduals obtained with 2D Gabor filters, can achieve competitive detection\nperformance against adaptive JPEG steganography. In this paper, an improved\nversion of the GFR is proposed. First, a novel histogram merging method is\nproposed according to the symmetries between different Gabor filters, thus\nmaking the features more compact and robust. Second, a new weighted histogram\nmethod is proposed by considering the position of the residual value in a\nquantization interval, making the features more sensitive to the slight changes\nin residual values. The experiments are given to demonstrate the effectiveness\nof our proposed methods. Finally, we design a CNN to duplicate the detector\nwith the improved GFR features and the ensemble classifier, thus optimizing the\ndesign of the filters used to form residuals in JPEG-phase-aware features.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.07576v1"
    },
    {
        "title": "A Robust Data Hiding Process Contributing to the Development of a\n  Semantic Web",
        "authors": [
            "Jacques M. Bahi",
            "Jean-François Couchot",
            "Nicolas Friot",
            "Christophe Guyeux"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In this paper, a novel steganographic scheme based on chaotic iterations is\nproposed. This research work takes place into the information hiding framework,\nand focus more specifically on robust steganography. Steganographic algorithms\ncan participate in the development of a semantic web: medias being on the\nInternet can be enriched by information related to their contents, authors,\netc., leading to better results for the search engines that can deal with such\ntags. As media can be modified by users for various reasons, it is preferable\nthat these embedding tags can resist to changes resulting from some classical\ntransformations as for example cropping, rotation, image conversion, and so on.\nThis is why a new robust watermarking scheme for semantic search engines is\nproposed in this document. For the sake of completeness, the robustness of this\nscheme is finally compared to existing established algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.08764v1"
    },
    {
        "title": "Evaluation of No Reference Bitstream-based Video Quality Assessment\n  Methods",
        "authors": [
            "Tiantian He",
            "Yankai Liu",
            "Rong Xie",
            "Xin Tang",
            "Li Song"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Many different parametric models for video quality assessment have been\nproposed in the past few years. This paper presents a review of nine recent\nmodels which cover a wide range of methodologies and have been validated for\nestimating video quality due to different degradation factors. Each model is\nbriefly described with key algorithms and relevant parametric formulas. The\ngeneralization capability of each model to estimate video quality in\nreal-application scenarios is evaluated and compared with other models, using a\ndataset created with video sequences from practical applications. These video\nsequences cover a wide range of possible realistic encoding parameters, labeled\nwith mean opinion scores (MOS) via subjective test. The weakness and strength\nof each model are remarked. Finally, future work towards a more general\nparametric model that could apply for a wider range of applications is\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.10143v1"
    },
    {
        "title": "An Advert Creation System for Next-Gen Publicity",
        "authors": [
            "Atul Nautiyal",
            "Killian McCabe",
            "Murhaf Hossari",
            "Soumyabrata Dev",
            "Matthew Nicholson",
            "Clare Conran",
            "Declan McKibben",
            "Jian Tang",
            "Xu Wei",
            "Francois Pitie"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  With the rapid proliferation of multimedia data in the internet, there has\nbeen a fast rise in the creation of videos for the viewers. This enables the\nviewers to skip the advertisement breaks in the videos, using ad blockers and\n'skip ad' buttons -- bringing online marketing and publicity to a stall. In\nthis paper, we demonstrate a system that can effectively integrate a new\nadvertisement into a video sequence. We use state-of-the-art techniques from\ndeep learning and computational photogrammetry, for effective detection of\nexisting adverts, and seamless integration of new adverts into video sequences.\nThis is helpful for targeted advertisement, paving the path for next-gen\npublicity.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.00163v1"
    },
    {
        "title": "Two-pass Light Field Image Compression for Spatial Quality and Angular\n  Consistency",
        "authors": [
            "Bichuan Guo",
            "Jiangtao Wen",
            "Yuxing Han"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The quality assessment of light field images presents new challenges to\nconventional compression methods, as the spatial quality is affected by the\noptical distortion of capturing devices, and the angular consistency affects\nthe performance of dynamic rendering applications. In this paper, we propose a\ntwo-pass encoding system for pseudo-temporal sequence based light field image\ncompression with a novel frame level bit allocation framework that optimizes\nspatial quality and angular consistency simultaneously. Frame level\nrate-distortion models are estimated during the first pass, and the second pass\nperforms the actual encoding with optimized bit allocations given by a two-step\nconvex programming. The proposed framework supports various encoder\nconfigurations. Experimental results show that comparing to the anchor HM 16.16\n(HEVC reference software), the proposed two-pass encoding system on average\nachieves 11.2% to 11.9% BD-rate reductions for the all-intra configuration,\n15.8% to 32.7% BD-rate reductions for the random-access configuration, and\n12.1% to 15.7% BD-rate reductions for the low-delay configuration. The\nresulting bit errors are limited, and the total time cost is less than twice of\nthe one-pass anchor. Comparing with our earlier low-delay configuration based\nmethod, the proposed system improves BD-rate reduction by 3.1% to 8.3%, reduces\nthe bit errors by more than 60%, and achieves more than 12x speed up.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.00630v1"
    },
    {
        "title": "An Efficient Approach for Geo-Multimedia Cross-Modal Retrieval",
        "authors": [
            "Lei Zhu",
            "Jun Long",
            "Chengyuan Zhang",
            "Ruipeng Chen",
            "Xinpan Yuan",
            "Zhan Yang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Due to the rapid development of mobile Internet techniques, cloud computation\nand popularity of online social networking and location-based services, massive\namount of multimedia data with geographical information is generated and\nuploaded to the Internet. In this paper, we propose a novel type of cross-modal\nmultimedia retrieval called geo-multimedia cross-modal retrieval which aims to\nsearch out a set of geo-multimedia objects based on geographical distance\nproximity and semantic similarity between different modalities. Previous\nstudies for cross-modal retrieval and spatial keyword search cannot address\nthis problem effectively because they do not consider multimedia data with\ngeo-tags and do not focus on this type of query. In order to address this\nproblem efficiently, we present the definition of $k$NN geo-multimedia\ncross-modal query at the first time and introduce relevant conceptions such as\ncross-modal semantic representation space. To bridge the semantic gap between\ndifferent modalities, we propose a method named cross-modal semantic matching\nwhich contains two important component, i.e., CorrProj and LogsTran, which aims\nto construct a common semantic representation space for cross-modal semantic\nsimilarity measurement. Besides, we designed a framework based on deep learning\ntechniques to implement common semantic representation space construction. In\naddition, a novel hybrid indexing structure named GMR-Tree combining\ngeo-multimedia data and R-Tree is presented and a efficient $k$NN search\nalgorithm called $k$GMCMS is designed. Comprehensive experimental evaluation on\nreal and synthetic dataset clearly demonstrates that our solution outperforms\nthe-state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.06277v1"
    },
    {
        "title": "Identification of Deep Network Generated Images Using Disparities in\n  Color Components",
        "authors": [
            "Haodong Li",
            "Bin Li",
            "Shunquan Tan",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  With the powerful deep network architectures, such as generative adversarial\nnetworks, one can easily generate photorealistic images. Although the generated\nimages are not dedicated for fooling human or deceiving biometric\nauthentication systems, research communities and public media have shown great\nconcerns on the security issues caused by these images. This paper addresses\nthe problem of identifying deep network generated (DNG) images. Taking the\ndifferences between camera imaging and DNG image generation into\nconsiderations, we analyze the disparities between DNG images and real images\nin different color components. We observe that the DNG images are more\ndistinguishable from real ones in the chrominance components, especially in the\nresidual domain. Based on these observations, we propose a feature set to\ncapture color image statistics for identifying DNG images. Additionally, we\nevaluate several detection situations, including the training-testing data are\nmatched or mismatched in image sources or generative models and detection with\nonly real images. Extensive experimental results show that the proposed method\ncan accurately identify DNG images and outperforms existing methods when the\ntraining and testing data are mismatched. Moreover, when the GAN model is\nunknown, our methods also achieves good performance with one-class\nclassification by using only real images for training.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.07276v3"
    },
    {
        "title": "Patch-based Contour Prior Image Denoising for Salt and Pepper Noise",
        "authors": [
            "Bo Fu",
            "XiaoYang Zhao",
            "Yi Li",
            "XiangHai Wang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The salt and pepper noise brings a significant challenge to image denoising\ntechnology, i.e. how to removal the noise clearly and retain the details\neffectively? In this paper, we propose a patch-based contour prior denoising\napproach for salt and pepper noise. First, noisy image is cut into patches as\nbasic representation unit, a discrete total variation model is designed to\nextract contour structures; Second, a weighted Euclidean distance is designed\nto search the most similar patches, then, corresponding contour stencils are\nextracted from these similar patches; At the last, we build filter from contour\nstencils in the framework of regression. Numerical results illustrate that the\nproposed method is competitive with the state-of-the-art methods in terms of\nthe peak signal-to-noise (PSNR) and visual effects.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.08567v1"
    },
    {
        "title": "Efficient Region of Visual Interests Search for Geo-multimedia Data",
        "authors": [
            "Chengyuan Zhang",
            "Yunwu Lin",
            "Lei Zhu",
            "Zuping Zhang",
            "Yan Tang",
            "Fang Huang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  With the proliferation of online social networking services and mobile smart\ndevices equipped with mobile communications module and position sensor module,\nmassive amount of multimedia data has been collected, stored and shared. This\ntrend has put forward higher request on massive multimedia data retrieval. In\nthis paper, we investigate a novel spatial query named region of visual\ninterests query (RoVIQ), which aims to search users containing geographical\ninformation and visual words. Three baseline methods are presented to introduce\nhow to exploit existing techniques to address this problem. Then we propose the\ndefinition of this query and related notions at the first time. To improve the\nperformance of query, we propose a novel spatial indexing structure called\nquadtree based inverted visual index which is a combination of quadtree,\ninverted index and visual words. Based on it, we design a efficient search\nalgorithm named region of visual interests search to support RoVIQ.\nExperimental evaluations on real geo-image datasets demonstrate that our\nsolution outperforms state-of-the-art method.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.09610v1"
    },
    {
        "title": "Wavelet Video Coding Algorithm Based on Energy Weighted Significance\n  Probability Balancing Tree",
        "authors": [
            "Chuan-Ming Song",
            "Bo Fu",
            "Xiang-Hai Wang",
            "Ming-Zhe Fu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  This work presents a 3-D wavelet video coding algorithm. By analyzing the\ncontribution of each biorthogonal wavelet basis to reconstructed signal's\nenergy, we weight each wavelet subband according to its basis energy. Based on\ndistribution of weighted coefficients, we further discuss a 3-D wavelet tree\nstructure named \\textbf{significance probability balancing tree}, which places\nthe coefficients with similar probabilities of being significant on the same\nlayer. It is implemented by using hybrid spatial orientation tree and\ntemporal-domain block tree. Subsequently, a novel 3-D wavelet video coding\nalgorithm is proposed based on the energy-weighted significance probability\nbalancing tree. Experimental results illustrate that our algorithm always\nachieves good reconstruction quality for different classes of video sequences.\nCompared with asymmetric 3-D orientation tree, the average peak signal-to-noise\nratio (PSNR) gain of our algorithm are 1.24dB, 2.54dB and 2.57dB for luminance\n(Y) and chrominance (U,V) components, respectively. Compared with\ntemporal-spatial orientation tree algorithm, our algorithm gains 0.38dB, 2.92dB\nand 2.39dB higher PSNR separately for Y, U, and V components. In addition, the\nproposed algorithm requires lower computation cost than those of the above two\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.09640v1"
    },
    {
        "title": "Efficient Multimedia Similarity Measurement Using Similar Elements",
        "authors": [
            "Chengyuan Zhang",
            "Yunwu Lin",
            "Lei Zhu",
            "Zuping Zhang",
            "Xinpan Yuan",
            "Fang Huang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Online social networking techniques and large-scale multimedia systems are\ndeveloping rapidly, which not only has brought great convenience to our daily\nlife, but generated, collected, and stored large-scale multimedia data. This\ntrend has put forward higher requirements and greater challenges on massive\nmultimedia data retrieval. In this paper, we investigate the problem of image\nsimilarity measurement which is used to lots of applications. At first we\npropose the definition of similarity measurement of images and the related\nnotions. Based on it we present a novel basic method of similarity measurement\nnamed SMIN. To improve the performance of calculation, we propose a novel\nindexing structure called SMI Temp Index (SMII for short). Besides, we\nestablish an index of potential similar visual words off-line to solve to\nproblem that the index cannot be reused. Experimental evaluations on two real\nimage datasets demonstrate that our solution outperforms state-of-the-art\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.03867v1"
    },
    {
        "title": "Cloud Gaming With Foveated Graphics",
        "authors": [
            "Gazi Illahi",
            "Thomas Van Gemert",
            "Matti Siekkinen",
            "Enrico Masala",
            "Antti Oulasvirta",
            "Antti Ylä-Jääski"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Cloud gaming enables playing high end games, originally designed for PC or\ngame console setups, on low end devices, such as net-books and smartphones, by\noffloading graphics rendering to GPU powered cloud servers. However,\ntransmitting the high end graphics requires a large amount of available network\nbandwidth, even though it is a compressed video stream. Foveated video encoding\n(FVE) reduces the bandwidth requirement by taking advantage of the non-uniform\nacuity of human visual system and by knowing where the user is looking. We have\ndesigned and implemented a system for cloud gaming with foveated graphics using\na consumer grade real-time eye tracker and an open source cloud gaming\nplatform. In this article, we describe the system and its evaluation through\nmeasurements with representative games from different genres to understand the\neffect of parameterization of the FVE scheme on bandwidth requirements and to\nunderstand its feasibility from the latency perspective. We also present\nresults from a user study. The results suggest that it is possible to find a\n\"sweet spot\" for the encoding parameters so that the users hardly notice the\npresence of foveated encoding but at the same time the scheme yields most of\nthe bandwidth savings achievable.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.05823v1"
    },
    {
        "title": "Binocular Rivalry - Psychovisual Challenge in Stereoscopic Video Error\n  Concealment",
        "authors": [
            "Md Mehedi Hasan",
            "John F. Arnold",
            "Michael R. Frater"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  During Stereoscopic 3D (S3D) video transmission, one or both views can be\naffected by bit errors and packet losses caused by adverse channel conditions,\ndelay or jitter. Typically, the Human Visual System (HVS) is incapable of\naligning and fusing stereoscopic content if one view is affected by artefacts\ncaused by compression, transmission and rendering with distorted patterns being\nperceived as alterations of the original which presents a shimmering effect\nknown as binocular rivalry and is detrimental to a user's Quality of Experience\n(QoE). This study attempts to quantify the effects of binocular rivalry for\nstereoscopic videos. Existing approaches, in which one or more frames are lost\nin one or both views undergo error concealment, are implemented. Then,\nsubjective testing is carried out on the error concealed 3D video sequences.\nThe evaluations provided by these subjects were then combined and analysed\nusing a standard Student t-test thus quantifying the impact of binocular\nrivalry and allowing the impact to be compared with that of monocular viewing.\nThe main focus is implementing error-resilient video communication, avoiding\nthe detrimental effects of binocular rivalry and improving the overall QoE of\nviewers.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.07792v1"
    },
    {
        "title": "Survey on Error Concealment Strategies and Subjective Testing of 3D\n  Videos",
        "authors": [
            "Md Mehedi Hasan",
            "Michael Frater",
            "John Arnold"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Over the last decade, different technologies to visualize 3D scenes have been\nintroduced and improved. These technologies include stereoscopic, multi-view,\nintegral imaging and holographic types. Despite increasing consumer interest;\npoor image quality, crosstalk or side effects of 3D displays and also the lack\nof defined broadcast standards has hampered the advancement of 3D displays to\nthe mass consumer market. Also, in real time transmission of 3DTV sequences\nover packet-based networks may results in visual quality degradations due to\npacket loss and others. In the conventional 2D videos different extrapolation\nand directional interpolation strategies have been used for concealing the\nmissing blocks but in 3D, it is still an emerging field of research. Few\nstudies have been carried out to define the assessment methods of stereoscopic\nimages and videos. But through industrial and commercial perspective,\nsubjective quality evaluation is the most direct way to evaluate human\nperception on 3DTV systems. This paper reviews the state-of-the-art error\nconcealment strategies and the subjective evaluation of 3D videos and proposes\na low complexity frame loss concealment method for the video decoder.\nSubjective testing on prominent datasets videos and comparison with existing\nconcealment methods show that the proposed method is very much efficient to\nconceal errors of stereoscopic videos in terms of computation time, comfort and\ndistortion.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.07793v1"
    },
    {
        "title": "An Iterative Refinement Approach for Social Media Headline Prediction",
        "authors": [
            "Chih-Chung Hsu",
            "Chia-Yen Lee",
            "Ting-Xuan Liao",
            "Jun-Yi Lee",
            "Tsai-Yne Hou",
            "Ying-Chu Kuo",
            "Jing-Wen Lin",
            "Ching-Yi Hsueh",
            "Zhong-Xuan Zhan",
            "Hsiang-Chin Chien"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this study, we propose a novel iterative refinement approach to predict\nthe popularity score of the social media meta-data effectively. With the rapid\ngrowth of the social media on the Internet, how to adequately forecast the view\ncount or popularity becomes more important. Conventionally, the ensemble\napproach such as random forest regression achieves high and stable performance\non various prediction tasks. However, most of the regression methods may not\nprecisely predict the extreme high or low values. To address this issue, we\nfirst predict the initial popularity score and retrieve their residues. In\norder to correctly compensate those extreme values, we adopt an ensemble\nregressor to compensate the residues to further improve the prediction\nperformance. Comprehensive experiments are conducted to demonstrate the\nproposed iterative refinement approach outperforms the state-of-the-art\nregression approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.08753v1"
    },
    {
        "title": "Listen to Dance: Music-driven choreography generation using\n  Autoregressive Encoder-Decoder Network",
        "authors": [
            "Juheon Lee",
            "Seohyun Kim",
            "Kyogu Lee"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Automatic choreography generation is a challenging task because it often\nrequires an understanding of two abstract concepts - music and dance - which\nare realized in the two different modalities, namely audio and video,\nrespectively. In this paper, we propose a music-driven choreography generation\nsystem using an auto-regressive encoder-decoder network. To this end, we first\ncollect a set of multimedia clips that include both music and corresponding\ndance motion. We then extract the joint coordinates of the dancer from video\nand the mel-spectrogram of music from audio, and train our network using\nmusic-choreography pairs as input. Finally, a novel dance motion is generated\nat the inference time when only music is given as an input. We performed a user\nstudy for a qualitative evaluation of the proposed method, and the results show\nthat the proposed model is able to generate musically meaningful and natural\ndance movements given an unheard song.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.00818v1"
    },
    {
        "title": "Facing Device Attribution Problem for Stabilized Video Sequences",
        "authors": [
            "Sara Mandelli",
            "Paolo Bestagini",
            "Luisa Verdoliva",
            "Stefano Tubaro"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  A problem deeply investigated by multimedia forensics researchers is the one\nof detecting which device has been used to capture a video. This enables to\ntrace down the owner of a video sequence, which proves extremely helpful to\nsolve copyright infringement cases as well as to fight distribution of illicit\nmaterial (e.g., underage clips, terroristic threats, etc.). Currently, the most\npromising methods to tackle this task exploit unique noise traces left by\ncamera sensors on acquired images. However, given the recent advancements in\nmotion stabilization of video content, robustness of sensor pattern noise-based\ntechniques are strongly hindered. Indeed, video stabilization introduces\ngeometric transformations between video frames, thus making camera fingerprint\nestimation problematic with classical approaches. In this paper, we deal with\nthe challenging problem of attributing stabilized videos to their recording\ndevice. Specifically, we propose: (i) a strategy to extract the characteristic\nfingerprint of a device, starting from either a set of images or stabilized\nvideo sequences; (ii) a strategy to match a stabilized video sequence with a\ngiven fingerprint in order to solve the device attribution problem. The\nproposed methodology is tested on videos coming from a set of different\nsmartphones, taken from the modern publicly available Vision Dataset. The\nconducted experiments also provide an interesting insight on the effect of\nmodern smartphones video stabilization algorithms on specific video frames.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.01820v1"
    },
    {
        "title": "Performance Comparison of Contemporary DNN Watermarking Techniques",
        "authors": [
            "Huili Chen",
            "Bita Darvish Rouhani",
            "Xinwei Fan",
            "Osman Cihan Kilinc",
            "Farinaz Koushanfar"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  DNNs shall be considered as the intellectual property (IP) of the model\nbuilder due to the impeding cost of designing/training a highly accurate model.\nResearch attempts have been made to protect the authorship of the trained model\nand prevent IP infringement using DNN watermarking techniques. In this paper,\nwe provide a comprehensive performance comparison of the state-of-the-art DNN\nwatermarking methodologies according to the essential requisites for an\neffective watermarking technique. We identify the pros and cons of each scheme\nand provide insights into the underlying rationale. Empirical results\ncorroborate that DeepSigns framework proposed in [4] has the best overall\nperformance in terms of the evaluation metrics. Our comparison facilitates the\ndevelopment of pending watermarking approaches and enables the model owner to\ndeploy the watermarking scheme that satisfying her requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03713v1"
    },
    {
        "title": "Distribution-Preserving Steganography Based on Text-to-Speech Generative\n  Models",
        "authors": [
            "Kejiang Chen",
            "Hang Zhou",
            "Hanqing Zhao",
            "Dongdong Chen",
            "Weiming Zhang",
            "Nenghai Yu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Steganography is the art and science of hiding secret messages in public\ncommunication so that the presence of the secret messages cannot be detected.\nThere are two distribution-preserving steganographic frameworks, one is\nsampling-based and the other is compression-based. The former requires a\nperfect sampler which yields data following the same distribution, and the\nlatter needs explicit distribution of generative objects. However, these two\nconditions are too strict even unrealistic in the traditional data environment,\ne.g. the distribution of natural images is hard to seize. Fortunately,\ngenerative models bring new vitality to distribution-preserving steganography,\nwhich can serve as the perfect sampler or provide the explicit distribution of\ngenerative media. Take text-to-speech generation task as an example, we propose\ndistribution-preserving steganography based on WaveGlow and WaveNet, which\ncorresponds to the former two categories. Steganalysis experiments and\ntheoretical analysis are conducted to demonstrate that the proposed methods can\npreserve the distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03732v3"
    },
    {
        "title": "A Ginga-enabled Digital Radio Mondiale Broadcasting chain: Signaling and\n  Definitions",
        "authors": [
            "Rafael Diniz",
            "Alan L. V. Guedes",
            "Sergio Colcher"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  ISDB-T International standard is currently adopted by most Latin America\ncountries and is already installed in most TV sets sold in recent years in the\nregion. To support interactive applications in Digital TV receivers, ISDB-T\ndefines the middleware Ginga. Similar to Digital TV, Digital Radio standards\nalso provide the means to carry interactive applications; however, their\nspecifications for interactive applications are usually more restricted than\nthe ones used in Digital TV. Also, interactive applications for Digital TV and\nDigital Radio are usually incompatible. Motivated by such observations, this\nreport considers the importance of interactive applications for both TV and\nRadio Broadcasting and the advantages of using the same middleware and\nlanguages specification for Digital TV and Radio. More specifically, it\nestablishes the signaling and definitions on how to transport and execute\nGinga-NCL and Ginga-HTML5 applications over DRM (Digital Radio Mondiale)\ntransmission. Ministry of Science, Technology, Innovation and Communication of\nBrazil is carrying trials with Digital Radio Mondiale standard in order to\ndefine the reference model of the Brazilian Digital Radio System (Portuguese:\nSistema Brasileiro de R\\'adio Digital - SBRD).\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04193v2"
    },
    {
        "title": "Tiyuntsong: A Self-Play Reinforcement Learning Approach for ABR Video\n  Streaming",
        "authors": [
            "Tianchi Huang",
            "Xin Yao",
            "Chenglei Wu",
            "Rui-Xiao Zhang",
            "Zhangyuan Pang",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Existing reinforcement learning~(RL)-based adaptive bitrate~(ABR) approaches\noutperform the previous fixed control rules based methods by improving the\nQuality of Experience~(QoE) score, as the QoE metric can hardly provide clear\nguidance for optimization, finally resulting in the unexpected strategies. In\nthis paper, we propose \\emph{Tiyuntsong}, a self-play reinforcement learning\napproach with generative adversarial network~(GAN)-based method for ABR video\nstreaming. Tiyuntsong learns strategies automatically by training two agents\nwho are competing against each other. Note that the competition results are\ndetermined by a set of rules rather than a numerical QoE score that allows\nclearer optimization objectives. Meanwhile, we propose GAN Enhancement Module\nto extract hidden features from the past status for preserving the information\nwithout the limitations of sequence lengths. Using testbed experiments, we show\nthat the utilization of GAN significantly improves the Tiyuntsong's\nperformance. By comparing the performance of ABRs, we observe that Tiyuntsong\nalso betters existing ABR algorithms in the underlying metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.06166v3"
    },
    {
        "title": "Motion Style Extraction Based on Sparse Coding Decomposition",
        "authors": [
            "Xuan Thanh Nguyen",
            "Thanh Ha Le",
            "Hongchuan Yu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  We present a sparse coding-based framework for motion style decomposition and\nsynthesis. Dynamic Time Warping is firstly used to synchronized input motions\nin the time domain as a pre-processing step. A sparse coding-based\ndecomposition has been proposed, we also introduce the idea of core component\nand basic motion. Decomposed motions are then combined, transfer to synthesize\nnew motions. Lastly, we develop limb length constraint as a post-processing\nstep to remove distortion skeletons. Our framework has the advantage of less\ntime-consuming, no manual alignment and large dataset requirement. As a result,\nour experiments show smooth and natural synthesized motion.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.06616v1"
    },
    {
        "title": "Content-Aware Personalised Rate Adaptation for Adaptive Streaming via\n  Deep Video Analysis",
        "authors": [
            "Guanyu Gao",
            "Linsen Dong",
            "Huaizheng Zhang",
            "Yonggang Wen",
            "Wenjun Zeng"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Adaptive bitrate (ABR) streaming is the de facto solution for achieving\nsmooth viewing experiences under unstable network conditions. However, most of\nthe existing rate adaptation approaches for ABR are content-agnostic, without\nconsidering the semantic information of the video content. Nevertheless,\nsemantic information largely determines the informativeness and interestingness\nof the video content, and consequently affects the QoE for video streaming. One\ncommon case is that the user may expect higher quality for the parts of video\ncontent that are more interesting or informative so as to reduce video\ndistortion and information loss, given that the overall bitrate budgets are\nlimited. This creates two main challenges for such a problem: First, how to\ndetermine which parts of the video content are more interesting? Second, how to\nallocate bitrate budgets for different parts of the video content with\ndifferent significances? To address these challenges, we propose a\nContent-of-Interest (CoI) based rate adaptation scheme for ABR. We first design\na deep learning approach for recognizing the interestingness of the video\ncontent, and then design a Deep Q-Network (DQN) approach for rate adaptation by\nincorporating video interestingness information. The experimental results show\nthat our method can recognize video interestingness precisely, and the bitrate\nallocation for ABR can be aligned with the interestingness of video content\nwhile not compromising the performances on objective QoE metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.06663v1"
    },
    {
        "title": "Hybrid Distortion Aggregated Visual Comfort Assessment for Stereoscopic\n  Image Retargeting",
        "authors": [
            "Ya Zhou",
            "Zhibo Chen",
            "Weiping Li"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Visual comfort is a quite important factor in 3D media service. Few research\nefforts have been carried out in this area especially in case of 3D content\nretargeting which may introduce more complicated visual distortions. In this\npaper, we propose a Hybrid Distortion Aggregated Visual Comfort Assessment\n(HDA-VCA) scheme for stereoscopic retargeted images (SRI), considering\naggregation of hybrid distortions including structure distortion, information\nloss, binocular incongruity and semantic distortion. Specifically, a Local-SSIM\nfeature is proposed to reflect the local structural distortion of SRI, and\ninformation loss is represented by Dual Natural Scene Statistics (D-NSS)\nfeature extracted from the binocular summation and difference channels.\nRegarding binocular incongruity, visual comfort zone, window violation,\nbinocular rivalry, and accommodation-vergence conflict of human visual system\n(HVS) are evaluated. Finally, the semantic distortion is represented by the\ncorrelation distance of paired feature maps extracted from original\nstereoscopic image and its retargeted image by using trained deep neural\nnetwork. We validate the effectiveness of HDA-VCA on published Stereoscopic\nImage Retargeting Database (SIRD) and two stereoscopic image databases IEEE-SA\nand NBU 3D-VCA. The results demonstrate HDA-VCA's superior performance in\nhandling hybrid distortions compared to state-of-the-art VCA schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.12687v1"
    },
    {
        "title": "Large-Scale and Fine-Grained Evaluation of Popular JPEG Forgery\n  Localization Schemes",
        "authors": [
            "Pawel Korus"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Over the years, researchers have proposed various approaches to JPEG forgery\ndetection and localization. In most cases, experimental evaluation was limited\nto JPEG quality levels that are multiples of 5 or 10. Each study used a\ndifferent dataset, making it difficult to directly compare the reported\nresults. The goal of this work is to perform a unified, large-scale and\nfine-grained evaluation of the most popular state-of-the-art detectors. The\nobtained results allow to compare the detectors with respect to various\ncriteria, and shed more light on the compression configurations where reliable\ntampering localization can be expected.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.12915v2"
    },
    {
        "title": "Visual Distortions in 360-degree Videos",
        "authors": [
            "Roberto G. de A. Azevedo",
            "Neil Birkbeck",
            "Francesca De Simone",
            "Ivan Janatra",
            "Balu Adsumilli",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Omnidirectional (or 360-degree) images and videos are emergent signals in\nmany areas such as robotics and virtual/augmented reality. In particular, for\nvirtual reality, they allow an immersive experience in which the user is\nprovided with a 360-degree field of view and can navigate throughout a scene,\ne.g., through the use of Head Mounted Displays. Since it represents the full\n360-degree field of view from one point of the scene, omnidirectional content\nis naturally represented as spherical visual signals. Current approaches for\ncapturing, processing, delivering, and displaying 360-degree content, however,\npresent many open technical challenges and introduce several types of\ndistortions in these visual signals. Some of the distortions are specific to\nthe nature of 360-degree images, and often different from those encountered in\nthe classical image communication framework. This paper provides a first\ncomprehensive review of the most common visual distortions that alter\n360-degree signals undergoing state of the art processing in common\napplications. While their impact on viewers' visual perception and on the\nimmersive experience at large is still unknown ---thus, it stays an open\nresearch topic--- this review serves the purpose of identifying the main causes\nof visual distortions in the end-to-end 360-degree content distribution\npipeline. It is essential as a basis for benchmarking different processing\ntechniques, allowing the effective design of new algorithms and applications.\nIt is also necessary to the deployment of proper psychovisual studies to\ncharacterise the human perception of these new images in interactive and\nimmersive applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.01848v1"
    },
    {
        "title": "Video Multimethod Assessment Fusion (VMAF) on 360VR contents",
        "authors": [
            "Marta Orduna",
            "César Díaz",
            "Lara Muñoz",
            "Pablo Pérez",
            "Ignacio Benito",
            "Narciso García"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  This paper describes the subjective experiments and subsequent analysis\ncarried out to validate the application of one of the most robust and\ninfluential video quality metrics, Video Multimethod Assessment Fusion (VMAF),\nto 360VR contents. VMAF is a full reference metric initially designed to work\nwith traditional 2D contents. Hence, at first, it cannot be assumed to be\ncompatible with the particularities of the scenario where omnidirectional\ncontent is visualized using a Head-Mounted Display (HMD). Therefore, through a\ncomplete set of tests, we prove that this metric can be successfully used\nwithout any specific training or adjustments to obtain the quality of 360VR\nsequences actually perceived by users.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.06279v1"
    },
    {
        "title": "Spec-ResNet: A General Audio Steganalysis scheme based on Deep Residual\n  Network of Spectrogram",
        "authors": [
            "Yanzhen Ren",
            "Dengkai Liu",
            "Qiaochu Xiong",
            "Jianming Fu",
            "Lina Wang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The widespread application of audio and video communication technology make\nthe compressed audio data flowing over the Internet, and make it become an\nimportant carrier for covert communication. There are many steganographic\nschemes emerged in the mainstream audio compression data, such as AAC and MP3,\nfollowed by many steganalysis schemes. However, these steganalysis schemes are\nonly effective in the specific embedded domain. In this paper, a general\nsteganalysis scheme Spec-ResNet (Deep Residual Network of Spectrogram) is\nproposed to detect the steganography schemes of different embedding domain for\nAAC and MP3. The basic idea is that the steganographic modification of\ndifferent embedding domain will all introduce the change of the decoded audio\nsignal. In this paper, the spectrogram, which is the visual representation of\nthe spectrum of frequencies of audio signal, is adopted as the input of the\nfeature network to extract the universal features introduced by steganography\nschemes; Deep Neural Network Spec-ResNet is well-designed to represent the\nsteganalysis feature; and the features extracted from different spectrogram\nwindows are combined to fully capture the steganalysis features. The experiment\nresults show that the proposed scheme has good detection accuracy and\ngenerality. The proposed scheme has better detection accuracy for three\ndifferent AAC steganographic schemes and MP3Stego than the state-of-arts\nsteganalysis schemes which are based on traditional hand-crafted or CNN-based\nfeature. To the best of our knowledge, the audio steganalysis scheme based on\nthe spectrogram and deep residual network is first proposed in this paper. The\nmethod proposed in this paper can be extended to the audio steganalysis of\nother codec or audio forensics.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.06838v2"
    },
    {
        "title": "On basis images for the digital image representation",
        "authors": [
            "V. N. Gorbachev",
            "L. A. Denisov",
            "E. M. Kaynarova",
            "I. K. Metelev",
            "E. S. Yakovleva"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Digital array orthogonal transformations that can be presented as a\ndecomposition over basis items or basis images are considered. The orthogonal\ntransform provides digital data scattering, a process of pixel energy\nredistributing, that is illustrated with the help of basis images. Data\nscattering plays important role for applications as image coding and\nwatermarking. We established a simple quantum analogues of basis images. They\nare representations of quantum operators that describe transition of single\nparticle between its states.\n  Considering basis images as items of a matrix, we introduced a block matrix\nthat is suitable for orthogonal transforms of multi-dimensional arrays such as\nblock vector, components of which are matrices. We present an orthogonal\ntransform that produces correlation between arrays. Due to correlation new\nfeature of data scattering was found. A presented detection algorithm is an\nexample of how it can be used in frequency domain watermarking.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.07840v1"
    },
    {
        "title": "Benefiting from Duplicates of Compressed Data: Shift-Based Holographic\n  Compression of Images",
        "authors": [
            "Yehuda Dar",
            "Alfred M. Bruckstein"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Storage systems often rely on multiple copies of the same compressed data,\nenabling recovery in case of binary data errors, of course, at the expense of a\nhigher storage cost. In this paper we show that a wiser method of duplication\nentails great potential benefits for data types tolerating approximate\nrepresentations, like images and videos. We propose a method to produce a set\nof distinct compressed representations for a given signal, such that any subset\nof them allows reconstruction of the signal at a quality depending only on the\nnumber of compressed representations utilized. Essentially, we implement the\nholographic representation idea, where all the representations are equally\nimportant in refining the reconstruction. Here we propose to exploit the shift\nsensitivity of common compression processes and generate holographic\nrepresentations via compression of various shifts of the signal. Two\nimplementations for the idea, based on standard compression methods, are\npresented: the first is a simple, optimization-free design. The second approach\noriginates in a challenging rate-distortion optimization, mitigated by the\nalternating direction method of multipliers (ADMM), leading to a process of\nrepeatedly applying standard compression techniques. Evaluation of the\napproach, in conjunction with the JPEG2000 image compression standard, shows\nthe effectiveness of the optimization in providing compressed holographic\nrepresentations that, by means of an elementary reconstruction process, enable\nimpressive gains of several dBs in PSNR over exact duplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.10812v2"
    },
    {
        "title": "Learning from History: Recreating and Repurposing Sister Harriet\n  Padberg's Computer Composed Canon and Free Fugue",
        "authors": [
            "Richard Savery",
            "Benjamin Genchel",
            "Jason Smith",
            "Anthony Caulkins",
            "Molly Jones",
            "Anna Savery"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Harriet Padberg wrote Computer-Composed Canon and Free Fugue as part of her\n1964 dissertation in Mathematics and Music at Saint Louis University. This\nprogram is one of the earliest examples of text-to-music software and\nalgorithmic composition, which are areas of great interest in the present-day\nfield of music technology. This paper aims to analyze the technological\ninnovation, aesthetic design process, and impact of Harriet Padberg's original\n1964 thesis as well as the design of a modern recreation and utilization, in\norder to gain insight to the nature of revisiting older works. Here, we present\nour open source recreation of Padberg's program with a modern interface and,\nthrough its use as an artistic tool by three composers, show how historical\nworks can be effectively used for new creative purposes in contemporary\ncontexts. Not Even One by Molly Jones draws on the historical and social\nsignificance of Harriet Padberg through using her program in a piece about the\nlack of representation of women judges in composition competitions. Brevity by\nAnna Savery utilizes the original software design as a composition tool, and\nThe Padberg Piano by Anthony Caulkins uses the melodic generation of the\noriginal to create a software instrument.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.04470v1"
    },
    {
        "title": "Hierarchical Representation Network for Steganalysis of QIM\n  Steganography in Low-Bit-Rate Speech Signals",
        "authors": [
            "Hao Yang",
            "Zhongliang Yang",
            "YongJian Bao",
            "Yongfeng Huang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  With the Volume of Voice over IP (VoIP) traffic rises shapely, more and more\nVoIP-based steganography methods have emerged in recent years, which poses a\ngreat threat to the security of cyberspace. Low bit-rate speech codecs are\nwidely used in the VoIP application due to its powerful compression capability.\nQIM steganography makes it possible to hide secret information in VoIP streams.\nPrevious research mostly focus on capturing the inter-frame correlation or\ninner-frame correlation features in code-words but ignore the hierarchical\nstructure which exists in speech frame. In this paper, motivated by the complex\nmulti-scale structure, we design a Hierarchical Representation Network to\ntackle the steganalysis of QIM steganography in low-bit-rate speech signal. In\nthe proposed model, Convolution Neural Network (CNN) is used to model the\nhierarchical structure in the speech frame, and three level of attention\nmechanisms are applied at different convolution block, enabling it to attend\ndifferentially to more and less important content in speech frame. Experiments\ndemonstrated that the steganalysis performance of the proposed method can\noutperforms the state-of-the-art methods especially in detecting both short and\nlow embeded speech samples. Moreover, our model needs less computation and has\nhigher time efficiency to be applied to real online services.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.04433v1"
    },
    {
        "title": "Real-Time Visual Navigation in Huge Image Sets Using Similarity Graphs",
        "authors": [
            "Kai Uwe Barthel",
            "Nico Hezel",
            "Konstantin Schall",
            "Klaus Jung"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Nowadays stock photo agencies often have millions of images. Non-stop viewing\nof 20 million images at a speed of 10 images per second would take more than\nthree weeks. This demonstrates the impossibility to inspect all images and the\ndifficulty to get an overview of the entire collection. Although there has been\na lot of effort to improve visual image search, there is little research and\nsupport for visual image exploration. Typically, users start \"exploring\" an\nimage collection with a keyword search or an example image for a similarity\nsearch. Both searches lead to long unstructured lists of result images. In\nearlier publications, we introduced the idea of graph-based image navigation\nand proposed an efficient algorithm for building hierarchical image similarity\ngraphs for dynamically changing image collections. In this demo we showcase\nreal-time visual exploration of millions of images with a standard web browser.\nSubsets of images are successively retrieved from the graph and displayed as a\nvisually sorted 2D image map, which can be zoomed and dragged to explore\nrelated concepts. Maintaining the positions of previously shown images creates\nthe impression of an \"endless map\". This approach allows an easy visual\nimage-based navigation, while preserving the complex image relationships of the\ngraph.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.06005v1"
    },
    {
        "title": "A Hybrid Control Scheme for Adaptive Live Streaming",
        "authors": [
            "Huan Peng",
            "Yuan Zhang",
            "Yongbei Yang",
            "Jinyao Yan"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The live streaming is more challenging than on-demand streaming, because the\nlow latency is also a strong requirement in addition to the trade-off between\nvideo quality and jitters in playback. To balance several inherently\nconflicting performance metrics and improve the overall quality of experience\n(QoE), many adaptation schemes have been proposed. Bitrate adaptation is one of\nthe major solutions for video streaming under time-varying network conditions,\nwhich works even better combining with some latency control methods, such as\nadaptive playback rate control and frame dropping. However, it still remains a\nchallenging problem to design an algorithm to combine these adaptation schemes\ntogether. To tackle this problem, we propose a hybrid control scheme for\nadaptive live streaming, namely HYSA, based on heuristic playback rate control,\nlatency-constrained bitrate control and QoE-oriented adaptive frame dropping.\nThe proposed scheme utilizes Kaufman's Adaptive Moving Average (KAMA) to\npredict segment bitrates for better rate decisions. Extensive simulations\ndemonstrate that HYSA outperforms most of the existing adaptation schemes on\noverall QoE.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.06043v1"
    },
    {
        "title": "Hiding data inside images using orthogonal moments",
        "authors": [
            "Anier Soria-Lorente",
            "Stefan Berres",
            "Ernesto Avila-Domenech"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this contribution we propose a novel steganographic method based on\nseveral orthogonal polynomials and their combinations. The steganographic\nalgorithm embeds a secrete message at the first eight coefficients of high\nfrequency image. Moreover, this embedding method uses the Beta chaotic map to\ndetermine the order of the blocks where the secret bits will be inserted. In\naddition, from a 128-bit private key and the steps of a cryptography algorithm\naccording to the Advanced Encryption Standard (AES) to generate the key\nexpansion, the proposed method generates a key expansion of 2560 bits, with the\npurpose to permute the first eight coefficients of high frequency before the\ninsertion. The insertion takes eventually place at the first eight high\nfrequency coefficients in the transformed orthogonal moments domain. Before the\ninsertion of the message the image undergoes a series of transformations. After\nthe insertion the inverse transformations are applied to the original\ntransformations in reverse order. The experimental work on the validation of\nthe algorithm consists of the calculation of the Peak Signal-to-Noise Ratio\n(PSNR), the Universal Image Quality Index (UIQI), the Image Fidelity (IF), and\nthe Relative Entropy (RE), comparing the same characteristics for the cover and\nstego image. The proposed algorithm improves the level of imperceptibility and\nsecurity analyzed through the PSNR and RE values, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.07383v1"
    },
    {
        "title": "Fast Steganalysis Method for VoIP Streams",
        "authors": [
            "Hao Yang",
            "ZhongLiang Yang",
            "YongJian Bao",
            "YongFeng Huang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this letter, we present a novel and extremely fast steganalysis method of\nVoice over IP (VoIP) streams, driven by the need for a quick and accurate\ndetection of possible steganography in VoIP streams. We firstly analyzed the\ncorrelations in carriers. To better exploit the correlation in code-words, we\nmapped vector quantization code-words into a semantic space. In order to\nachieve high detection efficiency, only one hidden layer is utilized to extract\nthe correlations between these code-words. Finally, based on the extracted\ncorrelation features, we used the softmax classifier to categorize the input\nstream carriers. To boost the performance of this proposed model, we\nincorporate a simple knowledge distillation framework into the training\nprocess. Experimental results show that the proposed method achieves\nstate-of-the-art performance both in detection accuracy and efficiency. In\nparticular, the processing time of this method on average is only about 0.05\\%\nwhen sample length is as short as 0.1s, attaching strong practical value to\nonline serving of steganography monitor.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.14571v1"
    },
    {
        "title": "Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow\n  Based QoE",
        "authors": [
            "Wei Quan",
            "Yuxuan Pan",
            "Bin Xiang",
            "Lin Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  With the merit of containing full panoramic content in one camera, Virtual\nReality (VR) and 360-degree videos have attracted more and more attention in\nthe field of industrial cloud manufacturing and training. Industrial Internet\nof Things (IoT), where many VR terminals needed to be online at the same time,\ncan hardly guarantee VR's bandwidth requirement. However, by making use of\nusers' quality of experience (QoE) awareness factors, including the relative\nmoving speed and depth difference between the viewpoint and other content,\nbandwidth consumption can be reduced. In this paper, we propose OFB-VR (Optical\nFlow Based VR), an interactive method of VR streaming that can make use of VR\nusers' QoE awareness to ease the bandwidth pressure. The Just-Noticeable\nDifference through Optical Flow Estimation (JND-OFE) is explored to quantify\nusers' awareness of quality distortion in 360-degree videos. Accordingly, a\nnovel 360-degree videos QoE metric based on PSNR and JND-OFE (PSNR-OF) is\nproposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling\nscheme to lessen the tiling overhead. A Reinforcement Learning(RL) method is\nimplemented to make use of historical data to perform Adaptive BitRate(ABR).\nFor evaluation, we take two prior VR streaming schemes, Pano and Plato, as\nbaselines. Vast evaluations show that our system can increase the mean PSNR-OF\nscore by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano\nand Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that\nOFB-VR is a promising prototype for actual interactive industrial VR. A\nprototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.07583v1"
    },
    {
        "title": "FAURAS: A Proxy-based Framework for Ensuring the Fairness of Adaptive\n  Video Streaming over HTTP/2 Server Push",
        "authors": [
            "Chanh Minh Tran",
            "Tho Nguyen Duc",
            "Phan Xuan Tan",
            "Eiji Kamioka"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  HTTP/2 video streaming has caught a lot of attentions in the development of\nmultimedia technologies over the last few years. In HTTP/2, the server push\nmechanism allows the server to deliver more video segments to the client within\na single request in order to deal with the requests explosion problem. As a\nresult, recent research efforts have been focusing on utilizing such a feature\nto enhance the streaming experience while reducing the request-related\noverhead. However, current works only optimize the performance of a single\nclient, without necessary concerns of possible influences on other clients in\nthe same network. When multiple streaming clients compete for a shared\nbandwidth in HTTP/1.1, they are likely to suffer from unfairness, which is\ndefined as the inequality in their bitrate selections. For HTTP/1.1, existing\nworks have proven that the network-assisted solutions are effective in solving\nthe unfairness problem. However, the feasibility of utilizing such an approach\nfor the HTTP/2 server push has not been investigated. Therefore, in this paper,\na novel proxy-based framework is proposed to overcome the unfairness problem in\nadaptive streaming over HTTP/2 with the server push. Experimental results\nconfirm the outperformance of the proposed framework in ensuring the fairness,\nassisting the clients to avoid rebuffering events and lower bitrate degradation\namplitude, while maintaining the mechanism of the server push feature.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.08619v1"
    },
    {
        "title": "Edge-assisted Viewport Adaptive Scheme for real-time Omnidirectional\n  Video transmission",
        "authors": [
            "Tao Guo",
            "Xikang Jiang",
            "Bin Xiang",
            "Lin Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Omnidirectional applications are immersive and highly interactive, which can\nimprove the efficiency of remote collaborative work among factory workers. The\ntransmission of omnidirectional video (OV) is the most important step in\nimplementing virtual remote collaboration. Compared with the ordinary video\ntransmission, OV transmission requires more bandwidth, which is still a huge\nburden even under 5G networks. The tile-based scheme can reduce bandwidth\nconsumption. However, it neither accurately obtain the field of view(FOV) area,\nnor difficult to support real-time OV streaming. In this paper, we propose an\nedge-assisted viewport adaptive scheme (EVAS-OV) to reduce bandwidth\nconsumption during real-time OV transmission. First, EVAS-OV uses a Gated\nRecurrent Unit(GRU) model to predict users' viewport. Then, users were divided\ninto multicast clusters thereby further reducing the consumption of computing\nresources. EVAS-OV reprojects OV frames to accurately obtain users' FOV area\nfrom pixel level and adopt a redundant strategy to reduce the impact of\nviewport prediction errors. All computing tasks were offloaded to edge servers\nto reduce the transmission delay and improve bandwidth utilization.\nExperimental results show that EVAS-OV can save more than 60\\% of bandwidth\ncompared with the non-viewport adaptive scheme. Compared to a two-layer scheme\nwith viewport adaptive, EVAS-OV still saves 30\\% of bandwidth.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.09580v1"
    },
    {
        "title": "FacebookVideoLive18: A Live Video Streaming Dataset for Streams Metadata\n  and Online Viewers Locations",
        "authors": [
            "Emna Baccour",
            "Aiman Erbad",
            "Kashif Bilal",
            "Amr Mohamed",
            "Mohsen Guizani",
            "Mounir Hamdi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  With the advancement in personal smart devices and pervasive network\nconnectivity, users are no longer passive content consumers, but also\ncontributors in producing new contents. This expansion in live services\nrequires a detailed analysis of broadcasters' and viewers' behavior to maximize\nusers' Quality of Experience (QoE). In this paper, we present a dataset\ngathered from one of the popular live streaming platforms: Facebook. In this\ndataset, we stored more than 1,500,000 live stream records collected in June\nand July 2018. These data include public live videos from all over the world.\nHowever, Facebook live API does not offer the possibility to collect online\nvideos with their fine grained data. The API allows to get the general data of\na stream, only if we know its ID (identifier). Therefore, using the live map\nwebsite provided by Facebook and showing the locations of online streams and\nlocations of viewers, we extracted video IDs and different coordinates along\nwith general metadata. Then, having these IDs and using the API, we can collect\nthe fine grained metadata of public videos that might be useful for the\nresearch community. We also present several preliminary analyses to describe\nand identify the patterns of the streams and viewers. Such fine grained details\nwill enable the multimedia community to recreate real-world scenarios\nparticularly for resource allocation, caching, computation, and transcoding in\nedge networks. Existing datasets do not provide the locations of the viewers,\nwhich limits the efforts made to allocate the multimedia resources as close as\npossible to viewers and to offer better QoE.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.10820v1"
    },
    {
        "title": "Impact of the Number of Votes on the Reliability and Validity of\n  Subjective Speech Quality Assessment in the Crowdsourcing Approach",
        "authors": [
            "Babak Naderi",
            "Tobias Hossfeld",
            "Matthias Hirth",
            "Florian Metzger",
            "Sebastian Möller",
            "Rafael Zequeira Jiménez"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The subjective quality of transmitted speech is traditionally assessed in a\ncontrolled laboratory environment according to ITU-T Rec. P.800. In turn, with\ncrowdsourcing, crowdworkers participate in a subjective online experiment using\ntheir own listening device, and in their own working environment. Despite such\nless controllable conditions, the increased use of crowdsourcing micro-task\nplatforms for quality assessment tasks has pushed a high demand for\nstandardized methods, resulting in ITU-T Rec. P.808. This work investigates the\nimpact of the number of judgments on the reliability and the validity of\nquality ratings collected through crowdsourcing-based speech quality\nassessments, as an input to ITU-T Rec. P.808 . Three crowdsourcing experiments\non different platforms were conducted to evaluate the overall quality of three\ndifferent speech datasets, using the Absolute Category Rating procedure. For\neach dataset, the Mean Opinion Scores (MOS) are calculated using differing\nnumbers of crowdsourcing judgements. Then the results are compared to MOS\nvalues collected in a standard laboratory experiment, to assess the validity of\ncrowdsourcing approach as a function of number of votes. In addition, the\nreliability of the average scores is analyzed by checking inter-rater\nreliability, gain in certainty, and the confidence of the MOS. The results\nprovide a suggestion on the required number of votes per condition, and allow\nto model its impact on validity and reliability.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.11300v1"
    },
    {
        "title": "Development and Validation of Pictographic Scales for Rapid Assessment\n  of Affective States in Virtual Reality",
        "authors": [
            "Christian Krüger",
            "Tanja Kojić",
            "Luis Meier",
            "Sebastian Möller",
            "Jan-Niklas Voigt-Antons"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper describes the development and validation of a continuous\npictographic scale for self-reported assessment of affective states in virtual\nenvironments. The developed tool, called Morph A Mood (MAM), consists of a 3D\ncharacter whose facial expression can be adjusted with simple controller\ngestures according to the perceived affective state to capture valence and\narousal scores. It was tested against the questionnaires Pick-A-Mood (PAM) and\nSelf-Assessment Manikin (SAM) in an experiment in which the participants (N =\n32) watched several one-minute excerpts from music videos of the DEAP database\nwithin a virtual environment and assessed their mood after each clip. The\nexperiment showed a high correlation with regard to valence, but only a\nmoderate one with regard to arousal. No statistically significant differences\nwere found between the SAM ratings of this experiment and MAM, but between the\nvalence values of MAM and the DEAP database and between the arousal values of\nMAM and PAM. In terms of user experience, MAM and PAM hardly differ.\nFurthermore, the experiment showed that assessments inside virtual environments\nare significantly faster than with paper-pencil methods, where media devices\nsuch as headphones and display goggles must be put on and taken off.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.00034v1"
    },
    {
        "title": "Robust Wavelet-Based Watermarking Using Dynamic Strength Factor",
        "authors": [
            "Mahsa Kadkhodaei",
            "Shadrokh Samavi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In unsecured network environments, ownership protection of digital contents,\nsuch as images, is becoming a growing concern. Different watermarking methods\nhave been proposed to address the copyright protection of digital materials.\nWatermarking methods are challenged with conflicting parameters of\nimperceptibility and robustness. While embedding a watermark with a high\nstrength factor increases robustness, it also decreases imperceptibility of the\nwatermark. Thus embedding in visually less sensitive regions, i.e., complex\nimage blocks could satisfy both requirements. This paper presents a new\nwavelet-based watermarking technique using an adaptive strength factor to\ntradeoff between watermark transparency and robustness. We measure variations\nof each image block to adaptively set a strength-factor for embedding the\nwatermark in that block. On the other hand, the decoder uses the selected\ncoefficients to safely extract the watermark through a voting algorithm. The\nproposed method shows better results in terms of PSNR and BER in comparison to\nrecent methods for attacks, such as Median Filter, Gaussian Filter, and JPEG\ncompression.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.02940v1"
    },
    {
        "title": "Delay Sensitivity Classification of Cloud Gaming Content",
        "authors": [
            "Saeed Shafiee Sabet",
            "Steven Schmidt",
            "Saman Zadtootaghaj",
            "Carsten Griwodz",
            "Sebastian Moller"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Cloud Gaming is an emerging service that catches growing interest in the\nresearch community as well as industry. While the paradigm shift from a game\nexecution on clients to streaming games from the cloud offers a variety of\nbenefits, the new services also require a highly reliable and low latency\nnetwork to achieve a satisfying Quality of Experience (QoE) for its users.\nUsing a cloud gaming service with high latency would harm the interaction of\nthe user with the game, leading to a decrease in playing performance and thus\nfrustration of players. However, the negative effect of delay on gaming QoE\ndepends strongly on the game content. At a certain level of delay, a slow-paced\ncard game is typically not as delay sensitive as a shooting game. For optimal\nresource allocation and quality estimation, it is highly important for cloud\nproviders, game developers, and network planners to consider the impact of the\ngame content. This paper contributes to a better understanding of the delay\nimpact on QoE for cloud gaming applications by identifying game characteristics\ninfluencing the delay perception of users. In addition, an expert evaluation\nmethodology to quantify these characteristics, as well as a delay sensitivity\nclassification based on a decision tree is presented. The ratings of 14 experts\nfor the quantification indicated an excellent level of agreement which\ndemonstrates the reliability of the proposed method. Additionally, the decision\ntree reached an accuracy of 86.6 % on determining the delay sensitivity classes\nwhich were derived from a large dataset of subjective input quality ratings\nduring a series of experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.05609v2"
    },
    {
        "title": "Transformation of Mean Opinion Scores to Avoid Misleading of Ranked\n  based Statistical Techniques",
        "authors": [
            "Babak Naderi",
            "Sebastian Möller"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The rank correlation coefficients and the ranked-based statistical tests (as\na subset of non-parametric techniques) might be misleading when they are\napplied to subjectively collected opinion scores. Those techniques assume that\nthe data is measured at least at an ordinal level and define a sequence of\nscores to represent a tied rank when they have precisely an equal numeric\nvalue.\n  In this paper, we show that the definition of tied rank, as mentioned above,\nis not suitable for Mean Opinion Scores (MOS) and might be misleading\nconclusions of rank-based statistical techniques. Furthermore, we introduce a\nmethod to overcome this issue by transforming the MOS values considering their\n$95\\%$ Confidence Intervals. The rank correlation coefficients and ranked-based\nstatistical tests can then be safely applied to the transformed values. We also\nprovide open-source software packages in different programming languages to\nutilize the application of our transformation method in the quality of\nexperience domain.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11490v1"
    },
    {
        "title": "Improving embedding efficiency for digital steganography by exploiting\n  similarities between secret and cover images",
        "authors": [
            "Alan A. Abdulla",
            "Harin Sellahewa",
            "Sabah A. Jassim"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Digital steganography is becoming a common tool for protecting sensitive\ncommunications in various applications such as crime(terrorism) prevention\nwhereby law enforcing personals need to remotely compare facial images captured\nat the scene of crime with faces databases of known criminals(suspects);\nexchanging military maps or surveillance video in hostile\nenvironment(situations); privacy preserving in the healthcare systems when\nstoring or exchanging patient medical images(records); and prevent bank\ncustomers accounts(records) from being accessed illegally by unauthorized\nusers. Existing digital steganography schemes for embedding secret images in\ncover image files tend not to exploit various redundancies in the secret image\nbit-stream to deal with the various conflicting requirements on embedding\ncapacity, stego-image quality, and un-detectibility. This paper is concerned\nwith the development of innovative image procedures and data hiding schemes\nthat exploit, as well as increase, similarities between secret image bit-stream\nand the cover image LSB plane. This will be achieved in two novel steps\ninvolving manipulating both the secret and the cover images,prior to embedding,\nto achieve higher 0:1 ratio in both the secret image bit-stream and the cover\nimage LSB plane. The above two steps strategy has been exploited to use a\nbit-plane(s) mapping technique, instead of bit-plane(s) replacement to make\neach cover pixel usable for secret embedding. This paper will demonstrate that\nthis strategy produces stego-images that have minimal distortion, high\nembedding efficiency, reasonably good stego-image quality and robustness\nagainst 3 well-known targeted steganalysis tools.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11974v1"
    },
    {
        "title": "Steganography Based on Pixel Intensity Value Decomposition",
        "authors": [
            "Alan Anwer Abdulla",
            "Harin Sellahewa",
            "Sabah A. Jassim"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper focuses on steganography based on pixel intensity value\ndecomposition. A number of existing schemes such as binary, Fibonacci, Prime,\nNatural, Lucas, and Catalan-Fibonacci (CF) are evaluated in terms of payload\ncapacity and stego quality. A new technique based on a specific representation\nis used to decompose pixel intensity values into 16 (virtual) bit-planes\nsuitable for embedding purposes. The new decomposition scheme has a desirable\nproperty whereby the sum of all bit-planes does not exceed the maximum pixel\nintensity value, i.e. 255. Experimental results demonstrate that the new\ndecomposition scheme offers a better compromise between payload capacity and\nstego quality than other existing decomposition schemes used for embedding\nmessages. However, embedding in the 6th bit-plane onwards, the proposed scheme\noffers better stego quality. In general, the new decomposition technique has\nless effect in terms of quality on pixel value when compared to most existing\npixel intensity value decomposition techniques when embedding messages in\nhigher bit-planes.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11977v1"
    },
    {
        "title": "Efficient High Capacity Steganography Technique",
        "authors": [
            "Alan Anwer Abdulla",
            "Sabah A. Jassim",
            "Harin Sellahewa"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Performance indicators characterizing modern steganographic techniques\ninclude capacity (i.e. the quantity of data that can be hidden in the cover\nmedium), stego quality (i.e. artifacts visibility), security (i.e.\nundetectability), and strength or robustness (intended as the resistance\nagainst active attacks aimed to destroy the secret message). Fibonacci based\nembedding techniques have been researched and proposed in the literature to\nachieve efficient steganography in terms of capacity with respect to stego\nquality. In this paper, we investigated an innovative idea that extends\nFibonacci-like steganography by bit-plane(s) mapping instead of bit-plane(s)\nreplacement. Our proposed algorithm increases embedding capacity using\nbit-plane mapping to embed two bits of the secret message in three bits of a\npixel of the cover, at the expense of a marginal loss in stego quality. While\nexisting Fibonacci embedding algorithms do not use certain intensities of the\ncover for embedding due to the limitation imposed by the Zeckendorf theorem,\nour proposal solve this problem and make all intensity values candidates for\nembedding. Experimental results demonstrate that the proposed technique double\nthe embedding capacity when compared to existing Fibonacci methods, and it is\nsecure against statistical attacks such as RS, POV, and difference image\nhistogram (DIH).\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11984v1"
    },
    {
        "title": "Stego Quality Enhancement by Message Size Reduction and Fibonacci\n  Bit-Plane Mapping",
        "authors": [
            "Alan A. Abdulla",
            "Harin Sellahewa",
            "Sabah A. Jassim"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  An efficient 2-step steganography technique is proposed to enhance stego\nimage quality and secret message un-detectability. The first step is a\npreprocessing algorithm that reduces the size of secret images without losing\ninformation. This results in improved stego image quality compared to other\nexisting image steganography methods. The proposed secret image size reduction\n(SISR) algorithm is an efficient spatial domain technique. The second step is\nan embedding mechanism that relies on Fibonacci representation of pixel\nintensities to minimize the effect of embedding on the stego image quality. The\nimprovement is attained by using bit-plane(s) mapping instead of bit-plane(s)\nreplacement for embedding. The proposed embedding mechanism outperforms the\nbinary based LSB randomly embedding in two ways: reduced effect on stego\nquality and increased robustness against statistical steganalysers.\nExperimental results demonstrate the benefits of the proposed scheme in terms\nof: 1) SISR ratio (indirectly results in increased capacity); 2) quality of the\nstego; and 3) robustness against steganalysers such as RS, and WS. Furthermore,\nexperimental results show that the proposed SISR algorithm can be extended to\nbe applicable on DICOM standard medical images. Future security standardization\nresearch is proposed that would focus on evaluating the security, performance,\nand effectiveness of steganography algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12467v1"
    },
    {
        "title": "Secure Steganography Technique Based on Bitplane Indexes",
        "authors": [
            "Alan Anwer Abdulla",
            "Sabah A. Jassim",
            "Harin Sellahewa"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper is concerned with secret hiding in multiple image bitplanes for\nincreased security without undermining capacity. A secure steganographic\nalgorithm based on bitplanes index manipulation is proposed. The index\nmanipulation is confined to the first two Least Significant Bits of the cover\nimage. The proposed algorithm has the property of un-detectability with respect\nto stego quality and payload capacity. Experimental results demonstrate that\nthe proposed technique is secure against statistical attacks such as pair of\nvalue (PoV), Weighted Stego steganalyser (WS), and Multi Bitplane Weighted\nStego steganalyser (MLSB-WS).\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12470v1"
    },
    {
        "title": "Reversible data hiding in encrypted images based on pixel prediction and\n  multi-MSB planes rearrangement",
        "authors": [
            "Zhaoxia Yin",
            "Xiaomeng She",
            "Jin Tang",
            "Bin Luo"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Great concern has arisen in the field of reversible data hiding in encrypted\nimages (RDHEI) due to the development of cloud storage and privacy protection.\nRDHEI is an effective technology that can embed additional data after image\nencryption, extract additional data error-free and reconstruct original images\nlosslessly. In this paper, a high-capacity and fully reversible RDHEI method is\nproposed, which is based on pixel prediction and multi-MSB (most significant\nbit) planes rearrangement. First, the median edge detector (MED) predictor is\nused to calculate the predicted value. Next, unlike previous methods, in our\nproposed method, signs of prediction errors (PEs) are represented by one bit\nplane and absolute values of PEs are represented by other bit planes. Then, we\ndivide bit planes into uniform blocks and non-uniform blocks, and rearrange\nthese blocks. Finally, according to different pixel prediction schemes,\ndifferent numbers of additional data are embedded adaptively. The experimental\nresults prove that our method has higher embedding capacity compared with\nstate-of-the-art RDHEI methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.03922v3"
    },
    {
        "title": "Reversible Data Hiding in Encrypted Images Based on Bit-plane\n  Compression of Prediction Error",
        "authors": [
            "Youqing Wu",
            "Wenjing Ma",
            "Yinyin Peng",
            "Ruiling Zhang",
            "Zhaoxia Yin"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  As a technology that can prevent the information from being disclosed, the\nreversible data hiding in encrypted images (RDHEI) acts as an important role in\nprivacy protection and information security. To make use of the image\nredundancy and further improve the embedding performance, a high-capacity RDHEI\nmethod based on bit-plane compression of prediction error is proposed in this\npaper. Firstly, the whole prediction error is calculated and divided into\nblocks of the same size. Then, the content owner rearranges the bit-plane of\nprediction error by block and compresses the bitstream with the joint encoding\nalgorithm to reserve room. Finally, the image is encrypted and the information\ncan be embedded into the reserved room. On the receiver side, the information\nextraction and the image recovery are performed separably. Experimental results\nshow that the proposed method brings higher embedding capacity than\nstate-of-the-art RDHEI works.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.04057v3"
    },
    {
        "title": "$\\ell_1$SABMIS: $\\ell_1$-minimization and sparse approximation based\n  blind multi-image steganography scheme",
        "authors": [
            "Rohit Agrawal"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Steganography plays a vital role in achieving secret data security by\nembedding it into cover media. The cover media and the secret data can be text\nor multimedia, such as images, videos, etc. In this paper, we propose a novel\n$\\ell_1$-minimization and sparse approximation based blind multi-image\nsteganography scheme, termed $\\ell_1$SABMIS. By using $\\ell_1$SABMIS, multiple\nsecret images can be hidden in a single cover image. In $\\ell_1$SABMIS, we\nsampled cover image into four sub-images, sparsify each sub-image block-wise,\nand then obtain linear measurements. Next, we obtain DCT (Discrete Cosine\nTransform) coefficients of the secret images and then embed them into the cover\nimage\\textquotesingle s linear measurements.\n  We perform experiments on several standard gray-scale images, and evaluate\nembedding capacity, PSNR (peak signal-to-noise ratio) value, mean SSIM\n(structural similarity) index, NCC (normalized cross-correlation) coefficient,\nNAE (normalized absolute error), and entropy. The value of these assessment\nmetrics indicates that $\\ell_1$SABMIS outperforms similar existing\nsteganography schemes. That is, we successfully hide more than two secret\nimages in a single cover image without degrading the cover image significantly.\nAlso, the extracted secret images preserve good visual quality, and\n$\\ell_1$SABMIS is resistant to steganographic attack.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.05025v1"
    },
    {
        "title": "QUALINET White Paper on Definitions of Immersive Media Experience (IMEx)",
        "authors": [
            "Andrew Perkis",
            "Christian Timmerer",
            "Sabina Baraković",
            "Jasmina Baraković Husić",
            "Søren Bech",
            "Sebastian Bosse",
            "Jean Botev",
            "Kjell Brunnström",
            "Luis Cruz",
            "Katrien De Moor",
            "Andrea de Polo Saibanti",
            "Wouter Durnez",
            "Sebastian Egger-Lampl",
            "Ulrich Engelke",
            "Tiago H. Falk",
            "Jesús Gutiérrez",
            "Asim Hameed",
            "Andrew Hines",
            "Tanja Kojic",
            "Dragan Kukolj",
            "Eirini Liotou",
            "Dragorad Milovanovic",
            "Sebastian Möller",
            "Niall Murray",
            "Babak Naderi",
            "Manuela Pereira",
            "Stuart Perry",
            "Antonio Pinheiro",
            "Andres Pinilla",
            "Alexander Raake",
            "Sarvesh Rajesh Agrawal",
            "Ulrich Reiter",
            "Rafael Rodrigues",
            "Raimund Schatz",
            "Peter Schelkens",
            "Steven Schmidt",
            "Saeed Shafiee Sabet",
            "Ashutosh Singla",
            "Lea Skorin-Kapov",
            "Mirko Suznjevic",
            "Stefan Uhrig",
            "Sara Vlahović",
            "Jan-Niklas Voigt-Antons",
            "Saman Zadtootaghaj"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  With the coming of age of virtual/augmented reality and interactive media,\nnumerous definitions, frameworks, and models of immersion have emerged across\ndifferent fields ranging from computer graphics to literary works. Immersion is\noftentimes used interchangeably with presence as both concepts are closely\nrelated. However, there are noticeable interdisciplinary differences regarding\ndefinitions, scope, and constituents that are required to be addressed so that\na coherent understanding of the concepts can be achieved. Such consensus is\nvital for paving the directionality of the future of immersive media\nexperiences (IMEx) and all related matters. The aim of this white paper is to\nprovide a survey of definitions of immersion and presence which leads to a\ndefinition of immersive media experience (IMEx). The Quality of Experience\n(QoE) for immersive media is described by establishing a relationship between\nthe concepts of QoE and IMEx followed by application areas of immersive media\nexperience. Influencing factors on immersive media experience are elaborated as\nwell as the assessment of immersive media experience. Finally, standardization\nactivities related to IMEx are highlighted and the white paper is concluded\nwith an outlook related to future developments.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.07032v2"
    },
    {
        "title": "Robust adaptive steganography based on dither modulation and\n  modification with re-compression",
        "authors": [
            "Zhaoxia Yin",
            "Longfei Ke"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Traditional adaptive steganography is a technique used for covert\ncommunication with high security, but it is invalid in the case of stego images\nare sent to legal receivers over networks which is lossy, such as JPEG\ncompression of channels. To deal with such problem, robust adaptive\nsteganography is proposed to enable the receiver to extract secret messages\nfrom the damaged stego images. Previous works utilize reverse engineering and\ncompression-resistant domain constructing to implement robust adaptive\nsteganography. In this paper, we adopt modification with re-compression scheme\nto improve the robustness of stego sequences in stego images. To balance\nsecurity and robustness, we move the embedding domain to the low frequency\nregion of DCT (Discrete Cosine Transform) coefficients to improve the security\nof robust adaptive steganography. In addition, we add additional check codes to\nfurther reduce the average extraction error rate based on the framework of\nE-DMAS (Enhancing Dither Modulation based robust Adaptive Steganography).\nCompared with GMAS (Generalized dither Modulation based robust Adaptive\nSteganography) and E-DMAS, experiment results show that our scheme can achieve\nstrong robustness and improve the security of robust adaptive steganography\ngreatly when the channel quality factor is known.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.08301v2"
    },
    {
        "title": "Ari: The Automated R Instructor",
        "authors": [
            "Sean Kross",
            "Jeffrey T. Leek",
            "John Muschelli"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  We present the ari package for automatically generating technology-focused\neducational videos. The goal of the package is to create reproducible videos,\nwith the ability to change and update video content seamlessly. We present\nseveral examples of generating videos including using R Markdown slide decks,\nPowerPoint slides, or simple images as source material. We also discuss how ari\ncan help instructors reach new audiences through programmatically translating\nmaterials into other languages.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.13477v2"
    },
    {
        "title": "ComboLoss for Facial Attractiveness Analysis with Squeeze-and-Excitation\n  Networks",
        "authors": [
            "Lu Xu",
            "Jinhai Xiang"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Loss function is crucial for model training and feature representation\nlearning, conventional models usually regard facial attractiveness recognition\ntask as a regression problem, and adopt MSE loss or Huber variant loss as\nsupervision to train a deep convolutional neural network (CNN) to predict\nfacial attractiveness score. Little work has been done to systematically\ncompare the performance of diverse loss functions. In this paper, we firstly\nsystematically analyze model performance under diverse loss functions. Then a\nnovel loss function named ComboLoss is proposed to guide the SEResNeXt50\nnetwork. The proposed method achieves state-of-the-art performance on SCUT-FBP,\nHotOrNot and SCUT-FBP5500 datasets with an improvement of 1.13%, 2.1% and 0.57%\ncompared with prior arts, respectively. Code and models are available at\nhttps://github.com/lucasxlu/ComboLoss.git.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.10721v1"
    },
    {
        "title": "Multi-domain Reversible Data Hiding in JPEG",
        "authors": [
            "Zhaoxia Yin",
            "Hongnian Guo",
            "Yang Du"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  As a branch of reversible data hiding (RDH), reversible data hiding in JEPG\nis particularly important. Because JPEG images are widely used, it is great\nsignificance to study reversible data hiding algorithm for JEPG images. The\nexisting JEPG reversible data methods can be divided into two categories, one\nis based on Discrete Cosine Transform (DCT) coefficients modification, the\nother is based on Huffman table modification, the methods based on DCT\ncoefficient modification result in large file expansion and visual quality\ndistortion, while the methods based on entropy coding domain modification have\nlow capacity and they may lead to large file expansion. In order to effectively\nsolve the problems in these two kinds of methods, this paper proposes a\nreversible data hiding in JPEG images methods based on multi-domain\nmodification. In this method, the secret data is divided into two parts by\npayload distribution algorithm, part of the secret data is first embedded in\nthe DCT coefficient domain, and then the remaining secret data is embedded in\nthe entropy coding domain. Experimental results demonstrate that most JPEG\nimage files with this scheme have smaller file size increment and higher\npayload than previous RDH schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.04959v1"
    },
    {
        "title": "CNN-based driving of block partitioning for intra slices encoding",
        "authors": [
            "Franck Galpin",
            "Fabien Racapé",
            "Sunil Jaiswal",
            "Philippe Bordes",
            "Fabrice Le Léannec",
            "Edouard François"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper provides a technical overview of a deep-learning-based encoder\nmethod aiming at optimizing next generation hybrid video encoders for driving\nthe block partitioning in intra slices. An encoding approach based on\nConvolutional Neural Networks is explored to partly substitute classical\nheuristics-based encoder speed-ups by a systematic and automatic process. The\nsolution allows controlling the trade-off between complexity and coding gains,\nin intra slices, with one single parameter. This algorithm was proposed at the\nCall for Proposals of the Joint Video Exploration Team (JVET) on video\ncompression with capability beyond HEVC. In All Intra configuration, for a\ngiven allowed topology of splits, a speed-up of $\\times 2$ is obtained without\nBD-rate loss, or a speed-up above $\\times 4$ with a loss below 1\\% in BD-rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.06691v1"
    },
    {
        "title": "A Block-Permutation-Based Encryption Scheme with Independent Processing\n  of RGB Components",
        "authors": [
            "Shoko Imaizumi",
            "Hitoshi Kiya"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper proposes a block-permutation-based encryption (BPBE) scheme for\nthe encryption-then-compression (ETC) system that enhances the color\nscrambling. A BPBE image can be obtained through four processes, positional\nscrambling, block rotation/flip, negative-positive transformation, and color\ncomponent shuffling, after dividing the original image into multiple blocks.\nThe proposed scheme scrambles the R, G, and B components independently in\npositional scrambling, block rotation/flip, and negative-positive\ntransformation, by assigning different keys to each color component. The\nconventional scheme considers the compression efficiency using JPEG and JPEG\n2000, which need a color conversion before the compression process by default.\nTherefore, the conventional scheme scrambles the color components identically\nin each process. In contrast, the proposed scheme takes into account the\nRGB-based compression, such as JPEG-LS, and thus can increase the extent of the\nscrambling. The resilience against jigsaw puzzle solver (JPS) can consequently\nbe increased owing to the wider color distribution of the BPBE image.\nAdditionally, the key space for resilience against brute-force attacks has also\nbeen expanded exponentially. Furthermore, the proposed scheme can maintain the\nJPEG-LS compression efficiency compared to the conventional scheme. We confirm\nthe effectiveness of the proposed scheme by experiments and analyses.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.08964v1"
    },
    {
        "title": "Cloud-Based Video Streaming Services: A Survey",
        "authors": [
            "Xiangbo Li",
            "Mahmoud Darwich",
            "Magdy Bayoumi",
            "Mohsen Amini Salehi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Video streaming, in various forms of video on demand (VOD), live, and 360\ndegree streaming, has grown dramatically during the past few years. In\ncomparison to traditional cable broadcasters whose contents can only be watched\non TVs, video streaming is ubiquitous and viewers can flexibly watch the video\ncontents on various devices, ranging from smart-phones to laptops and large TV\nscreens. Such ubiquity and flexibility are enabled by interweaving multiple\ntechnologies, such as video compression, cloud computing, content delivery\nnetworks, and several other technologies. As video streaming gains more\npopularity and dominates the Internet traffic, it is essential to understand\nthe way it operates and the interplay of different technologies involved in it.\nAccordingly, the first goal of this paper is to unveil sophisticated processes\nto deliver a raw captured video to viewers' devices. In particular, we\nelaborate on the video encoding, transcoding, packaging, encryption, and\ndelivery processes. We survey recent efforts in academia and industry to\nenhance these processes. As video streaming industry is increasingly becoming\nreliant on cloud computing, the second goal of this survey is to explore and\nsurvey the ways cloud services are utilized to enable video streaming services.\nThe third goal of the study is to position the undertaken research works in\ncloud-based video streaming and identify challenges that need to be obviated in\nfuture to advance cloud-based video streaming industry to a more flexible and\nuser-centric service.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.14976v1"
    },
    {
        "title": "Duration-Squeezing-Aware Communication and Computing for Proactive VR",
        "authors": [
            "Xing Wei",
            "Chenyang Yang",
            "Shengqian Han"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Proactive tile-based virtual reality video streaming computes and delivers\nthe predicted tiles to be requested before playback. All existing works\noverlook the important fact that computing and communication (CC) tasks for a\nsegment may squeeze the time for the tasks for the next segment, which will\ncause less and less available time for the latter segments. In this paper, we\njointly optimize the durations for CC tasks to maximize the completion rate of\nCC tasks under the task duration-squeezing-aware constraint. To ensure the\nlatter segments remain enough time for the tasks, the CC tasks for a segment\nare not allowed to squeeze the time for computing and delivering the subsequent\nsegment. We find the closed-form optimal solution, from which we find a\nminimum-resource-limited, an unconditional and a conditional resource-tradeoff\nregions, which are determined by the total time for proactive CC tasks and the\nplayback duration of a segment. Owing to the duration-squeezing-prohibited\nconstraints, the increase of the configured resources may not be always useful\nfor improving the completion rate of CC tasks. Numerical results validate the\nimpact of the duration-squeezing-prohibited constraints and illustrate the\nthree regions.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.00611v1"
    },
    {
        "title": "Domain Generalization for Document Authentication against Practical\n  Recapturing Attacks",
        "authors": [
            "Changsheng Chen",
            "Shuzheng Zhang",
            "Fengbo Lan",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Recapturing attack can be employed as a simple but effective anti-forensic\ntool for digital document images. Inspired by the document inspection process\nthat compares a questioned document against a reference sample, we proposed a\ndocument recapture detection scheme by employing Siamese network to compare and\nextract distinct features in a recapture document image. The proposed algorithm\ntakes advantages of both metric learning and image forensic techniques. Instead\nof adopting Euclidean distance-based loss function, we integrate the forensic\nsimilarity function with a triplet loss and a normalized softmax loss. After\ntraining with the proposed triplet selection strategy, the resulting feature\nembedding clusters the genuine samples near the reference while pushes the\nrecaptured samples apart. In the experiment, we consider practical domain\ngeneralization problems, such as the variations in printing/imaging devices,\nsubstrates, recapturing channels, and document types. To evaluate the\nrobustness of different approaches, we benchmark some popular off-the-shelf\nmachine learning-based approaches, a state-of-the-art document image detection\nscheme, and the proposed schemes with different network backbones under various\nexperimental protocols. Experimental results show that the proposed schemes\nwith different network backbones have consistently outperformed the\nstate-of-the-art approaches under different experimental settings.\nSpecifically, under the most challenging scenario in our experiment, i.e.,\nevaluation across different types of documents that produced by different\ndevices, we have achieved less than 5.00% APCER (Attack Presentation\nClassification Error Rate) and 5.56% BPCER (Bona Fide Presentation\nClassification Error Rate) by the proposed network with ResNeXt101 backbone at\n5% BPCER decision threshold.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.01404v2"
    },
    {
        "title": "QoE-driven Secure Video Transmission in Cloud-edge Collaborative\n  Networks",
        "authors": [
            "Tantan Zhao",
            "Lijun He",
            "Xinyu Huang",
            "Fan Li"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Video transmission over the backhaul link in cloud-edge collaborative\nnetworks usually suffers security risks, which is ignored in most of the\nexisting studies. The characteristics that video service can flexibly adjust\nthe encoding rates and provide acceptable encoding qualities, make the security\nrequirements more possible to be satisfied but tightly coupled with video\nencoding by introducing more restrictions on edge caching. In this paper, by\nconsidering the interaction between video encoding and edge caching, we\ninvestigate the quality of experience (QoE)-driven cross-layer optimization of\nsecure video transmission over the wireless backhaul link in cloud-edge\ncollaborative networks. First, we develop a secure transmission model based on\nvideo encoding and edge caching. By employing this model as the security\nconstraint, then we formulate a QoE-driven joint optimization problem subject\nto limited available caching capacity. To solve the optimization problem, we\npropose two algorithms: a near-optimal iterative algorithm (EC-VE) and a greedy\nalgorithm with low computational complexity (Greedy EC-VE). Simulation results\nshow that our proposed EC-VE can greatly improve user QoE within security\nconstraints, and the proposed Greedy EC-VE can obtain the tradeoff between QoE\nand computational complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.01484v3"
    },
    {
        "title": "Facial Biometric System for Recognition using Extended LGHP Algorithm on\n  Raspberry Pi",
        "authors": [
            "Soumendu Chakraborty",
            "Satish Kumar Singh",
            "Kush Kumar"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In todays world, where the need for security is paramount and biometric\naccess control systems are gaining mass acceptance due to their increased\nreliability, research in this area is quite relevant. Also with the advent of\nIOT devices and increased community support for cheap and small computers like\nRaspberry Pi its convenient than ever to design a complete standalone system\nfor any purpose. This paper proposes a Facial Biometric System built on the\nclient-server paradigm using Raspberry Pi 3 model B running a novel local\ndescriptor based parallel algorithm. This paper also proposes an extended\nversion of Local Gradient Hexa Pattern with improved accuracy. The proposed\nextended version of LGHP improved performance as shown in performance analysis.\nExtended LGHP shows improvement over other state-of-the-art descriptors namely\nLDP, LTrP, MLBP and LVP on the most challenging benchmark facial image\ndatabases, i.e. Cropped Extended Yale-B, CMU-PIE, color-FERET, LFW, and\nGhallager database. Proposed system is also compared with various patents\nhaving similar system design and intent to emphasize the difference and novelty\nof the system proposed.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03413v1"
    },
    {
        "title": "Network-Distributed Video Coding",
        "authors": [
            "Johan De Praeter",
            "Christopher Hollmann",
            "Rickard Sjoberg",
            "Glenn Van Wallendael",
            "Peter Lambert"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Nowadays, an enormous amount of videos are streamed every day to countless\nusers, all using different devices and networks. These videos must be adapted\nin order to provide users with the most suitable video representation based on\ntheir device properties and current network conditions. However, the two most\ncommon techniques for video adaptation, simulcast and transcoding, represent\ntwo extremes. The former offers excellent scalability, but requires a large\namount of storage, while the latter has a small storage cost, but is not\nscalable to many users due to the additional computing cost per requested\nrepresentation. As a third, in-between approach, network-distributed video\ncoding (NDVC) was proposed within the Moving Picture Experts Group (MPEG). The\naim of NDVC is to reduce the storage cost compared to simulcast, while\nretaining a smaller computing cost compared to transcoding. By exploring the\nproposed techniques for NDVC, we show the workings of this third option for\nvideo providers to deliver their contents to their clients.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.04475v1"
    },
    {
        "title": "An Ultra-Specific Image Dataset for Automated Insect Identification",
        "authors": [
            "D. L. Abeywardhana",
            "C. D. Dangalle",
            "Anupiya Nugaliyadde",
            "Yashas Mallawarachchi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Automated identification of insects is a tough task where many challenges\nlike data limitation, imbalanced data count, and background noise needs to be\novercome for better performance. This paper describes such an image dataset\nwhich consists of a limited, imbalanced number of images regarding six genera\nof subfamily Cicindelinae (tiger beetles) of order Coleoptera. The diversity of\nimage collection is at a high level as the images were taken from different\nsources, angles and on different scales. Thus, the salient regions of the\nimages have a large variation. Therefore, one of the main intentions in this\nprocess was to get an idea about the image dataset while comparing different\nunique patterns and features in images. The dataset was evaluated on different\nclassification algorithms including deep learning models based on different\napproaches to provide a benchmark. The dynamic nature of the dataset poses a\nchallenge to the image classification algorithms. However transfer learning\nmodels using softmax classifier performed well on current dataset. The tiger\nbeetle classification can be challenging even to a trained human eye,\ntherefore, this dataset opens a new avenue for the classification algorithms to\ndevelop, to identify features which human eyes have not identified.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.11463v1"
    },
    {
        "title": "An attention-based unsupervised adversarial model for movie review spam\n  detection",
        "authors": [
            "Yuan Gao",
            "Maoguo Gong",
            "Yu Xie",
            "A. K. Qin"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  With the prevalence of the Internet, online reviews have become a valuable\ninformation resource for people. However, the authenticity of online reviews\nremains a concern, and deceptive reviews have become one of the most urgent\nnetwork security problems to be solved. Review spams will mislead users into\nmaking suboptimal choices and inflict their trust in online reviews. Most\nexisting research manually extracted features and labeled training samples,\nwhich are usually complicated and time-consuming. This paper focuses primarily\non a neglected emerging domain - movie review, and develops a novel\nunsupervised spam detection model with an attention mechanism. By extracting\nthe statistical features of reviews, it is revealed that users will express\ntheir sentiments on different aspects of movies in reviews. An attention\nmechanism is introduced in the review embedding, and the conditional generative\nadversarial network is exploited to learn users' review style for different\ngenres of movies. The proposed model is evaluated on movie reviews crawled from\nDouban, a Chinese online community where people could express their feelings\nabout movies. The experimental results demonstrate the superior performance of\nthe proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.00955v1"
    },
    {
        "title": "Learning Image Aesthetic Assessment from Object-level Visual Components",
        "authors": [
            "Jingwen Hou",
            "Sheng Yang",
            "Weisi Lin",
            "Baoquan Zhao",
            "Yuming Fang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  As it is said by Van Gogh, great things are done by a series of small things\nbrought together. Aesthetic experience arises from the aggregation of\nunderlying visual components. However, most existing deep image aesthetic\nassessment (IAA) methods over-simplify the IAA process by failing to model\nimage aesthetics with clearly-defined visual components as building blocks. As\na result, the connection between resulting aesthetic predictions and underlying\nvisual components is mostly invisible and hard to be explicitly controlled,\nwhich limits the model in both performance and interpretability. This work aims\nto model image aesthetics from the level of visual components. Specifically,\nobject-level regions detected by a generic object detector are defined as\nvisual components, namely object-level visual components (OVCs). Then generic\nfeatures representing OVCs are aggregated for the aesthetic prediction based\nupon proposed object-level and graph attention mechanisms, which dynamically\ndetermines the importance of individual OVCs and relevance between OVC pairs,\nrespectively. Experimental results confirm the superiority of our framework\nover previous relevant methods in terms of SRCC and PLCC on the aesthetic\nrating distribution prediction. Besides, quantitative analysis is done towards\nmodel interpretation by observing how OVCs contribute to aesthetic predictions,\nwhose results are found to be supported by psychology on aesthetics and\nphotography rules. To the best of our knowledge, this is the first attempt at\nthe interpretation of a deep IAA model.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.01548v1"
    },
    {
        "title": "Subjective Assessment Experiments That Recruit Few Observers With\n  Repetitions (FOWR)",
        "authors": [
            "Pablo Perez",
            "Lucjan Janowski",
            "Narciso Garcia",
            "Margaret Pinson"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Recent studies have shown that it is possible to characterize subject bias\nand variance in subjective assessment tests. Apparent differences among\nsubjects can, for the most part, be explained by random factors. Building on\nthat theory, we propose a subjective test design where three to four team\nmembers each rate the stimuli multiple times. The results are comparable to a\nhigh performing objective metric. This provides a quick and simple way to\nanalyze new technologies and perform pre-tests for subjective assessment.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.02618v2"
    },
    {
        "title": "Spike Camera and Its Coding Methods",
        "authors": [
            "Siwei Dong",
            "Tiejun Huang",
            "Yonghong Tian"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  This paper introduces a spike camera with a distinct video capture scheme and\nproposes two methods of decoding the spike stream for texture reconstruction.\nThe spike camera captures light and accumulates the converted luminance\nintensity at each pixel. A spike is fired when the accumulated intensity\nexceeds the dispatch threshold. The spike stream generated by the camera\nindicates the luminance variation. Analyzing the patterns of the spike stream\nmakes it possible to reconstruct the picture of any moment which enables the\nplayback of high speed movement.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.04669v1"
    },
    {
        "title": "Towards SocialVR: Evaluating a Novel Technology for Watching Videos\n  Together",
        "authors": [
            "Mario Montagud",
            "Jie Li",
            "Gianluca Cernigliario",
            "Abdallah El Ali",
            "Sergi Fernandez",
            "Pablo Cesar"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Social VR enables people to interact over distance with others in real-time.\nIt allows remote people, typically represented as avatars, to communicate and\nperform activities together in a join shared virtual environment, extending the\ncapabilities of traditional social platforms like Facebook and Netflix. This\npaper explores the benefits and drawbacks provided by a lightweight and\nlow-cost Social VR platform (SocialVR), in which users are captured by several\ncameras and reconstructed in real-time. In particular, the paper contributes\nwith (1) the design and evaluation of an experimental protocol for Social VR\nexperiences; (2) the report of a production workflow for this new type of media\nexperiences; and (3) the results of experiments with both end-users (N=15\npairs) and professionals (N=25) to evaluate the potential of the SocialVR\nplatform. Results from the questionnaires and semi-structured interviews show\nthat end-users rated positively towards the experiences provided by the\nSocialVR platform, which enabled them to sense emotions and communicate\neffortlessly. End-users perceived the photo-realistic experience of SocialVR\nsimilar to face-to-face scenarios and appreciated this new creative medium.\nFrom a commercial perspective, professionals confirmed the potential of this\ncommunication medium and encourage further research for the adoption of the\nplatform in the commercial landscape\n",
        "pdf_link": "http://arxiv.org/pdf/2104.05060v1"
    },
    {
        "title": "Optimal Transmission of Multi-Quality Tiled 360 VR Video in MIMO-OFDMA\n  Systems",
        "authors": [
            "Chengjun Guo",
            "Ying Cui",
            "Zhi Liu",
            "Derrick Wing Kwan Ng"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In this paper, we study the optimal transmission of a multi-quality tiled 360\nvirtual reality (VR) video from a multi-antenna server (e.g., access point or\nbase station) to multiple single-antenna users in a multiple-input\nmultiple-output (MIMO)-orthogonal frequency division multiple access (OFDMA)\nsystem. We minimize the total transmission power with respect to the subcarrier\nallocation constraints, rate allocation constraints, and successful\ntransmission constraints, by optimizing the beamforming vector and subcarrier,\ntransmission power and rate allocation. The formulated resource allocation\nproblem is a challenging mixed discrete-continuous optimization problem. We\nobtain an asymptotically optimal solution in the case of a large antenna array,\nand a suboptimal solution in the general case. As far as we know, this is the\nfirst work providing optimization-based design for 360 VR video transmission in\nMIMO-OFDMA systems. Finally, by numerical results, we show that the proposed\nsolutions achieve significant improvement in performance compared to the\nexisting solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.06183v1"
    },
    {
        "title": "Landmarking for Navigational Streaming of Stored High-Dimensional Media",
        "authors": [
            "Yuan Yuan",
            "Gene Cheung",
            "Pascal Frossard",
            "H. Vicky Zhao",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Modern media data such as 360 videos and light field (LF) images are\ntypically captured in much higher dimensions than the observers' visual\ndisplays. To efficiently browse high-dimensional media over\nbandwidth-constrained networks, a navigational streaming model is considered: a\nclient navigates the large media space by dictating a navigation path to a\nserver, who in response transmits the corresponding pre-encoded media data\nunits (MDU) to the client one-by-one in sequence. Intra-coding an MDU (I-MDU)\nwould result in a large bitrate but I-MDU can be randomly accessed, while\ninter-coding an MDU (P-MDU) using another MDU as a predictor incurs a small\ncoding cost but imposes an order where the predictor must be first transmitted\nand decoded. From a compression perspective, the technical challenge is: how to\nachieve coding gain via inter-coding of MDUs, while enabling adequate random\naccess for satisfactory user navigation. To address this problem, we propose\nlandmarks, a selection of key MDUs from the high-dimensional media. Using\nlandmarks as predictors, nearby MDUs in local neighborhoods are intercoded,\nresulting in a predictive MDU structure with controlled coding cost. It means\nthat any requested MDU can be decoded by at most transmitting a landmark and an\ninter-coded MDU, enabling navigational random access. To build a landmarked MDU\nstructure, we employ tree-structured vector quantizer (TSVQ) to first optimize\nlandmark locations, then iteratively add/remove inter-coded MDUs as refinements\nusing a fast branch-and-bound technique. Taking interactive LF images and\nviewport adaptive 360 images as illustrative applications, and I-, P- and\npreviously proposed merge frames to intra- and inter-code MDUs, we show\nexperimentally that landmarked MDU structures can noticeably reduce the\nexpected transmission cost compared with MDU structures without landmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.06876v3"
    },
    {
        "title": "AffectiveNet: Affective-Motion Feature Learningfor Micro Expression\n  Recognition",
        "authors": [
            "Monu Verma",
            "Santosh Kumar Vipparthi",
            "Girdhari Singh"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Micro-expressions are hard to spot due to fleeting and involuntary moments of\nfacial muscles. Interpretation of micro emotions from video clips is a\nchallenging task. In this paper we propose an affective-motion imaging that\ncumulates rapid and short-lived variational information of micro expressions\ninto a single response. Moreover, we have proposed an\nAffectiveNet:affective-motion feature learning network that can perceive subtle\nchanges and learns the most discriminative dynamic features to describe the\nemotion classes. The AffectiveNet holds two blocks: MICRoFeat and MFL block.\nMICRoFeat block conserves the scale-invariant features, which allows network to\ncapture both coarse and tiny edge variations. While MFL block learns\nmicro-level dynamic variations from two different intermediate convolutional\nlayers. Effectiveness of the proposed network is tested over four datasets by\nusing two experimental setups: person independent (PI) and cross dataset (CD)\nvalidation. The experimental results of the proposed network outperforms the\nstate-of-the-art approaches with significant margin for MER approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.07569v1"
    },
    {
        "title": "CTU Depth Decision Algorithms for HEVC: A Survey",
        "authors": [
            "Ekrem Cetinkaya",
            "Hadi Amirpour",
            "Mohammad Ghanbari",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  High-Efficiency Video Coding (HEVC) surpasses its predecessors in encoding\nefficiency by introducing new coding tools at the cost of an increased encoding\ntime-complexity. The Coding Tree Unit (CTU) is the main building block used in\nHEVC. In the HEVC standard, frames are divided into CTUs with the predetermined\nsize of up to 64x64 pixels. Each CTU is then divided recursively into a number\nof equally sized square areas, known as Coding Units (CUs). Although this\ndiversity of frame partitioning increases encoding efficiency, it also causes\nan increase in the time complexity due to the increased number of ways to find\nthe optimal partitioning. To address this complexity, numerous algorithms have\nbeen proposed to eliminate unnecessary searches during partitioning CTUs by\nexploiting the correlation in the video. In this paper, existing CTU depth\ndecision algorithms for HEVC are surveyed. These algorithms are categorized\ninto two groups, namely statistics and machine learning approaches. Statistics\napproaches are further subdivided into neighboring and inherent approaches.\nNeighboring approaches exploit the similarity between adjacent CTUs to limit\nthe depth range of the current CTU, while inherent approaches use only the\navailable information within the current CTU. Machine learning approaches try\nto extract and exploit similarities implicitly. Traditional methods like\nsupport vector machines or random forests use manually selected features, while\nrecently proposed deep learning methods extract features during training.\nFinally, this paper discusses extending these methods to more recent video\ncoding formats such as Versatile Video Coding (VVC) and AOMedia Video 1(AV1).\n",
        "pdf_link": "http://arxiv.org/pdf/2104.08328v2"
    },
    {
        "title": "New Technology, New Rules for Journalism and a New World of Engagement",
        "authors": [
            "Loup M. Langton",
            "Mercedes L. de Uriarte",
            "Kim Grinfeder",
            "Paulo Nuno Vicente"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The ways in which people learn, communicate and engage in discussion have\nchanged profoundly during the past decade. As Jenkins related in her book, The\nConvergence Crisis: An Impending Paradigm Shift in Advertising, Millenials do\nnot want to be told the whole story. Rather, they want someone to begin a\nconversation that will engage others to become participants in the development\nof that story (2015). Technology now allows that to happen, sometimes with\nunintended and/or ill consequences, but technology also generates a dynamic\npotential to create international and interactive discourse aimed at addressing\nshared global challenges.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.08592v1"
    },
    {
        "title": "Privacy-aware VR streaming",
        "authors": [
            "Xing Wei",
            "Chenyang Yang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Proactive tile-based virtual reality (VR) video streaming employs the current\ntracking data of a user to predict future requested tiles, then renders and\ndelivers the predicted tiles to be requested before playback. The quality of\nexperience (QoE) depends on the overall performance of prediction, computing\n(i.e., rendering) and communication. All prior works neglect that users may\nhave privacy requirement, i.e., not all the current tracking data are allowed\nto be uploaded. In this paper, we investigate the privacy-aware VR streaming.\nWe first establish a dataset that collects the privacy requirement of 66 users\namong 18 panoramic videos. The dataset shows that the privacy requirements of\n360$^{\\circ}$ videos are heterogeneous. Only 41\\% of the total watched videos\nhave no privacy requirement. Based on these findings, we formulate the privacy\nrequirement as the \\textit{degree of privacy} (DoP), and investigate the impact\nof DoP on the proactive VR streaming. First, we find that with DoP, the length\nof the observation window and prediction window of a tile predictor should be\nvariable. Then, we jointly optimize the durations for computing and\ntransmitting the selected tiles as well as the computing and communication\ncapability, aimed at maximizing the QoE given arbitrary predictor and\nconfigured resources. From the obtained optimal closed-form solution, we find a\nresource-saturated region where DoP has no impact on the QoE and a\nresource-unsaturated region where the two-fold impacts of DoP are\ncontradictory. On the one hand, the increase of DoP will degrade the prediction\nperformance and thus degrade the QoE. On the other hand, the increase of DoP\nwill improve the capability of computing and communication and thus improve the\nQoE. Simulation results using two predictors and a real dataset validate the\nanalysis and demonstrate the overall impact of DoP on the QoE.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.09779v1"
    },
    {
        "title": "Improving Hierarchy Storage for Video Streaming in Cloud",
        "authors": [
            "Mahmoud Darwich",
            "Yasser Ismail",
            "Talal Darwich",
            "Magdy Bayoumi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Frequently accessed video streams are pre-transcoded into several formats to\nsatisfy the characteristics of all display devices. Storing several video\nstream formats imposes a high cost on video stream providers using the old\nclassical way. Alternatively, cloud providers offer a high flexibility of using\ntheir services and at a low cost relatively. Therefore, video stream companies\nadopted cloud technology to store their video streams. Generally, having all\nvideo streams stored in one type of cloud storage, the cost rises gradually.\nMore importantly, the variation of the access pattern to frequently accessed\nvideo streams impacts negatively the storage cost and increases it\nsignificantly. To optimize storage usage and lower its cost, we propose a\nmethod that manages the cloud hierarchy storage. Particularly, we develop an\nalgorithm that operates on parts of different videos that are frequently\naccessed and stores them in their suitable storage type cloud. Experiments came\nup with promising results on reducing the cost of using cloud storage by 18.75\n%.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.11317v1"
    },
    {
        "title": "DVMark: A Deep Multiscale Framework for Video Watermarking",
        "authors": [
            "Xiyang Luo",
            "Yinxiao Li",
            "Huiwen Chang",
            "Ce Liu",
            "Peyman Milanfar",
            "Feng Yang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Video watermarking embeds a message into a cover video in an imperceptible\nmanner, which can be retrieved even if the video undergoes certain\nmodifications or distortions. Traditional watermarking methods are often\nmanually designed for particular types of distortions and thus cannot\nsimultaneously handle a broad spectrum of distortions. To this end, we propose\na robust deep learning-based solution for video watermarking that is end-to-end\ntrainable. Our model consists of a novel multiscale design where the watermarks\nare distributed across multiple spatial-temporal scales. It gains robustness\nagainst various distortions through a differentiable distortion layer, whereas\nnon-differentiable distortions, such as popular video compression standards,\nare modeled by a differentiable proxy. Extensive evaluations on a wide variety\nof distortions show that our method outperforms traditional video watermarking\nmethods as well as deep image watermarking models by a large margin. We further\ndemonstrate the practicality of our method on a realistic video-editing\napplication.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.12734v1"
    },
    {
        "title": "Towards Harmonized Regional Style Transfer and Manipulation for Facial\n  Images",
        "authors": [
            "Cong Wang",
            "Fan Tang",
            "Yong Zhang",
            "Weiming Dong",
            "Tieru Wu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Regional facial image synthesis conditioned on semantic mask has achieved\ngreat success using generative adversarial networks. However, the appearance of\ndifferent regions may be inconsistent with each other when conducting regional\nimage editing. In this paper, we focus on the problem of harmonized regional\nstyle transfer and manipulation for facial images. The proposed approach\nsupports regional style transfer and manipulation at the same time. A\nmulti-scale encoder and style mapping networks are proposed in our work. The\nencoder is responsible for extracting regional styles of real faces. Style\nmapping networks generate styles from random samples for all facial regions. As\nthe key part of our work, we propose a multi-region style attention module to\nadapt the multiple regional style embeddings from a reference image to a target\nimage for generating harmonious and plausible results. Furthermore, we propose\na new metric \"harmony score\" and conduct experiments in a challenging setting:\nthree widely used face datasets are involved and we test the model by\ntransferring the regional facial appearance between datasets. Images in\ndifferent datasets are usually quite different, which makes the inconsistency\nbetween target and reference regions more obvious. Results show that our model\ncan generate reliable style transfer and multi-modal manipulation results\ncompared with SOTAs. Furthermore, we show two face editing applications using\nthe proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14109v1"
    },
    {
        "title": "LIQA: Lifelong Blind Image Quality Assessment",
        "authors": [
            "Jianzhao Liu",
            "Wei Zhou",
            "Jiahua Xu",
            "Xin Li",
            "Shukun An",
            "Zhibo Chen"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Existing blind image quality assessment (BIQA) methods are mostly designed in\na disposable way and cannot evolve with unseen distortions adaptively, which\ngreatly limits the deployment and application of BIQA models in real-world\nscenarios. To address this problem, we propose a novel Lifelong blind Image\nQuality Assessment (LIQA) approach, targeting to achieve the lifelong learning\nof BIQA. Without accessing to previous training data, our proposed LIQA can not\nonly learn new distortions, but also mitigate the catastrophic forgetting of\nseen distortions. Specifically, we adopt the Split-and-Merge distillation\nstrategy to train a single-head network that makes task-agnostic predictions.\nIn the split stage, we first employ a distortion-specific generator to obtain\nthe pseudo features of each seen distortion. Then, we use an auxiliary\nmulti-head regression network to generate the predicted quality of each seen\ndistortion. In the merge stage, we replay the pseudo features paired with\npseudo labels to distill the knowledge of multiple heads, which can build the\nfinal regressed single head. Experimental results demonstrate that the proposed\nLIQA method can handle the continuous shifts of different distortion types and\neven datasets. More importantly, our LIQA model can achieve stable performance\neven if the task sequence is long.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14115v1"
    },
    {
        "title": "Automatic Generation of H.264 Parameter Sets to Recover Video File\n  Fragments",
        "authors": [
            "Enes Altinisik",
            "Hüsrev Taha Sencar"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We address the problem of decoding video file fragments when the necessary\nencoding parameters are missing. With this objective, we propose a method that\nautomatically generates H.264 video headers containing these parameters and\nextracts coded pictures in the partially available compressed video data. To\naccomplish this, we examined a very large corpus of videos to learn patterns of\nencoding settings commonly used by encoders and created a parameter dictionary.\nFurther, to facilitate a more efficient search our method identifies\ncharacteristics of a coded bitstream to discriminate the entropy coding mode.\nIt also utilizes the application logs created by the decoder to identify\ncorrect parameter values. Evaluation of the effectiveness of the proposed\nmethod on more than 55K videos with diverse provenance shows that it can\ngenerate valid headers on average in 11.3 decoding trials per video. This\nresult represents an improvement by more than a factor of 10 over the\nconventional approach of video header stitching to recover video file\nfragments.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14522v3"
    },
    {
        "title": "The Big Brother NaijaTV Reality Show as Coordinate of Media Functions\n  and Dysfunctions",
        "authors": [
            "Bolu John Folayan",
            "Olubunmi Ajibade",
            "Olubunmi Dipo-Adedoyin",
            "Toyin Segun Onayinka",
            "Toluwani Titilola Folayan"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The mass media play at least five basic functions which include news\ndissemination, surveillance of the environment, correlation of the components\nof the society, entertainment and transmission of social heritage. Sometimes,\ndisruptions and impairments do occur in the performance of these roles and some\nof these basic functions become dysfunctions, which turn the media into\npurveyor of negative values. The present study investigates how popular the\nNigerian TV reality show, Big Brother Naija BBN, is perceived by its viewers.\nThree hundred heavy viewers of the program were surveyed from Lagos and Ede,\nSouth-West Nigeria, and their opinions and attitudes were sought regarding, why\nthey like or dislike the program; the gratifications that those who like the\nprogram derive and whether the BBN, as media content, is generally functional\nor dysfunctional to the society. Sixty six per cent 66 33.7 of respondents like\nthe program because it entertains. Half of the respondents, 99 50.5 dislike\nimmoral aspects of the program. The viewers affirm that the eviction part of\nthe program was their highest form of gratification. Most respondents, despite\npublic outcry against the program, consider the program to be functional.\nFindings reinforce the postulation that TV viewers are not passive consumers of\nmedia contents.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14532v1"
    },
    {
        "title": "Cross-Modal Music-Video Recommendation: A Study of Design Choices",
        "authors": [
            "Laure Pretet",
            "Gael Richard",
            "Geoffroy Peeters"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In this work, we study music/video cross-modal recommendation, i.e.\nrecommending a music track for a video or vice versa. We rely on a\nself-supervised learning paradigm to learn from a large amount of unlabelled\ndata. We rely on a self-supervised learning paradigm to learn from a large\namount of unlabelled data. More precisely, we jointly learn audio and video\nembeddings by using their co-occurrence in music-video clips. In this work, we\nbuild upon a recent video-music retrieval system (the VM-NET), which originally\nrelies on an audio representation obtained by a set of statistics computed over\nhandcrafted features. We demonstrate here that using audio representation\nlearning such as the audio embeddings provided by the pre-trained MuSimNet,\nOpenL3, MusicCNN or by AudioSet, largely improves recommendations. We also\nvalidate the use of the cross-modal triplet loss originally proposed in the\nVM-NET compared to the binary cross-entropy loss commonly used in\nself-supervised learning. We perform all our experiments using the Music Video\nDataset (MVD).\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14799v1"
    },
    {
        "title": "Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical\n  Visual Question Answering",
        "authors": [
            "Haifan Gong",
            "Guanqi Chen",
            "Sishuo Liu",
            "Yizhou Yu",
            "Guanbin Li"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Due to the severe lack of labeled data, existing methods of medical visual\nquestion answering usually rely on transfer learning to obtain effective image\nfeature representation and use cross-modal fusion of visual and linguistic\nfeatures to achieve question-related answer prediction. These two phases are\nperformed independently and without considering the compatibility and\napplicability of the pre-trained features for cross-modal fusion. Thus, we\nreformulate image feature pre-training as a multi-task learning paradigm and\nwitness its extraordinary superiority, forcing it to take into account the\napplicability of features for the specific image comprehension task.\nFurthermore, we introduce a cross-modal self-attention~(CMSA) module to\nselectively capture the long-range contextual relevance for more effective\nfusion of visual and linguistic features. Experimental results demonstrate that\nthe proposed method outperforms existing state-of-the-art methods. Our code and\nmodels are available at https://github.com/haifangong/CMSA-MTPT-4-MedicalVQA.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.00136v1"
    },
    {
        "title": "Multi-feature 360 Video Quality Estimation",
        "authors": [
            "Roberto G. de A. Azevedo",
            "Neil Birkbeck",
            "Ivan Janatra",
            "Balu Adsumilli",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We propose a new method for the visual quality assessment of 360-degree\n(omnidirectional) videos. The proposed method is based on computing multiple\nspatio-temporal objective quality features on viewports extracted from\n360-degree videos. A new model is learnt to properly combine these features\ninto a metric that closely matches subjective quality scores. The main\nmotivations for the proposed approach are that: 1) quality metrics computed on\nviewports better captures the user experience than metrics computed on the\nprojection domain; 2) the use of viewports easily supports different projection\nmethods being used in current 360-degree video systems; and 3) no individual\nobjective image quality metric always performs the best for all types of visual\ndistortions, while a learned combination of them is able to adapt to different\nconditions. Experimental results, based on both the largest available\n360-degree videos quality dataset and a cross-dataset validation, demonstrate\nthat the proposed metric outperforms state-of-the-art 360-degree and 2D video\nquality metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.00567v1"
    },
    {
        "title": "A Power and Area Efficient Lepton Hardware Encoder with Hash-based\n  Memory Optimization",
        "authors": [
            "Xiao Yan",
            "Zhixiong Di",
            "Bowen Huang",
            "Minjiang Li",
            "Wenqiang Wang",
            "Xiaoyang Zeng",
            "Yibo Fan"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Although it has been surpassed by many subsequent coding standards, JPEG\noccupies a large share of the storage load of the current data hosting service.\nTo reduce the storage costs, DropBox proposed a lossless secondary compression\nalgorithm, Lepton, to further improve the compression rate of JPEG images.\nHowever, the bloated probability models defined by Lepton severely restrict its\nthroughput and energy efficiency. To solve this problem, we construct an\nefficient access probability-based hash function for the probability models,\nand then propose a hardware-friendly memory optimization method by combining\nthe proposed hash function and the N-way Set-Associative unit. After that, we\ndesign a highly parameterized hardware structure for the probability models and\nfinally implement a power and area efficient Lepton hardware encoder. To the\nbest of our knowledge, this is the first hardware implementation of Lepton. The\nsynthesis result shows that the proposed hardware structure reduces the total\narea of the probability models by 70.97%. Compared with DropBox's software\nsolution, the throughput and the energy efficiency of the proposed Lepton\nhardware encoder are increased by 55.25 and 4899 times respectively. In terms\nof manufacturing cost, the proposed Lepton hardware encoder is also\nsignificantly lower than the general-purpose CPU used by DropBox.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01415v1"
    },
    {
        "title": "Insights on the V3C2 Dataset",
        "authors": [
            "Luca Rossetto",
            "Klaus Schoeffmann",
            "Abraham Bernstein"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  For research results to be comparable, it is important to have common\ndatasets for experimentation and evaluation. The size of such datasets,\nhowever, can be an obstacle to their use. The Vimeo Creative Commons Collection\n(V3C) is a video dataset designed to be representative of video content found\non the web, containing roughly 3800 hours of video in total, split into three\nshards. In this paper, we present insights on the second of these shards (V3C2)\nand discuss their implications for research areas, such as video retrieval, for\nwhich the dataset might be particularly useful. We also provide all the\nextracted data in order to simplify the use of the dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01475v1"
    },
    {
        "title": "Viewport-Aware Dynamic 360° Video Segment Categorization",
        "authors": [
            "Amaya Dharmasiri",
            "Chamara Kattadige",
            "Vincent Zhang",
            "Kanchana Thilakarathna"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Unlike conventional videos, 360{\\deg} videos give freedom to users to turn\ntheir heads, watch and interact with the content owing to its immersive\nspherical environment. Although these movements are arbitrary, similarities can\nbe observed between viewport patterns of different users and different videos.\nIdentifying such patterns can assist both content and network providers to\nenhance the 360{\\deg} video streaming process, eventually increasing the\nend-user Quality of Experience (QoE). But a study on how viewport patterns\ndisplay similarities across different video content, and their potential\napplications has not yet been done. In this paper, we present a comprehensive\nanalysis of a dataset of 88 360{\\deg} videos and propose a novel video\ncategorization algorithm that is based on similarities of viewports. First, we\npropose a novel viewport clustering algorithm that outperforms the existing\nalgorithms in terms of clustering viewports with similar positioning and speed.\nNext, we develop a novel and unique dynamic video segment categorization\nalgorithm that shows notable improvement in similarity for viewport\ndistributions within the clusters when compared to that of existing static\nvideo categorizations.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01701v2"
    },
    {
        "title": "Multimedia Edge Computing",
        "authors": [
            "Zhi Wang",
            "Wenwu Zhu",
            "Lifeng Sun",
            "Han Hu",
            "Ge Ma",
            "Ming Ma",
            "Haitian Pang",
            "Jiahui Ye",
            "Hongshan Li"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In this paper, we investigate the recent studies on multimedia edge\ncomputing, from sensing not only traditional visual/audio data but also\nindividuals' geographical preference and mobility behaviors, to performing\ndistributed machine learning over such data using the joint edge and cloud\ninfrastructure and using evolutional strategies like reinforcement learning and\nonline learning at edge devices to optimize the quality of experience for\nmultimedia services at the last mile proactively. We provide both a\nretrospective view of recent rapid migration (resp. merge) of cloud multimedia\nto (resp. and) edge-aware multimedia and insights on the fundamental guidelines\nfor designing multimedia edge computing strategies that target satisfying the\nchanging demand of quality of experience. By showing the recent research\nstudies and industrial solutions, we also provide future directions towards\nhigh-quality multimedia services over edge computing.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.02409v1"
    },
    {
        "title": "360NorVic: 360-Degree Video Classification from Mobile Encrypted Video\n  Traffic",
        "authors": [
            "Chamara Kattadige",
            "Aravindh Raman",
            "Kanchana Thilakarathna",
            "Andra Lutu",
            "Diego Perino"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Streaming 360{\\deg} video demands high bandwidth and low latency, and poses\nsignificant challenges to Internet Service Providers (ISPs) and Mobile Network\nOperators (MNOs). The identification of 360{\\deg} video traffic can therefore\nbenefits fixed and mobile carriers to optimize their network and provide better\nQuality of Experience (QoE) to the user. However, end-to-end encryption of\nnetwork traffic has obstructed identifying those 360{\\deg} videos from regular\nvideos. As a solution this paper presents 360NorVic, a near-realtime and\noffline Machine Learning (ML) classification engine to distinguish 360{\\deg}\nvideos from regular videos when streamed from mobile devices. We collect packet\nand flow level data for over 800 video traces from YouTube & Facebook\naccounting for 200 unique videos under varying streaming conditions. Our\nresults show that for near-realtime and offline classification at packet level,\naverage accuracy exceeds 95%, and that for flow level, 360NorVic achieves more\nthan 92% average accuracy. Finally, we pilot our solution in the commercial\nnetwork of a large MNO showing the feasibility and effectiveness of 360NorVic\nin production settings.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.03611v1"
    },
    {
        "title": "A Deep Learning Scheme for Efficient Multimedia IoT Data Compression",
        "authors": [
            "Hassan N. Noura",
            "Ola Salman",
            "Raphaël Couturier"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Given the voluminous nature of the multimedia sensed data, the Multimedia\nInternet of Things (MIoT) devices and networks will present several limitations\nin terms of power and communication overhead. One traditional solution to cope\nwith the large-size data challenge is to use lossy compression. However,\ncurrent lossy compression schemes require low compression rate to guarantee\nacceptable perceived image quality, which results in a low reduction of the\ncommunicated data size and consequently a low reduction in the energy and\nbandwidth consumption. Thus, an efficient compression solution is required for\nstriking a good balance between data size (and consequently communication\noverhead) and visual degradation. In this paper, a Deep-Learning (DL)\nsuper-resolution model is applied to recuperate high quality images (at the\napplication server side) given as input degraded images with a high compression\nratio (at the sender side). The experimental analysis shows the effectiveness\nof the proposed solution in enhancing the visual quality of the compressed and\ndown-scaled images. Consequently, the proposed solution reduces the overall\ncommunication overhead and power consumption of limited MIoT devices.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.09280v1"
    },
    {
        "title": "VAD360: Viewport Aware Dynamic 360-Degree Video Frame Tiling",
        "authors": [
            "Chamara Kattadige",
            "Kanchana Thilakarathna"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  360{\\deg} videos a.k.a. spherical videos are getting popular among users\nnevertheless, omnidirectional view of these videos demands high bandwidth and\nprocessing power at the end devices. Recently proposed viewport aware streaming\nmechanisms can reduce the amount of data transmitted by streaming a limited\nportion of the frame covering the current user viewport (VP). However, they\nstill suffer from sending a high amount of redundant data, as the fixed tile\nmechanisms can not provide finer granularity to the user VP. Though making the\ntiles smaller can provide a finer granularity for user viewport, high encoding\noverhead incurred. To overcome this trade-off, in this paper, we present a\ncomputational geometric approach based adaptive tiling mechanism named VAD360,\nwhich takes visual attention information on the 360{\\deg} video frame as the\ninput and provide a suitable non-overlapping variable size tile cover on the\nframe. Experimental results shows that VAD360 can save up to 31.1% of pixel\nredundancy before compression and 35.4% of bandwidth saving compared to\nrecently proposed fixed tile configurations, providing tile schemes within\n0.98($\\pm$0.11)s time frame.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.11563v1"
    },
    {
        "title": "Blind Quality Assessment for in-the-Wild Images via Hierarchical Feature\n  Fusion and Iterative Mixed Database Training",
        "authors": [
            "Wei Sun",
            "Xiongkuo Min",
            "Danyang Tu",
            "Guangtao Zhai",
            "Siwei Ma"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Image quality assessment (IQA) is very important for both end-users and\nservice providers since a high-quality image can significantly improve the\nuser's quality of experience (QoE) and also benefit lots of computer vision\nalgorithms. Most existing blind image quality assessment (BIQA) models were\ndeveloped for synthetically distorted images, however, they perform poorly on\nin-the-wild images, which are widely existed in various practical applications.\nIn this paper, we propose a novel BIQA model for in-the-wild images by\naddressing two critical problems in this field: how to learn better\nquality-aware feature representation, and how to solve the problem of\ninsufficient training samples in terms of their content and distortion\ndiversity. Considering that perceptual visual quality is affected by both\nlow-level visual features (e.g. distortions) and high-level semantic\ninformation (e.g. content), we first propose a staircase structure to\nhierarchically integrate the features from intermediate layers into the final\nfeature representation, which enables the model to make full use of visual\ninformation from low-level to high-level. Then an iterative mixed database\ntraining (IMDT) strategy is proposed to train the BIQA model on multiple\ndatabases simultaneously, so the model can benefit from the increase in both\ntraining samples and image content and distortion diversity and can learn a\nmore general feature representation. Experimental results show that the\nproposed model outperforms other state-of-the-art BIQA models on six\nin-the-wild IQA databases by a large margin. Moreover, the proposed model shows\nan excellent performance in the cross-database evaluation experiments, which\nfurther demonstrates that the learned feature representation is robust to\nimages with diverse distortions and content. The code is available at\nhttps://github.com/sunwei925/StairIQA.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.14550v3"
    },
    {
        "title": "Tree-Structured Data Clustering-Driven Neural Network for Intra\n  Prediction in Video Coding",
        "authors": [
            "Hengyu Man",
            "Xiaopeng Fan",
            "Ruiqin Xiong",
            "Debin Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  As a crucial part of video compression, intra prediction utilizes local\ninformation of images to eliminate the redundancy in spatial domain. In both\nthe High Efficiency Video Coding (H.265/HEVC) and Versatile Video Coding\n(H.266/VVC), multiple directional prediction modes are employed to find the\ntexture trend of each small block and then the prediction is made based on\nreference samples in the selected direction. Recently, the intra prediction\nschemes based on neural networks have achieved great success. In these methods,\nthe networks are trained and applied to intra prediction to assist the\ndirectional prediction modes. In this paper, we propose a novel tree-structured\ndata clustering-driven neural network (dubbed TreeNet) for intra prediction,\nwhich builds the networks and clusters the training data in a tree-structured\nmanner. Specifically, in each network split and training process of TreeNet,\nevery parent network on a leaf node is split into two child networks by adding\nor subtracting Gaussian random noise. Then a data clustering-driven training is\napplied to train the two derived child networks using the clustered training\ndata of their parent. To test the performance, TreeNet is integrated into VVC\nand HEVC to combine with or replace the directional prediction modes. In\naddition, a fast termination strategy is proposed to accelerate the search of\nTreeNet. The experimental results demonstrate that TreeNet with the fast\ntermination can reach an average of 2.8% Bjontegaard distortion rate (BD-rate)\nimprovement (up to 8.1%) and 4.9% BD-rate improvement (up to 8.2%) over VVC\n(VTM-4.0) and HEVC (HM-16.9) with all intra configuration, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.05481v2"
    },
    {
        "title": "An Audio-Driven System For Real-Time Music Visualisation",
        "authors": [
            "Max Graf",
            "Harold Chijioke Opara",
            "Mathieu Barthet"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Computer-generated visualisations can accompany recorded or live music to\ncreate novel audiovisual experiences for audiences. We present a system to\nstreamline the creation of audio-driven visualisations based on audio feature\nextraction and mapping interfaces. Its architecture is based on three modular\nsoftware components: backend (audio plugin), frontend (3D game-like\nenvironment), and middleware (visual mapping interface). We conducted a user\nevaluation comprising two stages. Results from the first stage (34\nparticipants) indicate that music visualisations generated with the system were\nsignificantly better at complementing the music than a baseline visualisation.\nNine participants took part in the second stage involving interactive tasks.\nOverall, the system yielded a Creativity Support Index above average (68.1) and\na System Usability Scale index (58.6) suggesting that ease of use can be\nimproved. Thematic analysis revealed that participants enjoyed the system's\nsynchronicity and expressive capabilities, but found technical problems and\ndifficulties understanding the audio feature terminology.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.10134v1"
    },
    {
        "title": "An Attention Self-supervised Contrastive Learning based Three-stage\n  Model for Hand Shape Feature Representation in Cued Speech",
        "authors": [
            "Jianrong Wang",
            "Nan Gu",
            "Mei Yu",
            "Xuewei Li",
            "Qiang Fang",
            "Li Liu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Cued Speech (CS) is a communication system for deaf people or hearing\nimpaired people, in which a speaker uses it to aid a lipreader in phonetic\nlevel by clarifying potentially ambiguous mouth movements with hand shape and\npositions. Feature extraction of multi-modal CS is a key step in CS\nrecognition. Recent supervised deep learning based methods suffer from noisy CS\ndata annotations especially for hand shape modality. In this work, we first\npropose a self-supervised contrastive learning method to learn the feature\nrepresentation of image without using labels. Secondly, a small amount of\nmanually annotated CS data are used to fine-tune the first module. Thirdly, we\npresent a module, which combines Bi-LSTM and self-attention networks to further\nlearn sequential features with temporal and contextual information. Besides, to\nenlarge the volume and the diversity of the current limited CS datasets, we\nbuild a new British English dataset containing 5 native CS speakers. Evaluation\nresults on both French and British English datasets show that our model\nachieves over 90% accuracy in hand shape recognition. Significant improvements\nof 8.75% (for French) and 10.09% (for British English) are achieved in CS\nphoneme recognition correctness compared with the state-of-the-art.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.14016v1"
    },
    {
        "title": "Learning from Synthetic Data for Opinion-free Blind Image Quality\n  Assessment in the Wild",
        "authors": [
            "Zhihua Wang",
            "Zhi-Ri Tang",
            "Jianguo Zhang",
            "Yuming Fang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Nowadays, most existing blind image quality assessment (BIQA) models 1) are\ndeveloped for synthetically-distorted images and often generalize poorly to\nauthentic ones; 2) heavily rely on human ratings, which are prohibitively\nlabor-expensive to collect. Here, we propose an $opinion$-$free$ BIQA method\nthat learns from synthetically-distorted images and multiple agents to assess\nthe perceptual quality of authentically-distorted ones captured in the wild\nwithout relying on human labels. Specifically, we first assemble a large number\nof image pairs from synthetically-distorted images and use a set of\nfull-reference image quality assessment (FR-IQA) models to assign pseudo-binary\nlabels of each pair indicating which image has higher quality as the\nsupervisory signal. We then train a convolutional neural network (CNN)-based\nBIQA model to rank the perceptual quality, optimized for consistency with the\nbinary labels. Since there exists domain shift between the synthetically- and\nauthentically-distorted images, an unsupervised domain adaptation (UDA) module\nis introduced to alleviate this issue. Extensive experiments demonstrate the\neffectiveness of our proposed $opinion$-$free$ BIQA model, yielding\nstate-of-the-art performance in terms of correlation with human opinion scores,\nas well as gMAD competition. Codes will be made publicly available upon\nacceptance.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.14076v3"
    },
    {
        "title": "Computer Vision-assisted Single-antenna and Single-anchor RSSI\n  Localization Harnessing Dynamic Blockage Events",
        "authors": [
            "Tomoya Sunami",
            "Sohei Itahara",
            "Yusuke Koda",
            "Takayuki Nishio",
            "Koji Yamamoto"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  This paper demonstrates the feasibility of single-antenna and single-RF\n(radio frequency)- anchor received power strength indicator (RSSI) localization\n(SARR-LOC) with the assistance of the computer vision (CV) technique.\nGenerally, to perform radio frequency (RF)-based device localization, either 1)\nfine-grained channel state information or 2) RSSIs from multiple antenna\nelements or multiple RF anchors (e.g., access points) is required. Meanwhile,\nowing to deficiency of single-antenna and single-anchor RSSI, which only\nindicates a coarse-grained distance information between a receiver and a\ntransmitter, realizing localization with single-antenna and single-anchor RSSI\nis challenging. Our key idea to address this challenge is to leverage CV\ntechnique and to estimate the most likely first Fresnel zone (FFZ) between the\nreceiver and transmitter, where the role of the RSSI is to detect blockage\ntimings. Specifically, historical positions of an obstacle that dynamically\nblocks the FFZ are detected by the CV technique, and we estimate positions at\nwhich a blockage starts and ends via a time series of RSSI. These estimated\nobstacle positions, in principle, coincide with points on the FFZ boundaries,\nenabling the estimation of the FFZ and localization of the transmitter. The\nexperimental evaluation revealed that the proposed SARR-LOC achieved the\nlocalization error less than 1.0 m in an indoor environment, which is\ncomparable to that of a conventional triangulation-based RSSI localization with\nmultiple RF anchors.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.04770v4"
    },
    {
        "title": "MMSys'21 Grand Challenge on Detecting Cheapfakes",
        "authors": [
            "Shivangi Aneja",
            "Cise Midoglu",
            "Duc-Tien Dang-Nguyen",
            "Michael Alexander Riegler",
            "Paal Halvorsen",
            "Matthias Niessner",
            "Balu Adsumilli",
            "Chris Bregler"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Cheapfake is a recently coined term that encompasses non-AI (\"cheap\")\nmanipulations of multimedia content. Cheapfakes are known to be more prevalent\nthan deepfakes. Cheapfake media can be created using editing software for\nimage/video manipulations, or even without using any software, by simply\naltering the context of an image/video by sharing the media alongside\nmisleading claims. This alteration of context is referred to as out-of-context\n(OOC) misuse} of media. OOC media is much harder to detect than fake media,\nsince the images and videos are not tampered. In this challenge, we focus on\ndetecting OOC images, and more specifically the misuse of real photographs with\nconflicting image captions in news items. The aim of this challenge is to\ndevelop and benchmark models that can be used to detect whether given samples\n(news image and associated captions) are OOC, based on the recently compiled\nCOSMOS dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.05297v1"
    },
    {
        "title": "Adaptive Streaming of 360 Videos with Perfect, Imperfect, and Unknown\n  FoV Viewing Probabilities in Wireless Networks",
        "authors": [
            "Lingzhi Zhao",
            "Ying Cui",
            "Zhi Liu",
            "Yunfei Zhang",
            "Sheng Yang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  This paper investigates adaptive streaming of one or multiple tiled 360\nvideos from a multi-antenna base station (BS) to one or multiple single-antenna\nusers, respectively, in a multi-carrier wireless system. We aim to maximize the\nvideo quality while keeping rebuffering time small via encoding rate adaptation\nat each group of pictures (GOP) and transmission adaptation at each\n(transmission) slot. To capture the impact of field-of-view (FoV) prediction,\nwe consider three cases of FoV viewing probability distributions, i.e.,\nperfect, imperfect, and unknown FoV viewing probability distributions, and use\nthe average total utility, worst average total utility, and worst total utility\nas the respective performance metrics. In the single-user scenario, we optimize\nthe encoding rates of the tiles, encoding rates of the FoVs, and transmission\nbeamforming vectors for all subcarriers to maximize the total utility in each\ncase. In the multi-user scenario, we adopt rate splitting with successive\ndecoding and optimize the encoding rates of the tiles, encoding rates of the\nFoVs, rates of the common and private messages, and transmission beamforming\nvectors for all subcarriers to maximize the total utility in each case. Then,\nwe separate the challenging optimization problem into multiple tractable\nproblems in each scenario. In the single-user scenario, we obtain a globally\noptimal solution of each problem using transformation techniques and the\nKarush-Kuhn-Tucker (KKT) conditions. In the multi-user scenario, we obtain a\nKKT point of each problem using the concave-convex procedure (CCCP). Finally,\nnumerical results demonstrate that the proposed solutions achieve notable gains\nover existing schemes in all three cases. To the best of our knowledge, this is\nthe first work revealing the impact of FoV prediction on the performance of\nadaptive streaming of tiled 360 videos.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.09491v2"
    },
    {
        "title": "JPEG Steganography with Embedding Cost Learning and Side-Information\n  Estimation",
        "authors": [
            "Jianhua Yang",
            "Yi Liao",
            "Fei Shang",
            "Xiangui Kang",
            "Yun-Qing Shi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  A great challenge to steganography has arisen with the wide application of\nsteganalysis methods based on convolutional neural networks (CNNs). To this\nend, embedding cost learning frameworks based on generative adversarial\nnetworks (GANs) have been proposed and achieved success for spatial\nsteganography. However, the application of GAN to JPEG steganography is still\nin the prototype stage; its anti-detectability and training efficiency should\nbe improved. In conventional steganography, research has shown that the\nside-information calculated from the precover can be used to enhance security.\nHowever, it is hard to calculate the side-information without the spatial\ndomain image. In this work, an embedding cost learning framework for JPEG\nSteganography via a Generative Adversarial Network (JS-GAN) has been proposed,\nthe learned embedding cost can be further adjusted asymmetrically according to\nthe estimated side-information. Experimental results have demonstrated that the\nproposed method can automatically learn a content-adaptive embedding cost\nfunction, and use the estimated side-information properly can effectively\nimprove the security performance. For example, under the attack of a classic\nsteganalyzer GFR with quality factor 75 and 0.4 bpnzAC, the proposed JS-GAN can\nincrease the detection error 2.58% over J-UNIWARD, and the estimated\nside-information aided version JS-GAN(ESI) can further increase the security\nperformance by 11.25% over JS-GAN.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.13151v1"
    },
    {
        "title": "Video-based Point Cloud Compression Artifact Removal",
        "authors": [
            "Anique Akhtar",
            "Wen Gao",
            "Li Li",
            "Zhu Li",
            "Wei Jia",
            "Shan Liu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Photo-realistic point cloud capture and transmission are the fundamental\nenablers for immersive visual communication. The coding process of dynamic\npoint clouds, especially video-based point cloud compression (V-PCC) developed\nby the MPEG standardization group, is now delivering state-of-the-art\nperformance in compression efficiency. V-PCC is based on the projection of the\npoint cloud patches to 2D planes and encoding the sequence as 2D texture and\ngeometry patch sequences. However, the resulting quantization errors from\ncoding can introduce compression artifacts, which can be very unpleasant for\nthe quality of experience (QoE). In this work, we developed a novel\nout-of-the-loop point cloud geometry artifact removal solution that can\nsignificantly improve reconstruction quality without additional bandwidth cost.\nOur novel framework consists of a point cloud sampling scheme, an artifact\nremoval network, and an aggregation scheme. The point cloud sampling scheme\nemploys a cube-based neighborhood patch extraction to divide the point cloud\ninto patches. The geometry artifact removal network then processes these\npatches to obtain artifact-removed patches. The artifact-removed patches are\nthen merged together using an aggregation scheme to obtain the final\nartifact-removed point cloud. We employ 3D deep convolutional feature learning\nfor geometry artifact removal that jointly recovers both the quantization\ndirection and the quantization noise level by exploiting projection and\nquantization prior. The simulation results demonstrate that the proposed method\nis highly effective and can considerably improve the quality of the\nreconstructed point cloud.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14179v1"
    },
    {
        "title": "Is there a \"language of music-video clips\" ? A qualitative and\n  quantitative study",
        "authors": [
            "Laure Prétet",
            "Gaël Richard",
            "Geoffroy Peeters"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Recommending automatically a video given a music or a music given a video has\nbecome an important asset for the audiovisual industry - with user-generated or\nprofessional content. While both music and video have specific temporal\norganizations, most current works do not consider those and only focus on\nglobally recommending a media. As a first step toward the improvement of these\nrecommendation systems, we study in this paper the relationship between music\nand video temporal organization. We do this for the case of official music\nvideos, with a quantitative and a qualitative approach. Our assumption is that\nthe movement in the music are correlated to the ones in the video. To validate\nthis, we first interview a set of internationally recognized music video\nexperts. We then perform a large-scale analysis of official music-video clips\n(which we manually annotated into video genres) using MIR description tools\n(downbeats and functional segments estimation) and Computer Vision tools (shot\ndetection). Our study confirms that a \"language of music-video clips\" exists;\ni.e. editors favor the co-occurrence of music and video events using strategies\nsuch as anticipation. It also highlights that the amount of co-occurrence\ndepends on the music and video genres.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.00970v1"
    },
    {
        "title": "What's Wrong with the Bottom-up Methods in Arbitrary-shape Scene Text\n  Detection",
        "authors": [
            "Chengpei Xu",
            "Wenjing Jia",
            "Tingcheng Cui",
            "Ruomei Wang",
            "Yuan-fang Zhang",
            "Xiangjian He"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The latest trend in the bottom-up perspective for arbitrary-shape scene text\ndetection is to reason the links between text segments using Graph\nConvolutional Network (GCN). Notwithstanding, the performance of the best\nperforming bottom-up method is still inferior to that of the best performing\ntop-down method even with the help of GCN. We argue that this is not mainly\ncaused by the limited feature capturing ability of the text proposal backbone\nor GCN, but by their failure to make a full use of visual-relational features\nfor suppressing false detection, as well as the sub-optimal route-finding\nmechanism used for grouping text segments. In this paper, we revitalize the\nclassic text detection frameworks by aggregating the visual-relational features\nof text with two effective false positive/negative suppression mechanisms.\nFirst, dense overlapping text segments depicting the `characterness' and\n`streamline' of text are generated for further relational reasoning and weakly\nsupervised segment classification. Here, relational graph features are used for\nsuppressing false positives/negatives. Then, to fuse the relational features\nwith visual features, a Location-Aware Transfer (LAT) module is designed to\ntransfer text's relational features into visual compatible features with a Fuse\nDecoding (FD) module to enhance the representation of text regions for the\nsecond step suppression. Finally, a novel multiple-text-map-aware\ncontour-approximation strategy is developed, instead of the widely-used\nroute-finding process. Experiments conducted on five benchmark datasets, i.e.,\nCTW1500, Total-Text, ICDAR2015, MSRA-TD500, and MLT2017 demonstrate that our\nmethod outperforms the state-of-the-art performance when being embedded in a\nclassic text detection framework, which revitalises the superb strength of the\nbottom-up methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.01809v2"
    },
    {
        "title": "Multi-clue reconstruction of sharing chains for social media images",
        "authors": [
            "Sebastiano Verde",
            "Cecilia Pasquini",
            "Federica Lago",
            "Alessandro Goller",
            "Francesco GB De Natale",
            "Alessandro Piva",
            "Giulia Boato"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The amount of multimedia content shared everyday, combined with the level of\nrealism reached by recent fake-generating technologies, threatens to impair the\ntrustworthiness of online information sources. The process of uploading and\nsharing data tends to hinder standard media forensic analyses, since multiple\nre-sharing steps progressively hide the traces of past manipulations. At the\nsame time though, new traces are introduced by the platforms themselves,\nenabling the reconstruction of the sharing history of digital objects, with\npossible applications in information flow monitoring and source identification.\nIn this work, we propose a supervised framework for the reconstruction of image\nsharing chains on social media platforms. The system is structured as a cascade\nof backtracking blocks, each of them tracing back one step of the sharing chain\nat a time. Blocks are designed as ensembles of classifiers trained to analyse\nthe input image independently from one another by leveraging different feature\nrepresentations that describe both content and container of the media object.\nIndividual decisions are then properly combined by a late fusion strategy.\nResults highlight the advantages of employing multiple clues, which allow\naccurately tracing back up to three steps along the sharing chain.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.02515v1"
    },
    {
        "title": "Two-pronged Strategy: Lightweight Augmented Graph Network Hashing for\n  Scalable Image Retrieval",
        "authors": [
            "Hui Cui",
            "Lei Zhu",
            "Jingjing Li",
            "Zhiyong Cheng",
            "Zheng Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Hashing learns compact binary codes to store and retrieve massive data\nefficiently. Particularly, unsupervised deep hashing is supported by powerful\ndeep neural networks and has the desirable advantage of label independence. It\nis a promising technique for scalable image retrieval. However, deep models\nintroduce a large number of parameters, which is hard to optimize due to the\nlack of explicit semantic labels and brings considerable training cost. As a\nresult, the retrieval accuracy and training efficiency of existing unsupervised\ndeep hashing are still limited. To tackle the problems, in this paper, we\npropose a simple and efficient \\emph{Lightweight Augmented Graph Network\nHashing} (LAGNH) method with a two-pronged strategy. For one thing, we extract\nthe inner structure of the image as the auxiliary semantics to enhance the\nsemantic supervision of the unsupervised hash learning process. For another, we\ndesign a lightweight network structure with the assistance of the auxiliary\nsemantics, which greatly reduces the number of network parameters that needs to\nbe optimized and thus greatly accelerates the training process. Specifically,\nwe design a cross-modal attention module based on the auxiliary semantic\ninformation to adaptively mitigate the adverse effects in the deep image\nfeatures. Besides, the hash codes are learned by multi-layer message passing\nwithin an adversarial regularized graph convolutional network. Simultaneously,\nthe semantic representation capability of hash codes is further enhanced by\nreconstructing the similarity graph.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.03914v1"
    },
    {
        "title": "Promoting Mental Well-Being for Audiences in a Live-Streaming Game by\n  Highlight-Based Bullet Comments",
        "authors": [
            "Junjie H. Xu",
            "Yulin Cai",
            "Zhou Fang",
            "Pujana Paliyawan"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  This paper proposes a method for generating bullet comments for\nlive-streaming games based on highlights (i.e., the exciting parts of video\nclips) extracted from the game content and evaluate the effect of mental health\npromotion. Game live streaming is becoming a popular theme for academic\nresearch. Compared to traditional online video sharing platforms, such as\nYoutube and Vimeo, video live streaming platform has the benefits of\ncommunicating with other viewers in real-time. In sports broadcasting, the\ncommentator plays an essential role as mood maker by making matches more\nexciting. The enjoyment emerged while watching game live streaming also\nbenefits the audience's mental health. However, many e-sports live streaming\nchannels do not have a commentator for entertaining viewers. Therefore, this\npaper presents a design of an AI commentator that can be embedded in live\nstreaming games. To generate bullet comments for real-time game live streaming,\nthe system employs highlight evaluation to detect the highlights, and generate\nthe bullet comments. An experiment is conducted and the effectiveness of\ngenerated bullet comments in a live-streaming fighting game channel is\nevaluated.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.08083v1"
    },
    {
        "title": "Improving Fake News Detection by Using an Entity-enhanced Framework to\n  Fuse Diverse Multimodal Clues",
        "authors": [
            "Peng Qi",
            "Juan Cao",
            "Xirong Li",
            "Huan Liu",
            "Qiang Sheng",
            "Xiaoyue Mi",
            "Qin He",
            "Yongbiao Lv",
            "Chenyang Guo",
            "Yingchao Yu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Recently, fake news with text and images have achieved more effective\ndiffusion than text-only fake news, raising a severe issue of multimodal fake\nnews detection. Current studies on this issue have made significant\ncontributions to developing multimodal models, but they are defective in\nmodeling the multimodal content sufficiently. Most of them only preliminarily\nmodel the basic semantics of the images as a supplement to the text, which\nlimits their performance on detection. In this paper, we find three valuable\ntext-image correlations in multimodal fake news: entity inconsistency, mutual\nenhancement, and text complementation. To effectively capture these multimodal\nclues, we innovatively extract visual entities (such as celebrities and\nlandmarks) to understand the news-related high-level semantics of images, and\nthen model the multimodal entity inconsistency and mutual enhancement with the\nhelp of visual entities. Moreover, we extract the embedded text in images as\nthe complementation of the original text. All things considered, we propose a\nnovel entity-enhanced multimodal fusion framework, which simultaneously models\nthree cross-modal correlations to detect diverse multimodal fake news.\nExtensive experiments demonstrate the superiority of our model compared to the\nstate of the art.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.10509v1"
    },
    {
        "title": "What Matters for Ad-hoc Video Search? A Large-scale Evaluation on\n  TRECVID",
        "authors": [
            "Aozhu Chen",
            "Fan Hu",
            "Zihan Wang",
            "Fangming Zhou",
            "Xirong Li"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  For quantifying progress in Ad-hoc Video Search (AVS), the annual TRECVID AVS\ntask is an important international evaluation. Solutions submitted by the task\nparticipants vary in terms of their choices of cross-modal matching models,\nvisual features and training data. As such, what one may conclude from the\nevaluation is at a high level that is insufficient to reveal the influence of\nthe individual components. In order to bridge the gap between the current\nsolution-level comparison and the desired component-wise comparison, we propose\nin this paper a large-scale and systematic evaluation on TRECVID. By selected\ncombinations of state-of-the-art matching models, visual features and\n(pre-)training data, we construct a set of 25 different solutions and evaluate\nthem on the TRECVID AVS tasks 2016--2020. The presented evaluation helps answer\nthe key question of what matters for AVS. The resultant observations and\nlearned lessons are also instructive for developing novel AVS solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.01774v2"
    },
    {
        "title": "How Camera Placement Affects Gameplay in Video Games",
        "authors": [
            "Markos Naftis",
            "George Tsatiris",
            "Kostas Karpouzis"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In video games, players' perception of the game world and related information\ndepends on their or the game designer's choice of a virtual camera model. In\nthis paper, we attempt to answer the research question of whether it is\npossible to identify which camera model is preferred by, fits and best serves\neach player depending on where they are in a game world and the kinds of\nchallenges they face. To this end, a special type of video game, combining\nchallenges from different game genres, was designed and developed with Unity;\nthirty players could choose from four camera models at their disposal,\ndepending on where they were in the game world, and utilize the most suitable\none to proceed. Each player's preference of camera model was collected using\nthe data platform Unity Analytics and then analyzed. The analysis of the\nresults showed that players managed to adapt to the logic and requirements of\nthe game challenges by choosing different cameras for each of them, depending\non the spatial requirements and the presence of enemies or platforms they\nshould jump across from.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.03750v1"
    },
    {
        "title": "'1e0a': A Computational Approach to Rhythm Training",
        "authors": [
            "Noel Alben",
            "Ranjani H. G"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We present a computational assessment system that promotes the learning of\nbasic rhythmic patterns. The system is capable of generating multiple rhythmic\npatterns with increasing complexity within various cycle lengths. For a\ngenerated rhythm pattern the performance assessment of the learner is carried\nout through the statistical deviations calculated from the onset detection and\ntemporal assessment of a learner's performance. This is compared with the\ngenerated pattern, and their performance accuracy forms the feedback to the\nlearner. The system proceeds to generate a new pattern of increased complexity\nwhen performance assessment results are within certain error bounds. The system\nthus mimics a learner-teacher relationship as the learner progresses in their\nfeedback-based learning. The choice of progression within a cycle for each\npattern is determined by a predefined complexity metric. This metric is based\non a coded element model for the perceptual processing of sequential stimuli.\nThe model earlier proposed for a sequence of tones and non-tones, is now used\nfor onsets and silences. This system is developed into a web-based application\nand provides accessibility for learning purposes. Analysis of the performance\nassessments shows that the complexity metric is indicative of the perceptual\nprocessing of rhythm patterns and can be used for rhythm learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04440v1"
    },
    {
        "title": "On the Robustness of \"Robust reversible data hiding scheme based on\n  two-layer embedding strategy\"",
        "authors": [
            "Wen Yin",
            "Longfei Ke",
            "Zhaoxia Yin",
            "Jin Tang",
            "Bin Luo"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In the paper \"Robust reversible data hiding scheme based on two-layer\nembedding strategy\" published in INS recently, Kumar et al. proposed a robust\nreversible data hiding (RRDH) scheme based on two-layer embedding. Secret data\nwas embedded into the most significant bit (MSB) planes to increase robustness,\nand a sorting strategy based on local complexity was adopted to reduce\ndistortion. However, Kumar et al.'s reversible data hiding (RDH) scheme is not\nas robust against joint photographic experts group (JPEG) compression as stated\nand can not be called RRDH. This comment first gives a brief description of\ntheir RDH scheme, then analyses their scheme's robustness from the perspective\nof JPEG compression principles. JPEG compression will change pixel values,\nthereby destroying auxiliary information and pixel value ordering required to\nextract secret data correctly, making their scheme not robust. Next, the\nchanges in both bit plane and pixel value ordering after JPEG compression are\nshown and analysed by different robustness-testing experiments. Finally, some\nsuggestions are given to improve the robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.11735v3"
    },
    {
        "title": "Spatial Information Refinement for Chroma Intra Prediction in Video\n  Coding",
        "authors": [
            "Chengyi Zou",
            "Shuai Wan",
            "Tiannan Ji",
            "Marta Mrak",
            "Marc Gorriz Blanch",
            "Luis Herranz"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Video compression benefits from advanced chroma intra prediction methods,\nsuch as the Cross-Component Linear Model (CCLM) which uses linear models to\napproximate the relationship between the luma and chroma components. Recently\nit has been proven that advanced cross-component prediction methods based on\nNeural Networks (NN) can bring additional coding gains. In this paper, spatial\ninformation refinement is proposed for improving NN-based chroma intra\nprediction. Specifically, the performance of chroma intra prediction can be\nimproved by refined down-sampling or by incorporating location information.\nExperimental results show that the two proposed methods obtain 0.31%, 2.64%,\n2.02% and 0.33%, 3.00%, 2.12% BD-rate reduction on Y, Cb and Cr components,\nrespectively, under All-Intra configuration, when implemented in Versatile\nVideo Coding (H.266/VVC) test model. Index Terms-Chroma intra prediction,\nconvolutional neural networks, spatial information refinement.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.11913v1"
    },
    {
        "title": "Revisiting Pre-analysis Information Based Rate Control in x265",
        "authors": [
            "Hewei Liu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Due to the excellent compression and high real-time performance, x265 is\nwidely used in practical applications. Combined with CU-tree based\npre-analysis, x265 rate control can obtain high rate-distortion (R-D)\nperformance. However, the pre-analysis information is not fully utilized, and\nthe accuracy of rate control is not satisfactory in x265 because of an\nempirical linear model. In this paper, we propose an improved cost-guided rate\ncontrol scheme for x265. Firstly, the pre-analysis information is further used\nto refine the bit allocation. Secondly, CU-tree is combined with the\nlambda-domain model for more accurate rate control and higher R-D performance.\nExperimental results show that compared with the original x265, our method can\nachieve 10.3\\% BD-rate gain with only 0.22\\textperthousand bitrate error.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.12294v3"
    },
    {
        "title": "High Capacity Reversible Data Hiding in Encrypted 3D Mesh Models Based\n  on Multi-MSB Prediction",
        "authors": [
            "Wanli Lv",
            "Lulu Cheng",
            "Zhaoxia Yin"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  As a new generation of digital media for covert transmission, three-dimension\n(3D) mesh models are frequently used and distributed on the network. Facing the\nhuge massive of network data, it is urgent to study a method to protect and\nstore this large amounts of data. In this paper, we proposed a high capacity\nreversible data hiding in encrypted 3D mesh models. This method divides the\nvertices of all 3D mesh into \"embedded sets\" and \"prediction sets\" based on the\nparity of the index. In addition, the multiple most significant bit (Multi-MSB)\nprediction reserved space is used to adaptively embed secret message, and the\nauxiliary information is compressed by arithmetic coding to further free up\nredundant space of the 3D mesh models. We use the majority voting system(MSV)\nprinciple to restore the original mesh model with high quality. The\nexperimental results show that our method achieves a higher embedding capacity\ncompared with state-of-the-art RDH-ED methods on 3D mesh models and can restore\nthe original 3D mesh models with high quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.01010v2"
    },
    {
        "title": "Impacts of Device Caching of Content Fractions on Expected Content\n  Quality",
        "authors": [
            "Dongjae Kim",
            "Minseok Choi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  This paper explores caching of fractions of a video content, not caching of\nan entire content, to increase the expected video quality. We first show that\nthe highest-quality content is better to be cached and propose the caching\npolicy of video chunks having different qualities. Our caching policy utilizes\nthe characteristics of video contents that video files can be encoded into\nmultiple versions with different qualities, each file consists of many chunks,\nand chunks can have different qualities. Extensive performance evaluations are\nconducted to show that caching of content fractions, rather than an entire\ncontent, can improve the expected video quality especially when the channel\nconditions is sufficiently good to cooperate with nearby BS or helpers.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.06520v1"
    },
    {
        "title": "Assisting News Media Editors with Cohesive Visual Storylines",
        "authors": [
            "Gonçalo Marcelino",
            "David Semedo",
            "André Mourão",
            "Saverio Blasi",
            "Marta Mrak",
            "João Magalhães"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Creating a cohesive, high-quality, relevant, media story is a challenge that\nnews media editors face on a daily basis. This challenge is aggravated by the\nflood of highly relevant information that is constantly pouring onto the\nnewsroom. To assist news media editors in this daunting task, this paper\nproposes a framework to organize news content into cohesive, high-quality,\nrelevant visual storylines. First, we formalize, in a nonsubjective manner, the\nconcept of visual story transition. Leveraging it, we propose four graph-based\nmethods of storyline creation, aiming for global story cohesiveness. These were\ncreated and implemented to take full advantage of existing graph algorithms,\nensuring their correctness and good computational performance. They leverage a\nstrong ensemble-based estimator which was trained to predict story transition\nquality based on both the semantic and visual features present in the pair of\nimages under scrutiny. A user study covered a total of 28 curated stories about\nsports and cultural events. Experiments showed that (i) visual transitions in\nstorylines can be learned with a quality above 90%, and (ii) the proposed graph\nmethods can produce cohesive storylines with quality in the range of 88% to\n96%.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.06805v1"
    },
    {
        "title": "ACOA -- Chronological Analysis of the Exhibition of Artistic Works",
        "authors": [
            "Daniela Prado",
            "Armanda Rodrigues",
            "Nuno Correia",
            "Rita Macedo",
            "Sofia Gomes"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We are currently prioritizing home activities, avoiding human contact, and\ncarrying out external activities mostly by necessity. Therefore, and due to the\nloss of adhesion to cultural events on the part of the population, the cultural\ndigital transformation process has been boosted, aiming to reach interested\ncommunities through digital media. The ACOA platform supports the organization\nof multiple sources of information related to creative processes behind complex\nartworks and their trajectories over time. This information is of great\ninterest to conservators and curators, as well as to the general public, as it\nallows to document changes in the artwork, from the moment it was conceived by\nthe artist, until its most recent exhibition. This platform houses a\nchronological evolution of the work, through the contextual dissemination of\nassociated multimedia content. Works by the Portuguese artist Ana Vieira\n(1940-2016) were chosen as case studies for the implementation of the platform.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.08146v2"
    },
    {
        "title": "FoV Privacy-aware VR Streaming",
        "authors": [
            "Xing Wei",
            "Chenyang Yang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Proactive tile-based virtual reality (VR) video streaming can use the trace\nof FoV and eye movement to predict future requested tiles, then renders and\ndelivers the predicted tiles before playback. The quality of experience (QoE)\ndepends on the combined effect of tile prediction and consumed resources.\nRecently, it has been found that with the FoV and eye movement data collected\nfor a user, one can infer the identity and preference of the user. Existing\nworks investigate the privacy protection for eye movement, but never address\nhow to protect the privacy in terms of FoV and how the privacy protection\naffects the QoE. In this paper, we strive to characterize and satisfy the FoV\nprivacy requirement. We consider \"trading resources for privacy\". We first add\ncamouflaged tile requests around the real FoV and define spatial degree of\nprivacy (SDoP) as a normalized number of camouflaged tile requests. By\nconsuming more resources to ensure SDoP, the real FoVs can be hidden. Then, we\nproceed to analyze the impacts of SDoP on the QoE by jointly optimizing the\ndurations for prediction, computing, and transmission that maximizes the QoE\ngiven arbitrary predictor, configured resources, and SDoP. We find that a\nlarger SDoP requires more resources but degrades the performance of tile\nprediction. Simulation with state-of-the-art predictors on a real dataset\nverifies the analysis and shows that a user requiring a larger SDoP can be\nserved with better QoE.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.10417v1"
    },
    {
        "title": "Compressed Geometric Arrays for Point Cloud Processing",
        "authors": [
            "Hoda Roodaki",
            "Mahdi Nazm Bojnordi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The ever-increasing demand for 3D modeling in the emerging immersive\napplications has made point clouds an essential class of data for 3D image and\nvideo processing. Tree based structures are commonly used for representing\npoint clouds where pointers are used to realize the connection between nodes.\nTree-based structures significantly suffer from irregular access patterns for\nlarge point clouds. Memory access indirection in such structures is disruptive\nto bandwidth efficiency and performance. In this paper, we propose a point\ncloud representation format based on compressed geometric arrays (CGA). Then,\nwe examine new methods for point cloud processing based on CGA. The proposed\nformat enables a higher bandwidth efficiency via eliminating memory access\nindirections (i.e., pointer chasing at the nodes of tree) thereby improving the\nefficiency of point cloud processing. Our experimental results show that using\nCGA for point cloud operations achieves 1328x speed up, 1321x better bandwidth\nutilization, and 54% reduction in the volume of transferred data as compared to\nthe state-of-the-art tree-based format from point cloud library (PCL).\n",
        "pdf_link": "http://arxiv.org/pdf/2110.11616v1"
    },
    {
        "title": "SHECS: A Local Smart Hands-free Elderly Care Support System on Smart AR\n  Glasses with AI Technology",
        "authors": [
            "Donghuo Zeng",
            "Jianming Wu",
            "Bo Yang",
            "Tomohiro Obara",
            "Akeri Okawa",
            "Nobuko Iino",
            "Gen Hattori",
            "Ryoichi Kawada",
            "Yasuhiro Takishima"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Some elderly care homes attempt to remedy the shortage of skilled caregivers\nand provide long-term care for the elderly residents, by enhancing the\nmanagement of the care support system with the aid of smart devices such as\nmobile phones and tablets. Since mobile phones and tablets lack the flexibility\nrequired for laborious elderly care work, smart AR glasses have already been\nconsidered. Although lightweight smart AR devices with a transparent display\nare more convenient and responsive in an elderly care workplace, fetching data\nfrom the server through the Internet results in network congestion not to\nmention the limited display area. To devise portable smart AR devices that\noperate smoothly, we first present a no keep alive Internet required smart\nhands-free elderly care support system that employs smart glasses with facial\nrecognition and text-to-speech synthesis technologies. Our support system\nutilizes automatic lightweight facial recognition to identify residents, and\ninformation about each resident in question can be obtained hands free link\nwith a local database. Moreover, a resident information can be displayed on\njust a portion of the AR smart glasses on the spot. Due to the limited size of\nthe display area, it cannot show all the necessary information. We exploit\nsynthesized voices in the system to read out the elderly care related\ninformation. By using the support system, caregivers can gain an understanding\nof each resident condition immediately, instead of having to devote\nconsiderable time in advance in obtaining the complete information of all\nelderly residents. Our lightweight facial recognition model achieved high\naccuracy with fewer model parameters than current state-of-the-art methods. The\nvalidation rate of our facial recognition system was 99.3% or higher with the\nfalse accept rate of 0.001, and caregivers rated the acceptability at 3.6 (5\nlevels) or higher.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.13538v1"
    },
    {
        "title": "Learning Explicit and Implicit Latent Common Spaces for Audio-Visual\n  Cross-Modal Retrieval",
        "authors": [
            "Donghuo Zeng",
            "Jianming Wu",
            "Gen Hattori",
            "Yi Yu",
            "Rong Xu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Learning common subspace is prevalent way in cross-modal retrieval to solve\nthe problem of data from different modalities having inconsistent distributions\nand representations that cannot be directly compared. Previous cross-modal\nretrieval methods focus on projecting the cross-modal data into a common space\nby learning the correlation between them to bridge the modality gap. However,\nthe rich semantic information in the video and the heterogeneous nature of\naudio-visual data leads to more serious heterogeneous gaps intuitively, which\nmay lead to the loss of key semantic content of video with single clue by the\nprevious methods when eliminating the modality gap, while the semantics of the\ncategories may undermine the properties of the original features. In this work,\nwe aim to learn effective audio-visual representations to support audio-visual\ncross-modal retrieval (AVCMR). We propose a novel model that maps audio-visual\nmodalities into two distinct shared latent subspaces: explicit and implicit\nshared spaces. In particular, the explicit shared space is used to optimize\npairwise correlations, where learned representations across modalities capture\nthe commonalities of audio-visual pairs and reduce the modality gap. The\nimplicit shared space is used to preserve the distinctive features between\nmodalities by maintaining the discrimination of audio/video patterns from\ndifferent semantic categories. Finally, the fusion of the features learned from\nthe two latent subspaces is used for the similarity computation of the AVCMR\ntask. The comprehensive experimental results on two audio-visual datasets\ndemonstrate that our proposed model for using two different latent subspaces\nfor audio-visual cross-modal learning is effective and significantly\noutperforms the state-of-the-art cross-modal models that learn features from a\nsingle subspace.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.13556v1"
    },
    {
        "title": "Soft Delivery: Survey on A New Paradigm for Wireless and Mobile\n  Multimedia Streaming",
        "authors": [
            "Takuya Fujihashi",
            "Toshiaki Koike-Akino",
            "Takashi Watanabe"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The increasing demand for video streaming services is the key driver of\nmodern wireless and mobile communications. For robust and high-quality delivery\nof video content over wireless and mobile networks, the main challenge is\nsending image and video signals to single and multiple users over unstable and\ndiverse channel environments. To this end, many studies have designed\ndigital-based video delivery schemes, which mainly consist of a sequence of\ndigital-based coding and transmission schemes. Although digital-based schemes\nperform well when the channel characteristics are known in advance, significant\nquality degradation, known as cliff and leveling effects, often occurs owing to\nthe fluctuating channel characteristics. To prevent cliff and leveling effects\nirrespective of the channel characteristics of each user, a new paradigm for\nwireless and mobile video streaming has been proposed. Soft delivery schemes\nskip the digital operations of quantization and entropy and channel coding\nwhile directly mapping the power-assigned frequency--domain coefficients onto\nthe transmission symbols. This modification is based on the fact that the pixel\ndistortion due to communication noise is proportional to the magnitude of the\nnoise, resulting in graceful quality improvement, wherein quality is improved\ngradually, according to the wireless channel quality without any cliff and\nleveling effects. Herein, we present a comprehensive summary of soft delivery\nschemes.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.08189v1"
    },
    {
        "title": "Triple Attention Network architecture for MovieQA",
        "authors": [
            "Ankit Shah",
            "Tzu-Hsiang Lin",
            "Shijie Wu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Movie question answering, or MovieQA is a multimedia related task wherein one\nis provided with a video, the subtitle information, a question and candidate\nanswers for it. The task is to predict the correct answer for the question\nusing the components of the multimedia - namely video/images, audio and text.\nTraditionally, MovieQA is done using the image and text component of the\nmultimedia. In this paper, we propose a novel network with triple-attention\narchitecture for the inclusion of audio in the Movie QA task. This architecture\nis fashioned after a traditional dual attention network focused only on video\nand text. Experiments show that the inclusion of audio using the\ntriple-attention network results provides complementary information for Movie\nQA task which is not captured by visual or textual component in the data.\nExperiments with a wide range of audio features show that using such a network\ncan indeed improve MovieQA performance by about 7% relative to just using only\nvisual features.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.09531v1"
    },
    {
        "title": "PointPCA: Point Cloud Objective Quality Assessment Using PCA-Based\n  Descriptors",
        "authors": [
            "Evangelos Alexiou",
            "Xuemei Zhou",
            "Irene Viola",
            "Pablo Cesar"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Point clouds denote a prominent solution for the representation of 3D\nphoto-realistic content in immersive applications. Similarly to other imaging\nmodalities, quality predictions for point cloud contents are vital for a wide\nrange of applications, enabling trade-off optimizations between data quality\nand data size in every processing step from acquisition to rendering. In this\nwork, we focus on use cases that consider human end-users consuming point cloud\ncontents and, hence, we concentrate on visual quality metrics. In particular,\nwe propose a set of perceptually relevant descriptors based on Principal\nComponent Analysis (PCA) decomposition, which is applied to both geometry and\ntexture data for full-reference point cloud quality assessment. Statistical\nfeatures are derived from these descriptors to characterize local shape and\nappearance properties for both a reference and a distorted point cloud. The\nextracted statistical features are subsequently compared to provide\ncorresponding predictions of visual quality for the distorted point cloud. As\npart of our method, a learning-based approach is proposed to fuse these\nindividual predictors to a unified perceptual score. We validate the accuracy\nof the individual predictors, as well as the unified quality scores obtained\nafter regression against subjectively annotated datasets, showing that our\nmetric outperforms state-of-the-art solutions. Insights regarding design\ndecisions are provided through exploratory studies, evaluating the performance\nof our metric under different parameter configurations, attribute domains,\ncolor spaces, and regression models. A software implementation of the proposed\nmetric is made available at the following link:\nhttps://github.com/cwi-dis/pointpca.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.12663v4"
    },
    {
        "title": "Mutltimodal AI Companion for Interactive Fairytale Co-creation",
        "authors": [
            "Ruiyang Liu",
            "Predrag K. Nikolic"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  AI fairy tale companions play an important role in early childhood education\nas an augmentation for parents' efforts to close the participation gap and\nboost kids' mental and language development. Existing systems are generally\ndesigned to provide vivid materials as unidirectional entertaining reading\nenvironments, e.g, visualizing inputting texts. However, due to the limited\nvocabulary of kids, these systems failed to afford effective interaction to\nmotivate kids to write their own fairy tales. In this work, we propose AI.R\nTaletorium, an illustrative, immersive, and inclusive multimodal AI companion,\nfor interactive fairy tale co-creation that actively involves kids to create\nfairy tales with both the AI agent and their normal peers. AI.R Taletorium\nconsists a neural story generator and a doodler-based fairy tale visualizer. We\ndesign a character-centric bidirectional connection mechanism between the story\ngenerator and visualizer equipped with Contrastive Language Image Pretraining\n(CLIP), thus enabling kids to participant in the story generation process by\nsimple sketching. Extensive experiments and user studies show that our system\nwas able to generate and visualize meaningful and vivid fairy tales with\nlimited training data and complete the full interaction cycle under various\ninputs (text, doodler) through the bidirectional connection.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.00331v1"
    },
    {
        "title": "Protecting Your NLG Models with Semantic and Robust Watermarks",
        "authors": [
            "Tao Xiang",
            "Chunlong Xie",
            "Shangwei Guo",
            "Jiwei Li",
            "Tianwei Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Natural language generation (NLG) applications have gained great popularity\ndue to the powerful deep learning techniques and large training corpus. The\ndeployed NLG models may be stolen or used without authorization, while\nwatermarking has become a useful tool to protect Intellectual Property (IP) of\ndeep models. However, existing watermarking technologies using backdoors are\neasily detected or harmful for NLG applications. In this paper, we propose a\nsemantic and robust watermarking scheme for NLG models that utilize unharmful\nphrase pairs as watermarks for IP protection. The watermarks give NLG models\npersonal preference for some special phrase combinations. Specifically, we\ngenerate watermarks by following a semantic combination pattern and\nsystematically augment the watermark corpus to enhance the robustness. Then, we\nembed these watermarks into an NLG model without misleading its original\nattention mechanism. We conduct extensive experiments and the results\ndemonstrate the effectiveness, robustness, and undetectability of the proposed\nscheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.05428v1"
    },
    {
        "title": "AI-Empowered Persuasive Video Generation: A Survey",
        "authors": [
            "Chang Liu",
            "Han Yu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Promotional videos are rapidly becoming a popular medium for persuading\npeople to change their behaviours in many settings (e.g., online shopping,\nsocial enterprise initiatives). Today, such videos are often produced by\nprofessionals, which is a time-, labour- and cost-intensive undertaking. In\norder to produce such contents to support a large applications (e.g.,\ne-commerce), the field of artificial intelligence (AI)-empowered persuasive\nvideo generation (AIPVG) has gained traction in recent years. This field is\ninterdisciplinary in nature, which makes it challenging for new researchers to\ngrasp. Currently, there is no comprehensive survey of AIPVG available. In this\npaper, we bridge this gap by reviewing key AI techniques that can be utilized\nto automatically generate persuasive videos. We offer a first-of-its-kind\ntaxonomy which divides AIPVG into three major steps: 1) visual material\nunderstanding, which extracts information from the visual materials (VMs)\nrelevant to the target of promotion; 2) visual storyline generation, which\nshortlists and arranges high-quality VMs into a sequence in order to compose a\nstoryline with persuasive power; and 3) post-production, which involves\nbackground music generation and still image animation to enhance viewing\nexperience. We also introduce the evaluation metrics and datasets commonly\nadopted in the field of AIPVG. We analyze the advantages and disadvantages of\nthe existing works belonging to the above-mentioned steps, and discuss\ninteresting potential future research directions.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.09401v1"
    },
    {
        "title": "Automated Vision-Based Wellness Analysis for Elderly Care Centers",
        "authors": [
            "Xijie Huang",
            "Jeffry Wicaksana",
            "Shichao Li",
            "Kwang-Ting Cheng"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The growth in the aging population requires caregivers to improve both\nefficiency and quality of healthcare. In this study, we develop an automatic,\nvision-based system for monitoring and analyzing the physical and mental\nwell-being of senior citizens. Through collaboration with Haven of Hope\nChristian Service, we collect video recording data in the care center with\nsurveillance cameras. We then process and extract personalized facial,\nactivity, and interaction features from the video data using deep neural\nnetworks. This integrated health information systems can assist caregivers to\ngain better insights into the seniors they are taking care of. These insights,\nincluding wellness metrics and long-term health patterns of senior citizens,\ncan help caregivers update their caregiving strategies. We report the findings\nof our analysis and evaluate the system quantitatively. We also summarize\ntechnical challenges and additional functionalities and technologies needed for\noffering a comprehensive system.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.10381v1"
    },
    {
        "title": "3D Point Cloud Reconstruction and SLAM as an Input",
        "authors": [
            "Ziyu Li",
            "Fangyang Ye",
            "Xinran Guan"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  To handle the different types of surface reconstruction tasks, we have\nreplicated as well as modified a few of reconstruction methods and have made\ncomparisons between the traditional method and data-driven method for\nreconstruction the surface of an object with dense point cloud as input. On top\nof that, we proposed a system using tightly-coupled SLAM as an input to\ngenerate deskewed point cloud and odometry and a Truncated Signed Distance\nFunction based Surface Reconstruction Library. To get higher accuracy,\nIMU(Inertial Measurement Unit) pre-integration and pose graph optimization are\nconduct in the SLAM part. With the help of the Robot Operating System, we could\nbuild a system containing those two parts, which can conduct a real-time\noutdoor surface reconstruction.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.12907v1"
    },
    {
        "title": "LSB Based Non Blind Predictive Edge Adaptive Image Steganography",
        "authors": [
            "Soumendu Chakraborty",
            "Anand Singh Jalal",
            "Charul Bhatnagar"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Image steganography is the art of hiding secret message in grayscale or color\nimages. Easy detection of secret message for any state-of-art image\nsteganography can break the stego system. To prevent the breakdown of the stego\nsystem data is embedded in the selected area of an image which reduces the\nprobability of detection. Most of the existing adaptive image steganography\ntechniques achieve low embedding capacity. In this paper a high capacity\nPredictive Edge Adaptive image steganography technique is proposed where\nselective area of cover image is predicted using Modified Median Edge Detector\n(MMED) predictor to embed the binary payload (data). The cover image used to\nembed the payload is a grayscale image. Experimental results show that the\nproposed scheme achieves better embedding capacity with minimum level of\ndistortion and higher level of security. The proposed scheme is compared with\nthe existing image steganography schemes. Results show that the proposed scheme\nachieves better embedding rate with lower level of distortion.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.01277v1"
    },
    {
        "title": "Federated AirNet: Hybrid Digital-Analog Neural Network Transmission for\n  Federated Learning",
        "authors": [
            "Takuya Fujihashi",
            "Toshiaki Koike-Akino",
            "Takashi Watanabe"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  A key issue in federated learning over wireless channels is how to exchange a\nlarge number of the model parameters via time-varying channels. Two types of\nsolutions based on digital and analog schemes are used typically. The\ndigital-based solution takes quantization and entropy coding for compression,\nwhereas transmissions via wireless channels may cause catastrophic errors owing\nto the all-or-nothing behavior in entropy coding. The analog-based solutions\nsuch as AirNet and AirComp use analog modulation for the parameter\ntransmissions. However, such an analog scheme often causes significant\ndistortion due to the source signal's large power without compression gain.\nThis paper proposes a novel hybrid digital-analog transmission-Federated\nAirNet--for the model parameter transmissions in federated learning. The\nFederated AirNet integrates low-rate digital coding and energy-compact analog\nmodulation. The digital coding offers the baseline of the model parameters and\ncompacts the source signal power. In addition, the residual parameters, which\nare obtained from the original and encoded model parameters, are\nanalog-modulated to enhance the baseline according to the instantaneous\nwireless channel quality. We show that the proposed Federated AirNet yields\nbetter image classification accuracy compared with the digital-based and\nanalog-based solutions over a wide range of wireless channel signal-to-noise\nratios (SNRs).\n",
        "pdf_link": "http://arxiv.org/pdf/2201.04557v1"
    },
    {
        "title": "Immersive Experiences and XR: A Game Engine or Multimedia Streaming\n  Problem?",
        "authors": [
            "Simon N. B. Gunkel",
            "Emmanouil Potetsianakis",
            "Tessa E. Klunder",
            "Alexander Toet",
            "Sylvie S. Dijkstra-Soudarissanane"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Recent improvements in Extended Reality (XR) technology created an increase\nin XR products and solutions in the industry, while raising new requirements\nfor new or improved architectural concepts. This need can be particularly\ncomplex as XR applications often relate both to 3D geometric rendering and\nmultimedia paradigms. This paper outlines the main concepts relevant to XR,\nboth from a game engineering and multimedia streaming system perspective. XR\nrequires new metadata and media/game orchestration to allow complex interaction\nbetween users, objects, and (volumetric) multimedia content, which also results\nin new requirements on synchronisation (i.e., for global object state and\npositioning). Furthermore, the paper presents the functional blocks needed in\nnew XR system architectures and how they will glue both (game and media) spaces\ntogether. The discussion of functional components and architecture relates to\nthe ongoing activities in relevant standardisation bodies like Khronos, MPEG,\nand 3GPP. To make XR successful on the long term, the industry needs to agree\non interoperable solutions and how to merge both game and media paradigms to\nallow complex multi-user XR applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.05552v1"
    },
    {
        "title": "Generalised Score Distribution: A Two-Parameter Discrete Distribution\n  Accurately Describing Responses from Quality of Experience Subjective\n  Experiments",
        "authors": [
            "Jakub Nawała",
            "Lucjan Janowski",
            "Bogdan Ćmiel",
            "Krzysztof Rusek",
            "Pablo Pérez"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Subjective responses from Multimedia Quality Assessment (MQA) experiments are\nconventionally analysed with methods not suitable for the data type these\nresponses represent. Furthermore, obtaining subjective responses is resource\nintensive. A method allowing reuse of existing responses would be thus\nbeneficial. Applying improper data analysis methods leads to difficult to\ninterpret results. This encourages drawing erroneous conclusions. Building upon\nexisting subjective responses is resource friendly and helps develop machine\nlearning (ML) based visual quality predictors. We show that using a discrete\nmodel for analysis of responses from MQA subjective experiments is feasible. We\nindicate that our proposed Generalised Score Distribution (GSD) properly\ndescribes response distributions observed in typical MQA experiments. We\nhighlight interpretability of GSD parameters and indicate that the GSD\noutperforms the approach based on sample empirical distribution when it comes\nto bootstrapping. We evidence that the GSD outcompetes the state-of-the-art\nmodel both in terms of goodness-of-fit and bootstrapping capabilities. To do\nall of that we analyse more than one million subjective responses from more\nthan 30 subjective experiments. Furthermore, we make the code implementing the\nGSD model and related analyses available through our GitHub repository:\nhttps://github.com/Qub3k/subjective-exp-consistency-check\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02177v1"
    },
    {
        "title": "Towards Making a Trojan-horse Attack on Text-to-Image Retrieval",
        "authors": [
            "Fan Hu",
            "Aozhu Chen",
            "Xirong Li"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  While deep learning based image retrieval is reported to be vulnerable to\nadversarial attacks, existing works are mainly on image-to-image retrieval with\ntheir attacks performed at the front end via query modification. By contrast,\nwe present in this paper the first study about a threat that occurs at the back\nend of a text-to-image retrieval (T2IR) system. Our study is motivated by the\nfact that the image collection indexed by the system will be regularly updated\ndue to the arrival of new images from various sources such as web crawlers and\nadvertisers. With malicious images indexed, it is possible for an attacker to\nindirectly interfere with the retrieval process, letting users see certain\nimages that are completely irrelevant w.r.t. their queries. We put this thought\ninto practice by proposing a novel Trojan-horse attack (THA). In particular, we\nconstruct a set of Trojan-horse images by first embedding word-specific\nadversarial information into a QR code and then putting the code on benign\nadvertising images. A proof-of-concept evaluation, conducted on two popular\nT2IR datasets (Flickr30k and MS-COCO), shows the effectiveness of the proposed\nTHA in a white-box mode.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.03861v4"
    },
    {
        "title": "A Systematic Review on Affective Computing: Emotion Models, Databases,\n  and Recent Advances",
        "authors": [
            "Yan Wang",
            "Wei Song",
            "Wei Tao",
            "Antonio Liotta",
            "Dawei Yang",
            "Xinlei Li",
            "Shuyong Gao",
            "Yixuan Sun",
            "Weifeng Ge",
            "Wei Zhang",
            "Wenqiang Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Affective computing plays a key role in human-computer interactions,\nentertainment, teaching, safe driving, and multimedia integration. Major\nbreakthroughs have been made recently in the areas of affective computing\n(i.e., emotion recognition and sentiment analysis). Affective computing is\nrealized based on unimodal or multimodal data, primarily consisting of physical\ninformation (e.g., textual, audio, and visual data) and physiological signals\n(e.g., EEG and ECG signals). Physical-based affect recognition caters to more\nresearchers due to multiple public databases. However, it is hard to reveal\none's inner emotion hidden purposely from facial expressions, audio tones, body\ngestures, etc. Physiological signals can generate more precise and reliable\nemotional results; yet, the difficulty in acquiring physiological signals also\nhinders their practical application. Thus, the fusion of physical information\nand physiological signals can provide useful features of emotional states and\nlead to higher accuracy. Instead of focusing on one specific field of affective\nanalysis, we systematically review recent advances in the affective computing,\nand taxonomize unimodal affect recognition as well as multimodal affective\nanalysis. Firstly, we introduce two typical emotion models followed by commonly\nused databases for affective computing. Next, we survey and taxonomize\nstate-of-the-art unimodal affect recognition and multimodal affective analysis\nin terms of their detailed architectures and performances. Finally, we discuss\nsome important aspects on affective computing and their applications and\nconclude this review with an indication of the most promising future\ndirections, such as the establishment of baseline dataset, fusion strategies\nfor multimodal affective analysis, and unsupervised learning models.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.06935v3"
    },
    {
        "title": "Literature Review on Image Compression, Tracking, Adaptive Training and\n  3D Data Transmission",
        "authors": [
            "Sravanti Chinta",
            "Rajat Bothra Jain"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The literature review presented below on Image Compression, Transmission of\n3D data over wireless networks and tracking of objects is the in depth study of\nResearch Papers done in Multimedia lab. Most of the papers presented in this\nliterature review have tackled the problems present in the conventional system\nand offered an optimal and practical solution.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.03532v1"
    },
    {
        "title": "Matrix Syncer -- A Multi-chain Data Aggregator For Supporting\n  Blockchain-based Metaverses",
        "authors": [
            "Xinyao Sun",
            "Yi Lu",
            "Jinghan Sun",
            "Bohao Tang",
            "Kyle D. Rehak",
            "Shuyi Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Due to the rising complexity of the metaverse's business logic and the\nlow-latency nature of the metaverse, developers typically encounter the\nchallenge of effectively reading, writing, and retrieving historical on-chain\ndata in order to facilitate their functional implementations at scale. While it\nis true that accessing blockchain states is simple, more advanced real-world\noperations such as search, aggregation, and conditional filtering are not\navailable when interacting directly with blockchain networks, particularly when\ndealing with requirements for on-chain event reflection. We offer Matrix\nSyncer, the ultimate middleware that bridges the data access gap between\nblockchains and end-user applications. Matrix Syncer is designed to facilitate\nthe consolidation of on-chain information into a distributed data warehouse\nwhile also enabling customized on-chain state transformation for a scalable\nstorage, access, and retrieval. It offers a unified layer for both on- and\noff-chain state, as well as a fast and flexible atomic query. Matrix Syncer is\neasily incorporated into any infrastructure to aggregate data from various\nblockchains concurrently, such as Ethereum and Flow. The system has been\ndeployed to support several metaverse projects with a total value of more than\n$15 million USD.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.04272v1"
    },
    {
        "title": "Codec Compression Efficiency Evaluation of MPEG-5 part 2 (LCEVC) using\n  Objective and Subjective Quality Assessment",
        "authors": [
            "Nabajeet Barman",
            "Steven Schmidt",
            "Saman Zadtootaghaj",
            "Maria G Martini"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  With the increasing advancements in video compression efficiency achieved by\nnewer codecs such as HEVC, AV1, and VVC, and intelligent encoding strategies,\nas well as improved bandwidth availability,there has been a proliferation and\nacceptance of newer services such as Netflix, Twitch, etc. However, such higher\ncompression efficiencies are achieved at the cost of higher complexity and\nencoding delay, while many applications are delay sensitive. Hence, there is a\nrequirement for faster, more efficient codecs to achieve higher encoding\nefficiency without significant trade-off in terms of both complexity and speed.\nWe present in this work an evaluation of the latest MPEG-5 Part 2 Low\nComplexity Enhancement Video Coding (LCEVC) for live gaming video streaming\napplications. The results are presented in terms of bitrate savings using both\nsubjective and objective quality measures as well as a comparison of the\nencoding speeds. Our results indicate that, for the encoding settings used in\nthis work, LCEVC outperforms both x264 and x265 codecs in terms of bitrate\nsavings using VMAF by approximately 42\\% and 38\\%. Using subjective results, it\nis found that LCEVC outperforms the respective base codecs, especially for low\nbitrates. This effect is more evident for x264 than for x265, i.e., for the\nlatter the absolute improvement of quality scores is smaller. The objective and\nsubjective results as well as sample video sequences are made available as part\nof an open dataset, LCEVC-LiveGaming at\nhttps://github.com/NabajeetBarman/LCEVC-LiveGaming.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05580v1"
    },
    {
        "title": "CWcollab: A Context-Aware Web-Based Collaborative Multimedia System",
        "authors": [
            "Chunxu Tang",
            "Beinan Wang",
            "C. Y. Roger Chen",
            "Huijun Wu"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Remote collaboration tools for conferencing and presentation are gaining\nsignificant popularity during the COVID-19 pandemic period. Most prior work has\nissues, such as a) limited support for media types, b) lack of interactivity,\nfor example, an efficient replay mechanism, c) large bandwidth consumption for\nscreen sharing tools. In this paper, we propose a general-purpose multimedia\ncollaboration platform-CWcollab. It supports collaboration on general\nmultimedia by using simple messages to represent media controls with an\nobject-prioritized synchronization approach. Thus, CWcollab can not only\nsupport fine-grained accurate collaboration, but also rich functionalities such\nas replay of these collaboration events. The evaluation shows hundreds of\nkilobytes can be enough to store the events in a collaboration session for\naccurate replays, compared with hundreds of megabytes of Google Hangouts.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.06134v1"
    },
    {
        "title": "Realistic Video Sequences for Subjective QoE Analysis",
        "authors": [
            "Kerim Hodzic",
            "Mirsad Cosovic",
            "Sasa Mrdovic",
            "Jason J. Quinlan",
            "Darijo Raca"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Multimedia streaming over the Internet (live and on demand) is the\ncornerstone of modern Internet carrying more than 60% of all traffic. With such\nhigh demand, delivering outstanding user experience is a crucial and\nchallenging task. To evaluate user QoE many researchers deploy subjective\nquality assessments where participants watch and rate videos artificially\ninfused with various temporal and spatial impairments. To aid current efforts\nin bridging the gap between the mapping of objective video QoE metrics to user\nexperience, we developed DashReStreamer, an open-source framework for\nre-creating adaptively streamed video in real networks. DashReStreamer utilises\na log created by a HAS algorithm run in an uncontrolled environment (i.e.,\nwired or wireless networks), encoding visual changes and stall events in one\nvideo file. These videos are applicable for subjective QoE evaluation mimicking\nrealistic network conditions.\n  To supplement DashReStreamer, we re-create 234 realistic video clips, based\non video logs collected from real mobile and wireless networks. In addition our\ndataset contains both video logs with all decisions made by the HASalgorithm\nand network bandwidth profile illustrating throughput distribution. We believe\nthis dataset and framework will permit other researchers in their pursuit for\nthe final frontier in understanding the impact of video QoE dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.06829v1"
    },
    {
        "title": "Cost Minimization of Cloud Services for On-Demand Video Streaming",
        "authors": [
            "Mahmoud Darwich",
            "Yasser Ismail",
            "Talal Darwich",
            "Magdy Bayoumi"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Cloud Technology is adopted to process video streams because of the great\nfeatures provided to video stream providers such as the high flexibility of\nusing virtual machines and storage servers at low rates. Video stream providers\nprepare several formats of the same video to satisfy all users' devices'\nspecifications. Video streams in the cloud are either transcoded or stored.\nHowever, storing all formats of videos is still costly. In this research, we\ndevelop an approach that optimizes cloud storage. Particularly, we propose a\nmethod that decides which video in which cloud storage should be stored to\nminimize the overall cost of cloud services. The results of the proposed\napproach are promising, it shows effectiveness when the number of frequently\naccessed video grow in a repository, and when the views of videos increases.\nThe proposed method decreases the cost of using cloud services by up to 22%.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.09423v1"
    },
    {
        "title": "Robust Audio-Visual Instance Discrimination via Active Contrastive Set\n  Mining",
        "authors": [
            "Hanyu Xuan",
            "Yihong Xu",
            "Shuo Chen",
            "Zhiliang Wu",
            "Jian Yang",
            "Yan Yan",
            "Xavier Alameda-Pineda"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The recent success of audio-visual representation learning can be largely\nattributed to their pervasive property of audio-visual synchronization, which\ncan be used as self-annotated supervision. As a state-of-the-art solution,\nAudio-Visual Instance Discrimination (AVID) extends instance discrimination to\nthe audio-visual realm. Existing AVID methods construct the contrastive set by\nrandom sampling based on the assumption that the audio and visual clips from\nall other videos are not semantically related. We argue that this assumption is\nrough, since the resulting contrastive sets have a large number of faulty\nnegatives. In this paper, we overcome this limitation by proposing a novel\nActive Contrastive Set Mining (ACSM) that aims to mine the contrastive sets\nwith informative and diverse negatives for robust AVID. Moreover, we also\nintegrate a semantically-aware hard-sample mining strategy into our ACSM. The\nproposed ACSM is implemented into two most recent state-of-the-art AVID methods\nand significantly improves their performance. Extensive experiments conducted\non both action and sound recognition on multiple datasets show the remarkably\nimproved performance of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12366v1"
    },
    {
        "title": "Timestamp-independent Haptic-Visual Synchronization",
        "authors": [
            "Yiwen Xu",
            "Liangtao Huang",
            "Tiesong Zhao",
            "Liqun Lin",
            "Ying Fang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The booming haptic data significantly improves the users'immersion during\nmultimedia interaction. As a result, the study of Haptic,Audio-Visual\nEnvironment(HAVE)has attracted attentions of multimedia community. To realize\nsuch a system, a challenging tack is the synchronization of multiple sensorial\nsignals that is critical to user experience. Despite of audio-visual\nsynchronization efforts, there is still a lack of haptic-aware multimedia\nsynchronization model. In this work, we propose a timestamp-independent\nsynchronization for haptic-visual signal transmission. First, we exploit the\nsequential correlations during delivery and playback of a haptic-visual\ncommunication system. Second, we develop a key sample extraction of haptic\nsignals based on the force feedback characteristics, and a key frame extraction\nof visual signals based on deep object detection. Third, we combine the key\nsamples and frames to synchronize the corresponding haptic-visual signals.\nWithout timestamps in signal flow, the proposed method is still effective and\nmore robust to complicated network conditions. Subjective evaluation also shows\na significant improvement of user experience with the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.03684v1"
    },
    {
        "title": "Evaluating the Impact of Tiled User-Adaptive Real-Time Point Cloud\n  Streaming on VR Remote Communication",
        "authors": [
            "Shishir Subramanyam",
            "Irene Viola",
            "Jack Jansen",
            "Evangelos Alexiou",
            "Alan Hanjalic",
            "Pablo Cesar"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Remote communication has rapidly become a part of everyday life in both\nprofessional and personal contexts. However, popular video conferencing\napplications present limitations in terms of quality of communication,\nimmersion and social meaning. VR remote communication applications offer a\ngreater sense of co-presence and mutual sensing of emotions between remote\nusers. Previous research on these applications has shown that realistic point\ncloud user reconstructions offer better immersion and communication as compared\nto synthetic user avatars. However, photorealistic point clouds require a large\nvolume of data per frame and are challenging to transmit over bandwidth-limited\nnetworks. Recent research has demonstrated significant improvements to\nperceived quality by optimizing the usage of bandwidth based on the position\nand orientation of the user's viewport with user-adaptive streaming. In this\nwork, we developed a real-time VR communication application with an adaptation\nengine that features tiled user-adaptive streaming based on user behaviour. The\napplication also supports traditional network adaptive streaming. The\ncontribution of this work is to evaluate the impact of tiled user-adaptive\nstreaming on quality of communication, visual quality, system performance and\ntask completion in a functional live VR remote communication system. We perform\na subjective evaluation with 33 users to compare the different streaming\nconditions with a neck exercise training task. As a baseline, we use\nuncompressed streaming requiring ca. 300Mbps and our solution achieves similar\nvisual quality with tiled adaptive streaming at 14Mbps. We also demonstrate\nstatistically significant gains to the quality of interaction and improvements\nto system performance and CPU consumption with tiled adaptive streaming as\ncompared to the more traditional network adaptive streaming.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04906v1"
    },
    {
        "title": "Towards the Effects of Alignment Edits on the Quality of Experience of\n  360 Videos",
        "authors": [
            "Lucas Althoff",
            "Alessandro Rodrigues",
            "Mylène C. Q. Farias"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The optimization of viewers' quality of experience (QoE) in 360 videos faces\ntwo major roadblocks: inaccurate adaptive streaming and viewers missing the\nplot of a story. Alignment edit emerged as a promising mechanism to avoid both\nissues at once. Alignment edits act on the content, matching the users'\nviewport with a region of interest in the video content. As a consequence,\nviewers' attention is focused, reducing exploratory behavior and enabling the\noptimization of network resources; in addition, it allows for a precise\nselection of events to be shown to viewers, supporting viewers to follow the\nstoryline. In this work, we investigate the effects of alignment edits on QoE\nby conducting two user studies. Specifically, we measured three QoE factors:\npresence, comfort, and overall QoE. We introduce a new alignment edit, named\n\\textit{Fade-rotation}, based on a mechanism to reduce cybersickness in VR\ngames. In the user studies, we tested four versions of fade-rotation and\ncompared them with instant alignment. We observed from the results that gradual\nalignment achieves good levels of comfort for all contents and rotational speed\ntested, showing its validity. We observed a decrease in head motion after both\nalignment edits, with the gradual edit reaching a reduction in head speed of\n8\\% greater than that of instant alignment, confirming the usefulness of these\nedits for streaming video on-demand. Finally, parameters to implement\n\\textit{Fade-rotation} are described.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.10649v1"
    },
    {
        "title": "Recent Advances in Rate Control: From Optimisation to Implementation and\n  Beyond",
        "authors": [
            "Xuekai Wei",
            "Mingliang Zhou",
            "Heqiang Wang",
            "Haoyan Yang",
            "Lei Chen",
            "Sam Kwong"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Video coding is a video compression technique that compresses the original\nvideo sequence to produce a smaller archive file or reduce the transmission\nbandwidth under constraints on the visual quality loss. Rate control (RC) plays\na critical role in video coding. It can achieve stable stream output in\npractical applications, especially real-time video applications such as video\nconferencing or game live streaming. Most RC algorithms either directly or\nindirectly characterise the relationship between the bit rate (R) and\nquantisation (Q) and then allocate bits to every coding unit so as to guarantee\nthe global bit rate and video quality level. This paper comprehensively reviews\nthe classic RC technologies used in international video standards of past\ngenerations, analyses the mathematical models and implementation mechanisms of\nvarious schemes, and compares the performance of recent state-of-the-art RC\nalgorithms. Finally, we discuss future directions and new application areas for\nRC methods. We hope that this review can help support the development,\nimplementation, and application of RC for new video coding standards.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.10815v2"
    },
    {
        "title": "A Rate Control Algorithm for Video-based Point Cloud Compression",
        "authors": [
            "Fangyu Shen",
            "Wei Gao"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Video-based point cloud compression (V-PCC) has been an emerging compression\ntechnology that projects the 3D point cloud into a 2D plane and uses high\nefficiency video coding (HEVC) to encode the projected 2D videos (geometry\nvideo and color video). In this work, we propose a rate control algorithm for\nthe all-intra (AI) configuration of V-PCC. Specifically, based on the\nquality-dependency existing in the projected videos, we develop an optimization\nformulation to allocate target bits between the geometry video and the color\nvideo. Furthermore, we design a two-pass method for HEVC to adapt to the new\ncharacteristics of projected videos, which significantly improves the accuracy\nof rate control. Experimental results demonstrate that our algorithm\noutperforms V-PCC without rate control in R-D performance with just 0.43%\nbitrate error.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.11825v1"
    },
    {
        "title": "Subtitle-based Viewport Prediction for 360-degree Virtual Tourism Video",
        "authors": [
            "Chuanzhe Jing",
            "Tho Nguyen Duc",
            "Phan Xuan Tan",
            "Eiji Kamioka"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  360-degree streaming videos can provide a rich immersive experiences to the\nusers. However, it requires an extremely high bandwidth network. One of the\ncommon solutions for saving bandwidth consumption is to stream only a portion\nof video covered by the user's viewport. To do that, the user's viewpoint\nprediction is indispensable. In existing viewport prediction methods, they\nmainly concentrate on the user's head movement trajectory and video saliency.\nNone of them consider navigation information contained in the video, which can\nturn the attention of the user to specific regions in the video with high\nprobability. Such information can be included in video subtitles, especially\nthe one in 360-degree virtual tourism videos. This fact reveals the potential\ncontribution of video subtitles to viewport prediction. Therefore, in this\npaper, a subtitle-based viewport prediction model for 360-degree virtual\ntourism videos is proposed. This model leverages the navigation information in\nthe video subtitles in addition to head movement trajectory and video saliency,\nto improve the prediction accuracy. The experimental results demonstrate that\nthe proposed model outperforms baseline methods which only use head movement\ntrajectory and video saliency for viewport prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.02367v1"
    },
    {
        "title": "Joint Geometric-Semantic Driven Character Line Drawing Generation",
        "authors": [
            "Cheng-Yu Fang",
            "Xian-Feng Han"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Character line drawing synthesis can be formulated as a special case of\nimage-to-image translation problem that automatically manipulates the\nphoto-to-line drawing style transformation. In this paper, we present the first\ngenerative adversarial network based end-to-end trainable translation\narchitecture, dubbed P2LDGAN, for automatic generation of high-quality\ncharacter drawings from input photos/images. The core component of our approach\nis the joint geometric-semantic driven generator, which uses our well-designed\ncross-scale dense skip connections framework to embed learned geometric and\nsemantic information for generating delicate line drawings. In order to support\nthe evaluation of our model, we release a new dataset including 1,532\nwell-matched pairs of freehand character line drawings as well as corresponding\ncharacter images/photos, where these line drawings with diverse styles are\nmanually drawn by skilled artists. Extensive experiments on our introduced\ndataset demonstrate the superior performance of our proposed models against the\nstate-of-the-art approaches in terms of quantitative, qualitative and human\nevaluations. Our code, models and dataset will be available at Github.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.02998v3"
    },
    {
        "title": "EEG-based Emotion Recognition with Spatial and Functional Brain Mapping\n  of CNS and PNS Signals",
        "authors": [
            "Zhiyao Cen",
            "Xiangwen Deng",
            "Hengjie Zheng",
            "Jianing Zhao",
            "Anjie Jin",
            "Chentao Fu",
            "Tianqi Wang",
            "Shangming Yang",
            "Jingdian Yang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Emotion plays a significant role in our daily life. Recognition of emotion is\nwide-spread in the field of health care and human-computer interaction. Emotion\nis the result of the coordinated activities of cortical and subcortical neural\nprocesses, which correlate to specific physiological responses. However, the\nexisting emotion recognition techniques failed to combine various physiological\nsignals as one integrated feature representation. Meanwhile, many researchers\nignored the problem of over-fitting model with high accuracy, which was\nactually false high accuracy caused by improper pre-processing. In this paper,\nsigmoid baseline filtering is conducted to solve the over-fitting problem from\nsource. To construct a physiological-based algorithm, a 3D spatial and\nfunctional brain mapping is proposed based on human physiological mechanism and\ninternational electrode system, which combines the signals of the central and\nperipheral nervous system together. By combining the baseline filtering, 3D\nbrain mapping, and simple 4D-CNN, a novel emotion recognition model is finally\nproposed. Experiment results demonstrate that the performance of the proposed\nmodel is comparable to the state of art algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.03330v1"
    },
    {
        "title": "ETMA: Efficient Transformer Based Multilevel Attention framework for\n  Multimodal Fake News Detection",
        "authors": [
            "Ashima Yadav",
            "Shivani Gaba",
            "Haneef Khan",
            "Ishan Budhiraja",
            "Akansha Singh",
            "Krishan Kant Singh"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this new digital era, social media has created a severe impact on the\nlives of people. In recent times, fake news content on social media has become\none of the major challenging problems for society. The dissemination of\nfabricated and false news articles includes multimodal data in the form of text\nand images. The previous methods have mainly focused on unimodal analysis.\nMoreover, for multimodal analysis, researchers fail to keep the unique\ncharacteristics corresponding to each modality. This paper aims to overcome\nthese limitations by proposing an Efficient Transformer based Multilevel\nAttention (ETMA) framework for multimodal fake news detection, which comprises\nthe following components: visual attention-based encoder, textual\nattention-based encoder, and joint attention-based learning. Each component\nutilizes the different forms of attention mechanism and uniquely deals with\nmultimodal data to detect fraudulent content. The efficacy of the proposed\nnetwork is validated by conducting several experiments on four real-world fake\nnews datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset,\nand Risdal Fake News Dataset using multiple evaluation metrics. The results\nshow that the proposed method outperforms the baseline methods on all four\ndatasets. Further, the computation time of the model is also lower than the\nstate-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.07331v2"
    },
    {
        "title": "LECA: A Learned Approach for Efficient Cover-agnostic Watermarking",
        "authors": [
            "Xiyang Luo",
            "Michael Goebel",
            "Elnaz Barshan",
            "Feng Yang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this work, we present an efficient multi-bit deep image watermarking\nmethod that is cover-agnostic yet also robust to geometric distortions such as\ntranslation and scaling as well as other distortions such as JPEG compression\nand noise. Our design consists of a light-weight watermark encoder jointly\ntrained with a deep neural network based decoder. Such a design allows us to\nretain the efficiency of the encoder while fully utilizing the power of a deep\nneural network. Moreover, the watermark encoder is independent of the image\ncontent, allowing users to pre-generate the watermarks for further efficiency.\nTo offer robustness towards geometric transformations, we introduced a learned\nmodel for predicting the scale and offset of the watermarked images. Moreover,\nour watermark encoder is independent of the image content, making the generated\nwatermarks universally applicable to different cover images. Experiments show\nthat our method outperforms comparably efficient watermarking methods by a\nlarge margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.10813v1"
    },
    {
        "title": "Video Analytics in Elite Soccer: A Distributed Computing Perspective",
        "authors": [
            "Debesh Jha",
            "Ashish Rauniyar",
            "Håvard D. Johansen",
            "Dag Johansen",
            "Michael A. Riegler",
            "Pål Halvorsen",
            "Ulas Bagci"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Ubiquitous sensors and Internet of Things (IoT) technologies have\nrevolutionized the sports industry, providing new methodologies for planning,\neffective coordination of training, and match analysis post game. New methods,\nincluding machine learning, image and video processing, have been developed for\nperformance evaluation, allowing the analyst to track the performance of a\nplayer in real-time. Following FIFA's 2015 approval of electronics performance\nand tracking system during games, performance data of a single player or the\nentire team is allowed to be collected using GPS-based wearables. Data from\npractice sessions outside the sporting arena is being collected in greater\nnumbers than ever before. Realizing the significance of data in professional\nsoccer, this paper presents video analytics, examines recent state-of-the-art\nliterature in elite soccer, and summarizes existing real-time video analytics\nalgorithms. We also discuss real-time crowdsourcing of the obtained data,\ntactical and technical performance, distributed computing and its importance in\nvideo analytics and propose a future research perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.11335v1"
    },
    {
        "title": "De-END: Decoder-driven Watermarking Network",
        "authors": [
            "Han Fang",
            "Zhaoyang Jia",
            "Yupeng Qiu",
            "Jiyi Zhang",
            "Weiming Zhang",
            "Ee-Chien Chang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  With recent advances in machine learning, researchers are now able to solve\ntraditional problems with new solutions. In the area of digital watermarking,\ndeep-learning-based watermarking technique is being extensively studied. Most\nexisting approaches adopt a similar encoder-driven scheme which we name END\n(Encoder-NoiseLayer-Decoder) architecture. In this paper, we revamp the\narchitecture and creatively design a decoder-driven watermarking network dubbed\nDe-END which greatly outperforms the existing END-based methods. The motivation\nfor designing De-END originated from the potential drawback we discovered in\nEND architecture: The encoder may embed redundant features that are not\nnecessary for decoding, limiting the performance of the whole network. We\nconducted a detailed analysis and found that such limitations are caused by\nunsatisfactory coupling between the encoder and decoder in END. De-END\naddresses such drawbacks by adopting a Decoder-Encoder-Noiselayer-Decoder\narchitecture. In De-END, the host image is firstly processed by the decoder to\ngenerate a latent feature map instead of being directly fed into the encoder.\nThis latent feature map is concatenated to the original watermark message and\nthen processed by the encoder. This change in design is crucial as it makes the\nfeature of encoder and decoder directly shared thus the encoder and decoder are\nbetter coupled. We conducted extensive experiments and the results show that\nthis framework outperforms the existing state-of-the-art (SOTA) END-based deep\nlearning watermarking both in visual quality and robustness. On the premise of\nthe same decoder structure, the visual quality (measured by PSNR) of De-END\nimproves by 1.6dB (45.16dB to 46.84dB), and extraction accuracy after JPEG\ncompression (QF=50) distortion outperforms more than 4% (94.9% to 99.1%).\n",
        "pdf_link": "http://arxiv.org/pdf/2206.13032v1"
    },
    {
        "title": "A Topic-Attentive Transformer-based Model For Multimodal Depression\n  Detection",
        "authors": [
            "Yanrong Guo",
            "Chenyang Zhu",
            "Shijie Hao",
            "Richang Hong"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Depression is one of the most common mental disorders, which imposes heavy\nnegative impacts on one's daily life. Diagnosing depression based on the\ninterview is usually in the form of questions and answers. In this process, the\naudio signals and their text transcripts of a subject are correlated to\ndepression cues and easily recorded. Therefore, it is feasible to build an\nAutomatic Depression Detection (ADD) model based on the data of these\nmodalities in practice. However, there are two major challenges that should be\naddressed for constructing an effective ADD model. The first challenge is the\norganization of the textual and audio data, which can be of various contents\nand lengths for different subjects. The second challenge is the lack of\ntraining samples due to the privacy concern. Targeting to these two challenges,\nwe propose the TOpic ATtentive transformer-based ADD model, abbreviated as\nTOAT. To address the first challenge, in the TOAT model, topic is taken as the\nbasic unit of the textual and audio data according to the question-answer form\nin a typical interviewing process. Based on that, a topic attention module is\ndesigned to learn the importance of of each topic, which helps the model better\nretrieve the depressed samples. To solve the issue of data scarcity, we\nintroduce large pre-trained models, and the fine-tuning strategy is adopted\nbased on the small-scale ADD training data. We also design a two-branch\narchitecture with a late-fusion strategy for building the TOAT model, in which\nthe textual and audio data are encoded independently. We evaluate our model on\nthe multimodal DAIC-WOZ dataset specifically designed for the ADD task.\nExperimental results show the superiority of our method. More importantly, the\nablation studies demonstrate the effectiveness of the key elements in the TOAT\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.13256v1"
    },
    {
        "title": "SDRTV-to-HDRTV via Hierarchical Dynamic Context Feature Mapping",
        "authors": [
            "Gang He",
            "Kepeng Xu",
            "Li Xu",
            "Chang Wu",
            "Ming Sun",
            "Xing Wen",
            "Yu-Wing Tai"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this work, we address the task of SDR videos to HDR\nvideos(SDRTV-to-HDRTV). Previous approaches use global feature modulation for\nSDRTV-to-HDRTV. Feature modulation scales and shifts the features in the\noriginal feature space, which has limited mapping capability. In addition, the\nglobal image mapping cannot restore detail in HDR frames due to the luminance\ndifferences in different regions of SDR frames. To resolve the appeal, we\npropose a two-stage solution. The first stage is a hierarchical Dynamic Context\nfeature mapping (HDCFM) model. HDCFM learns the SDR frame to HDR frame mapping\nfunction via hierarchical feature modulation (HME and HM ) module and a dynamic\ncontext feature transformation (DCT) module. The HME estimates the feature\nmodulation vector, HM is capable of hierarchical feature modulation, consisting\nof global feature modulation in series with local feature modulation, and is\ncapable of adaptive mapping of local image features. The DCT module constructs\na feature transformation module in conjunction with the context, which is\ncapable of adaptively generating a feature transformation matrix for feature\nmapping. Compared with simple feature scaling and shifting, the DCT module can\nmap features into a new feature space and thus has a more excellent feature\nmapping capability. In the second stage, we introduce a patch\ndiscriminator-based context generation model PDCG to obtain subjective quality\nenhancement of over-exposed regions. PDCG can solve the problem that the model\nis challenging to train due to the proportion of overexposed regions of the\nimage. The proposed method can achieve state-of-the-art objective and\nsubjective quality results. Specifically, HDCFM achieves a PSNR gain of 0.81 dB\nat a parameter of about 100K. The number of parameters is 1/14th of the\nprevious state-of-the-art methods. The test code will be released soon.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00319v2"
    },
    {
        "title": "Privacy-preserving Reflection Rendering for Augmented Reality",
        "authors": [
            "Yiqin Zhao",
            "Sheng Wei",
            "Tian Guo"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Many augmented reality (AR) applications rely on omnidirectional environment\nlighting to render photorealistic virtual objects. When the virtual objects\nconsist of reflective materials, such as a metallic sphere, the required\nlighting information to render such objects can consist of privacy-sensitive\ninformation that is outside the current camera view. In this paper, we show,\nfor the first time, that accuracy-driven multi-view environment lighting can\nreveal out-of-camera scene information and compromise privacy. We present a\nsimple yet effective privacy attack that extracts sensitive scene information\nsuch as human face and text information from the rendered objects, under a\nnumber of application scenarios.\n  To defend against such attacks, we develop a novel $IPC^{2}S$ defense and a\nconditional $R^2$ defense. Our $IPC^{2}S$ defense, used in conjunction with a\ngeneric lighting reconstruction method, preserves the scene geometry while\nobfuscating the privacy-sensitive information. As a proof-of-concept, we\nleverage existing OCR and face detection models to identify text and human\nfaces from past camera observations and blur the color pixels associated with\ndetected regions. We evaluate the visual quality impact of our defense by\ncomparing rendered virtual objects to ones rendered with a generic\nmulti-lighting reconstruction technique, ARKit, and $R^2$ defense. Our visual\nand quantitative results demonstrate that our defense leads to structurally\nsimilar reflections with up to 0.98 SSIM score across a variety of rendering\nscenarios while preserving sensitive information by reducing the automatic\nextraction success rate to at most 8.8%.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.03056v1"
    },
    {
        "title": "Human-centric Spatio-Temporal Video Grounding via the Combination of\n  Mutual Matching Network and TubeDETR",
        "authors": [
            "Fan Yu",
            "Zhixiang Zhao",
            "Yuchen Wang",
            "Yi Xu",
            "Tongwei Ren",
            "Gangshan Wu"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this technical report, we represent our solution for the Human-centric\nSpatio-Temporal Video Grounding (HC-STVG) track of the 4th Person in Context\n(PIC) workshop and challenge. Our solution is built on the basis of TubeDETR\nand Mutual Matching Network (MMN). Specifically, TubeDETR exploits a video-text\nencoder and a space-time decoder to predict the starting time, the ending time\nand the tube of the target person. MMN detects persons in images, links them as\ntubes, extracts features of person tubes and the text description, and predicts\nthe similarities between them to choose the most likely person tube as the\ngrounding result. Our solution finally finetunes the results by combining the\nspatio localization of MMN and with temporal localization of TubeDETR. In the\nHC-STVG track of the 4th PIC challenge, our solution achieves the third place.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.04201v2"
    },
    {
        "title": "ChoreoGraph: Music-conditioned Automatic Dance Choreography over a Style\n  and Tempo Consistent Dynamic Graph",
        "authors": [
            "Ho Yin Au",
            "Jie Chen",
            "Junkun Jiang",
            "Yike Guo"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  To generate dance that temporally and aesthetically matches the music is a\nchallenging problem, as the following factors need to be considered. First, the\naesthetic styles and messages conveyed by the motion and music should be\nconsistent. Second, the beats of the generated motion should be locally aligned\nto the musical features. And finally, basic choreomusical rules should be\nobserved, and the motion generated should be diverse. To address these\nchallenges, we propose ChoreoGraph, which choreographs high-quality dance\nmotion for a given piece of music over a Dynamic Graph. A data-driven learning\nstrategy is proposed to evaluate the aesthetic style and rhythmic connections\nbetween music and motion in a progressively learned cross-modality embedding\nspace. The motion sequences will be beats-aligned based on the music segments\nand then incorporated as nodes of a Dynamic Motion Graph. Compatibility factors\nsuch as the style and tempo consistency, motion context connection, action\ncompleteness, and transition smoothness are comprehensively evaluated to\ndetermine the node transition in the graph. We demonstrate that our\nrepertoire-based framework can generate motions with aesthetic consistency and\nrobustly extensible in diversity. Both quantitative and qualitative experiment\nresults show that our proposed model outperforms other baseline models.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.07386v1"
    },
    {
        "title": "FRAS: Federated Reinforcement Learning empowered Adaptive Point Cloud\n  Video Streaming",
        "authors": [
            "Yu Gao",
            "Pengyuan Zhou",
            "Zhi Liu",
            "Bo Han",
            "Pan Hui"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Point cloud video transmission is challenging due to high encoding/decoding\ncomplexity, high video bitrate, and low latency requirement. Consequently,\nconventional adaptive streaming methodologies often find themselves\nunsatisfactory to meet the requirements in threefold: 1) current algorithms\nreuse existing quality of experience (QoE) definitions while overlooking the\nunique features of point cloud video thus failing to provide optimal user\nexperience, 2) most deep learning approaches require long-span data collections\nto learn sufficiently varied network conditions and result in long training\nperiods and capacity occupation, 3) cloud training approaches pose privacy\nrisks caused by leakage of user reported service usage and networking\nconditions.\n  To overcome the limitations, we present FRAS, the first federated\nreinforcement learning framework, to the best of our knowledge, for adaptive\npoint cloud video streaming. We define a new QoE model which takes the unique\nfeatures of point cloud video into account. Each client uses reinforcement\nlearning (RL) to train video quality selection with the objective of optimizing\nthe user's QoE under multiple constraints. Then, a federated learning framework\nis integrated with the RL algorithm to enhance training performance with\nprivacy preservation. Extensive simulations using real point cloud videos and\nnetwork traces reveal the superiority of the proposed scheme over baseline\nschemes. We also implement a prototype that demonstrates the performance of\nFRAS via real-world tests.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.07394v4"
    },
    {
        "title": "Adaptive Marginalized Semantic Hashing for Unpaired Cross-Modal\n  Retrieval",
        "authors": [
            "Kaiyi Luo",
            "Chao Zhang",
            "Huaxiong Li",
            "Xiuyi Jia",
            "Chunlin Chen"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In recent years, Cross-Modal Hashing (CMH) has aroused much attention due to\nits fast query speed and efficient storage. Previous literatures have achieved\npromising results for Cross-Modal Retrieval (CMR) by discovering discriminative\nhash codes and modality-specific hash functions. Nonetheless, most existing CMR\nworks are subjected to some restrictions: 1) It is assumed that data of\ndifferent modalities are fully paired, which is impractical in real\napplications due to sample missing and false data alignment, and 2) binary\nregression targets including the label matrix and binary codes are too rigid to\neffectively learn semantic-preserving hash codes and hash functions. To address\nthese problems, this paper proposes an Adaptive Marginalized Semantic Hashing\n(AMSH) method which not only enhances the discrimination of latent\nrepresentations and hash codes by adaptive margins, but also can be used for\nboth paired and unpaired CMR. As a two-step method, in the first step, AMSH\ngenerates semantic-aware modality-specific latent representations with\nadaptively marginalized labels, which enlarges the distances between different\nclasses, and exploits the labels to preserve the inter-modal and intra-modal\nsemantic similarities into latent representations and hash codes. In the second\nstep, adaptive margin matrices are embedded into the hash codes, and enlarge\nthe gaps between positive and negative bits, which improves the discrimination\nand robustness of hash functions. On this basis, AMSH generates\nsimilarity-preserving hash codes and robust hash functions without strict\none-to-one data correspondence requirement. Experiments are conducted on\nseveral benchmark datasets to demonstrate the superiority and flexibility of\nAMSH over some state-of-the-art CMR methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.11880v1"
    },
    {
        "title": "GA2MIF: Graph and Attention Based Two-Stage Multi-Source Information\n  Fusion for Conversational Emotion Detection",
        "authors": [
            "Jiang Li",
            "Xiaoping Wang",
            "Guoqing Lv",
            "Zhigang Zeng"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Multimodal Emotion Recognition in Conversation (ERC) plays an influential\nrole in the field of human-computer interaction and conversational robotics\nsince it can motivate machines to provide empathetic services. Multimodal data\nmodeling is an up-and-coming research area in recent years, which is inspired\nby human capability to integrate multiple senses. Several graph-based\napproaches claim to capture interactive information between modalities, but the\nheterogeneity of multimodal data makes these methods prohibit optimal\nsolutions. In this work, we introduce a multimodal fusion approach named Graph\nand Attention based Two-stage Multi-source Information Fusion (GA2MIF) for\nemotion detection in conversation. Our proposed method circumvents the problem\nof taking heterogeneous graph as input to the model while eliminating complex\nredundant connections in the construction of graph. GA2MIF focuses on\ncontextual modeling and cross-modal modeling through leveraging Multi-head\nDirected Graph ATtention networks (MDGATs) and Multi-head Pairwise Cross-modal\nATtention networks (MPCATs), respectively. Extensive experiments on two public\ndatasets (i.e., IEMOCAP and MELD) demonstrate that the proposed GA2MIF has the\ncapacity to validly capture intra-modal long-range contextual information and\ninter-modal complementary information, as well as outperforms the prevalent\nState-Of-The-Art (SOTA) models by a remarkable margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.11900v6"
    },
    {
        "title": "ACM Multimedia Grand Challenge on Detecting Cheapfakes",
        "authors": [
            "Shivangi Aneja",
            "Cise Midoglu",
            "Duc-Tien Dang-Nguyen",
            "Sohail Ahmed Khan",
            "Michael Riegler",
            "Pål Halvorsen",
            "Chris Bregler",
            "Balu Adsumilli"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Cheapfake is a recently coined term that encompasses non-AI (``cheap'')\nmanipulations of multimedia content. Cheapfakes are known to be more prevalent\nthan deepfakes. Cheapfake media can be created using editing software for\nimage/video manipulations, or even without using any software, by simply\naltering the context of an image/video by sharing the media alongside\nmisleading claims. This alteration of context is referred to as out-of-context\n(OOC) misuse of media. OOC media is much harder to detect than fake media,\nsince the images and videos are not tampered. In this challenge, we focus on\ndetecting OOC images, and more specifically the misuse of real photographs with\nconflicting image captions in news items. The aim of this challenge is to\ndevelop and benchmark models that can be used to detect whether given samples\n(news image and associated captions) are OOC, based on the recently compiled\nCOSMOS dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.14534v1"
    },
    {
        "title": "Consistent Quality Oriented Rate Control in HEVC via Balancing Intra and\n  Inter Frame Coding",
        "authors": [
            "Wei Gao",
            "Qiuping Jiang",
            "Ronggang Wang",
            "Siwei Ma",
            "Ge Li",
            "Sam Kwong"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Consistent quality oriented rate control in video coding has attracted much\nmore attention. However, the existing efforts only focus on decreasing\nvariations between every two adjacent frames, but neglect coding trade-off\nproblem between intra and inter frames. In this paper, we deal with it from a\nnew perspective, where intra frame quantization parameter (IQP) and rate\ncontrol are optimized for balanced coding. First, due to the importance of\nintra frames, a new framework is proposed for consistent quality oriented IQP\nprediction, and then we remove unqualified IQP candidates using the proposed\npenalty term. Second, we extensively evaluate possible features, and select\ntarget bits per pixel for all remaining frames, average and standard variance\nof frame QPs, where equivalent acquisition methods for QP features are given.\nThird, predicted IQPs are clipped effectively according to bandwidth and\nprevious information for better bit rate accuracy. Compared with High\nEfficiency Video Coding (HEVC) reference baseline, experiments demonstrate that\nour method reduces quality fluctuation greatly by 37.2% on frame-level standard\nvariance of peak-signal-noise-ratio (PSNR) and 45.1% on that of structural\nsimilarity (SSIM). Moreover, it also can have satisfactory results on\nRate-Distortion (R-D) performance, bit accuracy and buffer control.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.00125v1"
    },
    {
        "title": "GraphMFT: A Graph Network based Multimodal Fusion Technique for Emotion\n  Recognition in Conversation",
        "authors": [
            "Jiang Li",
            "Xiaoping Wang",
            "Guoqing Lv",
            "Zhigang Zeng"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Multimodal machine learning is an emerging area of research, which has\nreceived a great deal of scholarly attention in recent years. Up to now, there\nare few studies on multimodal Emotion Recognition in Conversation (ERC). Since\nGraph Neural Networks (GNNs) possess the powerful capacity of relational\nmodeling, they have an inherent advantage in the field of multimodal learning.\nGNNs leverage the graph constructed from multimodal data to perform intra- and\ninter-modal information interaction, which effectively facilitates the\nintegration and complementation of multimodal data. In this work, we propose a\nnovel Graph network based Multimodal Fusion Technique (GraphMFT) for emotion\nrecognition in conversation. Multimodal data can be modeled as a graph, where\neach data object is regarded as a node, and both intra- and inter-modal\ndependencies existing between data objects can be regarded as edges. GraphMFT\nutilizes multiple improved graph attention networks to capture intra-modal\ncontextual information and inter-modal complementary information. In addition,\nthe proposed GraphMFT attempts to address the challenges of existing\ngraph-based multimodal conversational emotion recognition models such as MMGCN.\nEmpirical results on two public multimodal datasets reveal that our model\noutperforms the State-Of-The-Art (SOTA) approaches with the accuracy of 67.90%\nand 61.30%.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.00339v5"
    },
    {
        "title": "Where Are You Looking?: A Large-Scale Dataset of Head and Gaze Behavior\n  for 360-Degree Videos and a Pilot Study",
        "authors": [
            "Yili Jin",
            "Junhua Liu",
            "Fangxin Wang",
            "Shuguang Cui"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  360{\\deg} videos in recent years have experienced booming development.\nCompared to traditional videos, 360{\\deg} videos are featured with uncertain\nuser behaviors, bringing opportunities as well as challenges. Datasets are\nnecessary for researchers and developers to explore new ideas and conduct\nreproducible analyses for fair comparisons among different solutions. However,\nexisting related datasets mostly focused on users' field of view (FoV),\nignoring the more important eye gaze information, not to mention the integrated\nextraction and analysis of both FoV and eye gaze. Besides, users' behavior\npatterns are highly related to videos, yet most existing datasets only\ncontained videos with subjective and qualitative classification from video\ngenres, which lack quantitative analysis and fail to characterize the intrinsic\nproperties of a video scene. To this end, we first propose a quantitative\ntaxonomy for 360{\\deg} videos that contains three objective technical metrics.\nBased on this taxonomy, we collect a dataset containing users' head and gaze\nbehaviors simultaneously, which outperforms existing datasets with rich\ndimensions, large scale, strong diversity, and high frequency. Then we conduct\na pilot study on user's behaviors and get some interesting findings such as\nuser's head direction will follow his/her gaze direction with the most possible\ntime interval. A case of application in tile-based 360{\\deg} video streaming\nbased on our dataset is later conducted, demonstrating a great performance\nimprovement of existing works by leveraging our provided gaze information. Our\ndataset is available at https://cuhksz-inml.github.io/head_gaze_dataset/\n",
        "pdf_link": "http://arxiv.org/pdf/2208.04079v1"
    },
    {
        "title": "HERO: HiErarchical spatio-tempoRal reasOning with Contrastive Action\n  Correspondence for End-to-End Video Object Grounding",
        "authors": [
            "Mengze Li",
            "Tianbao Wang",
            "Haoyu Zhang",
            "Shengyu Zhang",
            "Zhou Zhao",
            "Wenqiao Zhang",
            "Jiaxu Miao",
            "Shiliang Pu",
            "Fei Wu"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Video Object Grounding (VOG) is the problem of associating spatial object\nregions in the video to a descriptive natural language query. This is a\nchallenging vision-language task that necessitates constructing the correct\ncross-modal correspondence and modeling the appropriate spatio-temporal context\nof the query video and caption, thereby localizing the specific objects\naccurately. In this paper, we tackle this task by a novel framework called\nHiErarchical spatio-tempoRal reasOning (HERO) with contrastive action\ncorrespondence. We study the VOG task at two aspects that prior works\noverlooked: (1) Contrastive Action Correspondence-aware Retrieval. Notice that\nthe fine-grained video semantics (e.g., multiple actions) is not totally\naligned with the annotated language query (e.g., single action), we first\nintroduce the weakly-supervised contrastive learning that classifies the video\nas action-consistent and action-independent frames relying on the video-caption\naction semantic correspondence. Such a design can build the fine-grained\ncross-modal correspondence for more accurate subsequent VOG. (2) Hierarchical\nSpatio-temporal Modeling Improvement. While transformer-based VOG models\npresent their potential in sequential modality (i.e., video and caption)\nmodeling, existing evidence also indicates that the transformer suffers from\nthe issue of the insensitive spatio-temporal locality. Motivated by that, we\ncarefully design the hierarchical reasoning layers to decouple fully connected\nmulti-head attention and remove the redundant interfering correlations.\nFurthermore, our proposed pyramid and shifted alignment mechanisms are\neffective to improve the cross-modal information utilization of neighborhood\nspatial regions and temporal frames. We conducted extensive experiments to show\nour HERO outperforms existing techniques by achieving significant improvement\non two benchmark datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.05818v1"
    },
    {
        "title": "A Privacy-Preserving and End-to-End-Based Encrypted Image Retrieval\n  Scheme",
        "authors": [
            "Zhixun Lu",
            "Qihua Feng",
            "Peiya Li"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Applying encryption technology to image retrieval can ensure the security and\nprivacy of personal images. The related researches in this field have focused\non the organic combination of encryption algorithm and artificial feature\nextraction. Many existing encrypted image retrieval schemes cannot prevent\nfeature leakage and file size increase or cannot achieve satisfied retrieval\nperformance. In this paper, A new end-to-end encrypted image retrieval scheme\nis presented. First, images are encrypted by using block rotation, new\northogonal transforms and block permutation during the JPEG compression\nprocess. Second, we combine the triplet loss and the cross entropy loss to\ntrain a network model, which contains gMLP modules, by end-to-end learning for\nextracting cipher-images' features. Compared with manual features extraction\nsuch as extracting color histogram, the end-to-end mechanism can economize on\nmanpower. Experimental results show that our scheme has good retrieval\nperformance, while can ensure compression friendly and no feature leakage.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.11876v1"
    },
    {
        "title": "Reproducibility Companion Paper: Describing Subjective Experiment\n  Consistency by $p$-Value P-P Plot",
        "authors": [
            "Jakub Nawała",
            "Lucjan Janowski",
            "Bogdan Ćmiel",
            "Krzysztof Rusek",
            "Marc A. Kastner",
            "Jan Zahálka"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this paper we reproduce experimental results presented in our earlier work\ntitled \"Describing Subjective Experiment Consistency by $p$-Value P-P Plot\"\nthat was presented in the course of the 28th ACM International Conference on\nMultimedia. The paper aims at verifying the soundness of our prior results and\nhelping others understand our software framework. We present artifacts that\nhelp reproduce tables, figures and all the data derived from raw subjective\nresponses that were included in our earlier work. Using the artifacts we show\nthat our results are reproducible. We invite everyone to use our software\nframework for subjective responses analyses going beyond reproducibility\nefforts.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.00526v1"
    },
    {
        "title": "Network-aware Prefetching Method for Short-Form Video Streaming",
        "authors": [
            "Duc Nguyen",
            "Phong Nguyen",
            "Vu Long",
            "Truong Thu Huong",
            "Pham Ngoc Nam"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Recent years have witnessed the rising of short-form video platforms such as\nTikTok. Apart from conventional videos, short-form videos are much shorter and\nusers frequently change the content to watch. Thus, it is crucial to have an\neffective streaming method for this new type of video. In this paper, we\npropose a resource-efficient prefetching method for short-form video streaming.\nTaking into account network throughput conditions and user viewing behaviors,\nthe proposed method dynamically adapts the amount of prefetched video data.\nExperiment results show that our method can reduce the data waste by 37~52%\ncompared to other existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.02927v1"
    },
    {
        "title": "Self-supervised Multi-Modal Video Forgery Attack Detection",
        "authors": [
            "Chenhui Zhao",
            "Xiang Li",
            "Rabih Younes"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Video forgery attack threatens the surveillance system by replacing the video\ncaptures with unrealistic synthesis, which can be powered by the latest augment\nreality and virtual reality technologies. From the machine perception aspect,\nvisual objects often have RF signatures that are naturally synchronized with\nthem during recording. In contrast to video captures, the RF signatures are\nmore difficult to attack given their concealed and ubiquitous nature. In this\nwork, we investigate multimodal video forgery attack detection methods using\nboth vision and wireless modalities. Since wireless signal-based human\nperception is environmentally sensitive, we propose a self-supervised training\nstrategy to enable the system to work without external annotation and thus can\nadapt to different environments. Our method achieves a perfect human detection\naccuracy and a high forgery attack detection accuracy of 94.38% which is\ncomparable with supervised methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.06345v2"
    },
    {
        "title": "TIMIT-TTS: a Text-to-Speech Dataset for Multimodal Synthetic Media\n  Detection",
        "authors": [
            "Davide Salvi",
            "Brian Hosler",
            "Paolo Bestagini",
            "Matthew C. Stamm",
            "Stefano Tubaro"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  With the rapid development of deep learning techniques, the generation and\ncounterfeiting of multimedia material are becoming increasingly straightforward\nto perform. At the same time, sharing fake content on the web has become so\nsimple that malicious users can create unpleasant situations with minimal\neffort. Also, forged media are getting more and more complex, with manipulated\nvideos that are taking the scene over still images. The multimedia forensic\ncommunity has addressed the possible threats that this situation could imply by\ndeveloping detectors that verify the authenticity of multimedia objects.\nHowever, the vast majority of these tools only analyze one modality at a time.\nThis was not a problem as long as still images were considered the most widely\nedited media, but now, since manipulated videos are becoming customary,\nperforming monomodal analyses could be reductive. Nonetheless, there is a lack\nin the literature regarding multimodal detectors, mainly due to the scarsity of\ndatasets containing forged multimodal data to train and test the designed\nalgorithms. In this paper we focus on the generation of an audio-visual\ndeepfake dataset. First, we present a general pipeline for synthesizing speech\ndeepfake content from a given real or fake video, facilitating the creation of\ncounterfeit multimodal material. The proposed method uses Text-to-Speech (TTS)\nand Dynamic Time Warping techniques to achieve realistic speech tracks. Then,\nwe use the pipeline to generate and release TIMIT-TTS, a synthetic speech\ndataset containing the most cutting-edge methods in the TTS field. This can be\nused as a standalone audio dataset, or combined with other state-of-the-art\nsets to perform multimodal research. Finally, we present numerous experiments\nto benchmark the proposed dataset in both mono and multimodal conditions,\nshowing the need for multimodal forensic detectors and more suitable data.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.08000v1"
    },
    {
        "title": "Adaptive 3D Mesh Steganography Based on Feature-Preserving Distortion",
        "authors": [
            "Yushu Zhang",
            "Jiahao Zhu",
            "Mignfu Xue",
            "Xinpeng Zhang",
            "Xiaochun Cao"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  3D mesh steganographic algorithms based on geometric modification are\nvulnerable to 3D steganalyzers. In this paper, we propose a highly adaptive 3D\nmesh steganography based on feature-preserving distortion (FPD), which\nguarantees high embedding capacity while effectively resisting 3D steganalysis.\nSpecifically, we first transform vertex coordinates into integers and derive\nbitplanes from them to construct the embedding domain. To better measure the\nmesh distortion caused by message embedding, we propose FPD based on the most\neffective sub-features of the state-of-the-art steganalytic feature set. By\nimproving and minimizing FPD, we can efficiently calculate the optimal\nvertex-changing distribution and simultaneously preserve mesh features, such as\nsteganalytic and geometric features, to a certain extent. By virtue of the\noptimal distribution, we adopt the Q-layered syndrome trellis coding (STC) for\npractical message embedding. However, when Q varies, calculating bit\nmodification probability (BMP) in each layer of Q-layered will be cumbersome.\nHence, we contrapuntally design a universal and automatic BMP calculation\napproach. Extensive experimental results demonstrate that the proposed\nalgorithm outperforms most state-of-the-art 3D mesh steganographic algorithms\nin terms of resisting 3D steganalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.08884v1"
    },
    {
        "title": "The Beauty of Repetition in Machine Composition Scenarios",
        "authors": [
            "Zhejing Hu",
            "Xiao Ma",
            "Yan Liu",
            "Gong Chen",
            "Yongxu Liu"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Repetition, a basic form of artistic creation, appears in most musical works\nand delivers enthralling aesthetic experiences.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.11426v1"
    },
    {
        "title": "Blind Robust VideoWatermarking Based on Adaptive Region Selection and\n  Channel Reference",
        "authors": [
            "Qinwei Chang",
            "Leichao Huang",
            "Shaoteng Liu",
            "Hualuo Liu",
            "Tianshu Yang",
            "Yexin Wang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Digital watermarking technology has a wide range of applications in video\ndistribution and copyright protection due to its excellent invisibility and\nconvenient traceability. This paper proposes a robust blind watermarking\nalgorithm using adaptive region selection and channel reference. By designing a\ncombinatorial selection algorithm using texture information and feature points,\nthe method realizes automatically selecting stable blocks which can avoid being\ndestroyed during video encoding and complex attacks. In addition, considering\nhuman's insensitivity to some specific color components, a channel-referenced\nwatermark embedding method is designed for less impact on video quality.\nMoreover, compared with other methods' embedding watermark only at low\nfrequencies, our method tends to modify low-frequency coefficients close to mid\nfrequencies, further ensuring stable retention of the watermark information in\nthe video encoding process. Experimental results show that the proposed method\nachieves excellent video quality and high robustness against geometric attacks,\ncompression, transcoding and camcorder recordings attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.13206v1"
    },
    {
        "title": "Explaining Hierarchical Features in Dynamic Point Cloud Processing",
        "authors": [
            "Pedro Gomes",
            "Silvia Rossi",
            "Laura Toni"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  This paper aims at bringing some light and understanding to the field of deep\nlearning for dynamic point cloud processing. Specifically, we focus on the\nhierarchical features learning aspect, with the ultimate goal of understanding\nwhich features are learned at the different stages of the process and what\ntheir meaning is. Last, we bring clarity on how hierarchical components of the\nnetwork affect the learned features and their importance for a successful\nlearning model. This study is conducted for point cloud prediction tasks,\nuseful for predicting coding applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.15557v1"
    },
    {
        "title": "Social VR and multi-party holographic communications: Opportunities,\n  Challenges and Impact in the Education and Training Sectors",
        "authors": [
            "Mario Montagud",
            "Gianluca Cernigliaro",
            "Miguel Arevalillo-Herráez",
            "Miguel García-Pineda",
            "Jaume Segura-Garcia",
            "Sergi Fernández"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Technological advances can bring many benefits to our daily lives, and this\nincludes the education and training sectors. In the last years, online\neducation, teaching and training models are becoming increasingly adopted, in\npart influenced by major circumstances like the pandemic. The use of\nvideoconferencing tools in such sectors has become fundamental, but recent\nresearch has shown their multiple limitations in terms of relevant aspects,\nlike comfort, interaction quality, situational awareness, (co-)presence, etc.\nThis study elaborates on a new communication, interaction and collaboration\nmedium that becomes a promising candidate to overcome such limitations, by\nadopting immersive technologies: Social Virtual Reality (VR). First, this\narticle provides a comprehensive review of studies having provided initial\nevidence on (potential) benefits provided by Social VR in relevant use cases\nrelated to education, such as online classes, training and co-design\nactivities, virtual conferences and interactive visits to virtual spaces, many\nof them including comparisons with classical tools like 2D conferencing.\nLikewise, the potential benefits of integrating realistic and volumetric users'\nrepresentations to enable multi-party holographic communications in Social VR\nis also discussed. Next, this article identifies and elaborates on key\nlimitations of existing studies in this field, including both technological and\nmethodological aspects. Finally, it discusses key remaining challenges to be\naddressed to fully exploit the potential of Social VR in the education sector.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.00330v1"
    },
    {
        "title": "A high accuracy and low complexity quality control method for image\n  compression",
        "authors": [
            "Xiao Yan",
            "Zhangxin Gong",
            "Wenqiang Wang",
            "Xiaoyang Zeng",
            "Yibo Fan"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  For large-scale still image coding tasks, the processing platform needs to\nensure that the coded images meet the quality requirement. Therefore, the\nquality control algorithms that generate adaptive QP towards a target quality\nlevel for image coding are of significant research value. However, the existing\nquality control methods are limited by low accuracy, excessive computational\ncost, or temporal information dependence. In this paper, we propose a concise\n{\\lambda} domain linear distortion model and an accurate model parameters\nestimation method based on the original data. Since the model parameters are\nobtained from the original data, the proposed method is decoupled from the RDO\nprocess and can be applied to different image encoders. Experiments show that\nthe proposed quality control algorithm achieves the highest control accuracy\nand the lowest delay in the literature at the same time. The application of\nAlibaba's e-commerce platform also shows that the proposed algorithm can\nsignificantly reduce the overall bitrate while greatly reducing the bad case\nratio.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.00821v1"
    },
    {
        "title": "A Conditional-Probability-Distribution Model for Bandwidth Estimation\n  with Application in Live Video Streaming",
        "authors": [
            "Weijia Zheng"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Experience of live video streaming can be improved if the video uploader has\nmore accurate knowledge about the future available bandwidth. Because with such\nknowledge, one is able to know what sizes should he encode the frames to be in\nan ever-changing network. Researchers have developed some algorithms to predict\nthroughputs in the literature, from where some are simple hence practical.\nHowever, limitation remains as most current bandwidth prediction methods are\npredicting a value, or a point estimate, of future bandwidth. Because in many\npractical scenarios, it is desirable to control the performance to some\ntargets, e.g., video delivery rate over a given target percentage, which cannot\nbe easily achieved via most current methods.\n  In this work, we propose the use of probability distribution to model future\nbandwidth. Specifically, we model future bandwidth using past data transfer\nmeasurements and then derive a probability model for use in the application.\nThis changes the selection of parameters in application into a probabilistic\nmanner such that given target performance can be achieved in the long run.\nInside our model, we use the conditional-probability method to correlate past\nand future bandwidth and hence further improve the estimating performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.01652v1"
    },
    {
        "title": "Improving Visual-Semantic Embedding with Adaptive Pooling and\n  Optimization Objective",
        "authors": [
            "Zijian Zhang",
            "Chang Shu",
            "Ya Xiao",
            "Yuan Shen",
            "Di Zhu",
            "Jing Xiao",
            "Youxin Chen",
            "Jey Han Lau",
            "Qian Zhang",
            "Zheng Lu"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Visual-Semantic Embedding (VSE) aims to learn an embedding space where\nrelated visual and semantic instances are close to each other. Recent VSE\nmodels tend to design complex structures to pool visual and semantic features\ninto fixed-length vectors and use hard triplet loss for optimization. However,\nwe find that: (1) combining simple pooling methods is no worse than these\nsophisticated methods; and (2) only considering the most\ndifficult-to-distinguish negative sample leads to slow convergence and poor\nRecall@K improvement. To this end, we propose an adaptive pooling strategy that\nallows the model to learn how to aggregate features through a combination of\nsimple pooling methods. We also introduce a strategy to dynamically select a\ngroup of negative samples to make the optimization converge faster and perform\nbetter. Experimental results on Flickr30K and MS-COCO demonstrate that a\nstandard VSE using our pooling and optimization strategies outperforms current\nstate-of-the-art systems (at least 1.0% on the metrics of recall) in\nimage-to-text and text-to-image retrieval. Source code of our experiments is\navailable at https://github.com/96-Zachary/vse_2ad.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.02206v1"
    },
    {
        "title": "UAV Placement for Real-time Video Acquisition: A Tradeoff between\n  Resolution and Delay",
        "authors": [
            "Tang Xiao-Wei",
            "Huang Xin-Lin Huang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Recently, UAVs endowed with high mobility, low cost, and remote control have\npromoted the development of UAV-assisted real-time video/image acquisition\napplications, which have a high demand for both transmission rate and image\nresolution. However, in conventional vertical photography model, the UAV should\nfly to the top of ground targets (GTs) to capture images, thus enlarge the\ntransmission delay. In this paper, we propose an oblique photography model,\nwhich allows the UAV to capture images of GTs from a far distance while still\nsatisfying the predetermined resolution requirement. Based on the proposed\noblique photography model, we further study the UAV placement problem in the\ncellular-connected UAV-assisted image acquisition system, which aims at\nminimizing the data transmission delay under the condition of satisfying the\npredetermined image resolution requirement. Firstly, the proposed scheme is\nfirst formulated as an intractable non-convex optimization problem. Then, the\noriginal problem is simplified to obtain a tractable suboptimal solution with\nthe help of the block coordinate descent and the successive convex\napproximation techniques. Finally, the numerical results are presented to show\nthe effectiveness of the proposed scheme. The numerical results have shown that\nthe proposed scheme can largely save the transmission time as compared to the\nconventional vertical photography model.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.04677v1"
    },
    {
        "title": "Multi-Player Immersive Communications and Interactions in Metaverse:\n  Challenges, Architecture, and Future Directions",
        "authors": [
            "Yakun Huang",
            "Xiuquan Qiao",
            "Haowen Wang",
            "Xiang Su",
            "Schahram Dustdar",
            "Ping Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The metaverse has awakened users' expectations of an immersive interaction\nthat fuses the virtual digital world and the physical world across space and\ntime. However, the metaverse is still in its infancy, typically expanding\nmulti-player applications (e.g., multi-player games) to implement a prototype\nwith the help of 5G/Beyond 5G, Artificial Intelligence, digital twin, and other\nenabling technologies. This article reviews the characteristics, key enabling\ntechnologies, and driving applications of the state-of-the-art metaverse. We\nfocus on the immersive interactions perspective of the metaverse from the\ntasks, inputs, and feedback across the users, digital world, and physical world\nand reveal the key challenges. Afterwards, we present a multi-player\ninteraction prototype platform based on a cloud-edge-device collaborative\nframework. Also, we evaluate it with centralized and device-to-device (D2D)\napproaches to verify the efficiency and flexibility of interactions. Finally,\nwe point out future research approaches and discuss potential solutions to\nenable more stable and higher quality multi-player interactions for metaverse\nservices.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.06802v1"
    },
    {
        "title": "ISCom: Interest-aware Semantic Communication Scheme for Point Cloud\n  Video Streaming",
        "authors": [
            "Yakun Huang",
            "Boyuan Bai",
            "Yuanwei Zhu",
            "Xiuquan Qiao",
            "Xiang Su",
            "Ping Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The provisioning of immersive point cloud video (PCV) streaming on pervasive\nmobile devices is a cornerstone for enabling immersive communication and\ninteractions in the future 6G metaverse era. However, most streaming techniques\nare dedicated to efficient PCV compression and codec extending from traditional\n3-DoF video services. Some emerging AI-enabled approaches are still in their\ninfancy phase and are constrained by intensive computational and adaptive flow\ntechniques. In this paper, we present ISCom, an Interest-aware Semantic\nCommunication Scheme for PCV, consisting of a region-of-interest (ROI)\nselection module, a lightweight PCV streaming module, and an intelligent\nscheduler. First, we propose a two-stage efficient ROI selection method for\nproviding interest-aware PCV streaming, which significantly reduces the data\nvolume. Second, we design a lightweight PCV encoder-decoder network for\nresource-constrained devices, adapting to the heterogeneous computing\ncapabilities of terminals. Third, we train a deep reinforcement learning\n(DRL)-based scheduler to adapt an optimal encoder-decoder network for various\ndevices, considering the dynamic network environments and computing\ncapabilities of different devices. Extensive experiments show that ISCom\noutperforms baselines on mobile devices at least 10 FPS and up to 22 FPS.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.06808v1"
    },
    {
        "title": "Content-adaptive Encoder Preset Prediction for Adaptive Live Streaming",
        "authors": [
            "Vignesh V Menon",
            "Hadi Amirpour",
            "Prajit T Rajendran",
            "Mohammad Ghanbari",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In live streaming applications, a fixed set of bitrate-resolution pairs\n(known as bitrate ladder) is generally used to avoid additional pre-processing\nrun-time to analyze the complexity of every video content and determine the\noptimized bitrate ladder. Furthermore, live encoders use the fastest available\npreset for encoding to ensure the minimum possible latency in streaming. For\nlive encoders, it is expected that the encoding speed is equal to the video\nframerate. An optimized encoding preset may result in (i) increased Quality of\nExperience (QoE) and (ii) improved CPU utilization while encoding. In this\nlight, this paper introduces a Content-Adaptive encoder Preset prediction\nScheme (CAPS) for adaptive live video streaming applications. In this scheme,\nthe encoder preset is determined using Discrete Cosine Transform\n(DCT)-energy-based low-complexity spatial and temporal features for every video\nsegment, the number of CPU threads allocated for each encoding instance, and\nthe target encoding speed. Experimental results show that CAPS yields an\noverall quality improvement of 0.83 dB PSNR and 3.81 VMAF with the same\nbitrate, compared to the fastest preset encoding of the HTTP Live Streaming\n(HLS) bitrate ladder using x265 HEVC open-source encoder. This is achieved by\nmaintaining the desired encoding speed and reducing CPU idle time.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.10330v1"
    },
    {
        "title": "A computational analysis on the relationship between melodic originality\n  and thematic fame in classical music from the Romantic period",
        "authors": [
            "Hudson Griffith"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this work, the researcher presents a novel approach to calculating melodic\noriginality based on the research by Simonton (1994). This novel formula is\nthen applied to a dataset of 428 classical music pieces from the Romantic\nperiod to analyze the relationship between melodic originality and thematic\nfame.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.12201v1"
    },
    {
        "title": "Fast multi-encoding to reduce the cost of video streaming",
        "authors": [
            "Hadi Amirpour",
            "Vignesh V Menon",
            "Ekrem Çetinkaya",
            "Adithyan Ilangovan",
            "Christian Feldmann",
            "Martin Smole",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The growth in video Internet traffic and advancements in video attributes\nsuch as framerate, resolution, and bit-depth boost the demand to devise a\nlarge-scale, highly efficient video encoding environment. This is even more\nessential for Dynamic Adaptive Streaming over HTTP (DASH)-based content\nprovisioning as it requires encoding numerous representations of the same video\ncontent. High Efficiency Video Coding (HEVC) is one standard video codec that\nsignificantly improves encoding efficiency over its predecessor Advanced Video\nCoding (AVC). This improvement is achieved at the expense of significantly\nincreased time complexity, which is a challenge for content and service\nproviders. As various representations are the same video content encoded at\ndifferent bitrates or resolutions, the encoding analysis information from the\nalready encoded representations can be shared to accelerate the encoding of\nother representations. Several state-of-the-art schemes first encode a single\nrepresentation, called a reference representation. During this encoding, the\nencoder creates analysis metadata with information such as the slicetype\ndecisions, CU, PU, TU partitioning, and the HEVC bitstream itself. The\nremaining representations, called dependent representations, analyze the above\nmetadata and then reuse it to skip searching some partitioning, thus, reducing\nthe computational complexity. With the emergence of cloud-based encoding\nservices, video encoding is accelerated by utilizing an increased number of\nresources, i.e., with multi-core CPUs, multiple representations can be encoded\nin parallel. This paper presents an overview of a wide range of multi-encoding\nschemes with and without the support of machine learning approaches integrated\ninto the HEVC Test Model (HM) and x265, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.13890v1"
    },
    {
        "title": "Improving Audio Captioning Using Semantic Similarity Metrics",
        "authors": [
            "Rehana Mahfuz",
            "Yinyi Guo",
            "Erik Visser"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Audio captioning quality metrics which are typically borrowed from the\nmachine translation and image captioning areas measure the degree of overlap\nbetween predicted tokens and gold reference tokens. In this work, we consider a\nmetric measuring semantic similarities between predicted and reference captions\ninstead of measuring exact word overlap. We first evaluate its ability to\ncapture similarities among captions corresponding to the same audio file and\ncompare it to other established metrics. We then propose a fine-tuning method\nto directly optimize the metric by backpropagating through a sentence embedding\nextractor and audio captioning network. Such fine-tuning results in an\nimprovement in predicted captions as measured by both traditional metrics and\nthe proposed semantic similarity captioning metric.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.16470v2"
    },
    {
        "title": "HoloLens 2 Sensor Streaming",
        "authors": [
            "Juan C. Dibene",
            "Enrique Dunn"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  We present a HoloLens 2 server application for streaming device data via TCP\nin real time. The server can stream data from the four grayscale cameras, depth\nsensor, IMU, front RGB camera, microphone, head tracking, eye tracking, and\nhand tracking. Each sent data frame has a timestamp and, optionally, the\ninstantaneous pose of the device in 3D space. The server allows downloading\ndevice calibration data, such as camera intrinsics, and can be integrated into\nUnity projects as a plugin, with support for basic upstream capabilities. To\nachieve real time video streaming at full frame rate, we leverage the video\nencoding capabilities of the HoloLens 2. Finally, we present a Python library\nfor receiving and decoding the data, which includes utilities that facilitate\npassing the data to other libraries. The source code, Python demos, and\nprecompiled binaries are available at https://github.com/jdibenes/hl2ss.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.02648v1"
    },
    {
        "title": "High Capacity Reversible Data Hiding for Encrypted 3D Mesh Models Based\n  on Topology",
        "authors": [
            "Yun Tang",
            "Lulu Cheng",
            "Wanli Lyv",
            "Zhaoxia Yin"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Reversible data hiding in encrypted domain(RDH-ED) can not only protect the\nprivacy of 3D mesh models and embed additional data, but also recover original\nmodels and extract additional data losslessly. However, due to the insufficient\nuse of model topology, the existing methods have not achieved satisfactory\nresults in terms of embedding capacity. To further improve the capacity, a\nRDH-ED method is proposed based on the topology of the 3D mesh models, which\ndivides the vertices into two parts: embedding set and prediction set. And\nafter integer mapping, the embedding ability of the embedding set is calculated\nby the prediction set. It is then passed to the data hider for embedding\nadditional data. Finally, the additional data and the original models can be\nextracted and recovered respectively by the receiver with the correct keys.\nExperiments declare that compared with the existing methods, this method can\nobtain the highest embedding capacity.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.02801v1"
    },
    {
        "title": "Video-Music Retrieval:A Dual-Path Cross-Modal Network",
        "authors": [
            "Xin Gu",
            "Yinghua Shen",
            "Chaohui Lv"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  We propose a method to recommend background music for videos. Current work\nrarely considers the emotional information of music, which is essential for\nvideo music retrieval. To achieve this, we design two paths to process content\ninformation and emotional information between modal. Based on characteristics\nof video and music, we design various feature extraction schemes and common\nrepresentation spaces. More importantly, we propose a way to combine content\ninformation with emotional information. Additionally, we make improvements to\nthe classical metric loss to be more suited to this task. Experiments show that\nthis dual path video music retrieval network can effectively merge information.\nCompare with existing methods, the retrieval task evaluation index: increasing\nRecall@1 by 3.94 and Recall@25 by 16.36.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.08878v1"
    },
    {
        "title": "Brain PET Synthesis from MRI Using Joint Probability Distribution of\n  Diffusion Model at Ultrahigh Fields",
        "authors": [
            "Taofeng Xie",
            "Chentao Cao",
            "Zhuoxu Cui",
            "Fanshi Li",
            "Zidong Wei",
            "Yanjie Zhu",
            "Ye Li",
            "Dong Liang",
            "Qiyu Jin",
            "Guoqing Chen",
            "Haifeng Wang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  MRI and PET are important modalities and can provide complementary\ninformation for the diagnosis of brain diseases because MRI can provide\nstructural information of brain and PET can obtain functional information of\nbrain. However, PET is usually missing. Especially, simultaneous PET and MRI\nimaging at ultrahigh field is not achievable in the current. Thus, synthetic\nPET using MRI at ultrahigh field is essential. In this paper, we synthetic PET\nusing MRI as a guide by joint probability distribution of diffusion model\n(JPDDM). Meanwhile, We utilized our model in Ultrahigh Fields.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.08901v2"
    },
    {
        "title": "Progressive Hologram Generation Based on Object Saliency",
        "authors": [
            "Shima Rafiei",
            "Shahram Shirani"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Computer-generated hologram (CGH) is promised to realize the next generation\nof 3D visual media with life-changing applications. However, one of the\nessential obstacles to this technology is the time-consuming hologram\ncomputation. Thus, facilitating the computation of the generated hologram is of\nsignificant importance in this area. We propose a progressive hologram\ngeneration based on object saliency using discrete wavelet transform. In our\nmethod, the object is decomposed into 3 levels of resolution using wavelet\ntransform. Then, based on the saliency of the object, a progressive resolution\nhologram is generated. Our model generates a low resolution hologram for\nnon-salient areas of an object and a high-resolution hologram for salient\nareas, thus reducing CGH generation time. We applied our method to a number of\nobjects and show that salient parts are reconstructed with high quality while\nit helps the process to speed up. Finally, we compare the SSIM of the\nreconstructed objects.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.09938v1"
    },
    {
        "title": "FakeSV: A Multimodal Benchmark with Rich Social Context for Fake News\n  Detection on Short Video Platforms",
        "authors": [
            "Peng Qi",
            "Yuyan Bu",
            "Juan Cao",
            "Wei Ji",
            "Ruihao Shui",
            "Junbin Xiao",
            "Danding Wang",
            "Tat-Seng Chua"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Short video platforms have become an important channel for news sharing, but\nalso a new breeding ground for fake news. To mitigate this problem, research of\nfake news video detection has recently received a lot of attention. Existing\nworks face two roadblocks: the scarcity of comprehensive and largescale\ndatasets and insufficient utilization of multimodal information. Therefore, in\nthis paper, we construct the largest Chinese short video dataset about fake\nnews named FakeSV, which includes news content, user comments, and publisher\nprofiles simultaneously. To understand the characteristics of fake news videos,\nwe conduct exploratory analysis of FakeSV from different perspectives.\nMoreover, we provide a new multimodal detection model named SV-FEND, which\nexploits the cross-modal correlations to select the most informative features\nand utilizes the social context information for detection. Extensive\nexperiments evaluate the superiority of the proposed method and provide\ndetailed comparisons of different methods and modalities for future works.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.10973v2"
    },
    {
        "title": "Robust-MSA: Understanding the Impact of Modality Noise on Multimodal\n  Sentiment Analysis",
        "authors": [
            "Huisheng Mao",
            "Baozheng Zhang",
            "Hua Xu",
            "Ziqi Yuan",
            "Yihe Liu"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Improving model robustness against potential modality noise, as an essential\nstep for adapting multimodal models to real-world applications, has received\nincreasing attention among researchers. For Multimodal Sentiment Analysis\n(MSA), there is also a debate on whether multimodal models are more effective\nagainst noisy features than unimodal ones. Stressing on intuitive illustration\nand in-depth analysis of these concerns, we present Robust-MSA, an interactive\nplatform that visualizes the impact of modality noise as well as simple defence\nmethods to help researchers know better about how their models perform with\nimperfect real-world data.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.13484v1"
    },
    {
        "title": "A Robust Image Steganographic Scheme against General Scaling Attacks",
        "authors": [
            "Qingliang Liu",
            "Jiangqun Ni",
            "Weizhe Zhang",
            "Xiangyang Luo",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Conventional covert image communication is assumed to transmit the message,\nin the securest way possible for a given payload, over lossless channels, and\nthe associated steganographic schemes are generally vulnerable to active\nattacks, e.g., JPEG re-compression, scaling, as seen on social networks.\nAlthough considerable progress has been made on robust steganography against\nJPEG re-compression, there exist few steganographic schemes capable of\nresisting to scaling attacks due to the tricky inverse interpolations involved\nin algorithm design. To tackle this issue, a framework for robust image\nsteganography resisting to scaling with general interpolations either in\nstandard form with fixed interpolation block, or pre-filtering based\nanti-aliasing implementation with variable block, is proposed in this paper.\nAnd the task of robust steganography can be formulated as one of constrained\ninteger programming aiming at perfectly recover the secret message from stego\nimage while minimizing the difference between cover and stego images. By\nintroducing a metric - the degree of pixel involvement (dPI) to identify the\nmodifiable pixels in cover image, the optimization problem above could be\neffectively solved using branch and bound algorithm (B\\&B). In addition, a\ncustomized distortion function for scaled stego images is adopted to further\nboost the security performance. Extensive experiments are carried out which\ndemonstrate that the proposed scheme could not only outperform the prior art in\nterms of security by a clear margin, but also be applicable to resisting the\nscaling attacks with various interpolation techniques at arbitrary scaling\nfactors (SFs).\n",
        "pdf_link": "http://arxiv.org/pdf/2212.02822v1"
    },
    {
        "title": "Technical Evaluation of HoloLens for Multimedia: A First Look",
        "authors": [
            "Yang Liu",
            "Haiwei Dong",
            "Longyu Zhang",
            "Abdulmotaleb El Saddik"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  A recently released cutting-edge AR device, Microsoft HoloLens, has attracted\nconsiderable attention with its advanced capabilities. In this article, we\nreport the design and execution of a series of experiments to quantitatively\nevaluate HoloLens' performance in head localization, real environment\nreconstruction, spatial mapping, hologram visualization, and speech\nrecognition.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.12907v1"
    },
    {
        "title": "Towards a QoE Model to Evaluate Holographic Augmented Reality Devices",
        "authors": [
            "Longyu Zhang",
            "Haiwei Dong",
            "Abdulmotaleb El Saddik"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Augmented reality (AR) technology is developing fast and provides users with\nnew ways to interact with the real-world surrounding environment. Although the\nperformance of holographic AR multimedia devices can be measured with\ntraditional quality-of-service parameters, a quality-of-experience (QoE) model\ncan better evaluate the device from the perspective of users. As there are\ncurrently no well-recognized models for measuring the QoE of a holographic AR\nmultimedia device, we present a QoE framework and model it with a fuzzy\ninference system to quantitatively evaluate the device.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.13842v1"
    },
    {
        "title": "Evaluating and Improving the Depth Accuracy of Kinect for Windows v2",
        "authors": [
            "Lin Yang",
            "Longyu Zhang",
            "Haiwei Dong",
            "Abdulhameed Alelaiwi",
            "Abdulmotaleb El Saddik"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Microsoft Kinect sensor has been widely used in many applications since the\nlaunch of its first version. Recently, Microsoft released a new version of\nKinect sensor with improved hardware. However, the accuracy assessment of the\nsensor remains to be answered. In this paper, we measure the depth accuracy of\nthe newly released Kinect v2 depth sensor, and obtain a cone model to\nillustrate its accuracy distribution. We then evaluate the variance of the\ncaptured depth values by depth entropy. In addition, we propose a trilateration\nmethod to improve the depth accuracy with multiple Kinects simultaneously. The\nexperimental results are provided to ascertain the proposed model and method.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.13844v1"
    },
    {
        "title": "Robust Digital Watermarking Method Based on Adaptive Feature Area\n  Extraction and Local Histogram Shifting",
        "authors": [
            "Zi-yu Jiang",
            "Chi-Man Pun",
            "Xiao-Chen Yuan",
            "Tong Liu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  A new local watermarking method based on histogram shifting has been proposed\nin this paper to deal with various signal processing attacks (e.g. median\nfiltering, JPEG compression and Gaussian noise addition) and geometric attacks\n(e.g. rotation, scaling and cropping). A feature detector is used to select\nlocal areas for embedding. Then stationary wavelet transform (SWT) is applied\non each local area for denoising by setting the corresponding diagonal\ncoefficients to zero. With the implementation of histogram shifting, the\nwatermark is embedded into denoised local areas. Meanwhile, a secret key is\nused in the embedding process which ensures the security that the watermark\ncannot be easily hacked. After the embedding process, the SWT diagonal\ncoefficients are used to reconstruct the watermarked image. With the proposed\nwatermarking method, we can achieve higher image quality and less bit error\nrate (BER) in the decoding process even after some attacks. Compared with\nglobal watermarking methods, the proposed watermarking scheme based on local\nhistogram shifting has the advantages of higher security and larger capacity.\nThe experimental results show the better image quality as well as lower BER\ncompared with the state-of-art watermarking methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.03837v1"
    },
    {
        "title": "Computational Creativity: Compose the Music for a Movie using only its\n  Automatically Extracted Brightness Curve",
        "authors": [
            "Felipe Ariani",
            "Marcelo Caetano",
            "Javier Elipe Gimeno",
            "Ivan Magrin-Chagnolleau"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Since its conception, the computer has found applications to accompany human\ncreativity. Today, the debate about computers and creativity involves several\nchallenges, such as understanding human creativity, modeling the creative\nprocess, and programming the computer to exhibit behavior that appears to be\ncreative to some extent. In this paper, we are interested in how the computer\ncan be used as a tool to promote creativity in a musical composition. We\nautomatically extracted the brightness curve from a silent movie and then used\nit to compose a piece of music to accompany the movie. We extracted several\nparameters from the brightness curve, and applied compositional rules from\nthese parameters to write the instrumental music for the film. The final\ncomposition has a synchronicity and aesthetic fit with the film that are\nsurprising. This compositional process also allowed for a degree of aesthetic\nfreedom that would otherwise have been impossible.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.09857v1"
    },
    {
        "title": "Registered Report : Perception of Other's Musical Preferences Based on\n  Their Personal Values",
        "authors": [
            "Sandy Manolios",
            "Catholijn M. Jonker",
            "Cynthia C. S. Liem"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The present work is part of a research line seeking to uncover the mysteries\nof what lies behind people's musical preferences in order to provide better\nmusic recommendations. More specifically, it takes the angle of personal\nvalues. Personal values are what we as people strive for, and are a popular\ntool in marketing research to understand customer preferences for certain types\nof product. Therefore, it makes sense to explore their usefulness in the music\ndomain. Based on a previous qualitative work using the Means-End theory, we\ndesigned a survey in an attempt to more quantitatively approach the\nrelationship between personal values and musical preferences. We support our\napproach with a simulation study as a tool to improve the experimental\nprocedure and decisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.10088v1"
    },
    {
        "title": "Smaller Is Bigger: Rethinking the Embedding Rate of Deep Hiding",
        "authors": [
            "Han Li",
            "Hangcheng Liu",
            "Shangwei Guo",
            "Mingliang Zhou",
            "Ning Wang",
            "Tao Xiang",
            "Tianwei Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Deep hiding, concealing secret information using Deep Neural Networks (DNNs),\ncan significantly increase the embedding rate and improve the efficiency of\nsecret sharing. Existing works mainly force on designing DNNs with higher\nembedding rates or fancy functionalities. In this paper, we want to answer some\nfundamental questions: how to increase and what determines the embedding rate\nof deep hiding. To this end, we first propose a novel Local Deep Hiding (LDH)\nscheme that significantly increases the embedding rate by hiding large secret\nimages into small local regions of cover images. Our scheme consists of three\nDNNs: hiding, locating, and revealing. We use the hiding network to convert a\nsecret image in a small imperceptible compact secret code that is embedded into\na random local region of a cover image. The locating network assists the\nrevealing process by identifying the position of secret codes in the stego\nimage, while the revealing network recovers all full-size secret images from\nthese identified local regions. Our LDH achieves an extremely high embedding\nrate, i.e., $16\\times24$ bpp and exhibits superior robustness to common image\ndistortions. We also conduct comprehensive experiments to evaluate our scheme\nunder various system settings. We further quantitatively analyze the trade-off\nbetween the embedding rate and image quality with different image restoration\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.11918v1"
    },
    {
        "title": "Practical Analyses of How Common Social Media Platforms and Photo\n  Storage Services Handle Uploaded Images",
        "authors": [
            "Duc-Tien Dang-Nguyen",
            "Vegard Velle Sjøen",
            "Dinh-Hai Le",
            "Thien-Phu Dao",
            "Anh-Duy Tran",
            "Minh-Triet Tran"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The research done in this study has delved deeply into the changes made to\ndigital images that are uploaded to three of the major social media platforms\nand image storage services in today's society: Facebook, Flickr, and Google\nPhotos. In addition to providing up-to-date data on an ever-changing landscape\nof different social media networks' digital fingerprints, a deep analysis of\nthe social networks' filename conventions has resulted in two new approaches in\n(i) estimating the true upload date of Flickr photos, regardless of whether the\ndates have been changed by the user or not, and regardless of whether the image\nis available to the public or has been deleted from the platform; (ii)\nrevealing the photo ID of a photo uploaded to Facebook based solely on the file\nname of the photo.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.12133v1"
    },
    {
        "title": "Memory-augmented Contrastive Learning for Talking Head Generation",
        "authors": [
            "Jianrong Wang",
            "Yaxin Zhao",
            "Li Liu",
            "Hongkai Fan",
            "Tianyi Xu",
            "Qi Li",
            "Sen Li"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Given one reference facial image and a piece of speech as input, talking head\ngeneration aims to synthesize a realistic-looking talking head video. However,\ngenerating a lip-synchronized video with natural head movements is challenging.\nThe same speech clip can generate multiple possible lip and head movements,\nthat is, there is no one-to-one mapping relationship between them. To overcome\nthis problem, we propose a Speech Feature Extractor (SFE) based on\nmemory-augmented self-supervised contrastive learning, which introduces the\nmemory module to store multiple different speech mapping results. In addition,\nwe introduce the Mixed Density Networks (MDN) into the landmark regression task\nto generate multiple predicted facial landmarks. Extensive qualitative and\nquantitative experiments show that the quality of our facial animation is\nsignificantly superior to that of the state-of-the-art (SOTA). The code has\nbeen released at https://github.com/Yaxinzhao97/MACL.git.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.13469v1"
    },
    {
        "title": "Video Quality Assessment with Texture Information Fusion for Streaming\n  Applications",
        "authors": [
            "Vignesh V Menon",
            "Prajit T Rajendran",
            "Reza Farahani",
            "Klaus Schoeffmann",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The rise in video streaming applications has increased the demand for video\nquality assessment (VQA). In 2016, Netflix introduced Video Multi-Method\nAssessment Fusion (VMAF), a full reference VQA metric that strongly correlates\nwith perceptual quality, but its computation is time-intensive. We propose a\nDiscrete Cosine Transform (DCT)-energy-based VQA with texture information\nfusion (VQ-TIF) model for video streaming applications that determines the\nvisual quality of the reconstructed video compared to the original video.\nVQ-TIF extracts Structural Similarity (SSIM) and spatiotemporal features of the\nframes from the original and reconstructed videos and fuses them using a long\nshort-term memory (LSTM)-based model to estimate the visual quality.\nExperimental results show that VQ-TIF estimates the visual quality with a\nPearson Correlation Coefficient (PCC) of 0.96 and a Mean Absolute Error (MAE)\nof 2.71, on average, compared to the ground truth VMAF scores. Additionally,\nVQ-TIF estimates the visual quality at a rate of 9.14 times faster than the\nstate-of-the-art VMAF implementation, along with an 89.44 % reduction in energy\nconsumption, assuming an Ultra HD (2160p) display resolution.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14465v2"
    },
    {
        "title": "Building a Modal-balanced BlockChain with Semantic Reconstruction",
        "authors": [
            "Zhijie Tan",
            "Xiang Yuan",
            "Shengwei Meng",
            "Yakun Huang",
            "Weiping Li",
            "Zhonghai Wu",
            "Tong Mo"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The current large blockchain systems (BTC Lightning network, Ethereum, etc.)\nare generally facing the problems of low persistence rates and high storage\ncosts. Therefore, users tend to store single modal (textual) information on the\nexisting blockchain systems. Inspired by semantic communication algorithms,\nthis paper presents a new algorithm to solve the serious imbalance between\ntextual and visual modals on blockchains. After semantic sampling of the\noriginal visual image, the resulting semantic text will be stored on the chain,\nand the end users can reconstruct a semantically similar image using the\n\\textbf{R}elative \\textbf{O}ptimal \\textbf{S}emantic \\textbf{I}sotope\n\\textbf{S}election algorithm. Experiments on the DIV2K dataset show that the\nblockchain with our algorithm can achieve 430,000 times the storage capacity\nand 550,000 times the persistence rate for the original visual data with\nacceptable semantic information loss.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.02428v1"
    },
    {
        "title": "Confidence-based Event-centric Online Video Question Answering on a\n  Newly Constructed ATBS Dataset",
        "authors": [
            "Weikai Kong",
            "Shuhong Ye",
            "Chenglin Yao",
            "Jianfeng Ren"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Deep neural networks facilitate video question answering (VideoQA), but the\nreal-world applications on video streams such as CCTV and live cast place\nhigher demands on the solver. To address the challenges of VideoQA on long\nvideos of unknown length, we define a new set of problems called Online\nOpen-ended Video Question Answering (O^2VQA). It requires an online\nstate-updating mechanism for the solver to decide if the collected information\nis sufficient to conclude an answer. We then propose a Confidence-based\nEvent-centric Online Video Question Answering (CEO-VQA) model to solve this\nproblem. Furthermore, a dataset called Answer Target in Background Stream\n(ATBS) is constructed to evaluate this newly developed online VideoQA\napplication. Compared to the baseline VideoQA method that watches the whole\nvideo, the experimental results show that the proposed method achieves a\nsignificant performance gain.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.03105v2"
    },
    {
        "title": "The Multimodal Information based Speech Processing (MISP) 2022\n  Challenge: Audio-Visual Diarization and Recognition",
        "authors": [
            "Zhe Wang",
            "Shilong Wu",
            "Hang Chen",
            "Mao-Kui He",
            "Jun Du",
            "Chin-Hui Lee",
            "Jingdong Chen",
            "Shinji Watanabe",
            "Sabato Siniscalchi",
            "Odette Scharenborg",
            "Diyuan Liu",
            "Baocai Yin",
            "Jia Pan",
            "Jianqing Gao",
            "Cong Liu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The Multi-modal Information based Speech Processing (MISP) challenge aims to\nextend the application of signal processing technology in specific scenarios by\npromoting the research into wake-up words, speaker diarization, speech\nrecognition, and other technologies. The MISP2022 challenge has two tracks: 1)\naudio-visual speaker diarization (AVSD), aiming to solve ``who spoken when''\nusing both audio and visual data; 2) a novel audio-visual diarization and\nrecognition (AVDR) task that focuses on addressing ``who spoken what when''\nwith audio-visual speaker diarization results. Both tracks focus on the Chinese\nlanguage, and use far-field audio and video in real home-tv scenarios: 2-6\npeople communicating each other with TV noise in the background. This paper\nintroduces the dataset, track settings, and baselines of the MISP2022\nchallenge. Our analyses of experiments and examples indicate the good\nperformance of AVDR baseline system, and the potential difficulties in this\nchallenge due to, e.g., the far-field video quality, the presence of TV noise\nin the background, and the indistinguishable speakers.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.06326v1"
    },
    {
        "title": "Multimedia Distribution Process Tracking for Android and iOS",
        "authors": [
            "Yu-Min Jeon",
            "Won-Mu Heo",
            "Jong-Min Kim",
            "Kyounggon Kim"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The crime of illegally filming and distributing images or videos worldwide is\nincreasing day by day. With the increasing penetration rate of smartphones,\nthere has been a rise in crimes involving secretly taking pictures of people's\nbodies and distributing them through messengers. However, little research has\nbeen done on these related issue. The crime of distributing media using the\nworld's popular messengers, WhatsApp and Telegram, is continuously increasing.\nIt is also common to see criminals distributing illegal footage through various\nmessengers to avoid being caught in the investigation network. As these crimes\nincrease, there will continue to be a need for professional investigative\npersonnel, and the time required for criminal investigations will continue to\nincrease. In this paper, we propose a multimedia forensic method for tracking\nfootprints by checking the media information that changes when images and\nvideos shot with a smartphone are transmitted through instant messengers. We\nhave selected 11 of the world's most popular instant messengers and two secure\nmessengers. In addition, we selected the most widely used Android and iOS\noperating systems for smartphones. Through this study, we were able to confirm\nthat it is possible to trace footprints related to the distribution of instant\nmessengers by analyzing transmitted images and videos. Thus, it was possible to\ndetermine which messengers were used to distribute the video when it was\ntransmitted through multiple messengers.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.03848v1"
    },
    {
        "title": "ITportrait: Image-Text Coupled 3D Portrait Domain Adaptation",
        "authors": [
            "Xiangwen Deng",
            "Yufeng Wang",
            "Yuanhao Cai",
            "Jingxiang Sun",
            "Yebin Liu",
            "Haoqian Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Domain adaptation of 3D portraits has gained more and more attention.\nHowever, the transfer mechanism of existing methods is mainly based on vision\nor language, which ignores the potential of vision-language combined guidance.\nIn this paper, we propose an Image-Text multi-modal framework, namely Image and\nText portrait (ITportrait), for 3D portrait domain adaptation. ITportrait\nrelies on a two-stage alternating training strategy. In the first stage, we\nemploy a 3D Artistic Paired Transfer (APT) method for image-guided style\ntransfer. APT constructs paired photo-realistic portraits to obtain accurate\nartistic poses, which helps ITportrait to achieve high-quality 3D style\ntransfer. In the second stage, we propose a 3D Image-Text Embedding (ITE)\napproach in the CLIP space. ITE uses a threshold function to self-adaptively\ncontrol the optimization direction of images or texts in the CLIP space.\nComprehensive experiments prove that our ITportrait achieves state-of-the-art\n(SOTA) results and benefits downstream tasks. All source codes and pre-trained\nmodels will be released to the public.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.04364v5"
    },
    {
        "title": "Deep Reinforcement Learning with Importance Weighted A3C for QoE\n  enhancement in Video Delivery Services",
        "authors": [
            "Mandan Naresh",
            "Paresh Saxena",
            "Manik Gupta"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Adaptive bitrate (ABR) algorithms are used to adapt the video bitrate based\non the network conditions to improve the overall video quality of experience\n(QoE). Recently, reinforcement learning (RL) and asynchronous advantage\nactor-critic (A3C) methods have been used to generate adaptive bit rate\nalgorithms and they have been shown to improve the overall QoE as compared to\nfixed rule ABR algorithms. However, a common issue in the A3C methods is the\nlag between behaviour policy and target policy. As a result, the behaviour and\nthe target policies are no longer synchronized which results in suboptimal\nupdates. In this work, we present ALISA: An Actor-Learner Architecture with\nImportance Sampling for efficient learning in ABR algorithms. ALISA\nincorporates importance sampling weights to give more weightage to relevant\nexperience to address the lag issues with the existing A3C methods. We present\nthe design and implementation of ALISA, and compare its performance to\nstate-of-the-art video rate adaptation algorithms including vanilla A3C\nimplemented in the Pensieve framework and other fixed-rule schedulers like BB,\nBOLA, and RB. Our results show that ALISA improves average QoE by up to 25%-48%\nhigher average QoE than Pensieve, and even more when compared to fixed-rule\nschedulers.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.04527v1"
    },
    {
        "title": "An Optimal SVC Bitstream Schema for Viewport-dependent 360-degree Video\n  Streaming",
        "authors": [
            "Gang Shen",
            "Mingyang Ma",
            "Guangxin Xu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  To deliver ultra-high resolution 360-degree video (such as 8K, 12K, or even\nhigher) across the internet, viewport-dependent streaming becomes necessary to\nsave bandwidth. During viewport switches, clients and servers will instantly\nexchange coordination info and contents for the given viewports. However, those\nviewport switches pose a serious challenge for video encoding because the\ntemporal dependency between contents within changing viewports is\nunpredictable. In existing practices, it is commonly noted that GOP (Group of\nPictures) size in a bitstream intrinsically prohibits the reduction of the\nviewport switch latency, such as Motion-to-photon (MTP) latency, or\nmotion-to-high-quality (MTHQ) latency. In this paper, we presented a Scalable\nVideo Coding (SVC) based bitstream schema, which can structurally remove the\nimpacts of GOP in viewport-dependent streaming and provide instant viewport\nswitches within one-frame time (the best possible). In addition, combined with\ntiling, this new coding schema allows an efficient packing of the non-adjacent\nregions within a viewport of 360-degree video. Our experiments also show that\nthe overall encoding with this SVC-based approach is faster than with\nmulti-stream approaches. Compared with current 360-degree video streaming\nsolutions based on MPEG-I OMAF, our approach is superior in terms of viewport\nswitch latency, simplicity of viewport packing, and encoding performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.05654v1"
    },
    {
        "title": "Cross-domain Food Image-to-Recipe Retrieval by Weighted Adversarial\n  Learning",
        "authors": [
            "Bin Zhu",
            "Chong-Wah Ngo",
            "Jingjing Chen",
            "Wing-Kwong Chan"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Food image-to-recipe aims to learn an embedded space linking the rich\nsemantics in recipes with the visual content in food image for cross-modal\nretrieval. The existing research works carry out the learning of such space by\nassuming that all the image-recipe training example pairs belong to the same\ncuisine. As a result, despite the excellent performance reported in the\nliterature, such space is not transferable for retrieving recipes of different\ncuisine. In this paper, we aim to address this issue by cross-domain food\nimage-to-recipe retrieval, such that by leveraging abundant image-recipe pairs\nin source domain (one cuisine), the embedding space is generalizable to a\ntarget domain (the other cuisine) that does not have images to pair with\nrecipes for training. With the intuition that the importance of different\nsource samples should vary, this paper proposes two novel mechanisms for\ncross-domain food image-to-recipe retrieval, i.e., source data selector and\nweighted cross-modal adversarial learning. The former aims to select source\nsamples similar to the target data and filter out distinctive ones for\ntraining. The latter is capable to assign higher weights to the source samples\nmore similar to the target data and lower weights to suppress the distinctive\nones for both cross-modal and adversarial learning. The weights are computed\nfrom the recipe features extracted from a pre-trained source model. Experiments\non three different cuisines (Chuan, Yue and Washoku) demonstrate that the\nproposed method manages to achieve state-of-the-art performances in all the\ntransfers.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.07387v1"
    },
    {
        "title": "Evaluating Strong Idempotence of Image Codec",
        "authors": [
            "Qian Zhang",
            "Tongda Xu",
            "Yanghao Li",
            "Yan Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In this paper, we first propose the concept of strong idempotent codec based\non idempotent codec. The idempotence of codec refers to the stability of codec\nto re-compression. Similarly, we define the strong idempotence of codec as the\nstability of codec to multiple quality re-compression, which is an important\nfeature of codec in the context of cloud transcoding. We provide a detailed\nexample of strong idempotent codec with known source distribution. Further, we\nformalize a testing protocol to evaluate the strong idempotence of image codec.\nAnd finally, we evaluate the strong idempotence of current image codecs,\nincluding traditional codec and recent neural codec. Experimental results show\nthat current image codecs are not stable to multi-round re-compression with\ndifferent qualities, even if they are close to idempotent.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.08269v1"
    },
    {
        "title": "Transcoding Quality Prediction for Adaptive Video Streaming",
        "authors": [
            "Vignesh V Menon",
            "Reza Farahani",
            "Prajit T Rajendran",
            "Mohammed Ghanbari",
            "Hermann Hellwagner",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In recent years, video streaming applications have proliferated the demand\nfor Video Quality Assessment VQA). Reduced reference video quality assessment\n(RR-VQA) is a category of VQA where certain features (e.g., texture, edges) of\nthe original video are provided for quality assessment. It is a popular\nresearch area for various applications such as social media, online games, and\nvideo streaming. This paper introduces a reduced reference Transcoding Quality\nPrediction Model (TQPM) to determine the visual quality score of the video\npossibly transcoded in multiple stages. The quality is predicted using Discrete\nCosine Transform (DCT)-energy-based features of the video (i.e., the video's\nbrightness, spatial texture information, and temporal activity) and the target\nbitrate representation of each transcoding stage. To do that, the problem is\nformulated, and a Long Short-Term Memory (LSTM)-based quality prediction model\nis presented. Experimental results illustrate that, on average, TQPM yields\nPSNR, SSIM, and VMAF predictions with an R2 score of 0.83, 0.85, and 0.87,\nrespectively, and Mean Absolute Error (MAE) of 1.31 dB, 1.19 dB, and 3.01,\nrespectively, for single-stage transcoding. Furthermore, an R2 score of 0.84,\n0.86, and 0.91, respectively, and MAE of 1.32 dB, 1.33 dB, and 3.25,\nrespectively, are observed for a two-stage transcoding scenario. Moreover, the\naverage processing time of TQPM for 4s segments is 0.328s, making it a\npractical VQA method in online streaming applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.10234v1"
    },
    {
        "title": "Neuro-OSVETA: A Robust Watermarking of 3D Meshes",
        "authors": [
            "Bata Vasc",
            "Nithin Raveendran",
            "Bane Vasic"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Best and practical watermarking schemes for copyright protection of 3D meshes\nare required to be blind and robust to attacks and errors. In this paper, we\npresent the latest developments in 3D blind watermarking with a special\nemphasis on our Ordered Statistics Vertex Extraction and Tracing Algorithm\n(OSVETA) algorithm and its improvements. OSVETA is based on a combination of\nquantization index modulation (QIM) and error correction coding using novel\nways for judicial selection of mesh vertices which are stable under mesh\nsimplification, and the technique we propose in this paper offers a systematic\nmethod for vertex selection based on neural networks replacing a heuristic\napproach in the OSVETA. The Neuro-OSVETA enables a more precise mesh geometry\nestimation and better curvature and topological feature estimation. These\nenhancements result in a more accurate identification of stable vertices\nresulting in significant reduction of deletion probability.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.10348v1"
    },
    {
        "title": "Green Video Complexity Analysis for Efficient Encoding in Adaptive Video\n  Streaming",
        "authors": [
            "Vignesh V Menon",
            "Christian Feldmann",
            "Klaus Schoeffmann",
            "Mohammad Ghanbari",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  For adaptive streaming applications, low-complexity and accurate video\ncomplexity features are necessary to analyze the video content in real time,\nwhich ensures fast and compression-efficient video streaming without\ndisruptions. State-of-the-art video complexity features are Spatial Information\n(SI) and Temporal Information (TI) features which do not correlate well with\nthe encoding parameters in adaptive streaming applications. To this light,\nVideo Complexity Analyzer (VCA) was introduced, determining the features based\non Discrete Cosine Transform (DCT)-energy. This paper presents optimizations on\nVCA for faster and energy-efficient video complexity analysis. Experimental\nresults show that VCA v2.0, using eight CPU threads, Single Instruction\nMultiple Data (SIMD), and low-pass DCT optimization, determines seven\ncomplexity features of Ultra High Definition 8-bit videos with better accuracy\nat a speed of up to 292.68 fps and an energy consumption of 97.06% lower than\nthe reference SITI implementation.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.12384v1"
    },
    {
        "title": "Robust image steganography against lossy JPEG compression based on\n  embedding domain selection and adaptive error correction",
        "authors": [
            "Xiaolong Duan",
            "Bin Li",
            "Zhaoxia Yin",
            "Xinpeng Zhang",
            "Bin Luo"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Transmitting images for communication on social networks has become routine,\nwhich is helpful for covert communication. The traditional steganography\nalgorithm is unable to successfully convey secret information since the social\nnetwork channel will perform lossy operations on images, such as JPEG\ncompression. Previous studies tried to solve this problem by enhancing the\nrobustness or making the cover adapt to the channel processing. In this study,\nwe proposed a robust image steganography method against lossy JPEG compression\nbased on embedding domain selection and adaptive error correction. To improve\nanti-steganalysis performance, the embedding domain is selected adaptively. To\nincrease robustness and lessen the impact on anti-steganalysis performance, the\nerror correction capacity of the error correction code is adaptively adjusted\nto eliminate redundancy. The experimental results show that the proposed method\nachieves better anti-steganalysis and robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13297v1"
    },
    {
        "title": "Scene Graph Lossless Compression with Adaptive Prediction for Objects\n  and Relations",
        "authors": [
            "Yufeng Zhang",
            "Weiyao Lin",
            "Wenrui Dai",
            "Huabin Liu",
            "Hongkai Xiong"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The scene graph is a new data structure describing objects and their pairwise\nrelationship within image scenes. As the size of scene graph in vision\napplications grows, how to losslessly and efficiently store such data on disks\nor transmit over the network becomes an inevitable problem. However, the\ncompression of scene graph is seldom studied before because of the complicated\ndata structures and distributions. Existing solutions usually involve\ngeneral-purpose compressors or graph structure compression methods, which is\nweak at reducing redundancy for scene graph data. This paper introduces a new\nlossless compression framework with adaptive predictors for joint compression\nof objects and relations in scene graph data. The proposed framework consists\nof a unified prior extractor and specialized element predictors to adapt for\ndifferent data elements. Furthermore, to exploit the context information within\nand between graph elements, Graph Context Convolution is proposed to support\ndifferent graph context modeling schemes for different graph elements. Finally,\na learned distribution model is devised to predict numerical data under\ncomplicated conditional constraints. Experiments conducted on labeled or\ngenerated scene graphs proves the effectiveness of the proposed framework in\nscene graph lossless compression task.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13359v1"
    },
    {
        "title": "A New Technique of the Virtual Reality Visualization of Complex Volume\n  Images from the Computer Tomography and Magnetic Resonance Imaging",
        "authors": [
            "Iva Vasic",
            "Roberto Pierdicca",
            "Emanuele Frontoni",
            "Bata Vasic"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  This paper presents a new technique for the virtual reality (VR)\nvisu-alization of complex volume images obtained from computer tomography (CT)\nand Magnetic Resonance Imaging (MRI) by combining three-dimensional (3D) mesh\nprocessing and software coding within the gaming engine. The method operates on\nreal representations of human organs avoiding any structural ap-proximations of\nthe real physiological shape. In order to obtain realistic repre-sentation of\nthe mesh model, geometrical and topological corrections are per-formed on the\nmesh surface with preserving real shape and geometric structure. Using\nmathematical intervention on the 3D model and mesh triangulation the second\npart of our algorithm ensures an automatic construction of new two-dimensional\n(2D) shapes that represent vector slices along any user chosen di-rection. The\nfinal result of our algorithm is developed software application that allows to\nuser complete visual experience and perceptual exploration of real human organs\nthrough spatial manipulation of their 3D models. Thus our pro-posed method\nachieves a threefold effect: i) high definition VR representation of real\nmodels of human organs, ii) the real time generated slices of such a model\nalong any directions, and iii) almost unlimited amount of training data for\nmachine learning that is very useful in process of diagnosis. In addition, our\ndeveloped application also offers significant benefits to educational process\nby ensuring interactive features and quality perceptual user experience.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.00116v1"
    },
    {
        "title": "Just Noticeable Difference-aware Per-Scene Bitrate-laddering for\n  Adaptive Video Streaming",
        "authors": [
            "Vignesh V Menon",
            "Jingwen Zhu",
            "Prajit T Rajendran",
            "Hadi Amirpour",
            "Patrick Le Callet",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In video streaming applications, a fixed set of bitrate-resolution pairs\n(known as a bitrate ladder) is typically used during the entire streaming\nsession. However, an optimized bitrate ladder per scene may result in (i)\ndecreased storage or delivery costs or/and (ii) increased Quality of\nExperience. This paper introduces a Just Noticeable Difference (JND)-aware\nper-scene bitrate ladder prediction scheme (JASLA) for adaptive video-on-demand\nstreaming applications. JASLA predicts jointly optimized resolutions and\ncorresponding constant rate factors (CRFs) using spatial and temporal\ncomplexity features for a given set of target bitrates for every scene, which\nyields an efficient constrained Variable Bitrate encoding. Moreover,\nbitrate-resolution pairs that yield distortion lower than one JND are\neliminated. Experimental results show that, on average, JASLA yields bitrate\nsavings of 34.42% and 42.67% to maintain the same PSNR and VMAF, respectively,\ncompared to the reference HTTP Live Streaming (HLS) bitrate ladder Constant\nBitrate encoding using x265 HEVC encoder, where the maximum resolution of\nstreaming is Full HD (1080p). Moreover, a 54.34% average cumulative decrease in\nstorage space is observed.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.00225v1"
    },
    {
        "title": "Datasheet for Subjective and Objective Quality Assessment Datasets",
        "authors": [
            "Nabajeet Barman",
            "Yuriy Reznik",
            "Maria Martini"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Over the years, many subjective and objective quality assessment datasets\nhave been created and made available to the research community. However, there\nis no standard process for documenting the various aspects of the dataset, such\nas details about the source sequences, number of test subjects, test\nmethodology, encoding settings, etc. Such information is often of great\nimportance to the users of the dataset as it can help them get a quick\nunderstanding of the motivation and scope of the dataset. Without such a\ntemplate, it is left to each reader to collate the information from the\nrelevant publication or website, which is a tedious and time-consuming process.\nIn some cases, the absence of a template to guide the documentation process can\nresult in an unintentional omission of some important information.\n  This paper addresses this simple but significant gap by proposing a datasheet\ntemplate for documenting various aspects of subjective and objective quality\nassessment datasets for multimedia data. The contributions presented in this\nwork aim to simplify the documentation process for existing and new datasets\nand improve their reproducibility. The proposed datasheet template is available\non GitHub, along with a few sample datasheets of a few open-source audiovisual\nsubjective and objective datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.02142v1"
    },
    {
        "title": "A Subjective Dataset for Multi-Screen Video Streaming Applications",
        "authors": [
            "Nabajeet Barman",
            "Yuriy Reznik",
            "Maria G. Martini"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In modern-era video streaming systems, videos are streamed and displayed on a\nwide range of devices. Such devices vary from large-screen UHD and HDTVs to\nmedium-screen Desktop PCs and Laptops to smaller-screen devices such as mobile\nphones and tablets. It is well known that a video is perceived differently when\ndisplayed on different devices. The viewing experience for a particular video\non smaller screen devices such as smartphones and tablets, which have high\npixel density, will be different with respect to the case where the same video\nis played on a large screen device such as a TV or PC monitor. Being able to\nmodel such relative differences in perception effectively can help in the\ndesign of better quality metrics and in the design of more efficient and\noptimized encoding profiles, leading to lower storage, encoding, and\ntransmission costs. This paper presents a new, open-source dataset consisting\nof subjective ratings for various encoded video sequences of different\nresolutions and bitrates (quality) when viewed on three devices of varying\nscreen sizes: TV, Tablet, and Mobile. Along with the subjective scores, an\nevaluation of some of the most famous and commonly used open-source objective\nquality metrics is also presented. It is observed that the performance of the\nmetrics varies a lot across different device types, with the recently\nstandardized ITU-T P.1204.3 Model, on average, outperforming their\nfull-reference counterparts. The dataset consisting of the videos, along with\ntheir subjective and objective scores, is available freely on Github at\nhttps://github.com/NabajeetBarman/Multiscreen-Dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03138v2"
    },
    {
        "title": "NeRF-QA: Neural Radiance Fields Quality Assessment Database",
        "authors": [
            "Pedro Martin",
            "António Rodrigues",
            "João Ascenso",
            "Maria Paula Queluz"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  This short paper proposes a new database - NeRF-QA - containing 48 videos\nsynthesized with seven NeRF based methods, along with their perceived quality\nscores, resulting from subjective assessment tests; for the videos selection,\nboth real and synthetic, 360 degrees scenes were considered. This database will\nallow to evaluate the suitability, to NeRF based synthesized views, of existing\nobjective quality metrics and also the development of new quality metrics,\nspecific for this case.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03176v1"
    },
    {
        "title": "MOSAIC: Spatially-Multiplexed Edge AI Optimization over Multiple\n  Concurrent Video Sensing Streams",
        "authors": [
            "Ila Gokarn",
            "Hemanth Sabella",
            "Yigong Hu",
            "Tarek Abdelzaher",
            "Archan Misra"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Sustaining high fidelity and high throughput of perception tasks over vision\nsensor streams on edge devices remains a formidable challenge, especially given\nthe continuing increase in image sizes (e.g., generated by 4K cameras) and\ncomplexity of DNN models. One promising approach involves criticality-aware\nprocessing, where the computation is directed selectively to critical portions\nof individual image frames. We introduce MOSAIC, a novel system for such\ncriticality-aware concurrent processing of multiple vision sensing streams that\nprovides a multiplicative increase in the achievable throughput with negligible\nloss in perception fidelity. MOSAIC determines critical regions from images\nreceived from multiple vision sensors and spatially bin-packs these regions\nusing a novel multi-scale Mosaic Across Scales (MoS) tiling strategy into a\nsingle canvas frame, sized such that the edge device can retain sufficiently\nhigh processing throughput. Experimental studies using benchmark datasets for\ntwo tasks, Automatic License Plate Recognition and Drone-based Pedestrian\nDetection, show that MOSAIC, executing on a Jetson TX2 edge device, can provide\ndramatic gains in the throughput vs. fidelity tradeoff. For instance, for\ndrone-based pedestrian detection, for a batch size of 4, MOSAIC can pack input\nframes from 6 cameras to achieve (a) 4.75x higher throughput (23 FPS per\ncamera, cumulatively 138FPS) with less than 1% accuracy loss, compared to a\nFirst Come First Serve (FCFS) processing paradigm.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03222v1"
    },
    {
        "title": "ChinaOpen: A Dataset for Open-world Multimodal Learning",
        "authors": [
            "Aozhu Chen",
            "Ziyuan Wang",
            "Chengbo Dong",
            "Kaibin Tian",
            "Ruixiang Zhao",
            "Xun Liang",
            "Zhanhui Kang",
            "Xirong Li"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  This paper introduces ChinaOpen, a dataset sourced from Bilibili, a popular\nChinese video-sharing website, for open-world multimodal learning. While the\nstate-of-the-art multimodal learning networks have shown impressive performance\nin automated video annotation and cross-modal video retrieval, their training\nand evaluation are primarily conducted on YouTube videos with English text.\nTheir effectiveness on Chinese data remains to be verified. In order to support\nmultimodal learning in the new context, we construct ChinaOpen-50k, a webly\nannotated training set of 50k Bilibili videos associated with user-generated\ntitles and tags. Both text-based and content-based data cleaning are performed\nto remove low-quality videos in advance. For a multi-faceted evaluation, we\nbuild ChinaOpen-1k, a manually labeled test set of 1k videos. Each test video\nis accompanied with a manually checked user title and a manually written\ncaption. Besides, each video is manually tagged to describe objects / actions /\nscenes shown in the visual content. The original user tags are also manually\nchecked. Moreover, with all the Chinese text translated into English,\nChinaOpen-1k is also suited for evaluating models trained on English data. In\naddition to ChinaOpen, we propose Generative Video-to-text Transformer (GVT)\nfor Chinese video captioning. We conduct an extensive evaluation of the\nstate-of-the-art single-task / multi-task models on the new dataset, resulting\nin a number of novel findings and insights.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.05880v2"
    },
    {
        "title": "Mover: Mask and Recovery based Facial Part Consistency Aware Method for\n  Deepfake Video Detection",
        "authors": [
            "Juan Hu",
            "Xin Liao",
            "Difei Gao",
            "Satoshi Tsutsui",
            "Qian Wang",
            "Zheng Qin",
            "Mike Zheng Shou"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Deepfake techniques have been widely used for malicious purposes, prompting\nextensive research interest in developing Deepfake detection methods. Deepfake\nmanipulations typically involve tampering with facial parts, which can result\nin inconsistencies across different parts of the face. For instance, Deepfake\ntechniques may change smiling lips to an upset lip, while the eyes remain\nsmiling. Existing detection methods depend on specific indicators of forgery,\nwhich tend to disappear as the forgery patterns are improved. To address the\nlimitation, we propose Mover, a new Deepfake detection model that exploits\nunspecific facial part inconsistencies, which are inevitable weaknesses of\nDeepfake videos. Mover randomly masks regions of interest (ROIs) and recovers\nfaces to learn unspecific features, which makes it difficult for fake faces to\nbe recovered, while real faces can be easily recovered. Specifically, given a\nreal face image, we first pretrain a masked autoencoder to learn facial part\nconsistency by dividing faces into three parts and randomly masking ROIs, which\nare then recovered based on the unmasked facial parts. Furthermore, to maximize\nthe discrepancy between real and fake videos, we propose a novel model with\ndual networks that utilize the pretrained encoder and masked autoencoder,\nrespectively. 1) The pretrained encoder is finetuned for capturing the encoding\nof inconsistent information in the given video. 2) The pretrained masked\nautoencoder is utilized for mapping faces and distinguishing real and fake\nvideos. Our extensive experiments on standard benchmarks demonstrate that Mover\nis highly effective.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.05943v1"
    },
    {
        "title": "PPO-ABR: Proximal Policy Optimization based Deep Reinforcement Learning\n  for Adaptive BitRate streaming",
        "authors": [
            "Mandan Naresh",
            "Paresh Saxena",
            "Manik Gupta"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Providing a high Quality of Experience (QoE) for video streaming in 5G and\nbeyond 5G (B5G) networks is challenging due to the dynamic nature of the\nunderlying network conditions. Several Adaptive Bit Rate (ABR) algorithms have\nbeen developed to improve QoE, but most of them are designed based on fixed\nrules and unsuitable for a wide range of network conditions. Recently, Deep\nReinforcement Learning (DRL) based Asynchronous Advantage Actor-Critic (A3C)\nmethods have recently demonstrated promise in their ability to generalise to\ndiverse network conditions, but they still have limitations. One specific issue\nwith A3C methods is the lag between each actor's behavior policy and central\nlearner's target policy. Consequently, suboptimal updates emerge when the\nbehavior and target policies become out of synchronization. In this paper, we\naddress the problems faced by vanilla-A3C by integrating the on-policy-based\nmulti-agent DRL method into the existing video streaming framework.\nSpecifically, we propose a novel system for ABR generation - Proximal Policy\nOptimization-based DRL for Adaptive Bit Rate streaming (PPO-ABR). Our proposed\nmethod improves the overall video QoE by maximizing sample efficiency using a\nclipped probability ratio between the new and the old policies on multiple\nepochs of minibatch updates. The experiments on real network traces demonstrate\nthat PPO-ABR outperforms state-of-the-art methods for different QoE variants.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.08114v1"
    },
    {
        "title": "AMD: Autoregressive Motion Diffusion",
        "authors": [
            "Bo Han",
            "Hao Peng",
            "Minjing Dong",
            "Yi Ren",
            "Yixuan Shen",
            "Chang Xu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Human motion generation aims to produce plausible human motion sequences\naccording to various conditional inputs, such as text or audio. Despite the\nfeasibility of existing methods in generating motion based on short prompts and\nsimple motion patterns, they encounter difficulties when dealing with long\nprompts or complex motions. The challenges are two-fold: 1) the scarcity of\nhuman motion-captured data for long prompts and complex motions. 2) the high\ndiversity of human motions in the temporal domain and the substantial\ndivergence of distributions from conditional modalities, leading to a\nmany-to-many mapping problem when generating motion with complex and long\ntexts. In this work, we address these gaps by 1) elaborating the first dataset\npairing long textual descriptions and 3D complex motions (HumanLong3D), and 2)\nproposing an autoregressive motion diffusion model (AMD). Specifically, AMD\nintegrates the text prompt at the current timestep with the text prompt and\naction sequences at the previous timestep as conditional information to predict\nthe current action sequences in an iterative manner. Furthermore, we present\nits generalization for X-to-Motion with \"No Modality Left Behind\", enabling the\ngeneration of high-definition and high-fidelity human motions based on\nuser-defined modality input.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.09381v8"
    },
    {
        "title": "Self-Training Boosted Multi-Factor Matching Network for Composed Image\n  Retrieval",
        "authors": [
            "Haokun Wen",
            "Xuemeng Song",
            "Jianhua Yin",
            "Jianlong Wu",
            "Weili Guan",
            "Liqiang Nie"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The composed image retrieval (CIR) task aims to retrieve the desired target\nimage for a given multimodal query, i.e., a reference image with its\ncorresponding modification text. The key limitations encountered by existing\nefforts are two aspects: 1) ignoring the multi-faceted query-target matching\nfactors; 2) ignoring the potential unlabeled reference-target image pairs in\nexisting benchmark datasets. To address these two limitations is non-trivial\ndue to the following challenges: 1) how to effectively model the multi-faceted\nmatching factors in a latent way without direct supervision signals; 2) how to\nfully utilize the potential unlabeled reference-target image pairs to improve\nthe generalization ability of the CIR model. To address these challenges, in\nthis work, we first propose a muLtI-faceted Matching Network (LIMN), which\nconsists of three key modules: multi-grained image/text encoder, latent\nfactor-oriented feature aggregation, and query-target matching modeling.\nThereafter, we design an iterative dual self-training paradigm to further\nenhance the performance of LIMN by fully utilizing the potential unlabeled\nreference-target image pairs in a semi-supervised manner. Specifically, we\ndenote the iterative dual self-training paradigm enhanced LIMN as LIMN+.\nExtensive experiments on three real-world datasets, FashionIQ, Shoes, and\nBirds-to-Words, show that our proposed method significantly surpasses the\nstate-of-the-art baselines.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.09979v2"
    },
    {
        "title": "A dual watermaking scheme based on Sobolev type orthogonal moments for\n  document authentication",
        "authors": [
            "Alicia María Centurión-Fajardo",
            "Alberto Lastra",
            "Anier Soria-Lorente"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  A dual watermarking scheme based on Sobolev type orthogonal moments, Charlier\nand Meixner, is proposed based on different discrete measures. The existing\nrelation through the connection formulas allows to provide with structure and\nrecurrence relations, together with two difference equations satisfied by such\nfamilies. Weighted polynomials derived from them are being applied in an\nembedding and extraction watermarking algorithm, comparing the results obtained\nin imperceptibly and robustness tests with other families of polynomials.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.10112v1"
    },
    {
        "title": "Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative\n  Multimodal Prompt",
        "authors": [
            "Xiaocui Yang",
            "Shi Feng",
            "Daling Wang",
            "Sun Qi",
            "Wenfang Wu",
            "Yifei Zhang",
            "Pengfei Hong",
            "Soujanya Poria"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  We have witnessed the rapid proliferation of multimodal data on numerous\nsocial media platforms. Conventional studies typically require massive labeled\ndata to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA).\nHowever, collecting and annotating fine-grained multimodal data for MABSA is\ntough. To alleviate the above issue, we perform three MABSA-related tasks with\nquite a small number of labeled multimodal samples. We first build diverse and\ncomprehensive multimodal few-shot datasets according to the data distribution.\nTo capture the specific prompt for each aspect term in a few-shot scenario, we\npropose a novel Generative Multimodal Prompt (GMP) model for MABSA, which\nincludes the Multimodal Encoder module and the N-Stream Decoders module. We\nfurther introduce a subtask to predict the number of aspect terms in each\ninstance to construct the multimodal prompt. Extensive experiments on two\ndatasets demonstrate that our approach outperforms strong baselines on two\nMABSA-related tasks in the few-shot setting.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.10169v2"
    },
    {
        "title": "Social Context-aware GCN for Video Character Search via Scene-prior\n  Enhancement",
        "authors": [
            "Wenjun Peng",
            "Weidong He",
            "Derong Xu",
            "Tong Xu",
            "Chen Zhu",
            "Enhong Chen"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  With the increasing demand for intelligent services of online video\nplatforms, video character search task has attracted wide attention to support\ndownstream applications like fine-grained retrieval and summarization. However,\ntraditional solutions only focus on visual or coarse-grained social information\nand thus cannot perform well when facing complex scenes, such as changing\ncamera view or character posture. Along this line, we leverage social\ninformation and scene context as prior knowledge to solve the problem of\ncharacter search in complex scenes. Specifically, we propose a\nscene-prior-enhanced framework, named SoCoSearch. We first integrate multimodal\nclues for scene context to estimate the prior probability of social\nrelationships, and then capture characters' co-occurrence to generate an\nenhanced social context graph. Afterwards, we design a social context-aware GCN\nframework to achieve feature passing between characters to obtain robust\nrepresentation for the character search task. Extensive experiments have\nvalidated the effectiveness of SoCoSearch in various metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.12348v1"
    },
    {
        "title": "MDVSC -- Wireless Model Division Video Semantic Communication",
        "authors": [
            "Zhicheng Bao",
            "Haotai Liang",
            "Chen Dong",
            "Xiaodong Xu",
            "Geng Liu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In this paper, we propose a new wireless video communication scheme to\nachieve high-efficiency video transmission over noisy channels. It exploits the\nidea of model division multiple access (MDMA) and extracts common semantic\nfeatures across video frames. Besides, deep joint source-channel coding (JSCC)\nis applied to overcome the distortion caused by noisy channels. The proposed\nframework is collected under the name model division video semantic\ncommunication (MDVSC). In particular, temporal relative video frames are first\ntransformed into a latent space for computing complexity reduction and data\nredistribution. Accordingly, a novel entropy-based variable length coding is\ndeveloped further to compress semantic information under the communication\nbandwidth cost limitation. The whole MDVSC is an end-to-end learnable system.\nIt can be formulated as an optimization problem whose goal is to minimize\nend-to-end transmission distortion under restricted communication resources.\nAcross standard video source test sequences, test results show that the MDVSC\noutperforms traditional wireless video coding schemes generally under\nperceptual quality metrics and has the ability to control code length\nprecisely.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.15799v1"
    },
    {
        "title": "Pivotuner: automatic real-time pure intonation and microtonal modulation",
        "authors": [
            "Dmitri Volkov"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Pivotuner is a VST3/AU MIDI effect plugin that automatically tunes note data\nin an adaptive pure intonation, in real time. Where previously pure intonation\nwas out of reach for most musicians due to difficulty and impracticality,\nPivotuner enables it to be achieved easily and straightforwardly by using novel\nyet simple algorithms. This may lead to more widespread exploration of pure\nintonation for a larger and more diverse crowd of musicians! This paper\nincludes a review of prior systems for adaptive pure intonation systems,\nincluding Hermode Tuning/Kontakt Dynamic Pure Tuning and Just Intonation. The\npaper introduces the notion of an adaptive tuning center and how it serves as a\nflexible underlying concept for multiple tuning algorithms, as well as\nextensions to offer greater control for performers, including pitch and tuning\ncenter locking and resetting, and gradual interpolation between equal\ntemperament and pure intonation. The paper then showcases some pieces which use\nPivotuner effectively, then discusses areas for future exploration within\nPivotuner's feature set, and plans for future development.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.03873v1"
    },
    {
        "title": "Two Heads Are Better Than One: Improving Fake News Video Detection by\n  Correlating with Neighbors",
        "authors": [
            "Peng Qi",
            "Yuyang Zhao",
            "Yufeng Shen",
            "Wei Ji",
            "Juan Cao",
            "Tat-Seng Chua"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The prevalence of short video platforms has spawned a lot of fake news\nvideos, which have stronger propagation ability than textual fake news. Thus,\nautomatically detecting fake news videos has been an important countermeasure\nin practice. Previous works commonly verify each news video individually with\nmultimodal information. Nevertheless, news videos from different perspectives\nregarding the same event are commonly posted together, which contain\ncomplementary or contradictory information and thus can be used to evaluate\neach other mutually. To this end, we introduce a new and practical paradigm,\ni.e., cross-sample fake news video detection, and propose a novel framework,\nNeighbor-Enhanced fakE news video Detection (NEED), which integrates the\nneighborhood relationship of new videos belonging to the same event. NEED can\nbe readily combined with existing single-sample detectors and further enhance\ntheir performances with the proposed graph aggregation (GA) and debunking\nrectification (DR) modules. Specifically, given the feature representations\nobtained from single-sample detectors, GA aggregates the neighborhood\ninformation with the dynamic graph to enrich the features of independent\nsamples. After that, DR explicitly leverages the relationship between debunking\nvideos and fake news videos to refute the candidate videos via textual and\nvisual consistency. Extensive experiments on the public benchmark demonstrate\nthat NEED greatly improves the performance of both single-modal (up to 8.34% in\naccuracy) and multimodal (up to 4.97% in accuracy) base detectors. Codes are\navailable in https://github.com/ICTMCG/NEED.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.05241v1"
    },
    {
        "title": "360TripleView: 360-Degree Video View Management System Driven by\n  Convergence Value of Viewing Preferences",
        "authors": [
            "Qian Zhou",
            "Michael Zink",
            "Ramesh Sitaraman",
            "Klara Nahrstedt"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  360-degree video has become increasingly popular in content consumption.\nHowever, finding the viewing direction for important content within each frame\nposes a significant challenge. Existing approaches rely on either viewer input\nor algorithmic determination to select the viewing direction, but neither mode\nconsistently outperforms the other in terms of content-importance. In this\npaper, we propose 360TripleView, the first view management system for\n360-degree video that automatically infers and utilizes the better view mode\nfor each frame, ultimately providing viewers with higher content-importance\nviews. Through extensive experiments and a user study, we demonstrate that\n360TripleView achieves over 90\\% accuracy in inferring the better mode and\nsignificantly enhances content-importance compared to existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.08089v2"
    },
    {
        "title": "Towards Long Form Audio-visual Video Understanding",
        "authors": [
            "Wenxuan Hou",
            "Guangyao Li",
            "Yapeng Tian",
            "Di Hu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  We live in a world filled with never-ending streams of multimodal\ninformation. As a more natural recording of the real scenario, long form\naudio-visual videos are expected as an important bridge for better exploring\nand understanding the world. In this paper, we propose the multisensory\ntemporal event localization task in long form videos and strive to tackle the\nassociated challenges. To facilitate this study, we first collect a large-scale\nLong Form Audio-visual Video (LFAV) dataset with 5,175 videos and an average\nvideo length of 210 seconds. Each of the collected videos is elaborately\nannotated with diversified modality-aware events, in a long-range temporal\nsequence. We then propose an event-centric framework for localizing\nmultisensory events as well as understanding their relations in long form\nvideos. It includes three phases in different levels: snippet prediction phase\nto learn snippet features, event extraction phase to extract event-level\nfeatures, and event interaction phase to study event relations. Experiments\ndemonstrate that the proposed method, utilizing the new LFAV dataset, exhibits\nconsiderable effectiveness in localizing multiple modality-aware events within\nlong form videos. Project website: http://gewu-lab.github.io/LFAV/\n",
        "pdf_link": "http://arxiv.org/pdf/2306.09431v1"
    },
    {
        "title": "Relevance-Based Compression of Cataract Surgery Videos",
        "authors": [
            "Natalia Mathá",
            "Klaus Schoeffmann",
            "Konstantin Schekotihin",
            "Stephanie Sarny",
            "Doris Putzgruber-Adamitsch",
            "Yosuf El-Shabrawi"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In the last decade, the need for storing videos from cataract surgery has\nincreased significantly. Hospitals continue to improve their imaging and\nrecording devices (e.g., microscopes and cameras used in microscopic surgery,\nsuch as ophthalmology) to enhance their post-surgical processing efficiency.\nThe video recordings enable a lot of user-cases after the actual surgery, for\nexample, teaching, documentation, and forensics. However, videos recorded from\noperations are typically stored in the internal archive without any\ndomain-specific compression, leading to a massive storage space consumption. In\nthis work, we propose a relevance-based compression scheme for videos from\ncataract surgery, which is based on content specifics of particular cataract\nsurgery phases. We evaluate our compression scheme with three state-of-the-art\nvideo codecs, namely H.264/AVC, H.265/HEVC, and AV1, and ask medical experts to\nevaluate the visual quality of encoded videos. Our results show significant\nsavings, in particular up to 95.94% when using H.264/AVC, up to 98.71% when\nusing H.265/HEVC, and up to 98.82% when using AV1.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12829v1"
    },
    {
        "title": "Mulsemedia Communication Research Challenges for Metaverse in 6G\n  Wireless Systems",
        "authors": [
            "Ian F. Akyildiz",
            "Hongzhi Guo",
            "Rui Dai",
            "Wolfgang Gerstacker"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Although humans have five basic senses, sight, hearing, touch, smell, and\ntaste, most multimedia systems in current systems only capture two of them,\nnamely, sight and hearing. With the development of the metaverse and related\ntechnologies, there is a growing need for a more immersive media format that\nleverages all human senses. Multisensory media(Mulsemedia) that can stimulate\nmultiple senses will play a critical role in the near future. This paper\nprovides an overview of the history, background, use cases, existing research,\ndevices, and standards of mulsemedia. Emerging mulsemedia technologies such as\nExtended Reality (XR) and Holographic-Type Communication (HTC) are introduced.\nAdditionally, the challenges in mulsemedia research from the perspective of\nwireless communication and networking are discussed. The potential of 6G\nwireless systems to address these challenges is highlighted, and several\nresearch directions that can advance mulsemedia communications are identified.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.16359v3"
    },
    {
        "title": "All-intra rate control using low complexity video features for Versatile\n  Video Coding",
        "authors": [
            "Vignesh V Menon",
            "Anastasia Henkel",
            "Prajit T Rajendran",
            "Christian R. Helmrich",
            "Adam Wieckowski",
            "Benjamin Bross",
            "Christian Timmerer",
            "Detlev Marpe"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Versatile Video Coding (VVC) allows for large compression efficiency gains\nover its predecessor, High Efficiency Video Coding (HEVC). The added efficiency\ncomes at the cost of increased runtime complexity, especially for encoding. It\nis thus highly relevant to explore all available runtime reduction options.\nThis paper proposes a novel first pass for two-pass rate control in all-intra\nconfiguration, using low-complexity video analysis and a Random Forest\n(RF)-based machine learning model to derive the data required for driving the\nsecond pass. The proposed method is validated using VVenC, an open and\noptimized VVC encoder. Compared to the default two-pass rate control algorithm\nin VVenC, the proposed method achieves around 32% reduction in encoding time\nfor the preset faster, while on average only causing 2% BD-rate increase and\nachieving similar rate control accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.16786v1"
    },
    {
        "title": "INDCOR White Paper 0: Interactive Digital Narratives (IDNs) -- A\n  Solution to the Challenge of Representing Complex Issues",
        "authors": [
            "Hartmut Koenitz",
            "Jonathan Barbara",
            "Lissa Holloway-Attaway",
            "Frank Nack",
            "Mirjam Palosaari Eladhari",
            "Agnes Bakk"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Citizens everywhere have the right to be well-informed. Yet, with the high\ncomplexity of many contemporary issues, such as global warming and migration,\nour means of information need to mutually adapt. Narrative has always been at\nthe core of information exchange - regardless of whether our ancestors sat\naround a fire and exchanged stories, or whether we read an article in a\nnewspaper, or watched a TV news broadcast. Yet, the narrative formats of the\nnewspaper article, the news broadcast, the documentary, and the textbook are\nseverely limited when it comes to representing highly complex topics which may\ninclude several competing - and sometimes equally valid - perspectives. Such\ncomplexity contributes to a high level of uncertainty due to a multitude of\nfactors affecting an outcome. Fortunately, with Interactive Digital Narrative\n(IDN), there is a novel media format which can address these challenges. IDNs\ncan present several different perspectives in the same work, and give audiences\nthe ability to explore them at will through decision-making. After experiencing\nthe consequences of their decisions, the audience can replay to revisit and\nchange these decisions in order to consider their alternatives. IDN works\nenable deep personalization and the inclusion of live data. These capabilities\nmake IDN a 21st century democratic medium, empowering citizens through the\nunderstanding of complex issues. In this white paper, we discuss the challenge\nof representing complexity, describe the advantages offered by IDNs, and point\nout opportunities and strategies for deployment.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.17498v1"
    },
    {
        "title": "Immersive Media and Massive Twinning: Advancing Towards the Metaverse",
        "authors": [
            "Wassim Hamidouche",
            "Lina Bariah",
            "Merouane Debbah"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The advent of the Metaverse concept has further expedited the evolution of\nhaptic, tactile internet, and multimedia applications with their VR/AR/XR\nservices, and therefore, fully-immersive sensing is most likely to define the\nnext generation of wireless networks as a key to realize the speculative vision\nof the Metaverse. In this magazine, we articulate different types of media that\nwe envision will be communicated between the cyber and physical twins in the\nMetaverse. In particular, we explore the advantages grasped by exploiting each\nkind, and we point out critical challenges pertinent to 3D data processing,\ncoding, transporting, and rendering. We further shed light on the role of\nfuture wireless networks in delivering the anticipated quality of immersion\nthrough the reliable streaming of multimedia signals between the digital twin\nand its physical counterpart. Specifically, we explore emergent communication\nparadigms, including semantic, holographic, and goal-oriented communication,\nwhich we expect to realize energy and spectrally efficient Metaverse while\nensuring ultra-low latency.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.01522v2"
    },
    {
        "title": "Anableps: Adapting Bitrate for Real-Time Communication Using VBR-encoded\n  Video",
        "authors": [
            "Zicheng Zhang",
            "Hao Chen",
            "Xun Cao",
            "Zhan Ma"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Content providers increasingly replace traditional constant bitrate with\nvariable bitrate (VBR) encoding in real-time video communication systems for\nbetter video quality. However, VBR encoding often leads to large and frequent\nbitrate fluctuation, inevitably deteriorating the efficiency of existing\nadaptive bitrate (ABR) methods. To tackle it, we propose the Anableps to\nconsider the network dynamics and VBR-encoding-induced video bitrate\nfluctuations jointly for deploying the best ABR policy. With this aim, Anableps\nuses sender-side information from the past to predict the video bitrate range\nof upcoming frames. Such bitrate range is then combined with the receiver-side\nobservations to set the proper bitrate target for video encoding using a\nreinforcement-learning-based ABR model. As revealed by extensive experiments on\na real-world trace-driven testbed, our Anableps outperforms the GCC with\nsignificant improvement of quality of experience, e.g., 1.88x video quality,\n57% less bitrate consumption, 85% less stalling, and 74% shorter interaction\ndelay.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.03436v1"
    },
    {
        "title": "Semantic Communications System with Model Division Multiple Access and\n  Controllable Coding Rate for Point Cloud",
        "authors": [
            "Xiaoyi Liu",
            "Haotai Liang",
            "Zhicheng Bao",
            "Chen Dong",
            "Xiaodong Xu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Point cloud, as a 3D representation, is widely used in autonomous driving,\nvirtual reality (VR), and augmented reality (AR). However, traditional\ncommunication systems think that the point cloud's semantic information is\nirrelevant to communication, which hinders the efficient transmission of point\nclouds in the era of artificial intelligence (AI). This paper proposes a point\ncloud based semantic communication system (PCSC), which uses AI-based encoding\ntechniques to extract the semantic information of the point cloud and joint\nsource-channel coding (JSCC) technology to overcome the distortion caused by\nnoise channels and solve the \"cliff effect\" in traditional communication. In\naddition, the system realizes the controllable coding rate without fine-tuning\nthe network. The method analyzes the coded semantic vector's importance and\ndiscards semantically-unimportant information, thereby improving the\ntransmission efficiency. Besides, PCSC and the recently proposed non-orthogonal\nmodel division multiple access (MDMA) technology are combined to design a point\ncloud MDMA transmission system (M-PCSC) for multi-user transmission. Relevant\nexperimental results show that the proposed method outperforms the traditional\nmethod 10dB in the same channel bandwidth ratio under the PSNR D1 and PSNR D2\nmetrics. In terms of transmission, the proposed method can effectively solve\nthe \"cliff effect\" in the traditional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.06027v1"
    },
    {
        "title": "Text-oriented Modality Reinforcement Network for Multimodal Sentiment\n  Analysis from Unaligned Multimodal Sequences",
        "authors": [
            "Yuxuan Lei",
            "Dingkang Yang",
            "Mingcheng Li",
            "Shunli Wang",
            "Jiawei Chen",
            "Lihua Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Multimodal Sentiment Analysis (MSA) aims to mine sentiment information from\ntext, visual, and acoustic modalities. Previous works have focused on\nrepresentation learning and feature fusion strategies. However, most of these\nefforts ignored the disparity in the semantic richness of different modalities\nand treated each modality in the same manner. That may lead to strong\nmodalities being neglected and weak modalities being overvalued. Motivated by\nthese observations, we propose a Text-oriented Modality Reinforcement Network\n(TMRN), which focuses on the dominance of the text modality in MSA. More\nspecifically, we design a Text-Centered Cross-modal Attention (TCCA) module to\nmake full interaction for text/acoustic and text/visual pairs, and a Text-Gated\nSelf-Attention (TGSA) module to guide the self-reinforcement of the other two\nmodalities. Furthermore, we present an adaptive fusion mechanism to decide the\nproportion of different modalities involved in the fusion process. Finally, we\ncombine the feature matrices into vectors to get the final representation for\nthe downstream tasks. Experimental results show that our TMRN outperforms the\nstate-of-the-art methods on two MSA benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.13205v1"
    },
    {
        "title": "Boon: A Neural Search Engine for Cross-Modal Information Retrieval",
        "authors": [
            "Yan Gong",
            "Georgina Cosma"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Visual-Semantic Embedding (VSE) networks can help search engines better\nunderstand the meaning behind visual content and associate it with relevant\ntextual information, leading to more accurate search results. VSE networks can\nbe used in cross-modal search engines to embed image and textual descriptions\nin a shared space, enabling image-to-text and text-to-image retrieval tasks.\nHowever, the full potential of VSE networks for search engines has yet to be\nfully explored. This paper presents Boon, a novel cross-modal search engine\nthat combines two state-of-the-art networks: the GPT-3.5-turbo large language\nmodel, and the VSE network VITR (VIsion Transformers with Relation-focused\nlearning) to enhance the engine's capabilities in extracting and reasoning with\nregional relationships in images. VITR employs encoders from CLIP that were\ntrained with 400 million image-description pairs and it was fine-turned on the\nRefCOCOg dataset. Boon's neural-based components serve as its main\nfunctionalities: 1) a 'cross-modal search engine' that enables end-users to\nperform image-to-text and text-to-image retrieval. 2) a 'multi-lingual\nconversational AI' component that enables the end-user to converse about one or\nmore images selected by the end-user. Such a feature makes the search engine\naccessible to a wide audience, including those with visual impairments. 3) Boon\nis multi-lingual and can take queries and handle conversations about images in\nmultiple languages. Boon was implemented using the Django and PyTorch\nframeworks. The interface and capabilities of the Boon search engine are\ndemonstrated using the RefCOCOg dataset, and the engine's ability to search for\nmultimedia through the web is facilitated by Google's API.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14240v2"
    },
    {
        "title": "Neural-based Cross-modal Search and Retrieval of Artwork",
        "authors": [
            "Yan Gong",
            "Georgina Cosma",
            "Axel Finke"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Creating an intelligent search and retrieval system for artwork images,\nparticularly paintings, is crucial for documenting cultural heritage, fostering\nwider public engagement, and advancing artistic analysis and interpretation.\nVisual-Semantic Embedding (VSE) networks are deep learning models used for\ninformation retrieval, which learn joint representations of textual and visual\ndata, enabling 1) cross-modal search and retrieval tasks, such as image-to-text\nand text-to-image retrieval; and 2) relation-focused retrieval to capture\nentity relationships and provide more contextually relevant search results.\nAlthough VSE networks have played a significant role in cross-modal information\nretrieval, their application to painting datasets, such as ArtUK, remains\nunexplored. This paper introduces BoonArt, a VSE-based cross-modal search\nengine that allows users to search for images using textual queries, and to\nobtain textual descriptions along with the corresponding images when using\nimage queries. The performance of BoonArt was evaluated using the ArtUK\ndataset. Experimental evaluations revealed that BoonArt achieved 97% Recall@10\nfor image-to-text retrieval, and 97.4% Recall@10 for text-to-image Retrieval.\nBy bridging the gap between textual and visual modalities, BoonArt provides a\nmuch-improved search performance compared to traditional search engines, such\nas the one provided by the ArtUK website. BoonArt can be utilised to work with\nother artwork datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14244v1"
    },
    {
        "title": "Instance-Wise Adaptive Tuning and Caching for Vision-Language Models",
        "authors": [
            "Chunjin Yang",
            "Fanman Meng",
            "Shuai Chen",
            "Mingyu Liu",
            "Runtong Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Large-scale vision-language models (LVLMs) pretrained on massive image-text\npairs have achieved remarkable success in visual representations. However,\nexisting paradigms to transfer LVLMs to downstream tasks encounter two primary\nchallenges. Firstly, the text features remain fixed after being calculated and\ncannot be adjusted according to image features, which decreases the model's\nadaptability. Secondly, the model's output solely depends on the similarity\nbetween the text and image features, leading to excessive reliance on LVLMs. To\naddress these two challenges, we introduce a novel two-branch model named the\nInstance-Wise Adaptive Tuning and Caching (ATC). Specifically, one branch\nimplements our proposed ConditionNet, which guides image features to form an\nadaptive textual cache that adjusts based on image features, achieving\ninstance-wise inference and improving the model's adaptability. The other\nbranch introduces the similarities between images and incorporates a learnable\nvisual cache, designed to decouple new and previous knowledge, allowing the\nmodel to acquire new knowledge while preserving prior knowledge. The model's\noutput is jointly determined by the two branches, thus overcoming the\nlimitations of existing methods that rely solely on LVLMs. Additionally, our\nmethod requires limited computing resources to tune parameters, yet outperforms\nexisting methods on 11 benchmark datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15983v1"
    },
    {
        "title": "VATP360: Viewport Adaptive 360-Degree Video Streaming based on Tile\n  Priority",
        "authors": [
            "Zhiyu Pang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  360-degree video becomes increasingly popular among users. In the current\nnetwork bandwidth, serving high resolution 360 degree video to users is quite\ndifficult. Most of the work has been devoted to the prediction of user\nviewports or tile-based adaptive algorithms. However, it is difficult to\npredict user viewports more accurately using only information such as user's\nhistorical viewports or video saliency maps. In this paper, we propose a\nviewport adaptive 360-degree video streaming method based on tile priority\n(VATP360), which tries to balance between the performance and the overhead. The\nproposed VATP360 consists of three main modules: viewport prediction, tile\npriority classification and bitrate allocation. In the viewport prediction\nmodule, object motion trajectory and predicted user's region-of-interest (ROI)\nare used to achieve accurate prediction of the user's future viewport. Then,\nthe predicted viewport, along with the object motion trajectory, are fed into\nthe proposed tile priority classification algorithm to assign different\npriorities to tiles, which would reduce the computational complexity of the\nbitrate allocation module. Finally in the bitrate allocation stage, we\nadaptively assign bitrates to tiles of different priority by reinforcement\nlearning. Experimental results on publicly available datasets have demonstrated\nthe effectiveness of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15984v2"
    },
    {
        "title": "Context-Aware Talking-Head Video Editing",
        "authors": [
            "Songlin Yang",
            "Wei Wang",
            "Jun Ling",
            "Bo Peng",
            "Xu Tan",
            "Jing Dong"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Talking-head video editing aims to efficiently insert, delete, and substitute\nthe word of a pre-recorded video through a text transcript editor. The key\nchallenge for this task is obtaining an editing model that generates new\ntalking-head video clips which simultaneously have accurate lip synchronization\nand motion smoothness. Previous approaches, including 3DMM-based (3D Morphable\nModel) methods and NeRF-based (Neural Radiance Field) methods, are sub-optimal\nin that they either require minutes of source videos and days of training time\nor lack the disentangled control of verbal (e.g., lip motion) and non-verbal\n(e.g., head pose and expression) representations for video clip insertion. In\nthis work, we fully utilize the video context to design a novel framework for\ntalking-head video editing, which achieves efficiency, disentangled motion\ncontrol, and sequential smoothness. Specifically, we decompose this framework\nto motion prediction and motion-conditioned rendering: (1) We first design an\nanimation prediction module that efficiently obtains smooth and lip-sync motion\nsequences conditioned on the driven speech. This module adopts a\nnon-autoregressive network to obtain context prior and improve the prediction\nefficiency, and it learns a speech-animation mapping prior with better\ngeneralization to novel speech from a multi-identity video dataset. (2) We then\nintroduce a neural rendering module to synthesize the photo-realistic and\nfull-head video frames given the predicted motion sequence. This module adopts\na pre-trained head topology and uses only few frames for efficient fine-tuning\nto obtain a person-specific rendering model. Extensive experiments demonstrate\nthat our method efficiently achieves smoother editing results with higher image\nquality and lip accuracy using less data than previous methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.00462v3"
    },
    {
        "title": "Learning Causality-inspired Representation Consistency for Video Anomaly\n  Detection",
        "authors": [
            "Yang Liu",
            "Zhaoyang Xia",
            "Mengyang Zhao",
            "Donglai Wei",
            "Yuzheng Wang",
            "Liu Siao",
            "Bobo Ju",
            "Gaoyun Fang",
            "Jing Liu",
            "Liang Song"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Video anomaly detection is an essential yet challenging task in the\nmultimedia community, with promising applications in smart cities and secure\ncommunities. Existing methods attempt to learn abstract representations of\nregular events with statistical dependence to model the endogenous normality,\nwhich discriminates anomalies by measuring the deviations to the learned\ndistribution. However, conventional representation learning is only a crude\ndescription of video normality and lacks an exploration of its underlying\ncausality. The learned statistical dependence is unreliable for diverse regular\nevents in the real world and may cause high false alarms due to\novergeneralization. Inspired by causal representation learning, we think that\nthere exists a causal variable capable of adequately representing the general\npatterns of regular events in which anomalies will present significant\nvariations. Therefore, we design a causality-inspired representation\nconsistency (CRC) framework to implicitly learn the unobservable causal\nvariables of normality directly from available normal videos and detect\nabnormal events with the learned representation consistency. Extensive\nexperiments show that the causality-inspired normality is robust to regular\nevents with label-independent shifts, and the proposed CRC framework can\nquickly and accurately detect various complicated anomalies from real-world\nsurveillance videos.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01537v1"
    },
    {
        "title": "Cuing Without Sharing: A Federated Cued Speech Recognition Framework via\n  Mutual Knowledge Distillation",
        "authors": [
            "Yuxuan Zhang",
            "Lei Liu",
            "Li Liu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Cued Speech (CS) is a visual coding tool to encode spoken languages at the\nphonetic level, which combines lip-reading and hand gestures to effectively\nassist communication among people with hearing impairments. The Automatic CS\nRecognition (ACSR) task aims to recognize CS videos into linguistic texts,\nwhich involves both lips and hands as two distinct modalities conveying\ncomplementary information. However, the traditional centralized training\napproach poses potential privacy risks due to the use of facial and gesture\nvideos in CS data. To address this issue, we propose a new Federated Cued\nSpeech Recognition (FedCSR) framework to train an ACSR model over the\ndecentralized CS data without sharing private information. In particular, a\nmutual knowledge distillation method is proposed to maintain cross-modal\nsemantic consistency of the Non-IID CS data, which ensures learning a unified\nfeature space for both linguistic and visual information. On the server side, a\nglobally shared linguistic model is trained to capture the long-term\ndependencies in the text sentences, which is aligned with the visual\ninformation from the local clients via visual-to-linguistic distillation. On\nthe client side, the visual model of each client is trained with its own local\ndata, assisted by linguistic-to-visual distillation treating the linguistic\nmodel as the teacher. To the best of our knowledge, this is the first approach\nto consider the federated ACSR task for privacy protection. Experimental\nresults on the Chinese CS dataset with multiple cuers demonstrate that our\napproach outperforms both mainstream federated learning baselines and existing\ncentralized state-of-the-art ACSR methods, achieving 9.7% performance\nimprovement for character error rate (CER) and 15.0% for word error rate (WER).\n",
        "pdf_link": "http://arxiv.org/pdf/2308.03432v1"
    },
    {
        "title": "COPA: Efficient Vision-Language Pre-training Through Collaborative\n  Object- and Patch-Text Alignment",
        "authors": [
            "Chaoya Jiang",
            "Haiyang Xu",
            "Wei Ye",
            "Qinghao Ye",
            "Chenliang Li",
            "Ming Yan",
            "Bin Bi",
            "Shikun Zhang",
            "Ji Zhang",
            "Fei Huang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Vision-Language Pre-training (VLP) methods based on object detection enjoy\nthe rich knowledge of fine-grained object-text alignment but at the cost of\ncomputationally expensive inference. Recent Visual-Transformer (ViT)-based\napproaches circumvent this issue while struggling with long visual sequences\nwithout detailed cross-modal alignment information. This paper introduces a\nViT-based VLP technique that efficiently incorporates object information\nthrough a novel patch-text alignment mechanism. Specifically, we convert\nobject-level signals into patch-level ones and devise a Patch-Text Alignment\npre-training task (PTA) to learn a text-aware patch detector. By using\noff-the-shelf delicate object annotations in 5\\% training images, we jointly\ntrain PTA with other conventional VLP objectives in an end-to-end manner,\nbypassing the high computational cost of object detection and yielding an\neffective patch detector that accurately detects text-relevant patches, thus\nconsiderably reducing patch sequences and accelerating computation within the\nViT backbone. Our experiments on a variety of widely-used benchmarks reveal\nthat our method achieves a speedup of nearly 88\\% compared to prior VLP models\nwhile maintaining competitive or superior performance on downstream tasks with\nsimilar model size and data scale.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.03475v2"
    },
    {
        "title": "Mamba: Bringing Multi-Dimensional ABR to WebRTC",
        "authors": [
            "Yueheng Li",
            "Zicheng Zhang",
            "Hao Chen",
            "Zhan Ma"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Contemporary real-time video communication systems, such as WebRTC, use an\nadaptive bitrate (ABR) algorithm to assure high-quality and low-delay services,\ne.g., promptly adjusting video bitrate according to the instantaneous network\nbandwidth. However, target bitrate decisions in the network and bitrate control\nin the codec are typically incoordinated and simply ignoring the effect of\ninappropriate resolution and frame rate settings also leads to compromised\nresults in bitrate control, thus devastatingly deteriorating the quality of\nexperience (QoE). To tackle these challenges, Mamba, an end-to-end\nmulti-dimensional ABR algorithm is proposed, which utilizes multi-agent\nreinforcement learning (MARL) to maximize the user's QoE by adaptively and\ncollaboratively adjusting encoding factors including the quantization\nparameters (QP), resolution, and frame rate based on observed states such as\nnetwork conditions and video complexity information in a video conferencing\nsystem. We also introduce curriculum learning to improve the training\nefficiency of MARL. Both the in-lab and real-world evaluation results\ndemonstrate the remarkable efficacy of Mamba.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.03643v1"
    },
    {
        "title": "Collaborative Edge Caching: a Meta Reinforcement Learning Approach with\n  Edge Sampling",
        "authors": [
            "Bowei He",
            "Yinan Mao",
            "Shiji Zhou",
            "Chen Ma",
            "Zhi Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Current learning-based edge caching schemes usually suffer from dynamic\ncontent popularity, e.g., in the emerging short video platforms, users' request\npatterns shift significantly over time and across different edges. An intuitive\nsolution for a specific local edge cache is to collect more request histories\nfrom other edge caches. However, uniformly merging these request histories may\nnot perform satisfactorily due to heterogeneous content distributions on\ndifferent edges. To solve this problem, we propose a collaborative edge caching\nframework. First, we design a meta-learning-based collaborative strategy to\nguarantee that the local model can timely meet the continually changing content\npopularity. Then, we design an edge sampling method to select more \"valuable\"\nneighbor edges to participate in the local training. To evaluate the proposed\nframework, we conduct trace-driven experiments to demonstrate the effectiveness\nof our design: it improves the average cache hit rate by up to $10.12\\%$\n(normalized) compared with other baselines.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.04205v1"
    },
    {
        "title": "A Forensic Methodology for Detecting Image Manipulations",
        "authors": [
            "Jiwon Lee",
            "Seungjae Jeon",
            "Yunji Park",
            "Jaehyun Chung",
            "Doowon Jeong"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  By applying artificial intelligence to image editing technology, it has\nbecome possible to generate high-quality images with minimal traces of\nmanipulation. However, since these technologies can be misused for criminal\nactivities such as dissemination of false information, destruction of evidence,\nand denial of facts, it is crucial to implement strong countermeasures. In this\nstudy, image file and mobile forensic artifacts analysis were conducted for\ndetecting image manipulation. Image file analysis involves parsing the metadata\nof manipulated images (e.g., Exif, DQT, and Filename Signature) and comparing\nthem with a Reference DB to detect manipulation. The Reference DB is a database\nthat collects manipulation-related traces left in image metadata, which serves\nas a criterion for detecting image manipulation. In the mobile forensic\nartifacts analysis, packages related to image editing tools were extracted and\nanalyzed to aid the detection of image manipulation. The proposed methodology\novercomes the limitations of existing graphic feature-based analysis and\ncombines with image processing techniques, providing the advantage of reducing\nfalse positives. The research results demonstrate the significant role of such\nmethodology in digital forensic investigation and analysis. Additionally, We\nprovide the code for parsing image metadata and the Reference DB along with the\ndataset of manipulated images, aiming to contribute to related research.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.04723v1"
    },
    {
        "title": "Understanding User Behavior in Volumetric Video Watching: Dataset,\n  Analysis and Prediction",
        "authors": [
            "Kaiyuan Hu",
            "Haowen Yang",
            "Yili Jin",
            "Junhua Liu",
            "Yongting Chen",
            "Miao Zhang",
            "Fangxin Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Volumetric video emerges as a new attractive video paradigm in recent years\nsince it provides an immersive and interactive 3D viewing experience with six\ndegree-of-freedom (DoF). Unlike traditional 2D or panoramic videos, volumetric\nvideos require dense point clouds, voxels, meshes, or huge neural models to\ndepict volumetric scenes, which results in a prohibitively high bandwidth\nburden for video delivery. Users' behavior analysis, especially the viewport\nand gaze analysis, then plays a significant role in prioritizing the content\nstreaming within users' viewport and degrading the remaining content to\nmaximize user QoE with limited bandwidth. Although understanding user behavior\nis crucial, to the best of our best knowledge, there are no available 3D\nvolumetric video viewing datasets containing fine-grained user interactivity\nfeatures, not to mention further analysis and behavior prediction. In this\npaper, we for the first time release a volumetric video viewing behavior\ndataset, with a large scale, multiple dimensions, and diverse conditions. We\nconduct an in-depth analysis to understand user behaviors when viewing\nvolumetric videos. Interesting findings on user viewport, gaze, and motion\npreference related to different videos and users are revealed. We finally\ndesign a transformer-based viewport prediction model that fuses the features of\nboth gaze and motion, which is able to achieve high accuracy at various\nconditions. Our prediction model is expected to further benefit volumetric\nvideo streaming optimization. Our dataset, along with the corresponding\nvisualization tools is accessible at\nhttps://cuhksz-inml.github.io/user-behavior-in-vv-watching/\n",
        "pdf_link": "http://arxiv.org/pdf/2308.07578v2"
    },
    {
        "title": "EMID: An Emotional Aligned Dataset in Audio-Visual Modality",
        "authors": [
            "Jialing Zou",
            "Jiahao Mei",
            "Guangze Ye",
            "Tianyu Huai",
            "Qiwei Shen",
            "Daoguo Dong"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In this paper, we propose Emotionally paired Music and Image Dataset (EMID),\na novel dataset designed for the emotional matching of music and images, to\nfacilitate auditory-visual cross-modal tasks such as generation and retrieval.\nUnlike existing approaches that primarily focus on semantic correlations or\nroughly divided emotional relations, EMID emphasizes the significance of\nemotional consistency between music and images using an advanced 13-dimension\nemotional model. By incorporating emotional alignment into the dataset, it aims\nto establish pairs that closely align with human perceptual understanding,\nthereby raising the performance of auditory-visual cross-modal tasks. We also\ndesign a supplemental module named EMI-Adapter to optimize existing cross-modal\nalignment methods. To validate the effectiveness of the EMID, we conduct a\npsychological experiment, which has demonstrated that considering the emotional\nrelationship between the two modalities effectively improves the accuracy of\nmatching in abstract perspective. This research lays the foundation for future\ncross-modal research in domains such as psychotherapy and contributes to\nadvancing the understanding and utilization of emotions in cross-modal\nalignment. The EMID dataset is available at https://github.com/ecnu-aigc/EMID.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.07622v2"
    },
    {
        "title": "Bamboo: Boosting Training Efficiency for Real-Time Video Streaming via\n  Online Grouped Federated Transfer Learning",
        "authors": [
            "Qianyuan Zheng",
            "Hao Chen",
            "Zhan Ma"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Most of the learning-based algorithms for bitrate adaptation are limited to\noffline learning, which inevitably suffers from the simulation-to-reality gap.\nOnline learning can better adapt to dynamic real-time communication scenes but\nstill face the challenge of lengthy training convergence time. In this paper,\nwe propose a novel online grouped federated transfer learning framework named\nBamboo to accelerate training efficiency. The preliminary experiments validate\nthat our method remarkably improves online training efficiency by up to 302%\ncompared to other reinforcement learning algorithms in various network\nconditions while ensuring the quality of experience (QoE) of real-time video\ncommunication.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.09948v1"
    },
    {
        "title": "Aparecium: Revealing Secrets from Physical Photographs",
        "authors": [
            "Zhe Lei",
            "Jie Zhang",
            "Jingtao Li",
            "Weiming Zhang",
            "Nenghai Yu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Watermarking is a crucial tool for safeguarding copyrights and can serve as a\nmore aesthetically pleasing alternative to QR codes. In recent years,\nwatermarking methods based on deep learning have proved superior robustness\nagainst complex physical distortions than traditional watermarking methods.\nHowever, they have certain limitations that render them less effective in\npractice. For instance, current solutions necessitate physical photographs to\nbe rectangular for accurate localization, cannot handle physical bending or\nfolding, and require the hidden area to be completely captured at a close\ndistance and small angle. To overcome these challenges, we propose a novel deep\nwatermarking framework dubbed \\textit{Aparecium}. Specifically, we preprocess\nsecrets (i.e., watermarks) into a pattern and then embed it into the cover\nimage, which is symmetrical to the final decoding-then-extracting process. To\ncapture the watermarked region from complex physical scenarios, a locator is\nalso introduced. Besides, we adopt a three-stage training strategy for training\nconvergence. Extensive experiments demonstrate that \\textit{Aparecium} is not\nonly robust against different digital distortions, but also can resist various\nphysical distortions, such as screen-shooting and printing-shooting, even in\nsevere cases including different shapes, curvature, folding, incompleteness,\nlong distances, and big angles while maintaining high visual quality.\nFurthermore, some ablation studies are also conducted to verify our design.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.12141v1"
    },
    {
        "title": "Exploring Transferability of Multimodal Adversarial Samples for\n  Vision-Language Pre-training Models with Contrastive Learning",
        "authors": [
            "Youze Wang",
            "Wenbo Hu",
            "Yinpeng Dong",
            "Hanwang Zhang",
            "Hang Su",
            "Richang Hong"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The integration of visual and textual data in Vision-Language Pre-training\n(VLP) models is crucial for enhancing vision-language understanding. However,\nthe adversarial robustness of these models, especially in the alignment of\nimage-text features, has not yet been sufficiently explored. In this paper, we\nintroduce a novel gradient-based multimodal adversarial attack method,\nunderpinned by contrastive learning, to improve the transferability of\nmultimodal adversarial samples in VLP models. This method concurrently\ngenerates adversarial texts and images within imperceptive perturbation,\nemploying both image-text and intra-modal contrastive loss. We evaluate the\neffectiveness of our approach on image-text retrieval and visual entailment\ntasks, using publicly available datasets in a black-box setting. Extensive\nexperiments indicate a significant advancement over existing single-modal\ntransfer-based adversarial attack methods and current multimodal adversarial\nattack approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.12636v4"
    },
    {
        "title": "Parameter-Efficient Transfer Learning for Audio-Visual-Language Tasks",
        "authors": [
            "Hongye Liu",
            "Xianhai Xie",
            "Yang Gao",
            "Size Li",
            "Zhou YU"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The pretrain-then-finetune paradigm has been widely used in various unimodal\nand multimodal tasks. However, finetuning all the parameters of a pre-trained\nmodel becomes prohibitive as the model size grows exponentially. To address\nthis issue, the adapter mechanism that freezes the pre-trained model and only\nfinetunes a few extra parameters is introduced and delivers promising results.\nMost studies on adapter architectures are dedicated to unimodal or bimodal\ntasks, while the adapter architectures for trimodal tasks have not been\ninvestigated yet. This paper introduces a novel Long Short-Term Trimodal\nAdapter (LSTTA) approach for video understanding tasks involving audio, visual,\nand language modalities. Based on the pre-trained from the three modalities,\nthe designed adapter module is inserted between the sequential blocks to model\nthe dense interactions across the three modalities. Specifically, LSTTA\nconsists of two types of complementary adapter modules, namely the long-term\nsemantic filtering module and the short-term semantic interaction module. The\nlong-term semantic filtering aims to characterize the temporal importance of\nthe video frames and the short-term semantic interaction module models local\ninteractions within short periods. Compared to previous state-of-the-art\ntrimodal learning methods pre-trained on a large-scale trimodal corpus, LSTTA\nis more flexible and can inherit any powerful unimodal or bimodal models.\nExperimental results on four typical trimodal learning tasks show the\neffectiveness of LSTTA over existing state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.14274v1"
    },
    {
        "title": "Prompting Vision Language Model with Knowledge from Large Language Model\n  for Knowledge-Based VQA",
        "authors": [
            "Yang Zhou",
            "Pengfei Cao",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Knowledge-based visual question answering is a very challenging and widely\nconcerned task. Previous methods adopts the implicit knowledge in large\nlanguage models (LLM) to achieve excellent results, but we argue that existing\nmethods may suffer from biasing understanding of the image and insufficient\nknowledge to solve the problem. In this paper, we propose PROOFREAD -PROmpting\nvision language model with knOwledge From laRgE lAnguage moDel, a novel,\nlightweight and efficient kowledge-based VQA framework, which make the vision\nlanguage model and the large language model cooperate to give full play to\ntheir respective strengths and bootstrap each other. In detail, our proposed\nmethod uses LLM to obtain knowledge explicitly, uses the vision language model\nwhich can see the image to get the knowledge answer, and introduces knowledge\nperceiver to filter out knowledge that is harmful for getting the correct final\nanswer. Experimental results on two datasets prove the effectiveness of our\napproach. Our method outperforms all state-of-the-art methods on the A-OKVQA\ndataset in two settings and also achieves relatively good performance on the\nOKVQA dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.15851v1"
    },
    {
        "title": "Edge-Assisted On-Device Model Update for Video Analytics in Adverse\n  Environments",
        "authors": [
            "Yuxin Kong",
            "Peng Yang",
            "Yan Cheng"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  While large deep neural networks excel at general video analytics tasks, the\nsignificant demand on computing capacity makes them infeasible for real-time\ninference on resource-constrained end cam-eras. In this paper, we propose an\nedge-assisted framework that continuously updates the lightweight model\ndeployed on the end cameras to achieve accurate predictions in adverse\nenvironments. This framework consists of three modules, namely, a key frame\nextractor, a trigger controller, and a retraining manager. The low-cost key\nframe extractor obtains frames that can best represent the current environment.\nThose frames are then transmitted and buffered as the retraining data for model\nupdate at the edge server. Once the trigger controller detects a significant\naccuracy drop in the selected frames, the retraining manager outputs the\noptimal retraining configuration balancing the accuracy and time cost. We\nprototype our system on two end devices of different computing capacities with\none edge server. The results demonstrate that our approach significantly\nimproves accuracy across all tested adverse environment scenarios (up to 24%)\nand reduces more than 50% of the retraining time compared to existing\nbenchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.16413v1"
    },
    {
        "title": "Edge-Assisted Lightweight Region-of-Interest Extraction and Transmission\n  for Vehicle Perception",
        "authors": [
            "Yan Cheng",
            "Peng Yang",
            "Ning Zhang",
            "Jiawei Hou"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  To enhance on-road environmental perception for autonomous driving, accurate\nand real-time analytics on high-resolution video frames generated from on-board\ncameras be-comes crucial. In this paper, we design a lightweight object\nlocation method based on class activation mapping (CAM) to rapidly capture the\nregion of interest (RoI) boxes that contain driving safety related objects from\non-board cameras, which can not only improve the inference accuracy of vision\ntasks, but also reduce the amount of transmitted data. Considering the limited\non-board computation resources, the RoI boxes extracted from the raw image are\noffloaded to the edge for further processing. Considering both the dynamics of\nvehicle-to-edge communications and the limited edge resources, we propose an\nadaptive RoI box offloading algorithm to ensure prompt and accurate inference\nby adjusting the down-sampling rate of each box. Extensive experimental results\non four high-resolution video streams demonstrate that our approach can\neffectively improve the overall accuracy by up to 16% and reduce the\ntransmission demand by up to 49%, compared with other benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.16417v1"
    },
    {
        "title": "End-Edge Coordinated Joint Encoding and Neural Enhancement for Low-Light\n  Video Analytics",
        "authors": [
            "Yuanyi He",
            "Peng Yang",
            "Tian Qin",
            "Ning Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In this paper, we investigate video analytics in low-light environments, and\npropose an end-edge coordinated system with joint video encoding and\nenhancement. It adaptively transmits low-light videos from cameras and performs\nenhancement and inference tasks at the edge. Firstly, according to our\nobservations, both encoding and enhancement for low-light videos have a\nsignificant impact on inference accuracy, which directly influences bandwidth\nand computation overhead. Secondly, due to the limitation of built-in\ncomputation resources, cameras perform encoding and transmitting frames to the\nedge. The edge executes neural enhancement to process low contrast, detail\nloss, and color distortion on low-light videos before inference. Finally, an\nadaptive controller is designed at the edge to select quantization parameters\nand scales of neural enhancement networks, aiming to improve the inference\naccuracy and meet the latency requirements. Extensive real-world experiments\ndemon-strate that, the proposed system can achieve a better trade-off between\ncommunication and computation resources and optimize the inference accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.16418v1"
    },
    {
        "title": "Target-Guided Composed Image Retrieval",
        "authors": [
            "Haokun Wen",
            "Xian Zhang",
            "Xuemeng Song",
            "Yinwei Wei",
            "Liqiang Nie"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Composed image retrieval (CIR) is a new and flexible image retrieval\nparadigm, which can retrieve the target image for a multimodal query, including\na reference image and its corresponding modification text. Although existing\nefforts have achieved compelling success, they overlook the conflict\nrelationship modeling between the reference image and the modification text for\nimproving the multimodal query composition and the adaptive matching degree\nmodeling for promoting the ranking of the candidate images that could present\ndifferent levels of matching degrees with the given query. To address these two\nlimitations, in this work, we propose a Target-Guided Composed Image Retrieval\nnetwork (TG-CIR). In particular, TG-CIR first extracts the unified global and\nlocal attribute features for the reference/target image and the modification\ntext with the contrastive language-image pre-training model (CLIP) as the\nbackbone, where an orthogonal regularization is introduced to promote the\nindependence among the attribute features. Then TG-CIR designs a target-query\nrelationship-guided multimodal query composition module, comprising a\ntarget-free student composition branch and a target-based teacher composition\nbranch, where the target-query relationship is injected into the teacher branch\nfor guiding the conflict relationship modeling of the student branch. Last,\napart from the conventional batch-based classification loss, TG-CIR\nadditionally introduces a batch-based target similarity-guided matching degree\nregularization to promote the metric learning process. Extensive experiments on\nthree benchmark datasets demonstrate the superiority of our proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.01366v1"
    },
    {
        "title": "Detecting False Alarms and Misses in Audio Captions",
        "authors": [
            "Rehana Mahfuz",
            "Yinyi Guo",
            "Arvind Krishna Sridhar",
            "Erik Visser"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Metrics to evaluate audio captions simply provide a score without much\nexplanation regarding what may be wrong in case the score is low. Manual human\nintervention is needed to find any shortcomings of the caption. In this work,\nwe introduce a metric which automatically identifies the shortcomings of an\naudio caption by detecting the misses and false alarms in a candidate caption\nwith respect to a reference caption, and reports the recall, precision and\nF-score. Such a metric is very useful in profiling the deficiencies of an audio\ncaptioning model, which is a milestone towards improving the quality of audio\ncaptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.03326v1"
    },
    {
        "title": "BOLA360: Near-optimal View and Bitrate Adaptation for 360-degree Video\n  Streaming",
        "authors": [
            "Ali Zeynali",
            "Mahsa Sahebdel",
            "Mohammad Hajiesmaili",
            "Ramesh K. Sitaraman"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Recent advances in omnidirectional cameras and AR/VR headsets have spurred\nthe adoption of 360-degree videos that are widely believed to be the future of\nonline video streaming. 360-degree videos allow users to wear a head-mounted\ndisplay (HMD) and experience the video as if they are physically present in the\nscene. Streaming high-quality 360-degree videos at scale is an unsolved problem\nthat is more challenging than traditional (2D) video delivery. The data rate\nrequired to stream 360-degree videos is an order of magnitude more than\ntraditional videos. Further, the penalty for rebuffering events where the video\nfreezes or displays a blank screen is more severe as it may cause\ncybersickness. We propose an online adaptive bitrate (ABR) algorithm for\n360-degree videos called BOLA360 that runs inside the client's video player and\norchestrates the download of video segments from the server so as to maximize\nthe quality-of-experience (QoE) of the user. BOLA360 conserves bandwidth by\ndownloading only those video segments that are likely to fall within the\nfield-of-view (FOV) of the user. In addition, BOLA360 continually adapts the\nbitrate of the downloaded video segments so as to enable a smooth playback\nwithout rebuffering. We prove that BOLA360 is near-optimal with respect to an\noptimal offline algorithm that maximizes QoE. Further, we evaluate BOLA360 on a\nwide range of network and user head movement profiles and show that it provides\n$13.6\\%$ to $372.5\\%$ more QoE than state-of-the-art algorithms. While ABR\nalgorithms for traditional (2D) videos have been well-studied over the last\ndecade, our work is the first ABR algorithm for 360-degree videos with both\ntheoretical and empirical guarantees on its performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.04023v2"
    },
    {
        "title": "CANF-VC++: Enhancing Conditional Augmented Normalizing Flows for Video\n  Compression with Advanced Techniques",
        "authors": [
            "Peng-Yu Chen",
            "Wen-Hsiao Peng"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Video has become the predominant medium for information dissemination,\ndriving the need for efficient video codecs. Recent advancements in learned\nvideo compression have shown promising results, surpassing traditional codecs\nin terms of coding efficiency. However, challenges remain in integrating\nfragmented techniques and incorporating new tools into existing codecs. In this\npaper, we comprehensively review the state-of-the-art CANF-VC codec and propose\nCANF-VC++, an enhanced version that addresses these challenges. We\nsystematically explore architecture design, reference frame type, training\nprocedure, and entropy coding efficiency, leading to substantial coding\nimprovements. CANF-VC++ achieves significant Bj{\\o}ntegaard-Delta rate savings\non conventional datasets UVG, HEVC Class B and MCL-JCV, outperforming the\nbaseline CANF-VC and even the H.266 reference software VTM. Our work\ndemonstrates the potential of integrating advancements in video compression and\nserves as inspiration for future research in the field.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.05382v1"
    },
    {
        "title": "Invertible Mosaic Image Hiding Network for Very Large Capacity Image\n  Steganography",
        "authors": [
            "Zihan Chen",
            "Tianrui Liu",
            "Jun-Jie Huang",
            "Wentao Zhao",
            "Xing Bi",
            "Meng Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The existing image steganography methods either sequentially conceal secret\nimages or conceal a concatenation of multiple images. In such ways, the\ninterference of information among multiple images will become increasingly\nsevere when the number of secret images becomes larger, thus restrict the\ndevelopment of very large capacity image steganography. In this paper, we\npropose an Invertible Mosaic Image Hiding Network (InvMIHNet) which realizes\nvery large capacity image steganography with high quality by concealing a\nsingle mosaic secret image. InvMIHNet consists of an Invertible Image Rescaling\n(IIR) module and an Invertible Image Hiding (IIH) module. The IIR module works\nfor downscaling the single mosaic secret image form by spatially splicing the\nmultiple secret images, and the IIH module then conceal this mosaic image under\nthe cover image. The proposed InvMIHNet successfully conceal and reveal up to\n16 secret images with a small number of parameters and memory consumption.\nExtensive experiments on ImageNet-1K, COCO and DIV2K show InvMIHNet outperforms\nstate-of-the-art methods in terms of both the imperceptibility of stego image\nand recover accuracy of secret image.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.08987v1"
    },
    {
        "title": "Unified Pretraining Target Based Video-music Retrieval With Music Rhythm\n  And Video Optical Flow Information",
        "authors": [
            "Tianjun Mao",
            "Shansong Liu",
            "Yunxuan Zhang",
            "Dian Li",
            "Ying Shan"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Background music (BGM) can enhance the video's emotion. However, selecting an\nappropriate BGM often requires domain knowledge. This has led to the\ndevelopment of video-music retrieval techniques. Most existing approaches\nutilize pretrained video/music feature extractors trained with different target\nsets to obtain average video/music-level embeddings. The drawbacks are\ntwo-fold. One is that different target sets for video/music pretraining may\ncause the generated embeddings difficult to match. The second is that the\nunderlying temporal correlation between video and music is ignored. In this\npaper, our proposed approach leverages a unified target set to perform\nvideo/music pretraining and produces clip-level embeddings to preserve temporal\ninformation. The downstream cross-modal matching is based on the clip-level\nfeatures with embedded music rhythm and optical flow information. Experiments\ndemonstrate that our proposed method can achieve superior performance over the\nstate-of-the-art methods by a significant margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.09421v1"
    },
    {
        "title": "Semantic Change Driven Generative Semantic Communication Framework",
        "authors": [
            "Wanting Yang",
            "Zehui Xiong",
            "Hongyang Du",
            "Yanli Yuan",
            "Tony Q. S. Quek"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The burgeoning generative artificial intelligence technology offers novel\ninsights into the development of semantic communication (SemCom) frameworks.\nThese frameworks hold the potential to address the challenges associated with\nthe black-box nature inherent in existing end-to-end training manner for the\nexisting SemCom framework, as well as deterioration of the user experience\ncaused by the inevitable error floor in deep learning-based SemCom. In this\npaper, we focus on the widespread remote monitoring scenario, and propose a\nsemantic change driven generative SemCom framework. Therein, the semantic\nencoder and semantic decoder can be optimized independently. Specifically, we\ndevelop a modular semantic encoder with value of information based semantic\nsampling function. In addition, we propose a conditional denoising diffusion\nprobabilistic mode-assisted semantic decoder that relies on received semantic\ninformation from the source, namely, the semantic map, and the local static\nscene information to remotely regenerate scenes. Moreover, we demonstrate the\neffectiveness of the proposed semantic encoder and decoder as well as the\nconsiderable potential in reducing energy consumption through simulation based\non the realistic $\\mathcal{F}$ composite channel fading model. The code is\navailable at https://github.com/wty2011jl/SCDGSC.git.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.12775v3"
    },
    {
        "title": "Encoder-Decoder-Based Intra-Frame Block Partitioning Decision",
        "authors": [
            "Yucheng Jiang",
            "Han Peng",
            "Yan Song",
            "Jie Yu",
            "Peng Zhang",
            "Songping Mai"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The recursive intra-frame block partitioning decision process, a crucial\ncomponent of the next-generation video coding standards, exerts significant\ninfluence over the encoding time. In this paper, we propose an encoder-decoder\nneural network (NN) to accelerate this process. Specifically, a CNN is utilized\nto compress the pixel data of the largest coding unit (LCU) into a fixed-length\nvector. Subsequently, a Transformer decoder is employed to transcribe the\nfixed-length vector into a variable-length vector, which represents the block\npartitioning outcomes of the encoding LCU. The vector transcription process\nadheres to the constraints imposed by the block partitioning algorithm. By\nfully parallelizing the NN prediction in the intra-mode decision, substantial\ntime savings can be attained during the decision phase. The experimental\nresults obtained from high-definition (HD) sequences coding demonstrate that\nthis framework achieves a remarkable 87.84\\% reduction in encoding time, with a\nrelatively small loss (8.09\\%) of coding performance compared to AVS3 HPM4.0.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.06412v1"
    },
    {
        "title": "Interactive Interior Design Recommendation via Coarse-to-fine Multimodal\n  Reinforcement Learning",
        "authors": [
            "He Zhang",
            "Ying Sun",
            "Weiyu Guo",
            "Yafei Liu",
            "Haonan Lu",
            "Xiaodong Lin",
            "Hui Xiong"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Personalized interior decoration design often incurs high labor costs. Recent\nefforts in developing intelligent interior design systems have focused on\ngenerating textual requirement-based decoration designs while neglecting the\nproblem of how to mine homeowner's hidden preferences and choose the proper\ninitial design. To fill this gap, we propose an Interactive Interior Design\nRecommendation System (IIDRS) based on reinforcement learning (RL). IIDRS aims\nto find an ideal plan by interacting with the user, who provides feedback on\nthe gap between the recommended plan and their ideal one. To improve\ndecision-making efficiency and effectiveness in large decoration spaces, we\npropose a Decoration Recommendation Coarse-to-Fine Policy Network (DecorRCFN).\nAdditionally, to enhance generalization in online scenarios, we propose an\nobject-aware feedback generation method that augments model training with\ndiversified and dynamic textual feedback. Extensive experiments on a real-world\ndataset demonstrate our method outperforms traditional methods by a large\nmargin in terms of recommendation accuracy. Further user studies demonstrate\nthat our method reaches higher real-world user satisfaction than baseline\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.07287v1"
    },
    {
        "title": "MCPNS: A Macropixel Collocated Position and Its Neighbors Search for\n  Plenoptic 2.0 Video Coding",
        "authors": [
            "Vinh Van Duong",
            "Thuc Nguyen Huu",
            "Jonghoon Yim",
            "Byeungwoo Jeon"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Recently, it was demonstrated that a newly focused plenoptic 2.0 camera can\ncapture much higher spatial resolution owing to its effective light field\nsampling, as compared to a traditional unfocused plenoptic 1.0 camera. However,\ndue to the nature difference of the optical structure between the plenoptic 1.0\nand 2.0 cameras, the existing fast motion estimation (ME) method for plenoptic\n1.0 videos is expected to be sub-optimal for encoding plenoptic 2.0 videos. In\nthis paper, we point out the main motion characteristic differences between\nplenoptic 1.0 and 2.0 videos and then propose a new fast ME, called macropixel\ncollocated position and its neighbors search (MCPNS) for plenoptic 2.0 videos.\nIn detail, we propose to reduce the number of macropixel collocated position\n(MCP) search candidates based on the new observation of center-biased motion\nvector distribution at macropixel resolution. After that, due to large motion\ndeviation behavior around each MCP location in plenoptic 2.0 videos, we propose\nto select a certain number of key MCP locations with the lowest matching cost\nto perform the neighbors MCP search to improve the motion search accuracy.\nDifferent from existing methods, our method can achieve better performance\nwithout requiring prior knowledge of microlens array orientations. Our\nsimulation results confirmed the effectiveness of the proposed algorithm in\nterms of both bitrate savings and computational costs compared to existing\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.08006v3"
    },
    {
        "title": "Energy-Efficient Multi-Codec Bitrate-Ladder Estimation for Adaptive\n  Video Streaming",
        "authors": [
            "Vignesh V Menon",
            "Reza Farahani",
            "Prajit T Rajendran",
            "Samira Afzal",
            "Klaus Schoeffmann",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  With the emergence of multiple modern video codecs, streaming service\nproviders are forced to encode, store, and transmit bitrate ladders of multiple\ncodecs separately, consequently suffering from additional energy costs for\nencoding, storage, and transmission. To tackle this issue, we introduce an\nonline energy-efficient Multi-Codec Bitrate ladder Estimation scheme (MCBE) for\nadaptive video streaming applications. In MCBE, quality representations within\nthe bitrate ladder of new-generation codecs (e.g., High Efficiency Video Coding\n(HEVC), Alliance for Open Media Video 1 (AV1)) that lie below the predicted\nrate-distortion curve of the Advanced Video Coding (AVC) codec are removed.\nMoreover, perceptual redundancy between representations of the bitrate ladders\nof the considered codecs is also minimized based on a Just Noticeable\nDifference (JND) threshold. Therefore, random forest-based models predict the\nVMAF score of bitrate ladder representations of each codec. In a live streaming\nsession where all clients support the decoding of AVC, HEVC, and AV1, MCBE\nachieves impressive results, reducing cumulative encoding energy by 56.45%,\nstorage energy usage by 94.99%, and transmission energy usage by 77.61%\n(considering a JND of six VMAF points). These energy reductions are in\ncomparison to a baseline bitrate ladder encoding based on current industry\npractice.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.09570v1"
    },
    {
        "title": "Redundancy-Adaptive Multimodal Learning for Imperfect Data",
        "authors": [
            "Mengxi Chen",
            "Jiangchao Yao",
            "Linyu Xing",
            "Yu Wang",
            "Ya Zhang",
            "Yanfeng Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Multimodal models trained on complete modality data often exhibit a\nsubstantial decrease in performance when faced with imperfect data containing\ncorruptions or missing modalities. To address this robustness challenge, prior\nmethods have explored various approaches from aspects of augmentation,\nconsistency or uncertainty, but these approaches come with associated drawbacks\nrelated to data complexity, representation, and learning, potentially\ndiminishing their overall effectiveness. In response to these challenges, this\nstudy introduces a novel approach known as the Redundancy-Adaptive Multimodal\nLearning (RAML). RAML efficiently harnesses information redundancy across\nmultiple modalities to combat the issues posed by imperfect data while\nremaining compatible with the complete modality. Specifically, RAML achieves\nredundancy-lossless information extraction through separate unimodal\ndiscriminative tasks and enforces a proper norm constraint on each unimodal\nfeature representation. Furthermore, RAML explicitly enhances multimodal fusion\nby leveraging fine-grained redundancy among unimodal features to learn\ncorrespondences between corrupted and untainted information. Extensive\nexperiments on various benchmark datasets under diverse conditions have\nconsistently demonstrated that RAML outperforms state-of-the-art methods by a\nsignificant margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.14496v1"
    },
    {
        "title": "RecipeMeta: Metapath-enhanced Recipe Recommendation on Heterogeneous\n  Recipe Network",
        "authors": [
            "Jialiang Shi",
            "Takahiro Komamizu",
            "Keisuke Doman",
            "Haruya Kyutoku",
            "Ichiro Ide"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Recipe is a set of instructions that describes how to make food. It can help\npeople from the preparation of ingredients, food cooking process, etc. to\nprepare the food, and increasingly in demand on the Web. To help users find the\nvast amount of recipes on the Web, we address the task of recipe\nrecommendation. Due to multiple data types and relationships in a recipe, we\ncan treat it as a heterogeneous network to describe its information more\naccurately. To effectively utilize the heterogeneous network, metapath was\nproposed to describe the higher-level semantic information between two entities\nby defining a compound path from peer entities. Therefore, we propose a\nmetapath-enhanced recipe recommendation framework, RecipeMeta, that combines\nGNN (Graph Neural Network)-based representation learning and specific\nmetapath-based information in a recipe to predict User-Recipe pairs for\nrecommendation. Through extensive experiments, we demonstrate that the proposed\nmodel, RecipeMeta, outperforms state-of-the-art methods for recipe\nrecommendation.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.15593v1"
    },
    {
        "title": "Automatic Edge Error Judgment in Figure Skating Using 3D Pose Estimation\n  from a Monocular Camera and IMUs",
        "authors": [
            "Ryota Tanaka",
            "Tomohiro Suzuki",
            "Kazuya Takeda",
            "Keisuke Fujii"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Automatic evaluating systems are fundamental issues in sports technologies.\nIn many sports, such as figure skating, automated evaluating methods based on\npose estimation have been proposed. However, previous studies have evaluated\nskaters' skills in 2D analysis. In this paper, we propose an automatic edge\nerror judgment system with a monocular smartphone camera and inertial sensors,\nwhich enable us to analyze 3D motions. Edge error is one of the most\nsignificant scoring items and is challenging to automatically judge due to its\n3D motion. The results show that the model using 3D joint position coordinates\nestimated from the monocular camera as the input feature had the highest\naccuracy at 83% for unknown skaters' data. We also analyzed the detailed motion\nanalysis for edge error judgment. These results indicate that the monocular\ncamera can be used to judge edge errors automatically. We will provide the\nfigure skating single Lutz jump dataset, including pre-processed videos and\nlabels, at https://github.com/ryota-takedalab/JudgeAI-LutzEdge.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.17193v1"
    },
    {
        "title": "Deep3DSketch+: Obtaining Customized 3D Model by Single Free-Hand Sketch\n  through Deep Learning",
        "authors": [
            "Ying Zang",
            "Chenglong Fu",
            "Tianrun Chen",
            "Yuanqi Hu",
            "Qingshan Liu",
            "Wenjun Hu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  As 3D models become critical in today's manufacturing and product design,\nconventional 3D modeling approaches based on Computer-Aided Design (CAD) are\nlabor-intensive, time-consuming, and have high demands on the creators. This\nwork aims to introduce an alternative approach to 3D modeling by utilizing\nfree-hand sketches to obtain desired 3D models. We introduce Deep3DSketch+,\nwhich is a deep-learning algorithm that takes the input of a single free-hand\nsketch and produces a complete and high-fidelity model that matches the sketch\ninput. The neural network has view- and structural-awareness enabled by a Shape\nDiscriminator (SD) and a Stroke Enhancement Module (SEM), which overcomes the\nlimitations of sparsity and ambiguity of the sketches. The network design also\nbrings high robustness to partial sketch input in industrial applications.Our\napproach has undergone extensive experiments, demonstrating its\nstate-of-the-art (SOTA) performance on both synthetic and real-world datasets.\nThese results validate the effectiveness and superiority of our method compared\nto existing techniques. We have demonstrated the conversion of free-hand\nsketches into physical 3D objects using additive manufacturing. We believe that\nour approach has the potential to accelerate product design and democratize\ncustomized manufacturing.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.18609v1"
    },
    {
        "title": "An Implementation of Multimodal Fusion System for Intelligent Digital\n  Human Generation",
        "authors": [
            "Yingjie Zhou",
            "Yaodong Chen",
            "Kaiyue Bi",
            "Lian Xiong",
            "Hui Liu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  With the rapid development of artificial intelligence (AI), digital humans\nhave attracted more and more attention and are expected to achieve a wide range\nof applications in several industries. Then, most of the existing digital\nhumans still rely on manual modeling by designers, which is a cumbersome\nprocess and has a long development cycle. Therefore, facing the rise of digital\nhumans, there is an urgent need for a digital human generation system combined\nwith AI to improve development efficiency. In this paper, an implementation\nscheme of an intelligent digital human generation system with multimodal fusion\nis proposed. Specifically, text, speech and image are taken as inputs, and\ninteractive speech is synthesized using large language model (LLM), voiceprint\nextraction, and text-to-speech conversion techniques. Then the input image is\nage-transformed and a suitable image is selected as the driving image. Then,\nthe modification and generation of digital human video content is realized by\ndigital human driving, novel view synthesis, and intelligent dressing\ntechniques. Finally, we enhance the user experience through style transfer,\nsuper-resolution, and quality evaluation. Experimental results show that the\nsystem can effectively realize digital human generation. The related code is\nreleased at https://github.com/zyj-2000/CUMT_2D_PhotoSpeaker.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.20251v1"
    },
    {
        "title": "Predictive Sampling for Efficient Pairwise Subjective Image Quality\n  Assessment",
        "authors": [
            "Shima Mohammadi",
            "João Ascenso"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Subjective image quality assessment studies are used in many scenarios, such\nas the evaluation of compression, super-resolution, and denoising solutions.\nAmong the available subjective test methodologies, pair comparison is\nattracting popularity due to its simplicity, reliability, and robustness to\nchanges in the test conditions, e.g. display resolutions. The main problem that\nimpairs its wide acceptance is that the number of pairs to compare by subjects\ngrows quadratically with the number of stimuli that must be considered.\nUsually, the paired comparison data obtained is fed into an aggregation model\nto obtain a final score for each degraded image and thus, not every comparison\ncontributes equally to the final quality score. In the past years, several\nsolutions that sample pairs (from all possible combinations) have been\nproposed, from random sampling to active sampling based on the past subjects'\ndecisions. This paper introduces a novel sampling solution called\n\\textbf{P}redictive \\textbf{S}ampling for \\textbf{P}airwise \\textbf{C}omparison\n(PS-PC) which exploits the characteristics of the input data to make a\nprediction of which pairs should be evaluated by subjects. The proposed\nsolution exploits popular machine learning techniques to select the most\ninformative pairs for subjects to evaluate, while for the other remaining\npairs, it predicts the subjects' preferences. The experimental results show\nthat PS-PC is the best choice among the available sampling algorithms with\nhigher performance for the same number of pairs. Moreover, since the choice of\nthe pairs is done \\emph{a priori} before the subjective test starts, the\nalgorithm is not required to run during the test and thus much more simple to\ndeploy in online crowdsourcing subjective tests.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.03850v1"
    },
    {
        "title": "Evaluation of Sampling Algorithms for a Pairwise Subjective Assessment\n  Methodology",
        "authors": [
            "Shima Mohammadi",
            "Joao Ascenso"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Subjective assessment tests are often employed to evaluate image processing\nsystems, notably image and video compression, super-resolution among others and\nhave been used as an indisputable way to provide evidence of the performance of\nan algorithm or system. While several methodologies can be used in a subjective\nquality assessment test, pairwise comparison tests are nowadays attracting a\nlot of attention due to their accuracy and simplicity. However, the number of\ncomparisons in a pairwise comparison test increases quadratically with the\nnumber of stimuli and thus often leads to very long tests, which is impractical\nfor many cases. However, not all the pairs contribute equally to the final\nscore and thus, it is possible to reduce the number of comparisons without\ndegrading the final accuracy. To do so, pairwise sampling methods are often\nused to select the pairs which provide more information about the quality of\neach stimuli. In this paper, a reliable and much-needed evaluation procedure is\nproposed and used for already available methods in the literature, especially\nconsidering the case of subjective evaluation of image and video codecs. The\nresults indicate that an appropriate selection of the pairs allows to achieve\nvery reliable scores while requiring the comparison of a much lower number of\npairs.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.06093v1"
    },
    {
        "title": "Content-Adaptive Variable Framerate Encoding Scheme for Green Live\n  Streaming",
        "authors": [
            "Vignesh V Menon",
            "Samira Afzal",
            "Prajit T Rajendran",
            "Klaus Schoeffmann",
            "Radu Prodan",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Adaptive live video streaming applications use a fixed predefined\nconfiguration for the bitrate ladder with constant framerate and encoding\npresets in a session. However, selecting optimized framerates and presets for\nevery bitrate ladder representation can enhance perceptual quality, improve\ncomputational resource allocation, and thus, the streaming energy efficiency.\nIn particular, low framerates for low-bitrate representations reduce\ncompression artifacts and decrease encoding energy consumption. In addition, an\noptimized preset may lead to improved compression efficiency. To this light,\nthis paper proposes a Content-adaptive Variable Framerate (CVFR) encoding\nscheme, which offers two modes of operation: ecological (ECO) and high-quality\n(HQ). CVFR-ECO optimizes for the highest encoding energy savings by predicting\nthe optimized framerate for each representation in the bitrate ladder. CVFR-HQ\ntakes it further by predicting each representation's optimized\nframerate-encoding preset pair using low-complexity discrete cosine transform\nenergy-based spatial and temporal features for compression efficiency and\nsustainable storage. We demonstrate the advantage of CVFR using the x264\nopen-source video encoder. The results show that CVFR-ECO yields an average\nPSNR and VMAF increase of 0.02 dB and 2.50 points, respectively, for the same\nbitrate, compared to the fastest preset highest framerate encoding. CVFR-ECO\nalso yields an average encoding and storage energy consumption reduction of\n34.54% and 76.24%, considering a just noticeable difference (JND) of six VMAF\npoints. In comparison, CVFR-HQ yields an average increase in PSNR and VMAF of\n2.43 dB and 10.14 points, respectively, for the same bitrate. Finally, CVFR-HQ\nresulted in an average reduction in storage energy consumption of 83.18%,\nconsidering a JND of six VMAF points.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.08074v1"
    },
    {
        "title": "Multimodal Characterization of Emotion within Multimedia Space",
        "authors": [
            "Dayo Samuel Banjo",
            "Connice Trimmingham",
            "Niloofar Yousefi",
            "Nitin Agarwal"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Technological advancement and its omnipresent connection have pushed humans\npast the boundaries and limitations of a computer screen, physical state, or\ngeographical location. It has provided a depth of avenues that facilitate\nhuman-computer interaction that was once inconceivable such as audio and body\nlanguage detection. Given the complex modularities of emotions, it becomes\nvital to study human-computer interaction, as it is the commencement of a\nthorough understanding of the emotional state of users and, in the context of\nsocial networks, the producers of multimodal information. This study first\nacknowledges the accuracy of classification found within multimodal emotion\ndetection systems compared to unimodal solutions. Second, it explores the\ncharacterization of multimedia content produced based on their emotions and the\ncoherence of emotion in different modalities by utilizing deep learning models\nto classify emotion across different modalities.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.11892v1"
    },
    {
        "title": "Weakly-Supervised Video Moment Retrieval via Regularized Two-Branch\n  Proposal Networks with Erasing Mechanism",
        "authors": [
            "Haoyuan Li",
            "Zhou Zhao",
            "Zhu Zhang",
            "Zhijie Lin"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Video moment retrieval is to identify the target moment according to the\ngiven sentence in an untrimmed video. Due to temporal boundary annotations of\nthe video are extremely time-consuming to acquire, modeling in the\nweakly-supervised setting is increasingly focused, where we only have access to\nthe video-sentence pairs during training. Most existing weakly-supervised\nmethods adopt a MIL-based framework to develop inter-sample confrontment, but\nneglect the intra-sample confrontment between moments with similar semantics.\nTherefore, these methods fail to distinguish the correct moment from plausible\nnegative moments. Further, the previous attention models in cross-modal\ninteraction tend to focus on a few dominant words exorbitantly, ignoring the\ncomprehensive video-sentence correspondence. In this paper, we propose a novel\nRegularized Two-Branch Proposal Network with Erasing Mechanism to consider the\ninter-sample and intra-sample confrontments simultaneously. Concretely, we\nfirst devise a language-aware visual filter to generate both enhanced and\nsuppressed video streams. Then, we design the sharable two-branch proposal\nmodule to generate positive and plausible negative proposals from the enhanced\nand suppressed branch respectively, contributing to sufficient confrontment.\nBesides, we introduce an attention-guided dynamic erasing mechanism in enhanced\nbranch to discover the complementary video-sentence relation. Moreover, we\napply two types of proposal regularization to stabilize the training process\nand improve model performance. The extensive experiments on ActivityCaption,\nCharades-STA and DiDeMo datasets show the effectiveness of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.13946v1"
    },
    {
        "title": "A Metadata Generation System with Semantic Understanding for Video\n  Retrieval in Film Production",
        "authors": [
            "Feilin Han",
            "Zhaoxu Meng"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In film production, metadata plays an important role in original raw video\nindexing and classification within the industrial post-production software.\nInspired by deep visual-semantic methods, we propose an automated image\ninformation extraction process to extend the diversity of metadata entities for\nmassive large-scale raw video searching and retrieval. In this paper, we\nintroduce the proposed system architecture and modules, integrating semantic\nannotation models and user-demand-oriented information fusion. We conducted\nexperiments to validate the effectiveness of our system on Film Raw Video\nSemantic Annotation Dataset (Film-RVSAD) and Slate Board Template Dataset\n(SBTD), two benchmark datasets built for cinematography-related semantic\nannotation and slate detection. Experimental results show that the proposed\nsystem provides an effective strategy to improve the efficiency of metadata\ngeneration and transformation, which is necessary and convenient for\ncollaborative work in the filmmaking process.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.00104v1"
    },
    {
        "title": "Two-stage dynamic creative optimization under sparse ambiguous samples\n  for e-commerce advertising",
        "authors": [
            "Guandong Li",
            "Xian Yang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Ad creative is one of the main mediums for e-commerce advertising. In our\napproach we decouple this dynamic creative optimization into two stages, a\ncascaded structure that can trade off between effectiveness and efficiency. In\nthe first stage, we train an automatic creative optimization architecture based\non autoco to simulate complex interactions between creative elements. Although\nwe obtained the ranking of different creatives under a sku, because we bucketed\nand merged historical data according to periods, this confuses the ctr\ndiversity of the same ad creatives on different days and weakens the ability to\nseparate ambiguous samples. Therefore, we propose a transformer-based rerank\nmodel. With the help of the rank model, we propose a distillation method to\nlearn the relative order of ideas and extract the ranking knowledge to guide\nthe rerank learning. The creative order soft labels under each sku are\ngenerated by the rank model to alleviate the dilemma that a large number of\nunder-represented creatives cannot obtain real labels. Through the knowledge\ndiffusion of rerank, the ambiguous samples are associated with the positive and\nnegative samples. Cascade rerank and autoco to output the estimated value of\nthe synthetic ad image. In the second stage, we designed a bandit model, and\nthe bandit selected one of the output ad of the first stage for timely\ndelivery. Experimental results show that our method can outperform competing\nbaselines in terms of sctr. Online A/B testing shows that our method improves\nctr by 10% compared to the baseline.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.01295v1"
    },
    {
        "title": "Deep3DSketch: 3D modeling from Free-hand Sketches with View- and\n  Structural-Aware Adversarial Training",
        "authors": [
            "Tianrun Chen",
            "Chenglong Fu",
            "Lanyun Zhu",
            "Papa Mao",
            "Jia Zhang",
            "Ying Zang",
            "Lingyun Sun"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  This work aims to investigate the problem of 3D modeling using single\nfree-hand sketches, which is one of the most natural ways we humans express\nideas. Although sketch-based 3D modeling can drastically make the 3D modeling\nprocess more accessible, the sparsity and ambiguity of sketches bring\nsignificant challenges for creating high-fidelity 3D models that reflect the\ncreators' ideas. In this work, we propose a view- and structural-aware deep\nlearning approach, \\textit{Deep3DSketch}, which tackles the ambiguity and fully\nuses sparse information of sketches, emphasizing the structural information.\nSpecifically, we introduced random pose sampling on both 3D shapes and 2D\nsilhouettes, and an adversarial training scheme with an effective progressive\ndiscriminator to facilitate learning of the shape structures. Extensive\nexperiments demonstrated the effectiveness of our approach, which outperforms\nexisting methods -- with state-of-the-art (SOTA) performance on both synthetic\nand real datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.04435v1"
    },
    {
        "title": "AFL-Net: Integrating Audio, Facial, and Lip Modalities with a Two-step\n  Cross-attention for Robust Speaker Diarization in the Wild",
        "authors": [
            "Yongkang Yin",
            "Xu Li",
            "Ying Shan",
            "Yuexian Zou"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Speaker diarization in real-world videos presents significant challenges due\nto varying acoustic conditions, diverse scenes, the presence of off-screen\nspeakers, etc. This paper builds upon a previous study (AVR-Net) and introduces\na novel multi-modal speaker diarization system, AFL-Net. The proposed AFL-Net\nincorporates dynamic lip movement as an additional modality to enhance the\nidentity distinction. Besides, unlike AVR-Net which extracts high-level\nrepresentations from each modality independently, AFL-Net employs a two-step\ncross-attention mechanism to sufficiently fuse different modalities, resulting\nin more comprehensive information to enhance the performance. Moreover, we also\nincorporated a masking strategy during training, where the face and lip\nmodalities are randomly obscured. This strategy enhances the impact of the\naudio modality on the system outputs. Experimental results demonstrate that\nAFL-Net outperforms state-of-the-art baselines, such as the AVR-Net and DyViSE.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.05730v2"
    },
    {
        "title": "Super-rays grouping scheme and novel coding architecture for\n  computational time reduction of graph-based Light Field coding",
        "authors": [
            "Bach Nguyen Gia",
            "Chanh Minh Tran",
            "Tho Nguyen Duc",
            "Tan Phan Xuan",
            "Eiji Kamioka"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Graph-based Light Field coding using the concept of super-rays is powerful to\nexploit signal redundancy along irregular shapes and achieves good energy\ncompaction, compared to rectangular block -based approaches. However, its main\nlimitation lies in the high time complexity for eigen-decomposition of each\nsuper-ray local graph, a high number of which can be found in a Light Field\nwhen segmented into super-rays. This paper examines a grouping scheme for\nsuper-rays in order to reduce the number of eigen-decomposition times, and\nproposes a novel coding architecture to handle the signal residual data arising\nfor each super-ray group, as a tradeoff to achieve lower computational time.\nExperimental results have shown to reduce a considerable amount of decoding\ntime for Light Field scenes, despite having a slight increase in the coding\nbitrates when compared with the original non-grouping super-ray -based\napproach. The proposal also remains to have competitive performance in Rate\nDistortion in comparison to HEVC-based and JPEG Pleno -based methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.05848v1"
    },
    {
        "title": "Probing Commonsense Reasoning Capability of Text-to-Image Generative\n  Models via Non-visual Description",
        "authors": [
            "Mianzhi Pan",
            "Jianfei Li",
            "Mingyue Yu",
            "Zheng Ma",
            "Kanzhi Cheng",
            "Jianbing Zhang",
            "Jiajun Chen"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Commonsense reasoning, the ability to make logical assumptions about daily\nscenes, is one core intelligence of human beings. In this work, we present a\nnovel task and dataset for evaluating the ability of text-to-image generative\nmodels to conduct commonsense reasoning, which we call PAINTaboo. Given a\ndescription with few visual clues of one object, the goal is to generate images\nillustrating the object correctly. The dataset was carefully hand-curated and\ncovered diverse object categories to analyze model performance comprehensively.\nOur investigation of several prevalent text-to-image generative models reveals\nthat these models are not proficient in commonsense reasoning, as anticipated.\nWe trust that PAINTaboo can improve our understanding of the reasoning\nabilities of text-to-image generative models.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.07294v2"
    },
    {
        "title": "MORE: A Multimodal Object-Entity Relation Extraction Dataset with a\n  Benchmark Evaluation",
        "authors": [
            "Liang He",
            "Hongke Wang",
            "Yongchang Cao",
            "Zhen Wu",
            "Jianbing Zhang",
            "Xinyu Dai"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Extracting relational facts from multimodal data is a crucial task in the\nfield of multimedia and knowledge graphs that feeds into widespread real-world\napplications. The emphasis of recent studies centers on recognizing relational\nfacts in which both entities are present in one modality and supplementary\ninformation is used from other modalities. However, such works disregard a\nsubstantial amount of multimodal relational facts that arise across different\nmodalities, such as one entity seen in a text and another in an image. In this\npaper, we propose a new task, namely Multimodal Object-Entity Relation\nExtraction, which aims to extract \"object-entity\" relational facts from image\nand text data. To facilitate research on this task, we introduce MORE, a new\ndataset comprising 21 relation types and 20,264 multimodal relational facts\nannotated on 3,559 pairs of textual news titles and corresponding images. To\nshow the challenges of Multimodal Object-Entity Relation Extraction, we\nevaluated recent state-of-the-art methods for multimodal relation extraction\nand conducted a comprehensive experimentation analysis on MORE. Our results\ndemonstrate significant challenges for existing methods, underlining the need\nfor further research on this task. Based on our experiments, we identify\nseveral promising directions for future research. The MORE dataset and code are\navailable at https://github.com/NJUNLP/MORE.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.09753v1"
    },
    {
        "title": "Statistical Analysis of Inter Coding in VVC Test Model (VTM)",
        "authors": [
            "Yiqun Liu",
            "Mohsen Abdoli",
            "Thomas Guionnet",
            "Christine Guillemot",
            "Aline Roumy"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The promising improvement in compression efficiency of Versatile Video Coding\n(VVC) compared to High Efficiency Video Coding (HEVC) comes at the cost of a\nnon-negligible encoder side complexity. The largely increased complexity\noverhead is a possible obstacle towards its industrial implementation. Many\npapers have proposed acceleration methods for VVC. Still, a better\nunderstanding of VVC complexity, especially related to new partitions and\ncoding tools, is desirable to help the design of new and better acceleration\nmethods. For this purpose, statistical analyses have been conducted, with a\nfocus on Coding Unit (CU) sizes and inter coding modes.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.10406v1"
    },
    {
        "title": "Low-Consumption Partial Transcoding by HEVC",
        "authors": [
            "Mohsen Abdoli",
            "Félix Henry",
            "Gordon Clare"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  A transcoding scheme for the High Efficiency Video Coding (HEVC) is proposed\nthat allows any partial frame modification to be followed by a partial\nre-compression of only the modified areas, while guaranteeing identical\nreconstruction of non-modified areas. To this end, first, syntax elements of\nall Coding Units (CU) in the frame are parsed and decoded according to their\nscan order. Then CUs that are collocated with a replaced area are re-encoded\nwith new content to generate a partial set of new syntax elements. In order to\navoid spatial propagation of the decoding mismatch due to the new content, CUs\non the border of the replaced area are losslessly coded such that\nreconstruction of immediately neighboring CUs in the scan order are protected\nfrom the modification. The proposed method has been implemented on top of the\nHEVC test Model (HM) in All-Intra (AI) coding configuration and experiments\nshow that, depending on the test parameters, it can offer both a bitrate saving\n(up to 4% in terms of BD-BR) and a transcoding acceleration (up to 83%)\ncompared to a full transcoding scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.12174v1"
    },
    {
        "title": "QoE modeling for Voice over IP: Simplified E-model Enhancement Utilizing\n  the Subjective MOS Prediction Model",
        "authors": [
            "Therdpong Daengsi",
            "Pongpisit Wuttidittachotti"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  This research proposes an enhanced measurement method for VoIP quality\nassessment which provides an improvement to accuracy and reliability. To\nimprove the objective measurement tool called the simplified E-model for the\nselected codec, G.729, it has been enhanced by utilizing a subjective MOS\nprediction model based on native Thai users, who use the Thai-tonal language.\nThen, the different results from the simplified E-model and subjective MOS\nprediction model were used to create the Bias function, before adding to the\nsimplified E-model. Finally, it has been found that the outputs from the\nenhanced simplified E-model for the G.729 codec shows better accuracy when\ncompared to the original simplified E-model, specially, after the enhanced\nmodel has been evaluated with 4 test sets. The major contribution of this\nenhancement is that errors are reduced by 58.87 % when compared to the generic\nsimplified E-model. That means the enhanced simplified E-model as proposed in\nthis study can provide improvement beyond the original simplified one\nsignificantly.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.15239v1"
    },
    {
        "title": "ITEACH-Net: Inverted Teacher-studEnt seArCH Network for Emotion\n  Recognition in Conversation",
        "authors": [
            "Haiyang Sun",
            "Zheng Lian",
            "Chenglong Wang",
            "Kang Chen",
            "Licai Sun",
            "Bin Liu",
            "Jianhua Tao"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  There remain two critical challenges that hinder the development of ERC.\nFirstly, there is a lack of exploration into mining deeper insights from the\ndata itself for conversational emotion tasks. Secondly, the systems exhibit\nvulnerability to random modality feature missing, which is a common occurrence\nin realistic settings. Focusing on these two key challenges, we propose a novel\nframework for incomplete multimodal learning in ERC, called \"Inverted\nTeacher-studEnt seArCH Network (ITEACH-Net).\" ITEACH-Net comprises two novel\ncomponents: the Emotion Context Changing Encoder (ECCE) and the Inverted\nTeacher-Student (ITS) framework. Specifically, leveraging the tendency for\nemotional states to exhibit local stability within conversational contexts,\nECCE captures these patterns and further perceives their evolution over time.\nRecognizing the varying challenges of handling incomplete versus complete data,\nITS employs a teacher-student framework to decouple the respective\ncomputations. Subsequently, through Neural Architecture Search, the student\nmodel develops enhanced computational capabilities for handling incomplete data\ncompared to the teacher model. During testing, we design a novel evaluation\nmethod, testing the model's performance under different missing rate conditions\nwithout altering the model weights. We conduct experiments on three benchmark\nERC datasets, and the results demonstrate that our ITEACH-Net outperforms\nexisting methods in incomplete multimodal ERC. We believe ITEACH-Net can\ninspire relevant research on the intrinsic nature of emotions within\nconversation scenarios and pave a more robust route for incomplete learning\ntechniques. Codes will be made available.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.15583v3"
    },
    {
        "title": "Cross-Modality and Within-Modality Regularization for Audio-Visual\n  DeepFake Detection",
        "authors": [
            "Heqing Zou",
            "Meng Shen",
            "Yuchen Hu",
            "Chen Chen",
            "Eng Siong Chng",
            "Deepu Rajan"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Audio-visual deepfake detection scrutinizes manipulations in public video\nusing complementary multimodal cues. Current methods, which train on fused\nmultimodal data for multimodal targets face challenges due to uncertainties and\ninconsistencies in learned representations caused by independent modality\nmanipulations in deepfake videos. To address this, we propose cross-modality\nand within-modality regularization to preserve modality distinctions during\nmultimodal representation learning. Our approach includes an audio-visual\ntransformer module for modality correspondence and a cross-modality\nregularization module to align paired audio-visual signals, preserving modality\ndistinctions. Simultaneously, a within-modality regularization module refines\nunimodal representations with modality-specific targets to retain\nmodal-specific details. Experimental results on the public audio-visual\ndataset, FakeAVCeleb, demonstrate the effectiveness and competitiveness of our\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05746v1"
    },
    {
        "title": "A Multi-Embedding Convergence Network on Siamese Architecture for Fake\n  Reviews",
        "authors": [
            "Sankarshan Dasgupta",
            "James Buckley"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this new digital era, accessibility to real-world events is moving towards\nweb-based modules. This is mostly visible on e-commerce websites where there is\nlimited availability of physical verification. With this unforeseen\ndevelopment, we depend on the verification in the virtual world to influence\nour decisions. One of the decision making process is deeply based on review\nreading. Reviews play an important part in this transactional process. And\nseeking a real review can be very tenuous work for the user. On the other hand,\nfake review heavily impacts these transaction records of a product. The article\npresents an implementation of a Siamese network for detecting fake reviews. The\nfake reviews dataset, consisting of 40K reviews, preprocessed with different\ntechniques. The cleaned data is passed through embeddings generated by MiniLM\nBERT for contextual relationship and Word2Vec for semantic relationship to form\nvectors. Further, the embeddings are trained in a Siamese network with LSTM\nlayers connected to fuzzy logic for decision-making. The results show that fake\nreviews can be detected with high accuracy on a siamese network for prediction\nand verification.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05995v1"
    },
    {
        "title": "Generative AI-enabled Mobile Tactical Multimedia Networks: Distribution,\n  Generation, and Perception",
        "authors": [
            "Minrui Xu",
            "Dusit Niyato",
            "Jiawen Kang",
            "Zehui Xiong",
            "Song Guo",
            "Yuguang Fang",
            "Dong In Kim"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Mobile multimedia networks (MMNs) demonstrate great potential in delivering\nlow-latency and high-quality entertainment and tactical applications, such as\nshort-video sharing, online conferencing, and battlefield surveillance. For\ninstance, in tactical surveillance of battlefields, scalability and\nsustainability are indispensable for maintaining large-scale military\nmultimedia applications in MMNs. Therefore, many data-driven networking\nsolutions are leveraged to optimize streaming strategies based on real-time\ntraffic analysis and resource monitoring. In addition, generative AI (GAI) can\nnot only increase the efficiency of existing data-driven solutions through data\naugmentation but also develop potential capabilities for MMNs, including\nAI-generated content (AIGC) and AI-aided perception. In this article, we\npropose the framework of GAI-enabled MMNs that leverage the capabilities of GAI\nin data and content synthesis to distribute high-quality and immersive\ninteractive content in wireless networks. Specifically, we outline the\nframework of GAI-enabled MMNs and then introduce its three main features,\nincluding distribution, generation, and perception. Furthermore, we propose a\nsecond-score auction mechanism for allocating network resources by considering\nGAI model values and other metrics jointly. The experimental results show that\nthe proposed auction mechanism can effectively increase social welfare by\nallocating resources and models with the highest user satisfaction.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.06386v1"
    },
    {
        "title": "Startup Delay Aware Short Video Ordering: Problem, Model, and A\n  Reinforcement Learning based Algorithm",
        "authors": [
            "Zhipeng Gao",
            "Chunxi Li",
            "Yongxiang Zhao",
            "Baoxian Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Short video applications have attracted billions of users on the Internet and\ncan satisfy diverse users' fragmented spare time with content-rich and\nduration-short videos. To achieve fast playback at user side, existing short\nvideo systems typically enforce burst transmission of initial segment of each\nvideo when being requested for improved quality of user experiences. However,\nsuch a way of burst transmissions can cause unexpected large startup delays at\nuser side. This is because users may frequently switch videos when sequentially\nwatching a list of short videos recommended by the server side, which can cause\nexcessive burst transmissions of initial segments of different short videos and\nthus quickly deplete the network transmission capacity. In this paper, we adopt\ntoken bucket to characterize the video transmission path between video server\nand each user, and accordingly study how to effectively reduce the startup\ndelay of short videos by effectively arranging the viewing order of a video\nlist at the server side. We formulate the optimal video ordering problem for\nminimizing the maximum video startup delay as a combinatorial optimization\nproblem and prove its NP-hardness. We accordingly propose a Partially Shared\nActor Critic reinforcement learning algorithm (PSAC) to learn optimized video\nordering strategy. Numerical results based on a real dataset provided by a\nlarge-scale short video service provider demonstrate that the proposed PSAC\nalgorithm can significantly reduce the video startup delay compared to baseline\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.07411v1"
    },
    {
        "title": "CLIPRerank: An Extremely Simple Method for Improving Ad-hoc Video Search",
        "authors": [
            "Aozhu Chen",
            "Fangming Zhou",
            "Ziyuan Wang",
            "Xirong Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Ad-hoc Video Search (AVS) enables users to search for unlabeled video content\nusing on-the-fly textual queries. Current deep learning-based models for AVS\nare trained to optimize holistic similarity between short videos and their\nassociated descriptions. However, due to the diversity of ad-hoc queries, even\nfor a short video, its truly relevant part w.r.t. a given query can be of\nshorter duration. In such a scenario, the holistic similarity becomes\nsuboptimal. To remedy the issue, we propose in this paper CLIPRerank, a\nfine-grained re-scoring method. We compute cross-modal similarities between\nquery and video frames using a pre-trained CLIP model, with multi-frame scores\naggregated by max pooling. The fine-grained score is weightedly added to the\ninitial score for search result reranking. As such, CLIPRerank is agnostic to\nthe underlying video retrieval models and extremely simple, making it a handy\nplug-in for boosting AVS. Experiments on the challenging TRECVID AVS benchmarks\n(from 2016 to 2021) justify the effectiveness of the proposed strategy.\nCLIPRerank consistently improves the TRECVID top performers and multiple\nexisting models including SEA, W2VV++, Dual Encoding, Dual Task, LAFF,\nCLIP2Video, TS2-Net and X-CLIP. Our method also works when substituting BLIP-2\nfor CLIP.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.08449v1"
    },
    {
        "title": "A Survey on Energy Consumption and Environmental Impact of Video\n  Streaming",
        "authors": [
            "Samira Afzal",
            "Narges Mehran",
            "Zoha Azimi Ourimi",
            "Farzad Tashtarian",
            "Hadi Amirpour",
            "Radu Prodan",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Climate change challenges require a notable decrease in worldwide greenhouse\ngas (GHG) emissions across technology sectors. Digital technologies, especially\nvideo streaming, accounting for most Internet traffic, make no exception. Video\nstreaming demand increases with remote working, multimedia communication\nservices (e.g., WhatsApp, Skype), video streaming content (e.g., YouTube,\nNetflix), video resolution (4K/8K, 50 fps/60 fps), and multi-view video, making\nenergy consumption and environmental footprint critical. This survey\ncontributes to a better understanding of sustainable and efficient video\nstreaming technologies by providing insights into the state-of-the-art and\npotential future directions for researchers, developers, and engineers, service\nproviders, hosting platforms, and consumers. We widen this survey's focus on\ncontent provisioning and content consumption based on the observation that\ncontinuously active network equipment underneath video streaming consumes\nsubstantial energy independent of the transmitted data type. We propose a\ntaxonomy of factors that affect the energy consumption in video streaming, such\nas encoding schemes, resource requirements, storage, content retrieval,\ndecoding, and display. We identify notable weaknesses in video streaming that\nrequire further research for improved energy efficiency: (1) fixed bitrate\nladders in HTTP live streaming; (2) inefficient hardware utilization of\nexisting video players; (3) lack of comprehensive open energy measurement\ndataset covering various device types and coding parameters for reproducible\nresearch.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.09854v1"
    },
    {
        "title": "Identity-Driven Multimedia Forgery Detection via Reference Assistance",
        "authors": [
            "Junhao Xu",
            "Jingjing Chen",
            "Xue Song",
            "Feng Han",
            "Haijun Shan",
            "Yugang Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recent advancements in \"deepfake\" techniques have paved the way for\ngenerating various media forgeries. In response to the potential hazards of\nthese media forgeries, many researchers engage in exploring detection methods,\nincreasing the demand for high-quality media forgery datasets. Despite this,\nexisting datasets have certain limitations. Firstly, most datasets focus on\nmanipulating visual modality and usually lack diversity, as only a few forgery\napproaches are considered. Secondly, the quality of media is often inadequate\nin clarity and naturalness. Meanwhile, the size of the dataset is also limited.\nThirdly, it is commonly observed that real-world forgeries are motivated by\nidentity, yet the identity information of the individuals portrayed in these\nforgeries within existing datasets remains under-explored. For detection,\nidentity information could be an essential clue to boost performance. Moreover,\nofficial media concerning relevant identities on the Internet can serve as\nprior knowledge, aiding both the audience and forgery detectors in determining\nthe true identity. Therefore, we propose an identity-driven multimedia forgery\ndataset, IDForge, which contains 249,138 video shots sourced from 324 wild\nvideos of 54 celebrities collected from the Internet. The fake video shots\ninvolve 9 types of manipulation across visual, audio, and textual modalities.\nAdditionally, IDForge provides extra 214,438 real video shots as a reference\nset for the 54 celebrities. Correspondingly, we propose the Reference-assisted\nMultimodal Forgery Detection Network (R-MFDN), aiming at the detection of\ndeepfake videos. Through extensive experiments on the proposed dataset, we\ndemonstrate the effectiveness of R-MFDN on the multimedia detection task.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.11764v2"
    },
    {
        "title": "MInD: Improving Multimodal Sentiment Analysis via Multimodal Information\n  Disentanglement",
        "authors": [
            "Weichen Dai",
            "Xingyu Li",
            "Zeyu Wang",
            "Pengbo Hu",
            "Ji Qi",
            "Jianlin Peng",
            "Yi Zhou"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Learning effective joint representations has been a central task in\nmulti-modal sentiment analysis. Previous works addressing this task focus on\nexploring sophisticated fusion techniques to enhance performance. However, the\ninherent heterogeneity of distinct modalities remains a core problem that\nbrings challenges in fusing and coordinating the multi-modal signals at both\nthe representational level and the informational level, impeding the full\nexploitation of multi-modal information. To address this problem, we propose\nthe Multi-modal Information Disentanglement (MInD) method, which decomposes the\nmulti-modal inputs into modality-invariant and modality-specific components\nthrough a shared encoder and multiple private encoders. Furthermore, by\nexplicitly training generated noise in an adversarial manner, MInD is able to\nisolate uninformativeness, thus improves the learned representations.\nTherefore, the proposed disentangled decomposition allows for a fusion process\nthat is simpler than alternative methods and results in improved performance.\nExperimental evaluations conducted on representative benchmark datasets\ndemonstrate MInD's effectiveness in both multi-modal emotion recognition and\nmulti-modal humor detection tasks. Code will be released upon acceptance of the\npaper.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.11818v2"
    },
    {
        "title": "Perceptual-oriented Learned Image Compression with Dynamic Kernel",
        "authors": [
            "Nianxiang Fu",
            "Junxi Zhang",
            "Huairui Wang",
            "Zhenzhong Chen"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we extend our prior research named DKIC and propose the\nperceptual-oriented learned image compression method, PO-DKIC. Specifically,\nDKIC adopts a dynamic kernel-based dynamic residual block group to enhance the\ntransform coding and an asymmetric space-channel context entropy model to\nfacilitate the estimation of gaussian parameters. Based on DKIC, PO-DKIC\nintroduces PatchGAN and LPIPS loss to enhance visual quality. Furthermore, to\nmaximize the overall perceptual quality under a rate constraint, we formulate\nthis challenge into a constrained programming problem and use the Linear\nInteger Programming method for resolution. The experiments demonstrate that our\nproposed method can generate realistic images with richer textures and finer\ndetails when compared to state-of-the-art image compression techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.13967v3"
    },
    {
        "title": "Optimal Quality and Efficiency in Adaptive Live Streaming with JND-Aware\n  Low latency Encoding",
        "authors": [
            "Vignesh V Menon",
            "Jingwen Zhu",
            "Prajit T Rajendran",
            "Samira Afzal",
            "Klaus Schoeffmann",
            "Patrick Le Callet",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In HTTP adaptive live streaming applications, video segments are encoded at a\nfixed set of bitrate-resolution pairs known as bitrate ladder. Live encoders\nuse the fastest available encoding configuration, referred to as preset, to\nensure the minimum possible latency in video encoding. However, an optimized\npreset and optimized number of CPU threads for each encoding instance may\nresult in (i) increased quality and (ii) efficient CPU utilization while\nencoding. For low latency live encoders, the encoding speed is expected to be\nmore than or equal to the video framerate. To this light, this paper introduces\na Just Noticeable Difference (JND)-Aware Low latency Encoding Scheme (JALE),\nwhich uses random forest-based models to jointly determine the optimized\nencoder preset and thread count for each representation, based on video\ncomplexity features, the target encoding speed, the total number of available\nCPU threads, and the target encoder. Experimental results show that, on\naverage, JALE yield a quality improvement of 1.32 dB PSNR and 5.38 VMAF points\nwith the same bitrate, compared to the fastest preset encoding of the HTTP Live\nStreaming (HLS) bitrate ladder using x265 HEVC open-source encoder with eight\nCPU threads used for each representation. These enhancements are achieved while\nmaintaining the desired encoding speed. Furthermore, on average, JALE results\nin an overall storage reduction of 72.70 %, a reduction in the total number of\nCPU threads used by 63.83 %, and a 37.87 % reduction in the overall encoding\ntime, considering a JND of six VMAF points.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15343v1"
    },
    {
        "title": "Energy-efficient Adaptive Video Streaming with Latency-Aware Dynamic\n  Resolution Encoding",
        "authors": [
            "Vignesh V Menon",
            "Amritha Premkumar",
            "Prajit T Rajendran",
            "Adam Wieckowski",
            "Benjamin Bross",
            "Christian Timmerer",
            "Detlev Marpe"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Traditional per-title encoding schemes aim to optimize encoding resolutions\nto deliver the highest perceptual quality for each representation. However,\nkeeping the encoding time within an acceptable threshold for a smooth user\nexperience is important to reduce the carbon footprint and energy consumption\non encoding servers in video streaming applications. Toward this realization,\nwe introduce an encoding latency-a ware dynamic resolution encoding scheme\n(LADRE) for adaptive video streaming applications. LADRE determines the\nencoding resolution for each target bitrate by utilizing a random forest-based\nprediction model for every video segment based on spatiotemporal features and\nthe acceptable target latency. Experimental results show that LADRE achieves an\noverall average quality improvement of 0.58 dB PSNR and 0.43 dB XPSNR while\nmaintaining the same bitrate, compared to the HTTP Live Streaming (HLS) bitrate\nladder encoding of 200 s segments using the VVenC encoder, when the encoding\nlatency for each representation is set to remain below the 200 s threshold.\nThis is accompanied by an 84.17 % reduction in overall encoding energy\nconsumption.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15346v1"
    },
    {
        "title": "Smart Fitting Room: A One-stop Framework for Matching-aware Virtual\n  Try-on",
        "authors": [
            "Mingzhe Yu",
            "Yunshan Ma",
            "Lei Wu",
            "Kai Cheng",
            "Xue Li",
            "Lei Meng",
            "Tat-Seng Chua"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The development of virtual try-on has revolutionized online shopping by\nallowing customers to visualize themselves in various fashion items, thus\nextending the in-store try-on experience to the cyber space. Although virtual\ntry-on has attracted considerable research initiatives, existing systems only\nfocus on the quality of image generation, overlooking whether the fashion item\nis a good match to the given person and clothes. Recognizing this gap, we\npropose to design a one-stop Smart Fitting Room, with the novel formulation of\nmatching-aware virtual try-on. Following this formulation, we design a Hybrid\nMatching-aware Virtual Try-On Framework (HMaVTON), which combines\nretrieval-based and generative methods to foster a more personalized virtual\ntry-on experience. This framework integrates a hybrid mix-and-match module and\nan enhanced virtual try-on module. The former can recommend fashion items\navailable on the platform to boost sales and generate clothes that meets the\ndiverse tastes of consumers. The latter provides high-quality try-on effects,\ndelivering a one-stop shopping service. To validate the effectiveness of our\napproach, we enlist the expertise of fashion designers for a professional\nevaluation, assessing the rationality and diversity of the clothes combinations\nand conducting an evaluation matrix analysis. Our method significantly enhances\nthe practicality of virtual try-on. The code is available at\nhttps://github.com/Yzcreator/HMaVTON.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.16825v2"
    },
    {
        "title": "Gain of Grain: A Film Grain Handling Toolchain for VVC-based Open\n  Implementations",
        "authors": [
            "Vignesh V Menon",
            "Adam Wieckowski",
            "Jens Brandenburg",
            "Benjamin Bross",
            "Thomas Schierl",
            "Detlev Marpe"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Film grain is a distinctive visual characteristic cherished by filmmakers and\ncinephiles for its ability to evoke nostalgia and artistic aesthetics. However,\nfaithful preservation of film grain during encoding poses unique challenges.\nFilm grain introduces random noise, complicating traditional compression\ntechniques. Consequently, specialized algorithms and encoding strategies have\nemerged, aiming to strike a harmonious equilibrium. This paper delves into the\nnuanced realm of film grain handling in Versatile Video Coding (VVC) encoding.\nWe explore the delicate balance between retaining the cinematic charm of film\ngrain and achieving efficient compression. Moreover, we discuss the importance\nof perceptual quality assessment and adaptive encoding techniques in preserving\nfilm grain fidelity. Additionally, we delve into the impact of film grain\nhandling on bitrate control and compression efficiency using VVenC, an open and\noptimized VVC encoder. Understanding the role of film grain and its nuanced\ntreatment within encoders becomes increasingly pivotal for delivering\nhigh-quality, grain-inclusive content in the digital age.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00622v1"
    },
    {
        "title": "Video Super-Resolution for Optimized Bitrate and Green Online Streaming",
        "authors": [
            "Vignesh V Menon",
            "Prajit T Rajendran",
            "Amritha Premkumar",
            "Benjamin Bross",
            "Detlev Marpe"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Conventional per-title encoding schemes strive to optimize encoding\nresolutions to deliver the utmost perceptual quality for each bitrate ladder\nrepresentation. Nevertheless, maintaining encoding time within an acceptable\nthreshold is equally imperative in online streaming applications. Furthermore,\nmodern client devices are equipped with the capability for fast\ndeep-learning-based video super-resolution (VSR) techniques, enhancing the\nperceptual quality of the decoded bitstream. This suggests that opting for\nlower resolutions in representations during the encoding process can curtail\nthe overall energy consumption without substantially compromising perceptual\nquality. In this context, this paper introduces a video super-resolution-based\nlatency-aware optimized bitrate encoding scheme (ViSOR) designed for online\nadaptive streaming applications. ViSOR determines the encoding resolution for\neach target bitrate, ensuring the highest achievable perceptual quality after\nVSR within the bound of a maximum acceptable latency. Random forest-based\nprediction models are trained to predict the perceptual quality after VSR and\nthe encoding time for each resolution using the spatiotemporal features\nextracted for each video segment. Experimental results show that ViSOR\ntargeting fast super-resolution convolutional neural network (FSRCNN) achieves\nan overall average bitrate reduction of 24.65 % and 32.70 % to maintain the\nsame PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder\nencoding of 4 s segments using the x265 encoder, when the maximum acceptable\nlatency for each representation is set as two seconds. Considering a just\nnoticeable difference (JND) of six VMAF points, the average cumulative storage\nconsumption and encoding energy for each segment is reduced by 79.32 % and\n68.21 %, respectively, contributing towards greener streaming.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.03513v1"
    },
    {
        "title": "BioNet-XR: Biological Network Visualization Framework for Virtual\n  Reality and Mixed Reality Environments",
        "authors": [
            "Busra Senderin",
            "Nurcan Tuncbag",
            "Elif Surer"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Protein-protein interaction networks (PPIN) enable the study of cellular\nprocesses in organisms. Visualizing PPINs in extended reality (XR), including\nvirtual reality (VR) and mixed reality (MR), is crucial for exploring\nsubnetworks, evaluating protein positions, and collaboratively analyzing and\ndiscussing on networks with the help of recent technological advancements.\nHere, we present BioNet-XR, a 3D visualization framework, to visualize PPINs in\nVR and MR environments. BioNet-XR was developed with the Unity3D game engine.\nOur framework provides state-of-the-art methods and visualization features\nincluding teleportation between nodes, general and first-person view to explore\nthe network, subnetwork construction via PageRank, Steiner tree, and all-pair\nshortest path algorithms for a given set of initial nodes. We used usability\ntests to gather feedback from both specialists (bioinformaticians) and\ngeneralists (multidisciplinary groups), addressing the need for usability\nevaluations of visualization tools. In the MR version of BioNet-XR, users can\nseamlessly transition to real-world environments and interact with protein\ninteraction networks. BioNet-XR is highly modular and adaptable for\nvisualization of other biological networks, such as metabolic and regulatory\nnetworks, and extension with additional network methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.03946v1"
    },
    {
        "title": "Reducing Latency for Multimedia Broadcast Services Over Mobile Networks",
        "authors": [
            "C. M. Lentisco",
            "L. Bellido",
            "A. Cárdenas",
            "R. F. Moyano",
            "D. Fernández"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimedia services over mobile networks pose several challenges, such as the\nefficient management of radio resources or the latency induced by network\ndelays and buffering requirements on the multimedia players. In Long Term\nEvolution (LTE) networks, the definition of multimedia broadcast services over\na common radio channel addresses the shortage of radio resources but introduces\nthe problem of network error recovery. In order to address network errors on\nLTE multimedia broadcast services, the current standards propose the combined\nuse of forward error correction and unicast recovery techniques at the\napplication level. This paper shows how to efficiently synchronize the\nbroadcasting server and the multimedia players and how to reduce service\nlatency by limiting the multimedia player buffer length. This is accomplished\nby analyzing the relation between the different parameters of the LTE\nmultimedia broadcast service, the multimedia player buffer length, and service\ninterruptions. A case study is simulated to confirm how the quality of the\nmultimedia service is improved by applying our proposals.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.06424v1"
    },
    {
        "title": "Design of a 5G Multimedia Broadcast Application Function Supporting\n  Adaptive Error Recovery",
        "authors": [
            "C. M. Lentisco",
            "L. Bellido",
            "A. Cárdenas",
            "R. F. Moyano",
            "D. Fernández"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The demand for mobile multimedia streaming services has been steadily growing\nin recent years. Mobile multimedia broadcasting addresses the shortage of radio\nresources but introduces a network error recovery problem. Retransmitting\nmultimedia segments that are not correctly broadcast can cause service\ndisruptions and increased service latency, affecting the quality of experience\nperceived by end users. With the advent of networking paradigms based on\nvirtualization technologies, mobile networks have been enabled with more\nflexibility and agility to deploy innovative services that improve the\nutilization of available network resources. This paper discusses how mobile\nmultimedia broadcast services can be designed to prevent service degradation by\nusing the computing capabilities provided by multiaccess edge computing (MEC)\nplatforms in the context of a 5G network architecture. An experimental platform\nhas been developed to evaluate the feasibility of a MEC application to provide\nadaptive error recovery for multimedia broadcast services. The results of the\nexperiments carried out show that the proposal provides a flexible mechanism\nthat can be deployed at the network edge to lower the impact of transmission\nerrors on latency and service disruptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.06437v1"
    },
    {
        "title": "Think before You Leap: Content-Aware Low-Cost Edge-Assisted Video\n  Semantic Segmentation",
        "authors": [
            "Mingxuan Yan",
            "Yi Wang",
            "Xuedou Xiao",
            "Zhiqing Luo",
            "Jianhua He",
            "Wei Wang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Offloading computing to edge servers is a promising solution to support\ngrowing video understanding applications at resource-constrained IoT devices.\nRecent efforts have been made to enhance the scalability of such systems by\nreducing inference costs on edge servers. However, existing research is not\ndirectly applicable to pixel-level vision tasks such as video semantic\nsegmentation (VSS), partly due to the fluctuating VSS accuracy and segment\nbitrate caused by the dynamic video content. In response, we present Penance, a\nnew edge inference cost reduction framework. By exploiting softmax outputs of\nVSS models and the prediction mechanism of H.264/AVC codecs, Penance optimizes\nmodel selection and compression settings to minimize the inference cost while\nmeeting the required accuracy within the available bandwidth constraints. We\nimplement Penance in a commercial IoT device with only CPUs. Experimental\nresults show that Penance consumes a negligible 6.8% more computation resources\nthan the optimal strategy while satisfying accuracy and bandwidth constraints\nwith a low failure rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.14326v2"
    },
    {
        "title": "Multimodal Interaction Modeling via Self-Supervised Multi-Task Learning\n  for Review Helpfulness Prediction",
        "authors": [
            "HongLin Gong",
            "Mengzhao Jia",
            "Liqiang Jing"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In line with the latest research, the task of identifying helpful reviews\nfrom a vast pool of user-generated textual and visual data has become a\nprominent area of study. Effective modal representations are expected to\npossess two key attributes: consistency and differentiation. Current methods\ndesigned for Multimodal Review Helpfulness Prediction (MRHP) face limitations\nin capturing distinctive information due to their reliance on uniform\nmultimodal annotation. The process of adding varied multimodal annotations is\nnot only time-consuming but also labor-intensive. To tackle these challenges,\nwe propose an auto-generated scheme based on multi-task learning to generate\npseudo labels. This approach allows us to simultaneously train for the global\nmultimodal interaction task and the separate cross-modal interaction subtasks,\nenabling us to learn and leverage both consistency and differentiation\neffectively. Subsequently, experimental results validate the effectiveness of\npseudo labels, and our approach surpasses previous textual and multimodal\nbaseline models on two widely accessible benchmark datasets, providing a\nsolution to the MRHP problem.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.18107v2"
    },
    {
        "title": "Towards Alleviating Text-to-Image Retrieval Hallucination for CLIP in\n  Zero-shot Learning",
        "authors": [
            "Hanyao Wang",
            "Yibing Zhan",
            "Liu Liu",
            "Liang Ding",
            "Yan Yang",
            "Jun Yu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Pretrained cross-modal models, for instance, the most representative CLIP,\nhave recently led to a boom in using pre-trained models for cross-modal\nzero-shot tasks, considering the generalization properties. However, we\nanalytically discover that CLIP suffers from the text-to-image retrieval\nhallucination, adversely limiting its capabilities under zero-shot learning:\nCLIP would select the image with the highest score when asked to figure out\nwhich image perfectly matches one given query text among several candidate\nimages even though CLIP knows contents in the image. Accordingly, we propose a\nBalanced Score with Auxiliary Prompts (BSAP) to mitigate the CLIP's\ntext-to-image retrieval hallucination under zero-shot learning. Specifically,\nwe first design auxiliary prompts to provide multiple reference outcomes for\nevery single image retrieval, then the outcomes derived from each retrieved\nimage in conjunction with the target text are normalized to obtain the final\nsimilarity, which alleviates hallucinations in the model. Additionally, we can\nmerge CLIP's original results and BSAP to obtain a more robust hybrid outcome\n(BSAP-H). Extensive experiments on two typical zero-shot learning tasks, i.e.,\nReferring Expression Comprehension (REC) and Referring Image Segmentation\n(RIS), are conducted to demonstrate the effectiveness of our BSAP.\nSpecifically, when evaluated on the validation dataset of RefCOCO in REC, BSAP\nincreases CLIP's performance by 20.6%. Further, we validate that our strategy\ncould be applied in other types of pretrained cross-modal models, such as ALBEF\nand BLIP.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.18400v2"
    },
    {
        "title": "Characterizing Multimedia Information Environment through Multi-modal\n  Clustering of YouTube Videos",
        "authors": [
            "Niloofar Yousefi",
            "Mainuddin Shaik",
            "Nitin Agarwal"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This study aims to investigate the comprehensive characterization of\ninformation content in multimedia (videos), particularly on YouTube. The\nresearch presents a multi-method framework for characterizing multimedia\ncontent by clustering signals from various modalities, such as audio, video,\nand text. With a focus on South China Sea videos as a case study, this approach\naims to enhance our understanding of online content, especially on YouTube. The\ndataset includes 160 videos, and our findings offer insights into content\nthemes and patterns within different modalities of a video based on clusters.\nText modality analysis revealed topical themes related to geopolitical\ncountries, strategies, and global security, while video and audio modality\nanalysis identified distinct patterns of signals related to diverse sets of\nvideos, including news analysis/reporting, educational content, and interviews.\nFurthermore, our findings uncover instances of content repurposing within video\nclusters, which were identified using the barcode technique and audio\nsimilarity assessments. These findings indicate potential content amplification\ntechniques. In conclusion, this study uniquely enhances our current\nunderstanding of multimedia content information based on modality clustering\ntechniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.18702v1"
    },
    {
        "title": "MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model",
        "authors": [
            "Sen Wang",
            "Jiangning Zhang",
            "Xin Tan",
            "Zhifeng Xie",
            "Chengjie Wang",
            "Lizhuang Ma"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The body movements accompanying speech aid speakers in expressing their\nideas. Co-speech motion generation is one of the important approaches for\nsynthesizing realistic avatars. Due to the intricate correspondence between\nspeech and motion, generating realistic and diverse motion is a challenging\ntask. In this paper, we propose MMoFusion, a Multi-modal co-speech Motion\ngeneration framework based on the diffusion model to ensure both the\nauthenticity and diversity of generated motion. We propose a progressive fusion\nstrategy to enhance the interaction of inter-modal and intra-modal, efficiently\nintegrating multi-modal information. Specifically, we employ a masked style\nmatrix based on emotion and identity information to control the generation of\ndifferent motion styles. Temporal modeling of speech and motion is partitioned\ninto style-guided specific feature encoding and shared feature encoding, aiming\nto learn both inter-modal and intra-modal features. Besides, we propose a\ngeometric loss to enforce the joints' velocity and acceleration coherence among\nframes. Our framework generates vivid, diverse, and style-controllable motion\nof arbitrary length through inputting speech and editing identity and emotion.\nExtensive experiments demonstrate that our method outperforms current co-speech\nmotion generation methods including upper body and challenging full body.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.02905v3"
    },
    {
        "title": "Reply with Sticker: New Dataset and Model for Sticker Retrieval",
        "authors": [
            "Bin Liang",
            "Bingbing Wang",
            "Zhixin Bai",
            "Qiwei Lang",
            "Mingwei Sun",
            "Kaiheng Hou",
            "Lanjun Zhou",
            "Ruifeng Xu",
            "Kam-Fai Wong"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Using stickers in online chatting is very prevalent on social media\nplatforms, where the stickers used in the conversation can express someone's\nintention/emotion/attitude in a vivid, tactful, and intuitive way. Existing\nsticker retrieval research typically retrieves stickers based on context and\nthe current utterance delivered by the user. That is, the stickers serve as a\nsupplement to the current utterance. However, in the real-world scenario, using\nstickers to express what we want to say rather than as a supplement to our\nwords only is also important. Therefore, in this paper, we create a new dataset\nfor sticker retrieval in conversation, called \\textbf{StickerInt}, where\nstickers are used to reply to previous conversations or supplement our\nwords\\footnote{We believe that the release of this dataset will provide a more\ncomplete paradigm than existing work for the research of sticker retrieval in\nthe open-domain online conversation.}. Based on the created dataset, we present\na simple yet effective framework for sticker retrieval in conversation based on\nthe learning of intention and the cross-modal relationships between\nconversation context and stickers, coined as \\textbf{Int-RA}. Specifically, we\nfirst devise a knowledge-enhanced intention predictor to introduce the\nintention information into the conversation representations. Subsequently, a\nrelation-aware sticker selector is devised to retrieve the response sticker via\ncross-modal relationships. Extensive experiments on the created dataset show\nthat the proposed model achieves state-of-the-art performance in sticker\nretrieval\\footnote{The dataset and source code of this work are released at\n\\url{https://github.com/HITSZ-HLT/Int-RA}.}.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.05427v3"
    },
    {
        "title": "Towards Real-World Stickers Use: A New Dataset for Multi-Tag Sticker\n  Recognition",
        "authors": [
            "Bingbing Wang",
            "Bin Liang",
            "Chun-Mei Feng",
            "Wangmeng Zuo",
            "Zhixin Bai",
            "Shijue Huang",
            "Kam-Fai Wong",
            "Xi Zeng",
            "Ruifeng Xu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In real-world conversations, the diversity and ambiguity of stickers often\nlead to varied interpretations based on the context, necessitating the\nrequirement for comprehensively understanding stickers and supporting\nmulti-tagging. To address this challenge, we introduce StickerTAG, the first\nmulti-tag sticker dataset comprising a collected tag set with 461 tags and\n13,571 sticker-tag pairs, designed to provide a deeper understanding of\nstickers. Recognizing multiple tags for stickers becomes particularly\nchallenging due to sticker tags usually are fine-grained attribute aware.\nHence, we propose an Attentive Attribute-oriented Prompt Learning method, ie,\nAtt$^2$PL, to capture informative features of stickers in a fine-grained manner\nto better differentiate tags. Specifically, we first apply an\nAttribute-oriented Description Generation (ADG) module to obtain the\ndescription for stickers from four attributes. Then, a Local Re-attention (LoR)\nmodule is designed to perceive the importance of local information. Finally, we\nuse prompt learning to guide the recognition process and adopt confidence\npenalty optimization to penalize the confident output distribution. Extensive\nexperiments show that our method achieves encouraging results for all commonly\nused metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.05428v3"
    },
    {
        "title": "Deep Bi-directional Attention Network for Image Super-Resolution Quality\n  Assessment",
        "authors": [
            "Yixiao Li",
            "Xiaoyuan Yang",
            "Jun Fu",
            "Guanghui Yue",
            "Wei Zhou"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  There has emerged a growing interest in exploring efficient quality\nassessment algorithms for image super-resolution (SR). However, employing deep\nlearning techniques, especially dual-branch algorithms, to automatically\nevaluate the visual quality of SR images remains challenging. Existing SR image\nquality assessment (IQA) metrics based on two-stream networks lack interactions\nbetween branches. To address this, we propose a novel full-reference IQA\n(FR-IQA) method for SR images. Specifically, producing SR images and evaluating\nhow close the SR images are to the corresponding HR references are separate\nprocesses. Based on this consideration, we construct a deep Bi-directional\nAttention Network (BiAtten-Net) that dynamically deepens visual attention to\ndistortions in both processes, which aligns well with the human visual system\n(HVS). Experiments on public SR quality databases demonstrate the superiority\nof our proposed BiAtten-Net over state-of-the-art quality assessment methods.\nIn addition, the visualization results and ablation study show the\neffectiveness of bi-directional attention.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.10406v2"
    },
    {
        "title": "Quality-Aware Dynamic Resolution Adaptation Framework for Adaptive Video\n  Streaming",
        "authors": [
            "Amritha Premkumar",
            "Prajit T Rajendran",
            "Vignesh V Menon",
            "Adam Wieckowski",
            "Benjamin Bross",
            "Detlev Marpe"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Traditional per-title encoding schemes aim to optimize encoding resolutions\nto deliver the highest perceptual quality for each representation. XPSNR is\nobserved to correlate better with the subjective quality of VVC-coded\nbitstreams. Towards this realization, we predict the average XPSNR of VVC-coded\nbitstreams using spatiotemporal complexity features of the video and the target\nencoding configuration using an XGBoost-based model. Based on the predicted\nXPSNR scores, we introduce a Quality-A ware Dynamic Resolution Adaptation\n(QADRA) framework for adaptive video streaming applications, where we determine\nthe convex-hull online. Furthermore, keeping the encoding and decoding times\nwithin an acceptable threshold is mandatory for smooth and energy-efficient\nstreaming. Hence, QADRA determines the encoding resolution and quantization\nparameter (QP) for each target bitrate by maximizing XPSNR while constraining\nthe maximum encoding and/ or decoding time below a threshold. QADRA implements\na JND-based representation elimination algorithm to remove perceptually\nredundant representations from the bitrate ladder. QADRA is an open-source\nPython-based framework published under the GNU GPLv3 license. Github:\nhttps://github.com/PhoenixVideo/QADRA Online documentation:\nhttps://phoenixvideo.github.io/QADRA/\n",
        "pdf_link": "http://arxiv.org/pdf/2403.10976v1"
    },
    {
        "title": "Fidelity-preserving Learning-Based Image Compression: Loss Function and\n  Subjective Evaluation Methodology",
        "authors": [
            "Shima Mohammadi",
            "Yaojun Wu",
            "João Ascenso"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Learning-based image compression methods have emerged as state-of-the-art,\nshowcasing higher performance compared to conventional compression solutions.\nThese data-driven approaches aim to learn the parameters of a neural network\nmodel through iterative training on large amounts of data. The optimization\nprocess typically involves minimizing the distortion between the decoded and\nthe original ground truth images. This paper focuses on perceptual optimization\nof learning-based image compression solutions and proposes: i) novel loss\nfunction to be used during training and ii) novel subjective test methodology\nthat aims to evaluate the decoded image fidelity. According to experimental\nresults from the subjective test taken with the new methodology, the\noptimization procedure can enhance image quality for low-rates while offering\nno advantage for high-rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.11241v1"
    },
    {
        "title": "Virbo: Multimodal Multilingual Avatar Video Generation in Digital\n  Marketing",
        "authors": [
            "Juan Zhang",
            "Jiahao Chen",
            "Cheng Wang",
            "Zhiwang Yu",
            "Tangquan Qi",
            "Can Liu",
            "Di Wu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  With the widespread popularity of internet celebrity marketing all over the\nworld, short video production has gradually become a popular way of presenting\nproducts information. However, the traditional video production industry\nusually includes series of procedures as script writing, video filming in a\nprofessional studio, video clipping, special effects rendering, customized\npost-processing, and so forth. Not to mention that multilingual videos is not\naccessible for those who could not speak multilingual languages. These\ncomplicated procedures usually needs a professional team to complete, and this\nmade short video production costly in both time and money. This paper presents\nan intelligent system that supports the automatic generation of talking avatar\nvideos, namely Virbo. With simply a user-specified script, Virbo could use a\ndeep generative model to generate a target talking videos. Meanwhile, the\nsystem also supports multimodal inputs to customize the video with specified\nface, specified voice and special effects. This system also integrated a\nmultilingual customization module that supports generate multilingual talking\navatar videos in a batch with hundreds of delicate templates and creative\nspecial effects. Through a series of user studies and demo tests, we found that\nVirbo can generate talking avatar videos that maintained a high quality of\nvideos as those from a professional team while reducing the entire production\ncosts significantly. This intelligent system will effectively promote the video\nproduction industry and facilitate the internet marketing neglecting of\nlanguage barriers and cost challenges.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.11700v2"
    },
    {
        "title": "PiGW: A Plug-in Generative Watermarking Framework",
        "authors": [
            "Rui Ma",
            "Mengxi Guo",
            "Li Yuming",
            "Hengyuan Zhang",
            "Cong Ma",
            "Yuan Li",
            "Xiaodong Xie",
            "Shanghang Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Integrating watermarks into generative images is a critical strategy for\nprotecting intellectual property and enhancing artificial intelligence\nsecurity. This paper proposes Plug-in Generative Watermarking (PiGW) as a\ngeneral framework for integrating watermarks into generative images. More\nspecifically, PiGW embeds watermark information into the initial noise using a\nlearnable watermark embedding network and an adaptive frequency spectrum mask.\nFurthermore, it optimizes training costs by gradually increasing timesteps.\nExtensive experiments demonstrate that PiGW enables embedding watermarks into\nthe generated image with negligible quality loss while achieving true\ninvisibility and high resistance to noise attacks. Moreover, PiGW can serve as\na plugin for various commonly used generative structures and multimodal\ngenerative content types. Finally, we demonstrate how PiGW can also be utilized\nfor detecting generated images, contributing to the promotion of secure AI\ndevelopment. The project code will be made available on GitHub.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.12053v2"
    },
    {
        "title": "Towards Low-Latency and Energy-Efficient Hybrid P2P-CDN Live Video\n  Streaming",
        "authors": [
            "Reza Farahani",
            "Christian Timmerer",
            "Hermann Hellwagner"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Streaming segmented videos over the Hypertext Transfer Protocol (HTTP) is an\nincreasingly popular approach in both live and video-on-demand (VoD)\napplications. However, designing a scalable and adaptable framework that\nreduces servers energy consumption and supports low latency and high quality\nservices, particularly for live video streaming scenarios, is still challenging\nfor Over-The-Top (OTT) service providers. To address such challenges, this\npaper introduces a new hybrid P2P-CDN framework that leverages new networking\nand computing paradigms, i.e., Network Function Virtualization (NFV) and edge\ncomputing for live video streaming. The proposed framework introduces a\nmulti-layer architecture and a tree of possible actions therein (an action\ntree), taking into account all available resources from peers, edge, and CDN\nservers to efficiently distribute video fetching and transcoding tasks across a\nhybrid P2P-CDN network, consequently enhancing the users latency and video\nquality. We also discuss our testbed designed to validate the framework and\ncompare it with baseline methods. The experimental results indicate that the\nproposed framework improves user Quality of Experience (QoE), reduces client\nserving latency, and improves edge server energy consumption compared to\nbaseline approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.16985v1"
    },
    {
        "title": "ConvBench: A Multi-Turn Conversation Evaluation Benchmark with\n  Hierarchical Capability for Large Vision-Language Models",
        "authors": [
            "Shuo Liu",
            "Kaining Ying",
            "Hao Zhang",
            "Yue Yang",
            "Yuqi Lin",
            "Tianle Zhang",
            "Chuanhao Li",
            "Yu Qiao",
            "Ping Luo",
            "Wenqi Shao",
            "Kaipeng Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This paper presents ConvBench, a novel multi-turn conversation evaluation\nbenchmark tailored for Large Vision-Language Models (LVLMs). Unlike existing\nbenchmarks that assess individual capabilities in single-turn dialogues,\nConvBench adopts a three-level multimodal capability hierarchy, mimicking human\ncognitive processes by stacking up perception, reasoning, and creativity. Each\nlevel focuses on a distinct capability, mirroring the cognitive progression\nfrom basic perception to logical reasoning and ultimately to advanced\ncreativity. ConvBench comprises 577 meticulously curated multi-turn\nconversations encompassing 215 tasks reflective of real-world demands.\nAutomatic evaluations quantify response performance at each turn and overall\nconversation level. Leveraging the capability hierarchy, ConvBench enables\nprecise attribution of conversation mistakes to specific levels. Experimental\nresults reveal a performance gap between multi-modal models, including GPT4-V,\nand human performance in multi-turn conversations. Additionally, weak\nfine-grained perception in multi-modal models contributes to reasoning and\ncreation failures. ConvBench serves as a catalyst for further research aimed at\nenhancing visual dialogues.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.20194v2"
    },
    {
        "title": "3DMambaIPF: A State Space Model for Iterative Point Cloud Filtering via\n  Differentiable Rendering",
        "authors": [
            "Qingyuan Zhou",
            "Weidong Yang",
            "Ben Fei",
            "Jingyi Xu",
            "Rui Zhang",
            "Keyi Liu",
            "Yeqi Luo",
            "Ying He"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Noise is an inevitable aspect of point cloud acquisition, necessitating\nfiltering as a fundamental task within the realm of 3D vision. Existing\nlearning-based filtering methods have shown promising capabilities on\nsmall-scale synthetic or real-world datasets. Nonetheless, the effectiveness of\nthese methods is constrained when dealing with a substantial quantity of point\nclouds. This limitation primarily stems from their limited denoising\ncapabilities for large-scale point clouds and their inclination to generate\nnoisy outliers after denoising. The recent introduction of State Space Models\n(SSMs) for long sequence modeling in Natural Language Processing (NLP) presents\na promising solution for handling large-scale data. Encouraged by iterative\npoint cloud filtering methods, we introduce 3DMambaIPF, firstly incorporating\nMamba (Selective SSM) architecture to sequentially handle extensive point\nclouds from large scenes, capitalizing on its strengths in selective input\nprocessing and long sequence modeling capabilities. Additionally, we integrate\na robust and fast differentiable rendering loss to constrain the noisy points\naround the surface. In contrast to previous methodologies, this differentiable\nrendering loss enhances the visual realism of denoised geometric structures and\naligns point cloud boundaries more closely with those observed in real-world\nobjects. Extensive evaluation on datasets comprising small-scale synthetic and\nreal-world models (typically with up to 50K points) demonstrate that our method\nachieves state-of-the-art results. Moreover, we showcase the superior\nscalability and efficiency of our method on large-scale models with about 500K\npoints, where the majority of the existing learning-based denoising methods are\nunable to handle.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.05522v2"
    },
    {
        "title": "AllTheDocks road safety dataset: A cyclist's perspective and experience",
        "authors": [
            "Chia-Yen Chiang",
            "Ruikang Zhong",
            "Jennifer Ding",
            "Joseph Wood",
            "Stephen Bee",
            "Mona Jaber"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Active travel is an essential component in intelligent transportation\nsystems. Cycling, as a form of active travel, shares the road space with\nmotorised traffic which often affects the cyclists' safety and comfort and\ntherefore peoples' propensity to uptake cycling instead of driving. This paper\npresents a unique dataset, collected by cyclists across London, that includes\nvideo footage, accelerometer, GPS, and gyroscope data. The dataset is then\nlabelled by an independent group of London cyclists to rank the safety level of\neach frame and to identify objects in the cyclist's field of vision that might\naffect their experience. Furthermore, in this dataset, the quality of the road\nis measured by the international roughness index of the surface, which\nindicates the comfort of cycling on the road. The dataset will be made\navailable for open access in the hope of motivating more research in this area\nto underpin the requirements for cyclists' safety and comfort and encourage\nmore people to replace vehicle travel with cycling.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.10528v1"
    },
    {
        "title": "Retrieval Augmented Verification: Unveiling Disinformation with\n  Structured Representations for Zero-Shot Real-Time Evidence-guided\n  Fact-Checking of Multi-modal Social media posts",
        "authors": [
            "Arka Ujjal Dey",
            "Artemis Llabrés",
            "Ernest Valveny",
            "Dimosthenis Karatzas"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Social Media posts, where real images are unscrupulously reused along with\nprovocative text to promote a particular idea, have been one of the major\nsources of disinformation. By design, these claims are without editorial\noversight and accessible to a vast population who otherwise may not have access\nto multiple information sources. This implies the need to fact-check these\nposts and clearly explain which parts of the posts are fake. In the supervised\nlearning setup, this is often reduced to a binary classification problem,\nneglecting all intermediate stages. Further, these claims often involve recent\nevents on which systems trained on historical data are prone to fail. In this\nwork, we propose a zero-shot approach by retrieving real-time web-scraped\nevidence from multiple news websites and matching them with the claim text and\nimage using pretrained language vision systems. We propose a graph structured\nrepresentation, which a) allows us to gather evidence automatically and b)\nhelps generate interpretable results by explicitly pointing out which parts of\nthe claim can not be verified. Our zero-shot method, with improved\ninterpretability, generates competitive results against the state-of-the-art\nmethods\n",
        "pdf_link": "http://arxiv.org/pdf/2404.10702v2"
    },
    {
        "title": "ConCLVD: Controllable Chinese Landscape Video Generation via Diffusion\n  Model",
        "authors": [
            "Dingming Liu",
            "Shaowei Li",
            "Ruoyan Zhou",
            "Lili Liang",
            "Yongguan Hong",
            "Fei Chao",
            "Rongrong Ji"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Chinese landscape painting is a gem of Chinese cultural and artistic heritage\nthat showcases the splendor of nature through the deep observations and\nimaginations of its painters. Limited by traditional techniques, these artworks\nwere confined to static imagery in ancient times, leaving the dynamism of\nlandscapes and the subtleties of artistic sentiment to the viewer's\nimagination. Recently, emerging text-to-video (T2V) diffusion methods have\nshown significant promise in video generation, providing hope for the creation\nof dynamic Chinese landscape paintings. However, challenges such as the lack of\nspecific datasets, the intricacy of artistic styles, and the creation of\nextensive, high-quality videos pose difficulties for these models in generating\nChinese landscape painting videos. In this paper, we propose CLV-HD (Chinese\nLandscape Video-High Definition), a novel T2V dataset for Chinese landscape\npainting videos, and ConCLVD (Controllable Chinese Landscape Video Diffusion),\na T2V model that utilizes Stable Diffusion. Specifically, we present a motion\nmodule featuring a dual attention mechanism to capture the dynamic\ntransformations of landscape imageries, alongside a noise adapter to leverage\nunsupervised contrastive learning in the latent space. Following the generation\nof keyframes, we employ optical flow for frame interpolation to enhance video\nsmoothness. Our method not only retains the essence of the landscape painting\nimageries but also achieves dynamic transitions, significantly advancing the\nfield of artistic video generation. The source code and dataset are available\nat https://anonymous.4open.science/r/ConCLVD-EFE3.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.12903v1"
    },
    {
        "title": "Towards Unified Representation of Multi-Modal Pre-training for 3D\n  Understanding via Differentiable Rendering",
        "authors": [
            "Ben Fei",
            "Yixuan Li",
            "Weidong Yang",
            "Lipeng Ma",
            "Ying He"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  State-of-the-art 3D models, which excel in recognition tasks, typically\ndepend on large-scale datasets and well-defined category sets. Recent advances\nin multi-modal pre-training have demonstrated potential in learning 3D\nrepresentations by aligning features from 3D shapes with their 2D RGB or depth\ncounterparts. However, these existing frameworks often rely solely on either\nRGB or depth images, limiting their effectiveness in harnessing a comprehensive\nrange of multi-modal data for 3D applications. To tackle this challenge, we\npresent DR-Point, a tri-modal pre-training framework that learns a unified\nrepresentation of RGB images, depth images, and 3D point clouds by pre-training\nwith object triplets garnered from each modality. To address the scarcity of\nsuch triplets, DR-Point employs differentiable rendering to obtain various\ndepth images. This approach not only augments the supply of depth images but\nalso enhances the accuracy of reconstructed point clouds, thereby promoting the\nrepresentative learning of the Transformer backbone. Subsequently, using a\nlimited number of synthetically generated triplets, DR-Point effectively learns\na 3D representation space that aligns seamlessly with the RGB-Depth image\nspace. Our extensive experiments demonstrate that DR-Point outperforms existing\nself-supervised learning methods in a wide range of downstream tasks, including\n3D object classification, part segmentation, point cloud completion, semantic\nsegmentation, and detection. Additionally, our ablation studies validate the\neffectiveness of DR-Point in enhancing point cloud understanding.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.13619v1"
    },
    {
        "title": "Tile-Weighted Rate-Distortion Optimized Packet Scheduling for\n  360$^\\circ$ VR Video Streaming",
        "authors": [
            "Haopeng Wang",
            "Haiwei Dong",
            "Abdulmotaleb El Saddik"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  A key challenge of 360$^\\circ$ VR video streaming is ensuring high quality\nwith limited network bandwidth. Currently, most studies focus on tile-based\nadaptive bitrate streaming to reduce bandwidth consumption, where resources in\nnetwork nodes are not fully utilized. This article proposes a tile-weighted\nrate-distortion (TWRD) packet scheduling optimization system to reduce data\nvolume and improve video quality. A multimodal spatial-temporal attention\ntransformer is proposed to predict viewpoint with probability that is used to\ndynamically weight tiles and corresponding packets. The packet scheduling\nproblem of determining which packets should be dropped is formulated as an\noptimization problem solved by a dynamic programming solution. Experiment\nresults demonstrate the proposed method outperforms the existing methods under\nvarious conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.14573v1"
    },
    {
        "title": "Simple but Effective Raw-Data Level Multimodal Fusion for Composed Image\n  Retrieval",
        "authors": [
            "Haokun Wen",
            "Xuemeng Song",
            "Xiaolin Chen",
            "Yinwei Wei",
            "Liqiang Nie",
            "Tat-Seng Chua"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Composed image retrieval (CIR) aims to retrieve the target image based on a\nmultimodal query, i.e., a reference image paired with corresponding\nmodification text. Recent CIR studies leverage vision-language pre-trained\n(VLP) methods as the feature extraction backbone, and perform nonlinear\nfeature-level multimodal query fusion to retrieve the target image. Despite the\npromising performance, we argue that their nonlinear feature-level multimodal\nfusion may lead to the fused feature deviating from the original embedding\nspace, potentially hurting the retrieval performance. To address this issue, in\nthis work, we propose shifting the multimodal fusion from the feature level to\nthe raw-data level to fully exploit the VLP model's multimodal encoding and\ncross-modal alignment abilities. In particular, we introduce a Dual Query\nUnification-based Composed Image Retrieval framework (DQU-CIR), whose backbone\nsimply involves a VLP model's image encoder and a text encoder. Specifically,\nDQU-CIR first employs two training-free query unification components:\ntext-oriented query unification and vision-oriented query unification, to\nderive a unified textual and visual query based on the raw data of the\nmultimodal query, respectively. The unified textual query is derived by\nconcatenating the modification text with the extracted reference image's\ntextual description, while the unified visual query is created by writing the\nkey modification words onto the reference image. Ultimately, to address diverse\nsearch intentions, DQU-CIR linearly combines the features of the two unified\nqueries encoded by the VLP model to retrieve the target image. Extensive\nexperiments on four real-world datasets validate the effectiveness of our\nproposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.15875v1"
    },
    {
        "title": "Dual Dynamic Threshold Adjustment Strategy for Deep Metric Learning",
        "authors": [
            "Xiruo Jiang",
            "Yazhou Yao",
            "Sheng Liu",
            "Fumin Shen",
            "Liqiang Nie",
            "Xiansheng Hua"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Loss functions and sample mining strategies are essential components in deep\nmetric learning algorithms. However, the existing loss function or mining\nstrategy often necessitate the incorporation of additional hyperparameters,\nnotably the threshold, which defines whether the sample pair is informative.\nThe threshold provides a stable numerical standard for determining whether to\nretain the pairs. It is a vital parameter to reduce the redundant sample pairs\nparticipating in training. Nonetheless, finding the optimal threshold can be a\ntime-consuming endeavor, often requiring extensive grid searches. Because the\nthreshold cannot be dynamically adjusted in the training stage, we should\nconduct plenty of repeated experiments to determine the threshold. Therefore,\nwe introduce a novel approach for adjusting the thresholds associated with both\nthe loss function and the sample mining strategy. We design a static Asymmetric\nSample Mining Strategy (ASMS) and its dynamic version Adaptive Tolerance ASMS\n(AT-ASMS), tailored for sample mining methods. ASMS utilizes differentiated\nthresholds to address the problems (too few positive pairs and too many\nredundant negative pairs) caused by only applying a single threshold to filter\nsamples. AT-ASMS can adaptively regulate the ratio of positive and negative\npairs during training according to the ratio of the currently mined positive\nand negative pairs. This meta-learning-based threshold generation algorithm\nutilizes a single-step gradient descent to obtain new thresholds. We combine\nthese two threshold adjustment algorithms to form the Dual Dynamic Threshold\nAdjustment Strategy (DDTAS). Experimental results show that our algorithm\nachieves competitive performance on CUB200, Cars196, and SOP datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.19282v1"
    },
    {
        "title": "Expert Insight-Enhanced Follow-up Chest X-Ray Summary Generation",
        "authors": [
            "Zhichuan Wang",
            "Kinhei Lee",
            "Qiao Deng",
            "Tiffany Y. So",
            "Wan Hang Chiu",
            "Yeung Yu Hui",
            "Bingjing Zhou",
            "Edward S. Hui"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  A chest X-ray radiology report describes abnormal findings not only from\nX-ray obtained at current examination, but also findings on disease progression\nor change in device placement with reference to the X-ray from previous\nexamination. Majority of the efforts on automatic generation of radiology\nreport pertain to reporting the former, but not the latter, type of findings.\nTo the best of the authors' knowledge, there is only one work dedicated to\ngenerating summary of the latter findings, i.e., follow-up summary. In this\nstudy, we therefore propose a transformer-based framework to tackle this task.\nMotivated by our observations on the significance of medical lexicon on the\nfidelity of summary generation, we introduce two mechanisms to bestow expert\ninsight to our model, namely expert soft guidance and masked entity modeling\nloss. The former mechanism employs a pretrained expert disease classifier to\nguide the presence level of specific abnormalities, while the latter directs\nthe model's attention toward medical lexicon. Extensive experiments were\nconducted to demonstrate that the performance of our model is competitive with\nor exceeds the state-of-the-art.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.00344v2"
    },
    {
        "title": "Task Presentation and Human Perception in Interactive Video Retrieval",
        "authors": [
            "Nina Willis",
            "Abraham Bernstein",
            "Luca Rossetto"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Interactive video retrieval is a cooperative process between humans and\nretrieval systems. Large-scale evaluation campaigns, however, often overlook\nhuman factors, such as the effects of perception, attention, and memory, when\nassessing media retrieval systems. Consequently, their setups fall short of\nemulating realistic retrieval scenarios. In this paper, we design novel task\npresentation modes based on concepts in media memorability, implement the\npipelines necessary for processing target video segments, and build a custom\nexperimental platform for the final evaluation. In order to study the effects\nof different task representation schemes, we conduct a large crowdsourced\nexperiment. Our findings demonstrate that the way in which the target of a\nvideo retrieval task is presented has a substantial influence on the difficulty\nof the retrieval task and that individuals can successfully retrieve a target\nvideo segment despite reducing or even altering the provided hints, opening up\na discussion around future evaluation protocols in the domain of interactive\nmedia retrieval.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.04279v1"
    },
    {
        "title": "Audio Matters Too! Enhancing Markerless Motion Capture with Audio\n  Signals for String Performance Capture",
        "authors": [
            "Yitong Jin",
            "Zhiping Qiu",
            "Yi Shi",
            "Shuangpeng Sun",
            "Chongwu Wang",
            "Donghao Pan",
            "Jiachen Zhao",
            "Zhenghao Liang",
            "Yuan Wang",
            "Xiaobing Li",
            "Feng Yu",
            "Tao Yu",
            "Qionghai Dai"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we touch on the problem of markerless multi-modal human motion\ncapture especially for string performance capture which involves inherently\nsubtle hand-string contacts and intricate movements. To fulfill this goal, we\nfirst collect a dataset, named String Performance Dataset (SPD), featuring\ncello and violin performances. The dataset includes videos captured from up to\n23 different views, audio signals, and detailed 3D motion annotations of the\nbody, hands, instrument, and bow. Moreover, to acquire the detailed motion\nannotations, we propose an audio-guided multi-modal motion capture framework\nthat explicitly incorporates hand-string contacts detected from the audio\nsignals for solving detailed hand poses. This framework serves as a baseline\nfor string performance capture in a completely markerless manner without\nimposing any external devices on performers, eliminating the potential of\nintroducing distortion in such delicate movements. We argue that the movements\nof performers, particularly the sound-producing gestures, contain subtle\ninformation often elusive to visual methods but can be inferred and retrieved\nfrom audio cues. Consequently, we refine the vision-based motion capture\nresults through our innovative audio-guided approach, simultaneously clarifying\nthe contact relationship between the performer and the instrument, as deduced\nfrom the audio. We validate the proposed framework and conduct ablation studies\nto demonstrate its efficacy. Our results outperform current state-of-the-art\nvision-based algorithms, underscoring the feasibility of augmenting visual\nmotion capture with audio modality. To the best of our knowledge, SPD is the\nfirst dataset for musical instrument performance, covering fine-grained hand\nmotion details in a multi-modal, large-scale collection.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.04963v1"
    },
    {
        "title": "MM-InstructEval: Zero-Shot Evaluation of (Multimodal) Large Language\n  Models on Multimodal Reasoning Tasks",
        "authors": [
            "Xiaocui Yang",
            "Wenfang Wu",
            "Shi Feng",
            "Ming Wang",
            "Daling Wang",
            "Yang Li",
            "Qi Sun",
            "Yifei Zhang",
            "Xiaoming Fu",
            "Soujanya Poria"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The rising popularity of multimodal large language models (MLLMs) has sparked\na significant increase in research dedicated to evaluating these models.\nHowever, current evaluation studies predominantly concentrate on the ability of\nmodels to comprehend and reason within a unimodal (vision-only) context,\noverlooking critical performance evaluations in complex multimodal reasoning\ntasks that integrate both visual and text contexts. Furthermore, tasks that\ndemand reasoning across multiple modalities pose greater challenges and require\na deep understanding of multimodal contexts. In this paper, we introduce a\ncomprehensive assessment framework named MM-InstructEval, which integrates a\ndiverse array of metrics to provide an extensive evaluation of the performance\nof various models and instructions across a broad range of multimodal reasoning\ntasks with vision-text contexts. MM-InstructEval enhances the research on the\nperformance of MLLMs in complex multimodal reasoning tasks, facilitating a more\nthorough and holistic zero-shot evaluation of MLLMs. We firstly utilize the\n\"Best Performance\" metric to determine the upper performance limit of each\nmodel across various datasets. The \"Mean Relative Gain\" metric provides an\nanalysis of the overall performance across different models and instructions,\nwhile the \"Stability\" metric evaluates their sensitivity to variations.\nHistorically, the research has focused on evaluating models independently or\nsolely assessing instructions, overlooking the interplay between models and\ninstructions. To address this gap, we introduce the \"Adaptability\" metric,\ndesigned to quantify the degree of adaptability between models and\ninstructions. Evaluations are conducted on 31 models (23 MLLMs) across 16\nmultimodal datasets, covering 6 tasks, with 10 distinct instructions. The\nextensive analysis enables us to derive novel insights.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07229v1"
    },
    {
        "title": "AsCL: An Asymmetry-sensitive Contrastive Learning Method for Image-Text\n  Retrieval with Cross-Modal Fusion",
        "authors": [
            "Ziyu Gong",
            "Chengcheng Mai",
            "Yihua Huang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The image-text retrieval task aims to retrieve relevant information from a\ngiven image or text. The main challenge is to unify multimodal representation\nand distinguish fine-grained differences across modalities, thereby finding\nsimilar contents and filtering irrelevant contents. However, existing methods\nmainly focus on unified semantic representation and concept alignment for\nmulti-modalities, while the fine-grained differences across modalities have\nrarely been studied before, making it difficult to solve the information\nasymmetry problem. In this paper, we propose a novel asymmetry-sensitive\ncontrastive learning method. By generating corresponding positive and negative\nsamples for different asymmetry types, our method can simultaneously ensure\nfine-grained semantic differentiation and unified semantic representation\nbetween multi-modalities. Additionally, a hierarchical cross-modal fusion\nmethod is proposed, which integrates global and local-level features through a\nmultimodal attention mechanism to achieve concept alignment. Extensive\nexperiments performed on MSCOCO and Flickr30K, demonstrate the effectiveness\nand superiority of our proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.10029v2"
    },
    {
        "title": "Universal Organizer of SAM for Unsupervised Semantic Segmentation",
        "authors": [
            "Tingting Li",
            "Gensheng Pei",
            "Xinhao Cai",
            "Huafeng Liu",
            "Qiong Wang",
            "Yazhou Yao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Unsupervised semantic segmentation (USS) aims to achieve high-quality\nsegmentation without manual pixel-level annotations. Existing USS models\nprovide coarse category classification for regions, but the results often have\nblurry and imprecise edges. Recently, a robust framework called the segment\nanything model (SAM) has been proven to deliver precise boundary object masks.\nTherefore, this paper proposes a universal organizer based on SAM, termed as\nUO-SAM, to enhance the mask quality of USS models. Specifically, using only the\noriginal image and the masks generated by the USS model, we extract visual\nfeatures to obtain positional prompts for target objects. Then, we activate a\nlocal region optimizer that performs segmentation using SAM on a per-object\nbasis. Finally, we employ a global region optimizer to incorporate global image\ninformation and refine the masks to obtain the final fine-grained masks.\nCompared to existing methods, our UO-SAM achieves state-of-the-art performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.11742v1"
    },
    {
        "title": "Synchronized Video Storytelling: Generating Video Narrations with\n  Structured Storyline",
        "authors": [
            "Dingyi Yang",
            "Chunru Zhan",
            "Ziheng Wang",
            "Biao Wang",
            "Tiezheng Ge",
            "Bo Zheng",
            "Qin Jin"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Video storytelling is engaging multimedia content that utilizes video and its\naccompanying narration to attract the audience, where a key challenge is\ncreating narrations for recorded visual scenes. Previous studies on dense video\ncaptioning and video story generation have made some progress. However, in\npractical applications, we typically require synchronized narrations for\nongoing visual scenes. In this work, we introduce a new task of Synchronized\nVideo Storytelling, which aims to generate synchronous and informative\nnarrations for videos. These narrations, associated with each video clip,\nshould relate to the visual content, integrate relevant knowledge, and have an\nappropriate word count corresponding to the clip's duration. Specifically, a\nstructured storyline is beneficial to guide the generation process, ensuring\ncoherence and integrity. To support the exploration of this task, we introduce\na new benchmark dataset E-SyncVidStory with rich annotations. Since existing\nMultimodal LLMs are not effective in addressing this task in one-shot or\nfew-shot settings, we propose a framework named VideoNarrator that can generate\na storyline for input videos and simultaneously generate narrations with the\nguidance of the generated or predefined storyline. We further introduce a set\nof evaluation metrics to thoroughly assess the generation. Both automatic and\nhuman evaluations validate the effectiveness of our approach. Our dataset,\ncodes, and evaluations will be released.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.14040v2"
    },
    {
        "title": "Large Language Models (LLMs): Deployment, Tokenomics and Sustainability",
        "authors": [
            "Haiwei Dong",
            "Shuang Xie"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The rapid advancement of Large Language Models (LLMs) has significantly\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\nintroduced comprehensive multi-modality capabilities. In this paper, we first\nexplored the deployment strategies, economic considerations, and sustainability\nchallenges associated with the state-of-the-art LLMs. More specifically, we\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\nand fine-tuning, highlighting their respective advantages and limitations.\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\ninference. Additionally, for the tokenomics of LLM services, we examined the\nbalance between performance and cost from the quality of experience (QoE)'s\nperspective of end users. Lastly, we envisioned the future hybrid architecture\nof LLM processing and its corresponding sustainability concerns, particularly\nin the environmental carbon footprint impact. Through these discussions, we\nprovided a comprehensive overview of the operational and strategic\nconsiderations essential for the responsible development and deployment of\nLLMs.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17147v1"
    },
    {
        "title": "Exploring the Robustness of Decision-Level Through Adversarial Attacks\n  on LLM-Based Embodied Models",
        "authors": [
            "Shuyuan Liu",
            "Jiawei Chen",
            "Shouwei Ruan",
            "Hang Su",
            "Zhaoxia Yin"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Embodied intelligence empowers agents with a profound sense of perception,\nenabling them to respond in a manner closely aligned with real-world\nsituations. Large Language Models (LLMs) delve into language instructions with\ndepth, serving a crucial role in generating plans for intricate tasks. Thus,\nLLM-based embodied models further enhance the agent's capacity to comprehend\nand process information. However, this amalgamation also ushers in new\nchallenges in the pursuit of heightened intelligence. Specifically, attackers\ncan manipulate LLMs to produce irrelevant or even malicious outputs by altering\ntheir prompts. Confronted with this challenge, we observe a notable absence of\nmulti-modal datasets essential for comprehensively evaluating the robustness of\nLLM-based embodied models. Consequently, we construct the Embodied Intelligent\nRobot Attack Dataset (EIRAD), tailored specifically for robustness evaluation.\nAdditionally, two attack strategies are devised, including untargeted attacks\nand targeted attacks, to effectively simulate a range of diverse attack\nscenarios. At the same time, during the attack process, to more accurately\nascertain whether our method is successful in attacking the LLM-based embodied\nmodel, we devise a new attack success evaluation method utilizing the BLIP2\nmodel. Recognizing the time and cost-intensive nature of the GCG algorithm in\nattacks, we devise a scheme for prompt suffix initialization based on various\ntarget tasks, thus expediting the convergence process. Experimental results\ndemonstrate that our method exhibits a superior attack success rate when\ntargeting LLM-based embodied models, indicating a lower level of decision-level\nrobustness in these models.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.19802v3"
    },
    {
        "title": "NeRF View Synthesis: Subjective Quality Assessment and Objective Metrics\n  Evaluation",
        "authors": [
            "Pedro Martin",
            "Antonio Rodrigues",
            "Joao Ascenso",
            "Maria Paula Queluz"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Neural radiance fields (NeRF) are a groundbreaking computer vision technology\nthat enables the generation of high-quality, immersive visual content from\nmultiple viewpoints. This capability has significant advantages for\napplications such as virtual/augmented reality, 3D modelling, and content\ncreation for the film and entertainment industry. However, the evaluation of\nNeRF methods poses several challenges, including a lack of comprehensive\ndatasets, reliable assessment methodologies, and objective quality metrics.\nThis paper addresses the problem of NeRF view synthesis (NVS) quality\nassessment thoroughly, by conducting a rigorous subjective quality assessment\ntest that considers several scene classes and recently proposed NVS methods.\nAdditionally, the performance of a wide range of state-of-the-art conventional\nand learning-based full-reference 2D image and video quality assessment metrics\nis evaluated against the subjective scores of the subjective study. This study\nfound that errors in camera pose estimation can result in spatial misalignments\nbetween synthesized and reference images, which need to be corrected before\napplying an objective quality metric. The experimental results are analyzed in\ndepth, providing a comparative evaluation of several NVS methods and objective\nquality metrics, across different classes of visual scenes, including real and\nsynthetic content for front-face and 360-degree camera trajectories.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.20078v3"
    },
    {
        "title": "Towards AI-Assisted Sustainable Adaptive Video Streaming Systems:\n  Tutorial and Survey",
        "authors": [
            "Reza Farahani",
            "Zoha Azimi",
            "Christian Timmerer",
            "Radu Prodan"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Improvements in networking technologies and the steadily increasing numbers\nof users, as well as the shift from traditional broadcasting to streaming\ncontent over the Internet, have made video applications (e.g., live and\nVideo-on-Demand (VoD)) predominant sources of traffic. Recent advances in\nArtificial Intelligence (AI) and its widespread application in various academic\nand industrial fields have focused on designing and implementing a variety of\nvideo compression and content delivery techniques to improve user Quality of\nExperience (QoE). However, providing high QoE services results in more energy\nconsumption and carbon footprint across the service delivery path, extending\nfrom the end user's device through the network and service infrastructure\n(e.g., cloud providers). Despite the importance of energy efficiency in video\nstreaming, there is a lack of comprehensive surveys covering state-of-the-art\nAI techniques and their applications throughout the video streaming lifecycle.\nExisting surveys typically focus on specific parts, such as video encoding,\ndelivery networks, playback, or quality assessment, without providing a\nholistic view of the entire lifecycle and its impact on energy consumption and\nQoE. Motivated by this research gap, this survey provides a comprehensive\noverview of the video streaming lifecycle, content delivery, energy and Video\nQuality Assessment (VQA) metrics and models, and AI techniques employed in\nvideo streaming. In addition, it conducts an in-depth state-of-the-art analysis\nfocused on AI-driven approaches to enhance the energy efficiency of end-to-end\naspects of video streaming systems (i.e., encoding, delivery network, playback,\nand VQA approaches). Finally, it discusses prospective research directions for\ndeveloping AI-assisted energy-aware video streaming systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.02302v1"
    },
    {
        "title": "Globally and Locally Optimized Pannini Projection for High FoV Rendering\n  of 360-degree Images",
        "authors": [
            "Falah Jabar",
            "Joao Ascenso",
            "Maria Paula Queluz"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  To render a spherical (360 degree or omnidirectional) image on planar\ndisplays, a 2D image -- called as viewport -- must be obtained by projecting a\nsphere region on a plane, according to the users viewing direction and a\npredefined field of view (FoV). However, any sphere to plan projection\nintroduces geometric distortions, such as object stretching and/or bending of\nstraight lines, which intensity increases with the considered FoV. In this\npaper, a fully automatic content-aware projection is proposed, aiming to reduce\nthe geometric distortions when high FoVs are used. This new projection is based\non the Pannini projection, whose parameters are firstly globally optimized\naccording to the image content, followed by a local conformality improvement of\nrelevant viewport objects. A crowdsourcing subjective test showed that the\nproposed projection is the most preferred solution among the considered\nstate-of-the-art sphere to plan projections, producing viewports with a more\npleasant visual quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.03282v1"
    },
    {
        "title": "Recognizing Everything from All Modalities at Once: Grounded Multimodal\n  Universal Information Extraction",
        "authors": [
            "Meishan Zhang",
            "Hao Fei",
            "Bin Wang",
            "Shengqiong Wu",
            "Yixin Cao",
            "Fei Li",
            "Min Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In the field of information extraction (IE), tasks across a wide range of\nmodalities and their combinations have been traditionally studied in isolation,\nleaving a gap in deeply recognizing and analyzing cross-modal information. To\naddress this, this work for the first time introduces the concept of grounded\nMultimodal Universal Information Extraction (MUIE), providing a unified task\nframework to analyze any IE tasks over various modalities, along with their\nfine-grained groundings. To tackle MUIE, we tailor a multimodal large language\nmodel (MLLM), Reamo, capable of extracting and grounding information from all\nmodalities, i.e., recognizing everything from all modalities at once. Reamo is\nupdated via varied tuning strategies, equipping it with powerful capabilities\nfor information recognition and fine-grained multimodal grounding. To address\nthe absence of a suitable benchmark for grounded MUIE, we curate a\nhigh-quality, diverse, and challenging test set, which encompasses IE tasks\nacross 9 common modality combinations with the corresponding multimodal\ngroundings. The extensive comparison of Reamo with existing MLLMs integrated\ninto pipeline approaches demonstrates its advantages across all evaluation\ndimensions, establishing a strong benchmark for the follow-up research. Our\nresources are publicly released at https://haofei.vip/MUIE.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.03701v2"
    },
    {
        "title": "StreamOptix: A Cross-layer Adaptive Video Delivery Scheme",
        "authors": [
            "Mufan Liu",
            "Le Yang",
            "Yifan Wang",
            "Yiling Xu",
            "Ye-Kui Wang",
            "Yunfeng Guan"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This paper presents a cross-layer video delivery scheme, StreamOptix, and\nproposes a joint optimization algorithm for video delivery that leverages the\ncharacteristics of the physical (PHY), medium access control (MAC), and\napplication (APP) layers. Most existing methods for optimizing video\ntransmission over different layers were developed individually. Realizing a\ncross-layer design has always been a significant challenge, mainly due to the\ncomplex interactions and mismatches in timescales between layers, as well as\nthe presence of distinct objectives in different layers. To address these\ncomplications, we take a divide-and-conquer approach and break down the\nformulated cross-layer optimization problem for video delivery into three\nsub-problems. We then propose a three-stage closedloop optimization framework,\nwhich consists of 1) an adaptive bitrate (ABR) strategy based on the link\ncapacity information from PHY, 2) a video-aware resource allocation scheme\naccounting for the APP bitrate constraint, and 3) a link adaptation technique\nutilizing the soft acknowledgment feedback (soft-ACK). The proposed framework\nalso supports the collections of the distorted bitstreams transmitted across\nthe link. This allows a more reasonable assessment of video quality compared to\nmany existing ABR methods that simply neglect the distortions occurring in the\nPHY layer. Experiments conducted under various network settings demonstrate the\neffectiveness and superiority of the new cross-layer optimization strategy. A\nbyproduct of this study is the development of more comprehensive performance\nmetrics on video delivery, which lays down the foundation for extending our\nsystem to multimodal communications in the future. Code for reproducing the\nexperimental results is available at https://github.com/Evan-sudo/StreamOptix.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.04632v1"
    },
    {
        "title": "A Subjective Quality Evaluation of 3D Mesh with Dynamic Level of Detail\n  in Virtual Reality",
        "authors": [
            "Duc Nguyen",
            "Tran Thuy Hien",
            "Truong Thu Huong"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  3D meshes are one of the main components of Virtual Reality applications.\nHowever, many network and computational resources are required to process 3D\nmeshes in real-time. A potential solution to this challenge is to dynamically\nadapt the Level of Detail (LoD) of a 3D mesh based on the object's position and\nthe user's viewpoint. In this paper, we conduct a subjective study to\ninvestigate users' quality perception of 3D meshes with dynamic Level of Detail\nin a Virtual Reality environment. The subjective experiment is carried out with\nfive 3D meshes of different characteristics, four Levels of Detail, and four\ndistance settings. The results of the experiment show that the impact of the\ndynamic level of detail depends on both the position of the 3D object in the\nvirtual world and the number of vertices of the original mesh. In addition, we\npresent a quality model that can accurately predict the MOS score of a LoD\nversion of a 3D mesh from the number of vertices and the distance from the\nviewpoint.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.06888v1"
    },
    {
        "title": "High-level Codes and Fine-grained Weights for Online Multi-modal Hashing\n  Retrieval",
        "authors": [
            "Yu-Wei Zhan",
            "Xiao-Ming Wu",
            "Xin Luo",
            "Yinwei Wei",
            "Xin-Shun Xu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In the real world, multi-modal data often appears in a streaming fashion, and\nthere is a growing demand for similarity retrieval from such non-stationary\ndata, especially at a large scale. In response to this need, online multi-modal\nhashing has gained significant attention. However, existing online multi-modal\nhashing methods face challenges related to the inconsistency of hash codes\nduring long-term learning and inefficient fusion of different modalities. In\nthis paper, we present a novel approach to supervised online multi-modal\nhashing, called High-level Codes, Fine-grained Weights (HCFW). To address these\nproblems, HCFW is designed by its non-trivial contributions from two primary\ndimensions: 1) Online Hashing Perspective. To ensure the long-term consistency\nof hash codes, especially in incremental learning scenarios, HCFW learns\nhigh-level codes derived from category-level semantics. Besides, these codes\nare adept at handling the category-incremental challenge. 2) Multi-modal\nHashing Aspect. HCFW introduces the concept of fine-grained weights designed to\nfacilitate the seamless fusion of complementary multi-modal data, thereby\ngenerating multi-modal weights at the instance level and enhancing the overall\nhashing performance. A comprehensive battery of experiments conducted on two\nbenchmark datasets convincingly underscores the effectiveness and efficiency of\nHCFW.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.10776v1"
    },
    {
        "title": "EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot",
        "authors": [
            "Hao Fei",
            "Han Zhang",
            "Bin Wang",
            "Lizi Liao",
            "Qian Liu",
            "Erik Cambria"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This paper introduces EmpathyEar, a pioneering open-source, avatar-based\nmultimodal empathetic chatbot, to fill the gap in traditional text-only\nempathetic response generation (ERG) systems. Leveraging the advancements of a\nlarge language model, combined with multimodal encoders and generators,\nEmpathyEar supports user inputs in any combination of text, sound, and vision,\nand produces multimodal empathetic responses, offering users, not just textual\nresponses but also digital avatars with talking faces and synchronized\nspeeches. A series of emotion-aware instruction-tuning is performed for\ncomprehensive emotional understanding and generation capabilities. In this way,\nEmpathyEar provides users with responses that achieve a deeper emotional\nresonance, closely emulating human-like empathy. The system paves the way for\nthe next emotional intelligence, for which we open-source the code for public\naccess.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.15177v1"
    },
    {
        "title": "Deep Mamba Multi-modal Learning",
        "authors": [
            "Jian Zhu",
            "Xin Zou",
            "Yu Cui",
            "Zhangmin Huang",
            "Chenshu Hu",
            "Bo Lyu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Inspired by the excellent performance of Mamba networks, we propose a novel\nDeep Mamba Multi-modal Learning (DMML). It can be used to achieve the fusion of\nmulti-modal features. We apply DMML to the field of multimedia retrieval and\npropose an innovative Deep Mamba Multi-modal Hashing (DMMH) method. It combines\nthe advantages of algorithm accuracy and inference speed. We validated the\neffectiveness of DMMH on three public datasets and achieved state-of-the-art\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.18007v1"
    },
    {
        "title": "Revisiting Vision-Language Features Adaptation and Inconsistency for\n  Social Media Popularity Prediction",
        "authors": [
            "Chih-Chung Hsu",
            "Chia-Ming Lee",
            "Yu-Fan Lin",
            "Yi-Shiuan Chou",
            "Chih-Yu Jian",
            "Chi-Han Tsai"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Social media popularity (SMP) prediction is a complex task involving\nmulti-modal data integration. While pre-trained vision-language models (VLMs)\nlike CLIP have been widely adopted for this task, their effectiveness in\ncapturing the unique characteristics of social media content remains\nunexplored. This paper critically examines the applicability of CLIP-based\nfeatures in SMP prediction, focusing on the overlooked phenomenon of semantic\ninconsistency between images and text in social media posts. Through extensive\nanalysis, we demonstrate that this inconsistency increases with post\npopularity, challenging the conventional use of VLM features. We provide a\ncomprehensive investigation of semantic inconsistency across different\npopularity intervals and analyze the impact of VLM feature adaptation on SMP\ntasks. Our experiments reveal that incorporating inconsistency measures and\nadapted text features significantly improves model performance, achieving an\nSRC of 0.729 and an MAE of 1.227. These findings not only enhance SMP\nprediction accuracy but also provide crucial insights for developing more\ntargeted approaches in social media analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.00556v1"
    },
    {
        "title": "SIDQL: An Efficient Keyframe Extraction and Motion Reconstruction\n  Framework in Motion Capture",
        "authors": [
            "Xuling Zhang",
            "Ziru Zhang",
            "Yuyang Wang",
            "Lik-hang Lee",
            "Pan Hui"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Metaverse, which integrates the virtual and physical worlds, has emerged as\nan innovative paradigm for changing people's lifestyles. Motion capture has\nbecome a reliable approach to achieve seamless synchronization of the movements\nbetween avatars and human beings, which plays an important role in diverse\nMetaverse applications. However, due to the continuous growth of data, current\ncommunication systems face a significant challenge of meeting the demand of\nultra-low latency during application. In addition, current methods also have\nshortcomings when selecting keyframes, e.g., relying on recognizing motion\ntypes and artificially selected keyframes. Therefore, the utilization of\nkeyframe extraction and motion reconstruction techniques could be considered a\nfeasible and promising solution. In this work, a new motion reconstruction\nalgorithm is designed in a spherical coordinate system involving location and\nvelocity information. Then, we formalize the keyframe extraction problem into\nan optimization problem to reduce the reconstruction error. Using Deep\nQ-Learning (DQL), the Spherical Interpolation based Deep Q-Learning (SIDQL)\nframework is proposed to generate proper keyframes for reconstructing the\nmotion sequences. We use the CMU database to train and evaluate the framework.\nOur scheme can significantly reduce the data volume and transmission latency\ncompared to various baselines while maintaining a reconstruction error of less\nthan 0.09 when extracting five keyframes.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.00925v1"
    },
    {
        "title": "Volume Tracking Based Reference Mesh Extraction for Time-Varying Mesh\n  Compression",
        "authors": [
            "Guodong Chen",
            "Libor Vasa",
            "Fulin Wang",
            "Mallesham Dasari"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Time-Varying meshes (TVMs), characterized by their varying connectivity and\nnumber of vertices, hold significant potential in immersive media and other\nvarious applications. However, their practical utilization is challenging due\nto their time-varying features and large file sizes. Creating a reference mesh\nthat contains the most essential information is a promising approach to\nutilizing shared information within TVMs to reduce storage and transmission\ncosts. We propose a novel method that employs volume tracking to extract\nreference meshes. First, we adopt as-rigid-as-possible (ARAP) volume tracking\non TVMs to get the volume centers for each mesh. Then, we use multidimensional\nscaling (MDS) to get reference centers that ensure the reference mesh avoids\nself-contact regions. Finally, we map the vertices of the meshes to reference\ncenters and extract the reference mesh. Our approach offers a feasible solution\nfor extracting reference meshes that can serve multiple purposes such as\nestablishing surface correspondence, deforming the reference mesh to different\nshapes for I-frame based mesh compression, or defining the global shape of the\nTVMs.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.02457v1"
    },
    {
        "title": "OpenVNA: A Framework for Analyzing the Behavior of Multimodal Language\n  Understanding System under Noisy Scenarios",
        "authors": [
            "Ziqi Yuan",
            "Baozheng Zhang",
            "Hua Xu",
            "Zhiyun Liang",
            "Kai Gao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  We present OpenVNA, an open-source framework designed for analyzing the\nbehavior of multimodal language understanding systems under noisy conditions.\nOpenVNA serves as an intuitive toolkit tailored for researchers, facilitating\nconvenience batch-level robustness evaluation and on-the-fly instance-level\ndemonstration. It primarily features a benchmark Python library for assessing\nglobal model robustness, offering high flexibility and extensibility, thereby\nenabling customization with user-defined noise types and models. Additionally,\na GUI-based interface has been developed to intuitively analyze local model\nbehavior. In this paper, we delineate the design principles and utilization of\nthe created library and GUI-based web platform. Currently, OpenVNA is publicly\naccessible at \\url{https://github.com/thuiar/OpenVNA}, with a demonstration\nvideo available at \\url{https://youtu.be/0Z9cW7RGct4}.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.02773v1"
    },
    {
        "title": "Differentially Processed Optimized Collaborative Rich Text Editor",
        "authors": [
            "Nishtha Jatana",
            "Mansehej Singh",
            "Charu Gupta",
            "Geetika Dhand",
            "Shaily Malik",
            "Pankaj Dadheech",
            "Nagender Aneja",
            "Sandhya Aneja"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  A collaborative real-time text editor is an application that allows multiple\nusers to edit a document simultaneously and merge their contributions\nautomatically. It can be made collaborative by implementing a conflict\nresolution algorithm either on the client side (in peer-to-peer collaboration)\nor on the server side (when using web sockets and a central server to monitor\nstate changes). Although web sockets are ideal for real-time text editors,\nusing multiple collaborative editors on one connection can create problems.\nThis is because a single web connection cannot monitor which user is\ncollaborating on which application state, leading to unnecessary network\nqueries and data being delivered to the wrong state. To address this issue, the\ncurrent solution is to open multiple web socket connections, with one web\nsocket per collaboration application. However, this can add significant\noverhead proportional to the number of apps utilized. In this study, we\ndemonstrate an algorithm that enables using a single web socket for multiple\ncollaborative applications in a collaborative editor. Our method involves\nmodifying the socket's code to track which application's shared state is being\nworked on and by whom. This allows for the simultaneous collaboration of\nmultiple states in real-time, with infinite users, without opening a different\nsocket for each application. Our optimized editor showed an efficiency\nimprovement of over 96% in access time duration. This approach can be\nimplemented in other collaborative editors and web applications with similar\narchitecture to improve performance and eliminate issues arising from network\noverload.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.03027v1"
    },
    {
        "title": "TSC-PCAC: Voxel Transformer and Sparse Convolution Based Point Cloud\n  Attribute Compression for 3D Broadcasting",
        "authors": [
            "Zixi Guo",
            "Yun Zhang",
            "Linwei Zhu",
            "Hanli Wang",
            "Gangyi Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Point cloud has been the mainstream representation for advanced 3D\napplications, such as virtual reality and augmented reality. However, the\nmassive data amounts of point clouds is one of the most challenging issues for\ntransmission and storage. In this paper, we propose an end-to-end voxel\nTransformer and Sparse Convolution based Point Cloud Attribute Compression\n(TSC-PCAC) for 3D broadcasting. Firstly, we present a framework of the\nTSC-PCAC, which include Transformer and Sparse Convolutional Module (TSCM)\nbased variational autoencoder and channel context module. Secondly, we propose\na two-stage TSCM, where the first stage focuses on modeling local dependencies\nand feature representations of the point clouds, and the second stage captures\nglobal features through spatial and channel pooling encompassing larger\nreceptive fields. This module effectively extracts global and local interpoint\nrelevance to reduce informational redundancy. Thirdly, we design a TSCM based\nchannel context module to exploit interchannel correlations, which improves the\npredicted probability distribution of quantized latent representations and thus\nreduces the bitrate. Experimental results indicate that the proposed TSC-PCAC\nmethod achieves an average of 38.53%, 21.30%, and 11.19% Bjontegaard Delta\nbitrate reductions compared to the Sparse-PCAC, NF-PCAC, and G-PCC v23 methods,\nrespectively. The encoding/decoding time costs are reduced up to 97.68%/98.78%\non average compared to the Sparse-PCAC. The source code and the trained models\nof the TSC-PCAC are available at https://github.com/igizuxo/TSC-PCAC.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.04284v2"
    },
    {
        "title": "TOP:A New Target-Audience Oriented Content Paraphrase Task",
        "authors": [
            "Boda Lin",
            "Jiaxin Shi",
            "Haolong Yan",
            "Binghao Tang",
            "Xiaocheng Gong",
            "Si Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recommendation systems usually recommend the existing contents to different\nusers. However, in comparison to static recommendation methods, a\nrecommendation logic that dynamically adjusts based on user interest\npreferences may potentially attract a larger user base. Thus, we consider\nparaphrasing existing content based on the interests of the users to modify the\ncontent to better align with the preferences of users. In this paper, we\npropose a new task named Target-Audience Oriented Content Paraphrase aims to\ngenerate more customized contents for the target audience. We introduce the\ntask definition and the corresponding framework for the proposed task and the\ncreation of the corresponding datasets. We utilize the Large Language Models\n(LLMs) and Large Vision Models (LVMs) to accomplish the base implementation of\nthe TOP framework and provide the referential baseline results for the proposed\ntask.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.09992v1"
    },
    {
        "title": "Multi-source Knowledge Enhanced Graph Attention Networks for Multimodal\n  Fact Verification",
        "authors": [
            "Han Cao",
            "Lingwei Wei",
            "Wei Zhou",
            "Songlin Hu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal fact verification is an under-explored and emerging field that has\ngained increasing attention in recent years. The goal is to assess the veracity\nof claims that involve multiple modalities by analyzing the retrieved evidence.\nThe main challenge in this area is to effectively fuse features from different\nmodalities to learn meaningful multimodal representations. To this end, we\npropose a novel model named Multi-Source Knowledge-enhanced Graph Attention\nNetwork (MultiKE-GAT). MultiKE-GAT introduces external multimodal knowledge\nfrom different sources and constructs a heterogeneous graph to capture complex\ncross-modal and cross-source interactions. We exploit a Knowledge-aware Graph\nFusion (KGF) module to learn knowledge-enhanced representations for each claim\nand evidence and eliminate inconsistencies and noises introduced by redundant\nentities. Experiments on two public benchmark datasets demonstrate that our\nmodel outperforms other comparison methods, showing the effectiveness and\nsuperiority of the proposed model.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.10474v1"
    },
    {
        "title": "LLM-based query paraphrasing for video search",
        "authors": [
            "Jiaxin Wu",
            "Chong-Wah Ngo",
            "Wing-Kwong Chan",
            "Sheng-Hua Zhong"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Text-to-video retrieval answers user queries through search by concepts and\nembeddings. Limited by the size of the concept bank and the amount of training\ndata, answering queries in the wild is not always effective due to the\nout-of-vocabulary problem. Furthermore, neither concept-based nor\nembedding-based search can perform reasoning to consolidate the search results\nfor complex queries mixed with logical and spatial constraints. To address\nthese problems, we leverage large language models (LLM) to paraphrase the query\nby text-to-text (T2T), text-to-image (T2I), and image-to-text (I2T)\ntransformations. These transformations rephrase abstract concepts into simple\nwords to address the out-of-vocabulary problem. Furthermore, the complex\nrelationship in a query can be decoupled into simpler sub-queries, yielding\nbetter retrieval performance when fusing the search results of these\nsub-queries. To address the LLM hallucination problem, this paper also proposes\na novel consistency-based verification strategy to filter the paraphrased\nqueries that are factually incorrect. Extensive experiments are conducted for\nad-hoc video search and known-item search on the TRECVid datasets. We provide\nempirical insights into how traditionally difficult-to-answer queries can be\nresolved by query paraphrasing.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.12341v1"
    },
    {
        "title": "Enhancing Film Grain Coding in VVC: Improving Encoding Quality and\n  Efficiency",
        "authors": [
            "Vignesh V Menon",
            "Adam Wieckowski",
            "Christian Stoffers",
            "Jens Brandenburg",
            "Christian Lehmann",
            "Benjamin Bross",
            "Thomas Schierl",
            "Detlev Marpe"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This paper presents an in-depth analysis of film grain handling in\nopen-source implementations of the Versatile Video Coding (VVC) standard. We\nfocus on two key components: the Film Grain Analysis (FGA) module implemented\nin VVenC and the Film Grain Synthesis (FGS) module implemented in VVdeC. We\ndescribe the methodologies used to implement these modules and discuss the\ngeneration of Supplementary Enhancement Information (SEI) parameters to signal\nfilm grain characteristics in the encoded video sequences. Additionally, we\nconduct subjective and objective evaluations across Full HD videos to assess\nthe effectiveness of film grain handling. Our results demonstrate the\ncapability of the FGA and FGS techniques to accurately analyze and synthesize\nfilm grain, thereby improving the visual quality of encoded video content.\nOverall, our study contributes to advancing the understanding and\nimplementation of film grain handling techniques in VVC open-source\nimplementations, with implications for enhancing the viewing experience in\nmultimedia applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.12465v1"
    },
    {
        "title": "Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large\n  Language Models",
        "authors": [
            "Qiong Wu",
            "Zhaoxi Ke",
            "Yiyi Zhou",
            "Gen Luo",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.14093v2"
    },
    {
        "title": "Fine-grained Knowledge Graph-driven Video-Language Learning for Action\n  Recognition",
        "authors": [
            "Rui Zhang",
            "Yafen Lu",
            "Pengli Ji",
            "Junxiao Xue",
            "Xiaoran Yan"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recent work has explored video action recognition as a video-text matching\nproblem and several effective methods have been proposed based on large-scale\npre-trained vision-language models. However, these approaches primarily operate\nat a coarse-grained level without the detailed and semantic understanding of\naction concepts by exploiting fine-grained semantic connections between actions\nand body movements. To address this gap, we propose a contrastive\nvideo-language learning framework guided by a knowledge graph, termed KG-CLIP,\nwhich incorporates structured information into the CLIP model in the video\ndomain. Specifically, we construct a multi-modal knowledge graph composed of\nmulti-grained concepts by parsing actions based on compositional learning. By\nimplementing a triplet encoder and deviation compensation to adaptively\noptimize the margin in the entity distance function, our model aims to improve\nalignment of entities in the knowledge graph to better suit complex\nrelationship learning. This allows for enhanced video action recognition\ncapabilities by accommodating nuanced associations between graph components. We\ncomprehensively evaluate KG-CLIP on Kinetics-TPS, a large-scale action parsing\ndataset, demonstrating its effectiveness compared to competitive baselines.\nEspecially, our method excels at action recognition with few sample frames or\nlimited training data, which exhibits excellent data utilization and learning\ncapabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.14146v1"
    },
    {
        "title": "EidetiCom: A Cross-modal Brain-Computer Semantic Communication Paradigm\n  for Decoding Visual Perception",
        "authors": [
            "Linfeng Zheng",
            "Peilin Chen",
            "Shiqi Wang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Brain-computer interface (BCI) facilitates direct communication between the\nhuman brain and external systems by utilizing brain signals, eliminating the\nneed for conventional communication methods such as speaking, writing, or\ntyping. Nevertheless, the continuous generation of brain signals in BCI\nframeworks poses challenges for efficient storage and real-time transmission.\nWhile considering the human brain as a semantic source, the meaningful\ninformation associated with cognitive activities often gets obscured by\nsubstantial noise present in acquired brain signals, resulting in abundant\nredundancy. In this paper, we propose a cross-modal brain-computer semantic\ncommunication paradigm, named EidetiCom, for decoding visual perception under\nlimited-bandwidth constraint. The framework consists of three hierarchical\nlayers, each responsible for compressing the semantic information of brain\nsignals into representative features. These low-dimensional compact features\nare transmitted and converted into semantically meaningful representations at\nthe receiver side, serving three distinct tasks for decoding visual perception:\nbrain signal-based visual classification, brain-to-caption translation, and\nbrain-to-image generation, in a scalable manner. Through extensive qualitative\nand quantitative experiments, we demonstrate that the proposed paradigm\nfacilitates the semantic communication under low bit rate conditions ranging\nfrom 0.017 to 0.192 bits-per-sample, achieving high-quality semantic\nreconstruction and highlighting its potential for efficient storage and\nreal-time communication of brain recordings in BCI applications, such as\neidetic memory storage and assistive communication for patients.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.14936v1"
    },
    {
        "title": "Structure-Aware Residual-Center Representation for Self-Supervised\n  Open-Set 3D Cross-Modal Retrieval",
        "authors": [
            "Yang Xu",
            "Yifan Feng",
            "Yu Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Existing methods of 3D cross-modal retrieval heavily lean on category\ndistribution priors within the training set, which diminishes their efficacy\nwhen tasked with unseen categories under open-set environments. To tackle this\nproblem, we propose the Structure-Aware Residual-Center Representation (SRCR)\nframework for self-supervised open-set 3D cross-modal retrieval. To address the\ncenter deviation due to category distribution differences, we utilize the\nResidual-Center Embedding (RCE) for each object by nested auto-encoders, rather\nthan directly mapping them to the modality or category centers. Besides, we\nperform the Hierarchical Structure Learning (HSL) approach to leverage the\nhigh-order correlations among objects for generalization, by constructing a\nheterogeneous hypergraph structure based on hierarchical inter-modality,\nintra-object, and implicit-category correlations. Extensive experiments and\nablation studies on four benchmarks demonstrate the superiority of our proposed\nframework compared to state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.15376v1"
    },
    {
        "title": "The Sketchfab 3D Creative Commons Collection (S3D3C)",
        "authors": [
            "Florian Spiess",
            "Raphael Waltenspül",
            "Heiko Schuldt"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The technology to capture, create, and use three-dimensional (3D) models has\nbecome increasingly accessible in recent years.\n  With increasing numbers of use cases for 3D models and collections of rapidly\nincreasing size, better methods to analyze the content of 3D models are\nrequired.\n  While previously proposed 3D model collections for research purposes exist,\nthese often contain only untextured geometry and are typically designed for a\nspecific application, which limits their use in quantitative evaluations of\nmodern 3D model analysis methods.\n  In this paper, we introduce the Sketchfab 3D Creative Commons Collection\n(S3D3C), a new 3D model research collection consisting of 40,802 creative\ncommons licensed models downloaded from the 3D model platform Sketchfab.\n  By including popular freely available models with a wide variety of technical\nproperties, such as textures, materials, and animations, we enable its use in\nthe evaluation of state-of-the-art geometry-based and view-based 3D model\nanalysis and retrieval techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.17205v1"
    },
    {
        "title": "An Inverse Partial Optimal Transport Framework for Music-guided Movie\n  Trailer Generation",
        "authors": [
            "Yutong Wang",
            "Sidan Zhu",
            "Hongteng Xu",
            "Dixin Luo"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Trailer generation is a challenging video clipping task that aims to select\nhighlighting shots from long videos like movies and re-organize them in an\nattractive way. In this study, we propose an inverse partial optimal transport\n(IPOT) framework to achieve music-guided movie trailer generation. In\nparticular, we formulate the trailer generation task as selecting and sorting\nkey movie shots based on audio shots, which involves matching the latent\nrepresentations across visual and acoustic modalities. We learn a multi-modal\nlatent representation model in the proposed IPOT framework to achieve this aim.\nIn this framework, a two-tower encoder derives the latent representations of\nmovie and music shots, respectively, and an attention-assisted Sinkhorn\nmatching network parameterizes the grounding distance between the shots' latent\nrepresentations and the distribution of the movie shots. Taking the\ncorrespondence between the movie shots and its trailer music shots as the\nobserved optimal transport plan defined on the grounding distances, we learn\nthe model by solving an inverse partial optimal transport problem, leading to a\nbi-level optimization strategy. We collect real-world movies and their trailers\nto construct a dataset with abundant label information called CMTD and,\naccordingly, train and evaluate various automatic trailer generators. Compared\nwith state-of-the-art methods, our IPOT method consistently shows superiority\nin subjective visual effects and objective quantitative measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19456v2"
    },
    {
        "title": "The Future is Meta: Metadata, Formats and Perspectives towards\n  Interactive and Personalized AV Content",
        "authors": [
            "Alexander Weller",
            "Werner Bleisteiner",
            "Christian Hufnagel",
            "Michael Iber"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The production of media content has undergone tremendous changes in recent\nyears. Multiple daily content updates are just as common for some platforms as\nis processing the provided content specifically for their target audiences.\nSuch features are made possible through metadata, which make information\naccessible by categorizing it. In conjunction with AI-supported tools, metadata\nare shaping the future of audio-visual content production, distribution and\nconsumption. It allows editors to effectively search through archives like in\nthe Tailored Media Project, broadcasters to provide content that is adapted to\nusers' surroundings like in the ARD Audiothek unterwegs project, or give users\nthe ability to experience audio-visual content from different perspectives like\nin the ORPHEUS project. Although these projects provide comprehensive insight\ninto the potential of metadata, their integration in existing infrastructures\nmeets several limitations. For example, content-related metadata may initially\nbe generated at some point during the production process but will then be lost\nat later stages due to current standards and incomplete software\nimplementations. In our contribution, we will discuss requirements and\npotential approaches and give an outlook on possible fields of application and\nuse-cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19590v1"
    },
    {
        "title": "HeadsetOff: Enabling Photorealistic Video Conferencing on Economical VR\n  Headsets",
        "authors": [
            "Yili Jin",
            "Xize Duan",
            "Fangxin Wang",
            "Xue Liu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Virtual Reality (VR) has become increasingly popular for remote\ncollaboration, but video conferencing poses challenges when the user's face is\ncovered by the headset. Existing solutions have limitations in terms of\naccessibility. In this paper, we propose HeadsetOff, a novel system that\nachieves photorealistic video conferencing on economical VR headsets by\nleveraging voice-driven face reconstruction. HeadsetOff consists of three main\ncomponents: a multimodal predictor, a generator, and an adaptive controller.\nThe predictor effectively predicts user future behavior based on different\nmodalities. The generator employs voice, head motion, and eye blink to animate\nthe human face. The adaptive controller dynamically selects the appropriate\ngenerator model based on the trade-off between video quality and delay.\nExperimental results demonstrate the effectiveness of HeadsetOff in achieving\nhigh-quality, low-latency video conferencing on economical VR headsets.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19988v2"
    },
    {
        "title": "Multimodal Fusion via Hypergraph Autoencoder and Contrastive Learning\n  for Emotion Recognition in Conversation",
        "authors": [
            "Zijian Yi",
            "Ziming Zhao",
            "Zhishu Shen",
            "Tiehua Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal emotion recognition in conversation (MERC) seeks to identify the\nspeakers' emotions expressed in each utterance, offering significant potential\nacross diverse fields. The challenge of MERC lies in balancing speaker modeling\nand context modeling, encompassing both long-distance and short-distance\ncontexts, as well as addressing the complexity of multimodal information\nfusion. Recent research adopts graph-based methods to model intricate\nconversational relationships effectively. Nevertheless, the majority of these\nmethods utilize a fixed fully connected structure to link all utterances,\nrelying on convolution to interpret complex context. This approach can\ninherently heighten the redundancy in contextual messages and excessive graph\nnetwork smoothing, particularly in the context of long-distance conversations.\nTo address this issue, we propose a framework that dynamically adjusts\nhypergraph connections by variational hypergraph autoencoder (VHGAE), and\nemploys contrastive learning to mitigate uncertainty factors during the\nreconstruction process. Experimental results demonstrate the effectiveness of\nour proposal against the state-of-the-art methods on IEMOCAP and MELD datasets.\nWe release the code to support the reproducibility of this work at\nhttps://github.com/yzjred/-HAUCL.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.00970v2"
    },
    {
        "title": "Towards Multimodal Emotional Support Conversation Systems",
        "authors": [
            "Yuqi Chu",
            "Lizi Liao",
            "Zhiyuan Zhou",
            "Chong-Wah Ngo",
            "Richang Hong"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The integration of conversational artificial intelligence (AI) into mental\nhealth care promises a new horizon for therapist-client interactions, aiming to\nclosely emulate the depth and nuance of human conversations. Despite the\npotential, the current landscape of conversational AI is markedly limited by\nits reliance on single-modal data, constraining the systems' ability to\nempathize and provide effective emotional support. This limitation stems from a\npaucity of resources that encapsulate the multimodal nature of human\ncommunication essential for therapeutic counseling. To address this gap, we\nintroduce the Multimodal Emotional Support Conversation (MESC) dataset, a\nfirst-of-its-kind resource enriched with comprehensive annotations across text,\naudio, and video modalities. This dataset captures the intricate interplay of\nuser emotions, system strategies, system emotion, and system responses, setting\na new precedent in the field. Leveraging the MESC dataset, we propose a general\nSequential Multimodal Emotional Support framework (SMES) grounded in\nTherapeutic Skills Theory. Tailored for multimodal dialogue systems, the SMES\nframework incorporates an LLM-based reasoning model that sequentially generates\nuser emotion recognition, system strategy prediction, system emotion\nprediction, and response generation. Our rigorous evaluations demonstrate that\nthis framework significantly enhances the capability of AI systems to mimic\ntherapist behaviors with heightened empathy and strategic responsiveness. By\nintegrating multimodal data in this innovative manner, we bridge the critical\ngap between emotion recognition and emotional support, marking a significant\nadvancement in conversational AI for mental health support.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.03650v2"
    },
    {
        "title": "The algorithmic nature of song-sequencing: statistical regularities in\n  music albums",
        "authors": [
            "Pedro Neto",
            "Martin Hartmann",
            "Geoff Luck",
            "Petri Toiviainen"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Based on a review of anecdotal beliefs, we explored patterns of\ntrack-sequencing within professional music albums. We found that songs with\nhigh levels of valence, energy and loudness are more likely to be positioned at\nthe beginning of each album. We also found that transitions between consecutive\ntracks tend to alternate between increases and decreases of valence and energy.\nThese findings were used to build a system which automates the process of\nalbum-sequencing. Our results and hypothesis have both practical and\ntheoretical applications. Practically, sequencing regularities can be used to\ninform playlist generation systems. Theoretically, we show weak to moderate\nsupport for the idea that music is perceived in both global and local contexts.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04383v1"
    },
    {
        "title": "Emotional Cues Extraction and Fusion for Multi-modal Emotion Prediction\n  and Recognition in Conversation",
        "authors": [
            "Haoxiang Shi",
            "Ziqi Liang",
            "Jun Yu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Emotion Prediction in Conversation (EPC) aims to forecast the emotions of\nforthcoming utterances by utilizing preceding dialogues. Previous EPC\napproaches relied on simple context modeling for emotion extraction,\noverlooking fine-grained emotion cues at the word level. Additionally, prior\nworks failed to account for the intrinsic differences between modalities,\nresulting in redundant information. To overcome these limitations, we propose\nan emotional cues extraction and fusion network, which consists of two stages:\na modality-specific learning stage that utilizes word-level labels and prosody\nlearning to construct emotion embedding spaces for each modality, and a\ntwo-step fusion stage for integrating multi-modal features. Moreover, the\nemotion features extracted by our model are also applicable to the Emotion\nRecognition in Conversation (ERC) task. Experimental results validate the\nefficacy of the proposed method, demonstrating superior performance on both\nIEMOCAP and MELD datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04547v1"
    },
    {
        "title": "Deep joint source-channel coding for wireless point cloud transmission",
        "authors": [
            "Cixiao Zhang",
            "Mufan Liu",
            "Wenjie Huang",
            "Yin Xu",
            "Yiling Xu",
            "Dazhi He"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The growing demand for high-quality point cloud transmission over wireless\nnetworks presents significant challenges, primarily due to the large data sizes\nand the need for efficient encoding techniques. In response to these\nchallenges, we introduce a novel system named Deep Point Cloud Semantic\nTransmission (PCST), designed for end-to-end wireless point cloud transmission.\nOur approach employs a progressive resampling framework using sparse\nconvolution to project point cloud data into a semantic latent space. These\nsemantic features are subsequently encoded through a deep joint source-channel\n(JSCC) encoder, generating the channel-input sequence. To enhance transmission\nefficiency, we use an adaptive entropy-based approach to assess the importance\nof each semantic feature, allowing transmission lengths to vary according to\ntheir predicted entropy. PCST is robust across diverse Signal-to-Noise Ratio\n(SNR) levels and supports an adjustable rate-distortion (RD) trade-off,\nensuring flexible and efficient transmission. Experimental results indicate\nthat PCST significantly outperforms traditional separate source-channel coding\n(SSCC) schemes, delivering superior reconstruction quality while achieving over\na 50% reduction in bandwidth usage.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04889v1"
    },
    {
        "title": "Navigating Weight Prediction with Diet Diary",
        "authors": [
            "Yinxuan Gui",
            "Bin Zhu",
            "Jingjing Chen",
            "Chong-Wah Ngo",
            "Yu-Gang Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Current research in food analysis primarily concentrates on tasks such as\nfood recognition, recipe retrieval and nutrition estimation from a single\nimage. Nevertheless, there is a significant gap in exploring the impact of food\nintake on physiological indicators (e.g., weight) over time. This paper\naddresses this gap by introducing the DietDiary dataset, which encompasses\ndaily dietary diaries and corresponding weight measurements of real users.\nFurthermore, we propose a novel task of weight prediction with a dietary diary\nthat aims to leverage historical food intake and weight to predict future\nweights. To tackle this task, we propose a model-agnostic time series\nforecasting framework. Specifically, we introduce a Unified Meal Representation\nLearning (UMRL) module to extract representations for each meal. Additionally,\nwe design a diet-aware loss function to associate food intake with weight\nvariations. By conducting experiments on the DietDiary dataset with two\nstate-of-the-art time series forecasting models, NLinear and iTransformer, we\ndemonstrate that our proposed framework achieves superior performance compared\nto the original models. We make our dataset, code, and models publicly\navailable at: https://yxg1005.github.io/weight-prediction/.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.05445v2"
    },
    {
        "title": "Joint Optimization of Buffer Delay and HARQ for Video Communications",
        "authors": [
            "Baoping Cheng",
            "Peng Lei",
            "Xiaoyan Xie",
            "Tao Fu",
            "Yukun Zhang",
            "Xiaoming Tao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  To improve the quality of experience (QoE) in video communication over lossy\nnetworks, this paper presents a transmission method that jointly optimizes\nbuffer delay and Hybrid Automatic Repeat request (HARQ), referred to as\nBD-HARQ. This method operates on packet group and employs dynamic buffer delay\ncombined with HARQ strategy for transmission. By defining the QoE based on\nmetrics such as buffer delay, Forward Error Correction (FEC) redundancy, and\ndata recovery rate, the proposed method derives its closed-form expression\nthrough rigorous mathematical modeling and analysis. The optimal transmission\nparameters, i.e., the buffer delay and the FEC redundancy, are then determined\nand implemented, guaranteeing the real-time performance, transmission\nefficiency, and data recovery rate of video communication. Experimental results\ndemonstrate that the proposed method aligns well with its theoretical\nexpectations, and that it can provide up to 13.7% higher QoE compared to\nexisting methods and increase the tolerance for packet loss rate from 15%-22%\nto up to 31% while maintaining a high QoE.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.07957v1"
    },
    {
        "title": "SpeechEE: A Novel Benchmark for Speech Event Extraction",
        "authors": [
            "Bin Wang",
            "Meishan Zhang",
            "Hao Fei",
            "Yu Zhao",
            "Bobo Li",
            "Shengqiong Wu",
            "Wei Ji",
            "Min Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Event extraction (EE) is a critical direction in the field of information\nextraction, laying an important foundation for the construction of structured\nknowledge bases. EE from text has received ample research and attention for\nyears, yet there can be numerous real-world applications that require direct\ninformation acquisition from speech signals, online meeting minutes, interview\nsummaries, press releases, etc. While EE from speech has remained\nunder-explored, this paper fills the gap by pioneering a SpeechEE, defined as\ndetecting the event predicates and arguments from a given audio speech. To\nbenchmark the SpeechEE task, we first construct a large-scale high-quality\ndataset. Based on textual EE datasets under the sentence, document, and\ndialogue scenarios, we convert texts into speeches through both manual\nreal-person narration and automatic synthesis, empowering the data with diverse\nscenarios, languages, domains, ambiences, and speaker styles. Further, to\neffectively address the key challenges in the task, we tailor an E2E SpeechEE\nsystem based on the encoder-decoder architecture, where a novel Shrinking Unit\nmodule and a retrieval-aided decoding mechanism are devised. Extensive\nexperimental results on all SpeechEE subsets demonstrate the efficacy of the\nproposed model, offering a strong baseline for the task. At last, being the\nfirst work on this topic, we shed light on key directions for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09462v2"
    },
    {
        "title": "Exploring the Role of Audio in Multimodal Misinformation Detection",
        "authors": [
            "Moyang Liu",
            "Yukun Liu",
            "Ruibo Fu",
            "Zhengqi Wen",
            "Jianhua Tao",
            "Xuefei Liu",
            "Guanjun Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  With the rapid development of deepfake technology, especially the deep audio\nfake technology, misinformation detection on the social media scene meets a\ngreat challenge. Social media data often contains multimodal information which\nincludes audio, video, text, and images. However, existing multimodal\nmisinformation detection methods tend to focus only on some of these\nmodalities, failing to comprehensively address information from all modalities.\nTo comprehensively address the various modal information that may appear on\nsocial media, this paper constructs a comprehensive multimodal misinformation\ndetection framework. By employing corresponding neural network encoders for\neach modality, the framework can fuse different modality information and\nsupport the multimodal misinformation detection task. Based on the constructed\nframework, this paper explores the importance of the audio modality in\nmultimodal misinformation detection tasks on social media. By adjusting the\narchitecture of the acoustic encoder, the effectiveness of different acoustic\nfeature encoders in the multimodal misinformation detection tasks is\ninvestigated. Furthermore, this paper discovers that audio and video\ninformation must be carefully aligned, otherwise the misalignment across\ndifferent audio and video modalities can severely impair the model performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.12558v1"
    },
    {
        "title": "Cap2Sum: Learning to Summarize Videos by Generating Captions",
        "authors": [
            "Cairong Zhao",
            "Chutian Wang",
            "Zifan Song",
            "Guosheng Hu",
            "Haonan Chen",
            "Xiaofan Zhai"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  With the rapid growth of video data on the internet, video summarization is\nbecoming a very important AI technology. However, due to the high labelling\ncost of video summarization, existing studies have to be conducted on\nsmall-scale datasets, leading to limited performance and generalization\ncapacity. In this work, we introduce the use of dense video captions as a\nsupervision signal to train video summarization models. Motivated by this, we\npropose Cap2Sum, a model that learns to summarize videos by generating\ncaptions, to exploit dense video caption annotations. This weakly-supervised\napproach allows us to train the models on large-scale dense video caption\ndatasets to achieve better performance and generalization capacity. To further\nimprove the generalization capacity, we introduce a CLIP (a strong\nvision-language model) Prior mechanism to enhance the learning of important\nobjects that captions may ignore in the videos. In practice, Cap2Sum can\nperform zero-shot video summarization or be fine-tuned by the ground-truth\nsummary or video caption of the target dataset. To examine the performance of\nCap2Sum after weakly-supervised fine-tuning by the video captions, we propose\ntwo new datasets, TVSum-Caption and SumMe-Caption, which are derived from two\ncommon video summarization datasets and will be publicly released. We conduct\nextensive experiments and the results demonstrate that our method achieves\nsignificant improvements in performance and generalization capacity compared\nwith previous methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.12800v1"
    },
    {
        "title": "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion\n  Recognition",
        "authors": [
            "Cam-Van Thi Nguyen",
            "The-Son Le",
            "Anh-Tuan Mai",
            "Duc-Trong Le"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal Emotion Recognition in Conversations (ERC) is a typical multimodal\nlearning task in exploiting various data modalities concurrently. Prior studies\non effective multimodal ERC encounter challenges in addressing modality\nimbalances and optimizing learning across modalities. Dealing with these\nproblems, we present a novel framework named Ada2I, which consists of two\ninseparable modules namely Adaptive Feature Weighting (AFW) and Adaptive\nModality Weighting (AMW) for feature-level and modality-level balancing\nrespectively via leveraging both Inter- and Intra-modal interactions.\nAdditionally, we introduce a refined disparity ratio as part of our training\noptimization strategy, a simple yet effective measure to assess the overall\ndiscrepancy of the model's learning process when handling multiple modalities\nsimultaneously. Experimental results validate the effectiveness of Ada2I with\nstate-of-the-art performance compared to baselines on three benchmark datasets,\nparticularly in addressing modality imbalances.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.12895v1"
    },
    {
        "title": "Sec2Sec Co-attention for Video-Based Apparent Affective Prediction",
        "authors": [
            "Mingwei Sun",
            "Kunpeng Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Video-based apparent affect detection plays a crucial role in video\nunderstanding, as it encompasses various elements such as vision, audio,\naudio-visual interactions, and spatiotemporal information, which are essential\nfor accurate video predictions. However, existing approaches often focus on\nextracting only a subset of these elements, resulting in the limited predictive\ncapacity of their models. To address this limitation, we propose a novel\nLSTM-based network augmented with a Transformer co-attention mechanism for\npredicting apparent affect in videos. We demonstrate that our proposed Sec2Sec\nCo-attention Transformer surpasses multiple state-of-the-art methods in\npredicting apparent affect on two widely used datasets: LIRIS-ACCEDE and First\nImpressions. Notably, our model offers interpretability, allowing us to examine\nthe contributions of different time points to the overall prediction. The\nimplementation is available at: https://github.com/nestor-sun/sec2sec.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.15209v1"
    },
    {
        "title": "MultiMediate'24: Multi-Domain Engagement Estimation",
        "authors": [
            "Philipp Müller",
            "Michal Balazia",
            "Tobias Baur",
            "Michael Dietz",
            "Alexander Heimerl",
            "Anna Penzkofer",
            "Dominik Schiller",
            "François Brémond",
            "Jan Alexandersson",
            "Elisabeth André",
            "Andreas Bulling"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Estimating the momentary level of participant's engagement is an important\nprerequisite for assistive systems that support human interactions. Previous\nwork has addressed this task in within-domain evaluation scenarios, i.e.\ntraining and testing on the same dataset. This is in contrast to real-life\nscenarios where domain shifts between training and testing data frequently\noccur. With MultiMediate'24, we present the first challenge addressing\nmulti-domain engagement estimation. As training data, we utilise the NOXI\ndatabase of dyadic novice-expert interactions. In addition to within-domain\ntest data, we add two new test domains. First, we introduce recordings\nfollowing the NOXI protocol but covering languages that are not present in the\nNOXI training data. Second, we collected novel engagement annotations on the\nMPIIGroupInteraction dataset which consists of group discussions between three\nto four people. In this way, MultiMediate'24 evaluates the ability of\napproaches to generalise across factors such as language and cultural\nbackground, group size, task, and screen-mediated vs. face-to-face interaction.\nThis paper describes the MultiMediate'24 challenge and presents baseline\nresults. In addition, we discuss selected challenge solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.16625v1"
    },
    {
        "title": "Video to Music Moment Retrieval",
        "authors": [
            "Zijie Xin",
            "Minquan Wang",
            "Ye Ma",
            "Bo Wang",
            "Quan Chen",
            "Peng Jiang",
            "Xirong Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Adding proper background music helps complete a short video to be shared.\nTowards automating the task, previous research focuses on video-to-music\nretrieval (VMR), aiming to find amidst a collection of music the one best\nmatching the content of a given video. Since music tracks are typically much\nlonger than short videos, meaning the returned music has to be cut to a shorter\nmoment, there is a clear gap between the practical need and VMR. In order to\nbridge the gap, we propose in this paper video to music moment retrieval (VMMR)\nas a new task. To tackle the new task, we build a comprehensive dataset\nAd-Moment which contains 50K short videos annotated with music moments and\ndevelop a two-stage approach. In particular, given a test video, the most\nsimilar music is retrieved from a given collection. Then, a Transformer based\nmusic moment localization is performed. We term this approach Retrieval and\nLocalization (ReaL). Extensive experiments on real-world datasets verify the\neffectiveness of the proposed method for VMMR.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.16990v1"
    },
    {
        "title": "Multi-Reference Generative Face Video Compression with Contrastive\n  Learning",
        "authors": [
            "Goluck Konuko",
            "Giuseppe Valenzise"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Generative face video coding (GFVC) has been demonstrated as a potential\napproach to low-latency, low bitrate video conferencing. GFVC frameworks\nachieve an extreme gain in coding efficiency with over 70% bitrate savings when\ncompared to conventional codecs at bitrates below 10kbps. In recent MPEG/JVET\nstandardization efforts, all the information required to reconstruct video\nsequences using GFVC frameworks are adopted as part of the supplemental\nenhancement information (SEI) in existing compression pipelines. In light of\nthis development, we aim to address a challenge that has been weakly addressed\nin prior GFVC frameworks, i.e., reconstruction drift as the distance between\nthe reference and target frames increases. This challenge creates the need to\nupdate the reference buffer more frequently by transmitting more Intra-refresh\nframes, which are the most expensive element of the GFVC bitstream. To overcome\nthis problem, we propose instead multiple reference animation as a robust\napproach to minimizing reconstruction drift, especially when used in a\nbi-directional prediction mode. Further, we propose a contrastive learning\nformulation for multi-reference animation. We observe that using a contrastive\nlearning framework enhances the representation capabilities of the animation\ngenerator. The resulting framework, MRDAC (Multi-Reference Deep Animation\nCodec) can therefore be used to compress longer sequences with fewer reference\nframes or achieve a significant gain in reconstruction accuracy at comparable\nbitrates to previous frameworks. Quantitative and qualitative results show\nsignificant coding and reconstruction quality gains compared to previous GFVC\nmethods, and more accurate animation quality in presence of large pose and\nfacial expression changes.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.01029v1"
    },
    {
        "title": "Privacy-Preserving Multimedia Mobile Cloud Computing Using Protective\n  Perturbation",
        "authors": [
            "Zhongze Tang",
            "Mengmei Ye",
            "Yao Liu",
            "Sheng Wei"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Mobile cloud computing has been adopted in many multimedia applications,\nwhere the resource-constrained mobile device sends multimedia data (e.g.,\nimages) to remote cloud servers to request computation-intensive multimedia\nservices (e.g., image recognition). While significantly improving the\nperformance of the mobile applications, the cloud-based mechanism often causes\nprivacy concerns as the multimedia data and services are offloaded from the\ntrusted user device to untrusted cloud servers. Several recent studies have\nproposed perturbation-based privacy preserving mechanisms, which obfuscate the\noffloaded multimedia data to eliminate privacy exposures without affecting the\nfunctionality of the remote multimedia services. However, the existing privacy\nprotection approaches require the deployment of computation-intensive\nperturbation generation on the resource-constrained mobile devices. Also, the\nobfuscated images are typically not compliant with the standard image\ncompression algorithms and suffer from significant bandwidth consumption. In\nthis paper, we develop a novel privacy-preserving multimedia mobile cloud\ncomputing framework, namely $PMC^2$, to address the resource and bandwidth\nchallenges. $PMC^2$ employs secure confidential computing in the cloud to\ndeploy the perturbation generator, which addresses the resource challenge while\nmaintaining the privacy. Furthermore, we develop a neural compressor\nspecifically trained to compress the perturbed images in order to address the\nbandwidth challenge. We implement $PMC^2$ in an end-to-end mobile cloud\ncomputing system, based on which our evaluations demonstrate superior latency,\npower efficiency, and bandwidth consumption achieved by $PMC^2$ while\nmaintaining high accuracy in the target multimedia service.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.01710v1"
    },
    {
        "title": "Adaptive Offloading and Enhancement for Low-Light Video Analytics on\n  Mobile Devices",
        "authors": [
            "Yuanyi He",
            "Peng Yang",
            "Tian Qin",
            "Jiawei Hou",
            "Ning Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we explore adaptive offloading and enhancement strategies for\nvideo analytics tasks on computing-constrained mobile devices in low-light\nconditions. We observe that the accuracy of low-light video analytics varies\nfrom different enhancement algorithms. The root cause could be the disparities\nin the effectiveness of enhancement algorithms for feature extraction in\nanalytic models. Specifically, the difference in class activation maps (CAMs)\nbetween enhanced and low-light frames demonstrates a positive correlation with\nvideo analytics accuracy. Motivated by such observations, a novel enhancement\nquality assessment method is proposed on CAMs to evaluate the effectiveness of\ndifferent enhancement algorithms for low-light videos. Then, we design a\nmulti-edge system, which adaptively offloads and enhances low-light video\nanalytics tasks from mobile devices. To achieve the trade-off between the\nenhancement quality and the latency for all system-served mobile devices, we\npropose a genetic-based scheduling algorithm, which can find a near-optimal\nsolution in a reasonable time to meet the latency requirement. Thereby, the\noffloading strategies and the enhancement algorithms are properly selected\nunder the condition of limited end-edge bandwidth and edge computation\nresources. Simulation experiments demonstrate the superiority of the proposed\nsystem, improving accuracy up to 20.83\\% compared to existing benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.05297v1"
    },
    {
        "title": "A CLIP-based siamese approach for meme classification",
        "authors": [
            "Javier Huertas-Tato",
            "Christos Koutlis",
            "Symeon Papadopoulos",
            "David Camacho",
            "Ioannis Kompatsiaris"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Memes are an increasingly prevalent element of online discourse in social\nnetworks, especially among young audiences. They carry ideas and messages that\nrange from humorous to hateful, and are widely consumed. Their potentially high\nimpact requires adequate means of control to moderate their use in large scale.\nIn this work, we propose SimCLIP a deep learning-based architecture for\ncross-modal understanding of memes, leveraging a pre-trained CLIP encoder to\nproduce context-aware embeddings and a Siamese fusion technique to capture the\ninteractions between text and image. We perform an extensive experimentation on\nseven meme classification tasks across six datasets. We establish a new state\nof the art in Memotion7k with a 7.25% relative F1-score improvement, and\nachieve super-human performance on Harm-P with 13.73% F1-Score improvement. Our\napproach demonstrates the potential for compact meme classification models,\nenabling accurate and efficient meme monitoring. We share our code at\nhttps://github.com/jahuerta92/meme-classification-simclip\n",
        "pdf_link": "http://arxiv.org/pdf/2409.05772v1"
    },
    {
        "title": "REVISION: A Roadmap on Adaptive Video Streaming Optimization",
        "authors": [
            "Farzad Tashtarian",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Due to the soaring popularity of video applications and the consequent rise\nin video traffic on the Internet, technologies like HTTP Adaptive Streaming\n(HAS) are crucial for delivering high Quality of Experience (QoE) to consumers.\nHAS technology enables video players on consumer devices to enhance viewer\nengagement by dynamically adapting video content quality based on network\nconditions. This is especially relevant for consumer electronics as it ensures\nan optimized viewing experience across a variety of devices, from smartphones\nto smart TVs. This paper introduces REVISION, an efficient roadmap designed to\nenhance adaptive video streaming, a core feature of modern consumer\nelectronics. The REVISION optimization triangle highlights three essential\naspects for improving streaming: Objective, Input Space, and Action Domain.\nAdditionally, REVISION proposes a novel layer-based architecture tailored to\nrefine video streaming systems, comprising Application, Control and Management,\nand Resource layers. Each layer is designed to optimize different components of\nthe streaming process, which is directly linked to the performance and\nefficiency of consumer devices. By adopting the principles of the REVISION,\nmanufacturers and developers can significantly improve the streaming\ncapabilities of consumer electronics, thereby enriching the consumer's\nmultimedia experience and accommodating the increasing demand for high-quality,\nreal-time video content. This approach addresses the complexities of today's\ndiverse video streaming ecosystem and paves the way for future advancements in\nconsumer technology.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.06051v1"
    },
    {
        "title": "Bridging Discrete and Continuous: A Multimodal Strategy for Complex\n  Emotion Detection",
        "authors": [
            "Jiehui Jia",
            "Huan Zhang",
            "Jinhua Liang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In the domain of human-computer interaction, accurately recognizing and\ninterpreting human emotions is crucial yet challenging due to the complexity\nand subtlety of emotional expressions. This study explores the potential for\ndetecting a rich and flexible range of emotions through a multimodal approach\nwhich integrates facial expressions, voice tones, and transcript from video\nclips. We propose a novel framework that maps variety of emotions in a\nthree-dimensional Valence-Arousal-Dominance (VAD) space, which could reflect\nthe fluctuations and positivity/negativity of emotions to enable a more variety\nand comprehensive representation of emotional states. We employed K-means\nclustering to transit emotions from traditional discrete categorization to a\ncontinuous labeling system and built a classifier for emotion recognition upon\nthis system. The effectiveness of the proposed model is evaluated using the\nMER2024 dataset, which contains culturally consistent video clips from Chinese\nmovies and TV series, annotated with both discrete and open-vocabulary emotion\nlabels. Our experiment successfully achieved the transformation between\ndiscrete and continuous models, and the proposed model generated a more diverse\nand comprehensive set of emotion vocabulary while maintaining strong accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.07901v1"
    },
    {
        "title": "Prototypical Prompting for Text-to-image Person Re-identification",
        "authors": [
            "Shuanglin Yan",
            "Jun Liu",
            "Neng Dong",
            "Liyan Zhang",
            "Jinhui Tang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we study the problem of Text-to-Image Person Re-identification\n(TIReID), which aims to find images of the same identity described by a text\nsentence from a pool of candidate images. Benefiting from Vision-Language\nPre-training, such as CLIP (Contrastive Language-Image Pretraining), the TIReID\ntechniques have achieved remarkable progress recently. However, most existing\nmethods only focus on instance-level matching and ignore identity-level\nmatching, which involves associating multiple images and texts belonging to the\nsame person. In this paper, we propose a novel prototypical prompting framework\n(Propot) designed to simultaneously model instance-level and identity-level\nmatching for TIReID. Our Propot transforms the identity-level matching problem\ninto a prototype learning problem, aiming to learn identity-enriched\nprototypes. Specifically, Propot works by 'initialize, adapt, enrich, then\naggregate'. We first use CLIP to generate high-quality initial prototypes.\nThen, we propose a domain-conditional prototypical prompting (DPP) module to\nadapt the prototypes to the TIReID task using task-related information.\nFurther, we propose an instance-conditional prototypical prompting (IPP) module\nto update prototypes conditioned on intra-modal and inter-modal instances to\nensure prototype diversity. Finally, we design an adaptive prototype\naggregation module to aggregate these prototypes, generating final\nidentity-enriched prototypes. With identity-enriched prototypes, we diffuse its\nrich identity information to instances through prototype-to-instance\ncontrastive loss to facilitate identity-level matching. Extensive experiments\nconducted on three benchmarks demonstrate the superiority of Propot compared to\nexisting TIReID methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.09427v1"
    },
    {
        "title": "Multi-view Hypergraph-based Contrastive Learning Model for Cold-Start\n  Micro-video Recommendation",
        "authors": [
            "Sisuo Lyu",
            "Xiuze Zhou",
            "Xuming Hu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  With the widespread use of mobile devices and the rapid growth of micro-video\nplatforms such as TikTok and Kwai, the demand for personalized micro-video\nrecommendation systems has significantly increased. Micro-videos typically\ncontain diverse information, such as textual metadata, visual cues (e.g., cover\nimages), and dynamic video content, significantly affecting user interaction\nand engagement patterns. However, most existing approaches often suffer from\nthe problem of over-smoothing, which limits their ability to capture\ncomprehensive interaction information effectively. Additionally, cold-start\nscenarios present ongoing challenges due to sparse interaction data and the\nunderutilization of available interaction signals.\n  To address these issues, we propose a Multi-view Hypergraph-based Contrastive\nlearning model for cold-start micro-video Recommendation (MHCR). MHCR\nintroduces a multi-view multimodal feature extraction layer to capture\ninteraction signals from various perspectives and incorporates multi-view\nself-supervised learning tasks to provide additional supervisory signals.\nThrough extensive experiments on two real-world datasets, we show that MHCR\nsignificantly outperforms existing video recommendation models and effectively\nmitigates cold-start challenges. Our code is available at\nhttps://anonymous.4open.science/r/MHCR-02EF.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.09638v1"
    },
    {
        "title": "Enhancing Few-Shot Classification without Forgetting through Multi-Level\n  Contrastive Constraints",
        "authors": [
            "Bingzhi Chen",
            "Haoming Zhou",
            "Yishu Liu",
            "Biqing Zeng",
            "Jiahui Pan",
            "Guangming Lu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Most recent few-shot learning approaches are based on meta-learning with\nepisodic training. However, prior studies encounter two crucial problems: (1)\n\\textit{the presence of inductive bias}, and (2) \\textit{the occurrence of\ncatastrophic forgetting}. In this paper, we propose a novel Multi-Level\nContrastive Constraints (MLCC) framework, that jointly integrates\nwithin-episode learning and across-episode learning into a unified interactive\nlearning paradigm to solve these issues. Specifically, we employ a space-aware\ninteraction modeling scheme to explore the correct inductive paradigms for each\nclass between within-episode similarity/dis-similarity distributions.\nAdditionally, with the aim of better utilizing former prior knowledge, a\ncross-stage distribution adaption strategy is designed to align the\nacross-episode distributions from different time stages, thus reducing the\nsemantic gap between existing and past prediction distribution. Extensive\nexperiments on multiple few-shot datasets demonstrate the consistent\nsuperiority of MLCC approach over the existing state-of-the-art baselines.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.11286v1"
    },
    {
        "title": "Analyzing Recursiveness in Multimodal Generative Artificial\n  Intelligence: Stability or Divergence?",
        "authors": [
            "Javier Conde",
            "Tobias Cheung",
            "Gonzalo Martínez",
            "Pedro Reviriego",
            "Rik Sarkar"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  One of the latest trends in generative Artificial Intelligence is tools that\ngenerate and analyze content in different modalities, such as text and images,\nand convert information from one to the other. From a conceptual point of view,\nit is interesting to study whether these modality changes incur information\nloss and to what extent. This is analogous to variants of the classical game\ntelephone, where players alternate between describing images and creating\ndrawings based on those descriptions leading to unexpected transformations of\nthe original content. In the case of AI, modality changes can be applied\nrecursively, starting from an image to extract a text that describes it; using\nthe text to generate a second image, extracting a text that describes it, and\nso on. As this process is applied recursively, AI tools are generating content\nfrom one mode to use them to create content in another mode and so on. Ideally,\nthe embeddings of all of them would remain close to those of the original\ncontent so that only small variations are observed in the generated content\nversus the original one. However, it may also be the case the distance to the\noriginal embeddings increases in each iteration leading to a divergence in the\nprocess and to content that is barely related to the original one. In this\npaper, we present the results of an empirical study on the impact of recursive\nmodality changes using GPT-4o, a state-of-the-art AI multimodal tool, and\nDALL-E 3. The results show that the multimodality loop diverges from the\ninitial image without converging to anything specific. We have observed\ndifferences depending on the type of initial image and the configuration of the\nmodels. These findings are particularly relevant due to the increasing use of\nthese tools for content generation, reconstruction, and adaptation, and their\npotential implications for the content on the Internet of the future.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.16297v1"
    },
    {
        "title": "Language-oriented Semantic Communication for Image Transmission with\n  Fine-Tuned Diffusion Model",
        "authors": [
            "Xinfeng Wei",
            "Haonan Tong",
            "Nuocheng Yang",
            "Changchuan Yin"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Ubiquitous image transmission in emerging applications brings huge overheads\nto limited wireless resources. Since that text has the characteristic of\nconveying a large amount of information with very little data, the transmission\nof the descriptive text of an image can reduce the amount of transmitted data.\nIn this context, this paper develops a novel semantic communication framework\nbased on a text-2-image generative model (Gen-SC). In particular, a transmitter\nconverts the input image to textual modality data. Then the text is transmitted\nthrough a noisy channel to the receiver. The receiver then uses the received\ntext to generate images. Additionally, to improve the robustness of text\ntransmission over noisy channels, we designed a transformer-based text\ntransmission codec model. Moreover, we obtained a personalized knowledge base\nby fine-tuning the diffusion model to meet the requirements of task-oriented\ntransmission scenarios. Simulation results show that the proposed framework can\nachieve high perceptual quality with reducing the transmitted data volume by up\nto 99% and is robust to wireless channel noise in terms of portrait image\ntransmission.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.17104v1"
    },
    {
        "title": "Modeling the Popularity of Events on Web by Sparsity and\n  Mutual-Excitation Guided Graph Neural Network",
        "authors": [
            "Jiaxin Deng",
            "Linlin Jia",
            "Junbiao Pang",
            "Qingming Huang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The content of a webpage described or posted an event in the cyberspace\ninevitably reflects viewpoints, values and trends of the physical society.\nMapping an event on web to the popularity score plays a pivot role to sense the\nsocial trends from the cyberspace. However, the complex semantic correspondence\nbetween texts and images, as well as the implicit text-image-popularity mapping\nmechanics pose a significant challenge to this non-trivial task. In this paper,\nwe address this problem from a viewpoint of understanding the interpretable\nmapping mechanics. Concretely, we organize the keywords from different events\ninto an unified graph. The unified graph facilitates to model the popularity of\nevents via two-level mappings, i.e., the self excitation and the mutual\nexcitation. The self-excitation assumes that each keyword forms the popularity\nwhile the mutual-excitation models that two keywords would excite each other to\ndetermine the popularity of an event. Specifically, we use Graph Neural Network\n(GNN) as the backbone to model the self-excitation, the mutual excitation and\nthe context of images into a sparse and deep factor model. Besides, to our best\nknowledge, we release a challenge web event dataset for the popularity\nprediction task. The experimental results on three public datasets demonstrate\nthat our method achieves significant improvements and outperforms the\nstate-of-the-art methods. Dataset is publicly available at:\nhttps://github.com/pangjunbiao/Hot-events-dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.17678v1"
    },
    {
        "title": "Signal Processing for Haptic Surface Modeling: a Review",
        "authors": [
            "Antonio Luigi Stefani",
            "Niccolò Bisagno",
            "Andrea Rosani",
            "Nicola Conci",
            "Francesco De Natale"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Haptic feedback has been integrated into Virtual and Augmented Reality,\ncomplementing acoustic and visual information and contributing to an all-round\nimmersive experience in multiple fields, spanning from the medical domain to\nentertainment and gaming. Haptic technologies involve complex\ncross-disciplinary research that encompasses sensing, data representation,\ninteractive rendering, perception, and quality of experience. The standard\nprocessing pipeline, consists of (I) sensing physical features in the real\nworld using a transducer, (II) modeling and storing the collected information\nin some digital format, (III) communicating the information, and finally, (IV)\nrendering the haptic information through appropriate devices, thus producing a\nuser experience (V) perceptually close to the original physical world. Among\nthese areas, sensing, rendering and perception have been deeply investigated\nand are the subject of different comprehensive surveys available in the\nliterature. Differently, research dealing with haptic surface modeling and data\nrepresentation still lacks a comprehensive dissection. In this work, we aim at\nproviding an overview on modeling and representation of haptic surfaces from a\nsignal processing perspective, covering the aspects that lie in between haptic\ninformation acquisition on one side and rendering and perception on the other\nside. We analyze, categorize, and compare research papers that address the\nhaptic surface modeling and data representation, pointing out existing gaps and\npossible research directions.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.20142v1"
    },
    {
        "title": "Energy-Quality-aware Variable Framerate Pareto-Front for Adaptive Video\n  Streaming",
        "authors": [
            "Prajit T Rajendran",
            "Samira Afzal",
            "Vignesh V Menon",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Optimizing framerate for a given bitrate-spatial resolution pair in adaptive\nvideo streaming is essential to maintain perceptual quality while considering\ndecoding complexity. Low framerates at low bitrates reduce compression\nartifacts and decrease decoding energy. We propose a novel method,\nDecoding-complexity aware Framerate Prediction (DECODRA), which employs a\nVariable Framerate Pareto-front approach to predict an optimized framerate that\nminimizes decoding energy under quality degradation constraints. DECODRA\ndynamically adjusts the framerate based on current bitrate and spatial\nresolution, balancing trade-offs between framerate, perceptual quality, and\ndecoding complexity. Extensive experimentation with the Inter-4K dataset\ndemonstrates DECODRA's effectiveness, yielding an average decoding energy\nreduction of up to 13.45%, with minimal VMAF reduction of 0.33 points at a\nlow-quality degradation threshold, compared to the default 60 fps encoding.\nEven at an aggressive threshold, DECODRA achieves significant energy savings of\n13.45% while only reducing VMAF by 2.11 points. In this way, DECODRA extends\nmobile device battery life and reduces the energy footprint of streaming\nservices by providing a more energy-efficient video streaming pipeline.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.00849v1"
    },
    {
        "title": "Perceptual Quality Assessment of Octree-RAHT Encoded 3D Point Clouds",
        "authors": [
            "Dongshuai Duan",
            "Honglei Su",
            "Qi Liu",
            "Hui Yuan",
            "Wei Gao",
            "Jiarun Song",
            "Zhou Wang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  No-reference bitstream-layer point cloud quality assessment (PCQA) can be\ndeployed without full decoding at any network node to achieve real-time quality\nmonitoring. In this work, we focus on the PCQA problem dedicated to Octree-RAHT\nencoding mode. First, to address the issue that existing PCQA databases have a\nsmall scale and limited distortion levels, we establish the WPC5.0 database\nwhich is the first one dedicated to Octree-RAHT encoding mode with a scale of\n400 distorted point clouds (PCs) including 4 geometric multiplied by 5 attitude\ndistortion levels. Then, we propose the first PCQA model dedicated to\nOctree-RAHT encoding mode by parsing PC bitstreams without full decoding. The\nmodel introduces texture bitrate (TBPP) to predict texture complexity (TC) and\nfurther derives the texture distortion factor. In addition, the Geometric\nQuantization Parameter (PQS) is used to estimate the geometric distortion\nfactor, which is then integrated into the model along with the texture\ndistortion factor to obtain the proposed PCQA model named streamPCQ-OR. The\nproposed model has been compared with other advanced PCQA methods on the\nWPC5.0, BASICS and M-PCCD databases, and experimental results show that our\nmodel has excellent performance while having very low computational complexity,\nproviding a reliable choice for time-critical applications. To facilitate\nsubsequent research, the database and source code will be publicly released at\nhttps://github.com/qdushl/Waterloo-Point-Cloud-Database-5.0.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.06729v2"
    },
    {
        "title": "Contrastive Knowledge Distillation for Robust Multimodal Sentiment\n  Analysis",
        "authors": [
            "Zhongyi Sang",
            "Kotaro Funakoshi",
            "Manabu Okumura"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal sentiment analysis (MSA) systems leverage information from\ndifferent modalities to predict human sentiment intensities. Incomplete\nmodality is an important issue that may cause a significant performance drop in\nMSA systems. By generative imputation, i.e., recovering the missing data from\navailable data, systems may achieve robust performance but will lead to high\ncomputational costs. This paper introduces a knowledge distillation method,\ncalled `Multi-Modal Contrastive Knowledge Distillation' (MM-CKD), to address\nthe issue of incomplete modality in video sentiment analysis with lower\ncomputation cost, as a novel non-imputation-based method. We employ Multi-view\nSupervised Contrastive Learning (MVSC) to transfer knowledge from a teacher\nmodel to student models. This approach not only leverages cross-modal knowledge\nbut also introduces cross-sample knowledge with supervision, jointly improving\nthe performance of both teacher and student models through online learning. Our\nmethod gives competitive results with significantly lower computational costs\nthan state-of-the-art imputation-based methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.08692v1"
    },
    {
        "title": "Rethinking Bjøntegaard Delta for Compression Efficiency Evaluation:\n  Are We Calculating It Precisely and Reliably?",
        "authors": [
            "Xinyu Hang",
            "Shenpeng Song",
            "Zhimeng Huang",
            "Chuanmin Jia",
            "Siwei Ma",
            "Wen Gao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  For decades, the Bj{\\o}ntegaard Delta (BD) has been the metric for evaluating\ncodec Rate-Distortion (R-D) performance. Yet, in most studies, BD is determined\nusing just 4-5 R-D data points, could this be sufficient? As codecs and quality\nmetrics advance, does the conventional BD estimation still hold up? Crucially,\nare the performance improvements of new codecs and tools genuine, or merely\nartifacts of estimation flaws? This paper addresses these concerns by\nreevaluating BD estimation. We present a novel approach employing a\nparameterized deep neural network to model R-D curves with high precision\nacross various metrics, accompanied by a comprehensive R-D dataset. This\napproach both assesses the reliability of BD calculations and serves as a\nprecise BD estimator. Our findings advocate for the adoption of rigorous R-D\nsampling and reliability metrics in future compression research to ensure the\nvalidity and reliability of results.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12220v1"
    },
    {
        "title": "A 3D Framework for Improving Low-Latency Multi-Channel Live Streaming",
        "authors": [
            "Aizierjiang Aiersilan",
            "Zhiqiang Wang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The advent of 5G has driven the demand for high-quality, low-latency live\nstreaming. However, challenges such as managing the increased data volume,\nensuring synchronization across multiple streams, and maintaining consistent\nquality under varying network conditions persist, particularly in real-time\nvideo streaming. To address these issues, we propose a novel framework that\nleverages 3D virtual environments within game engines (eg. Unity 3D) to\noptimize multi-channel live streaming. Our approach consolidates multi-camera\nvideo data into a single stream using multiple virtual 3D canvases,\nsignificantly increasing channel amounts while reducing latency and enhancing\nuser flexibility. For demonstration of our approach, we utilize the Unity 3D\nengine to integrate multiple video inputs into a single-channel stream,\nsupporting one-to-many broadcasting, one-to-one video calling, and real-time\ncontrol of video channels. By mapping video data onto a world-space canvas and\ncapturing it via an in-world camera, we minimize redundant data transmission,\nachieving efficient, low-latency streaming. Our results demonstrate that this\nmethod outperforms existing multi-channel live streaming solutions in both\nlatency reduction and user interaction. Our live video streaming system\naffiliated with this paper is also open-source at\nhttps://github.com/Aizierjiang/LiveStreaming.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16284v1"
    },
    {
        "title": "Personalized Playback Technology: How Short Video Services Create\n  Excellent User Experience",
        "authors": [
            "Weihui Deng",
            "Zhiwei Fan",
            "Deliang Fu",
            "Yun Gong",
            "Shenglan Huang",
            "Xiaocheng Li",
            "Zheng Li",
            "Yiting Liao",
            "He Liu",
            "Chunyu Qiao",
            "Bin Wang",
            "Zhen Wang",
            "Zhengyu Xiong"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Short-form video content has become increasingly popular and influential in\nrecent years. Its concise yet engaging format aligns well with todays'\nfast-paced and on-the-go lifestyles, making it a dominating trend in the\ndigital world. As one of the front runners in the short video platform space,\nByteDance has been highly successful in delivering a one-of-a-kind short video\nexperience and attracting billions of users worldwide. One key contributing\nfactor is its advanced end-to-end personalized short video playback technology,\nwhere we pioneered and developed the new technical field over the past five\nyears to optimize user experience. This paper introduces the major concepts and\nmethodologies of this personalized video playback technology that distinguish\nit from traditional multimedia technologies. More details, including goal\nsetting, iterative process, modeling, experimental methods and required\nsupporting systems, are also provided to encourage deeper research in this\narea.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.17073v2"
    },
    {
        "title": "Cross-Platform Neural Video Coding: A Case Study",
        "authors": [
            "Ruhan Conceição",
            "Marcelo Porto",
            "Wen-Hsiao Peng",
            "Luciano Agostini"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we first show that current learning-based video codecs,\nspecifically the SSF codec, are not suitable for real-world applications due to\nthe mismatch between the encoder and decoder caused by floating-point round-off\nerrors. To address this issue, we propose the static quantization of the hyper\nprior decoding path. The quantization parameters are determined through an\nexhaustive search of all possible combinations of observers and quantization\nschemes from PyTorch. For the SSF codec, when encoding and decoding on\ndifferent machines, the proposed solution effectively mitigates the mismatch\nissue and enhances compression efficiency results by preventing severe image\nquality degradation. When encoding and decoding are performed on the same\nmachine, it constrains the average BD-rate increase to 9.93% and 9.02% for UVG\nand HEVC-B sequences, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.20145v1"
    },
    {
        "title": "Multimodal Semantic Communication for Generative Audio-Driven Video\n  Conferencing",
        "authors": [
            "Haonan Tong",
            "Haopeng Li",
            "Hongyang Du",
            "Zhaohui Yang",
            "Changchuan Yin",
            "Dusit Niyato"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This paper studies an efficient multimodal data communication scheme for\nvideo conferencing. In our considered system, a speaker gives a talk to the\naudiences, with talking head video and audio being transmitted. Since the\nspeaker does not frequently change posture and high-fidelity transmission of\naudio (speech and music) is required, redundant visual video data exists and\ncan be removed by generating the video from the audio. To this end, we propose\na wave-to-video (Wav2Vid) system, an efficient video transmission framework\nthat reduces transmitted data by generating talking head video from audio. In\nparticular, full-duration audio and short-duration video data are synchronously\ntransmitted through a wireless channel, with neural networks (NNs) extracting\nand encoding audio and video semantics. The receiver then combines the decoded\naudio and video data, as well as uses a generative adversarial network (GAN)\nbased model to generate the lip movement videos of the speaker. Simulation\nresults show that the proposed Wav2Vid system can reduce the amount of\ntransmitted data by up to 83% while maintaining the perceptual quality of the\ngenerated conferencing video.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.22112v1"
    },
    {
        "title": "Learning-based Lossless Event Data Compression",
        "authors": [
            "Ahmadreza Sezavar",
            "Catarina Brites",
            "Joao Ascenso"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Emerging event cameras acquire visual information by detecting time domain\nbrightness changes asynchronously at the pixel level and, unlike conventional\ncameras, are able to provide high temporal resolution, very high dynamic range,\nlow latency, and low power consumption. Considering the huge amount of data\ninvolved, efficient compression solutions are very much needed. In this\ncontext, this paper presents a novel deep-learning-based lossless event data\ncompression scheme based on octree partitioning and a learned hyperprior model.\nThe proposed method arranges the event stream as a 3D volume and employs an\noctree structure for adaptive partitioning. A deep neural network-based entropy\nmodel, using a hyperprior, is then applied. Experimental results demonstrate\nthat the proposed method outperforms traditional lossless data compression\ntechniques in terms of compression ratio and bits per event.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.03010v1"
    },
    {
        "title": "Investigating Conceptual Blending of a Diffusion Model for Improving\n  Nonword-to-Image Generation",
        "authors": [
            "Chihaya Matsuhira",
            "Marc A. Kastner",
            "Takahiro Komamizu",
            "Takatsugu Hirayama",
            "Ichiro Ide"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Text-to-image diffusion models sometimes depict blended concepts in the\ngenerated images. One promising use case of this effect would be the\nnonword-to-image generation task which attempts to generate images intuitively\nimaginable from a non-existing word (nonword). To realize nonword-to-image\ngeneration, an existing study focused on associating nonwords with\nsimilar-sounding words. Since each nonword can have multiple similar-sounding\nwords, generating images containing their blended concepts would increase\nintuitiveness, facilitating creative activities and promoting computational\npsycholinguistics. Nevertheless, no existing study has quantitatively evaluated\nthis effect in either diffusion models or the nonword-to-image generation\nparadigm. Therefore, this paper first analyzes the conceptual blending in a\npretrained diffusion model, Stable Diffusion. The analysis reveals that a high\npercentage of generated images depict blended concepts when inputting an\nembedding interpolating between the text embeddings of two text prompts\nreferring to different concepts. Next, this paper explores the best text\nembedding space conversion method of an existing nonword-to-image generation\nframework to ensure both the occurrence of conceptual blending and image\ngeneration quality. We compare the conventional direct prediction approach with\nthe proposed method that combines $k$-nearest neighbor search and linear\nregression. Evaluation reveals that the enhanced accuracy of the embedding\nspace conversion by the proposed method improves the image generation quality,\nwhile the emergence of conceptual blending could be attributed mainly to the\nspecific dimensions of the high-dimensional text embedding space.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.03595v1"
    },
    {
        "title": "Inter-Frame Coding for Dynamic Meshes via Coarse-to-Fine Anchor Mesh\n  Generation",
        "authors": [
            "He Huang",
            "Lizhi Hou",
            "Qi Yang",
            "Yiling Xu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In the current Video-based Dynamic Mesh Coding (V-DMC) standard, inter-frame\ncoding is restricted to mesh frames with constant topology. Consequently,\ntemporal redundancy is not fully leveraged, resulting in suboptimal compression\nefficacy. To address this limitation, this paper introduces a novel\ncoarse-to-fine scheme to generate anchor meshes for frames with time-varying\ntopology. Initially, we generate a coarse anchor mesh using an octree-based\nnearest neighbor search. Motion estimation compensates for regions with\nsignificant motion changes during this process. However, the quality of the\ncoarse mesh is low due to its suboptimal vertices. To enhance details, the fine\nanchor mesh is further optimized using the Quadric Error Metrics (QEM)\nalgorithm to calculate more precise anchor points. The inter-frame anchor mesh\ngenerated herein retains the connectivity of the reference base mesh, while\nconcurrently preserving superior quality. Experimental results show that our\nmethod achieves 7.2% ~ 10.3% BD-rate gain compared to the existing V-DMC test\nmodel version 7.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.03921v1"
    },
    {
        "title": "Content-Adaptive Rate-Quality Curve Prediction Model in Media Processing\n  System",
        "authors": [
            "Shibo Yin",
            "Zhiyu Zhang",
            "Peirong Ning",
            "Qiubo Chen",
            "Jing Chen",
            "Quan Zhou",
            "Li Song"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In streaming media services, video transcoding is a common practice to\nalleviate bandwidth demands. Unfortunately, traditional methods employing a\nuniform rate factor (RF) across all videos often result in significant\ninefficiencies. Content-adaptive encoding (CAE) techniques address this by\ndynamically adjusting encoding parameters based on video content\ncharacteristics. However, existing CAE methods are often tightly coupled with\nspecific encoding strategies, leading to inflexibility. In this paper, we\npropose a model that predicts both RF-quality and RF-bitrate curves, which can\nbe utilized to derive a comprehensive bitrate-quality curve. This approach\nfacilitates flexible adjustments to the encoding strategy without necessitating\nmodel retraining. The model leverages codec features, content features, and\nanchor features to predict the bitrate-quality curve accurately. Additionally,\nwe introduce an anchor suspension method to enhance prediction accuracy.\nExperiments confirm that the actual quality metric (VMAF) of the compressed\nvideo stays within 1 of the target, achieving an accuracy of 99.14%. By\nincorporating our quality improvement strategy with the rate-quality curve\nprediction model, we conducted online A/B tests, obtaining both +0.107%\nimprovements in video views and video completions and +0.064% app duration\ntime. Our model has been deployed on the Xiaohongshu App.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05295v1"
    },
    {
        "title": "Interdisciplinary Translations: Sensory Perception as a Universal\n  Language",
        "authors": [
            "Xindi Kang",
            "Xuanyang Huang",
            "Mingdong Song",
            "Varvara Guljajeva",
            "JoAnn Kuchera-Morin"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This paper investigates sensory perception's pivotal role as a universal\ncommunicative bridge across varied cultures and disciplines, and how it\nmanifests its value in the study of media art, human computer interaction and\nartificial intelligence. By analyzing its function in non-verbal communication\nthrough interactive systems, and drawing on the interpretive model in\ntranslation studies where \"sense\" acts as a mediation between two languages,\nthis paper illustrates how interdisciplinary communication in media art and\nhuman-computer interaction is afforded by the abstract language of human\nsensory perception. Specific examples from traditional art, interactive media\nart, HCI, communication, and translation studies demonstrate how sensory\nfeedback translates and conveys meaning across diverse modalities of expression\nand how it fosters connections between humans, art, and technology. Pertaining\nto this topic, this paper analyzes the impact of sensory feedback systems in\ndesigning interactive experiences, and reveals the guiding role of sensory\nperception in the design philosophy of AI systems. Overall, the study aims to\nbroaden the understanding of sensory perception's role in communication,\nhighlighting its significance in the evolution of interactive experiences and\nits capacity to unify art, science, and the human experience.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05374v1"
    },
    {
        "title": "Low Complexity Learning-based Lossless Event-based Compression",
        "authors": [
            "Ahmadreza Sezavar",
            "Catarina Brites",
            "Joao Ascenso"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Event cameras are a cutting-edge type of visual sensors that capture data by\ndetecting brightness changes at the pixel level asynchronously. These cameras\noffer numerous benefits over conventional cameras, including high temporal\nresolution, wide dynamic range, low latency, and lower power consumption.\nHowever, the substantial data rates they produce require efficient compression\ntechniques, while also fulfilling other typical application requirements, such\nas the ability to respond to visual changes in real-time or near real-time.\nAdditionally, many event-based applications demand high accuracy, making\nlossless coding desirable, as it retains the full detail of the sensor data.\nLearning-based methods show great potential due to their ability to model the\nunique characteristics of event data thus allowing to achieve high compression\nrates. This paper proposes a low-complexity lossless coding solution based on\nthe quadtree representation that outperforms traditional compression algorithms\nin efficiency and speed, ensuring low computational complexity and minimal\ndelay for real-time applications. Experimental results show that the proposed\nmethod delivers better compression ratios, i.e., with fewer bits per event, and\nlower computational complexity compared to current lossless data compression\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07155v1"
    },
    {
        "title": "Harmonizing Pixels and Melodies: Maestro-Guided Film Score Generation\n  and Composition Style Transfer",
        "authors": [
            "F. Qi",
            "L. Ni",
            "C. Xu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  We introduce a film score generation framework to harmonize visual pixels and\nmusic melodies utilizing a latent diffusion model. Our framework processes film\nclips as input and generates music that aligns with a general theme while\noffering the capability to tailor outputs to a specific composition style. Our\nmodel directly produces music from video, utilizing a streamlined and efficient\ntuning mechanism on ControlNet. It also integrates a film encoder adept at\nunderstanding the film's semantic depth, emotional impact, and aesthetic\nappeal. Additionally, we introduce a novel, effective yet straightforward\nevaluation metric to evaluate the originality and recognizability of music\nwithin film scores. To fill this gap for film scores, we curate a comprehensive\ndataset of film videos and legendary original scores, injecting domain-specific\nknowledge into our data-driven generation model. Our model outperforms existing\nmethodologies in creating film scores, capable of generating music that\nreflects the guidance of a maestro's style, thereby redefining the benchmark\nfor automated film scores and laying a robust groundwork for future research in\nthis domain. The code and generated samples are available at\nhttps://anonymous.4open.science/r/HPM.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07539v1"
    },
    {
        "title": "TopoCode: Topologically Informed Error Detection and Correction in\n  Communication Systems",
        "authors": [
            "Hongzhi Guo"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Traditional error detection and correction codes focus on bit-level fidelity,\nwhich is insufficient for emerging technologies like eXtended Reality (XR) and\nholographic communications requiring high-data-rate, low-latency systems.\nBit-level metrics cannot comprehensively evaluate Quality-of-Service (QoS) in\nthese scenarios. This letter proposes TopoCode which leverages Topological Data\nAnalysis (TDA) and persistent homology to encode topological information for\nmessage-level error detection and correction. It introduces minimal redundancy\nwhile enabling effective data reconstruction, especially in low Signal-to-Noise\nRatio (SNR) conditions. TopoCode offers a promising approach to meet the\ndemands of next-generation communication systems prioritizing semantic accuracy\nand message-level integrity.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.12825v2"
    },
    {
        "title": "SpikEmo: Enhancing Emotion Recognition With Spiking Temporal Dynamics in\n  Conversations",
        "authors": [
            "Xiaomin Yu",
            "Feiyang Wang",
            "Ziyue Qiao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In affective computing, the task of Emotion Recognition in Conversations\n(ERC) has emerged as a focal area of research. The primary objective of this\ntask is to predict emotional states within conversations by analyzing\nmultimodal data including text, audio, and video. While existing studies have\nprogressed in extracting and fusing representations from multimodal data, they\noften overlook the temporal dynamics in the data during conversations. To\naddress this challenge, we have developed the SpikEmo framework, which is based\non spiking neurons and employs a Semantic & Dynamic Two-stage Modeling approach\nto more precisely capture the complex temporal features of multimodal emotional\ndata. Additionally, to tackle the class imbalance and emotional semantic\nsimilarity problems in the ERC tasks, we have devised an innovative combination\nof loss functions that significantly enhances the model's performance when\ndealing with ERC data characterized by long-tail distributions. Extensive\nexperiments conducted on multiple ERC benchmark datasets demonstrate that\nSpikEmo significantly outperforms existing state-of-the-art methods in ERC\ntasks. Our code is available at https://github.com/Yu-xm/SpikEmo.git.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13917v1"
    },
    {
        "title": "Optimal Transcoding Preset Selection for Live Video Streaming",
        "authors": [
            "Zahra Nabizadeh",
            "Maedeh Jamali",
            "Nader Karimi",
            "Shadrokh Samavi",
            "Shahram Shirani"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In today's digital landscape, video content dominates internet traffic,\nunderscoring the need for efficient video processing to support seamless live\nstreaming experiences on platforms like YouTube Live, Twitch, and Facebook\nLive. This paper introduces a comprehensive framework designed to optimize\nvideo transcoding parameters, with a specific focus on preset and bitrate\nselection to minimize distortion while respecting constraints on bitrate and\ntranscoding time. The framework comprises three main steps: feature extraction,\nprediction, and optimization. It leverages extracted features to predict\ntranscoding time and rate-distortion, employing both supervised and\nunsupervised methods. By utilizing integer linear programming, it identifies\nthe optimal sequence of presets and bitrates for video segments, ensuring\nreal-time application feasibility under set constraints. The results\ndemonstrate the framework's effectiveness in enhancing video quality for live\nstreaming, maintaining high standards of video delivery while managing\ncomputational resources efficiently. This optimization approach meets the\nevolving demands of video delivery by offering a solution for real-time\ntranscoding optimization. Evaluation using the User Generated Content dataset\nshowed an average PSNR improvement of 1.5 dB over the default Twitch\nconfiguration, highlighting significant PSNR gains. Additionally, subsequent\nexperiments demonstrated a BD-rate reduction of -49.60%, reinforcing the\nframework's superior performance over Twitch's default configuration.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.14613v1"
    },
    {
        "title": "Fully Automatic Deep Learning Pipeline for Whole Slide Image Quality\n  Assessment",
        "authors": [
            "Falah Jabar",
            "Lill-Tove Rasmussen Busund",
            "Biagio Ricciuti",
            "Masoud Tafavvoghi",
            "Mette Pøhl",
            "Sigve Andersen",
            "Tom Donnem",
            "David J. Kwiatkowski",
            "Mehrdad Rakaee"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In recent years, the use of deep learning (DL) methods, including\nconvolutional neural networks (CNNs) and vision transformers (ViTs), has\nsignificantly advanced computational pathology, enhancing both diagnostic\naccuracy and efficiency. Hematoxylin and Eosin (H&E) Whole Slide Images (WSI)\nplays a crucial role by providing detailed tissue samples for the analysis and\ntraining of DL models. However, WSIs often contain regions with artifacts such\nas tissue folds, blurring, as well as non-tissue regions (background), which\ncan negatively impact DL model performance. These artifacts are diagnostically\nirrelevant and can lead to inaccurate results. This paper proposes a fully\nautomatic supervised DL pipeline for WSI Quality Assessment (WSI-QA) that uses\na fused model combining CNNs and ViTs to detect and exclude WSI regions with\nartifacts, ensuring that only qualified WSI regions are used to build DL-based\ncomputational pathology applications. The proposed pipeline employs a\npixel-based segmentation model to classify WSI regions as either qualified or\nnon-qualified based on the presence of artifacts. The proposed model was\ntrained on a large and diverse dataset and validated with internal and external\ndata from various human organs, scanners, and H&E staining procedures.\nQuantitative and qualitative evaluations demonstrate the superiority of the\nproposed model, which outperforms state-of-the-art methods in WSI artifact\ndetection. The proposed model consistently achieved over 95% accuracy,\nprecision, recall, and F1 score across all artifact types. Furthermore, the\nWSI-QA pipeline shows strong generalization across different tissue types and\nscanning conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.16885v1"
    },
    {
        "title": "Racism in the Machine: Visualization Ethics in Digital Humanities\n  Projects",
        "authors": [
            "K. J. Hepworth",
            "Christopher Church"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Data visualizations are inherently rhetorical, and therefore bias-laden\nvisual artifacts that contain both explicit and implicit arguments. The\nimplicit arguments depicted in data visualizations are the net result of many\nseemingly minor decisions about data and design from inception of a research\nproject through to final publication of the visualization. Data workflow,\nselected visualization formats, and individual design decisions made within\nthose formats all frame and direct the possible range of interpretation, and\nthe potential for harm of any data visualization. Considering this, it is\nimperative that we take an ethical approach to the creation and use of data\nvisualizations. Therefore, we have suggested an ethical data visualization\nworkflow with the dual aim of minimizing harm to the subjects of our study and\nthe audiences viewing our visualization, while also maximizing the explanatory\ncapacity and effectiveness of the visualization itself. To explain this ethical\ndata visualization workflow, we examine two recent digital mapping projects,\nRacial Terror Lynchings and Map of White Supremacy Mob Violence.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.17704v1"
    },
    {
        "title": "Uncertainty-driven Sampling for Efficient Pairwise Comparison Subjective\n  Assessment",
        "authors": [
            "Shima Mohammadi",
            "João Ascenso"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Assessing image quality is crucial in image processing tasks such as\ncompression, super-resolution, and denoising. While subjective assessments\ninvolving human evaluators provide the most accurate quality scores, they are\nimpractical for large-scale or continuous evaluations due to their high cost\nand time requirements. Pairwise comparison subjective assessment tests, which\nrank image pairs instead of assigning scores, offer more reliability and\naccuracy but require numerous comparisons, leading to high costs. Although\nobjective quality metrics are more efficient, they lack the precision of\nsubjective tests, which are essential for benchmarking and training\nlearning-based quality metrics. This paper proposes an uncertainty-based\nsampling method to optimize the pairwise comparison subjective assessment\nprocess. By utilizing deep learning models to estimate human preferences and\nidentify pairs that need human labeling, the approach reduces the number of\nrequired comparisons while maintaining high accuracy. The key contributions\ninclude modeling uncertainty for accurate preference predictions and for\npairwise sampling. The experimental results demonstrate superior performance of\nthe proposed approach compared to traditional active sampling methods. Software\nis publicly available at: shimamohammadi/LBPS-EIC\n",
        "pdf_link": "http://arxiv.org/pdf/2411.18372v1"
    },
    {
        "title": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark",
        "authors": [
            "Changsheng Gao",
            "Yifan Ma",
            "Qiaoxi Chen",
            "Yenan Xu",
            "Dong Liu",
            "Weisi Lin"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding methods are applied\nto reduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three contributions to this\nfield. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nadvance the field of feature coding, facilitating more efficient large model\ndeployment. All source code and the dataset are now available at\n\\href{https://github.com/chansongoal/FCM-LM/tree/master}{https://github.com/chansongoal/FCM-LM/tree/master}.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.04307v2"
    },
    {
        "title": "Multimodal Classification and Out-of-distribution Detection for\n  Multimodal Intent Understanding",
        "authors": [
            "Hanlei Zhang",
            "Qianrui Zhou",
            "Hua Xu",
            "Jianhua Su",
            "Roberto Evans",
            "Kai Gao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal intent understanding is a significant research area that requires\neffectively leveraging multiple modalities to analyze human language. Existing\nmethods face two main challenges in this domain. Firstly, they have limitations\nin capturing nuanced and high-level semantics underlying complex\nin-distribution (ID) multimodal intents. Secondly, they exhibit poor\ngeneralization when confronted with unseen out-of-distribution (OOD) data in\nreal-world scenarios. To address these issues, we propose a novel method for\nboth ID classification and OOD detection (MIntOOD). We first introduce a\nweighted feature fusion network that models multimodal representations\neffectively. This network dynamically learns the importance of each modality,\nadapting to multimodal contexts. To develop discriminative representations that\nare conducive to both tasks, we synthesize pseudo-OOD data from convex\ncombinations of ID data and engage in multimodal representation learning from\nboth coarse-grained and fine-grained perspectives. The coarse-grained\nperspective focuses on distinguishing between ID and OOD binary classes, while\nthe fine-grained perspective enhances the understanding of ID data by\nincorporating binary confidence scores. These scores help to gauge the\ndifficulty of each sample, improving the classification of different ID\nclasses. Additionally, the fine-grained perspective captures instance-level\ninteractions between ID and OOD samples, promoting proximity among similar\ninstances and separation from dissimilar ones. We establish baselines for three\nmultimodal intent datasets and build an OOD benchmark. Extensive experiments on\nthese datasets demonstrate that our method significantly improves OOD detection\nperformance with a 3-10% increase in AUROC scores while achieving new\nstate-of-the-art results in ID classification. The full data and codes are\navailable at https://github.com/thuiar/MIntOOD.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.12453v1"
    },
    {
        "title": "Muse: A Multimodal Conversational Recommendation Dataset with\n  Scenario-Grounded User Profiles",
        "authors": [
            "Zihan Wang",
            "Xiaocui Yang",
            "Yongkang Liu",
            "Shi Feng",
            "Daling Wang",
            "Yifei Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Current conversational recommendation systems focus predominantly on text.\nHowever, real-world recommendation settings are generally multimodal, causing a\nsignificant gap between existing research and practical applications. To\naddress this issue, we propose Muse, the first multimodal conversational\nrecommendation dataset. Muse comprises 83,148 utterances from 7,000\nconversations centered around the Clothing domain. Each conversation contains\ncomprehensive multimodal interactions, rich elements, and natural dialogues.\nData in Muse are automatically synthesized by a multi-agent framework powered\nby multimodal large language models (MLLMs). It innovatively derives user\nprofiles from real-world scenarios rather than depending on manual design and\nhistory data for better scalability, and then it fulfills conversation\nsimulation and optimization. Both human and LLM evaluations demonstrate the\nhigh quality of conversations in Muse. Additionally, fine-tuning experiments on\nthree MLLMs demonstrate Muse's learnable patterns for recommendations and\nresponses, confirming its value for multimodal conversational recommendation.\nOur dataset and codes are available at\n\\url{https://anonymous.4open.science/r/Muse-0086}.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.18416v1"
    },
    {
        "title": "SFE-Net: Harnessing Biological Principles of Differential Gene\n  Expression for Improved Feature Selection in Deep Learning Networks",
        "authors": [
            "Yuqi Li",
            "Yuanzhong Zheng",
            "Yaoxuan Wang",
            "Jianjun Yin",
            "Haojun Fei"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In the realm of DeepFake detection, the challenge of adapting to various\nsynthesis methodologies such as Faceswap, Deepfakes, Face2Face, and\nNeuralTextures significantly impacts the performance of traditional machine\nlearning models. These models often suffer from static feature representation,\nwhich struggles to perform consistently across diversely generated deepfake\ndatasets. Inspired by the biological concept of differential gene expression,\nwhere gene activation is dynamically regulated in response to environmental\nstimuli, we introduce the Selective Feature Expression Network (SFE-Net). This\ninnovative framework integrates selective feature activation principles into\ndeep learning architectures, allowing the model to dynamically adjust feature\npriorities in response to varying deepfake generation techniques. SFE-Net\nemploys a novel mechanism that selectively enhances critical features essential\nfor accurately detecting forgeries, while reducing the impact of irrelevant or\nmisleading cues akin to adaptive evolutionary processes in nature. Through\nrigorous testing on a range of deepfake datasets, SFE-Net not only surpasses\nexisting static models in detecting sophisticated forgeries but also shows\nenhanced generalization capabilities in cross-dataset scenarios. Our approach\nsignificantly mitigates overfitting by maintaining a dynamic balance between\nfeature exploration and exploitation, thus producing more robust and effective\ndeepfake detection models. This bio-inspired strategy paves the way for\ndeveloping adaptive deep learning systems that are finely tuned to address the\nnuanced challenges posed by the varied nature of digital forgeries in modern\ndigital forensics.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.20799v1"
    },
    {
        "title": "Enhancing Neural Adaptive Wireless Video Streaming via Lower-Layer\n  Information Exposure and Online Tuning",
        "authors": [
            "Lingzhi Zhao",
            "Ying Cui",
            "Yuhang Jia",
            "Yunfei Zhang",
            "Klara Nahrstedt"
        ],
        "category": "cs.MM",
        "published_year": "2025",
        "summary": "  Deep reinforcement learning (DRL) demonstrates its promising potential in the\nrealm of adaptive video streaming and has recently received increasing\nattention. However, existing DRL-based methods for adaptive video streaming use\nonly application (APP) layer information, adopt heuristic training methods, and\ntrain generalized neural networks with pre-collected data. This paper aims to\nboost the quality of experience (QoE) of adaptive wireless video streaming by\nusing lower-layer information, deriving a rigorous training method, and\nadopting online tuning with real-time data. First, we formulate a more\ncomprehensive and accurate adaptive wireless video streaming problem as an\ninfinite stage discounted Markov decision process (MDP) problem by additionally\nincorporating past and lower-layer information, allowing a flexible tradeoff\nbetween QoE and costs for obtaining system information and solving the problem.\nIn the offline scenario (only with pre-collected data), we propose an enhanced\nasynchronous advantage actor-critic (eA3C) method by jointly optimizing the\nparameters of parameterized policy and value function. Specifically, we build\nan eA3C network consisting of a policy network and a value network that can\nutilize cross-layer, past, and current information and jointly train the eA3C\nnetwork using pre-collected samples. In the online scenario (with additional\nreal-time data), we propose two continual learning-based online tuning methods\nfor designing better policies for a specific user with different QoE and\ntraining time tradeoffs. Finally, experimental results show that the proposed\noffline policy can improve the QoE by 6.8~14.4% compared to the state-of-arts\nin the offline scenario, and the proposed online policies can further achieve\n6~28% gains in QoE over the proposed offline policy in the online scenario.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.01044v1"
    },
    {
        "title": "DiffCL: A Diffusion-Based Contrastive Learning Framework with Semantic\n  Alignment for Multimodal Recommendations",
        "authors": [
            "Qiya Song",
            "Jiajun Hu",
            "Lin Xiao",
            "Bin Sun",
            "Xieping Gao",
            "Shutao Li"
        ],
        "category": "cs.MM",
        "published_year": "2025",
        "summary": "  Multimodal recommendation systems integrate diverse multimodal information\ninto the feature representations of both items and users, thereby enabling a\nmore comprehensive modeling of user preferences. However, existing methods are\nhindered by data sparsity and the inherent noise within multimodal data, which\nimpedes the accurate capture of users' interest preferences. Additionally,\ndiscrepancies in the semantic representations of items across different\nmodalities can adversely impact the prediction accuracy of recommendation\nmodels. To address these challenges, we introduce a novel diffusion-based\ncontrastive learning framework (DiffCL) for multimodal recommendation. DiffCL\nemploys a diffusion model to generate contrastive views that effectively\nmitigate the impact of noise during the contrastive learning phase.\nFurthermore, it improves semantic consistency across modalities by aligning\ndistinct visual and textual semantic information through stable ID embeddings.\nFinally, the introduction of the Item-Item Graph enhances multimodal feature\nrepresentations, thereby alleviating the adverse effects of data sparsity on\nthe overall system performance. We conduct extensive experiments on three\npublic datasets, and the results demonstrate the superiority and effectiveness\nof the DiffCL.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.01066v1"
    },
    {
        "title": "An Efficient NVoD Scheme Using Implicit Error Correction and Subchannels\n  for Wireless Networks",
        "authors": [
            "Rafael Asorey-Cacheda",
            "Antonio-Javier Garcia-Sanchez",
            "Joan Garcia-Haro"
        ],
        "category": "cs.MM",
        "published_year": "2025",
        "summary": "  Implicit Error Correction (IEC) is a near Video-on-Demand (nVoD) scheme that\ntrades bandwidth utilization for initial playback delay to potentially support\nan infinite number of users. Additionally, it provides error protection without\nany further bandwidth increase by exploiting the implicit redundancy of nVoD\nprotocols, using linear combinations of the segments transmitted in a given\ntime slot. However, IEC packet loss protection is weaker at the beginning of\nthe playback due to the lack of implicit redundancy and lower decoding\nefficiency, resulting in worse subjective playback quality. In tackling this\nissue, this paper contributes with an extension of the original nVoD\narchitecture, enhancing its performance by adding a new element namely,\nsubchannels. These subdivisions of the original channels do not provide further\npacket loss protection but significantly improve the decoding efficiency, which\nin turn increases playback quality, especially at the beginning. Even for very\nhigh packet loss probabilities, subchannels are designed to obtain higher\ndecoding efficiency which results in greater packet loss protection than that\nprovided by IEC. The proposed scheme is especially useful in wireless\ncooperative networks using techniques such as network coding, as content\ntransmissions can be split into different subchannels in order to maximize\nnetwork efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.07650v1"
    },
    {
        "title": "Global Platform for Rich Media Conferencing and Collaboration",
        "authors": [
            "Harvey B. Newman",
            "Philippe Galvez",
            "Gregory Denis",
            "David Collados",
            "Kun Wei",
            "David Adamczyk"
        ],
        "category": "cs.MM",
        "published_year": "2003",
        "summary": "  The Virtual Rooms Videoconferencing Service (VRVS) provides a worldwide\nvideoconferencing service and collaborative environment to the research and\neducation communities. This system provides a low cost, bandwidth-efficient,\nextensible means for videoconferencing and remote collaboration over networks\nwithin the High Energy and Nuclear Physics communities (HENP). VRVS has become\na standard part of the toolset used daily by a large sector of HENP, and it is\nused increasingly for other DoE/NSF-supported programs. The current features\nincluded multi-protocol, multi-OS support for all significant video enabled\nclients including: H.323, Mbone, QuickTime, MPEG2, Java Media Framework, and\nother clients. The current architecture makes VRVS a distributed, highly\nfunctional, and efficient software-only system for multipoint audio, video and\nweb conferencing and collaboration over global IP networks. VRVS has developed\nthe VRVS-AG Reflector and a specialized Web interface that enables end users to\nconnect to any Access Grid (AG) session, in any of the AG \"virtual venues\" from\nanywhere worldwide. The VRVS system has now been running for the last five and\nhalf years, offering to the HENP community a working and reliable tool for\ncollaboration within groups and among physicists dispersed world-wide. The goal\nof this ongoing effort is to develop the next generation collaborative systems\nrunning over next generation networks. The new developments area integrate\nemerging standards, include all security aspects, and will extend the range of\nVRVS video technologies supported to cover the latest high end standards\nquality. We will focus the discussion on the new capability provides by the\nlatest version V3.0 and its future evolution.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0306116v2"
    },
    {
        "title": "From Digital Television to Internet?",
        "authors": [
            "Vita Hinze-Hoare"
        ],
        "category": "cs.MM",
        "published_year": "2004",
        "summary": "  This paper provides a general technical overview of the Multimedia Home\nPlatform (MHP) specifications. MHP is a generic interface between digital\napplications and user machines, whether they happen to be set top boxes,\ndigital TV sets or Multimedia PC's. MHP extends the DVB open standards.\nAddressed are MHP architexture, System core and MHP Profiles.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0409059v1"
    },
    {
        "title": "Self-Organizing the Abstract: Canvas as a Swarm Habitat for Collective\n  Memory, Perception and Cooperative Distributed Creativity",
        "authors": [
            "Vitorino Ramos"
        ],
        "category": "cs.MM",
        "published_year": "2004",
        "summary": "  Past experiences under the designation of \"Swarm Paintings\" conducted in\n2001, not only confirmed the possibility of realizing an artificial art (thus\nnon-human), as introduced into the process the questioning of creative\nmigration, specifically from the computer monitors to the canvas via a robotic\nharm. In more recent self-organized based research we seek to develop and\nprofound the initial ideas by using a swarm of autonomous robots (ARTsBOT\nproject 2002-03), that \"live\" avoiding the purpose of being merely a simple\nperpetrator of order streams coming from an external computer, but instead,\nthat actually co-evolve within the canvas space, acting (that is, laying ink)\naccording to simple inner threshold stimulus response functions, reacting\nsimultaneously to the chromatic stimulus present in the canvas environment done\nby the passage of their team-mates, as well as by the distributed feedback,\naffecting their future collective behaviour. In parallel, and in what respects\nto certain types of collective systems, we seek to confirm, in a physically\nembedded way, that the emergence of order (even as a concept) seems to be found\nat a lower level of complexity, based on simple and basic interchange of\ninformation, and on the local dynamic of parts, who, by self-organizing\nmechanisms tend to form an lived whole, innovative and adapting, allowing for\nemergent open-ended creative and distributed production. KEYWORDS: ArtSBots\nProject, Swarm Intelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm\nPaintings, Robot Paintings, Non-Human Art, Painting Emergence and Cooperation,\nArt and Complexity, ArtBots: The Robot Talent Show.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0412073v1"
    },
    {
        "title": "On the Design of Perceptual MPEG-Video Encryption Algorithms",
        "authors": [
            "Shujun Li",
            "Guanrong Chen",
            "Albert Cheung",
            "Bharat Bhargava",
            "Kwok-Tung Lo"
        ],
        "category": "cs.MM",
        "published_year": "2005",
        "summary": "  In this paper, some existing perceptual encryption algorithms of MPEG videos\nare reviewed and some problems, especially security defects of two recently\nproposed MPEG-video perceptual encryption schemes, are pointed out. Then, a\nsimpler and more effective design is suggested, which selectively encrypts\nfixed-length codewords (FLC) in MPEG-video bitstreams under the control of\nthree perceptibility factors. The proposed design is actually an encryption\nconfiguration that can work with any stream cipher or block cipher. Compared\nwith the previously-proposed schemes, the new design provides more useful\nfeatures, such as strict size-preservation, on-the-fly encryption and multiple\nperceptibility, which make it possible to support more applications with\ndifferent requirements. In addition, four different measures are suggested to\nprovide better security against known/chosen-plaintext attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0501014v3"
    },
    {
        "title": "A Distributed Multimedia Communication System and its Applications to\n  E-Learning",
        "authors": [
            "Hans L. Cycon",
            "Thomas C. Schmidt",
            "Matthias Waehlisch",
            "Mark Palkow",
            "Henrik Regensburg"
        ],
        "category": "cs.MM",
        "published_year": "2005",
        "summary": "  In this paper we report on a multimedia communication system including a\nVCoIP (Video Conferencing over IP) software with a distributed architecture and\nits applications for teaching scenarios. It is a simple, ready-to-use scheme\nfor distributed presenting, recording and streaming multimedia content. We also\nintroduce and investigate concepts and experiments to IPv6 user and session\nmobility, with the special focus on real-time video group communication.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0504106v1"
    },
    {
        "title": "Cryptanalysis of an MPEG-Video Encryption Scheme Based on Secret Huffman\n  Tables",
        "authors": [
            "Shujun Li",
            "Guanrong Chen",
            "Albert Cheung",
            "Kwok-Tung Lo"
        ],
        "category": "cs.MM",
        "published_year": "2005",
        "summary": "  This paper studies the security of a recently-proposed MPEG-video encryption\nscheme based on secret Huffman tables. Our cryptanalysis shows that: 1) the key\nspace of the encryption scheme is not sufficiently large against\ndivide-and-conquer (DAC) attack and known-plaintext attack; 2) it is possible\nto decrypt a cipher-video with a partially-known key, thus dramatically\nreducing the complexity of the DAC brute-force attack in some cases; 3) its\nsecurity against the chosen-plaintext attack is very weak. Some experimental\nresults are included to support the cryptanalytic results with a brief discuss\non how to improve this MPEG-video encryption scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0509035v2"
    },
    {
        "title": "A constructive and unifying framework for zero-bit watermarking",
        "authors": [
            "Teddy Furon"
        ],
        "category": "cs.MM",
        "published_year": "2006",
        "summary": "  In the watermark detection scenario, also known as zero-bit watermarking, a\nwatermark, carrying no hidden message, is inserted in content. The watermark\ndetector checks for the presence of this particular weak signal in content. The\narticle looks at this problem from a classical detection theory point of view,\nbut with side information enabled at the embedding side. This means that the\nwatermark signal is a function of the host content. Our study is twofold. The\nfirst step is to design the best embedding function for a given detection\nfunction, and the best detection function for a given embedding function. This\nyields two conditions, which are mixed into one `fundamental' partial\ndifferential equation. It appears that many famous watermarking schemes are\nindeed solution to this `fundamental' equation. This study thus gives birth to\na constructive framework unifying solutions, so far perceived as very\ndifferent.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0606034v2"
    },
    {
        "title": "Security Analysis of A Chaos-based Image Encryption Algorithm",
        "authors": [
            "Shiguo Lian",
            "Jinsheng Sun",
            "Zhiquan Wang"
        ],
        "category": "cs.MM",
        "published_year": "2006",
        "summary": "  The security of Fridrich Image Encryption Algorithm against brute-force\nattack, statistical attack, known-plaintext attack and select-plaintext attack\nis analyzed by investigating the properties of the involved chaotic maps and\ndiffusion functions. Based on the given analyses, some means are proposed to\nstrengthen the overall performance of the focused cryptosystem.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0608119v1"
    },
    {
        "title": "Double Sided Watermark Embedding and Detection with Perceptual Analysis",
        "authors": [
            "Jidong Zhong",
            "Shangteng Huang"
        ],
        "category": "cs.MM",
        "published_year": "2007",
        "summary": "  In our previous work, we introduced a double-sided technique that utilizes\nbut not reject the host interference. Due to its nice property of utilizing but\nnot rejecting the host interference, it has a big advantage over the host\ninterference schemes in that the perceptual analysis can be easily implemented\nfor our scheme to achieve the locally bounded maximum embedding strength. Thus,\nin this work, we detail how to implement the perceptual analysis in our\ndouble-sided schemes since the perceptual analysis is very important for\nimproving the fidelity of watermarked contents. Through the extensive\nperformance comparisons, we can further validate the performance advantage of\nour double-sided schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.1925v1"
    },
    {
        "title": "Watermark Embedding and Detection",
        "authors": [
            "Jidong Zhong"
        ],
        "category": "cs.MM",
        "published_year": "2007",
        "summary": "  The embedder and the detector (or decoder) are the two most important\ncomponents of the digital watermarking systems. Thus in this work, we discuss\nhow to design a better embedder and detector (or decoder). I first give a\nsummary of the prospective applications of watermarking technology and major\nwatermarking schemes in the literature. My review on the literature closely\ncenters upon how the side information is exploited at both embedders and\ndetectors. In Chapter 3, I explore the optimum detector or decoder according to\na particular probability distribution of the host signals. We found that the\nperformance of both multiplicative and additive spread spectrum schemes depends\non the shape parameter of the host signals. For spread spectrum schemes, the\nperformance of the detector or the decoder is reduced by the host interference.\nThus I present a new host-interference rejection technique for the\nmultiplicative spread spectrum schemes. Its embedding rule is tailored to the\noptimum detection or decoding rule. Though the host interference rejection\nschemes enjoy a big performance gain over the traditional spread spectrum\nschemes, their drawbacks that it is difficult for them to be implemented with\nthe perceptual analysis to achieve the maximum allowable embedding level\ndiscourage their use in real scenarios. Thus, in the last chapters of this\nwork, I introduce a double-sided technique to tackle this drawback. It differs\nfrom the host interference rejection schemes in that it utilizes but does not\nreject the host interference at its embedder. The perceptual analysis can be\neasily implemented in our scheme to achieve the maximum allowable level of\nembedding strength.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.0427v1"
    },
    {
        "title": "Multimedia Content Distribution in Hybrid Wireless Networks using\n  Weighted Clustering",
        "authors": [
            "Adrian Andronache",
            "Matthias R. Brust",
            "Steffen Rothkugel"
        ],
        "category": "cs.MM",
        "published_year": "2007",
        "summary": "  Fixed infrastructured networks naturally support centralized approaches for\ngroup management and information provisioning. Contrary to infrastructured\nnetworks, in multi-hop ad-hoc networks each node acts as a router as well as\nsender and receiver. Some applications, however, requires hierarchical\narrangements that-for practical reasons-has to be done locally and\nself-organized. An additional challenge is to deal with mobility that causes\npermanent network partitioning and re-organizations. Technically, these\nproblems can be tackled by providing additional uplinks to a backbone network,\nwhich can be used to access resources in the Internet as well as to inter-link\nmultiple ad-hoc network partitions, creating a hybrid wireless network. In this\npaper, we present a prototypically implemented hybrid wireless network system\noptimized for multimedia content distribution. To efficiently manage the ad-hoc\ncommunicating devices a weighted clustering algorithm is introduced. The\nproposed localized algorithm deals with mobility, but does not require\ngeographical information or distances.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.1141v1"
    },
    {
        "title": "On the Performance of Joint Fingerprint Embedding and Decryption Scheme",
        "authors": [
            "Shiguo Lian",
            "Zhongxuan Liu",
            "Zhen Ren",
            "Haila Wang"
        ],
        "category": "cs.MM",
        "published_year": "2007",
        "summary": "  Till now, few work has been done to analyze the performances of joint\nfingerprint embedding and decryption schemes. In this paper, the security of\nthe joint fingerprint embedding and decryption scheme proposed by Kundur et al.\nis analyzed and improved. The analyses include the security against\nunauthorized customer, the security against authorized customer, the\nrelationship between security and robustness, the relationship between\nsecu-rity and imperceptibility and the perceptual security. Based these\nanalyses, some means are proposed to strengthen the system, such as multi-key\nencryp-tion and DC coefficient encryption. The method can be used to analyze\nother JFD schemes. It is expected to provide valuable information to design JFD\nschemes.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.3076v1"
    },
    {
        "title": "A quick search method for audio signals based on a piecewise linear\n  representation of feature trajectories",
        "authors": [
            "Akisato Kimura",
            "Kunio Kashino",
            "Takayuki Kurozumi",
            "Hiroshi Murase"
        ],
        "category": "cs.MM",
        "published_year": "2007",
        "summary": "  This paper presents a new method for a quick similarity-based search through\nlong unlabeled audio streams to detect and locate audio clips provided by\nusers. The method involves feature-dimension reduction based on a piecewise\nlinear representation of a sequential feature trajectory extracted from a long\naudio stream. Two techniques enable us to obtain a piecewise linear\nrepresentation: the dynamic segmentation of feature trajectories and the\nsegment-based Karhunen-L\\'{o}eve (KL) transform. The proposed search method\nguarantees the same search results as the search method without the proposed\nfeature-dimension reduction method in principle. Experiment results indicate\nsignificant improvements in search speed. For example the proposed method\nreduced the total search time to approximately 1/12 that of previous methods\nand detected queries in approximately 0.3 seconds from a 200-hour audio\ndatabase.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4180v1"
    },
    {
        "title": "Secure Fractal Image Coding",
        "authors": [
            "Shiguo Lian"
        ],
        "category": "cs.MM",
        "published_year": "2007",
        "summary": "  In recent work, various fractal image coding methods are reported, which\nadopt the self-similarity of images to compress the size of images. However,\ntill now, no solutions for the security of fractal encoded images have been\nprovided. In this paper, a secure fractal image coding scheme is proposed and\nevaluated, which encrypts some of the fractal parameters during fractal\nencoding, and thus, produces the encrypted and encoded image. The encrypted\nimage can only be recovered by the correct key. To keep secure and efficient,\nonly the suitable parameters are selected and encrypted through in-vestigating\nthe properties of various fractal parameters, including parameter space,\nparameter distribu-tion and parameter sensitivity. The encryption process does\nnot change the file format, keeps secure in perception, and costs little time\nor computational resources. These properties make it suitable for secure image\nencoding or transmission.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.3500v1"
    },
    {
        "title": "Steganography of VoIP Streams",
        "authors": [
            "Wojciech Mazurczyk",
            "Krzysztof Szczypiorski"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  The paper concerns available steganographic techniques that can be used for\ncreating covert channels for VoIP (Voice over Internet Protocol) streams. Apart\nfrom characterizing existing steganographic methods we provide new insights by\npresenting two new techniques. The first one is network steganography solution\nwhich exploits free/unused protocols' fields and is known for IP, UDP or TCP\nprotocols but has never been applied to RTP (Real-Time Transport Protocol) and\nRTCP (Real-Time Control Protocol) which are characteristic for VoIP. The second\nmethod, called LACK (Lost Audio Packets Steganography), provides hybrid\nstorage-timing covert channel by utilizing delayed audio packets. The results\nof the experiment, that was performed to estimate a total amount of data that\ncan be covertly transferred during typical VoIP conversation phase, regardless\nof steganalysis, are also included in this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.2938v2"
    },
    {
        "title": "Covert Channels in SIP for VoIP signalling",
        "authors": [
            "Wojciech Mazurczyk",
            "Krzysztof Szczypiorski"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  In this paper, we evaluate available steganographic techniques for SIP\n(Session Initiation Protocol) that can be used for creating covert channels\nduring signaling phase of VoIP (Voice over IP) call. Apart from characterizing\nexisting steganographic methods we provide new insights by introducing new\ntechniques. We also estimate amount of data that can be transferred in\nsignalling messages for typical IP telephony call.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.3538v1"
    },
    {
        "title": "Computer Art in the Former Soviet Bloc",
        "authors": [
            "Eric Engle"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  Documents early computer art in the Soviet bloc and describes Marxist art\ntheory.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.0524v1"
    },
    {
        "title": "A First Step to Convolutive Sparse Representation",
        "authors": [
            "Hamed Firouzi",
            "Massoud Babaie-Zadeh",
            "Aria Ghasemian",
            "Christian Jutten"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  In this paper an extension of the sparse decomposition problem is considered\nand an algorithm for solving it is presented. In this extension, it is known\nthat one of the shifted versions of a signal s (not necessarily the original\nsignal itself) has a sparse representation on an overcomplete dictionary, and\nwe are looking for the sparsest representation among the representations of all\nthe shifted versions of s. Then, the proposed algorithm finds simultaneously\nthe amount of the required shift, and the sparse representation. Experimental\nresults emphasize on the performance of our algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.3485v1"
    },
    {
        "title": "Optimization of automatically generated multi-core code for the LTE\n  RACH-PD algorithm",
        "authors": [
            "Maxime Pelcat",
            "Slaheddine Aridhi",
            "Jean François Nezan"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  Embedded real-time applications in communication systems require high\nprocessing power. Manual scheduling devel-oped for single-processor\napplications is not suited to multi-core architectures. The Algorithm\nArchitecture Matching (AAM) methodology optimizes static application\nimplementation on multi-core architectures. The Random Access Channel Preamble\nDetection (RACH-PD) is an algorithm for non-synchronized access of Long Term\nEvolu-tion (LTE) wireless networks. LTE aims to improve the spectral efficiency\nof the next generation cellular system. This paper de-scribes a complete\nmethodology for implementing the RACH-PD. AAM prototyping is applied to the\nRACH-PD which is modelled as a Synchronous DataFlow graph (SDF). An efficient\nimplemen-tation of the algorithm onto a multi-core DSP, the TI C6487, is then\nexplained. Benchmarks for the solution are given.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0582v1"
    },
    {
        "title": "A New Trend in Optimization on Multi Overcomplete Dictionary toward\n  Inpainting",
        "authors": [
            "SeyyedMajid Valiollahzadeh",
            "Mohammad Nazari",
            "Massoud Babaie-Zadeh",
            "Christian Jutten"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  Recently, great attention was intended toward overcomplete dictionaries and\nthe sparse representations they can provide. In a wide variety of signal\nprocessing problems, sparsity serves a crucial property leading to high\nperformance. Inpainting, the process of reconstructing lost or deteriorated\nparts of images or videos, is an interesting application which can be handled\nby suitably decomposition of an image through combination of overcomplete\ndictionaries. This paper addresses a novel technique of such a decomposition\nand investigate that through inpainting of images. Simulations are presented to\ndemonstrate the validation of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2405v1"
    },
    {
        "title": "Probabilistic SVM/GMM Classifier for Speaker-Independent Vowel\n  Recognition in Continues Speech",
        "authors": [
            "Mohammad Nazari",
            "Abolghasem Sayadiyan",
            "SeyedMajid Valiollahzadeh"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  In this paper, we discuss the issues in automatic recognition of vowels in\nPersian language. The present work focuses on new statistical method of\nrecognition of vowels as a basic unit of syllables. First we describe a vowel\ndetection system then briefly discuss how the detected vowels can feed to\nrecognition unit. According to pattern recognition, Support Vector Machines\n(SVM) as a discriminative classifier and Gaussian mixture model (GMM) as a\ngenerative model classifier are two most popular techniques. Current\nstate-ofthe- art systems try to combine them together for achieving more power\nof classification and improving the performance of the recognition systems. The\nmain idea of the study is to combine probabilistic SVM and traditional GMM\npattern classification with some characteristic of speech like band-pass energy\nto achieve better classification rate. This idea has been analytically\nformulated and tested on a FarsDat based vowel recognition system. The results\nshow inconceivable increases in recognition accuracy. The tests have been\ncarried out by various proposed vowel recognition algorithms and the results\nhave been compared.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2411v1"
    },
    {
        "title": "Condition for Energy Efficient Watermarking with Random Vector Model\n  without WSS Assumption",
        "authors": [
            "Bin Yan",
            "Zheming Lu",
            "Yinjing Guo"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  Energy efficient watermarking preserves the watermark energy after linear\nattack as much as possible. We consider in this letter non-stationary signal\nmodels and derive conditions for energy efficient watermarking under random\nvector model without WSS assumption. We find that the covariance matrix of the\nenergy efficient watermark should be proportional to host covariance matrix to\nbest resist the optimal linear removal attacks. In WSS process our result\nreduces to the well known power spectrum condition. Intuitive geometric\ninterpretation of the results are also discussed which in turn also provide\nmore simpler proof of the main results.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.1407v1"
    },
    {
        "title": "Gradient-based adaptive interpolation in super-resolution image\n  restoration",
        "authors": [
            "Jinyu Chu",
            "Ju Liu",
            "Jianping Qiao",
            "Xiaoling Wang",
            "Yujun Li"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  This paper presents a super-resolution method based on gradient-based\nadaptive interpolation. In this method, in addition to considering the distance\nbetween the interpolated pixel and the neighboring valid pixel, the\ninterpolation coefficients take the local gradient of the original image into\naccount. The smaller the local gradient of a pixel is, the more influence it\nshould have on the interpolated pixel. And the interpolated high resolution\nimage is finally deblurred by the application of wiener filter. Experimental\nresults show that our proposed method not only substantially improves the\nsubjective and objective quality of restored images, especially enhances edges,\nbut also is robust to the registration error and has low computational\ncomplexity.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.3995v1"
    },
    {
        "title": "Quality assessment of the MPEG-4 scalable video CODEC",
        "authors": [
            "Florian Niedermeier",
            "Michael Niedermeier",
            "Harald Kosch"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  In this paper, the performance of the emerging MPEG-4 SVC CODEC is evaluated.\nIn the first part, a brief introduction on the subject of quality assessment\nand the development of the MPEG-4 SVC CODEC is given. After that, the used test\nmethodologies are described in detail, followed by an explanation of the actual\ntest scenarios. The main part of this work concentrates on the performance\nanalysis of the MPEG-4 SVC CODEC - both objective and subjective.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.0667v1"
    },
    {
        "title": "A New Approach to Manage QoS in Distributed Multimedia Systems",
        "authors": [
            "Bechir Alaya",
            "Claude Duvallet",
            "Bruno Sadeg"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  Dealing with network congestion is a criterion used to enhance quality of\nservice (QoS) in distributed multimedia systems. The existing solutions for the\nproblem of network congestion ignore scalability considerations because they\nmaintain a separate classification for each video stream. In this paper, we\npropose a new method allowing to control QoS provided to clients according to\nthe network congestion, by discarding some frames when needed. The technique\nproposed, called (m,k)-frame, is scalable with little degradation in\napplication performances. (m,k)-frame method is issued from the notion of\n(m,k)-firm realtime constraints which means that among k invocations of a task,\nm invocations must meet their deadline. Our simulation studies show the\nusefulness of (m,k)-frame method to adapt the QoS to the real conditions in a\nmultimedia application, according to the current system load. Notably, the\nsystem must adjust the QoS provided to active clients1 when their number\nvaries, i.e. dynamic arrival of clients.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.4936v1"
    },
    {
        "title": "Component based platform for multimedia applications",
        "authors": [
            "Ovidiu Ratoi",
            "Piroska Haller",
            "Ioan Salomie",
            "Bela Genge"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  We propose a platform for distributed multimedia applications which\nsimplifies the development process and at the same time ensures application\nportability, flexibility and performance. The platform is implemented using the\nNetscape Portable Runtime (NSPR) and the Cross-Platform Component Object Model\n(XPCOM).\n",
        "pdf_link": "http://arxiv.org/pdf/0908.3082v1"
    },
    {
        "title": "Robustness of the Digital Image Watermarking Techniques against\n  Brightness and Rotation Attack",
        "authors": [
            "Harsh K Verma",
            "Abhishek Narain Singh",
            "Raman Kumar"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  The recent advent in the field of multimedia proposed a many facilities in\ntransport, transmission and manipulation of data. Along with this advancement\nof facilities there are larger threats in authentication of data, its licensed\nuse and protection against illegal use of data. A lot of digital image\nwatermarking techniques have been designed and implemented to stop the illegal\nuse of the digital multimedia images. This paper compares the robustness of\nthree different watermarking schemes against brightness and rotation attacks.\nThe robustness of the watermarked images has been verified on the parameters of\nPSNR (Peak Signal to Noise Ratio), RMSE (Root Mean Square Error) and MAE (Mean\nAbsolute Error).\n",
        "pdf_link": "http://arxiv.org/pdf/0909.3554v1"
    },
    {
        "title": "Analysis, Design and Simulation of a New System for Internet Multimedia\n  Transmission Guarantee",
        "authors": [
            "O. Said",
            "S. Bahgat",
            "M. Ghoniemy",
            "Y. Elawdy"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  QoS is a very important issue for multimedia communication systems. In this\npaper, a new system that reinstalls the relation between the QoS elements\n(RSVP, routing protocol, sender, and receiver) during the multimedia\ntransmission is proposed, then an alternative path is created in case of\noriginal multimedia path failure. The suggested system considers the resulting\nproblems that may be faced within and after the creation of rerouting path.\nFinally, the proposed system is simulated using OPNET 11.5 simulation package.\nSimulation results show that our proposed system outperforms the old one in\nterms of QoS parameters like packet loss and delay jitter.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.0179v1"
    },
    {
        "title": "Prefix based Chaining Scheme for Streaming Popular Videos using Proxy\n  servers in VoD",
        "authors": [
            "M Dakshayini",
            "T R GopalaKrishnan Nair"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  Streaming high quality videos consumes significantly large amount of network\nresources. In this context request to service delay, network traffic,\ncongestion and server overloading are the main parameters to be considered in\nvideo streaming over the internet that effect the quality of service (QoS). In\nthis paper, we propose an efficient architecture as a cluster of proxy servers\nand clients that uses a peer to peer (P2P) approach to cooperatively stream the\nvideo using chaining technique. We consider the following two key issues in the\nproposed architecture (1) Prefix caching technique to accommodate more number\nof videos close to client (2) Cooperative client and proxy chaining to achieve\nthe network efficiency. Our simulation results shows that the proposed approach\nyields a prefix caching close to the optimal solution minimizing WAN bandwidth\nusage on server-proxy path by utilizing the proxy-client and client-client path\nbandwidth, which is much cheaper than the expensive server proxy path\nbandwidth, server load, and client rejection ratio significantly using\nchaining.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.1471v1"
    },
    {
        "title": "A Reliable Replication Strategy for VoD System using Markov Chain",
        "authors": [
            "R. Ashok Kumar",
            "K. Ganesan"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  In this paper we have investigated on the reliability of streams for a VoD\nsystem. The objective of the paper is to maximize the availability of streams\nfor the peers in the VoD system. We have achieved this by using data\nreplication technique in the peers. Hence, we proposed a new data replication\ntechnique to optimally store the videos in the peers. The new data replication\ntechnique generates more number of replicas than the existing techniques such\nas random, minimum request and maximize hit. We have also investigated by\napplying the CTMC model for the reliability of replications during the peer\nfailures. Our result shows that the mean lifetime of replicas are more under\nvarious circumstances. We have addressed the practical issues of efficient\nutilization of overall bandwidth and buffer in the VoD system. We achieved\ngreater success playback probability of videos than the existing techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.1011v1"
    },
    {
        "title": "Distributed Rate Allocation Policies for Multi-Homed Video Streaming\n  over Heterogeneous Access Networks",
        "authors": [
            "Xiaoqing Zhu",
            "Piyush Agrawal",
            "Jatinder Pal Singh",
            "Tansu Alpcan",
            "Bernd Girod"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  We consider the problem of rate allocation among multiple simultaneous video\nstreams sharing multiple heterogeneous access networks. We develop and evaluate\nan analytical framework for optimal rate allocation based on observed available\nbit rate (ABR) and round-trip time (RTT) over each access network and video\ndistortion-rate (DR) characteristics. The rate allocation is formulated as a\nconvex optimization problem that minimizes the total expected distortion of all\nvideo streams. We present a distributed approximation of its solution and\ncompare its performance against H-infinity optimal control and two heuristic\nschemes based on TCP-style additive-increase-multiplicative decrease (AIMD)\nprinciples. The various rate allocation schemes are evaluated in simulations of\nmultiple high-definition (HD) video streams sharing multiple access networks.\nOur results demonstrate that, in comparison with heuristic AIMD-based schemes,\nboth media-aware allocation and H-infinity optimal control benefit from\nproactive congestion avoidance and reduce the average packet loss rate from 45%\nto below 2%. Improvement in average received video quality ranges between 1.5\nto 10.7 dB in PSNR for various background traffic loads and video playout\ndeadlines. Media-aware allocation further exploits its knowledge of the video\nDR characteristics to achieve a more balanced video quality among all streams.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1013v1"
    },
    {
        "title": "Designing a Truly Integrated (Onsite and Online) Conference: Concept,\n  Processes, Solutions",
        "authors": [
            "Alexei Botchkarev",
            "Lian Zhao",
            "Hamed Rasouli"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Web conferencing tools have entered the mainstream of business applications.\nUsing web conferencing for IEEE conferences has a good potential of adding\nvalue to both organizers and participants. Authors propose a concept of Truly\nIntegrated Conference (TIC) according to which a multi-point\nworldwide-distributed network of conference online authors/participants will\nenhance the standard (centralized) IEEE conference model, which requires\nattendance of the participants in person at the main conference location. The\nconcept entails seamless integration of the onsite and online conference\nsystems, including data/presentation, video, audio channels. Benefits and\nchallenges of the TIC concept are analyzed. Requirements to the web\nconferencing system capable of supporting the TIC conference are presented and\nreviewed against commercial web conferencing tools. Case study of the IEEE\nToronto International Conference ? Science and Technology for Humanity, which\nwas the first realization of TIC, is presented which analyzes various aspects\n(organizational, technological, and financial) of the integrated conference.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1794v1"
    },
    {
        "title": "Avoiding Interruptions - QoE Trade-offs in Block-coded Streaming Media\n  Applications",
        "authors": [
            "Ali Parandehgheibi",
            "Muriel Medard",
            "Srinivas Shakkottai",
            "Asu Ozdaglar"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  We take an analytical approach to study Quality of user Experience (QoE) for\nvideo streaming applications. First, we show that random linear network coding\napplied to blocks of video frames can significantly simplify the packet\nrequests at the network layer and save resources by avoiding duplicate packet\nreception. Network coding allows us to model the receiver's buffer as a queue\nwith Poisson arrivals and deterministic departures. We consider the probability\nof interruption in video playback as well as the number of initially buffered\npackets (initial waiting time) as the QoE metrics. We characterize the optimal\ntrade-off between these metrics by providing upper and lower bounds on the\nminimum initial buffer size, required to achieve certain level of interruption\nprobability for different regimes of the system parameters. Our bounds are\nasymptotically tight as the file size goes to infinity.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1937v2"
    },
    {
        "title": "A New Image Steganography Based On First Component Alteration Technique",
        "authors": [
            "Amanpreet Kaur",
            "Renu Dhir",
            "Geeta Sikka"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  In this paper, A new image steganography scheme is proposed which is a kind\nof spatial domain technique. In order to hide secret data in cover-image, the\nfirst component alteration technique is used. Techniques used so far focuses\nonly on the two or four bits of a pixel in a image (at the most five bits at\nthe edge of an image) which results in less peak to signal noise ratio and high\nroot mean square error. In this technique, 8 bits of blue components of pixels\nare replaced with secret data bits. Proposed scheme can embed more data than\nprevious schemes and shows better image quality. To prove this scheme, several\nexperiments are performed, and are compared the experimental results with the\nrelated previous works.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1972v1"
    },
    {
        "title": "Evaluating Effectiveness of Tamper Proofing on Dynamic Graph Software\n  Watermarks",
        "authors": [
            "Malik Sikandar Hayat Khiyal",
            "Aihab Khan",
            "Sehrish Amjad",
            "M. Shahid Khalil"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  For enhancing the protection level of dynamic graph software watermarks and\nfor the purpose of conducting the analysis which evaluates the effect of\nintegrating two software protection techniques such as software watermarking\nand tamper proofing, constant encoding technique along with the enhancement\nthrough the idea of constant splitting is proposed. In this paper Thomborson\ntechnique has been implemented with the scheme of breaking constants which\nenables to encode all constants without having any consideration about their\nvalues with respect to the value of watermark tree. Experimental analysis which\nhave been conducted and provided in this paper concludes that the constant\nencoding process significantly increases the code size, heap space usage, and\nexecution time, while making the tamper proofed code resilient to variety of\nsemantic preserving program transformation attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1974v1"
    },
    {
        "title": "An Improved DC Recovery Method from AC Coefficients of DCT-Transformed\n  Images",
        "authors": [
            "Shujun Li",
            "Junaid Jameel Ahmad",
            "Dietmar Saupe",
            "C. -C. Jay Kuo"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Motivated by the work of Uehara et al. [1], an improved method to recover DC\ncoefficients from AC coefficients of DCT-transformed images is investigated in\nthis work, which finds applications in cryptanalysis of selective multimedia\nencryption. The proposed under/over-flow rate minimization (FRM) method employs\nan optimization process to get a statistically more accurate estimation of\nunknown DC coefficients, thus achieving a better recovery performance. It was\nshown by experimental results based on 200 test images that the proposed DC\nrecovery method significantly improves the quality of most recovered images in\nterms of the PSNR values and several state-of-the-art objective image quality\nassessment (IQA) metrics such as SSIM and MS-SSIM.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.1727v3"
    },
    {
        "title": "The Fast Haar Wavelet Transform for Signal & Image Processing",
        "authors": [
            "V. Ashok",
            "T. Balakumaran",
            "C. Gowrishankar",
            "I. L. A. Vennila",
            "A. Nirmal kumar"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  A method for the design of Fast Haar wavelet for signal processing and image\nprocessing has been proposed. In the proposed work, the analysis bank and\nsynthesis bank of Haar wavelet is modified by using polyphase structure.\nFinally, the Fast Haar wavelet was designed and it satisfies alias free and\nperfect reconstruction condition. Computational time and computational\ncomplexity is reduced in Fast Haar wavelet transform.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2184v1"
    },
    {
        "title": "Dual Watermarking Scheme with Encryption",
        "authors": [
            "R. Dhanalakshmi",
            "K. Thaiyalnayaki"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Digital Watermarking is used for copyright protection and authentication. In\nthe proposed system, a Dual Watermarking Scheme based on DWT SVD with chaos\nencryption algorithm, will be developed to improve the robustness and\nprotection along with security. DWT and SVD have been used as a mathematical\ntool to embed watermark in the image. Two watermarks are embedded in the host\nimage. The secondary is embedded into primary watermark and the resultant\nwatermarked image is encrypted using chaos based logistic map. This provides an\nefficient and secure way for image encryption and transmission. The watermarked\nimage is decrypted and a reliable watermark extraction scheme is developed for\nthe extraction of the primary as well as secondary watermark from the distorted\nimage.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2414v1"
    },
    {
        "title": "Optimization Digital Image Watermarking Technique for Patent Protection",
        "authors": [
            "Mahmoud Elnajjar",
            "A. A Zaidan",
            "B. B Zaidan",
            "Mohamed Elhadi M. Sharif",
            "Hamdan. O. Alanazi"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  The rapid development of multimedia and internet allows for wide distribution\nof digital media data. It becomes much easier to edit, modify and duplicate\ndigital information besides that, digital documents are also easy to copy and\ndistribute, therefore it will be faced by many threats. It is a big security\nand privacy issue. Another problem with digital document and video is that\nundetectable modifications can be made with very simple and widely available\nequipment, which put the digital material for evidential purposes under\nquestion With the large flood of information and the development of the digital\nformat, it become necessary to find appropriate protection because of the\nsignificance, accuracy and sensitivity of the information, therefore multimedia\ntechnology and popularity of internet communications they have great interest\nin using digital watermarks for the purpose of copy protection and content\nauthentication. Digital watermarking is a technique used to embed a known piece\nof digital data within another piece of digital data .A digital data may\nrepresent a digital signature or digital watermark that is embedded in the host\nmedia. The signature or watermark is hidden such that it's perceptually and\nstatistically undetectable. Then this signature or watermark can be extracted\nfrom the host media and used to identify the owner of the media.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.4049v1"
    },
    {
        "title": "New Classification Methods for Hiding Information into Two Parts:\n  Multimedia Files and Non Multimedia Files",
        "authors": [
            "Hamdan. O. Alanazi",
            "A. A. Zaidan",
            "B. B. Zaidan",
            "Hamid A. Jalab",
            "Zaidoon Kh. AL-Ani"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  With the rapid development of various multimedia technologies, more and more\nmultimedia data are generated and transmitted in the medical, commercial, and\nmilitary fields, which may include some sensitive information which should not\nbe accessed by or can only be partially exposed to the general users.\nTherefore, security and privacy has become an important, Another problem with\ndigital document and video is that undetectable modifications can be made with\nvery simple and widely available equipment, which put the digital material for\nevidential purposes under question .With the large flood of information and the\ndevelopment of the digital format Information hiding considers one of the\ntechniques which used to protect the important information. The main goals for\nthis paper, provides a general overview of the New Classification Methods for\nHiding Information into Two Parts: Multimedia Files and Non Multimedia Files.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.4084v1"
    },
    {
        "title": "Feature-Based Adaptive Tolerance Tree (FATT): An Efficient Indexing\n  Technique for Content-Based Image Retrieval Using Wavelet Transform",
        "authors": [
            "Dr. P. AnandhaKumar",
            "V. Balamurugan"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  This paper introduces a novel indexing and access method, called Feature-\nBased Adaptive Tolerance Tree (FATT), using wavelet transform is proposed to\norganize large image data sets efficiently and to support popular image access\nmechanisms like Content Based Image Retrieval (CBIR).Conventional database\nsystems are designed for managing textual and numerical data and retrieving\nsuch data is often based on simple comparisons of text or numerical values.\nHowever, this method is no longer adequate for images, since the digital\npresentation of images does not convey the reality of images. Retrieval of\nimages become difficult when the database is very large. This paper addresses\nsuch problems and presents a novel indexing technique, Feature Based Adaptive\nTolerance Tree (FATT), which is designed to bring an effective solution\nespecially for indexing large databases. The proposed indexing scheme is then\nused along with a query by image content, in order to achieve the ultimate goal\nfrom the user point of view that is retrieval of all relevant images. FATT\nindexing technique, features of the image is extracted using 2-dimensional\ndiscrete wavelet transform (2DDWT) and index code is generated from the\ndeterminant value of the features. Multiresolution analysis technique using\n2D-DWT can decompose the image into components at different scales, so that the\ncoarest scale components carry the global approximation information while the\nfiner scale components contain the detailed information. Experimental results\nshow that the FATT outperforms M-tree upto 200%, Slim-tree up to 120% and HCT\nupto 89%. FATT indexing technique is adopted to increase the efficiently of\ndata storage and retrieval.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1229v1"
    },
    {
        "title": "SAR Image Segmentation using Vector Quantization Technique on Entropy\n  Images",
        "authors": [
            "H. B. Kekre",
            "Saylee Gharge",
            "Tanuja K. Sarode"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  The development and application of various remote sensing platforms result in\nthe production of huge amounts of satellite image data. Therefore, there is an\nincreasing need for effective querying and browsing in these image databases.\nIn order to take advantage and make good use of satellite images data, we must\nbe able to extract meaningful information from the imagery. Hence we proposed a\nnew algorithm for SAR image segmentation. In this paper we propose segmentation\nusing vector quantization technique on entropy image. Initially, we obtain\nentropy image and in second step we use Kekre's Fast Codebook Generation (KFCG)\nalgorithm for segmentation of the entropy image. Thereafter, a codebook of size\n128 was generated for the Entropy image. These code vectors were further\nclustered in 8 clusters using same KFCG algorithm and converted into 8 images.\nThese 8 images were displayed as a result. This approach does not lead to over\nsegmentation or under segmentation. We compared these results with well known\nGray Level Co-occurrence Matrix. The proposed algorithm gives better\nsegmentation with less complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1789v1"
    },
    {
        "title": "Human Daily Activities Indexing in Videos from Wearable Cameras for\n  Monitoring of Patients with Dementia Diseases",
        "authors": [
            "Svebor Karaman",
            "Jenny Benois-Pineau",
            "Rémi Mégret",
            "Vladislavs Dovgalecs",
            "Jean-François Dartigues",
            "Yann Gaëstel"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  Our research focuses on analysing human activities according to a known\nbehaviorist scenario, in case of noisy and high dimensional collected data. The\ndata come from the monitoring of patients with dementia diseases by wearable\ncameras. We define a structural model of video recordings based on a Hidden\nMarkov Model. New spatio-temporal features, color features and localization\nfeatures are proposed as observations. First results in recognition of\nactivities are promising.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.4134v1"
    },
    {
        "title": "Reliable Multicasting for Device-to-Device Radio Underlaying Cellular\n  Networks",
        "authors": [
            "Wei Yang",
            "Wanlu Sun",
            "Lihua Li"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  This paper proposes Leader in Charge (LiC), a reliable multicast architecture\nfor device-to-device (D2D) radio underlaying cellular networks. The\nmulticast-requesting user equipments (UEs) in close proximity form a D2D\ncluster to receive the multicast packets through cooperation. In addition to\nreceiving the multicast packets from the eNB, UEs share what they received from\nthe multicast on short-range links among UEs, namely the D2D links, to exploit\nthe wireless resources a more efficient way. Consequently, we show that\nutilizing the D2D links in cellular networks increases the throughput of a\nmulticast session by means of simulation. We also discuss some practical issues\nfacing the integration of LiC into the current cellular networks. In\nparticular, we propose efficient delay control mechanism to reduce the average\nand maximum delay experienced by LiC users, which is further confirmed by the\nsimulation results.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.3741v2"
    },
    {
        "title": "Alternatives to speech in low bit rate communication systems",
        "authors": [
            "Cristina Videira Lopes",
            "Pedro M. Q. Aguiar"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  This paper describes a framework and a method with which speech communication\ncan be analyzed. The framework consists of a set of low bit rate, short-range\nacoustic communication systems, such as speech, but that are quite different\nfrom speech. The method is to systematically compare these systems according to\ndifferent objective functions such as data rate, computational overhead,\npsychoacoustic effects and semantics. One goal of this study is to better\nunderstand the nature of human communication. Another goal is to identify\nacoustic communication systems that are more efficient than human speech for\nsome specific purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.3951v1"
    },
    {
        "title": "A Color Image Digital Watermarking Scheme Based on SOFM",
        "authors": [
            "J. Anitha",
            "S. Immanuel Alex Pandian"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Digital watermarking technique has been presented and widely researched to\nsolve some important issues in the digital world, such as copyright protection,\ncopy protection and content authentication. Several robust watermarking schemes\nbased on vector quantization (VQ) have been presented. In this paper, we\npresent a new digital image watermarking method based on SOFM vector quantizer\nfor color images. This method utilizes the codebook partition technique in\nwhich the watermark bit is embedded into the selected VQ encoded block. The\nmain feature of this scheme is that the watermark exists both in VQ compressed\nimage and in the reconstructed image. The watermark extraction can be performed\nwithout the original image. The watermark is hidden inside the compressed\nimage, so much transmission time and storage space can be saved when the\ncompressed data are transmitted over the Internet. Simulation results\ndemonstrate that the proposed method has robustness against various image\nprocessing operations without sacrificing compression performance and the\ncomputational speed.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.5127v1"
    },
    {
        "title": "Peer-to-Peer Multimedia Sharing based on Social Norms",
        "authors": [
            "Yu Zhang",
            "Mihaela van der Schaar"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Empirical data shows that in the absence of incentives, a peer participating\nin a Peer-to-Peer (P2P) network wishes to free-riding. Most solutions for\nproviding incentives in P2P networks are based on direct reciprocity, which are\nnot appropriate for most P2P multimedia sharing networks due to the unique\nfeatures exhibited by such networks: large populations of anonymous agents\ninteracting infrequently, asymmetric interests of peers, network errors, and\nmultiple concurrent transactions. In this paper, we design and rigorously\nanalyze a new family of incentive protocols that utilizes indirect reciprocity\nwhich is based on the design of efficient social norms. In the proposed P2P\nprotocols, the social norms consist of a social strategy, which represents the\nrule prescribing to the peers when they should or should not provide content to\nother peers, and a reputation scheme, which rewards or punishes peers depending\non whether they comply or not with the social strategy. We first define the\nconcept of a sustainable social norm, under which no peer has an incentive to\ndeviate. We then formulate the problem of designing optimal social norms, which\nselects the social norm that maximizes the network performance among all\nsustainable social norms. Hence, we prove that it becomes in the self-interest\nof peers to contribute their content to the network rather than to free-ride.\nWe also investigate the impact of various punishment schemes on the social\nwelfare as well as how should the optimal social norms be designed if\naltruistic and malicious peers are active in the network. Our results show that\noptimal social norms are capable of providing significant improvements in the\nsharing efficiency of multimedia P2P networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.1503v1"
    },
    {
        "title": "Quasi-Optimal Network Utility Maximization for Scalable Video Streaming",
        "authors": [
            "Mohammad Sadegh Talebi",
            "Ahmad Khonsari",
            "Mohammad Hassan Hajiesmaili",
            "Sina Jafarpour"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  This paper addresses rate control for transmission of scalable video streams\nvia Network Utility Maximization (NUM) formulation. Due to stringent QoS\nrequirements of video streams and specific characterization of utility\nexperienced by end-users, one has to solve nonconvex and even nonsmooth NUM\nformulation for such streams, where dual methods often prove incompetent.\nConvexification plays an important role in this work as it permits the use of\nexisting dual methods to solve an approximate to the NUM problem iteratively\nand distributively. Hence, to tackle the nonsmoothness and nonconvexity, we aim\nat reformulating the NUM problem through approximation and transformation of\nthe ideal discretely adaptive utility function for scalable video streams. The\nreformulated problem is shown to be a D.C. (Difference of Convex) problem. We\nleveraged Sequential Convex Programming (SCP) approach to replace the nonconvex\nD.C. problem by a sequence of convex problems that aim to approximate the\noriginal D.C. problem. We then solve each convex problem produced by SCP\napproach using existing dual methods. This procedure is the essence of two\ndistributed iterative rate control algorithms proposed in this paper, for which\none can show the convergence to a locally optimal point of the nonconvex D.C.\nproblem and equivalently to a locally optimal point of an approximate to the\noriginal nonconvex problem. Our experimental results show that the proposed\nrate control algorithms converge with tractable convergence behavior.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.2604v2"
    },
    {
        "title": "Ontology based approach for video transmission over the network",
        "authors": [
            "Rachit Mohan Garg",
            "Yamini Sood",
            "Neha Tyagi"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  With the increase in the bandwidth & the transmission speed over the\ninternet, transmission of multimedia objects like video, audio, images has\nbecome an easier work. In this paper we provide an approach that can be useful\nfor transmission of video objects over the internet without much fuzz. The\napproach provides a ontology based framework that is used to establish an\nautomatic deployment of video transmission system. Further the video is\ncompressed using the structural flow mechanism that uses the wavelet principle\nfor compression of video frames. Finally the video transmission algorithm known\nas RRDBFSF algorithm is provided that makes use of the concept of restrictive\nflooding to avoid redundancy thereby increasing the efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.5699v1"
    },
    {
        "title": "Hiding Secret Information in Movie Clip: A Steganographic Approach",
        "authors": [
            "G. Sahoo",
            "Rajesh Kumar Tiwari"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Establishing hidden communication is an important subject of discussion that\nhas gained increasing importance nowadays with the development of the internet.\nOne of the key methods for establishing hidden communication is steganography.\nModern day steganography mainly deals with hiding information within files like\nimage, text, html, binary files etc. These file contains small irrelevant\ninformation that can be substituted for small secret data. To store a high\ncapacity secret data these carrier files are not very supportive. To overcome\nthe problem of storing the high capacity secret data with the utmost security\nfence, we have proposed a novel methodology for concealing a voluminous data\nwith high levels of security wall by using movie clip as a carrier file.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.0829v1"
    },
    {
        "title": "Priority based Interface Selection for Overlaying Heterogeneous Networks",
        "authors": [
            "Mostafa Zaman Chowdhury",
            "Yeong Min Jang"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Offering of different attractive opportunities by different wireless\ntechnologies trends the convergence of heterogeneous networks for the future\nwireless communication system. To make a seamless handover among the\nheterogeneous networks, the optimization of the power consumption, and optimal\nselection of interface are the challenging issues for convergence networks. The\naccess of multi interfaces simultaneously reduces the handover latency and data\nloss in heterogeneous handover. The mobile node (MN) maintains one interface\nconnection while other interface is used for handover process. However, it\ncauses much battery power consumption. In this paper we propose an efficient\ninterface selection scheme including interface selection algorithms, interface\nselection procedures considering battery power consumption and user mobility\nwith other existing parameters for overlaying networks. We also propose a\npriority based network selection scheme according to the service types. MN's\nbattery power level, provision of QoS/QoE in the target network and our\nproposed priority parameters are considered as more important parameters for\nour interface selection algorithm. The performances of the proposed scheme are\nverified using numerical analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.0837v1"
    },
    {
        "title": "Streaming Multimedia Information Using the Features of the DVB-S Card",
        "authors": [
            "Radu Arsinte",
            "Eugen Lupu"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  This paper presents a study of audio-video streaming using the additional\npossibilities of a DVB-S card. The board used for experiments (Technisat\nSkyStar 2) is one of the most frequently used cards for this purpose. Using the\nmain blocks of the board's software support it is possible the implement a\nreally useful and full functional system for audio-video streaming. The\nstreaming is possible to be implemented either for decoded MPEG stream or for\ntransport stream. In this last case it is possible to view not only a program,\nbut any program from the same multiplex. This allows us to implement\n",
        "pdf_link": "http://arxiv.org/pdf/1105.0826v1"
    },
    {
        "title": "Service Level Agreement for the QoS Guaranteed Mobile IPTV Services over\n  Mobile WiMAX Networks",
        "authors": [
            "Mostafa Zaman Chowdhury",
            "Bui Minh Trung",
            "Yeong Min Jang",
            "Young-Il Kim",
            "Won Ryu"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  While mobile IPTV services are supported through the mobile WiMAX networks,\nthere must need some guaranteed bandwidth for the IPTV services especially if\nIPTV and non-IPTV services are simultaneously supported by the mobile WiMAX\nnetworks. The quality of an IPTV service definitely depends on the allocated\nbandwidth for that channel. However, due to the high quality IPTV services and\nto support of huge non-IPTV traffic over mobile WiMAX networks, it is not\npossible to guarantee the sufficient amount of the limited mobile WiMAX\nbandwidth for the mobile IPTV services every time. A Service Level Agreement\n(SLA) between the mobile IPTV service provider and mobile WiMAX network\noperator to reserve sufficient bandwidth for the IPTV calls can increase the\nsatisfaction level of the mobile IPTV users. In this paper, we propose a SLA\nnegotiation procedure for mobile IPTV users over mobile WiMAX networks. The\nBandwidth Broker controls the allocated bandwidth for IPTV and non-IPTV users.\nThe proposed dynamically reserved bandwidth for the IPTV services increases the\nIPTV user's satisfaction level. The simulation results state that, our proposed\nscheme is able to provide better user satisfaction level for the IPTV users.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.4431v1"
    },
    {
        "title": "Scale-Invariant Local Descriptor for Event Recognition in 1D Sensor\n  Signals",
        "authors": [
            "Jierui Xie",
            "Mandis S. Beigi"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  In this paper, we introduce a shape-based, time-scale invariant feature\ndescriptor for 1-D sensor signals. The time-scale invariance of the feature\nallows us to use feature from one training event to describe events of the same\nsemantic class which may take place over varying time scales such as walking\nslow and walking fast. Therefore it requires less training set. The descriptor\ntakes advantage of the invariant location detection in the scale space theory\nand employs a high level shape encoding scheme to capture invariant local\nfeatures of events. Based on this descriptor, a scale-invariant classifier with\n\"R\" metric (SIC-R) is designed to recognize multi-scale events of human\nactivities. The R metric combines the number of matches of keypoint in scale\nspace with the Dynamic Time Warping score. SICR is tested on various types of\n1-D sensors data from passive infrared, accelerometer and seismic sensors with\nmore than 90% classification accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.5675v1"
    },
    {
        "title": "Nested Graph Words for Object Recognition",
        "authors": [
            "Svebor Karaman",
            "Jenny Benois-Pineau",
            "Rémi Mégret"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  In this paper, we propose a new, scalable approach for the task of object\nbased image search or object recognition. Despite the very large literature\nexisting on the scalability issues in CBIR in the sense of retrieval\napproaches, the scalability of media and scalability of features remain an\nissue. In our work we tackle the problem of scalability and structural\norganization of features. The proposed features are nested local graphs built\nupon sets of SURF feature points with Delaunay triangulation. A\nBag-of-Visual-Words (BoVW) framework is applied on these graphs, giving birth\nto a Bag-of-Graph-Words representation. The nested nature of the descriptors\nconsists in scaling from trivial Delaunay graphs - isolated feature points - by\nincreasing the number of nodes layer by layer up to graphs with maximal number\nof nodes. For each layer of graphs its proper visual dictionary is built. The\nexperiments conducted on the SIVAL data set reveal that the graph features at\ndifferent layers exhibit complementary performances on the same content. The\nnested approach, the combination of all existing layers, yields significant\nimprovement of the object recognition performance compared to single level\napproaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.2729v2"
    },
    {
        "title": "Study of a Hybrid - Analog TV and Ethernet- Home Data Link using a\n  Coaxial Cable",
        "authors": [
            "Radu Arsinte"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  The paper presents an implementation and compatibility tests of a simple home\nnetwork implemented in a nonconventional manner using a CATV coaxial cable.\nReusing the cable, normally designated to supply RF modulated TV signals from\ncable TV networks, makes possible to add data services as well. A short\npresentation of the technology is given with an investigation of the main\nperformances obtained using this technique. The measurements revealed that this\nsimple solution makes possible to have both TV and data services with\nperformances close to traditional home data services: cable modems or ADSL,\nwith minimal investments. This technology keeps also open the possibility for\nfuture improvements of the network: DVB-C or Data via Cable Modems.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.2222v1"
    },
    {
        "title": "Label-Specific Training Set Construction from Web Resource for Image\n  Annotation",
        "authors": [
            "Jinhui Tang",
            "Shuicheng Yan",
            "Tat-Seng Chua",
            "Ramesh Jain"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Recently many research efforts have been devoted to image annotation by\nleveraging on the associated tags/keywords of web images as training labels. A\nkey issue to resolve is the relatively low accuracy of the tags. In this paper,\nwe propose a novel semi-automatic framework to construct a more accurate and\neffective training set from these web media resources for each label that we\nwant to learn. Experiments conducted on a real-world dataset demonstrate that\nthe constructed training set can result in higher accuracy for image\nannotation.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.2859v1"
    },
    {
        "title": "Recent Trends and Research Issues in Video Association Mining",
        "authors": [
            "Vijayakumar V",
            "Nedunchezhian R"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  With the ever-growing digital libraries and video databases, it is\nincreasingly important to understand and mine the knowledge from video database\nautomatically. Discovering association rules between items in a large video\ndatabase plays a considerable role in the video data mining research areas.\nBased on the research and development in the past years, application of\nassociation rule mining is growing in different domains such as surveillance,\nmeetings, broadcast news, sports, archives, movies, medical data, as well as\npersonal and online media collections. The purpose of this paper is to provide\ngeneral framework of mining the association rules from video database. This\narticle is also represents the research issues in video association mining\nfollowed by the recent trends.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2040v1"
    },
    {
        "title": "Steganography Algorithm to Hide Secret Message inside an Image",
        "authors": [
            "Rosziati Ibrahim",
            "Teoh Suk Kuan"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  In this paper, the authors propose a new algorithm to hide data inside image\nusing steganography technique. The proposed algorithm uses binary codes and\npixels inside an image. The zipped file is used before it is converted to\nbinary codes to maximize the storage of data inside the image. By applying the\nproposed algorithm, a system called Steganography Imaging System (SIS) is\ndeveloped. The system is then tested to see the viability of the proposed\nalgorithm. Various sizes of data are stored inside the images and the PSNR\n(Peak signal-to-noise ratio) is also captured for each of the images tested.\nBased on the PSNR value of each images, the stego image has a higher PSNR\nvalue. Hence this new steganography algorithm is very efficient to hide the\ndata inside the image.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2809v1"
    },
    {
        "title": "Real-time detection and tracking of multiple objects with partial\n  decoding in H.264/AVC bitstream domain",
        "authors": [
            "Wonsang You",
            "M. S. Houari Sabirin",
            "Munchurl Kim"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  In this paper, we show that we can apply probabilistic spatiotemporal\nmacroblock filtering (PSMF) and partial decoding processes to effectively\ndetect and track multiple objects in real time in H.264|AVC bitstreams with\nstationary background. Our contribution is that our method cannot only show\nfast processing time but also handle multiple moving objects that are\narticulated, changing in size or internally have monotonous color, even though\nthey contain a chaotic set of non-homogeneous motion vectors inside. In\naddition, our partial decoding process for H.264|AVC bitstreams enables to\nimprove the accuracy of object trajectories and overcome long occlusion by\nusing extracted color information.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.4743v1"
    },
    {
        "title": "A new hybrid jpeg image compression scheme using symbol reduction\n  technique",
        "authors": [
            "Bheshaj Kumar",
            "Kavita Thakur",
            "G. R. Sinha"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Lossy JPEG compression is a widely used compression technique. Normally the\nJPEG standard technique uses three process mapping reduces interpixel\nredundancy, quantization, which is lossy process and entropy encoding, which is\nconsidered lossless process. In this paper, a new technique has been proposed\nby combining the JPEG algorithm and Symbol Reduction Huffman technique for\nachieving more compression ratio. The symbols reduction technique reduces the\nnumber of symbols by combining together to form a new symbol. As a result of\nthis technique the number of Huffman code to be generated also reduced. It is\nsimple fast and easy to implement. The result shows that the performance of\nstandard JPEG method can be improved by proposed method. This hybrid approach\nachieves about 20% more compression ratio than the Standard JPEG.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.4943v1"
    },
    {
        "title": "Genetic Algorithm to Make Persistent Security and Quality of Image in\n  Steganography from RS Analysis",
        "authors": [
            "T. R. Gopalakrishnan Nair",
            "Suma V",
            "Manas S"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Retention of secrecy is one of the significant features during communication\nactivity. Steganography is one of the popular methods to achieve secret\ncommunication between sender and receiver by hiding message in any form of\ncover media such as an audio, video, text, images etc. Least significant bit\nencoding is the simplest encoding method used by many steganography programs to\nhide secret message in 24bit, 8bit colour images and grayscale images.\nSteganalysis is a method of detecting secret message hidden in a cover media\nusing steganography. RS steganalysis is one of the most reliable steganalysis\nwhich performs statistical analysis of the pixels to successfully detect the\nhidden message in an image. However, existing steganography method protects the\ninformation against RS steganalysis in grey scale images. This paper presents a\nsteganography method using genetic algorithm to protect against the RS attack\nin colour images. Stego image is divided into number of blocks. Subsequently,\nwith the implementation of natural evolution on the stego image using genetic\nalgorithm enables to achieve optimized security and image quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.2616v1"
    },
    {
        "title": "Image Enhancement with Statistical Estimation",
        "authors": [
            "Aroop Mukherjee",
            "Soumen Kanrar"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Contrast enhancement is an important area of research for the image analysis.\nOver the decade, the researcher worked on this domain to develop an efficient\nand adequate algorithm. The proposed method will enhance the contrast of image\nusing Binarization method with the help of Maximum Likelihood Estimation (MLE).\nThe paper aims to enhance the image contrast of bimodal and multi-modal images.\nThe proposed methodology use to collect mathematical information retrieves from\nthe image. In this paper, we are using binarization method that generates the\ndesired histogram by separating image nodes. It generates the enhanced image\nusing histogram specification with binarization method. The proposed method has\nshowed an improvement in the image contrast enhancement compare with the other\nimage.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.1365v1"
    },
    {
        "title": "Joint Reconstruction of Multi-view Compressed Images",
        "authors": [
            "Vijayaraghavan Thirumalai",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  The distributed representation of correlated multi-view images is an\nimportant problem that arise in vision sensor networks. This paper concentrates\non the joint reconstruction problem where the distributively compressed\ncorrelated images are jointly decoded in order to improve the reconstruction\nquality of all the compressed images. We consider a scenario where the images\ncaptured at different viewpoints are encoded independently using common coding\nsolutions (e.g., JPEG, H.264 intra) with a balanced rate distribution among\ndifferent cameras. A central decoder first estimates the underlying correlation\nmodel from the independently compressed images which will be used for the joint\nsignal recovery. The joint reconstruction is then cast as a constrained convex\noptimization problem that reconstructs total-variation (TV) smooth images that\ncomply with the estimated correlation model. At the same time, we add\nconstraints that force the reconstructed images to be consistent with their\ncompressed versions. We show by experiments that the proposed joint\nreconstruction scheme outperforms independent reconstruction in terms of image\nquality, for a given target bit rate. In addition, the decoding performance of\nour proposed algorithm compares advantageously to state-of-the-art distributed\ncoding schemes based on disparity learning and on the DISCOVER.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.4326v1"
    },
    {
        "title": "Improvement of ISOM by using filter",
        "authors": [
            "Imen Chaabouni",
            "Wiem Fourati",
            "Med Salim Bouhlel"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Image compression helps in storing the transmitted data in proficient way by\ndecreasing its redundancy. This technique helps in transferring more digital or\nmultimedia data over internet as it increases the storage space. It is\nimportant to maintain the image quality even if it is compressed to certain\nextent. Depend upon this the image compression is classified into two\ncategories : lossy and lossless image compression. There are many lossy digital\nimage compression techniques exists. Among this Incremental Self Organizing Map\nis a familiar one. The good pictures quality can be retrieved if image\ndenoising technique is used for compression and also provides better\ncompression ratio. Image denoising is an important pre-processing step for many\nimage analysis and computer vision system. It refers to the task of recovering\na good estimate of the true image from a degraded observation without altering\nand changing useful structure in the image such as discontinuities and edges.\nMany approaches have been proposed to remove the noise effectively while\npreserving the original image details and features as much as possible. This\npaper proposes a technique for image compression using Incremental Self\nOrganizing Map (ISOM) with Discret Wavelet Transform (DWT) by applying\nfiltering techniques which play a crucial role in enhancing the quality of a\nreconstructed image. The experimental result shows that the proposed technique\nobtained better compression ratio value.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.2268v1"
    },
    {
        "title": "Comparison of Speech Activity Detection Techniques for Speaker\n  Recognition",
        "authors": [
            "Md. Sahidullah",
            "Goutam Saha"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Speech activity detection (SAD) is an essential component for a variety of\nspeech processing applications. It has been observed that performances of\nvarious speech based tasks are very much dependent on the efficiency of the\nSAD. In this paper, we have systematically reviewed some popular SAD techniques\nand their applications in speaker recognition. Speaker verification system\nusing different SAD technique are experimentally evaluated on NIST speech\ncorpora using Gaussian mixture model- universal background model (GMM-UBM)\nbased classifier for clean and noisy conditions. It has been found that two\nGaussian modeling based SAD is comparatively better than other SAD techniques\nfor different types of noises.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.0297v2"
    },
    {
        "title": "Navigation domain representation for interactive multiview imaging",
        "authors": [
            "Thomas Maugey",
            "Ismael Daribo",
            "Gene Cheung",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Enabling users to interactively navigate through different viewpoints of a\nstatic scene is a new interesting functionality in 3D streaming systems. While\nit opens exciting perspectives towards rich multimedia applications, it\nrequires the design of novel representations and coding techniques in order to\nsolve the new challenges imposed by interactive navigation. Interactivity\nclearly brings new design constraints: the encoder is unaware of the exact\ndecoding process, while the decoder has to reconstruct information from\nincomplete subsets of data since the server can generally not transmit images\nfor all possible viewpoints due to resource constrains. In this paper, we\npropose a novel multiview data representation that permits to satisfy bandwidth\nand storage constraints in an interactive multiview streaming system. In\nparticular, we partition the multiview navigation domain into segments, each of\nwhich is described by a reference image and some auxiliary information. The\nauxiliary information enables the client to recreate any viewpoint in the\nnavigation segment via view synthesis. The decoder is then able to navigate\nfreely in the segment without further data request to the server; it requests\nadditional data only when it moves to a different segment. We discuss the\nbenefits of this novel representation in interactive navigation systems and\nfurther propose a method to optimize the partitioning of the navigation domain\ninto independent segments, under bandwidth and storage constraints.\nExperimental results confirm the potential of the proposed representation;\nnamely, our system leads to similar compression performance as classical\ninter-view coding, while it provides the high level of flexibility that is\nrequired for interactive streaming. Hence, our new framework represents a\npromising solution for 3D data representation in novel interactive multimedia\nservices.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.5041v2"
    },
    {
        "title": "A Non-Blind Watermarking Scheme for Gray Scale Images in Discrete\n  Wavelet Transform Domain using Two Subbands",
        "authors": [
            "Abdur Shahid",
            "Shahriar Badsha",
            "Md. Rethwan Kabeer",
            "Junaid Ahsan",
            "Mufti Mahmud"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Digital watermarking is the process to hide digital pattern directly into a\ndigital content. Digital watermarking techniques are used to address digital\nrights management, protect information and conceal secrets. An invisible\nnon-blind watermarking approach for gray scale images is proposed in this\npaper. The host image is decomposed into 3-levels using Discrete Wavelet\nTransform. Based on the parent-child relationship between the wavelet\ncoefficients the Set Partitioning in Hierarchical Trees (SPIHT) compression\nalgorithm is performed on the LH3, LH2, HL3 and HL2 subbands to find out the\nsignificant coefficients. The most significant coefficients of LH2 and HL2\nbands are selected to embed a binary watermark image. The selected significant\ncoefficients are modulated using Noise Visibility Function, which is considered\nas the best strength to ensure better imperceptibility. The approach is tested\nagainst various image processing attacks such as addition of noise, filtering,\ncropping, JPEG compression, histogram equalization and contrast adjustment. The\nexperimental results reveal the high effectiveness of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.2699v1"
    },
    {
        "title": "Network Coding Meets Multimedia: a Review",
        "authors": [
            "Enrico Magli",
            "Mea Wang",
            "Pascal Frossard",
            "Athina Markopoulou"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  While every network node only relays messages in a traditional communication\nsystem, the recent network coding (NC) paradigm proposes to implement simple\nin-network processing with packet combinations in the nodes. NC extends the\nconcept of \"encoding\" a message beyond source coding (for compression) and\nchannel coding (for protection against errors and losses). It has been shown to\nincrease network throughput compared to traditional networks implementation, to\nreduce delay and to provide robustness to transmission errors and network\ndynamics. These features are so appealing for multimedia applications that they\nhave spurred a large research effort towards the development of\nmultimedia-specific NC techniques. This paper reviews the recent work in NC for\nmultimedia applications and focuses on the techniques that fill the gap between\nNC theory and practical applications. It outlines the benefits of NC and\npresents the open challenges in this area. The paper initially focuses on\nmultimedia-specific aspects of network coding, in particular delay, in-network\nerror control, and media-specific error control. These aspects permit to handle\nvarying network conditions as well as client heterogeneity, which are critical\nto the design and deployment of multimedia systems. After introducing these\ngeneral concepts, the paper reviews in detail two applications that lend\nthemselves naturally to NC via the cooperation and broadcast models, namely\npeer-to-peer multimedia streaming and wireless networking.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.4206v1"
    },
    {
        "title": "Content based video retrieval",
        "authors": [
            "B. V. Patel",
            "B. B. Meshram"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Content based video retrieval is an approach for facilitating the searching\nand browsing of large image collections over World Wide Web. In this approach,\nvideo analysis is conducted on low level visual properties extracted from video\nframe. We believed that in order to create an effective video retrieval system,\nvisual perception must be taken into account. We conjectured that a technique\nwhich employs multiple features for indexing and retrieval would be more\neffective in the discrimination and search tasks of videos. In order to\nvalidate this claim, content based indexing and retrieval systems were\nimplemented using color histogram, various texture features and other\napproaches. Videos were stored in Oracle 9i Database and a user study measured\ncorrectness of response.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.4683v1"
    },
    {
        "title": "Multi-View Video Packet Scheduling",
        "authors": [
            "Laura Toni",
            "Thomas Maugey",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  In multiview applications, multiple cameras acquire the same scene from\ndifferent viewpoints and generally produce correlated video streams. This\nresults in large amounts of highly redundant data. In order to save resources,\nit is critical to handle properly this correlation during encoding and\ntransmission of the multiview data. In this work, we propose a\ncorrelation-aware packet scheduling algorithm for multi-camera networks, where\ninformation from all cameras are transmitted over a bottleneck channel to\nclients that reconstruct the multiview images. The scheduling algorithm relies\non a new rate-distortion model that captures the importance of each view in the\nscene reconstruction. We propose a problem formulation for the optimization of\nthe packet scheduling policies, which adapt to variations in the scene content.\nThen, we design a low complexity scheduling algorithm based on a trellis search\nthat selects the subset of candidate packets to be transmitted towards\neffective multiview reconstruction at clients. Extensive simulation results\nconfirm the gain of our scheduling algorithm when inter-source correlation\ninformation is used in the scheduler, compared to scheduling policies with no\ninformation about the correlation or non-adaptive scheduling policies. We\nfinally show that increasing the optimization horizon in the packet scheduling\nalgorithm improves the transmission performance, especially in scenarios where\nthe level of correlation rapidly varies with time.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.4455v2"
    },
    {
        "title": "High Quality Image Interpolation via Local Autoregressive and Nonlocal\n  3-D Sparse Regularization",
        "authors": [
            "Xinwei Gao",
            "Jian Zhang",
            "Feng Jiang",
            "Xiaopeng Fan",
            "Siwei Ma",
            "Debin Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  In this paper, we propose a novel image interpolation algorithm, which is\nformulated via combining both the local autoregressive (AR) model and the\nnonlocal adaptive 3-D sparse model as regularized constraints under the\nregularization framework. Estimating the high-resolution image by the local AR\nregularization is different from these conventional AR models, which weighted\ncalculates the interpolation coefficients without considering the rough\nstructural similarity between the low-resolution (LR) and high-resolution (HR)\nimages. Then the nonlocal adaptive 3-D sparse model is formulated to regularize\nthe interpolated HR image, which provides a way to modify these pixels with the\nproblem of numerical stability caused by AR model. In addition, a new\nSplit-Bregman based iterative algorithm is developed to solve the above\noptimization problem iteratively. Experiment results demonstrate that the\nproposed algorithm achieves significant performance improvements over the\ntraditional algorithms in terms of both objective quality and visual perception\n",
        "pdf_link": "http://arxiv.org/pdf/1212.6058v1"
    },
    {
        "title": "A Poisson Hidden Markov Model for Multiview Video Traffic",
        "authors": [
            "Lorenzo Rossi",
            "Jacob Chakareski",
            "Pascal Frossard",
            "Stefania Colonnese"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Multiview video has recently emerged as a means to improve user experience in\nnovel multimedia services. We propose a new stochastic model to characterize\nthe traffic generated by a Multiview Video Coding (MVC) variable bit rate\nsource. To this aim, we resort to a Poisson Hidden Markov Model (P-HMM), in\nwhich the first (hidden) layer represents the evolution of the video activity\nand the second layer represents the frame sizes of the multiple encoded views.\nWe propose a method for estimating the model parameters in long MVC sequences.\nWe then present extensive numerical simulations assessing the model's ability\nto produce traffic with realistic characteristics for a general class of MVC\nsequences. We then extend our framework to network applications where we show\nthat our model is able to accurately describe the sender and receiver buffers\nbehavior in MVC transmission. Finally, we derive a model of user behavior for\ninteractive view selection, which, in conjunction with our traffic model, is\nable to accurately predict actual network load in interactive multiview\nservices.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.0344v1"
    },
    {
        "title": "A Novel Digital Watermarking Algorithm using Random Matrix Image",
        "authors": [
            "Mahimn Pandya",
            "Hiren Joshi",
            "Ashish Jani"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  The availability of bandwidth for internet access is sufficient enough to\ncommunicate digital assets. These digital assets are subjected to various types\nof threats. [19] As a result of this, protection mechanism required for the\nprotection of digital assets is of priority in research. The threat of current\nfocus is unauthorized copying of digital assets which give boost to piracy.\nThis under the copyright act is illegal and a robust mechanism is required to\ncurb this kind of unauthorized copy. To safeguard the copyright digital assets,\na robust digital watermarking technique is needed. The existing digital\nwatermarking techniques protect digital assets by embedding a digital watermark\ninto a host digital image. This embedding does induce slight distortion in the\nhost image but the distortion is usually too small to be noticed. At the same\ntime the embedded watermark must be robust enough to with stand deliberate\nattacks. There are various techniques of digital watermarking but researchers\nare making constant efforts to increase the robustness of the watermark image.\nThe layered approach of watermarking based on Huffman coding [5] can soon\nincrease the robustness of digital watermark.[11] Ultimately, increasing the\nsecurity of copyright of protection. The proposed work is in similar direction\nwhere in RMI (Random Matrix Image) is used in place of Huffman coding. This\ninnovative algorithm has considerably increased the robustness in digital\nwatermark while also enhancing security of production\n",
        "pdf_link": "http://arxiv.org/pdf/1301.4337v2"
    },
    {
        "title": "Video Tester -- A multiple-metric framework for video quality assessment\n  over IP networks",
        "authors": [
            "Iñaki Ucar",
            "Jorge Navarro-Ortiz",
            "Pablo Ameigeiras",
            "Juan M. Lopez-Soler"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  This paper presents an extensible and reusable framework which addresses the\nproblem of video quality assessment over IP networks. The proposed tool\n(referred to as Video-Tester) supports raw uncompressed video encoding and\ndecoding. It also includes different video over IP transmission methods (i.e.:\nRTP over UDP unicast and multicast, as well as RTP over TCP). In addition, it\nis furnished with a rich set of offline analysis capabilities. Video-Tester\nanalysis includes QoS and bitstream parameters estimation (i.e.: bandwidth,\npacket inter-arrival time, jitter and loss rate, as well as GOP size and\nI-frame loss rate). Our design facilitates the integration of virtually any\nexisting video quality metric thanks to the adopted Python-based modular\napproach. Video-Tester currently provides PSNR, SSIM, ITU-T G.1070 video\nquality metric, DIV and PSNR-based MOS estimations. In order to promote its use\nand extension, Video-Tester is open and publicly available.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.5793v1"
    },
    {
        "title": "The Robust Digital Image Watermarking using Quantization and Fuzzy Logic\n  Approach in DWT Domain",
        "authors": [
            "Nallagarla Ramamurthy",
            "S. Varadarajan"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  In this paper a novel approach to embed watermark into the host image using\nquantization with the help of Dynamic Fuzzy Inference System (DFIS) is\nproposed. The cover image is decomposed up to 3- levels using quantization and\nDiscrete Wavelet Transform (DWT). A bitmap of size 64x64 pixels is embedded\ninto the host image using DFIS rule base. The DFIS is utilized to generate the\nwatermark weighting function to embed the imperceptible watermark. The\nimplemented watermarking algorithm is imperceptible and robust to some normal\nattacks such as JPEG Compression, salt&pepper noise, median filtering, rotation\nand cropping.\n  Keywords: Watermark, Quantization, Dynamic Fuzzy Inference System,\nImperceptible, Robust, JPEG Compression, Cropping.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.4233v1"
    },
    {
        "title": "Secure Video Streaming Plug-In",
        "authors": [
            "Avinash Bhujbal",
            "Ashish Jagtap",
            "Devendra Gurav",
            "Tino Jameskutty"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Video sharing sites like YouTube, Metacafe, Dailymotion, Vimeo, etc. provide\na platform for media content sharing among its users. Some of these videos are\ncopyright protected and restricted from being downloaded and saved. But users\ncan use various download managers or application programs to download and save\nthese videos. This affects the incoming traffic on these websites reducing\ntheir hit rate and consequently reducing their revenue. Adobe Flash Player is\nthe most commonly used player for watching online videos. It uses RTMP (Real\nTime Messaging Protocol) to stream audio, video and data over the Internet,\nbetween a Flash Player and Adobe Flash Media Server.Here, we propose a plug-in\nthat enables the site owner control over downloading of videos from such\nwebsite. The plug-in will be installed at the client side with the consent of\nthe user. When the video is being played this plug-in will send unique keys to\nthe media server. The server will continue streaming the video after verifying\nthe keys. Download managers or application programs will not be able to\ndownload the videos as they wont be able to create the unique keys that need to\nbe sent to the server.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.1697v1"
    },
    {
        "title": "Medical Information Embedding in Compressed Watermarked Intravascular\n  Ultrasound Video",
        "authors": [
            "Nilanjan Dey",
            "Suvojit Acharjee",
            "Debalina Biswas",
            "Achintya Das",
            "Sheli Sinha Chaudhuri"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  In medical field, intravascular ultrasound (IVUS) is a tomographic imaging\nmodality, which can identify the boundaries of different layers of blood\nvessels. IVUS can detect myocardial infarction (heart attack) that remains\nignored and unattended when only angioplasty is done. During the past decade,\nit became easier for some individuals or groups to copy and transmits digital\ninformation without the permission of the owner. For increasing authentication\nand security of copyrights, digital watermarking, an information hiding\ntechnique, was introduced. Achieving watermarking technique with lesser amount\nof distortion in biomedical data is a challenging task. Watermark can be\nembedded into an image or in a video. As video data is a huge amount of\ninformation, therefore a large storage area is needed which is not feasible. In\nthis case motion vector based video compression is done to reduce size. In this\npresent paper, an Electronic Patient Record (EPR) is embedded as watermark\nwithin an IVUS video and then motion vector is calculated. This proposed method\nproves robustness as the extracted watermark has good PSNR value and less MSE.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.2211v1"
    },
    {
        "title": "Image compression using anti-forensics method",
        "authors": [
            "M. S. Sreelakshmi",
            "D. Venkataraman"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  A large number of image forensics methods are available which are capable of\nidentifying image tampering. But these techniques are not capable of addressing\nthe anti-forensics method which is able to hide the trace of image tampering.\nIn this paper anti-forensics method for digital image compression has been\nproposed. This anti-forensics method is capable of removing the traces of image\ncompression. Additionally, technique is also able to remove the traces of\nblocking artifact that are left by image compression algorithms that divide an\nimage into segments during compression process. This method is targeted to\nremove the compression fingerprints of JPEG compression.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.2330v1"
    },
    {
        "title": "StegTorrent: a Steganographic Method for the P2P File Sharing Service",
        "authors": [
            "Pawel Kopiczko",
            "Wojciech Mazurczyk",
            "Krzysztof Szczypiorski"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  The paper proposes StegTorrent a new network steganographic method for the\npopular P2P file transfer service-BitTorrent. It is based on modifying the\norder of data packets in the peer-peer data exchange protocol. Unlike other\nexisting steganographic methods that modify the packets' order it does not\nrequire any synchronization. Experimental results acquired from prototype\nimplementation proved that it provides high steganographic bandwidth of up to\n270 b/s while introducing little transmission distortion and providing\ndifficult detectability.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.4264v1"
    },
    {
        "title": "Odd-Even Embedding Scheme Based Modified Reversible Watermarking\n  Technique using Blueprint",
        "authors": [
            "Arijit Kumar Pal",
            "Poulami Das",
            "Nilanjan Dey"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Digital watermarking is a technique of information adding or information\nhiding in order to identify the owner of the data in multimedia content. It\nseems that a signal or digital image can permanently embed over another digital\ndata providing a good way to protect intellectual property from illegal\nreplication. The cover data that is transmitted through the internet hides the\nwatermark in a computer aided assertion method such that it becomes\nundetectable. Finally it stands as a hindrance over many operations without\nharming the embedded host document. Unfortunately, many owners of the digital\nmaterials such as images, text, audio and video are reluctant to the spreading\nof their documents on the web or other networked environment, because the ease\nof duplicating digital materials facilitates copyright violation. Digital media\ndistribution occurs through various channels. The cover data may or may not\nhold any relation with the watermark information. In the last two decades, a\nconsiderable amount of research has been done on the digital watermarking of\nmultimedia files such as audio, video, images and text. Different type of\nwatermarking algorithms has been proposed by the researchers to achieve high\nlevel of security and authenticity. In our proposed method, a modified\nreversible watermarking technique is introduced, which employs a blueprint\ngeneration of original image based on odd-even embedding methodology to yield\nlarge data hiding capacity, security as well as high watermarked quality. The\nexperimental results demonstrate that, no matter how much secret data is\nembedded, the watermarked quality is about 51dB in this proposed scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.5972v1"
    },
    {
        "title": "Hiding Image in Image by Five Modulus Method for Image Steganography",
        "authors": [
            "Firas A. Jassim"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  This paper is to create a practical steganographic implementation to hide\ncolor image (stego) inside another color image (cover). The proposed technique\nuses Five Modulus Method to convert the whole pixels within both the cover and\nthe stego images into multiples of five. Since each pixels inside the stego\nimage is divisible by five then the whole stego image could be divided by five\nto get new range of pixels 0..51. Basically, the reminder of each number that\nis not divisible by five is either 1,2,3 or 4 when divided by 5. Subsequently,\nthen a 4-by-4 window size has been implemented to accommodate the proposed\ntechnique. For each 4-by-4 window inside the cover image, a number from 1 to 4\ncould be embedded secretly from the stego image. The previous discussion must\nbe applied separately for each of the R, G, and B arrays. Moreover, a stego-key\ncould be combined with the proposed algorithm to make it difficult for any\nadversary to extract the secret image from the cover image. Based on the PSNR\nvalue, the extracted stego image has high PSNR value. Hence this new\nsteganography algorithm is very efficient to hide color images.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.1571v1"
    },
    {
        "title": "Metrics for Video Quality Assessment in Mobile Scenarios",
        "authors": [
            "Gaurav Pande"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  With exponential increase in the volumes of video traffic in cellular\nnet-works, there is an increasing need for optimizing the quality of video\ndelivery. 4G networks (Long Term Evolution Advanced or LTE A) are being\nintroduced in many countries worldwide, which allow a downlink speed of upto 1\nGbps and uplink of 100 Mbps over a single base station. This makes a strong\npush towards video broadcasting over LTE networks, characterizing its\nperformance and developing metrics which can be deployed to provide user\nfeedback of video quality and feed-back them to network operators to fine tune\nthe network. In this paper, we characterize the performance of video\ntransmission over LTE A physical layer using popular video quality metrics such\nas SSIM, Blocking, Blurring, NIQE and BRISQUE. We conduct experiments to find a\nsuitable no-reference metrics for mobile scenario and find that Blocking\nMetrics is most promising in case of channel or modulation variations but it\ndoes not perform well to quantize variations in compression ratios. The metrics\nBRISQUE is very efficient in quantizing this distortion and performs well in\ncase of network variations also.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3758v1"
    },
    {
        "title": "Security Issues In Speech Watermarking for Information Transmission",
        "authors": [
            "Rupa Patel",
            "Urmila Shrawankar"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  The secure transmission of speech information is a significant issue faced by\nmany security professionals and individuals. By applying voice-encryption\ntechnique any kind of encrypted sensitive speech data such as password can be\ntransmitted. But this has the serious disadvantage that by means of\ncryptanalysis attack encrypted data can be compromised. Increasing the strength\nof encryption/decryption results in an associated increased in the cost.\nAdditional techniques like stenography and digital watermarking can be used to\nconceal information in an undetectable way in audio data. However this\nwatermarked audio data has to be send through unreliable media and an\neavesdropper might get hold of secret message and can also determine the\nidentity of a speaker who is sending the information since human voice contains\ninformation based on its characteristics such as frequency, pitch, and energy.\nThis paper proposes Normalized Speech Watermarking technique. Speech signal is\nnormalized to hide the identity of the speaker who is sending the information\nand then speech watermarking technique is applied on this normalized signal\nthat contains the message (password) so that what information is transmitted\nshould not be unauthorizedly revealed.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.6872v1"
    },
    {
        "title": "Secure Transmission of Password Using Speech Watermarking",
        "authors": [
            "Rupa Patel",
            "Urmila Shrawankar",
            "V. M Thakare"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Internet is one of the most valuable resources for information communication\nand retrievals. Most multimedia signals today are in digital formats. The\ndigital data can be duplicated and edited with great ease which has led to a\nneed for data integrity and protection of digital data. The security\nrequirements such as integrity or data authentication can be met by\nimplementing security measures using digital watermarking techniques. In this\npaper a blind speech watermarking algorithm that embeds the watermark signal\ndata in the musical (sequence) host signal by using frequency masking is used.\nA different logarithmic approach is proposed. In this regard a logarithmic\nfunction is first applied to watermark data. Then the transformed signal is\nembedded to the converted version of host signal which is obtained by applying\nFast Fourier transform method. Finally using inverse Fast Fourier Transform and\nantilogarithmic function watermark signal is retrieved.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.8080v1"
    },
    {
        "title": "Survey on QoE\\QoS Correlation Models For Multimedia Services",
        "authors": [
            "Mohammed Alreshoodi",
            "John Woods"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  This paper presents a brief review of some existing correlation models which\nattempt to map Quality of Service (QoS) to Quality of Experience (QoE) for\nmultimedia services. The term QoS refers to deterministic network behaviour, so\nthat data can be transported with a minimum of packet loss, delay and maximum\nbandwidth. QoE is a subjective measure that involves human dimensions; it ties\ntogether user perception, expectations, and experience of the application and\nnetwork performance. The Holy Grail of subjective measurement is to predict it\nfrom the objective measurements; in other words predict QoE from a given set of\nQoS parameters or vice versa. Whilst there are many quality models for\nmultimedia, most of them are only partial solutions to predicting QoE from a\ngiven QoS. This contribution analyses a number of previous attempts and\noptimisation techniquesthat can reliably compute the weighting coefficients for\nthe QoS/QoE mapping.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.0221v1"
    },
    {
        "title": "Enhanced Tiny Encryption Algorithm with Embedding (ETEA)",
        "authors": [
            "Deepali Virmani",
            "Nidhi Beniwal",
            "Gargi Mandal",
            "Saloni Talwar"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  As computer systems become more pervasive and complex, security is\nincreasingly important. Secure Transmission refers to the transfer of data such\nas confidential or proprietary information over a secure channel. Many secure\ntransmission methods require a type of encryption. Secure transmissions are put\nin place to prevent attacks such as ARP spoofing and general data loss. Hence,\nin order to provide a better security mechanism, in this paper we propose\nEnhanced Tiny Encryption Algorithm with Embedding (ETEA), a data hiding\ntechnique called steganography along with the technique of encryption\n(Cryptography). The advantage of ETEA is that it incorporates cryptography and\nsteganography. The advantage proposed algorithm is that it hides the messages.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.6920v1"
    },
    {
        "title": "MAS for video objects segmentation and tracking based on active contours\n  and SURF descriptor",
        "authors": [
            "Mohamed Chakroun",
            "Ali Wali",
            "Adel M. Alimi"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  In computer vision, video segmentation and tracking is an important\nchallenging issue. In this paper, we describe a new video sequences\nsegmentation and tracking algorithm based on MAS \"multi-agent systems\" and SURF\n\"Speeded Up Robust Features\". Our approach consists in modelling a multi-agent\nsystem for segmenting the first image from a video sequence and tracking\nobjects in the video sequences. The used agents are supervisor and explorator\nagents, they are communicating between them and they inspire in their behavior\nfrom active contours approaches. The tracking of objects is based on SURF\ndescriptors \"Speed Up Robust Features\". We used the DIMA platform and \"API\nAteji PX\" (an extension of the Java language to facilitate parallel programming\non heterogeneous architectures) to implement this algorithm. The experimental\nresults indicate that the proposed algorithm is more robust and faster than\nprevious approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.0315v1"
    },
    {
        "title": "Improved Watermarking Scheme Using Discrete Cosine Transform and Schur\n  Decomposition",
        "authors": [
            "Henri Bruno Razafindradina",
            "Nicolas Raft Razafindrakoto",
            "Paul Auguste Randriamitantsoa"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Watermarking is a technique which consists in introducing a brand, the name\nor the logo of the author, in an image in order to protect it against illegal\ncopy. The capacity of the existing watermark channel is often limited. We\npropose in this paper a new robust method which consists in adding the\ntriangular matrix of the mark obtained after the Schur decomposition to the DCT\ntransform of the host image. The unitary matrix acts as secret key for the\nextraction of the mark. Unlike most watermarking algorithms, the host image and\nthe mark have the same size. The results show that our method is robust against\nattack techniques as : JPEG compression, colors reducing, adding noise,\nfiltering, cropping, low rotations, and histogram spreading.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.0435v1"
    },
    {
        "title": "Multimodal Approach for Video Surveillance Indexing and Retrieval",
        "authors": [
            "Ali Wali",
            "Adel M. Alimi"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  In this paper, we present an overview of a multimodal system to indexing and\nsearching video sequence by the content that has been developed within the\nREGIMVid project. A large part of our system has been developed as part of\nTRECVideo evaluation. The MAVSIR platform provides High-level feature\nextraction from audio-visual content and concept/event-based video retrieval.\nWe illustrate the architecture of the system as well as provide an overview of\nthe descriptors supported to date. Then we demonstrate the usefulness of the\ntoolbox in the context of feature extraction, concepts/events learning and\nretrieval in large collections of video surveillance dataset. The results are\nencouraging as we are able to get good results on several event categories,\nwhile for all events we have gained valuable insights and experience.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.1150v1"
    },
    {
        "title": "An Efficient Transport Protocol for delivery of Multimedia An Efficient\n  Transport Protocol for delivery of Multimedia Content in Wireless Grids",
        "authors": [
            "Suresh Jaganathan",
            "Srinivasan Arulanadam",
            "Damodaram Avula"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  A grid computing system is designed for solving complicated scientific and\ncommercial problems effectively,whereas mobile computing is a traditional\ndistributed system having computing capability with mobility and adopting\nwireless communications. Media and Entertainment fields can take advantage from\nboth paradigms by applying its usage in gaming applications and multimedia data\nmanagement. Multimedia data has to be stored and retrieved in an efficient and\neffective manner to put it in use. In this paper, we proposed an application\nlayer protocol for delivery of multimedia data in wireless girds i.e.\nmultimedia grid protocol (MMGP). To make streaming efficient a new video\ncompression algorithm called dWave is designed and embedded in the proposed\nprotocol. This protocol will provide faster, reliable access and render an\nimperceptible QoS in delivering multimedia in wireless grid environment and\ntackles the challenging issues such as i) intermittent connectivity, ii) device\nheterogeneity, iii) weak security and iv) device mobility.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.2393v1"
    },
    {
        "title": "Band Codes for Energy-Efficient Network Coding with Application to P2P\n  Mobile Streaming",
        "authors": [
            "Attilio Fiandrotti",
            "Valerio Bioglio",
            "Marco Grangetto",
            "Rossano Gaeta",
            "Enrico Magli"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  A key problem in random network coding (NC) lies in the complexity and energy\nconsumption associated with the packet decoding processes, which hinder its\napplication in mobile environments. Controlling and hence limiting such factors\nhas always been an important but elusive research goal, since the packet degree\ndistribution, which is the main factor driving the complexity, is altered in a\nnon-deterministic way by the random recombinations at the network nodes. In\nthis paper we tackle this problem proposing Band Codes (BC), a novel class of\nnetwork codes specifically designed to preserve the packet degree distribution\nduring packet encoding, ecombination and decoding. BC are random codes over\nGF(2) that exhibit low decoding complexity, feature limited and controlled\ndegree distribution by construction, and hence allow to effectively apply NC\neven in energy-constrained scenarios. In particular, in this paper we motivate\nand describe our new design and provide a thorough analysis of its performance.\nWe provide numerical simulations of the performance of BC in order to validate\nthe analysis and assess the overhead of BC with respect to a onventional NC\nscheme. Moreover, peer-to-peer media streaming experiments with a random-push\nprotocol show that BC reduce the decoding complexity by a factor of two, to a\npoint where NC-based mobile streaming to mobile devices becomes practically\nfeasible.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.0316v1"
    },
    {
        "title": "Speech Enhancement using Kernel and Normalized Kernel Affine Projection\n  Algorithm",
        "authors": [
            "Bolimera Ravi",
            "T. Kishore Kumar"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  The goal of this paper is to investigate the speech signal enhancement using\nKernel Affine Projection Algorithm (KAPA) and Normalized KAPA. The removal of\nbackground noise is very important in many applications like speech\nrecognition, telephone conversations, hearing aids, forensic, etc. Kernel\nadaptive filters shown good performance for removal of noise. If the evaluation\nof background noise is more slowly than the speech, i.e., noise signal is more\nstationary than the speech, we can easily estimate the noise during the pauses\nin speech. Otherwise it is more difficult to estimate the noise which results\nin degradation of speech. In order to improve the quality and intelligibility\nof speech, unlike time and frequency domains, we can process the signal in new\ndomain like Reproducing Kernel Hilbert Space (RKHS) for high dimensional to\nyield more powerful nonlinear extensions. For experiments, we have used the\ndatabase of noisy speech corpus (NOIZEUS). From the results, we observed the\nremoval noise in RKHS has great performance in signal to noise ratio values in\ncomparison with conventional adaptive filters.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.2359v1"
    },
    {
        "title": "Robust watermarking based on DWT SVD",
        "authors": [
            "Anumol Joseph",
            "K. Anusudha"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Digital information revolution has brought about many advantages and new\nissues. The protection of ownership and the prevention of unauthorized\nmanipulation of digital audio, image, and video materials has become an\nimportant concern due to the ease of editing and perfect reproduction.\nWatermarking is identified as a major means to achieve copyright protection. It\nis a branch of information hiding which is used to hide proprietary information\nin digital media like photographs, digital music, digital video etc. In this\npaper, a new image watermarking algorithm that is robust against various\nattacks is presented. DWT (Discrete Wavelet Transform) and SVD (Singular Value\nDecomposition) have been used to embed two watermarks in the HL and LH bands of\nthe host image. Simulation evaluation demonstrates that the proposed technique\nwithstand various attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.2423v7"
    },
    {
        "title": "Steganography using the Extensible Messaging and Presence Protocol\n  (XMPP)",
        "authors": [
            "Reshad Patuck",
            "Julio Hernandez-Castro"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  We present here the first work to propose different mechanisms for hiding\ndata in the Extensible Messaging and Presence Protocol (XMPP). This is a very\npopular instant messaging protocol used by many messaging platforms such as\nGoogle Talk, Cisco, LiveJournal and many others. Our paper describes how to\nsend a secret message from one XMPP client to another, without raising the\nsuspicion of any intermediaries. The methods described primarily focus on using\nthe underlying protocol as a means for steganography, unlike other related\nworks that try to hide data in the content of instant messages. In doing so, we\nprovide a more robust means of data hiding and additionally offer some\npreliminary analysis of its general security, in particular against\nentropic-based steganalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.0524v1"
    },
    {
        "title": "An Efficient Method for Image and Audio Steganography using Least\n  Significant Bit (LSB) Substitution",
        "authors": [
            "Ankit Chadha",
            "Neha Satam"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  In order to improve the data hiding in all types of multimedia data formats\nsuch as image and audio and to make hidden message imperceptible, a novel\nmethod for steganography is introduced in this paper. It is based on Least\nSignificant Bit (LSB) manipulation and inclusion of redundant noise as secret\nkey in the message. This method is applied to data hiding in images. For data\nhiding in audio, Discrete Cosine Transform (DCT) and Discrete Wavelet Transform\n(DWT) both are used. All the results displayed prove to be time-efficient and\neffective. Also the algorithm is tested for various numbers of bits. For those\nvalues of bits, Mean Square Error (MSE) and Peak-Signal-to-Noise-Ratio (PSNR)\nare calculated and plotted. Experimental results show that the stego-image is\nvisually indistinguishable from the original cover-image when n<=4, because of\nbetter PSNR which is achieved by this technique. The final results obtained\nafter steganography process does not reveal presence of any hidden message,\nthus qualifying the criteria of imperceptible message.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.1083v1"
    },
    {
        "title": "Mobile Multimedia Streaming Techniques : QoE and Energy Consumption\n  Perspective",
        "authors": [
            "Mohammad Ashraful Hoque",
            "Matti Siekkinen",
            "Jukka K. Nurminen",
            "Mika Aalto",
            "Sasu Tarkoma"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Multimedia streaming to mobile devices is challenging for two reasons. First,\nthe way content is delivered to a client must ensure that the user does not\nexperience a long initial playback delay or a distorted playback in the middle\nof a streaming session. Second, multimedia streaming applications are among the\nmost energy hungry applications in smartphones. The energy consumption mostly\ndepends on the delivery techniques and on the power management techniques of\nwireless access technologies (Wi-Fi, 3G, and 4G). In order to provide insights\non what kind of streaming techniques exist, how they work on different mobile\nplatforms, their efforts in providing smooth quality of experience, and their\nimpact on energy consumption of mobile phones, we did a large set of active\nmeasurements with several smartphones having both Wi-Fi and cellular network\naccess. Our analysis reveals five different techniques to deliver the content\nto the video players. The selection of a technique depends on the mobile\nplatform, device, player, quality, and service. The results from our traffic\nand power measurements allow us to conclude that none of the identified\ntechniques is optimal because they take none of the following facts into\naccount: access technology used, user behavior, and user preferences concerning\ndata waste. We point out the technique with optimal playback buffer\nconfiguration, which provides the most attractive trade-offs in particular\nsituations.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.4317v2"
    },
    {
        "title": "Fake View Analytics in Online Video Services",
        "authors": [
            "Liang Chen",
            "Yipeng Zhou",
            "Dah Ming Chiu"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Online video-on-demand(VoD) services invariably maintain a view count for\neach video they serve, and it has become an important currency for various\nstakeholders, from viewers, to content owners, advertizers, and the online\nservice providers themselves. There is often significant financial incentive to\nuse a robot (or a botnet) to artificially create fake views. How can we detect\nthe fake views? Can we detect them (and stop them) using online algorithms as\nthey occur? What is the extent of fake views with current VoD service\nproviders? These are the questions we study in the paper. We develop some\nalgorithms and show that they are quite effective for this problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.5050v1"
    },
    {
        "title": "Evaluating the Performance of IPTV over Fixed WiMAX",
        "authors": [
            "Jamil Hamodi",
            "Khaled Salah",
            "Ravindra Thool"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  IEEE specifies different modulation techniques for WiMAX; namely, BPSK, QPSK,\n16 QAM and 64 QAM. This paper studies the performance of Internet Protocol\nTelevision (IPTV) over Fixed WiMAX system considering different combinations of\ndigital modulation. The performance is studied taking into account a number of\nkey system parameters which include the variation in the video coding,\npath-loss, scheduling service classes different rated codes in FEC channel\ncoding. The performance study was conducted using OPNET simulation. The\nperformance is studied in terms of packet lost, packet jitter delay, end-to-end\ndelay, and network throughput. Simulation results show that higher order\nmodulation and coding schemes (namely, 16 QAM and 64 QAM) yield better\nperformance than that of QPSK.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.7442v1"
    },
    {
        "title": "WaterRPG: A Graph-based Dynamic Watermarking Model for Software\n  Protection",
        "authors": [
            "Ioannis Chionis",
            "Maria Chroni",
            "Stavros D. Nikolopoulos"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Software watermarking involves embedding a unique identifier or,\nequivalently, a watermark value within a software to prove owner's authenticity\nand thus to prevent or discourage copyright infringement. Towards the embedding\nprocess, several graph theoretic watermarking algorithmic techniques encode the\nwatermark values as graph structures and embed them in application programs.\nRecently, we presented an efficient codec system for encoding a watermark\nnumber $w$ as a reducible permutation graph $F[\\pi^*]$ through the use of\nself-inverting permutations $\\pi^*$. In this paper, we propose a dynamic\nwatermarking model, which we call WaterRPG, for embedding the watermark graph\n$F[\\pi^*]$ into an application program $P$. The main idea behind the proposed\nwatermarking model is a systematic use of appropriate calls of specific\nfunctions of the program $P$. More precisely, for a specific input $I_{key}$ of\nthe program $P$, our model takes the dynamic call-graph $G(P, I_{key})$ of $P$\nand the watermark graph $F[\\pi^*]$, and produces the watermarked program $P^*$\nhaving the following key property: its dynamic call-graph $G(P^*, I_{key})$ is\nisomorphic to the watermark graph $F[\\pi^*]$. Within this idea the program\n$P^*$ is produced by only altering appropriate calls of specific functions of\nthe input application program $P$. We have implemented our watermarking model\nWaterRPG in real application programs and evaluated its functionality under\nvarious and broadly used watermarking assessment criteria. The evaluation\nresults show that our model efficiently watermarks Java application programs\nwith respect to several watermarking metrics like data-rate, bytecode\ninstructions overhead, resiliency, time and space efficiency. Moreover, the\nembedded watermarks withstand several software obfuscation and optimization\nattacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.6658v1"
    },
    {
        "title": "On Importance of Steganographic Cost For Network Steganography",
        "authors": [
            "Wojciech Mazurczyk",
            "Steffen Wendzel",
            "Ignacio Azagra Villares",
            "Krzysztof Szczypiorski"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Network steganography encompasses the information hiding techniques that can\nbe applied in communication network environments and that utilize hidden data\ncarriers for this purpose. In this paper we introduce a characteristic called\nsteganographic cost which is an indicator for the degradation or distortion of\nthe carrier caused by the application of the steganographic method. Based on\nexemplary cases for single- and multi-method steganographic cost analyses we\nobserve that it can be an important characteristic that allows to express\nhidden data carrier degradation - similarly as MSE (Mean-Square Error) or PSNR\n(Peak Signal-to-Noise Ratio) are utilized for digital media steganography.\nSteganographic cost can moreover be helpful to analyse the relationships\nbetween two or more steganographic methods applied to the same hidden data\ncarrier.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.2519v1"
    },
    {
        "title": "Performance Comparison of Linear Prediction based Vocoders in Linux\n  Platform",
        "authors": [
            "Lani Rachel Mathew",
            "Ancy S. Anselam",
            "Sakuntala S. Pillai"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Linear predictive coders form an important class of speech coders. This paper\ndescribes the software level implementation of linear prediction based\nvocoders, viz. Code Excited Linear Prediction (CELP), Low-Delay CELP (LD-CELP)\nand Mixed Excitation Linear Prediction (MELP) at bit rates of 4.8 kb/s, 16 kb/s\nand 2.4 kb/s respectively. The C programs of the vocoders have been compiled\nand executed in Linux platform. Subjective testing with the help of Mean\nOpinion Score test has been performed. Waveform analysis has been done using\nPraat and Adobe Audition software. The results show that MELP and CELP produce\ncomparable quality while the quality of LD-CELP coder is much higher, at the\nexpense of higher bit rate.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.6473v1"
    },
    {
        "title": "Subjective and Objective Quality Assessment of Image: A Survey",
        "authors": [
            "Pedram Mohammadi",
            "Abbas Ebrahimi-Moghadam",
            "Shahram Shirani"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  With the increasing demand for image-based applications, the efficient and\nreliable evaluation of image quality has increased in importance. Measuring the\nimage quality is of fundamental importance for numerous image processing\napplications, where the goal of image quality assessment (IQA) methods is to\nautomatically evaluate the quality of images in agreement with human quality\njudgments. Numerous IQA methods have been proposed over the past years to\nfulfill this goal. In this paper, a survey of the quality assessment methods\nfor conventional image signals, as well as the newly emerged ones, which\nincludes the high dynamic range (HDR) and 3-D images, is presented. A\ncomprehensive explanation of the subjective and objective IQA and their\nclassification is provided. Six widely used subjective quality datasets, and\nperformance measures are reviewed. Emphasis is given to the full-reference\nimage quality assessment (FR-IQA) methods, and 9 often-used quality measures\n(including mean squared error (MSE), structural similarity index (SSIM),\nmulti-scale structural similarity index (MS-SSIM), visual information fidelity\n(VIF), most apparent distortion (MAD), feature similarity measure (FSIM),\nfeature similarity measure for color images (FSIMC), dynamic range independent\nmeasure (DRIM), and tone-mapped images quality index (TMQI)) are carefully\ndescribed, and their performance and computation time on four subjective\nquality datasets are evaluated. Furthermore, a brief introduction to 3-D IQA is\nprovided and the issues related to this area of research are reviewed.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.7799v1"
    },
    {
        "title": "Characterizing Internet Video for Large-scale Active Measurements",
        "authors": [
            "Saba Ahsan",
            "Varun Singh",
            "Jörg Ott"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  The availability of high definition video content on the web has brought\nabout a significant change in the characteristics of Internet video, but not\nmany studies on characterizing video have been done after this change. Video\ncharacteristics such as video length, format, target bit rate, and resolution\nprovide valuable input to design Adaptive Bit Rate (ABR) algorithms, sizing\nplayout buffers in Dynamic Adaptive HTTP streaming (DASH) players, model the\nvariability in video frame sizes, etc. This paper presents datasets collected\nin 2013 and 2014 that contains over 130,000 videos from YouTube's most viewed\n(or most popular) video charts in 58 countries. We describe the basic\ncharacteristics of the videos on YouTube for each category, format, video\nlength, file size, and data rate variation, observing that video length and\nfile size fit a log normal distribution. We show that three minutes of a video\nsuffice to represent its instant data rate fluctuation and that we can infer\ndata rate characteristics of different video resolutions from a single given\none. Based on our findings, we design active measurements for measuring the\nperformance of Internet video.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.5777v1"
    },
    {
        "title": "Precision-Energy-Throughput Scaling Of Generic Matrix Multiplication and\n  Convolution Kernels Via Linear Projections",
        "authors": [
            "Mohammad Ashraful Anam",
            "Paul N. Whatmough",
            "Yiannis Andreopoulos"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Generic matrix multiplication (GEMM) and one-dimensional\nconvolution/cross-correlation (CONV) kernels often constitute the bulk of the\ncompute- and memory-intensive processing within image/audio recognition and\nmatching systems. We propose a novel method to scale the energy and processing\nthroughput of GEMM and CONV kernels for such error-tolerant multimedia\napplications by adjusting the precision of computation. Our technique employs\nlinear projections to the input matrix or signal data during the top-level GEMM\nand CONV blocking and reordering. The GEMM and CONV kernel processing then uses\nthe projected inputs and the results are accumulated to form the final outputs.\nThroughput and energy scaling takes place by changing the number of projections\ncomputed by each kernel, which in turn produces approximate results, i.e.\nchanges the precision of the performed computation. Results derived from a\nvoltage- and frequency-scaled ARM Cortex A15 processor running face recognition\nand music matching algorithms demonstrate that the proposed approach allows for\n280%~440% increase of processing throughput and 75%~80% decrease of energy\nconsumption against optimized GEMM and CONV kernels without any impact in the\nobtained recognition or matching accuracy. Even higher gains can be obtained if\none is willing to tolerate some reduction in the accuracy of the recognition\nand matching applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.2860v1"
    },
    {
        "title": "Radio Resource Allocation for Scalable Video Services over Wireless\n  Cellular Networks",
        "authors": [
            "Mostafa Zaman Chowdhury",
            "Tuan Nguyen",
            "Young-Il Kim",
            "Won Ryu",
            "Yeong Min Jang"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Good quality video services always require higher bandwidth. Hence, to\nprovide the video services e.g., multicast/broadcast services (MBS) and unicast\nservices along with the existing voice, internet, and other background traffic\nservices over the wireless cellular networks, it is required to efficiently\nmanage the wireless resources in order to reduce the overall forced call\ntermination probability, to maximize the overall service quality, and to\nmaximize the revenue. Fixed bandwidth allocation for the MBS sessions either\nreduces the quality of the MBS videos and bandwidth utilization or increases\nthe overall forced call termination probability and of course the handover call\ndropping probability as well. Scalable Video Coding (SVC) technique allows the\nvariable bit rate allocation for the video services. In this paper, we propose\na bandwidth allocation scheme that efficiently allocates bandwidth among the\nMBS sessions and the non-MBS traffic calls (e.g., voice, unicast, internet, and\nother background traffic). The proposed scheme reduces the bandwidth allocation\nfor the MBS sessions during the congested traffic condition only to accommodate\nmore calls in the system. Instead of allocating fixed bandwidths for the BMS\nsessions and the non-MBS traffic, our scheme allocates variable bandwidths for\nthem. However, the minimum quality of the videos is guaranteed by allocating\nminimum bandwidth for them. Using the mathematical and numerical analyses, we\nshow that the proposed scheme maximizes the bandwidth utilization and\nsignificantly reduces the overall forced call termination probability as well\nas the handover call dropping probability.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.3628v1"
    },
    {
        "title": "Call Admission Control based on Adaptive Bandwidth Allocation for\n  Wireless Networks",
        "authors": [
            "Mostafa Zaman Chowdhury",
            "Yeong Min Jang",
            "Zygmunt J. Haas"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Provisioning of Quality of Service (QoS) is a key issue in any multi-media\nsystem. However, in wireless systems, supporting QoS requirements of different\ntraffic types is more challenging due to the need to minimize two performance\nmetrics - the probability of dropping a handover call and the probability of\nblocking a new call. Since QoS requirements are not as stringent for\nnon-real-time traffic types, as opposed to real-time traffic, more calls can be\naccommodated by releasing some bandwidth from the already admitted\nnon-real-time traffic calls. If we require that such a released bandwidth to\naccept a handover call ought to be larger than the bandwidth to accept a new\ncall, then the resulting probability of dropping a handover call will be\nsmaller than the probability of blocking a new call. In this paper we propose\nan efficient Call Admission Control (CAC) that relies on adaptive multi-level\nbandwidth-allocation scheme for non-real-time calls. The scheme allows\nreduction of the call dropping probability along with increase of the bandwidth\nutilization. The numerical results show that the proposed scheme is capable of\nattaining negligible handover call dropping probability without sacrificing\nbandwidth utilization.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.3630v1"
    },
    {
        "title": "Block Based Medical Image Watermarking Technique for Tamper Detection\n  and Recovery",
        "authors": [
            "Eswaraiah Rayachoti",
            "Sreenivasa Reddy Edara"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  In this paper, we propose a novel fragile block based medical image\nwatermarking technique for embedding data of patient into medical image,\nverifying the integrity of ROI (Region of Interest), detecting the tampered\nblocks inside ROI and recovering original ROI with less size authentication and\nrecovery data and with simple mathematical calculations. In the proposed\nmethod, the medical image is divided into three regions called ROI, RONI\n(Region of Non Interest) and border pixels. Later, authentication data of ROI\nand Electronic Patient Record (EPR) are compressed using Run Length Encoding\n(RLE) technique and then embedded into ROI. Recovery information of ROI is\nembedded inside RONI and information of ROI is embedded inside border pixels.\nResults of experiments conducted on several medical images reveal that proposed\nmethod produces high quality watermarked medical images, identifies tampered\nareas inside ROI of watermarked medical images and recovers the original ROI.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.6143v1"
    },
    {
        "title": "Crowdsourced Live Streaming over the Cloud",
        "authors": [
            "Fei Chen",
            "Cong Zhang",
            "Feng Wang",
            "Jiangchuan Liu"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Empowered by today's rich tools for media generation and distribution, and\nthe convenient Internet access, crowdsourced streaming generalizes the\nsingle-source streaming paradigm by including massive contributors for a video\nchannel. It calls a joint optimization along the path from crowdsourcers,\nthrough streaming servers, to the end-users to minimize the overall latency.\nThe dynamics of the video sources, together with the globalized request demands\nand the high computation demand from each sourcer, make crowdsourced live\nstreaming challenging even with powerful support from modern cloud computing.\nIn this paper, we present a generic framework that facilitates a cost-effective\ncloud service for crowdsourced live streaming. Through adaptively leasing, the\ncloud servers can be provisioned in a fine granularity to accommodate\ngeo-distributed video crowdsourcers. We present an optimal solution to deal\nwith service migration among cloud instances of diverse lease prices. It also\naddresses the location impact to the streaming quality. To understand the\nperformance of the proposed strategies in the realworld, we have built a\nprototype system running over the planetlab and the Amazon/Microsoft Cloud. Our\nextensive experiments demonstrate that the effectiveness of our solution in\nterms of deployment cost and streaming quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.06314v1"
    },
    {
        "title": "A Secure Cyclic Steganographic Technique for Color Images using\n  Randomization",
        "authors": [
            "Khan Muhammad",
            "Jamil Ahmad",
            "Naeem Ur Rehman",
            "Zahoor Jan",
            "Rashid Jalal Qureshi"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Information Security is a major concern in today's modern era. Almost all the\ncommunicating bodies want the security, confidentiality and integrity of their\npersonal data. But this security goal cannot be achieved easily when we are\nusing an open network like Internet. Steganography provides one of the best\nsolutions to this problem. This paper represents a new Cyclic Steganographic T\nechnique (CST) based on Least Significant Bit (LSB) for true color (RGB)\nimages. The proposed method hides the secret data in the LSBs of cover image\npixels in a randomized cyclic manner. The proposed technique is evaluated using\nboth subjective and objective analysis using histograms changeability, Peak\nSignal-to-Noise Ratio (PSNR) and Mean Square Error (MSE). Experimentally it is\nfound that the proposed method gives promising results in terms of security,\nimperceptibility and robustness as compared to some existent methods and\nvindicates this new algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.07808v1"
    },
    {
        "title": "Hybrid coding of visual content and local image features",
        "authors": [
            "Luca Baroffio",
            "Matteo Cesana",
            "Alessandro Redondi",
            "Marco Tagliasacchi",
            "Stefano Tubaro"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Distributed visual analysis applications, such as mobile visual search or\nVisual Sensor Networks (VSNs) require the transmission of visual content on a\nbandwidth-limited network, from a peripheral node to a processing unit.\nTraditionally, a Compress-Then-Analyze approach has been pursued, in which\nsensing nodes acquire and encode the pixel-level representation of the visual\ncontent, that is subsequently transmitted to a sink node in order to be\nprocessed. This approach might not represent the most effective solution, since\nseveral analysis applications leverage a compact representation of the content,\nthus resulting in an inefficient usage of network resources. Furthermore,\ncoding artifacts might significantly impact the accuracy of the visual task at\nhand. To tackle such limitations, an orthogonal approach named\nAnalyze-Then-Compress has been proposed. According to such a paradigm, sensing\nnodes are responsible for the extraction of visual features, that are encoded\nand transmitted to a sink node for further processing. In spite of improved\ntask efficiency, such paradigm implies the central processing node not being\nable to reconstruct a pixel-level representation of the visual content. In this\npaper we propose an effective compromise between the two paradigms, namely\nHybrid-Analyze-Then-Compress (HATC) that aims at jointly encoding visual\ncontent and local image features. Furthermore, we show how a target tradeoff\nbetween image quality and task accuracy might be achieved by accurately\nallocating the bitrate to either visual content or local features.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.07828v1"
    },
    {
        "title": "Coding local and global binary visual features extracted from video\n  sequences",
        "authors": [
            "Luca Baroffio",
            "Antonio Canclini",
            "Matteo Cesana",
            "Alessandro Redondi",
            "Marco Tagliasacchi",
            "Stefano Tubaro"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Binary local features represent an effective alternative to real-valued\ndescriptors, leading to comparable results for many visual analysis tasks,\nwhile being characterized by significantly lower computational complexity and\nmemory requirements. When dealing with large collections, a more compact\nrepresentation based on global features is often preferred, which can be\nobtained from local features by means of, e.g., the Bag-of-Visual-Word (BoVW)\nmodel. Several applications, including for example visual sensor networks and\nmobile augmented reality, require visual features to be transmitted over a\nbandwidth-limited network, thus calling for coding techniques that aim at\nreducing the required bit budget, while attaining a target level of efficiency.\nIn this paper we investigate a coding scheme tailored to both local and global\nbinary features, which aims at exploiting both spatial and temporal redundancy\nby means of intra- and inter-frame coding. In this respect, the proposed coding\nscheme can be conveniently adopted to support the Analyze-Then-Compress (ATC)\nparadigm. That is, visual features are extracted from the acquired content,\nencoded at remote nodes, and finally transmitted to a central controller that\nperforms visual analysis. This is in contrast with the traditional approach, in\nwhich visual content is acquired at a node, compressed and then sent to a\ncentral unit for further processing, according to the Compress-Then-Analyze\n(CTA) paradigm. In this paper we experimentally compare ATC and CTA by means of\nrate-efficiency curves in the context of two different visual analysis tasks:\nhomography estimation and content-based retrieval. Our results show that the\nnovel ATC paradigm based on the proposed coding primitives can be competitive\nwith CTA, especially in bandwidth limited scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.07939v1"
    },
    {
        "title": "Macroblock Classification Method for Video Applications Involving\n  Motions",
        "authors": [
            "Weiyao Lin",
            "Ming-Ting Sun",
            "Hongxiang Li",
            "Zhenzhong Chen",
            "Wei Li",
            "Bing Zhou"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  In this paper, a macroblock classification method is proposed for various\nvideo processing applications involving motions. Based on the analysis of the\nMotion Vector field in the compressed video, we propose to classify Macroblocks\nof each video frame into different classes and use this class information to\ndescribe the frame content. We demonstrate that this low-computation-complexity\nmethod can efficiently catch the characteristics of the frame. Based on the\nproposed macroblock classification, we further propose algorithms for different\nvideo processing applications, including shot change detection, motion\ndiscontinuity detection, and outlier rejection for global motion estimation.\nExperimental results demonstrate that the methods based on the proposed\napproach can work effectively on these applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00087v1"
    },
    {
        "title": "A Survey On Video Forgery Detection",
        "authors": [
            "Sowmya K. N.",
            "H. R. Chennamma"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The Digital Forgeries though not visibly identifiable to human perception it\nmay alter or meddle with underlying natural statistics of digital content.\nTampering involves fiddling with video content in order to cause damage or make\nunauthorized alteration/modification. Tampering detection in video is\ncumbersome compared to image when considering the properties of the video.\nTampering impacts need to be studied and the applied technique/method is used\nto establish the factual information for legal course in judiciary. In this\npaper we give an overview of the prior literature and challenges involved in\nvideo forgery detection where passive approach is found.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00843v1"
    },
    {
        "title": "YFCC100M: The New Data in Multimedia Research",
        "authors": [
            "Bart Thomee",
            "David A. Shamma",
            "Gerald Friedland",
            "Benjamin Elizalde",
            "Karl Ni",
            "Douglas Poland",
            "Damian Borth",
            "Li-Jia Li"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M),\nthe largest public multimedia collection that has ever been released. The\ndataset contains a total of 100 million media objects, of which approximately\n99.2 million are photos and 0.8 million are videos, all of which carry a\nCreative Commons license. Each media object in the dataset is represented by\nseveral pieces of metadata, e.g. Flickr identifier, owner name, camera, title,\ntags, geo, media source. The collection provides a comprehensive snapshot of\nhow photos and videos were taken, described, and shared over the years, from\nthe inception of Flickr in 2004 until early 2014. In this article we explain\nthe rationale behind its creation, as well as the implications the dataset has\nfor science, research, engineering, and development. We further present several\nnew challenges in multimedia research that can now be expanded upon with our\ndataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.01817v2"
    },
    {
        "title": "The YLI-MED Corpus: Characteristics, Procedures, and Plans",
        "authors": [
            "Julia Bernd",
            "Damian Borth",
            "Benjamin Elizalde",
            "Gerald Friedland",
            "Heather Gallagher",
            "Luke Gottlieb",
            "Adam Janin",
            "Sara Karabashlieva",
            "Jocelyn Takahashi",
            "Jennifer Won"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The YLI Multimedia Event Detection corpus is a public-domain index of videos\nwith annotations and computed features, specialized for research in multimedia\nevent detection (MED), i.e., automatically identifying what's happening in a\nvideo by analyzing the audio and visual content. The videos indexed in the\nYLI-MED corpus are a subset of the larger YLI feature corpus, which is being\ndeveloped by the International Computer Science Institute and Lawrence\nLivermore National Laboratory based on the Yahoo Flickr Creative Commons 100\nMillion (YFCC100M) dataset. The videos in YLI-MED are categorized as depicting\none of ten target events, or no target event, and are annotated for additional\nattributes like language spoken and whether the video has a musical score. The\nannotations also include degree of annotator agreement and average annotator\nconfidence scores for the event categorization of each video. Version 1.0 of\nYLI-MED includes 1823 \"positive\" videos that depict the target events and\n48,138 \"negative\" videos, as well as 177 supplementary videos that are similar\nto event videos but are not positive examples. Our goal in producing YLI-MED is\nto be as open about our data and procedures as possible. This report describes\nthe procedures used to collect the corpus; gives detailed descriptive\nstatistics about the corpus makeup (and how video attributes affected\nannotators' judgments); discusses possible biases in the corpus introduced by\nour procedural choices and compares it with the most similar existing dataset,\nTRECVID MED's HAVIC corpus; and gives an overview of our future plans for\nexpanding the annotation effort.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.04250v1"
    },
    {
        "title": "A Low-throughput Wavelet-based Steganography Audio Scheme",
        "authors": [
            "P. Carrion",
            "H. M. de Oliveira",
            "R. M. Campello de Souza"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This paper presents the preliminary of a novel scheme of steganography, and\nintroduces the idea of combining two secret keys in the operation. The first\nsecret key encrypts the text using a standard cryptographic scheme (e.g. IDEA,\nSAFER+, etc.) prior to the wavelet audio decomposition. The way in which the\ncipher text is embedded in the file requires another key, namely a stego-key,\nwhich is associated with features of the audio wavelet analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.07551v1"
    },
    {
        "title": "On the Security of a Revised Fragile Watermarking Scheme",
        "authors": [
            "Daniel Caragata"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This paper analyzes a revised fragile watermarking scheme proposed by Botta\net al. which was developed as a revision of the watermarking scheme previously\nproposed by Rawat et al. A new attack is presented that allows an attacker to\napply a valid watermark on tampered images, therefore circumventing the\nprotection that the watermarking scheme under study was supposed to offer.\nFurthermore, the presented attack has very low computational and memory\nrequirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.05226v1"
    },
    {
        "title": "Deviation Based Pooling Strategies For Full Reference Image Quality\n  Assessment",
        "authors": [
            "Hossein Ziaei Nafchi",
            "Rachid Hedjam",
            "Atena Shahkolaei",
            "Mohamed Cheriet"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The state-of-the-art pooling strategies for perceptual image quality\nassessment (IQA) are based on the mean and the weighted mean. They are robust\npooling strategies which usually provide a moderate to high performance for\ndifferent IQAs. Recently, standard deviation (SD) pooling was also proposed.\nAlthough, this deviation pooling provides a very high performance for a few\nIQAs, its performance is lower than mean poolings for many other IQAs. In this\npaper, we propose to use the mean absolute deviation (MAD) and show that it is\na more robust and accurate pooling strategy for a wider range of IQAs. In fact,\nMAD pooling has the advantages of both mean pooling and SD pooling. The joint\ncomputation and use of the MAD and SD pooling strategies is also considered in\nthis paper. Experimental results provide useful information on the choice of\nthe proper deviation pooling strategy for different IQA models.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.06786v2"
    },
    {
        "title": "Micro protocol engineering for unstructured carriers: On the embedding\n  of steganographic control protocols into audio transmissions",
        "authors": [
            "Matthias Naumann",
            "Steffen Wendzel",
            "Wojciech Mazurczyk",
            "Jörg Keller"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Network steganography conceals the transfer of sensitive information within\nunobtrusive data in computer networks. So-called micro protocols are\ncommunication protocols placed within the payload of a network steganographic\ntransfer. They enrich this transfer with features such as reliability, dynamic\noverlay routing, or performance optimization --- just to mention a few. We\npresent different design approaches for the embedding of hidden channels with\nmicro protocols in digitized audio signals under consideration of different\nrequirements. On the basis of experimental results, our design approaches are\ncompared, and introduced into a protocol engineering approach for micro\nprotocols.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.07757v1"
    },
    {
        "title": "Joint Data Scheduling and FEC Coding for Multihomed Wireless Video\n  Delivery",
        "authors": [
            "Jasmin Fantel",
            "Yan Gao"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This paper studies the problem of mobile video delivery in heterogenous\nwireless networks from a server to multihomed device. Most existing works only\nconsider delivering video streaming on single path which bandwidth is limited\ncausing ultimate video transmission rate. To solve this live video streaming\ntransmission bottleneck problem, we propose a novel solution named Joint Data\nAllocation and Fountain Coding (JDAFC) method that contain below characters:\n(1) path selection, (2) dynamic data allocation, and (3) fountain coding. We\nevaluate the performance of JDAFC by simulation experiments using Exata and\nJVSM and compare it with some reference solutions. Experimental results\nrepresent that JDAFC outperforms the competing solutions in improving the video\npeak signal-to-noise ratio as well as reducing the end-to-end delay.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.05174v1"
    },
    {
        "title": "Mobile Multi-View Object Image Search",
        "authors": [
            "Fatih Calisir",
            "Muhammet Bastan",
            "Ozgur Ulusoy",
            "Ugur Gudukbay"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  High user interaction capability of mobile devices can help improve the\naccuracy of mobile visual search systems. At query time, it is possible to\ncapture multiple views of an object from different viewing angles and at\ndifferent scales with the mobile device camera to obtain richer information\nabout the object compared to a single view and hence return more accurate\nresults. Motivated by this, we developed a mobile multi-view object image\nsearch system, using a client-server architecture. Multi-view images of objects\nacquired by the mobile clients are processed and local features are sent to the\nserver, which combines the query image representations with early/late fusion\nmethods based on bag-of-visual-words and sends back the query results. We\nperformed a comprehensive analysis of early and late fusion approaches using\nvarious similarity functions, on an existing single view and a new multi-view\nobject image database. The experimental results show that multi-view search\nprovides significantly better retrieval accuracy compared to single view\nsearch.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.08861v2"
    },
    {
        "title": "Estimating snow cover from publicly available images",
        "authors": [
            "Roman Fedorov",
            "Alessandro Camerada",
            "Piero Fraternali",
            "Marco Tagliasacchi"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  In this paper we study the problem of estimating snow cover in mountainous\nregions, that is, the spatial extent of the earth surface covered by snow. We\nargue that publicly available visual content, in the form of user generated\nphotographs and image feeds from outdoor webcams, can both be leveraged as\nadditional measurement sources, complementing existing ground, satellite and\nairborne sensor data. To this end, we describe two content acquisition and\nprocessing pipelines that are tailored to such sources, addressing the specific\nchallenges posed by each of them, e.g., identifying the mountain peaks,\nfiltering out images taken in bad weather conditions, handling varying\nillumination conditions. The final outcome is summarized in a snow cover index,\nwhich indicates for a specific mountain and day of the year, the fraction of\nvisible area covered by snow, possibly at different elevations. We created a\nmanually labelled dataset to assess the accuracy of the image snow covered area\nestimation, achieving 90.0% precision at 91.1% recall. In addition, we show\nthat seasonal trends related to air temperature are captured by the snow cover\nindex.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.01055v1"
    },
    {
        "title": "\"The Good, The Bad And The Ugly\": Evaluation of Wi-Fi Steganography",
        "authors": [
            "Krzysztof Szczypiorski",
            "Artur Janicki",
            "Steffen Wendzel"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  In this paper we propose a new method for the evaluation of network\nsteganography algorithms based on the new concept of \"the moving observer\". We\nconsidered three levels of undetectability named: \"good\", \"bad\", and \"ugly\". To\nillustrate this method we chose Wi-Fi steganography as a solid family of\ninformation hiding protocols. We present the state of the art in this area\ncovering well-known hiding techniques for 802.11 networks. \"The moving\nobserver\" approach could help not only in the evaluation of steganographic\nalgorithms, but also might be a starting point for a new detection system of\nnetwork steganography. The concept of a new detection system, called MoveSteg,\nis explained in detail.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.04978v2"
    },
    {
        "title": "Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual\n  Sentiment Prediction",
        "authors": [
            "Victor Campos",
            "Amaia Salvador",
            "Brendan Jou",
            "Xavier Giró-i-Nieto"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Visual media are powerful means of expressing emotions and sentiments. The\nconstant generation of new content in social networks highlights the need of\nautomated visual sentiment analysis tools. While Convolutional Neural Networks\n(CNNs) have established a new state-of-the-art in several vision problems,\ntheir application to the task of sentiment analysis is mostly unexplored and\nthere are few studies regarding how to design CNNs for this purpose. In this\nwork, we study the suitability of fine-tuning a CNN for visual sentiment\nprediction as well as explore performance boosting techniques within this deep\nlearning setting. Finally, we provide a deep-dive analysis into a benchmark,\nstate-of-the-art network architecture to gain insight about how to design\npatterns for CNNs on the task of visual sentiment prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05056v2"
    },
    {
        "title": "An Extension of Interactive Scores for Multimedia Scenarios with\n  Temporal Relations for Micro and Macro Controls",
        "authors": [
            "Mauricio Toro",
            "Myriam Desainte-Catherine",
            "Julien Castet"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Software to design multimedia scenarios is usually based either on a fixed\ntimeline or on cue lists, but both models are unrelated temporally. On the\ncontrary, the formalism of interactive scores can describe multimedia scenarios\nwith flexible and fixed temporal relations among the objects of the scenario,\nbut cannot express neither temporal relations for micro controls nor signal\nprocessing. We extend interactive scores with such relations and with sound\nprocessing. We show some applications and we describe how they can be\nimplemented in Pure Data. Our implementation has low average relative jitter\neven under high cpu load.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.03090v1"
    },
    {
        "title": "Secure Image Steganography using Cryptography and Image Transposition",
        "authors": [
            "Khan Muhammad",
            "Jamil Ahmad",
            "Muhammad Sajjad",
            "Muhammad Zubair"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Information security is one of the most challenging problems in today's\ntechnological world. In order to secure the transmission of secret data over\nthe public network (Internet), various schemes have been presented over the\nlast decade. Steganography combined with cryptography, can be one of the best\nchoices for solving this problem. This paper proposes a new steganographic\nmethod based on gray-level modification for true colour images using image\ntransposition, secret key and cryptography. Both the secret key and secret\ninformation are initially encrypted using multiple encryption algorithms\n(bitxor operation, bits shuffling, and stego key-based encryption); these are,\nsubsequently, hidden in the host image pixels. In addition, the input image is\ntransposed before data hiding. Image transposition, bits shuffling, bitxoring,\nstego key-based encryption, and gray-level modification introduce five\ndifferent security levels to the proposed scheme, making the data recovery\nextremely difficult for attackers. The proposed technique is evaluated by\nobjective analysis using various image quality assessment metrics, producing\npromising results in terms of imperceptibility and security. Moreover, the high\nquality stego images and its minimal histogram changeability, also validate the\neffectiveness of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.04413v1"
    },
    {
        "title": "High-Quality, Low-Delay Music Coding in the Opus Codec",
        "authors": [
            "Jean-Marc Valin",
            "Gregory Maxwell",
            "Timothy B. Terriberry",
            "Koen Vos"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The IETF recently standardized the Opus codec as RFC6716. Opus targets a wide\nrange of real-time Internet applications by combining a linear prediction coder\nwith a transform coder. We describe the transform coder, with particular\nattention to the psychoacoustic knowledge built into the format. The result\nout-performs existing audio codecs that do not operate under real-time\nconstraints.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.04845v1"
    },
    {
        "title": "A Full-Bandwidth Audio Codec With Low Complexity And Very Low Delay",
        "authors": [
            "Jean-Marc Valin",
            "Timothy B. Terriberry",
            "Gregory Maxwell"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  We propose an audio codec that addresses the low-delay requirements of some\napplications such as network music performance. The codec is based on the\nmodified discrete cosine transform (MDCT) with very short frames and uses\ngain-shape quantization to preserve the spectral envelope. The short frame\nsizes required for low delay typically hinder the performance of transform\ncodecs. However, at 96 kbit/s and with only 4 ms algorithmic delay, the\nproposed codec out-performs the ULD codec operating at the same rate. The total\ncomplexity of the codec is small, at only 17 WMOPS for real-time operation at\n48 kHz.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05311v1"
    },
    {
        "title": "Steganography -- A Game of Hide and Seek in Information Communication",
        "authors": [
            "Sanjeeb Kumar Behera",
            "Minati Mishra"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  With the growth of communication over computer networks, how to maintain the\nconfidentiality and security of transmitted information have become some of the\nimportant issues. In order to transfer data securely to the destination without\nunwanted disclosure or damage, nature inspired hide and seek tricks such as,\ncryptography and Steganography are heavily in use. Just like the Chameleon and\nmany other bio-species those change their body color and hide themselves in the\nbackground in order to protect them from external attacks, Cryptography and\nSteganography are techniques those are used to encrypt and hide the secret data\ninside other media to ensure data security. This paper discusses the concept of\na simple spatial domain LSB Steganography that encrypts the secrets using\nFibonacci- Lucas transformation, before hiding, for better security.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.00493v1"
    },
    {
        "title": "Trends toward real-time network data steganography",
        "authors": [
            "James Collins",
            "Sos Agaian"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Network steganography has been a well-known covert data channeling method for\nover three decades. The basic set of techniques and implementation tools have\nnot changed significantly since their introduction in the early 1980's. In this\npaper, we review the predominant methods of classical network steganography,\ndescribing the detailed operations and resultant challenges involved in\nembedding data in the network transport domain. We also consider the various\ncyber threat vectors of network steganography and point out the major\ndifferences between classical network steganography and the widely known\nend-point multimedia embedding techniques, which focus exclusively on static\ndata modification for data hiding. We then challenge the security community by\nintroducing an entirely new network dat hiding methodology, which we refer to\nas real-time network data steganography. Finally we provide the groundwork for\nthis fundamental change of covert network data embedding by forming a basic\nframework for real-time network data operations that will open the path for\neven further advances in computer network security.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.02778v1"
    },
    {
        "title": "Mainstreaming video annotation software for critical video analysis",
        "authors": [
            "Matthew Martin",
            "James Charlton",
            "Andy M. Connor"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The range of video annotation software currently available is set within\ncommercially specialized professions, distributed via outdated sources or\nthrough online video hosting services. As video content becomes an increasingly\nsignificant tool for analysis, there is a demand for appropriate digital\nannotation techniques that offer equivalent functionality to tools used for\nannotation of text based literature sources. This paper argues for the\nimportance of video annotating as an effective method for research that is as\naccessible as literature annotation is. Video annotation has been shown to\ntrigger higher learning and engagement but research struggles to explain the\nabsence of video annotation in contemporary structures of education practice.\nIn both academic and informal settings the use of video playback as a\nmeaningful tool of analysis is apparent, yet the availability of supplementary\nannotation software is not within obvious grasp or even prevalent in\nstandardized computer software. Practical software tools produced by the\nresearcher have demonstrated effective video annotation in a short development\ntime. With software design programs available for rapid application creation,\nthis paper also highlights the absence of a development community. This paper\nargues that video annotation is an accessible tool, not just for academic\ncontexts, but also for wider practical video analysis applications, potentially\nbecoming a mainstream learning tool. This paper thus presents a practical\nmultimodal public approach to video research that potentially affords a deeper\nanalysis of media content. This is supported by an in-depth consideration of\nthe motivation for undertaking video annotation and a critical analysis of\ncurrently available tools.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.05799v1"
    },
    {
        "title": "Towards Reduced Reference Parametric Models for Estimating Audiovisual\n  Quality in Multimedia Services",
        "authors": [
            "Edip Demirbilek",
            "Jean-Charles Grégoire"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  We have developed reduced reference parametric models for estimating\nperceived quality in audiovisual multimedia services. We have created 144\nunique configurations for audiovisual content including various application and\nnetwork parameters such as bitrates and distortions in terms of bandwidth,\npacket loss rate and jitter. To generate the data needed for model training and\nvalidation we have tasked 24 subjects, in a controlled environment, to rate the\noverall audiovisual quality on the absolute category rating (ACR) 5-level\nquality scale. We have developed models using Random Forest and Neural Network\nbased machine learning methods in order to estimate Mean Opinion Scores (MOS)\nvalues. We have used information retrieved from the packet headers and side\ninformation provided as network parameters for model training. Random Forest\nbased models have performed better in terms of Root Mean Square Error (RMSE)\nand Pearson correlation coefficient. The side information proved to be very\neffective in developing the model. We have found that, while the model\nperformance might be improved by replacing the side information with more\naccurate bit stream level measurements, they are performing well in estimating\nperceived quality in audiovisual multimedia services.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.07211v1"
    },
    {
        "title": "Predictive No-Reference Assessment of Video Quality",
        "authors": [
            "Maria Torres Vega",
            "Decebal Constantin Mocanu",
            "Antonio Liotta"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Among the various means to evaluate the quality of video streams,\nNo-Reference (NR) methods have low computation and may be executed on thin\nclients. Thus, NR algorithms would be perfect candidates in cases of real-time\nquality assessment, automated quality control and, particularly, in adaptive\nmobile streaming. Yet, existing NR approaches are often inaccurate, in\ncomparison to Full-Reference (FR) algorithms, especially under lossy network\nconditions. In this work, we present an NR method that combines machine\nlearning with simple NR metrics to achieve a quality index comparably as\naccurate as the Video Quality Metric (VQM) Full-Reference algorithm. Our method\nis tested in an extensive dataset (960 videos), under lossy network conditions\nand considering nine different machine learning algorithms. Overall, we achieve\nan over 97% correlation with VQM, while allowing real-time assessment of video\nquality of experience in realistic streaming scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.07322v2"
    },
    {
        "title": "Detecting Violence in Video using Subclasses",
        "authors": [
            "Xirong Li",
            "Yujia Huo",
            "Jieping Xu",
            "Qin Jin"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This paper attacks the challenging problem of violence detection in videos.\nDifferent from existing works focusing on combining multi-modal features, we go\none step further by adding and exploiting subclasses visually related to\nviolence. We enrich the MediaEval 2015 violence dataset by \\emph{manually}\nlabeling violence videos with respect to the subclasses. Such fine-grained\nannotations not only help understand what have impeded previous efforts on\nlearning to fuse the multi-modal features, but also enhance the generalization\nability of the learned fusion to novel test data. The new subclass based\nsolution, with AP of 0.303 and P100 of 0.55 on the MediaEval 2015 test set,\noutperforms several state-of-the-art alternatives. Notice that our solution\ndoes not require fine-grained annotations on the test set, so it can be\ndirectly applied on novel and fully unlabeled videos. Interestingly, our study\nshows that motion related features, though being essential part in previous\nsystems, are dispensable.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.08088v1"
    },
    {
        "title": "Advanced Transport Options for the Dynamic Adaptive Streaming over HTTP",
        "authors": [
            "Christian Timmerer",
            "Alan Bertoni"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Multimedia streaming over HTTP is no longer a niche research topic as it has\nentered our daily live. The common assumption is that it is deployed on top of\nthe existing infrastructure utilizing application (HTTP) and transport (TCP)\nlayer protocols as is. Interestingly, standards like MPEG's Dynamic Adaptive\nStreaming over HTTP (DASH) do not mandate the usage of any specific transport\nprotocol allowing for sufficient deployment flexibility which is further\nsupported by emerging developments within both protocol layers. This paper\ninvestigates and evaluates the usage of advanced transport options for the\ndynamic adaptive streaming over HTTP. We utilize a common test setup to\nevaluate HTTP/2.0 and Google's Quick UDP Internet Connections (QUIC) protocol\nin the context of DASH-based services.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00264v1"
    },
    {
        "title": "High Capacity Image Steganography using Adjunctive Numerical\n  Representations with Multiple Bit-Plane Decomposition Methods",
        "authors": [
            "James Collins",
            "Sos Agaian"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  LSB steganography is a one of the most widely used methods for implementing\ncovert data channels in image file exchanges [1][2]. The low computational\ncomplexity and implementation simplicity of the algorithm are significant\nfactors for its popularity with the primary reason being low image distortion.\nMany attempts have been made to increase the embedding capacity of LSB\nalgorithms by expanding into the second or third binary layers of the image\nwhile maintaining a low probability of detection with minimal distortive\neffects [2][3][4]. In this paper, we introduce an advanced technique for\ncovertly embedding data within images using redundant number system\ndecomposition over non-standard digital bit planes. Both grayscale and\nbit-mapped images are equally effective as cover files. It will be shown that\nthis unique steganography method has minimal visual distortive affects while\nalso preserving the cover file statistics, making it less susceptible to most\ngeneral steganography detection algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.02312v1"
    },
    {
        "title": "Bidirectional Long-Short Term Memory for Video Description",
        "authors": [
            "Yi Bin",
            "Yang Yang",
            "Zi Huang",
            "Fumin Shen",
            "Xing Xu",
            "Heng Tao Shen"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Video captioning has been attracting broad research attention in multimedia\ncommunity. However, most existing approaches either ignore temporal information\namong video frames or just employ local contextual temporal knowledge. In this\nwork, we propose a novel video captioning framework, termed as\n\\emph{Bidirectional Long-Short Term Memory} (BiLSTM), which deeply captures\nbidirectional global temporal structure in video. Specifically, we first devise\na joint visual modelling approach to encode video data by combining a forward\nLSTM pass, a backward LSTM pass, together with visual features from\nConvolutional Neural Networks (CNNs). Then, we inject the derived video\nrepresentation into the subsequent language model for initialization. The\nbenefits are in two folds: 1) comprehensively preserving sequential and visual\ninformation; and 2) adaptively learning dense visual features and sparse\nsemantic representations for videos and sentences, respectively. We verify the\neffectiveness of our proposed video captioning framework on a commonly-used\nbenchmark, i.e., Microsoft Video Description (MSVD) corpus, and the\nexperimental results demonstrate that the superiority of the proposed approach\nas compared to several state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.04631v1"
    },
    {
        "title": "N-queens-based algorithm for moving object detection in distributed\n  wireless sensor networks",
        "authors": [
            "Biljana Stojkoska",
            "Danco Davcev",
            "Vladimir Trajkovik"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The main constraint of wireless sensor networks (WSN) in enabling wireless\nimage communication is the high energy requirement, which may exceed even the\nfuture capabilities of battery technologies. In this paper we have shown that\nthis bottleneck can be overcome by developing local in-network image processing\nalgorithm that offers optimal energy consumption. Our algorithm is very\nsuitable for intruder detection applications. Each node is responsible for\nprocessing the image captured by the video sensor, which consists of NxN\nblocks. If an intruder is detected in the monitoring region, the node will\ntransmit the image for further processing. Otherwise, the node takes no action.\nResults provided from our experiments show that our algorithm is better than\nthe traditional moving object detection techniques by a factor of (N/2) in\nterms of energy savings.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.07583v1"
    },
    {
        "title": "De-Hashing: Server-Side Context-Aware Feature Reconstruction for Mobile\n  Visual Search",
        "authors": [
            "Yin-Hsi Kuo",
            "Winston H. Hsu"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Due to the prevalence of mobile devices, mobile search becomes a more\nconvenient way than desktop search. Different from the traditional desktop\nsearch, mobile visual search needs more consideration for the limited resources\non mobile devices (e.g., bandwidth, computing power, and memory consumption).\nThe state-of-the-art approaches show that bag-of-words (BoW) model is robust\nfor image and video retrieval; however, the large vocabulary tree might not be\nable to be loaded on the mobile device. We observe that recent works mainly\nfocus on designing compact feature representations on mobile devices for\nbandwidth-limited network (e.g., 3G) and directly adopt feature matching on\nremote servers (cloud). However, the compact (binary) representation might fail\nto retrieve target objects (images, videos). Based on the hashed binary codes,\nwe propose a de-hashing process that reconstructs BoW by leveraging the\ncomputing power of remote servers. To mitigate the information loss from binary\ncodes, we further utilize contextual information (e.g., GPS) to reconstruct a\ncontext-aware BoW for better retrieval results. Experiment results show that\nthe proposed method can achieve competitive retrieval accuracy as BoW while\nonly transmitting few bits from mobile devices.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.08999v1"
    },
    {
        "title": "Towards Network-Failure-Tolerant Content Delivery for Web Content",
        "authors": [
            "Wen Hu",
            "Zhi Wang",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Popularly used to distribute a variety of multimedia content items in today\nInternet, HTTP-based web content delivery still suffers from various content\ndelivery failures. Hindered by the expensive deployment cost, the conventional\nCDN can not deploy as many edge servers as possible to successfully deliver\ncontent items to all users under these delivery failures. In this paper, we\npropose a joint CDN and peer-assisted web content delivery framework to address\nthe delivery failure problem. Different from conventional peer-assisted\napproaches for web content delivery, which mainly focus on alleviating the CDN\nservers bandwidth load, we study how to use a browser-based peer-assisted\nscheme, namely WebRTC, to resolve content delivery failures. To this end, we\ncarry out large-scale measurement studies on how users access and view\nwebpages. Our measurement results demonstrate the challenges (e.g., peers stay\non a webpage extremely short) that can not be directly solved by conventional\nP2P strategies, and some important webpage viewing patterns. Due to these\nunique characteristics, WebRTC peers open up new possibilities for helping the\nweb content delivery, coming with the problem of how to utilize the dynamic\nresources efficiently. We formulate the peer selection that is the critical\nstrategy in our framework, as an optimization problem, and design a heuristic\nalgorithm based on the measurement insights to solve it. Our simulation\nexperiments driven by the traces from Tencent QZone demonstrate the\neffectiveness of our design: compared with non-peer-assisted strategy and\nrandom peer selection strategy, our design significantly improves the\nsuccessful relay ratio of web content items under network failures, e.g., our\ndesign improves the content download ratio up to 60% even when users located in\na particular region (e.g., city) where none can connect to the regional CDN\nserver.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.01159v1"
    },
    {
        "title": "A Measurement Study of TCP Performance for Chunk Delivery in DASH",
        "authors": [
            "Wen Hu",
            "Zhi Wang",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Dynamic Adaptive Streaming over HTTP (DASH) has emerged as an increasingly\npopular paradigm for video streaming [13], in which a video is segmented into\nmany chunks delivered to users by HTTP request/response over Transmission\nControl Protocol (TCP) con- nections. Therefore, it is intriguing to study the\nperformance of strategies implemented in conventional TCPs, which are not\ndedicated for video streaming, e.g., whether chunks are efficiently delivered\nwhen users per- form interactions with the video players. In this paper, we\nconduct mea- surement studies on users chunk requesting traces in DASH from a\nrep- resentative video streaming provider, to investigate users behaviors in\nDASH, and TCP-connection-level traces from CDN servers, to investi- gate the\nperformance of TCP for DASH. By studying how video chunks are delivered in both\nthe slow start and congestion avoidance phases, our observations have revealed\nthe performance characteristics of TCP for DASH as follows: (1) Request\npatterns in DASH have a great impact on the performance of TCP variations\nincluding cubic; (2) Strategies in conventional TCPs may cause user perceived\nquality degradation in DASH streaming; (3) Potential improvement to TCP\nstrategies for better delivery in DASH can be further explored.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.01172v1"
    },
    {
        "title": "Natural Steganography: cover-source switching for better steganography",
        "authors": [
            "Patrick Bas"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This paper proposes a new steganographic scheme relying on the principle of\ncover-source switching, the key idea being that the embedding should switch\nfrom one cover-source to another. The proposed implementation, called Natural\nSteganography, considers the sensor noise naturally present in the raw images\nand uses the principle that, by the addition of a specific noise the\nsteganographic embedding tries to mimic a change of ISO sensitivity. The\nembedding methodology consists in 1) perturbing the image in the raw domain, 2)\nmodeling the perturbation in the processed domain, 3) embedding the payload in\nthe processed domain. We show that this methodology is easily tractable\nwhenever the processes are known and enables to embed large and undetectable\npayloads. We also show that already used heuristics such as synchronization of\nembedding changes or detectability after rescaling can be respectively\nexplained by operations such as color demosaicing and down-scaling kernels.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.07824v1"
    },
    {
        "title": "PicHunt: Social Media Image Retrieval for Improved Law Enforcement",
        "authors": [
            "Sonal Goel",
            "Niharika Sachdeva",
            "Ponnurangam Kumaraguru",
            "A V Subramanyam",
            "Divam Gupta"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  First responders are increasingly using social media to identify and reduce\ncrime for well-being and safety of the society. Images shared on social media\nhurting religious, political, communal and other sentiments of people, often\ninstigate violence and create law & order situations in society. This results\nin the need for first responders to inspect the spread of such images and users\npropagating them on social media. In this paper, we present a comparison\nbetween different hand-crafted features and a Convolutional Neural Network\n(CNN) model to retrieve similar images, which outperforms state-of-art\nhand-crafted features. We propose an Open-Source-Intelligent (OSINT) real-time\nimage search system, robust to retrieve modified images that allows first\nresponders to analyze the current spread of images, sentiments floating and\ndetails of users propagating such content. The system also aids officials to\nsave time of manually analyzing the content by reducing the search space on an\naverage by 67%.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.00905v2"
    },
    {
        "title": "Media Query Processing For The Internet-of-Things: Coupling Of Device\n  Energy Consumption And Cloud Infrastructure Billing",
        "authors": [
            "Francesco Renna",
            "Joseph Doyle",
            "Vasileios Giotsas",
            "Yiannis Andreopoulos"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Audio/visual recognition and retrieval applications have recently garnered\nsignificant attention within Internet-of-Things (IoT) oriented services, given\nthat video cameras and audio processing chipsets are now ubiquitous even in\nlow-end embedded systems. In the most typical scenario for such services, each\ndevice extracts audio/visual features and compacts them into feature\ndescriptors, which comprise media queries. These queries are uploaded to a\nremote cloud computing service that performs content matching for\nclassification or retrieval applications. Two of the most crucial aspects for\nsuch services are: (i) controlling the device energy consumption when using the\nservice; (ii) reducing the billing cost incurred from the cloud infrastructure\nprovider. In this paper we derive analytic conditions for the optimal coupling\nbetween the device energy consumption and the incurred cloud infrastructure\nbilling. Our framework encapsulates: the energy consumption to produce and\ntransmit audio/visual queries, the billing rates of the cloud infrastructure,\nthe number of devices concurrently connected to the same cloud server, {the\nquery volume constraint of each cluster of devices,} and the statistics of the\nquery data production volume per device. Our analytic results are validated via\na deployment with: (i) the device side comprising compact image descriptors\n(queries) computed on Beaglebone Linux embedded platforms and transmitted to\nAmazon Web Services (AWS) Simple Storage Service; (ii) the cloud side carrying\nout image similarity detection via AWS Elastic Compute Cloud (EC2) instances,\nwith the AWS Auto Scaling being used to control the number of instances\naccording to the demand.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.00925v1"
    },
    {
        "title": "Mining Fashion Outfit Composition Using An End-to-End Deep Learning\n  Approach on Set Data",
        "authors": [
            "Yuncheng Li",
            "LiangLiang Cao",
            "Jiang Zhu",
            "Jiebo Luo"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Composing fashion outfits involves deep understanding of fashion standards\nwhile incorporating creativity for choosing multiple fashion items (e.g.,\nJewelry, Bag, Pants, Dress). In fashion websites, popular or high-quality\nfashion outfits are usually designed by fashion experts and followed by large\naudiences. In this paper, we propose a machine learning system to compose\nfashion outfits automatically. The core of the proposed automatic composition\nsystem is to score fashion outfit candidates based on the appearances and\nmeta-data. We propose to leverage outfit popularity on fashion oriented\nwebsites to supervise the scoring component. The scoring component is a\nmulti-modal multi-instance deep learning system that evaluates instance\naesthetics and set compatibility simultaneously. In order to train and evaluate\nthe proposed composition system, we have collected a large scale fashion outfit\ndataset with 195K outfits and 368K fashion items from Polyvore. Although the\nfashion outfit scoring and composition is rather challenging, we have achieved\nan AUC of 85% for the scoring component, and an accuracy of 77% for a\nconstrained composition task.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.03016v2"
    },
    {
        "title": "Steganalyzer performances in operational contexts",
        "authors": [
            "Yousra A. Fadil",
            "Jean-François Couchot",
            "Raphaël Couturier",
            "Christophe Guyeux"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Steganography and steganalysis are two important branches of the information\nhiding field of research. Steganography methods consist in hiding information\nin such a way that the secret message is undetectable for the uninitiated.\nSteganalyzis encompasses all the techniques that attempt to detect the presence\nof such hidden information. This latter is usually designed by making\nclassifiers able to separate innocent images from steganographied ones\naccording to their differences on well-selected features. We wonder, in this\narticle whether it is possible to construct a kind of universal steganalyzer\nwithout any knowledge regarding the steganographier side. The effects on the\nclassification score of a modification of either parameters or methods between\nthe learning and testing stages are then evaluated, while the possibility to\nimprove the separation score by merging many methods during learning stage is\ndeeper investigated.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05850v1"
    },
    {
        "title": "Automatic Synchronization of Multi-User Photo Galleries",
        "authors": [
            "E. Sansone",
            "K. Apostolidis",
            "N. Conci",
            "G. Boato",
            "V. Mezaris",
            "F. G. B. De Natale"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  In this paper we address the issue of photo galleries synchronization, where\npictures related to the same event are collected by different users. Existing\nsolutions to address the problem are usually based on unrealistic assumptions,\nlike time consistency across photo galleries, and often heavily rely on\nheuristics, limiting therefore the applicability to real-world scenarios. We\npropose a solution that achieves better generalization performance for the\nsynchronization task compared to the available literature. The method is\ncharacterized by three stages: at first, deep convolutional neural network\nfeatures are used to assess the visual similarity among the photos; then, pairs\nof similar photos are detected across different galleries and used to construct\na graph; eventually, a probabilistic graphical model is used to estimate the\ntemporal offset of each pair of galleries, by traversing the minimum spanning\ntree extracted from this graph. The experimental evaluation is conducted on\nfour publicly available datasets covering different types of events,\ndemonstrating the strength of our proposed method. A thorough discussion of the\nobtained results is provided for a critical assessment of the quality in\nsynchronization.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.06770v2"
    },
    {
        "title": "FPGA implementation of the procedures for video quality assessment",
        "authors": [
            "Maciej Wielgosz",
            "Michał Karwatowski",
            "Marcin Pietroń",
            "Kazimierz Wiatr"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Video resolutions used in variety of media are constantly rising. While\nmanufacturers struggle to perfect their screens it is also important to ensure\nhigh quality of displayed image. Overall quality can be measured using Mean\nOpinion Score (MOS). Video quality can be affected by miscellaneous artifacts,\nappearing at every stage of video creation and transmission. In this paper, we\npresent a solution to calculate four distinct video quality metrics that can be\napplied to a real time video quality assessment system. Our assessment module\nis capable of processing 8K resolution in real time set at the level of 30\nframes per second. Throughput of 2.19 GB/s surpasses performance of pure\nsoftware solutions. To concentrate on architectural optimization, the module\nwas created using high level language.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.06109v3"
    },
    {
        "title": "Deep Quality: A Deep No-reference Quality Assessment System",
        "authors": [
            "Prajna Paramita Dash",
            "Akshaya Mishra",
            "Alexander Wong"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Image quality assessment (IQA) continues to garner great interest in the\nresearch community, particularly given the tremendous rise in consumer video\ncapture and streaming. Despite significant research effort in IQA in the past\nfew decades, the area of no-reference image quality assessment remains a great\nchallenge and is largely unsolved. In this paper, we propose a novel\nno-reference image quality assessment system called Deep Quality, which\nleverages the power of deep learning to model the complex relationship between\nvisual content and the perceived quality. Deep Quality consists of a novel\nmulti-scale deep convolutional neural network, trained to learn to assess image\nquality based on training samples consisting of different distortions and\ndegradations such as blur, Gaussian noise, and compression artifacts.\nPreliminary results using the CSIQ benchmark image quality dataset showed that\nDeep Quality was able to achieve strong quality prediction performance (89%\npatch-level and 98% image-level prediction accuracy), being able to achieve\nsimilar performance as full-reference IQA methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.07170v1"
    },
    {
        "title": "MoveSteg: A Method of Network Steganography Detection",
        "authors": [
            "Krzysztof Szczypiorski",
            "Tomasz Tyl"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This article presents a new method for detecting a source point of time based\nnetwork steganography - MoveSteg. A steganography carrier could be an example\nof multimedia stream made with packets. These packets are then delayed\nintentionally to send hidden information using time based steganography\nmethods. The presented analysis describes a method that allows finding the\nsource of steganography stream in network that is under our management.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.01955v1"
    },
    {
        "title": "Steganography between Silence Intervals of Audio in Video Content Using\n  Chaotic Maps",
        "authors": [
            "Muhammad Fahad Khan",
            "Faisal Baig",
            "Saira Beg"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Steganography is the art of hiding data, in such a way that it is\nundetectable under traffic-pattern analysis and the data hidden is only known\nto the receiver and the sender. In this paper new method of text steganography\nover the silence interval of audio in a video file, is presented. In the\nproposed method first the audio signal is extracted from the video. After doing\naudio enhancement, the data on the audio signal is steganographed using new\ntechnique and then audio signal is rewritten in video file again.\nhttp://www.learnrnd.com/All_latest_research_findings.php\n  To enhance the security level we apply chaotic maps on arbitrary text.\nFurthermore, the algorithm in this paper, gives a technique which states that\nundetectable stegotext and cover-text has same probability distribution and no\nstatistical test can detect the presence of the hidden message.\nhttp://www.learnrnd.com/detail.php?id=Biohack_Eyes_through_Chlorin_e6_eye_drop_:Stanford_University_Research\n  Moreover, hidden message does not affect the transmission rate of video file\nat all.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.04346v1"
    },
    {
        "title": "A Classification Engine for Image Ballistics of Social Data",
        "authors": [
            "Oliver Giudice",
            "Antonino Paratore",
            "Marco Moltisanti",
            "Sebastiano Battiato"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Image Forensics has already achieved great results for the source camera\nidentification task on images. Standard approaches for data coming from Social\nNetwork Platforms cannot be applied due to different processes involved (e.g.,\nscaling, compression, etc.). Over 1 billion images are shared each day on the\nInternet and obtaining information about their history from the moment they\nwere acquired could be exploited for investigation purposes. In this paper, a\nclassification engine for the reconstruction of the history of an image, is\npresented. Specifically, exploiting K-NN and decision trees classifiers and\na-priori knowledge acquired through image analysis, we propose an automatic\napproach that can understand which Social Network Platform has processed an\nimage and the software application used to perform the image upload. The engine\nmakes use of proper alterations introduced by each platform as features.\nResults, in terms of global accuracy on a dataset of 2720 images, confirm the\neffectiveness of the proposed strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.06347v1"
    },
    {
        "title": "A Novel Boundary Matching Algorithm for Video Temporal Error Concealment",
        "authors": [
            "Seyed Mojtaba Marvasti-Zadeh",
            "Hossein Ghanei-Yakhdan",
            "Shohreh Kasaei"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  With the fast growth of communication networks, the video data transmission\nfrom these networks is extremely vulnerable. Error concealment is a technique\nto estimate the damaged data by employing the correctly received data at the\ndecoder. In this paper, an efficient boundary matching algorithm for estimating\ndamaged motion vectors (MVs) is proposed. The proposed algorithm performs error\nconcealment for each damaged macro block (MB) according to the list of\nidentified priority of each frame. It then uses a classic boundary matching\ncriterion or the proposed boundary matching criterion adaptively to identify\nmatching distortion in each boundary of candidate MB. Finally, the candidate MV\nwith minimum distortion is selected as an MV of damaged MB and the list of\npriorities is updated. Experimental results show that the proposed algorithm\nimproves both objective and subjective qualities of reconstructed frames\nwithout any significant increase in computational cost. The PSNR for test\nsequences in some frames is increased about 4.7, 4.5, and 4.4 dB compared to\nthe classic boundary matching, directional boundary matching, and directional\ntemporal boundary matching algorithm, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.07753v1"
    },
    {
        "title": "Identification of image source using serialnumber-based watermarking\n  under Compressive Sensing conditions",
        "authors": [
            "Andjela Draganic",
            "Milan Maric",
            "Irena Orovic",
            "Srdjan Stankovic"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Although the protection of ownership and the prevention of unauthorized\nmanipulation of digital images becomes an important concern, there is also a\nbig issue of image source origin authentication. This paper proposes a\nprocedure for the identification of the image source and content by using the\nPublic Key Cryptography Signature (PKCS). The procedure is based on the PKCS\nwatermarking of the images captured with numerous automatic observing cameras\nin the Trap View cloud system. Watermark is created based on 32-bit PKCS serial\nnumber and embedded into the captured image. Watermark detection on the\nreceiver side extracts the serial number and indicates the camera which\ncaptured the image by comparing the original and the extracted serial numbers.\nThe watermarking procedure is designed to provide robustness to image\noptimization based on the Compressive Sensing approach. Also, the procedure is\ntested under various attacks and shows successful identification of ownership.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00383v1"
    },
    {
        "title": "Unsupervised Steganalysis Based on Artificial Training Sets",
        "authors": [
            "Daniel Lerch-Hostalot",
            "David Megías"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In this paper, an unsupervised steganalysis method that combines artificial\ntraining setsand supervised classification is proposed. We provide a formal\nframework for unsupervisedclassification of stego and cover images in the\ntypical situation of targeted steganalysis (i.e.,for a known algorithm and\napproximate embedding bit rate). We also present a completeset of experiments\nusing 1) eight different image databases, 2) image features based on\nRichModels, and 3) three different embedding algorithms: Least Significant Bit\n(LSB) matching,Highly undetectable steganography (HUGO) and Wavelet Obtained\nWeights (WOW). Weshow that the experimental results outperform previous methods\nbased on Rich Models inthe majority of the tested cases. At the same time, the\nproposed approach bypasses theproblem of Cover Source Mismatch -when the\nembedding algorithm and bit rate are known-, since it removes the need of a\ntraining database when we have a large enough testing set.Furthermore, we\nprovide a generic proof of the proposed framework in the machine\nlearningcontext. Hence, the results of this paper could be extended to other\nclassification problemssimilar to steganalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00796v1"
    },
    {
        "title": "Depth Estimation using Modified Cost Function for Occlusion Handling",
        "authors": [
            "Krzysztof Wegner",
            "Olgierd Stankiewicz",
            "Marek Domanski"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The paper presents a novel approach to occlusion handling problem in depth\nestimation using three views. A solution based on modification of similarity\ncost function is proposed. During the depth estimation via optimization\nalgorithms like Graph Cut similarity metric is constantly updated so that only\nnon-occluded fragments in side views are considered. At each iteration of the\nalgorithm non-occluded fragments are detected based on side view virtual depth\nmaps synthesized from the best currently estimated depth map of the center\nview. Then similarity metric is updated for correspondence search only in\nnon-occluded regions of the side views. The experimental results, conducted on\nwell-known 3D video test sequences, have proved that the depth maps estimated\nwith the proposed approach provide about 1.25 dB virtual view quality\nimprovement in comparison to the virtual view synthesized based on depth maps\ngenerated by the state-of-the-art MPEG Depth Estimation Reference Software.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00919v2"
    },
    {
        "title": "Refining Image Categorization by Exploiting Web Images and General\n  Corpus",
        "authors": [
            "Yazhou Yao",
            "Jian Zhang",
            "Fumin Shen",
            "Xiansheng Hua",
            "Wankou Yang",
            "Zhenmin Tang"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Studies show that refining real-world categories into semantic subcategories\ncontributes to better image modeling and classification. Previous image\nsub-categorization work relying on labeled images and WordNet's hierarchy is\nnot only labor-intensive, but also restricted to classify images into NOUN\nsubcategories. To tackle these problems, in this work, we exploit general\ncorpus information to automatically select and subsequently classify web images\ninto semantic rich (sub-)categories. The following two major challenges are\nwell studied: 1) noise in the labels of subcategories derived from the general\ncorpus; 2) noise in the labels of images retrieved from the web. Specifically,\nwe first obtain the semantic refinement subcategories from the text perspective\nand remove the noise by the relevance-based approach. To suppress the search\nerror induced noisy images, we then formulate image selection and classifier\nlearning as a multi-class multi-instance learning problem and propose to solve\nthe employed problem by the cutting-plane algorithm. The experiments show\nsignificant performance gains by using the generated data of our way on both\nimage categorization and sub-categorization tasks. The proposed approach also\nconsistently outperforms existing weakly supervised and web-supervised\napproaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.05451v1"
    },
    {
        "title": "Medical Image Watermarking using 2D-DWT with Enhanced security and\n  capacity",
        "authors": [
            "Ali Sharifara",
            "Amir Ghaderi"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Teleradiology enables medical images to be transferred over the computer\nnetworks for many purposes including clinical interpretation, diagnosis,\narchive, etc. In telemedicine, medical images can be manipulated while\ntransferring. In addition, medical information security requirements are\nspecified by the legislative rules, and concerned entities must adhere to them.\nIn this research, we propose a new scheme based on 2-dimensional Discrete\nWavelet Transform (2D DWT) to improve the robustness and authentication of\nmedical images. In addition, the current research improves security and\ncapacity of watermarking using encryption and compression in medical images.\nThe evaluation is performed on the personal dataset, which contains 194 CTI and\n68 MRI cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.05778v1"
    },
    {
        "title": "Multi-Stream Switching for Interactive Virtual Reality Video Streaming",
        "authors": [
            "Gene Cheung",
            "Zhi Liu",
            "Zhiyou Ma",
            "Jack Z. G. Tan"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Virtual reality (VR) video provides an immersive 360 viewing experience to a\nuser wearing a head-mounted display: as the user rotates his head,\ncorrespondingly different fields-of-view (FoV) of the 360 video are rendered\nfor observation. Transmitting the entire 360 video in high quality over\nbandwidth-constrained networks from server to client for real-time playback is\nchallenging. In this paper we propose a multi-stream switching framework for VR\nvideo streaming: the server pre-encodes a set of VR video streams covering\ndifferent view ranges that account for server-client round trip time (RTT)\ndelay, and during streaming the server transmits and switches streams according\nto a user's detected head rotation angle. For a given RTT, we formulate an\noptimization to seek multiple VR streams of different view ranges and the\nhead-angle-to-stream mapping function simultaneously, in order to minimize the\nexpected distortion subject to bandwidth and storage constraints. We propose an\nalternating algorithm that, at each iteration, computes the optimal streams\nwhile keeping the mapping function fixed and vice versa. Experiments show that\nfor the same bandwidth, our multi-stream switching scheme outperforms a\nnon-switching single-stream approach by up to 2.9dB in PSNR.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.09090v1"
    },
    {
        "title": "An Evaluation of Digital Image Forgery Detection Approaches",
        "authors": [
            "Abhishek Kashyap",
            "Rajesh Singh Parmar",
            "Megha Agrawal",
            "Hariom Gupta"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  With the headway of the advanced image handling software and altering tools,\na computerized picture can be effectively controlled. The identification of\nimage manipulation is vital in light of the fact that an image can be utilized\nas legitimate confirmation, in crime scene investigation, and in numerous\ndifferent fields. The image forgery detection techniques intend to confirm the\ncredibility of computerized pictures with no prior information about the\noriginal image. There are numerous routes for altering a picture, for example,\nresampling, splicing, and copy-move. In this paper, we have examined different\ntype of image forgery and their detection techniques; mainly we focused on\npixel based image forgery detection techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.09968v2"
    },
    {
        "title": "A New Steganographic Technique Matching the Secret Message and Cover\n  image Binary Value",
        "authors": [
            "G. Umamaheswari",
            "Dr. C. P. Sumathi"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Steganography involves hiding a secret message or image inside another cover\nimage. Changes are made in the cover image without affecting visual quality of\nthe image. In contrast to cryptography, Steganography provides complete secrecy\nof the communication. Security of very sensitive data can be enhanced by\ncombining cryptography and steganography. A new technique that uses the concept\nof Steganography to obtain the position values from an image is suggested. This\npaper proposes a new method where no change is made to the cover image, only\nthe pixel position LSB (Least Significant Bit) values that match with the\nsecret message bit values are noted in a separate position file. At the sending\nend the position file along with the cover image is sent. At the receiving end\nthe position file is opened only with a secret key. The bit positions are taken\nfrom the position file and the LSB values from the positions are combined to\nget ASCII values and then form characters of the secret message\n",
        "pdf_link": "http://arxiv.org/pdf/1704.02698v1"
    },
    {
        "title": "CNN based music emotion classification",
        "authors": [
            "Xin Liu",
            "Qingcai Chen",
            "Xiangping Wu",
            "Yan Liu",
            "Yang Liu"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Music emotion recognition (MER) is usually regarded as a multi-label tagging\ntask, and each segment of music can inspire specific emotion tags. Most\nresearchers extract acoustic features from music and explore the relations\nbetween these features and their corresponding emotion tags. Considering the\ninconsistency of emotions inspired by the same music segment for human beings,\nseeking for the key acoustic features that really affect on emotions is really\na challenging task. In this paper, we propose a novel MER method by using deep\nconvolutional neural network (CNN) on the music spectrograms that contains both\nthe original time and frequency domain information. By the proposed method, no\nadditional effort on extracting specific features required, which is left to\nthe training procedure of the CNN model. Experiments are conducted on the\nstandard CAL500 and CAL500exp dataset. Results show that, for both datasets,\nthe proposed method outperforms state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.05665v1"
    },
    {
        "title": "A Rate Adaptation Algorithm for Tile-based 360-degree Video Streaming",
        "authors": [
            "Arnob Ghosh",
            "Vaneet Aggarwal",
            "Feng Qian"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In the 360-degree immersive video, a user only views a part of the entire raw\nvideo frame based on her viewing direction. However, today's 360-degree video\nplayers always fetch the entire panoramic view regardless of users' head\nmovement, leading to significant bandwidth waste that can be potentially\navoided. In this paper, we propose a novel adaptive streaming scheme for\n360-degree videos. The basic idea is to fetch the invisible portion of a video\nat the lowest quality based on users' head movement prediction and to\nadaptively decide the video playback quality for the visible portion based on\nbandwidth prediction. Doing both in a robust manner requires overcome a series\nof challenges, such as jointly considering the spatial and temporal domains,\ntolerating prediction errors, and achieving low complexity. To overcome these\nchallenges, we first define quality of experience (QoE) metrics for adaptive\n360-degree video streaming. We then formulate an optimization problem and solve\nit at a low complexity. The algorithm strategically leverages both future\nbandwidth and the distribution of users' head positions to determine the\nquality level of each tile (i.e., a sub-area of a raw frame). We further\nprovide theoretical proof showing that our algorithm achieves optimality under\npractical assumptions. Numerical results show that our proposed algorithms\nsignificantly boost the user QoE by at least 20\\% compared to baseline\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.08215v1"
    },
    {
        "title": "TFDASH: A Fairness, Stability, and Efficiency Aware Rate Control\n  Approach for Multiple Clients over DASH",
        "authors": [
            "Chao Zhou",
            "Chia-Wen Lin",
            "Xinggong Zhang",
            "Zongming Guo"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Dynamic adaptive streaming over HTTP (DASH) has recently been widely deployed\nin the Internet and adopted in the industry. It, however, does not impose any\nadaptation logic for selecting the quality of video fragments requested by\nclients and suffers from lackluster performance with respect to a number of\ndesirable properties: efficiency, stability, and fairness when multiple players\ncompete for a bottleneck link. In this paper, we propose a throughput-friendly\nDASH (TFDASH) rate control scheme for video streaming with multiple clients\nover DASH to well balance the trade-offs among efficiency, stability, and\nfairness. The core idea behind guaranteeing fairness and high efficiency\n(bandwidth utilization) is to avoid OFF periods during the downloading process\nfor all clients, i.e., the bandwidth is in perfect-subscription or\nover-subscription with bandwidth utilization approach to 100\\%. We also propose\na dual-threshold buffer model to solve the instability problem caused by the\nabove idea. As a result, by integrating these novel components, we also propose\na probability-driven rate adaption logic taking into account several key\nfactors that most influence visual quality, including buffer occupancy, video\nplayback quality, video bit-rate switching frequency and amplitude, to\nguarantee high-quality video streaming. Our experiments evidently demonstrate\nthe superior performance of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.08535v2"
    },
    {
        "title": "Deriving Quests from Open World Mechanics",
        "authors": [
            "Ryan Alexander",
            "Chris Martens"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Open world games present players with more freedom than games with linear\nprogression structures. However, without clearly-defined objectives, they often\nleave players without a sense of purpose. Most of the time, quests and\nobjectives are hand-authored and overlaid atop an open world's mechanics. But\nwhat if they could be generated organically from the gameplay itself? The goal\nof our project was to develop a model of the mechanics in Minecraft that could\nbe used to determine the ideal placement of objectives in an open world\nsetting. We formalized the game logic of Minecraft in terms of logical rules\nthat can be manipulated in two ways: they may be executed to generate graphs\nrepresentative of the player experience when playing an open world game with\nlittle developer direction; and they may be statically analyzed to determine\ndependency orderings, feedback loops, and bottlenecks. These analyses may then\nbe used to place achievements on gameplay actions algorithmically.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.00341v1"
    },
    {
        "title": "A New Parallel Message-distribution Technique for Cost-based\n  Steganography",
        "authors": [
            "Mehdi Sharifzadeh",
            "Chirag Agarwal",
            "Mahdi Salarian",
            "Dan Schonfeld"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  This paper presents two novel approaches to increase performance bounds of\nimage steganography under the criteria of minimizing distortion. First, in\norder to efficiently use the images' capacities, we propose using parallel\nimages in the embedding stage. The result is then used to prove sub-optimality\nof the message distribution technique used by all cost based algorithms\nincluding HUGO, S-UNIWARD, and HILL. Second, a new distribution approach is\npresented to further improve the security of these algorithms. Experiments show\nthat this distribution method avoids embedding in smooth regions and thus\nachieves a better performance, measured by state-of-the-art steganalysis, when\ncompared with the current used distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.08616v2"
    },
    {
        "title": "HTTP adaptive streaming with indoors-outdoors detection in mobile\n  networks",
        "authors": [
            "Sami Mekki",
            "Theodoros Karagkioules",
            "Stefan Valentin"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In mobile networks, users may lose coverage when entering a building due to\nthe high signal attenuation at windows and walls. Under such conditions,\nservices with minimum bit-rate requirements, such as video streaming, often\nshow poor Quality-of-Experience (QoE). We will present a Bayesian detector that\ncombines measurements from two Smartphone sensors to decide if a user is inside\na building or not. Based on this coverage classification, we will propose an\nHTTP adaptive streaming (HAS) algorithm to increase playback stability at a\nhigh average bitrate. Measurements in a typical office building show high\naccuracy for the presented detector and superior QoE for the proposed HAS\nalgorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.08809v1"
    },
    {
        "title": "OmniArt: Multi-task Deep Learning for Artistic Data Analysis",
        "authors": [
            "Gjorgji Strezoski",
            "Marcel Worring"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Vast amounts of artistic data is scattered on-line from both museums and art\napplications. Collecting, processing and studying it with respect to all\naccompanying attributes is an expensive process. With a motivation to speed up\nand improve the quality of categorical analysis in the artistic domain, in this\npaper we propose an efficient and accurate method for multi-task learning with\na shared representation applied in the artistic domain. We continue to show how\ndifferent multi-task configurations of our method behave on artistic data and\noutperform handcrafted feature approaches as well as convolutional neural\nnetworks. In addition to the method and analysis, we propose a challenge like\nnature to the new aggregated data set with almost half a million samples and\nstructured meta-data to encourage further research and societal engagement.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.00684v1"
    },
    {
        "title": "Lossless Image and Intra-frame Compression with Integer-to-Integer DST",
        "authors": [
            "Fatih Kamisli"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Video coding standards are primarily designed for efficient lossy\ncompression, but it is also desirable to support efficient lossless compression\nwithin video coding standards using small modifications to the lossy coding\narchitecture. A simple approach is to skip transform and quantization, and\nsimply entropy code the prediction residual. However, this approach is\ninefficient at compression. A more efficient and popular approach is to skip\ntransform and quantization but also process the residual block with DPCM, along\nthe horizontal or vertical direction, prior to entropy coding. This paper\nexplores an alternative approach based on processing the residual block with\ninteger-to-integer (i2i) transforms. I2i transforms can map integer pixels to\ninteger transform coefficients without increasing the dynamic range and can be\nused for lossless compression. We focus on lossless intra coding and develop\nnovel i2i approximations of the odd type-3 DST (ODST-3). Experimental results\nwith the HEVC reference software show that the developed i2i approximations of\nthe ODST-3 improve lossless intra-frame compression efficiency with respect to\nHEVC version 2, which uses the popular DPCM method, by an average 2.7% without\na significant effect on computational complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07154v1"
    },
    {
        "title": "Image Processing Operations Identification via Convolutional Neural\n  Network",
        "authors": [
            "Bolin Chen",
            "Haodong Li",
            "Weiqi Luo"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In recent years, image forensics has attracted more and more attention, and\nmany forensic methods have been proposed for identifying image processing\noperations. Up to now, most existing methods are based on hand crafted\nfeatures, and just one specific operation is considered in their methods. In\nmany forensic scenarios, however, multiple classification for various image\nprocessing operations is more practical. Besides, it is difficult to obtain\neffective features by hand for some image processing operations. In this paper,\ntherefore, we propose a new convolutional neural network (CNN) based method to\nadaptively learn discriminative features for identifying typical image\nprocessing operations. We carefully design the high pass filter bank to get the\nimage residuals of the input image, the channel expansion layer to mix up the\nresulting residuals, the pooling layers, and the activation functions employed\nin our method. The extensive results show that the proposed method can\noutperform the currently best method based on hand crafted features and three\nrelated methods based on CNN for image steganalysis and/or forensics, achieving\nthe state-of-the-art results. Furthermore, we provide more supplementary\nresults to show the rationality and robustness of the proposed model.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.02908v1"
    },
    {
        "title": "Contrast Enhancement of Brightness-Distorted Images by Improved Adaptive\n  Gamma Correction",
        "authors": [
            "Gang Cao",
            "Lihui Huang",
            "Huawei Tian",
            "Xianglin Huang",
            "Yongbin Wang",
            "Ruicong Zhi"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  As an efficient image contrast enhancement (CE) tool, adaptive gamma\ncorrection (AGC) was previously proposed by relating gamma parameter with\ncumulative distribution function (CDF) of the pixel gray levels within an\nimage. ACG deals well with most dimmed images, but fails for globally bright\nimages and the dimmed images with local bright regions. Such two categories of\nbrightness-distorted images are universal in real scenarios, such as improper\nexposure and white object regions. In order to attenuate such deficiencies,\nhere we propose an improved AGC algorithm. The novel strategy of negative\nimages is used to realize CE of the bright images, and the gamma correction\nmodulated by truncated CDF is employed to enhance the dimmed ones. As such,\nlocal over-enhancement and structure distortion can be alleviated. Both\nqualitative and quantitative experimental results show that our proposed method\nyields consistently good CE results.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.04427v2"
    },
    {
        "title": "Acceleration of Histogram-Based Contrast Enhancement via Selective\n  Downsampling",
        "authors": [
            "Gang Cao",
            "Huawei Tian",
            "Lifang Yu",
            "Xianglin Huang",
            "Yongbin Wang"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In this paper, we propose a general framework to accelerate the universal\nhistogram-based image contrast enhancement (CE) algorithms. Both spatial and\ngray-level selective down-sampling of digital images are adopted to decrease\ncomputational cost, while the visual quality of enhanced images is still\npreserved and without apparent degradation. Mapping function calibration is\nnovelly proposed to reconstruct the pixel mapping on the gray levels missed by\ndownsampling. As two case studies, accelerations of histogram equalization (HE)\nand the state-of-the-art global CE algorithm, i.e., spatial mutual information\nand PageRank (SMIRANK), are presented detailedly. Both quantitative and\nqualitative assessment results have verified the effectiveness of our proposed\nCE acceleration framework. In typical tests, computational efficiencies of HE\nand SMIRANK have been speeded up by about 3.9 and 13.5 times, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.04583v3"
    },
    {
        "title": "A new adaptive method for hiding data in images",
        "authors": [
            "Kazem Qazanfari",
            "Reza Safabaksh"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  LSB method is one of the well-known steganography methods which hides the\nmessage bits into the least significant bit of pixel values. This method\nchanges the statistical information of images, which causes to have an\nunsecured channel. To increase the security of this method against the\nsteganalysis methods, in this paper an adaptive method for hiding data into\nimages will be proposed. So, the amount of data and the method which is used\nfor hiding data in each area of image will be different. Experimental results\nshow that the security of the proposed method is higher than general LSB method\nand in some cases the capacity of the carrier signal is increased.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.06729v1"
    },
    {
        "title": "Region-Based Image Retrieval Revisited",
        "authors": [
            "Ryota Hinami",
            "Yusuke Matsui",
            "Shin'ichi Satoh"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Region-based image retrieval (RBIR) technique is revisited. In early attempts\nat RBIR in the late 90s, researchers found many ways to specify region-based\nqueries and spatial relationships; however, the way to characterize the\nregions, such as by using color histograms, were very poor at that time. Here,\nwe revisit RBIR by incorporating semantic specification of objects and\nintuitive specification of spatial relationships. Our contributions are the\nfollowing. First, to support multiple aspects of semantic object specification\n(category, instance, and attribute), we propose a multitask CNN feature that\nallows us to use deep learning technique and to jointly handle multi-aspect\nobject specification. Second, to help users specify spatial relationships among\nobjects in an intuitive way, we propose recommendation techniques of spatial\nrelationships. In particular, by mining the search results, a system can\nrecommend feasible spatial relationships among the objects. The system also can\nrecommend likely spatial relationships by assigned object category names based\non language prior. Moreover, object-level inverted indexing supports very fast\nshortlist generation, and re-ranking based on spatial constraints provides\nusers with instant RBIR experiences.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.09106v1"
    },
    {
        "title": "End-to-end Trained CNN Encode-Decoder Networks for Image Steganography",
        "authors": [
            "Atique ur Rehman",
            "Rafia Rahim",
            "M Shahroz Nadeem",
            "Sibt ul Hussain"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  All the existing image steganography methods use manually crafted features to\nhide binary payloads into cover images. This leads to small payload capacity\nand image distortion. Here we propose a convolutional neural network based\nencoder-decoder architecture for embedding of images as payload. To this end,\nwe make following three major contributions: (i) we propose a deep learning\nbased generic encoder-decoder architecture for image steganography; (ii) we\nintroduce a new loss function that ensures joint end-to-end training of\nencoder-decoder networks; (iii) we perform extensive empirical evaluation of\nproposed architecture on a range of challenging publicly available datasets\n(MNIST, CIFAR10, PASCAL-VOC12, ImageNet, LFW) and report state-of-the-art\npayload capacity at high PSNR and SSIM values.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.07201v1"
    },
    {
        "title": "Probabilistic Semantic Retrieval for Surveillance Videos with Activity\n  Graphs",
        "authors": [
            "Yuting Chen",
            "Joseph Wang",
            "Yannan Bai",
            "Gregory Castañón",
            "Venkatesh Saligrama"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  We present a novel framework for finding complex activities matching\nuser-described queries in cluttered surveillance videos. The wide diversity of\nqueries coupled with unavailability of annotated activity data limits our\nability to train activity models. To bridge the semantic gap we propose to let\nusers describe an activity as a semantic graph with object attributes and\ninter-object relationships associated with nodes and edges, respectively. We\nlearn node/edge-level visual predictors during training and, at test-time,\npropose to retrieve activity by identifying likely locations that match the\nsemantic graph. We formulate a novel CRF based probabilistic activity\nlocalization objective that accounts for mis-detections, mis-classifications\nand track-losses, and outputs a likelihood score for a candidate grounded\nlocation of the query in the video. We seek groundings that maximize overall\nprecision and recall. To handle the combinatorial search over all\nhigh-probability groundings, we propose a highest precision subgraph matching\nalgorithm. Our method outperforms existing retrieval methods on benchmarked\ndatasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.06204v2"
    },
    {
        "title": "How to augment a small learning set for improving the performances of a\n  CNN-based steganalyzer?",
        "authors": [
            "Mehdi Yedroudj",
            "Marc Chaumont",
            "Frédéric Comby"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Deep learning and convolutional neural networks (CNN) have been intensively\nused in many image processing topics during last years. As far as steganalysis\nis concerned, the use of CNN allows reaching the state-of-the-art results. The\nperformances of such networks often rely on the size of their learning\ndatabase. An obvious preliminary assumption could be considering that \"the\nbigger a database is, the better the results are\". However, it appears that\ncautions have to be taken when increasing the database size if one desire to\nimprove the classification accuracy i.e. enhance the steganalysis efficiency.\nTo our knowledge, no study has been performed on the enrichment impact of a\nlearning database on the steganalysis performance. What kind of images can be\nadded to the initial learning set? What are the sensitive criteria: the camera\nmodels used for acquiring the images, the treatments applied to the images, the\ncameras proportions in the database, etc? This article continues the work\ncarried out in a previous paper, and explores the ways to improve the\nperformances of CNN. It aims at studying the effects of \"base augmentation\" on\nthe performance of steganalysis using a CNN. We present the results of this\nstudy using various experimental protocols and various databases to define the\ngood practices in base augmentation for steganalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04076v2"
    },
    {
        "title": "Multiple Description Convolutional Neural Networks for Image Compression",
        "authors": [
            "Lijun Zhao",
            "Huihui Bai",
            "Anhong Wang",
            "Yao Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Multiple description coding (MDC) is able to stably transmit the signal in\nthe un-reliable and non-prioritized networks, which has been broadly studied\nfor several decades. However, the traditional MDC doesn't well leverage image's\ncontext features to generate multiple descriptions. In this paper, we propose a\nnovel standard-compliant convolutional neural network-based MDC framework in\nterm of image's context features. Firstly, multiple description generator\nnetwork (MDGN) is designed to produce appearance-similar yet feature-different\nmultiple descriptions automatically according to image's content, which are\ncompressed by standard codec. Secondly, we present multiple description\nreconstruction network (MDRN) including side reconstruction network (SRN) and\ncentral reconstruction network (CRN). When any one of two lossy descriptions is\nreceived at the decoder, SRN network is used to improve the quality of this\ndecoded lossy description by removing the compression artifact and up-sampling\nsimultaneously. Meanwhile, we utilize CRN network with two decoded descriptions\nas inputs for better reconstruction, if both of lossy descriptions are\navailable. Thirdly, multiple description virtual codec network (MDVCN) is\nproposed to bridge the gap between MDGN network and MDRN network in order to\ntrain an end-to-end MDC framework. Here, two learning algorithms are provided\nto train our whole framework. In addition to structural similarity loss\nfunction, the produced descriptions are used as opposing labels with multiple\ndescription distance loss function to regularize the training of MDGN network.\nThese losses guarantee that the generated description images are structurally\nsimilar yet finely diverse. Experimental results show a great deal of objective\nand subjective quality measurements to validate the efficiency of the proposed\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06611v2"
    },
    {
        "title": "An Optimized Information-Preserving Relational Database Watermarking\n  Scheme for Ownership Protection of Medical Data",
        "authors": [
            "Muhammad Kamran",
            "Muddassar Farooq"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Recently, a significant amount of interest has been developed in motivating\nphysicians to use e-health technology (especially Electronic Medical Records\n(EMR) systems). An important utility of such EMR systems is: a next generation\nof Clinical Decision Support Systems (CDSS) will extract knowledge from these\nelectronic medical records to enable physicians to do accurate and effective\ndiagnosis. It is anticipated that in future such medical records will be shared\nthrough cloud among different physicians to improve the quality of health care.\nTherefore, right protection of medical records is important to protect their\nownership once they are shared with third parties. Watermarking is a proven\nwell known technique to achieve this objective. The challenges associated with\nwatermarking of EMR systems are: (1) some fields in EMR are more relevant in\nthe diagnosis process; as a result, small variations in them could change the\ndiagnosis, and (2) a misdiagnosis might not only result in a life threatening\nscenario but also might lead to significant costs of the treatment for the\npatients. The major contribution of this paper is an information-preserving\nwatermarking scheme to address the above-mentioned challenges. We model the\nwatermarking process as a constrained optimization problem. We demonstrate,\nthrough experiments, that our scheme not only preserves the diagnosis accuracy\nbut is also resilient to well known attacks for corrupting the watermark. Last\nbut not least, we also compare our scheme with a well known threshold-based\nscheme to evaluate relative merits of a classifier. Our pilot studies reveal\nthat -- using proposed information-preserving scheme -- the overall\nclassification accuracy is never degraded by more than 1%. In comparison, the\ndiagnosis accuracy, using the threshold-based technique, is degraded by more\nthan 18% in a worst case scenario.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.09741v1"
    },
    {
        "title": "Learning to score the figure skating sports videos",
        "authors": [
            "Chengming Xu",
            "Yanwei Fu",
            "Bing Zhang",
            "Zitian Chen",
            "Yu-Gang Jiang",
            "Xiangyang Xue"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  This paper targets at learning to score the figure skating sports videos. To\naddress this task, we propose a deep architecture that includes two\ncomplementary components, i.e., Self-Attentive LSTM and Multi-scale\nConvolutional Skip LSTM. These two components can efficiently learn the local\nand global sequential information in each video. Furthermore, we present a\nlarge-scale figure skating sports video dataset -- FisV dataset. This dataset\nincludes 500 figure skating videos with the average length of 2 minutes and 50\nseconds. Each video is annotated by two scores of nine different referees,\ni.e., Total Element Score(TES) and Total Program Component Score (PCS). Our\nproposed model is validated on FisV and MIT-skate datasets. The experimental\nresults show the effectiveness of our models in learning to score the figure\nskating videos.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.02774v3"
    },
    {
        "title": "Demoiréing of Camera-Captured Screen Images Using Deep Convolutional\n  Neural Network",
        "authors": [
            "Bolin Liu",
            "Xiao Shu",
            "Xiaolin Wu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Taking photos of optoelectronic displays is a direct and spontaneous way of\ntransferring data and keeping records, which is widely practiced. However, due\nto the analog signal interference between the pixel grids of the display screen\nand camera sensor array, objectionable moir\\'e (alias) patterns appear in\ncaptured screen images. As the moir\\'e patterns are structured and highly\nvariant, they are difficult to be completely removed without affecting the\nunderneath latent image. In this paper, we propose an approach of deep\nconvolutional neural network for demoir\\'eing screen photos. The proposed DCNN\nconsists of a coarse-scale network and a fine-scale network. In the\ncoarse-scale network, the input image is first downsampled and then processed\nby stacked residual blocks to remove the moir\\'e artifacts. After that, the\nfine-scale network upsamples the demoir\\'ed low-resolution image back to the\noriginal resolution. Extensive experimental results have demonstrated that the\nproposed technique can efficiently remove the moir\\'e patterns for camera\nacquired screen images; the new technique outperforms the existing ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03809v1"
    },
    {
        "title": "Learning for Video Compression",
        "authors": [
            "Zhibo Chen",
            "Tianyu He",
            "Xin Jin",
            "Feng Wu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  One key challenge to learning-based video compression is that motion\npredictive coding, a very effective tool for video compression, can hardly be\ntrained into a neural network. In this paper we propose the concept of\nPixelMotionCNN (PMCNN) which includes motion extension and hybrid prediction\nnetworks. PMCNN can model spatiotemporal coherence to effectively perform\npredictive coding inside the learning network. On the basis of PMCNN, we\nfurther explore a learning-based framework for video compression with\nadditional components of iterative analysis/synthesis, binarization, etc.\nExperimental results demonstrate the effectiveness of the proposed scheme.\nAlthough entropy coding and complex configurations are not employed in this\npaper, we still demonstrate superior performance compared with MPEG-2 and\nachieve comparable results with H.264 codec. The proposed learning-based scheme\nprovides a possible new direction to further improve compression efficiency and\nfunctionalities of future video coding.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.09869v2"
    },
    {
        "title": "Generative Steganography by Sampling",
        "authors": [
            "Zhuo Zhang",
            "Jia Liu",
            "Yan Ke",
            "Yu Lei",
            "Jun Li",
            "Minqing Zhang",
            "Xiaoyuan Yang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this paper, a novel data-driven information hiding scheme called\ngenerative steganography by sampling (GSS) is proposed. Unlike in traditional\nmodification-based steganography, in our method the stego image is directly\nsampled by a powerful generator: no explicit cover is used. Both parties share\na secret key used for message embedding and extraction. The Jensen-Shannon\ndivergence is introduced as a new criterion for evaluating the security of\ngenerative steganography. Based on these principles, we propose a simple\npractical generative steganography method that uses semantic image inpainting.\nThe message is written in advance to an uncorrupted region that needs to be\nretained in the corrupted image. Then, the corrupted image with the secret\nmessage is fed into a Generator trained by a generative adversarial network\n(GAN) for semantic completion. Message loss and prior loss terms are proposed\nfor penalizing message extraction error and unrealistic stego image. In our\ndesign, we first train a generator whose training target is the generation of\nnew data samples from the same distribution as that of existing training data.\nNext, for the trained generator, backpropagation to the message and prior loss\nare introduced to optimize the coding of the input noise data for the\ngenerator. The presented experiments demonstrate the potential of the proposed\nframework based on both qualitative and quantitative evaluations of the\ngenerated stego images.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10531v2"
    },
    {
        "title": "Weakly-supervised Visual Instrument-playing Action Detection in Videos",
        "authors": [
            "Jen-Yu Liu",
            "Yi-Hsuan Yang",
            "Shyh-Kang Jeng"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Instrument playing is among the most common scenes in music-related videos,\nwhich represent nowadays one of the largest sources of online videos. In order\nto understand the instrument-playing scenes in the videos, it is important to\nknow what instruments are played, when they are played, and where the playing\nactions occur in the scene. While audio-based recognition of instruments has\nbeen widely studied, the visual aspect of the music instrument playing remains\nlargely unaddressed in the literature. One of the main obstacles is the\ndifficulty in collecting annotated data of the action locations for\ntraining-based methods. To address this issue, we propose a weakly-supervised\nframework to find when and where the instruments are played in the videos. We\npropose to use two auxiliary models, a sound model and an object model, to\nprovide supervisions for training the instrument-playing action model. The\nsound model provides temporal supervisions, while the object model provides\nspatial supervisions. They together can simultaneously provide temporal and\nspatial supervisions. The resulted model only needs to analyze the visual part\nof a music video to deduce which, when and where instruments are played. We\nfound that the proposed method significantly improves the localization\naccuracy. We evaluate the result of the proposed method temporally and\nspatially on a small dataset (totally 5,400 frames) that we manually annotated.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.02031v1"
    },
    {
        "title": "Hierarchical One Permutation Hashing: Efficient Multimedia Near\n  Duplicate Detection",
        "authors": [
            "Chengyuan Zhang",
            "Yunwu Lin",
            "Lei Zhu",
            "XinPan Yuan",
            "Jun Long",
            "Fang Huang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  With advances in multimedia technologies and the proliferation of smart\nphone, digital cameras, storage devices, there are a rapidly growing massive\namount of multimedia data collected in many applications such as multimedia\nretrieval and management system, in which the data element is composed of text,\nimage, video and audio. Consequently, the study of multimedia near duplicate\ndetection has attracted significant concern from research organizations and\ncommercial communities. Traditional solution minwish hashing (\\minwise) faces\ntwo challenges: expensive preprocessing time and lower comparison speed. Thus,\nthis work first introduce a hashing method called one permutation hashing\n(\\oph) to shun the costly preprocessing time. Based on \\oph, a more efficient\nstrategy group based one permutation hashing (\\goph) is developed to deal with\nthe high comparison time. Based on the fact that the similarity of most\nmultimedia data is not very high, this work design an new hashing method namely\nhierarchical one permutation hashing (\\hoph) to further improve the\nperformance. Comprehensive experiments on real multimedia datasets clearly show\nthat with similar accuracy \\hoph is five to seven times faster than\n",
        "pdf_link": "http://arxiv.org/pdf/1805.11254v2"
    },
    {
        "title": "A Robust Algorithm for Tile-based 360-degree Video Streaming with\n  Uncertain FoV Estimation",
        "authors": [
            "Arnob Ghosh",
            "Vaneet Aggarwal",
            "Feng Qian"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  We propose a robust scheme for streaming 360-degree immersive videos to\nmaximize the quality of experience (QoE). Our streaming approach introduces a\nholistic analytical framework built upon the formal method of stochastic\noptimization. We propose a robust algorithm which provides a streaming rate\nsuch that the video quality degrades below that rate with very low probability\neven in presence of uncertain head movement, and bandwidth. It assumes the\nknowledge of the viewing probability of different portions (tiles) of a\npanoramic scene. Such probabilities can be easily derived from crowdsourced\nmeasurements performed by 360 video content providers. We then propose\nefficient methods to solve the problem at runtime while achieving a bounded\noptimality gap (in terms of the QoE). We implemented our proposed approaches\nusing emulation. Using real users' head movement traces and real cellular\nbandwidth traces, we show that our algorithms significantly outperform the\nbaseline algorithms by at least in $30\\%$ in the QoE metric. Our algorithm\ngives a streaming rate which is $50\\%$ higher compared to the baseline\nalgorithms when the prediction error is high.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.00816v1"
    },
    {
        "title": "Novel Quality Metric for Duration Variability Compensation in Speaker\n  Verification using i-Vectors",
        "authors": [
            "Arnab Poddar",
            "Md Sahidullah",
            "Goutam Saha"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Automatic speaker verification (ASV) is the process to recognize persons\nusing voice as biometric. The ASV systems show considerable recognition\nperformance with sufficient amount of speech from matched condition. One of the\ncrucial challenges of ASV technology is to improve recognition performance with\nspeech segments of short duration. In short duration condition, the model\nparameters are not properly estimated due to inadequate speech information, and\nthis results poor recognition accuracy even with the state-of-the-art i-vector\nbased ASV system. We hypothesize that considering the estimation quality during\nrecognition process would help to improve the ASV performance. This can be\nincorporated as a quality measure during fusion of ASV systems. This paper\ninvestigates a new quality measure for i-vector representation of speech\nutterances computed directly from Baum-Welch statistics. The proposed metric is\nsubsequently used as quality measure during fusion of ASV systems. In\nexperiments with the NIST SRE 2008 corpus, We have shown that inclusion of\nproposed quality metric exhibits considerable improvement in speaker\nverification performance. The results also indicate the potentiality of the\nproposed method in real-world scenario with short test utterances.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.00828v1"
    },
    {
        "title": "HEVC Inter Coding Using Deep Recurrent Neural Networks and Artificial\n  Reference Pictures",
        "authors": [
            "Felix Haub",
            "Thorsten Laude",
            "Jörn Ostermann"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The efficiency of motion compensated prediction in modern video codecs highly\ndepends on the available reference pictures. Occlusions and non-linear motion\npose challenges for the motion compensation and often result in high bit rates\nfor the prediction error. We propose the generation of artificial reference\npictures using deep recurrent neural networks. Conceptually, a reference\npicture at the time instance of the currently coded picture is generated from\npreviously reconstructed conventional reference pictures. Based on these\nartificial reference pictures, we propose a complete coding pipeline based on\nHEVC. By using the artificial reference pictures for motion compensated\nprediction, average BD-rate gains of 1.5% over HEVC are achieved.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.02137v1"
    },
    {
        "title": "Proactive Video Chunks Caching and Processing for Latency and Cost\n  Minimization in Edge Networks",
        "authors": [
            "Emna Baccour",
            "Aiman Erbad",
            "Amr Mohamed",
            "Kashif Bilal",
            "Mohsen Guizani"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Recently, the growing demand for rich multimedia content such as Video on\nDemand (VoD) has made the data transmission from content delivery networks\n(CDN) to end-users quite challenging. Edge networks have been proposed as an\nextension to CDN networks to alleviate this excessive data transfer through\ncaching and to delegate the computation tasks to edge servers. To maximize the\ncaching efficiency in the edge networks, different Mobile Edge Computing (MEC)\nservers assist each others to effectively select which content to store and the\nappropriate computation tasks to process. In this paper, we adopt a\ncollaborative caching and transcoding model for VoD in MEC networks. However,\nunlike other models in the literature, different chunks of the same video are\nnot fetched and cached in the same MEC server. Instead, neighboring servers\nwill collaborate to store and transcode different video chunks and consequently\noptimize the limited resources usage. Since we are dealing with chunks caching\nand processing, we propose to maximize the edge efficiency by studying the\nviewers watching pattern and designing a probabilistic model where chunks\npopularities are evaluated. Based on this model, popularity-aware policies,\nnamely Proactive caching policy (PcP) and Cache replacement Policy (CrP), are\nintroduced to cache only highest probably requested chunks. In addition to PcP\nand CrP, an online algorithm (PCCP) is proposed to schedule the collaborative\ncaching and processing. The evaluation results prove that our model and\npolicies give better performance than approaches using conventional replacement\npolicies. This improvement reaches up to 50% in some cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.06501v1"
    },
    {
        "title": "Receiver-driven Video Multicast over NOMA Systems in Heterogeneous\n  Environments",
        "authors": [
            "Xiaoda Jiang",
            "Hancheng Lu",
            "Chang Wen Chen",
            "Feng Wu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Non-orthogonal multiple access (NOMA) has shown potential for scalable\nmulticast of video data. However, one key drawback for NOMA-based video\nmulticast is the limited number of layers allowed by the embedded successive\ninterference cancellation algorithm, failing to meet satisfaction of\nheterogeneous receivers. We propose a novel receiver-driven superposed video\nmulticast (Supcast) scheme by integrating Softcast, an analog-like transmission\nscheme, into the NOMA-based system to achieve high bandwidth efficiency as well\nas gradual decoding quality proportional to channel conditions at receivers.\nAlthough Softcast allows gradual performance by directly transmitting\npower-scaled transformation coefficients of frames, it suffers performance\ndegradation due to discarding coefficients under insufficient bandwidth and its\npower allocation strategy cannot be directly applied in NOMA due to\ninterference. In Supcast, coefficients are grouped into chunks, which are basic\nunits for power allocation and superposition scheduling. By bisecting chunks\ninto base-layer chunks and enhanced-layer chunks, the joint power allocation\nand chunk scheduling is formulated as a distortion minimization problem. A\ntwo-stage power allocation strategy and a near-optimal low-complexity algorithm\nfor chunk scheduling based on the matching theory are proposed. Simulation\nresults have shown the advantage of Supcast against Softcast as well as the\nreference scheme in NOMA under various practical scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.06713v2"
    },
    {
        "title": "D{é}tection de locuteurs dans les s{é}ries TV",
        "authors": [
            "Xavier Bost",
            "Georges Linares"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Speaker diarization of audio streams turns out to be particularly challenging\nwhen applied to fictional films, where many characters talk in various acoustic\nconditions (background music, sound effects, variations in intonation...).\nDespite this acoustic variability, such movies exhibit specific visual\npatterns, particularly within dialogue scenes. In this paper, we introduce a\ntwo-step method to achieve speaker diarization in TV series: speaker\ndiarization is first performed locally within scenes visually identified as\ndialogues; then, the hypothesized local speakers are compared to each other\nduring a second clustering process in order to detect recurring speakers: this\nsecond stage of clustering is subject to the constraint that the different\nspeakers involved in the same dialogue have to be assigned to different\nclusters. The performances of our approach are compared to those obtained by\nstandard speaker diarization tools applied to the same data.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07200v1"
    },
    {
        "title": "Audiovisual speaker diarization of TV series",
        "authors": [
            "Xavier Bost",
            "Georges Linarès",
            "Serigne Gueye"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Speaker diarization may be difficult to achieve when applied to narrative\nfilms, where speakers usually talk in adverse acoustic conditions: background\nmusic, sound effects, wide variations in intonation may hide the inter-speaker\nvariability and make audio-based speaker diarization approaches error prone. On\nthe other hand, such fictional movies exhibit strong regularities at the image\nlevel, particularly within dialogue scenes. In this paper, we propose to\nperform speaker diarization within dialogue scenes of TV series by combining\nthe audio and video modalities: speaker diarization is first performed by using\neach modality, the two resulting partitions of the instance set are then\noptimally matched, before the remaining instances, corresponding to cases of\ndisagreement between both modalities, are finally processed. The results\nobtained by applying such a multi-modal approach to fictional films turn out to\noutperform those obtained by relying on a single modality.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07205v2"
    },
    {
        "title": "Constrained speaker diarization of TV series based on visual patterns",
        "authors": [
            "Xavier Bost",
            "Georges Linares"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Speaker diarization, usually denoted as the ''who spoke when'' task, turns\nout to be particularly challenging when applied to fictional films, where many\ncharacters talk in various acoustic conditions (background music, sound\neffects...). Despite this acoustic variability , such movies exhibit specific\nvisual patterns in the dialogue scenes. In this paper, we introduce a two-step\nmethod to achieve speaker diarization in TV series: a speaker diarization is\nfirst performed locally in the scenes detected as dialogues; then, the\nhypothesized local speakers are merged in a second agglomerative clustering\nprocess, with the constraint that speakers locally hypothesized to be distinct\nmust not be assigned to the same cluster. The performances of our approach are\ncompared to those obtained by standard speaker diarization tools applied to the\nsame data.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07209v2"
    },
    {
        "title": "Learning based Facial Image Compression with Semantic Fidelity Metric",
        "authors": [
            "Zhibo Chen",
            "Tianyu He"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Surveillance and security scenarios usually require high efficient facial\nimage compression scheme for face recognition and identification. While either\ntraditional general image codecs or special facial image compression schemes\nonly heuristically refine codec separately according to face verification\naccuracy metric. We propose a Learning based Facial Image Compression (LFIC)\nframework with a novel Regionally Adaptive Pooling (RAP) module whose\nparameters can be automatically optimized according to gradient feedback from\nan integrated hybrid semantic fidelity metric, including a successfully\nexploration to apply Generative Adversarial Network (GAN) as metric directly in\nimage compression scheme. The experimental results verify the framework's\nefficiency by demonstrating performance improvement of 71.41%, 48.28% and\n52.67% bitrate saving separately over JPEG2000, WebP and neural network-based\ncodecs under the same face verification accuracy distortion metric. We also\nevaluate LFIC's superior performance gain compared with latest specific facial\nimage codecs. Visual experiments also show some interesting insight on how LFIC\ncan automatically capture the information in critical areas based on semantic\ndistortion metrics for optimized compression, which is quite different from the\nheuristic way of optimization in traditional image compression algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.10067v2"
    },
    {
        "title": "HoloCast: Graph Signal Processing for Graceful Point Cloud Delivery",
        "authors": [
            "Takuya Fujihashi",
            "Toshiaki Koike-Akino",
            "Takashi Watanabe",
            "Philip V. Orlik"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In conventional point cloud delivery, a sender uses octree-based digital\nvideo compression to stream three-dimensional (3D) points and the corresponding\ncolor attributes over band-limited links, e.g., wireless channels, for 3D scene\nreconstructions. However, the digital-based delivery schemes have an issue\ncalled cliff effect, where the 3D reconstruction quality is a step function in\nterms of wireless channel quality. We propose a novel scheme of point cloud\ndelivery, called HoloCast, to gracefully improve the reconstruction quality\nwith the improvement of wireless channel quality. HoloCast regards the 3D\npoints and color components as graph signals and directly transmits\nlinear-transformed signals based on graph Fourier transform (GFT), without\ndigital quantization and entropy coding operations. One of main contributions\nin HoloCast is that the use of GFT can deal with non-ordered and non-uniformly\ndistributed multi-dimensional signals such as holographic data unlike\nconventional delivery schemes. Performance results with point cloud data show\nthat HoloCast yields better 3D reconstruction quality compared to digital-based\nmethods in noisy wireless environment.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.03247v1"
    },
    {
        "title": "Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial\n  Networks",
        "authors": [
            "Amanda Duarte",
            "Francisco Roldan",
            "Miquel Tubau",
            "Janna Escur",
            "Santiago Pascual",
            "Amaia Salvador",
            "Eva Mohedano",
            "Kevin McGuinness",
            "Jordi Torres",
            "Xavier Giro-i-Nieto"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Speech is a rich biometric signal that contains information about the\nidentity, gender and emotional state of the speaker. In this work, we explore\nits potential to generate face images of a speaker by conditioning a Generative\nAdversarial Network (GAN) with raw speech input. We propose a deep neural\nnetwork that is trained from scratch in an end-to-end fashion, generating a\nface directly from the raw speech waveform without any additional identity\ninformation (e.g reference image or one-hot encoding). Our model is trained in\na self-supervised approach by exploiting the audio and visual signals naturally\naligned in videos. With the purpose of training from video data, we present a\nnovel dataset collected for this work, with high-quality videos of youtubers\nwith notable expressiveness in both the speech and visual signals.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.10195v1"
    },
    {
        "title": "Resource Allocation Mechanism for Media Handling Services in Cloud\n  Multimedia Conferencing",
        "authors": [
            "Abbas Soltanian",
            "Diala Naboulsi",
            "Roch Glitho",
            "Halima Elbiaze"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Multimedia conferencing is the conversational exchange of multimedia content\nbetween multiple parties. It has a wide range of applications (e.g., Massively\nMultiplayer Online Games (MMOGs) and distance learning). Media handling\nservices (e.g., video mixing, transcoding, and compressing) are critical to\nmultimedia conferencing. However, efficient resource usage and scalability\nstill remain important challenges. Unfortunately, the cloud-based approaches\nproposed so far have several deficiencies in terms of efficiency in resource\nusage and scaling, while meeting Quality of Service (QoS) requirements. This\npaper proposes a solution which optimizes resource allocation and scales in\nterms of the number of participants while guaranteeing QoS. Moreover, our\nsolution composes different media handling services to support the\nparticipants' demands. We formulate the resource allocation problem\nmathematically as an Integer Linear Programming (ILP) problem and design a\nheuristic for it. We evaluate our proposed solution for different numbers of\nparticipants and different participants' geographical distributions. Simulation\nresults show that our resource allocation mechanism can compose the media\nhandling services and allocate the required resources in an optimal manner\nwhile honoring the QoS in terms of end-to-end delay.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.11722v1"
    },
    {
        "title": "GANs-NQM: A Generative Adversarial Networks based No Reference Quality\n  Assessment Metric for RGB-D Synthesized Views",
        "authors": [
            "Suiyi Ling",
            "Jing Li",
            "Junle Wang",
            "Patrick Le Callet"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this paper, we proposed a no-reference (NR) quality metric for RGB plus\nimage-depth (RGB-D) synthesis images based on Generative Adversarial Networks\n(GANs), namely GANs-NQM. Due to the failure of the inpainting on dis-occluded\nregions in RGB-D synthesis process, to capture the non-uniformly distributed\nlocal distortions and to learn their impact on perceptual quality are\nchallenging tasks for objective quality metrics. In our study, based on the\ncharacteristics of GANs, we proposed i) a novel training strategy of GANs for\nRGB-D synthesis images using existing large-scale computer vision datasets\nrather than RGB-D dataset; ii) a referenceless quality metric based on the\ntrained discriminator by learning a `Bag of Distortion Word' (BDW) codebook and\na local distortion regions selector; iii) a hole filling inpainter, i.e., the\ngenerator of the trained GANs, for RGB-D dis-occluded regions as a side\noutcome. According to the experimental results on IRCCyN/IVC DIBR database, the\nproposed model outperforms the state-of-the-art quality metrics, in addition,\nis more applicable in real scenarios. The corresponding context inpainter also\nshows appealing results over other inpainting algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.12088v1"
    },
    {
        "title": "Extension of JPEG XS for Two-Layer Lossless Coding",
        "authors": [
            "Hiroyuki Kobayashi",
            "Hitoshi Kiya"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  A two-layer lossless image coding method compatible with JPEG XS is proposed.\nJPEG XS is a new international standard for still image coding that has the\ncharacteristics of very low latency and very low complexity. However, it does\nnot support lossless coding, although it can achieve visual lossless coding.\nThe proposed method has a two-layer structure similar to JPEG XT, which\nconsists of JPEG XS coding and a lossless coding method. As a result, it\nenables us to losslessly restore original images, while maintaining\ncompatibility with JPEG XS.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04558v1"
    },
    {
        "title": "JQF: Optimal JPEG Quantization Table Fusion by Simulated Annealing on\n  Texture Images and Predicting Textures",
        "authors": [
            "Chen-Hsiu Huang",
            "Ja-Ling Wu"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  JPEG has been a widely used lossy image compression codec for nearly three\ndecades. The JPEG standard allows to use customized quantization table;\nhowever, it's still a challenging problem to find an optimal quantization table\nwithin acceptable computational cost. This work tries to solve the dilemma of\nbalancing between computational cost and image specific optimality by\nintroducing a new concept of texture mosaic images. Instead of optimizing a\nsingle image or a collection of representative images, the simulated annealing\ntechnique is applied to texture mosaic images to search for an optimal\nquantization table for each texture category. We use pre-trained VGG-16 CNN\nmodel to learn those texture features and predict the new image's texture\ndistribution, then fuse optimal texture tables to come out with an image\nspecific optimal quantization table. On the Kodak dataset with the quality\nsetting $Q=95$, our experiment shows a size reduction of 23.5% over the JPEG\nstandard table with a slightly 0.35% FSIM decrease, which is visually\nunperceivable. The proposed JQF method achieves per image optimality for JPEG\nencoding with less than one second additional timing cost. The online demo is\navailable at https://matthorn.s3.amazonaws.com/JQF/qtbl_vis.html\n",
        "pdf_link": "http://arxiv.org/pdf/2008.05672v2"
    },
    {
        "title": "Quality of Service (QoS): Measurements of Video Streaming",
        "authors": [
            "Sajida Karim",
            "Hui He",
            "Asif Ali Laghari",
            "Hina Madiha"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Nowadays video streaming is growing over the social clouds, where end-users\nalways want to share High Definition (HD) videos among friends. Mostly videos\nwere recorded via smartphones and other HD devices and short time videos have a\nbig file size. The big file size of videos required high bandwidth to upload\nand download on the Internet and also required more time to load in a web page\nfor play. So avoiding this problem social cloud compress videos during the\nupload for smooth play and fast loading in a web page. Compression decreases\nthe video quality which also decreases the quality of experience of end users.\nIn this paper we measure the QoS of different standard video file formats on\nsocial clouds; they varied from each other in resolution, audio/video bitrate,\nand storage size.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.12017v1"
    },
    {
        "title": "An Approach for Text Steganography Based on Markov Chains",
        "authors": [
            "H. Hernan Moraldo"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  A text steganography method based on Markov chains is introduced, together\nwith a reference implementation. This method allows for information hiding in\ntexts that are automatically generated following a given Markov model. Other\nMarkov - based systems of this kind rely on big simplifications of the language\nmodel to work, which produces less natural looking and more easily detectable\ntexts. The method described here is designed to generate texts within a good\napproximation of the original language model provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.0915v1"
    },
    {
        "title": "Toward Green Media Delivery: Location-Aware Opportunities and Approaches",
        "authors": [
            "Hatem Abou-zeid",
            "Hosssam S. Hassenein"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Mobile media has undoubtedly become the predominant source of traffic in\nwireless networks. The result is not only congestion and poor\nQuality-of-Experience, but also an unprecedented energy drain at both the\nnetwork and user devices. In order to sustain this continued growth, novel\ndisruptive paradigms of media delivery are urgently needed. We envision that\ntwo key contemporary advancements can be leveraged to develop greener media\ndelivery platforms: 1) the proliferation of navigation hardware and software in\nmobile devices has created an era of location-awareness, where both the current\nand future user locations can be predicted; and 2) the rise of context-aware\nnetwork architectures and self-organizing functionalities is enabling context\nsignaling and in-network adaptation. With these developments in mind, this\narticle investigates the opportunities of exploiting location-awareness to\nenable green end-to-end media delivery. In particular, we discuss and propose\napproaches for location-based adaptive video quality planning, in-network\ncaching, content prefetching, and long-term radio resource management. To\nprovide insights on the energy savings, we then present a cross-layer framework\nthat jointly optimizes resource allocation and multi-user video quality using\nlocation predictions. Finally, we highlight some of the future research\ndirections for location-aware media delivery in the conclusion.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.1148v1"
    },
    {
        "title": "A new Watermarking Technique for Medical Image using Hierarchical\n  Encryption",
        "authors": [
            "Med Karim Abdmouleh",
            "Ali Khalfallah",
            "Med Salim Bouhlel"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  In recent years, characterized by the innovation of technology and the\ndigital revolution, the field of media has become important. The transfer and\nexchange of multimedia data and duplication have become major concerns of\nresearchers. Consequently, protecting copyrights and ensuring service safety is\nneeded. Cryptography has a specific role, is to protect secret files against\nunauthorized access. In this paper, a hierarchical cryptosystem algorithm based\non Logistic Map chaotic systems is proposed. The results show that the proposed\nmethod improves the security of the image. Experimental results on a database\nof 200 medical images show that the proposed method significantly gives better\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.4587v1"
    },
    {
        "title": "Halftone Image Watermarking by Content Aware Double-sided Embedding\n  Error Diffusion",
        "authors": [
            "Yuanfang Guo",
            "Oscar C. Au",
            "Rui Wang",
            "Lu Fang",
            "Xiaochun Cao"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In this paper, we carry out a performance analysis from a probabilistic\nperspective to introduce the EDHVW methods' expected performances and\nlimitations. Then, we propose a new general error diffusion based halftone\nvisual watermarking (EDHVW) method, Content aware Double-sided Embedding Error\nDiffusion (CaDEED), via considering the expected watermark decoding performance\nwith specific content of the cover images and watermark, different noise\ntolerance abilities of various cover image content and the different importance\nlevels of every pixel (when being perceived) in the secret pattern (watermark).\nTo demonstrate the effectiveness of CaDEED, we propose CaDEED with expectation\nconstraint (CaDEED-EC) and CaDEED-NVF&IF (CaDEED-N&I). Specifically, we build\nCaDEED-EC by only considering the expected performances of specific cover\nimages and watermark. By adopting the noise visibility function (NVF) and\nproposing the importance factor (IF) to assign weights to every embedding\nlocation and watermark pixel, respectively, we build the specific method\nCaDEED-N&I. In the experiments, we select the optimal parameters for NVF and IF\nvia extensive experiments. In both the numerical and visual comparisons, the\nexperimental results demonstrate the superiority of our proposed work.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05726v1"
    },
    {
        "title": "MVP2P: Layer-Dependency-Aware Live MVC Video Streaming over Peer-to-Peer\n  Networks",
        "authors": [
            "Zhao Liu",
            "Niall Murray",
            "Brian Lee",
            "Enda Fallon",
            "Yuansong Qiao"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Multiview video supports observing a scene from different viewpoints. The\nJoint Video Team (JVT) developed H.264/MVC to enhance the compression\nefficiency for multiview video, however, MVC encoded multiview video (MVC\nvideo) still requires high bitrates for transmission. This paper investigates\nlive MVC video streaming over Peer-to-Peer (P2P) networks. The goal is to\nminimize the server bandwidth costs whist ensuring high streaming quality to\npeers. MVC employs intra-view and inter-view prediction structures, which leads\nto a complicated layer dependency relationship. As the peers' outbound\nbandwidth is shared while supplying all the MVC video layers, the bandwidth\nallocation to one MVC layer affects the available outbound bandwidth of the\nother layers. To optimise the utilisation of the peers' outbound bandwidth for\nproviding video layers, a maximum flow based model is proposed which considers\nthe MVC video layer dependency and the layer supplying relationship between\npeers. Based on the model, a layer dependency aware live MVC video streaming\nmethod over a BitTorrent-like P2P network is proposed, named MVP2P. The key\ncomponents of MVP2P include a chunk scheduling strategy and a peer selection\nstrategy for receiving peers, and a bandwidth scheduling algorithm for\nsupplying peers. To evaluate the efficiency of the proposed solution, MVP2P is\ncompared with existing methods considering the constraints of peer bandwidth,\npeer numbers, view switching rates, and peer churns. The test results show that\nMVP2P significantly outperforms the existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.08061v2"
    },
    {
        "title": "Benchmarking Multimodal Sentiment Analysis",
        "authors": [
            "Erik Cambria",
            "Devamanyu Hazarika",
            "Soujanya Poria",
            "Amir Hussain",
            "R. B. V. Subramaanyam"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  We propose a framework for multimodal sentiment analysis and emotion\nrecognition using convolutional neural network-based feature extraction from\ntext and visual modalities. We obtain a performance improvement of 10% over the\nstate of the art by combining visual, text and audio features. We also discuss\nsome major issues frequently ignored in multimodal sentiment analysis research:\nthe role of speaker-independent models, importance of the modalities and\ngeneralizability. The paper thus serve as a new benchmark for further research\nin multimodal sentiment analysis and also demonstrates the different facets of\nanalysis to be considered while performing such tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09538v1"
    },
    {
        "title": "A Time-Frequency Perspective on Audio Watermarking",
        "authors": [
            "Haijian Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Existing audio watermarking methods usually treat the host audio signals of a\nfunction of time or frequency individually, while considering them in the joint\ntime-frequency (TF) domain has received less attention. This paper proposes an\naudio watermarking framework from the perspective of TF analysis. The proposed\nframework treats the host audio signal in the 2-dimensional (2D) TF plane, and\nselects a series of patches within the 2D TF image. These patches correspond to\nthe TF clusters with minimum averaged energy, and are used to form the feature\nvectors for watermark embedding. Classical spread spectrum embedding schemes\nare incorporated in the framework. The feature patches that carry the\nwatermarks only occupy a few TF regions of the host audio signal, thus leading\nto improved imperceptibility property. In addition, since the feature patches\ncontain a neighborhood area of TF representation of audio samples, the\ncorrelations among the samples within a single patch could be exploited for\nimproved robustness against a series of processing attacks. Extensive\nexperiments are carried out to illustrate the effectiveness of the proposed\nsystem, as compared to its counterpart systems. The aim of this work is to shed\nsome light on the notion of audio watermarking in TF feature domain, which may\npotentially lead us to more robust watermarking solutions against malicious\nattacks.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.03156v1"
    },
    {
        "title": "A Comparative Evaluation of Temporal Pooling Methods for Blind Video\n  Quality Assessment",
        "authors": [
            "Zhengzhong Tu",
            "Chia-Ju Chen",
            "Li-Heng Chen",
            "Neil Birkbeck",
            "Balu Adsumilli",
            "Alan C. Bovik"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Many objective video quality assessment (VQA) algorithms include a key step\nof temporal pooling of frame-level quality scores. However, less attention has\nbeen paid to studying the relative efficiencies of different pooling methods on\nno-reference (blind) VQA. Here we conduct a large-scale comparative evaluation\nto assess the capabilities and limitations of multiple temporal pooling\nstrategies on blind VQA of user-generated videos. The study yields insights and\ngeneral guidance regarding the application and selection of temporal pooling\nmodels. In addition, we also propose an ensemble pooling model built on top of\nhigh-performing temporal pooling models. Our experimental results demonstrate\nthe relative efficacies of the evaluated temporal pooling models, using several\npopular VQA algorithms, and evaluated on two recent large-scale natural video\nquality databases. In addition to the new ensemble model, we provide a general\nrecipe for applying temporal pooling of frame-based quality predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.10651v1"
    },
    {
        "title": "Subjective Quality Assessment for YouTube UGC Dataset",
        "authors": [
            "Joong Gon Yim",
            "Yilin Wang",
            "Neil Birkbeck",
            "Balu Adsumilli"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Due to the scale of social video sharing, User Generated Content (UGC) is\ngetting more attention from academia and industry. To facilitate\ncompression-related research on UGC, YouTube has released a large-scale\ndataset. The initial dataset only provided videos, limiting its use in quality\nassessment. We used a crowd-sourcing platform to collect subjective quality\nscores for this dataset. We analyzed the distribution of Mean Opinion Score\n(MOS) in various dimensions, and investigated some fundamental questions in\nvideo quality assessment, like the correlation between full video MOS and\ncorresponding chunk MOS, and the influence of chunk variation in quality score\naggregation.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.12275v1"
    },
    {
        "title": "On the Robustness of the Delay-Based Fingerprint Embedding Scheme",
        "authors": [
            "Shiguo Lian"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  The delay-based fingerprint embedding was recently proposed to support more\nusers in secure media distribution scenario. In this embedding scheme, some\nusers are assigned the same fingerprint code with only different embedding\ndelay. The algorithm's robustness against collusion attacks is investigated.\nHowever, its robustness against common desynchronization attacks, e.g.,\ncropping and time shifting, is not considered. In this paper, desynchronization\nattacks are used to break the delay-based fingerprint embedding algorithm. To\nimprove the robustness, two means are proposed to keep the embedded fingerprint\ncodes synchronized, i.e., adding a synchronization fingerprint and adopting the\nrelative delay to detect users. Analyses and experiments are given to show the\nimprovements.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.0625v1"
    },
    {
        "title": "A Novel Steganography Algorithm for Hiding Text in Image using Five\n  Modulus Method",
        "authors": [
            "Firas A. Jassim"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  The needs for steganographic techniques for hiding secret message inside\nimages have been arise. This paper is to create a practical steganographic\nimplementation to hide text inside grey scale images. The secret message is\nhidden inside the cover image using Five Modulus Method. The novel algorithm is\ncalled (ST-FMM. FMM which consists of transforming all the pixels within the\n5X5 window size into its corresponding multiples of 5. After that, the secret\nmessage is hidden inside the 5X5 window as a non-multiples of 5. Since the\nmodulus of non-multiples of 5 are 1,2,3 and 4, therefore; if the reminder is\none of these, then this pixel represents a secret character. The secret key\nthat has to be sent is the window size. The main advantage of this novel\nalgorithm is to keep the size of the cover image constant while the secret\nmessage increased in size. Peak signal-to-noise ratio is captured for each of\nthe images tested. Based on the PSNR value of each images, the stego image has\nhigh PSNR value. Hence this new steganography algorithm is very efficient to\nhide the data inside the image.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.0642v1"
    },
    {
        "title": "Anisotropic Diffusion for Details Enhancement in Multi-Exposure Image\n  Fusion",
        "authors": [
            "Harbinder Singh",
            "Vinay Kumar",
            "Sunil Bhooshan"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  We develop a multiexposure image fusion method based on texture features,\nwhich exploits the edge preserving and intraregion smoothing property of\nnonlinear diffusion filters based on partial differential equations (PDE). With\nthe captured multiexposure image series, we first decompose images into base\nlayers and detail layers to extract sharp details and fine details,\nrespectively. The magnitude of the gradient of the image intensity is utilized\nto encourage smoothness at homogeneous regions in preference to inhomogeneous\nregions. Then, we have considered texture features of the base layer to\ngenerate a mask (i.e., decision mask) that guides the fusion of base layers in\nmultiresolution fashion. Finally, well-exposed fused image is obtained that\ncombines fused base layer and the detail layers at each scale across all the\ninput exposures. Proposed algorithm skipping complex High Dynamic Range Image\n(HDRI) generation and tone mapping steps to produce detail preserving image for\ndisplay on standard dynamic range display devices. Moreover, our technique is\neffective for blending flash/no-flash image pair and multifocus images, that\nis, images focused on different targets.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.2818v1"
    },
    {
        "title": "Comparison of secure and high capacity color image steganography\n  techniques in RGB and YCbCr domains",
        "authors": [
            "S. Hemalatha",
            "U. Dinesh Acharya",
            "A. Renuka"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Steganography is one of the methods used for secret communication.\nSteganography attempts to hide the existence of the information. The object\nused to hide the secret information is called as cover object. Images are the\nmost popular cover objects used for steganography. Different techniques have to\nbe used for color image steganography and grey scale image steganography since\nthey are stored in different ways. Color image are normally stored with 24 bit\ndepth and grey scale images are stored with 8 bit depth. Color images can hold\nlarge amount of secret information since they have three color components.\nDifferent color spaces namely RGB (Red Green Blue), HSV (Hue, Saturation,\nValue), YUV, YIQ, YCbCr (Luminance, Chrominance) etc. are used to represent\ncolor images. Color image steganography can be done in any color space domain.\nIn this paper color image steganography in RGB and YCbCr domain are compared.\nThe secret information considered is grey scale image. Since RGB is the common\nmethod of representation, hiding secret information in this format is not\nsecure.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.3026v1"
    },
    {
        "title": "Smart Streaming for Online Video Services",
        "authors": [
            "Liang Chen",
            "Yipeng Zhou",
            "Dah Ming Chiu"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Bandwidth consumption is a significant concern for online video service\nproviders. Practical video streaming systems usually use some form of HTTP\nstreaming (progressive download) to let users download the video at a faster\nrate than the video bitrate. Since users may quit before viewing the complete\nvideo, however, much of the downloaded video will be \"wasted\". To the extent\nthat users' departure behavior can be predicted, we develop smart streaming\nthat can be used to improve user QoE with limited server bandwidth or save\nbandwidth cost with unlimited server bandwidth. Through measurement, we extract\ncertain user behavior properties for implementing such smart streaming, and\ndemonstrate its advantage using prototype implementation as well as\nsimulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.4581v4"
    },
    {
        "title": "Robust Video Watermarking Schemes in Phase domain Using Binary Phase\n  Shift Keying",
        "authors": [
            "K. Meenakshi",
            "Ch. Srinivasa Rao",
            "K. Satya Prasad"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  This paper presents a robust video watermarking scheme in Discrete Fourier\nTransform (DFT) and Sequencyordered Complex Hadamard Transform (SCHT). The DFT\nand SCHT coefficients are complex and consist of both magnitude and phase and\nare well suited to adopt phase shift keying techniques to embed the watermark.\nIn the proposed schemes, the phases of DFT and SCHT coefficients are modified\nto convey watermark information using binary phase shift keying in cover video.\nLow amplitude block selection (LABS) is used to improve transparency, amplitude\nboost to improve the resistance of watermark from signal processing and\ncompression attacks and spread spectrum technique is used for encrypting\nwatermark in order to protect it from third party. It is observed that both\nalgorithms showing more or less same robustness but SCHT offers high\ntransparency, simple implementation and less computational cost than DFT.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.1314v1"
    },
    {
        "title": "Reduction of Field Loss by a Video Processing System",
        "authors": [
            "Dr. Timur Mirzoev"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Streaming of 60 de-interlaced fields per second digital uncompressed video\nwith 720x480 resolution without a loss of video fields is one of the desired\ntechnologies by scientists in biomechanics. If it is possible to stream digital\nuncompressed video without dropped video fields, then a sophisticated computer\nanalysis of the transmitted via IEEE 1394a connection video is possible. Such\nprocess is used in biomechanics when it is important to analyze athletes\nperformance via streaming digital uncompressed video to a computer and then\nanalyzing it using specific software such as Arial Performance Analysis\nSystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.2592v1"
    },
    {
        "title": "A blind robust watermarking scheme based on svd and circulant matrices",
        "authors": [
            "Noui Oussama",
            "Noui Lemnouar"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Multimedia security has been the aim point of considerable research activity\nbecause of its wide application area. The major technology to achieve copyright\nprotection, content authentication, access control and multimedia security is\nwatermarking which is the process of embedding data into a multimedia element\nsuch as image or audio, this embedded data can later be extracted from, or\ndetected in the embedded element for different purposes. In this work, a blind\nwatermarking algorithm based on SVD and circulant matrices has been presented.\nEvery circulant matrix is associated with a matrix for which the SVD\ndecomposition coincides with the spectral decomposition. This leads to improve\nthe Chandra algorithm [1], our presentation will include a discussion on the\ndata hiding capacity, watermark transparency and robustness against a wide\nrange of common image processing attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.2952v1"
    },
    {
        "title": "Motion-Compensated Coding and Frame-Rate Up-Conversion: Models and\n  Analysis",
        "authors": [
            "Yehuda Dar",
            "Alfred M. Bruckstein"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Block-based motion estimation (ME) and compensation (MC) techniques are\nwidely used in modern video processing algorithms and compression systems. The\ngreat variety of video applications and devices results in numerous compression\nspecifications. Specifically, there is a diversity of frame-rates and\nbit-rates. In this paper, we study the effect of frame-rate and compression\nbit-rate on block-based ME and MC as commonly utilized in inter-frame coding\nand frame-rate up conversion (FRUC). This joint examination yields a\ncomprehensive foundation for comparing MC procedures in coding and FRUC. First,\nthe video signal is modeled as a noisy translational motion of an image. Then,\nwe theoretically model the motion-compensated prediction of an available and\nabsent frames as in coding and FRUC applications, respectively. The theoretic\nMC-prediction error is further analyzed and its autocorrelation function is\ncalculated for coding and FRUC applications. We show a linear relation between\nthe variance of the MC-prediction error and temporal-distance. While the\naffecting distance in MC-coding is between the predicted and reference frames,\nMC-FRUC is affected by the distance between the available frames used for the\ninterpolation. Moreover, the dependency in temporal-distance implies an inverse\neffect of the frame-rate. FRUC performance analysis considers the prediction\nerror variance, since it equals to the mean-squared-error of the interpolation.\nHowever, MC-coding analysis requires the entire autocorrelation function of the\nerror; hence, analytic simplicity is beneficial. Therefore, we propose two\nconstructions of a separable autocorrelation function for prediction error in\nMC-coding. We conclude by comparing our estimations with experimental results.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.3290v1"
    },
    {
        "title": "Antescofo Intermediate Representation",
        "authors": [
            "Florent Jacquemard",
            "Clément Poncelet Sanchez"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  We describe an intermediate language designed as a medium-level internal\nrepresentation of programs of the interactive music system Antescofo. This\nrepresentation is independent both of the Antescofo source language and of the\narchitecture of the execution platform. It is used in tasks such as\nverification of timings, model-based conformance testing, static control-flow\nanalysis or simulation. This language is essentially a flat representation of\nAntescofo's code, as a finite state machine extended with local and global\nvariables, with delays and with concurrent threads creation. It features a\nsmall number of simple instructions which are either blocking (wait for\nexternal event, signal or duration) or not (variable assignment, message\nemission and control).\n",
        "pdf_link": "http://arxiv.org/pdf/1404.7335v1"
    },
    {
        "title": "Using Facebook for Image Steganography",
        "authors": [
            "Jason Hiney",
            "Tejas Dakve",
            "Krzysztof Szczypiorski",
            "Kris Gaj"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Because Facebook is available on hundreds of millions of desktop and mobile\ncomputing platforms around the world and because it is available on many\ndifferent kinds of platforms (from desktops and laptops running Windows, Unix,\nor OS X to hand held devices running iOS, Android, or Windows Phone), it would\nseem to be the perfect place to conduct steganography. On Facebook, information\nhidden in image files will be further obscured within the millions of pictures\nand other images posted and transmitted daily. Facebook is known to alter and\ncompress uploaded images so they use minimum space and bandwidth when displayed\non Facebook pages. The compression process generally disrupts attempts to use\nFacebook for image steganography. This paper explores a method to minimize the\ndisruption so JPEG images can be used as steganography carriers on Facebook.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.02071v1"
    },
    {
        "title": "StegBlocks: ensuring perfect undetectability of network steganography",
        "authors": [
            "Wojciech Fraczek",
            "Krzysztof Szczypiorski"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The paper presents StegBlocks, which defines a new concept for performing\nundetectable hidden communication. StegBlocks is a general approach for\nconstructing methods of network steganography. In StegBlocks, one has to\ndetermine objects with defined properties which will be used to transfer hidden\nmessages. The objects are dependent on a specific network protocol (or\napplication) used as a carrier for a given network steganography method.\nMoreover, the paper presents the approach to perfect undetectability of network\nsteganography, which was developed based on the rules of undetectability for\ngeneral steganography. The approach to undetectability of network steganography\nwas used to show the possibility of developing perfectly undetectable network\nsteganography methods using the StegBlocks concept.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.02311v1"
    },
    {
        "title": "A Novel Approach for Image Steganography in Spatial Domain",
        "authors": [
            "Fatema Akhter"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This paper presents a new approach for hiding information in digital image in\nspatial domain. In this approach three bits of message is embedded in a pixel\nusing Lucas number system but only one bit plane is allowed for alternation.\nThe experimental results show that the proposed method has the larger capacity\nof embedding data, high peak signal to noise ratio compared to existing methods\nand is hardly detectable for steganolysis algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.03681v1"
    },
    {
        "title": "A QoS Guarantee Strategy for Multimedia Conferencing based on Bayesian\n  Networks",
        "authors": [
            "Junfei Huang",
            "Guochu Shou"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Service Oriented Architecture (SOA) is commonly employed in the design and\nimplementation of web service systems. The key technology to enable media\ncommunications in the context of SOA is the Service Oriented Communication. To\nexploit the advantage of SOA, we design and implement a web-based multimedia\nconferencing system that provides users with a hybrid orchestration of web and\ncommunication services. As the current SOA lacks effective QoS guarantee\nsolutions for multimedia services, the user satisfaction is greatly challenged\nwith QoS violations, e.g., low video PSNR (Peak Signal-to-Noise Ratio) and long\nplayback delay. Motivated by addressing the critical problem, we firstly employ\nthe Business Process Execution Language (BPEL) service engine for the hybrid\nservices orchestration and execution. Secondly, we propose a novel\ncontext-aware approach to quantify and leverage the causal relationships\nbetween QoS metrics and available contexts based on Bayesian networks (CABIN).\nThis approach includes three phases: (1) information discretization, (2) causal\nrelationship profiling, and (3) optimal context tuning. We implement CABIN in a\nreal-life multimedia conferencing system and compare its performance with\nexisting delay and throughput oriented schemes. Experimental results show that\nCABIN outperforms the competing approaches in improving the video quality in\nterms of PSNR. It also provides a one-stop shop controls both the web and\ncommunication services.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.06312v1"
    },
    {
        "title": "Data-driven Approaches for Social Video Distribution",
        "authors": [
            "Zhi Wang"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The Internet has recently witnessed the convergence of online social network\nservices and online video services: users import videos from content sharing\nsites, and propagate them along the social connections by re-sharing them. Such\nsocial behaviors have dramatically reshaped how videos are disseminated, and\nthe users are now actively engaged to be part of the social ecosystem, rather\nthan being passively consumers. Despite the increasingly abundant bandwidth and\ncomputation resources, the ever increasing data volume of user generated video\ncontent and the boundless coverage of socialized sharing have presented\nunprecedented challenges. In this paper, we first presents the challenges in\nsocial-aware video delivery. Then, we present a principal framework for\ndata-driven social video delivery approaches. Moreover, we identify the unique\ncharacteristics of social-aware video access and the social content\npropagation, and closely reveal the design of individual modules and their\nintegration towards enhancing users' experience in the social network context.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.08125v1"
    },
    {
        "title": "A new approach for image compression using normal matrices",
        "authors": [
            "E. Kokabifar",
            "G. B. Loghmani",
            "A. Latif"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  In this paper, we present methods for image compression on the basis of\neigenvalue decomposition of normal matrices. The proposed methods are\nconvenient and self-explanatory, requiring fewer and easier computations as\ncompared to some existing methods. Through the proposed techniques, the image\nis transformed to the space of normal matrices. Then, the properties of\nspectral decomposition are dealt with to obtain compressed images. Experimental\nresults are provided to illustrate the validity of the methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.08811v1"
    },
    {
        "title": "A proposal project for a blind image quality assessment by learning\n  distortions from the full reference image quality assessments",
        "authors": [
            "Stéfane Paris"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This short paper presents a perspective plan to build a null reference image\nquality assessment. Its main goal is to deliver both the objective score and\nthe distortion map for a given distorted image without the knowledge of its\nreference image.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.04354v1"
    },
    {
        "title": "NEWCAST: Anticipating Resource Management and QoE Provisioning for\n  Mobile Video Streaming",
        "authors": [
            "Imen Triki",
            "Rachid El-Azouzi",
            "Majed Haddad"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The knowledge of future throughput variations in mobile networks becomes more\nand more possible today thanks to the rich contextual information provided by\nmobile applications and services and smartphone sensors. It is even likely that\nsuch contextual information, which may include traffic, mobility and radio\nconditions will lead to a novel agile resource management not yet thought of.\nIn this paper, we propose an framework (called NEWCAST) that anticipates the\nthroughput variations to deliver video streaming content. We develop an\noptimization problem that realizes a fundamental trade-off among critical\nmetrics that impact the user's perceptual quality of experience (QoE) and the\ncost of system utilization. Both simulated and real-world throughput traces\ncollected from [1], were carried out to evaluate the performance of NEWCAST. In\nparticular, we show from our numerical results that NEWCAST provides the\nefficiency that the new 5G architectures require in terms of computational\ncomplexity and robustness. We also implement a prototype system of NEWCAST and\nevaluate it in a real environment with a real player to show its efficiency and\nscalability compared to baseline adaptive bitrate algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.05705v4"
    },
    {
        "title": "WiLiTV: A Low-Cost Wireless Framework for Live TV Services",
        "authors": [
            "Rajeev Kumar",
            "Robert S Margolies",
            "Rittwik Jana",
            "Yong Liu",
            "Shivendra Panwar"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  With the evolution of HDTV and Ultra HDTV, the bandwidth requirement for\nIP-based TV content is rapidly increasing. Consumers demand uninterrupted\nservice with a high Quality of Experience (QoE). Service providers are\nconstantly trying to differentiate themselves by innovating new ways of\ndistributing content more efficiently with lower cost and higher penetration.\nIn this work, we propose a cost-efficient wireless framework (WiLiTV) for\ndelivering live TV services, consisting of a mix of wireless access\ntechnologies (e.g. Satellite, WiFi and LTE overlay links). In the proposed\narchitecture, live TV content is injected into the network at a few residential\nlocations using satellite dishes. The content is then further distributed to\nother homes using a house-to-house WiFi network or via an overlay LTE network.\nOur problem is to construct an optimal TV distribution network with the minimum\nnumber of satellite injection points, while preserving the highest QoE, for\ndifferent neighborhood densities. We evaluate the framework using realistic\ntime-varying demand patterns and a diverse set of home location data. Our study\ndemonstrates that the architecture requires 75 - 90% fewer satellite injection\npoints, compared to traditional architectures. Furthermore, we show that most\ncost savings can be obtained using simple and practical relay routing\nsolutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.02669v1"
    },
    {
        "title": "Investigating the role of musical genre in human perception of music\n  stretching resistance",
        "authors": [
            "Jun Chen",
            "Chaokun Wang"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  To stretch a music piece to a given length is a common demand in people's\ndaily lives, e.g., in audio-video synchronization and animation production.\nHowever, it is not always guaranteed that the stretched music piece is\nacceptable for general audience since music stretching suffers from people's\nperceptual artefacts. Over-stretching a music piece will make it uncomfortable\nfor human psychoacoustic hearing. The research on music stretching resistance\nattempts to estimate the maximum stretchability of music pieces to further\navoid over-stretch. It has been observed that musical genres can significantly\nimprove the accuracy of automatic estimation of music stretching resistance,\nbut how musical genres are related to music stretching resistance has never\nbeen explained or studied in detail in the literature. In this paper, the\ncharacteristics of music stretching resistance are compared across different\nmusical genres. It is found that music stretching resistance has strong\nintra-genre cohesiveness and inter-genre discrepancies in the experiments.\nMoreover, the ambiguity and the symmetry of music stretching resistance are\nalso observed in the experimental analysis. These findings lead to a new\nmeasurement on the similarity between different musical genres based on their\nmusic stretching resistance. In addition, the analysis of variance (ANOVA) also\nsupports the findings in this paper by verifying the significance of musical\ngenre in shaping music stretching resistance.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.03274v1"
    },
    {
        "title": "From Photo Streams to Evolving Situations",
        "authors": [
            "Mengfan Tang",
            "Feiping Nie",
            "Siripen Pongpaichet",
            "Ramesh Jain"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Photos are becoming spontaneous, objective, and universal sources of\ninformation. This paper develops evolving situation recognition using photo\nstreams coming from disparate sources combined with the advances of deep\nlearning. Using visual concepts in photos together with space and time\ninformation, we formulate the situation detection into a semi-supervised\nlearning framework and propose new graph-based models to solve the problem. To\nextend the method for unknown situations, we introduce a soft label method\nwhich enables the traditional semi-supervised learning framework to accurately\npredict predefined labels as well as effectively form new clusters. To overcome\nthe noisy data which degrades graph quality, leading to poor recognition\nresults, we take advantage of two kinds of noise-robust norms which can\neliminate the adverse effects of outliers in visual concepts and improve the\naccuracy of situation recognition. Finally, we demonstrate the idea and the\neffectiveness of the proposed model on Yahoo Flickr Creative Commons 100\nMillion.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.05878v1"
    },
    {
        "title": "Projection based advanced motion model for cubic mapping for 360-degree\n  video",
        "authors": [
            "Li Li",
            "Zhu Li",
            "Madhukar Budagavi",
            "Houqiang Li"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  This paper proposes a novel advanced motion model to handle the irregular\nmotion for the cubic map projection of 360-degree video. Since the irregular\nmotion is mainly caused by the projection from the sphere to the cube map, we\nfirst try to project the pixels in both the current picture and reference\npicture from unfolding cube back to the sphere. Then through utilizing the\ncharacteristic that most of the motions in the sphere are uniform, we can\nderive the relationship between the motion vectors of various pixels in the\nunfold cube. The proposed advanced motion model is implemented in the High\nEfficiency Video Coding reference software. Experimental results demonstrate\nthat quite obvious performance improvement can be achieved for the sequences\nwith obvious motions.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.06277v1"
    },
    {
        "title": "Software Defined Media: Virtualization of Audio-Visual Services",
        "authors": [
            "Manabu Tsukada",
            "Keiko Ogawa",
            "Masahiro Ikeda",
            "Takuro Sone",
            "Kenta Niwa",
            "Shoichiro Saito",
            "Takashi Kasuya",
            "Hideki Sunahara",
            "Hiroshi Esaki"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Internet-native audio-visual services are witnessing rapid development. Among\nthese services, object-based audio-visual services are gaining importance. In\n2014, we established the Software Defined Media (SDM) consortium to target new\nresearch areas and markets involving object-based digital media and\nInternet-by-design audio-visual environments. In this paper, we introduce the\nSDM architecture that virtualizes networked audio-visual services along with\nthe development of smart buildings and smart cities using Internet of Things\n(IoT) devices and smart building facilities. Moreover, we design the SDM\narchitecture as a layered architecture to promote the development of innovative\napplications on the basis of rapid advancements in software-defined networking\n(SDN). Then, we implement a prototype system based on the architecture, present\nthe system at an exhibition, and provide it as an SDM API to application\ndevelopers at hackathons. Various types of applications are developed using the\nAPI at these events. An evaluation of SDM API access shows that the prototype\nSDM platform effectively provides 3D audio reproducibility and interactiveness\nfor SDM applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.07452v1"
    },
    {
        "title": "Evaluation of the Performance of Adaptive HTTP Streaming Systems",
        "authors": [
            "Anatoliy Zabrovskiy",
            "Evgeny Petrov",
            "Evgeny Kuzmin",
            "Christian Timmerer"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Adaptive video streaming over HTTP is becoming omnipresent in our daily life.\nIn the past, dozens of research papers have proposed novel approaches to\naddress different aspects of adaptive streaming and a decent amount of player\nimplementations (commercial and open source) are available. However, state of\nthe art evaluations are sometimes superficial as many proposals only\ninvestigate a certain aspect of the problem or focus on a specific platform -\nplayer implementations used in actual services are rarely considered. HTML5 is\nnow available on many platforms and foster the deployment of adaptive media\nstreaming applications. We propose a common evaluation framework for adaptive\nHTML5 players and demonstrate its applicability by evaluating eight different\nplayers which are actually deployed in real-world services.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.02459v1"
    },
    {
        "title": "Predicting TED Talk Ratings from Language and Prosody",
        "authors": [
            "Md Iftekhar Tanveer",
            "Md Kamrul Hassan",
            "Daniel Gildea",
            "M. Ehsan Hoque"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  We use the largest open repository of public speaking---TED Talks---to\npredict the ratings of the online viewers. Our dataset contains over 2200 TED\nTalk transcripts (includes over 200 thousand sentences), audio features and the\nassociated meta information including about 5.5 Million ratings from\nspontaneous visitors of the website. We propose three neural network\narchitectures and compare with statistical machine learning. Our experiments\nreveal that it is possible to predict all the 14 different ratings with an\naverage AUC of 0.83 using the transcripts and prosody features only. The\ndataset and the complete source code is available for further analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.03940v1"
    },
    {
        "title": "Stereoscopic Omnidirectional Image Quality Assessment Based on\n  Predictive Coding Theory",
        "authors": [
            "Zhibo Chen",
            "Jiahua Xu",
            "Chaoyi Lin",
            "Wei Zhou"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Objective quality assessment of stereoscopic omnidirectional images is a\nchallenging problem since it is influenced by multiple aspects such as\nprojection deformation, field of view (FoV) range, binocular vision, visual\ncomfort, etc. Existing studies show that classic 2D or 3D image quality\nassessment (IQA) metrics are not able to perform well for stereoscopic\nomnidirectional images. However, very few research works have focused on\nevaluating the perceptual visual quality of omnidirectional images, especially\nfor stereoscopic omnidirectional images. In this paper, based on the predictive\ncoding theory of the human vision system (HVS), we propose a stereoscopic\nomnidirectional image quality evaluator (SOIQE) to cope with the\ncharacteristics of 3D 360-degree images. Two modules are involved in SOIQE:\npredictive coding theory based binocular rivalry module and multi-view fusion\nmodule. In the binocular rivalry module, we introduce predictive coding theory\nto simulate the competition between high-level patterns and calculate the\nsimilarity and rivalry dominance to obtain the quality scores of viewport\nimages. Moreover, we develop the multi-view fusion module to aggregate the\nquality scores of viewport images with the help of both content weight and\nlocation weight. The proposed SOIQE is a parametric model without necessary of\nregression learning, which ensures its interpretability and generalization\nperformance. Experimental results on our published stereoscopic omnidirectional\nimage quality assessment database (SOLID) demonstrate that our proposed SOIQE\nmethod outperforms state-of-the-art metrics. Furthermore, we also verify the\neffectiveness of each proposed module on both public stereoscopic image\ndatasets and panoramic image datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05165v1"
    },
    {
        "title": "Grounding Object Detections With Transcriptions",
        "authors": [
            "Yasufumi Moriya",
            "Ramon Sanabria",
            "Florian Metze",
            "Gareth J. F. Jones"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  A vast amount of audio-visual data is available on the Internet thanks to\nvideo streaming services, to which users upload their content. However, there\nare difficulties in exploiting available data for supervised statistical models\ndue to the lack of labels. Unfortunately, generating labels for such amount of\ndata through human annotation can be expensive, time-consuming and prone to\nannotation errors. In this paper, we propose a method to automatically extract\nentity-video frame pairs from a collection of instruction videos by using\nspeech transcriptions and videos. We conduct experiments on image recognition\nand visual grounding tasks on the automatically constructed entity-video frame\ndataset of How2. The models will be evaluated on new manually annotated portion\nof How2 dev5 and val set and on the Flickr30k dataset. This work constitutes a\nfirst step towards meta-algorithms capable of automatically construct\ntask-specific training sets.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.06147v2"
    },
    {
        "title": "Probabilistic Tile Visibility-Based Server-Side Rate Adaptation for\n  Adaptive 360-Degree Video Streaming",
        "authors": [
            "Junni Zou",
            "Chenglin Li",
            "Chengming Liu",
            "Qin Yang",
            "Hongkai Xiong",
            "Eckehard Steinbach"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this paper, we study the server-side rate adaptation problem for streaming\ntile-based adaptive 360-degree videos to multiple users who are competing for\ntransmission resources at the network bottleneck. Specifically, we develop a\nconvolutional neural network (CNN)-based viewpoint prediction model to capture\nthe nonlinear relationship between the future and historical viewpoints. A\nLaplace distribution model is utilized to characterize the probability\ndistribution of the prediction error. Given the predicted viewpoint, we then\nmap the viewport in the spherical space into its corresponding planar\nprojection in the 2-D plane, and further derive the visibility probability of\neach tile based on the planar projection and the prediction error probability.\nAccording to the visibility probability, tiles are classified as viewport,\nmarginal and invisible tiles. The server-side tile rate allocation problem for\nmultiple users is then formulated as a non-linear discrete optimization problem\nto minimize the overall received video distortion of all users and the quality\ndifference between the viewport and marginal tiles of each user, subject to the\ntransmission capacity constraints and users' specific viewport requirements. We\ndevelop a steepest descent algorithm to solve this non-linear discrete\noptimization problem, by initializing the feasible starting point in accordance\nwith the optimal solution of its continuous relaxation. Extensive experimental\nresults show that the proposed algorithm can achieve a near-optimal solution,\nand outperforms the existing rate adaptation schemes for tile-based adaptive\n360-video streaming.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.08575v1"
    },
    {
        "title": "An optimal mode selection algorithm for scalable video coding",
        "authors": [
            "L. Balaji",
            "K. K. Thyagharajan",
            "C. Raja",
            "A. Dhanalakshmi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Scalable video coding (SVC) is extended from its predecessor advanced video\ncoding (AVC) because of its flexible transmission to all type of gadgets.\nHowever, SVC is more flexible and scalable than AVC, but it is more complex in\ndetermining the computations than AVC. The traditional full search method in\nthe standard H.264 SVC consumes more encoding time for computation. This\ncomplexity in computation need to be reduced and many fast mode decision (FMD)\nalgorithms were developed, but many fail to balance in all the three measures\nsuch as peak signal to noise ratio (PSNR), encoding time and bit rate. In this\npaper, the proposed optimal mode selection algorithm based on the orientation\nof pixels achieves better time saving, good PSNR and coding efficiency. The\nproposed algorithm is compared with the standard H.264 JSVM reference software\nand found to be 57.44% time saving, 0.43 dB increments in PSNR and 0.23%\ncompression in bit rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03523v1"
    },
    {
        "title": "Exploring Speech Cues in Web-mined COVID-19 Conversational Vlogs",
        "authors": [
            "Kexin Feng",
            "Preeti Zanwar",
            "Amir H. Behzadan",
            "Theodora Chaspari"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The COVID-19 pandemic caused by the novel SARS-Coronavirus-2 (n-SARS-CoV-2)\nhas impacted people's lives in unprecedented ways. During the time of the\npandemic, social vloggers have used social media to actively share their\nopinions or experiences in quarantine. This paper collected videos from YouTube\nto track emotional responses in conversational vlogs and their potential\nassociations with events related to the pandemic. In particular, vlogs uploaded\nfrom locations in New York City were analyzed given that this was one of the\nfirst epicenters of the pandemic in the United States. We observed some common\npatterns in vloggers' acoustic and linguistic features across the time span of\nthe quarantine, which is indicative of changes in emotional reactivity.\nAdditionally, we investigated fluctuations of acoustic and linguistic patterns\nin relation to COVID-19 events in the New York area (e.g. the number of daily\nnew cases, number of deaths, and extension of stay-at-home order and state of\nemergency). Our results indicate that acoustic features, such as\nzero-crossing-rate, jitter, and shimmer, can be valuable for analyzing\nemotional reactivity in social media videos. Our findings further indicate that\nsome of the peaks of the acoustic and linguistic indices align with COVID-19\nevents, such as the peak in the number of deaths and emergency declaration.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.07504v1"
    },
    {
        "title": "A Human-Computer Duet System for Music Performance",
        "authors": [
            "Yuen-Jen Lin",
            "Hsuan-Kai Kao",
            "Yih-Chih Tseng",
            "Ming Tsai",
            "Li Su"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Virtual musicians have become a remarkable phenomenon in the contemporary\nmultimedia arts. However, most of the virtual musicians nowadays have not been\nendowed with abilities to create their own behaviors, or to perform music with\nhuman musicians. In this paper, we firstly create a virtual violinist, who can\ncollaborate with a human pianist to perform chamber music automatically without\nany intervention. The system incorporates the techniques from various fields,\nincluding real-time music tracking, pose estimation, and body movement\ngeneration. In our system, the virtual musician's behavior is generated based\non the given music audio alone, and such a system results in a low-cost,\nefficient and scalable way to produce human and virtual musicians'\nco-performance. The proposed system has been validated in public concerts.\nObjective quality assessment approaches and possible ways to systematically\nimprove the system are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.07816v1"
    },
    {
        "title": "An enhanced performance for H.265/SHVC based on combined AEGBM3D filter\n  and back-propagation neural network",
        "authors": [
            "L. Balaji",
            "K. K. Thyagharajan"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper deals with the latest video coding standard H265 SHVC, a scalable\nextension to High Efficiency Video Coding (HEVC). HEVC introduces new coding\ntools compared to its predecessor and is backward compatible with all types of\nelectronic gadgets. The gadgets with different display capabilities cannot be\noffered the same quality video due to the constraints in transmission bandwidth\nis a major problem. One solution to this problem will be the compression of the\nvideo sequence which is focused in this paper to preserve or increase PSNR\nwhile reducing bit-rate besides a novel method implemented in SHVC encoder. The\nnovel method undergoes a combined AEGBM3D (adaptive edge guided block-matching\nand 3D) filtering and back-propagation technique. The technique includes an\nAEGBM3D filter which avoids spatial redundancy and de-noise frames; hence\nenhancement in PSNR is achieved. The obtained PSNR of the video is compared\nwith the set threshold PSNR to maintain PSNR above the threshold by repeated\nAEGBM3D filtering. The BP technique based on the neural network machine\nlearning approach continually restrains the output if the input block does not\ncontain a feature they were trained to recognize. This frequent control over\nthe output produces few bits; hence reduction in bit-rate is achieved. The\nsimulation results show that the proposed technique delivers an average\nincrement of 0.16 and 0.25dB in PSNR and an average decrement of 28 and 37% in\nbit-rate for 1.5 and 2 times spatial ratios respectively, compared with the\nexisting methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09428v1"
    },
    {
        "title": "MUSE2020 challenge report",
        "authors": [
            "Ruichen Li",
            "JingWen Hu",
            "Shuai Guo",
            "Jinming Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper is a brief report for MUSE2020 challenge. We present our solution\nfor Muse-Wild sub challenge. The aim of this challenge is to investigate\nsentiment analysis method in real-world situation. Our solutions achieve the\nbest CCC performance of 0.4670, 0.3571 for arousal, and valence respectively on\nthe challenge validation set, which outperforms the baseline system with\ncorresponding CCC of 0.3078 and 1506.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.14059v1"
    },
    {
        "title": "Performance Evaluation of Video Communications over 4G Network",
        "authors": [
            "Gaurav Pande"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  With exponential increase in the volumes of video traffic in cellular\nnet-works, there is an increasing need for optimizing the quality of video\ndelivery. 4G networks (Long Term Evolution Advanced or LTE A) are being\nintroduced in many countries worldwide, which allow a downlink speed of upto 1\nGbps and uplink of 100 Mbps over a single base station. In this paper, we\ncharacterize the performance of LTE A physical layer in terms of transmitted\nvideo quality when the channel condi-tions and LTE settings are varied. We test\nthe performance achieved as the channel quality is changed and HARQ features\nare enabled in physical layer. Blocking and blurring metrics were used to model\nimage quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.1887v1"
    },
    {
        "title": "An Adaptive Statistical Non-uniform Quantizer for Detail Wavelet\n  Components in Lossy JPEG2000 Image Compression",
        "authors": [
            "Madhur Srivastava",
            "Satish K. Singh",
            "Prasanta K. Panigrahi"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  The paper presents a non-uniform quantization method for the Detail\ncomponents in the JPEG2000 standard. Incorporating the fact that the\ncoefficients lying towards the ends of the histogram plot of each Detail\ncomponent represent the structural information of an image, the quantization\nstep sizes become smaller at they approach the ends of the histogram plot. The\nvariable quantization step sizes are determined by the actual statistics of the\nwavelet coefficients. Mean and standard deviation are the two statistical\nparameters used iteratively to obtain the variable step sizes. Moreover, the\nmean of the coefficients lying within the step size is chosen as the quantized\nvalue, contrary to the deadzone uniform quantizer which selects the midpoint of\nthe quantization step size as the quantized value. The experimental results of\nthe deadzone uniform quantizer and the proposed non-uniform quantizer are\nobjectively compared by using Mean-Squared Error (MSE) and Mean Structural\nSimilarity Index Measure (MSSIM), to evaluate the quantization error and\nreconstructed image quality, respectively. Subjective analysis of the\nreconstructed images is also carried out. Through the objective and subjective\nassessments, it is shown that the non-uniform quantizer performs better than\nthe deadzone uniform quantizer in the perceptual quality of the reconstructed\nimage, especially at low bitrates. More importantly, unlike the deadzone\nuniform quantizer, the non-uniform quantizer accomplishes better visual quality\nwith a few quantized values.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.1986v3"
    },
    {
        "title": "Quantum Image Representation Through Two-Dimensional Quantum States and\n  Normalized Amplitude",
        "authors": [
            "Madhur Srivastava",
            "Subhayan R. Moulick",
            "Prasanta K. Panigrahi"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  We propose a novel method for image representation in quantum computers,\nwhich uses the two-dimensional (2-D) quantum states to locate each pixel in an\nimage through row-location and column-location vectors for identifying each\npixel location. The quantum state of an image is the linear superposition of\nthe tensor product of the m-qubits row-location vector and the n-qubits\ncolumn-location vector of each pixel. It enables the natural quantum\nrepresentation of rectangular images that other methods lack. The\namplitude/intensity of each pixel is incorporated into the coefficient values\nof the pixel's quantum state, without using any qubits. Due to the fact that\nlinear superposition, tensor product and qubits form the fundamental basis of\nquantum computing, the proposed method presents the machine level\nrepresentation of images on quantum computers. Unlike other methods, this\nmethod is a pure quantum representation without any classical components.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.2251v4"
    },
    {
        "title": "Wave Atom Based Watermarking",
        "authors": [
            "Ijaz Bukhari",
            " Nuhman-ul-Haq",
            "Khizar Hyat"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Watermarking helps in ensuring originality, ownership and copyrights of a\ndigital image. This paper aims at embedding a Watermark in an image using Wave\nAtom Transform. Preference of Wave Atoms on other transformations has been due\nto its sparser expansion, adaptability to the direction of local pattern, and\nsharp frequency localization. In this scheme, we had tried to spread the\nwatermark in an image so that the information at one place is very small and\nundetectable. In order to extract the watermark and verify ownership of an\nimage, one would have the advantage of prior knowledge of embedded locations. A\nnoise of high amplitude will be needed to be added to the image for watermark\ndistortion. Furthermore, the information spread will ensure the robustness of\nthe watermark data. The proposed scheme has the ability to withstand malicious\noperations and attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.3021v2"
    },
    {
        "title": "Using Bias Optimization for Reversible Data Hiding Using Image\n  Interpolation",
        "authors": [
            "Andrew Rudder",
            "Wayne Goodridge",
            "Shareeda Mohammed"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  In this paper, we propose a reversible data hiding method in the spatial\ndomain for compressed grayscale images. The proposed method embeds secret bits\ninto a compressed thumbnail of the original image by using a novel\ninterpolation method and the Neighbour Mean Interpolation (NMI) technique as\nscaling up to the original image occurs. Experimental results presented in this\npaper show that the proposed method has significantly improved embedding\ncapacities over the approach proposed by Jung and Yoo.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.4102v1"
    },
    {
        "title": "Image Credibility Analysis with Effective Domain Transferred Deep\n  Networks",
        "authors": [
            "Zhiwei Jin",
            "Juan Cao",
            "Jiebo Luo",
            "Yongdong Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Numerous fake images spread on social media today and can severely jeopardize\nthe credibility of online content to public. In this paper, we employ deep\nnetworks to learn distinct fake image related features. In contrast to\nauthentic images, fake images tend to be eye-catching and visually striking.\nCompared with traditional visual recognition tasks, it is extremely challenging\nto understand these psychologically triggered visual patterns in fake images.\nTraditional general image classification datasets, such as ImageNet set, are\ndesigned for feature learning at the object level but are not suitable for\nlearning the hyper-features that would be required by image credibility\nanalysis. In order to overcome the scarcity of training samples of fake images,\nwe first construct a large-scale auxiliary dataset indirectly related to this\ntask. This auxiliary dataset contains 0.6 million weakly-labeled fake and real\nimages collected automatically from social media. Through an AdaBoost-like\ntransfer learning algorithm, we train a CNN model with a few instances in the\ntarget training set and 0.6 million images in the collected auxiliary set. This\nlearning algorithm is able to leverage knowledge from the auxiliary set and\ngradually transfer it to the target task. Experiments on a real-world testing\nset show that our proposed domain transferred CNN model outperforms several\ncompeting baselines. It obtains superiror results over transfer learning\nmethods based on the general ImageNet set. Moreover, case studies show that our\nproposed method reveals some interesting patterns for distinguishing fake and\nauthentic images.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.05328v1"
    },
    {
        "title": "Binary Subspace Coding for Query-by-Image Video Retrieval",
        "authors": [
            "Ruicong Xu",
            "Yang Yang",
            "Yadan Luo",
            "Fumin Shen",
            "Zi Huang",
            "Heng Tao Shen"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The query-by-image video retrieval (QBIVR) task has been attracting\nconsiderable research attention recently. However, most existing methods\nrepresent a video by either aggregating or projecting all its frames into a\nsingle datum point, which may easily cause severe information loss. In this\npaper, we propose an efficient QBIVR framework to enable an effective and\nefficient video search with image query. We first define a\nsimilarity-preserving distance metric between an image and its orthogonal\nprojection in the subspace of the video, which can be equivalently transformed\nto a Maximum Inner Product Search (MIPS) problem.\n  Besides, to boost the efficiency of solving the MIPS problem, we propose two\nasymmetric hashing schemes, which bridge the domain gap of images and videos.\nThe first approach, termed Inner-product Binary Coding (IBC), preserves the\ninner relationships of images and videos in a common Hamming space. To further\nimprove the retrieval efficiency, we devise a Bilinear Binary Coding (BBC)\napproach, which employs compact bilinear projections instead of a single large\nprojection matrix. Extensive experiments have been conducted on four real-world\nvideo datasets to verify the effectiveness of our proposed approaches as\ncompared to the state-of-the-arts.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.01657v1"
    },
    {
        "title": "Algorithmic Analysis of Invisible Video Watermarking using LSB Encoding\n  Over a Client-Server Framework",
        "authors": [
            "Poorna Banerjee Dasgupta"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Video watermarking is extensively used in many media-oriented applications\nfor embedding watermarks, i.e. hidden digital data, in a video sequence to\nprotect the video from illegal copying and to identify manipulations made in\nthe video. In case of an invisible watermark, the human eye can not perceive\nany difference in the video, but a watermark extraction application can read\nthe watermark and obtain the embedded information. Although numerous\nmethodologies exist for embedding watermarks, many of them have shortcomings\nwith respect to performance efficiency, especially over a distributed network.\nThis paper proposes and analyses a 2-bit Least Significant Bit (LSB) parallel\nalgorithmic approach for achieving performance efficiency to watermark and\ndistribute videos over a client-server framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.04688v1"
    },
    {
        "title": "Semantic Perceptual Image Compression using Deep Convolution Networks",
        "authors": [
            "Aaditya Prakash",
            "Nick Moran",
            "Solomon Garber",
            "Antonella DiLillo",
            "James Storer"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  It has long been considered a significant problem to improve the visual\nquality of lossy image and video compression. Recent advances in computing\npower together with the availability of large training data sets has increased\ninterest in the application of deep learning cnns to address image recognition\nand image processing tasks. Here, we present a powerful cnn tailored to the\nspecific task of semantic image understanding to achieve higher visual quality\nin lossy compression. A modest increase in complexity is incorporated to the\nencoder which allows a standard, off-the-shelf jpeg decoder to be used. While\njpeg encoding may be optimized for generic images, the process is ultimately\nunaware of the specific content of the image to be compressed. Our technique\nmakes jpeg content-aware by designing and training a model to identify multiple\nsemantic regions in a given image. Unlike object detection techniques, our\nmodel does not require labeling of object positions and is able to identify\nobjects in a single pass. We present a new cnn architecture directed\nspecifically to image compression, which generates a map that highlights\nsemantically-salient regions so that they can be encoded at higher quality as\ncompared to background regions. By adding a complete set of features for every\nclass, and then taking a threshold over the sum of all feature activations, we\ngenerate a map that highlights semantically-salient regions so that they can be\nencoded at a better quality compared to background regions. Experiments are\npresented on the Kodak PhotoCD dataset and the MIT Saliency Benchmark dataset,\nin which our algorithm achieves higher visual quality for the same compressed\nsize.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08712v2"
    },
    {
        "title": "Creating A Multi-track Classical Musical Performance Dataset for\n  Multimodal Music Analysis: Challenges, Insights, and Applications",
        "authors": [
            "Bochen Li",
            "Xinzhao Liu",
            "Karthik Dinesh",
            "Zhiyao Duan",
            "Gaurav Sharma"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  We introduce a dataset for facilitating audio-visual analysis of music\nperformances. The dataset comprises 44 simple multi-instrument classical music\npieces assembled from coordinated but separately recorded performances of\nindividual tracks. For each piece, we provide the musical score in MIDI format,\nthe audio recordings of the individual tracks, the audio and video recording of\nthe assembled mixture, and ground-truth annotation files including frame-level\nand note-level transcriptions. We describe our methodology for the creation of\nthe dataset, particularly highlighting our approaches for addressing the\nchallenges involved in maintaining synchronization and expressiveness. We\ndemonstrate the high quality of synchronization achieved with our proposed\napproach by comparing the dataset with existing widely-used music audio\ndatasets.\n  We anticipate that the dataset will be useful for the development and\nevaluation of existing music information retrieval (MIR) tasks, as well as for\nnovel multi-modal tasks. We benchmark two existing MIR tasks (multi-pitch\nanalysis and score-informed source separation) on the dataset and compare with\nother existing music audio datasets. Additionally, we consider two novel\nmulti-modal MIR tasks (visually informed multi-pitch analysis and polyphonic\nvibrato analysis) enabled by the dataset and provide evaluation measures and\nbaseline systems for future comparisons (from our recent work). Finally, we\npropose several emerging research directions that the dataset enables.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08727v3"
    },
    {
        "title": "Efficient Interactive Search for Geo-tagged Multimedia Data",
        "authors": [
            "Jun Long",
            "Lei Zhu",
            "Chengyuan Zhang",
            "Zhan Yang",
            "Yunwu Lin",
            "Ruipeng Chen"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Due to the advances in mobile computing and multimedia techniques, there are\nvast amount of multimedia data with geographical information collected in\nmultifarious applications. In this paper, we propose a novel type of image\nsearch named interactive geo-tagged image search which aims to find out a set\nof images based on geographical proximity and similarity of visual content, as\nwell as the preference of users. Existing approaches for spatial keyword query\nand geo-image query cannot address this problem effectively since they do not\nconsider these three type of information together for query. In order to solve\nthis challenge efficiently, we propose the definition of interactive top-$k$\ngeo-tagged image query and then present a framework including candidate search\nstage , interaction stage and termination stage. To enhance the searching\nefficiency in a large-scale database, we propose the candidate search algorithm\nnamed GI-SUPER Search based on a new notion called superior relationship and\nGIR-Tree, a novel index structure. Furthermore, two candidate selection methods\nare proposed for learning the preferences of the user during the interaction.\nAt last, the termination procedure and estimation procedure are introduced in\nbrief. Experimental evaluation on real multimedia dataset demonstrates that our\nsolution has a really high performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.00571v1"
    },
    {
        "title": "Steganography Security: Principle and Practice",
        "authors": [
            "Yan Ke",
            "Jia Liu",
            "Min-qing Zhang",
            "Ting-ting Su",
            "Xiao-yuan Yang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  This paper focuses on several theoretical issues and principles in\nsteganography security, and defines four security levels by analyzing the\ncorresponding algorithm instances. In the theoretical analysis, we discuss the\ndifferences between steganography security and watermarking security. The two\nnecessary conditions for the steganography security are obtained. Under the\ncurrent technology situation, we then analyze the indistinguishability of the\ncover and stego-cover, and consider that the steganography security should rely\non the key secrecy with algorithms open. By specifying the role of key in\nsteganography, the necessary conditions for a secure steganography algorithm in\ntheory are formally presented. When analyzing the security instances, we have\nclassified the steganalysis attacks according to their variable access to the\nsteganography system, and then defined the four security levels. The higher\nlevel security one has, the higher level attacks one can resist. We have also\npresented algorithm instances based on current technical conditions, and\nanalyzed their data hiding process, security level, and practice requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.03618v2"
    },
    {
        "title": "Multiuser Video Streaming Rate Adaptation: A Physical Layer\n  Resource-Aware Deep Reinforcement Learning Approach",
        "authors": [
            "Kexin Tang",
            "Nuowen Kan",
            "Junni Zou",
            "Xiao Fu",
            "Mingyi Hong",
            "Hongkai Xiong"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  We consider a multi-user video streaming service optimization problem over a\ntime-varying and mutually interfering multi-cell wireless network. The key\nresearch challenge is to appropriately adapt each user's video streaming rate\naccording to the radio frequency environment (e.g., channel fading and\ninterference level) and service demands (e.g., play request), so that the\nusers' long-term experience for watching videos can be optimized. To address\nthe above challenge, we propose a novel two-level cross-layer optimization\nframework for multiuser adaptive video streaming over wireless networks. The\nkey idea is to jointly design the physical layer optimization-based beamforming\nscheme (performed at the base stations) and the application layer Deep\nReinforcement Learning (DRL)-based scheme (performed at the user terminals), so\nthat a highly complex multi-user, cross-layer, time-varying video streaming\nproblem can be decomposed into relatively simple problems and solved\neffectively. Our strategy represents a significant departure for the existing\nschemes where either short-term user experience optimization is considered, or\nonly single-user point-to-point long-term optimization is considered. Extensive\nsimulations based on real-data sets show that the proposed cross-layer design\nis effective and promising.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.00637v1"
    },
    {
        "title": "Vignette: Perceptual Compression for Video Storage and Processing\n  Systems",
        "authors": [
            "Amrita Mazumdar",
            "Brandon Haynes",
            "Magdalena Balazinska",
            "Luis Ceze",
            "Alvin Cheung",
            "Mark Oskin"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Compressed videos constitute 70% of Internet traffic, and video upload growth\nrates far outpace compute and storage improvement trends. Past work in\nleveraging perceptual cues like saliency, i.e., regions where viewers focus\ntheir perceptual attention, reduces compressed video size while maintaining\nperceptual quality, but requires significant changes to video codecs and\nignores the data management of this perceptual information.\n  In this paper, we propose Vignette, a compression technique and storage\nmanager for perception-based video compression. Vignette complements\noff-the-shelf compression software and hardware codec implementations.\nVignette's compression technique uses a neural network to predict saliency\ninformation used during transcoding, and its storage manager integrates\nperceptual information into the video storage system to support a perceptual\ncompression feedback loop. Vignette's saliency-based optimizations reduce\nstorage by up to 95% with minimal quality loss, and Vignette videos lead to\npower savings of 50% on mobile phones during video playback. Our results\ndemonstrate the benefit of embedding information about the human visual system\ninto the architecture of video storage systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.01372v1"
    },
    {
        "title": "Development of Video Frame Enhancement Technique Using Pixel Intensity\n  Analysis",
        "authors": [
            "H. A. Abdulkareem",
            "A. M. S. Tekanyi",
            "I. Yau",
            "B. O. Sadiq"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  This paper developed a brightness enhancement technique for video frame pixel\nintensity improvement. Frames extracted from the six sample video data used in\nthis work were stored in the form of images in a buffer. Noise was added to the\nextracted image frames to vary the intensity of their pixels so that the pixel\nvalues of the noisy images differ from their true values in order to determine\nthe efficiency of the developed technique. Simulation results showed that, the\ndeveloped technique was efficient with an improved pixel intensity and\nhistogram distribution. The Peak to Signal Noise Ratio evaluation showed that\nthe efficiency of the developed technique for both grayscale and coloured video\nframes were improved by PSNR of 12.45%, 16.32%, 27.57% and 19.83% over the grey\nlevel colour (black and white) for the NAELS1.avi, NAELS2.avi, NTA1.avi and\nNTA2.avi respectively. Also, a percentage improvement of 28.93% and 31.68% were\nobtained for the coloured image over the grey level image for Akiyo.avi and\nForman.avi benchmark video frame, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.04985v1"
    },
    {
        "title": "A multimodal movie review corpus for fine-grained opinion mining",
        "authors": [
            "Alexandre Garcia",
            "Slim Essid",
            "Florence d'Alché-Buc",
            "Chloé Clavel"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this paper, we introduce a set of opinion annotations for the POM movie\nreview dataset, composed of 1000 videos. The annotation campaign is motivated\nby the development of a hierarchical opinion prediction framework allowing one\nto predict the different components of the opinions (e.g. polarity and aspect)\nand to identify the corresponding textual spans. The resulting annotations have\nbeen gathered at two granularity levels: a coarse one (opinionated span) and a\nfiner one (span of opinion components). We introduce specific categories in\norder to make the annotation of opinions easier for movie reviews. For example,\nsome categories allow the discovery of user recommendation and preference in\nmovie reviews. We provide a quantitative analysis of the annotations and report\nthe inter-annotator agreement under the different levels of granularity. We\nprovide thus the first set of ground-truth annotations which can be used for\nthe task of fine-grained multimodal opinion prediction. We provide an analysis\nof the data gathered through an inter-annotator study and show that a linear\nstructured predictor learns meaningful features even for the prediction of\nscarce labels. Both the annotations and the baseline system are made publicly\navailable. https://github.com/eusip/POM/\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10102v2"
    },
    {
        "title": "SmartBullets: A Cloud-Assisted Bullet Screen Filter based on Deep\n  Learning",
        "authors": [
            "Haoran Niu",
            "Jiangnan Li",
            "Yu Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Bullet-screen is a technique that enables the website users to send real-time\ncomment `bullet' cross the screen. Compared with the traditional review of a\nvideo, bullet-screen provides new features of feeling expression to video\nwatching and more iterations between video viewers. However, since all the\ncomments from the viewers are shown on the screen publicly and simultaneously,\nsome low-quality bullets will reduce the watching enjoyment of the users.\nAlthough the bullet-screen video websites have provided filter functions based\non regular expression, bad bullets can still easily pass the filter through\nmaking a small modification.\n  In this paper, we present SmartBullets, a user-centered bullet-screen filter\nbased on deep learning techniques. A convolutional neural network is trained as\nthe classifier to determine whether a bullet need to be removed according to\nits quality. Moreover, to increase the scalability of the filter, we employ a\ncloud-assisted framework by developing a backend cloud server and a front-end\nbrowser extension. The evaluation of 40 volunteers shows that SmartBullets can\neffectively remove the low-quality bullets and improve the overall watching\nexperience of viewers.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.05925v1"
    },
    {
        "title": "Statistical Learning Based Congestion Control for Real-time Video\n  Communication",
        "authors": [
            "Tongyu Dai",
            "Xinggong Zhang",
            "Yihang Zhang",
            "Zongming Guo"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  With the increasing demands on interactive video applications, how to adapt\nvideo bit rate to avoid network congestion has become critical, since\ncongestion results in self-inflicted delay and packet loss which deteriorate\nthe quality of real-time video service. The existing congestion control is hard\nto simultaneously achieve low latency, high throughput, good adaptability and\nfair bandwidth allocation, mainly because of the hardwired control strategy and\negocentric convergence objective. To address these issues, we propose an\nend-to-end statistical learning based congestion control, named Iris. By\nexploring the underlying principles of self-inflicted delay, we reveal that\ncongestion delay is determined by sending rate, receiving rate and network\nstatus, which inspires us to control video bit rate using a\nstatistical-learning congestion control model. The key idea of Iris is to force\nall flows to converge to the same queue load, and adjust the bit rate by the\nmodel. All flows keep a small and fixed number of packets queuing in the\nnetwork, thus the fair bandwidth allocation and low latency are both achieved.\nBesides, the adjustment step size of sending rate is updated by online\nlearning, to better adapt to dynamically changing networks. We carried out\nextensive experiments to evaluate the performance of Iris, with the\nimplementations of transport layer (UDP) and application layer (QUIC)\nrespectively. The testing environment includes emulated network, real-world\nInternet and commercial LTE networks. Compared against TCP flavors and\nstate-of-the-art protocols, Iris is able to achieve high bandwidth utilization,\nlow latency and good fairness concurrently. Especially over QUIC, Iris is able\nto increase the video bitrate up to 25%, and PSNR up to 1dB.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.05998v2"
    },
    {
        "title": "Reactive Video Caching via long-short-term fusion approach",
        "authors": [
            "Rui-Xiao Zhang",
            "Tianchi Huang",
            "Chenglei Wu",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Video caching has been a basic network functionality in today's network\narchitectures. Although the abundance of caching replacement algorithms has\nbeen proposed recently, these methods all suffer from a key limitation: due to\ntheir immature rules, inaccurate feature engineering or unresponsive model\nupdate, they cannot strike a balance between the long-term history and\nshort-term sudden events. To address this concern, we propose LA-E2, a\nlong-short-term fusion caching replacement approach, which is based on a\nlearning-aided exploration-exploitation process. Specifically, by effectively\ncombining the deep neural network (DNN) based prediction with the online\nexploitation-exploration process through a \\emph{top-k} method, LA-E2 can both\nmake use of the historical information and adapt to the constantly changing\npopularity responsively. Through the extensive experiments in two real-world\ndatasets, we show that LA-E2 can achieve state-of-the-art performance and\ngeneralize well. Especially when the cache size is small, our approach can\noutperform the baselines by 17.5\\%-68.7\\% higher in total hit rate.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.06650v1"
    },
    {
        "title": "Economical Caching for Scalable Videos in Cache-enabled Heterogeneous\n  Networks",
        "authors": [
            "Xuewei Zhang",
            "Tiejun Lv",
            "Yuan Ren",
            "Wei Ni",
            "Norman C. Beaulieu",
            "Y. Jay Guo"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  We develop the optimal economical caching schemes in cache-enabled\nheterogeneous networks, while delivering multimedia video services with\npersonalized viewing qualities to mobile users. By applying scalable video\ncoding (SVC), each video file to be requested is divided into one base layer\n(BL) and several enhancement layers (ELs). In order to assign different\ntransmission tasks, the serving small-cell base stations (SBSs) are grouped\ninto K clusters. The SBSs are able to cache and cooperatively transmit BL and\nEL contents to the user. We analytically derive the expressions for successful\ntransmission probability and ergodic service rate, and then the closed form of\nEConomical Efficiency (ECE) is obtained. In order to enhance the ECE\nperformance, we formulate the ECE optimization problems for two cases. In the\nfirst case, with equal cache size equipped at each SBS, the layer caching\nindicator is determined. Since this problem is NP-hard, after the l0-norm\napproximation, the discrete optimization variables are relaxed to be\ncontinuous, and this relaxed problem is convex. Next, based on the optimal\nsolution derived from the relaxed problem, we devise a greedystrategy based\nheuristic algorithm to achieve the near-optimal layer caching indicators. In\nthe second case, the cache size for each SBS, the layer size and the layer\ncaching indicator are jointly optimized. This problem is a mixed integer\nprogramming problem, which is more challenging. To effectively solve this\nproblem, the original ECE maximization problem is divided into two subproblems.\nThese two subproblems are iteratively solved until the original optimization\nproblem is convergent. Numerical results verify the correctness of theoretical\nderivations. Additionally, compared to the most popular layer placement\nstrategy, the performance superiority of the proposed SVC-based caching schemes\nis testified.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08765v1"
    },
    {
        "title": "Multiple reconstruction compression framework based on PNG image",
        "authors": [
            "Zhiqing Lu",
            "Zhaoxia Yin",
            "Bin Luo"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  It is shown that neural networks (NNs) achieve excellent performances in\nimage compression and reconstruction. However, there are still many\nshortcomings in the practical application, which eventually lead to the loss of\nneural network image processing ability. Based on this, this paper proposes a\njoint framework based on neural network and zoom compression. The framework\nfirst encodes the incoming PNG or JPEG image information, and then the image is\nconverted into binary input decoder to reconstruct the intermediate state\nimage, next we import the intermediate state image into the zooming compressor\nand re-pressurize it, and reconstruct the final image. From the experimental\nresults, this method can better process the digital image and suppress the\nreverse expansion problem, and the compression effect can be improved by 4 to\n10 times as much as that of using RNN alone, showing better ability in\napplication. In this paper, the method is transmitted over a digital image, the\neffect is far better than the existing compression method alone, the Human\nvisual system can not feel the change of the effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08967v3"
    },
    {
        "title": "EncryptGAN: Image Steganography with Domain Transform",
        "authors": [
            "Ziqiang Zheng",
            "Hongzhi Liu",
            "Zhibin Yu",
            "Haiyong Zheng",
            "Yang Wu",
            "Yang Yang",
            "Jianbo Shi"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  We propose an image steganographic algorithm called EncryptGAN, which\ndisguises private image communication in an open communication channel. The\ninsight is that content transform between two very different domains (e.g.,\nface to flower) allows one to hide image messages in one domain (face) and\ncommunicate using its counterpart in another domain (flower). The key\ningredient in our method, unlike related approaches, is a specially trained\nnetwork to extract transformed images from both domains and use them as the\npublic and private keys. We ensure the image communication remain secret except\nfor the intended recipient even when the content transformation networks are\nexposed.\n  To communicate, one directly pastes the `message' image onto a larger public\nkey image (face). Depending on the location and content of the message image,\nthe `disguise' image (flower) alters its appearance and shape while maintaining\nits overall objectiveness (flower). The recipient decodes the alternated image\nto uncover the original image message using its message image key. We implement\nthe entire procedure as a constrained Cycle-GAN, where the public and the\nprivate key generating network is used as an additional constraint to the cycle\nconsistency. Comprehensive experimental results show our EncryptGAN outperforms\nthe state-of-arts in terms of both encryption and security measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.11582v2"
    },
    {
        "title": "Optimizing Adaptive Video Streaming in Mobile Networks via Online\n  Learning",
        "authors": [
            "Theodoros Karagkioules",
            "Georgios S. Paschos",
            "Nikolaos Liakopoulos",
            "Attilio Fiandrotti",
            "Dimitrios Tsilimantos",
            "Marco Cagnazzo"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this paper, we propose a novel algorithm for video rate adaptation in HTTP\nAdaptive Streaming (HAS), based on online learning. The proposed algorithm,\nnamed Learn2Adapt (L2A), is shown to provide a robust rate adaptation strategy\nwhich, unlike most of the state-of-the-art techniques, does not require\nparameter tuning, channel model assumptions or application-specific\nadjustments. These properties make it very suitable for mobile users, who\ntypically experience fast variations in channel characteristics. Simulations\nshow that L2A improves on the overall Quality of Experience (QoE) and in\nparticular the average streaming rate, a result obtained independently of the\nchannel and application scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.11705v2"
    },
    {
        "title": "TS-RNN: Text Steganalysis Based on Recurrent Neural Networks",
        "authors": [
            "Zhongliang Yang",
            "Ke Wang",
            "Jian Li",
            "Yongfeng Huang",
            "Yu-Jin Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  With the rapid development of natural language processing technologies, more\nand more text steganographic methods based on automatic text generation\ntechnology have appeared in recent years. These models use the powerful\nself-learning and feature extraction ability of the neural networks to learn\nthe feature expression of massive normal texts. Then they can automatically\ngenerate dense steganographic texts which conform to such statistical\ndistribution based on the learned statistical patterns. In this paper, we\nobserve that the conditional probability distribution of each word in the\nautomatically generated steganographic texts will be distorted after embedded\nwith hidden information. We use Recurrent Neural Networks (RNNs) to extract\nthese feature distribution differences and then classify those features into\ncover text and stego text categories. Experimental results show that the\nproposed model can achieve high detection accuracy. Besides, the proposed model\ncan even make use of the subtle differences of the feature distribution of\ntexts to estimate the amount of hidden information embedded in the generated\nsteganographic text.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.13087v1"
    },
    {
        "title": "CIS-Net: A Novel CNN Model for Spatial Image Steganalysis via Cover\n  Image Suppression",
        "authors": [
            "Songtao Wu",
            "Sheng-hua Zhong",
            "Yan Liu",
            "Mengyuan Liu"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Image steganalysis is a special binary classification problem that aims to\nclassify natural cover images and suspected stego images which are the results\nof embedding very weak secret message signals into covers. How to effectively\nsuppress cover image content and thus make the classification of cover images\nand stego images easier is the key of this task. Recent researches show that\nConvolutional Neural Networks (CNN) are very effective to detect steganography\nby learning discriminative features between cover images and their stegos.\nSeveral deep CNN models have been proposed via incorporating domain knowledge\nof image steganography/steganalysis into the design of the network and achieve\nstate of the art performance on standard database. Following such direction, we\npropose a novel model called Cover Image Suppression Network (CIS-Net), which\nimproves the performance of spatial image steganalysis by suppressing cover\nimage content as much as possible in model learning. Two novel layers, the\nSingle-value Truncation Layer (STL) and Sub-linear Pooling Layer (SPL), are\nproposed in this work. Specifically, STL truncates input values into a same\nthreshold when they are out of a predefined interval. Theoretically, we have\nproved that STL can reduce the variance of input feature map without\ndeteriorating useful information. For SPL, it utilizes sub-linear power\nfunction to suppress large valued elements introduced by cover image contents\nand aggregates weak embedded signals via average pooling. Extensive experiments\ndemonstrate the proposed network equipped with STL and SPL achieves better\nperformance than rich model classifiers and existing CNN models on challenging\nsteganographic algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06540v1"
    },
    {
        "title": "Enhanced Spatially Interleaved Techniques for Multi-View Distributed\n  Video Coding",
        "authors": [
            "Nantheera Anantrasirichai",
            "Dimitris Agrafiotis"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  This paper presents a multi-view distributed video coding framework for\nindependent camera encoding and centralized decoding. Spatio-temporal-view\nconcealment methods are developed that exploit the interleaved nature of the\nemployed hybrid KEY/Wyner-Ziv frames for block-wise generation of the side\ninformation (SI). We study a number of view concealment methods and develop a\njoint approach that exploits all available correlation for forming the side\ninformation. We apply a diversity technique for fusing multiple such\npredictions thereby achieving more reliable results. We additionally introduce\nsystems enhancements for further improving the rate distortion performance\nthrough selective feedback, inter-view bitplane projection and frame\nsubtraction. Results show a significant improvement in performance relative to\nH.264 intra coding of up to 25% reduction in bitrate or equivalently 2.5 dB\nincrease in PSNR.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.07854v1"
    },
    {
        "title": "Look, Read and Feel: Benchmarking Ads Understanding with Multimodal\n  Multitask Learning",
        "authors": [
            "Huaizheng Zhang",
            "Yong Luo",
            "Qiming Ai",
            "Yonggang Wen"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Given the massive market of advertising and the sharply increasing online\nmultimedia content (such as videos), it is now fashionable to promote\nadvertisements (ads) together with the multimedia content. It is exhausted to\nfind relevant ads to match the provided content manually, and hence, some\nautomatic advertising techniques are developed. Since ads are usually hard to\nunderstand only according to its visual appearance due to the contained visual\nmetaphor, some other modalities, such as the contained texts, should be\nexploited for understanding. To further improve user experience, it is\nnecessary to understand both the topic and sentiment of the ads. This motivates\nus to develop a novel deep multimodal multitask framework to integrate multiple\nmodalities to achieve effective topic and sentiment prediction simultaneously\nfor ads understanding. In particular, our model first extracts multimodal\ninformation from ads and learn high-level and comparable representations. The\nvisual metaphor of the ad is decoded in an unsupervised manner. The obtained\nrepresentations are then fed into the proposed hierarchical multimodal\nattention modules to learn task-specific representations for final prediction.\nA multitask loss function is also designed to train both the topic and\nsentiment prediction models jointly in an end-to-end manner. We conduct\nextensive experiments on the latest and large advertisement dataset and achieve\nstate-of-the-art performance for both prediction tasks. The obtained results\ncould be utilized as a benchmark for ads understanding.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10248v2"
    },
    {
        "title": "Hiding Data in Images Using Cryptography and Deep Neural Network",
        "authors": [
            "Kartik Sharma",
            "Ashutosh Aggarwal",
            "Tanay Singhania",
            "Deepak Gupta",
            "Ashish Khanna"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Steganography is an art of obscuring data inside another quotidian file of\nsimilar or varying types. Hiding data has always been of significant importance\nto digital forensics. Previously, steganography has been combined with\ncryptography and neural networks separately. Whereas, this research combines\nsteganography, cryptography with the neural networks all together to hide an\nimage inside another container image of the larger or same size. Although the\ncryptographic technique used is quite simple, but is effective when convoluted\nwith deep neural nets. Other steganography techniques involve hiding data\nefficiently, but in a uniform pattern which makes it less secure. This method\ntargets both the challenges and make data hiding secure and non-uniform.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10413v2"
    },
    {
        "title": "SUR-FeatNet: Predicting the Satisfied User Ratio Curvefor Image\n  Compression with Deep Feature Learning",
        "authors": [
            "Hanhe Lin",
            "Vlad Hosu",
            "Chunling Fan",
            "Yun Zhang",
            "Yuchen Mu",
            "Raouf Hamzaoui",
            "Dietmar Saupe"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The satisfied user ratio (SUR) curve for a lossy image compression scheme,\ne.g., JPEG, characterizes the complementary cumulative distribution function of\nthe just noticeable difference (JND), the smallest distortion level that can be\nperceived by a subject when a reference image is compared to a distorted one. A\nsequence of JNDs can be defined with a suitable successive choice of reference\nimages. We propose the first deep learning approach to predict SUR curves. We\nshow how to apply maximum likelihood estimation and the Anderson-Darling test\nto select a suitable parametric model for the distribution function. We then\nuse deep feature learning to predict samples of the SUR curve and apply the\nmethod of least squares to fit the parametric model to the predicted samples.\nOur deep learning approach relies on a siamese convolutional neural network,\ntransfer learning, and deep feature learning, using pairs consisting of a\nreference image and a compressed image for training. Experiments on the MCL-JCI\ndataset showed state-of-the-art performance. For example, the mean\nBhattacharyya distances between the predicted and ground truth first, second,\nand third JND distributions were 0.0810, 0.0702, and 0.0522, respectively, and\nthe corresponding average absolute differences of the peak signal-to-noise\nratio at a median of the first JND distribution were 0.58, 0.69, and 0.58 dB.\nFurther experiments on the JND-Pano dataset showed that the method transfers\nwell to high resolution panoramic images viewed on head-mounted displays.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.02002v2"
    },
    {
        "title": "Natural Steganography in JPEG Domain with a Linear Development Pipeline",
        "authors": [
            "Taburet Théo",
            "Bas Patrick",
            "Sawaya Wadih",
            "Jessica Fridrich"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In order to achieve high practical security, Natural Steganography (NS) uses\ncover images captured at ISO sensitivity $ISO_{1}$ and generates stego images\nmimicking ISO sensitivity $ISO_{2}>ISO_{1}$. This is achieved by adding a stego\nsignal to the cover that mimics the sensor photonic noise. This paper proposes\nan embedding mechanism to perform NS in the JPEG domain after linear\ndevelopments by explicitly computing the correlations between DCT coefficients\nbefore quantization. In order to compute the covariance matrix of the photonic\nnoise in the DCT domain, we first develop the matrix representation of\ndemosaicking, luminance averaging, pixel section, and 2D-DCT. A detailed\nanalysis of the resulting covariance matrix is done in order to explain the\norigins of the correlations between the coefficients of $3\\times3$ DCT blocks.\nAn embedding scheme is then presented that takes in order to take into account\nall the correlations. It employs 4 sub-lattices and 64 lattices per\nsub-lattices. The modification probabilities of each DCT coefficient are then\nderived by computing conditional probabilities from the multivariate Gaussian\ndistribution using the Cholesky decomposition of the covariance matrix. This\nderivation is also used to compute the embedding capacity of each image. Using\na specific database called E1 Base, we show that in the JPEG domain NS\n(J-Cov-NS) enables to achieve high capacity (more than 2 bits per non-zero AC\nDCT) and with high practical security ($P_{\\mathrm{E}}\\simeq40\\%$ using DCTR\nfrom QF 75 to QF 100).\n",
        "pdf_link": "http://arxiv.org/pdf/2001.02653v2"
    },
    {
        "title": "Low-latency Cloud-based Volumetric Video Streaming Using Head Motion\n  Prediction",
        "authors": [
            "Serhan Gül",
            "Dimitri Podborski",
            "Thomas Buchholz",
            "Thomas Schierl",
            "Cornelius Hellge"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Volumetric video is an emerging key technology for immersive representation\nof 3D spaces and objects. Rendering volumetric video requires lots of\ncomputational power which is challenging especially for mobile devices. To\nmitigate this, we developed a streaming system that renders a 2D view from the\nvolumetric video at a cloud server and streams a 2D video stream to the client.\nHowever, such network-based processing increases the motion-to-photon (M2P)\nlatency due to the additional network and processing delays. In order to\ncompensate the added latency, prediction of the future user pose is necessary.\nWe developed a head motion prediction model and investigated its potential to\nreduce the M2P latency for different look-ahead times. Our results show that\nthe presented model reduces the rendering errors caused by the M2P latency\ncompared to a baseline system in which no prediction is performed.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.06466v2"
    },
    {
        "title": "An Effective Automatic Image Annotation Model Via Attention Model and\n  Data Equilibrium",
        "authors": [
            "Amir Vatani",
            "Milad Taleby Ahvanooey",
            "Mostafa Rahimi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Nowadays, a huge number of images are available. However, retrieving a\nrequired image for an ordinary user is a challenging task in computer vision\nsystems. During the past two decades, many types of research have been\nintroduced to improve the performance of the automatic annotation of images,\nwhich are traditionally focused on content-based image retrieval. Although,\nrecent research demonstrates that there is a semantic gap between content-based\nimage retrieval and image semantics understandable by humans. As a result,\nexisting research in this area has caused to bridge the semantic gap between\nlow-level image features and high-level semantics. The conventional method of\nbridging the semantic gap is through the automatic image annotation (AIA) that\nextracts semantic features using machine learning techniques. In this paper, we\npropose a novel AIA model based on the deep learning feature extraction method.\nThe proposed model has three phases, including a feature extractor, a tag\ngenerator, and an image annotator. First, the proposed model extracts\nautomatically the high and low-level features based on dual-tree continues\nwavelet transform (DT-CWT), singular value decomposition, distribution of color\nton, and the deep neural network. Moreover, the tag generator balances the\ndictionary of the annotated keywords by a new log-entropy auto-encoder (LEAE)\nand then describes these keywords by word embedding. Finally, the annotator\nworks based on the long-short-term memory (LSTM) network in order to obtain the\nimportance degree of specific features of the image. The experiments conducted\non two benchmark datasets confirm that the superiority of the proposed model\ncompared to the previous models in terms of performance criteria.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10590v1"
    },
    {
        "title": "Multi-view data capture using edge-synchronised mobiles",
        "authors": [
            "Matteo Bortolon",
            "Paul Chippendale",
            "Stefano Messelodi",
            "Fabio Poiesi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Multi-view data capture permits free-viewpoint video (FVV) content creation.\nTo this end, several users must capture video streams, calibrated in both time\nand pose, framing the same object/scene, from different viewpoints.\nNew-generation network architectures (e.g. 5G) promise lower latency and larger\nbandwidth connections supported by powerful edge computing, properties that\nseem ideal for reliable FVV capture. We have explored this possibility, aiming\nto remove the need for bespoke synchronisation hardware when capturing a scene\nfrom multiple viewpoints, making it possible through off-the-shelf mobiles. We\npropose a novel and scalable data capture architecture that exploits edge\nresources to synchronise and harvest frame captures. We have designed an edge\ncomputing unit that supervises the relaying of timing triggers to and from\nmultiple mobiles, in addition to synchronising frame harvesting. We empirically\nshow the benefits of our edge computing unit by analysing latencies and show\nthe quality of 3D reconstruction outputs against an alternative and popular\ncentralised solution based on Unity3D.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03286v1"
    },
    {
        "title": "Interval type-2 fuzzy logic system based similarity evaluation for image\n  steganography",
        "authors": [
            "Zubair Ashraf",
            "Mukul Lata Roy",
            "Pranab K. Muhuri",
            "Q. M. Danish Lohani"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Similarity measure, also called information measure, is a concept used to\ndistinguish different objects. It has been studied from different contexts by\nemploying mathematical, psychological, and fuzzy approaches. Image\nsteganography is the art of hiding secret data into an image in such a way that\nit cannot be detected by an intruder. In image steganography, hiding secret\ndata in the plain or non-edge regions of the image is significant due to the\nhigh similarity and redundancy of the pixels in their neighborhood. However,\nthe similarity measure of the neighboring pixels, i.e., their proximity in\ncolor space, is perceptual rather than mathematical. This paper proposes an\ninterval type 2 fuzzy logic system (IT2 FLS) to determine the similarity\nbetween the neighboring pixels by involving an instinctive human perception\nthrough a rule-based approach. The pixels of the image having high similarity\nvalues, calculated using the proposed IT2 FLS similarity measure, are selected\nfor embedding via the least significant bit (LSB) method. We term the proposed\nprocedure of steganography as IT2 FLS LSB method. Moreover, we have developed\ntwo more methods, namely, type 1 fuzzy logic system based least significant\nbits (T1FLS LSB) and Euclidean distance based similarity measures for least\nsignificant bit (SM LSB) steganographic methods. Experimental simulations were\nconducted for a collection of images and quality index metrics, such as PSNR,\nUQI, and SSIM are used. All the three steganographic methods are applied on\ndatasets and the quality metrics are calculated. The obtained stego images and\nresults are shown and thoroughly compared to determine the efficacy of the IT2\nFLS LSB method. Finally, we have done a comparative analysis of the proposed\napproach with the existing well-known steganographic methods to show the\neffectiveness of our proposed steganographic method.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03310v1"
    },
    {
        "title": "Building a Manga Dataset \"Manga109\" with Annotations for Multimedia\n  Applications",
        "authors": [
            "Kiyoharu Aizawa",
            "Azuma Fujimoto",
            "Atsushi Otsubo",
            "Toru Ogawa",
            "Yusuke Matsui",
            "Koki Tsubota",
            "Hikaru Ikuta"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Manga, or comics, which are a type of multimodal artwork, have been left\nbehind in the recent trend of deep learning applications because of the lack of\na proper dataset. Hence, we built Manga109, a dataset consisting of a variety\nof 109 Japanese comic books (94 authors and 21,142 pages) and made it publicly\navailable by obtaining author permissions for academic use. We carefully\nannotated the frames, speech texts, character faces, and character bodies; the\ntotal number of annotations exceeds 500k. This dataset provides numerous manga\nimages and annotations, which will be beneficial for use in machine learning\nalgorithms and their evaluation. In addition to academic use, we obtained\nfurther permission for a subset of the dataset for industrial use. In this\narticle, we describe the details of the dataset and present a few examples of\nmultimedia processing applications (detection, retrieval, and generation) that\napply existing deep learning methods and are made possible by the dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.04425v2"
    },
    {
        "title": "Spatiotemporal Adaptive Quantization for the Perceptual Video Coding of\n  RGB 4:4:4 Data",
        "authors": [
            "Lee Prangnell",
            "Victor Sanchez"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Due to the spectral sensitivity phenomenon of the Human Visual System (HVS),\nthe color channels of raw RGB 4:4:4 sequences contain significant psychovisual\nredundancies; these redundancies can be perceptually quantized. The default\nquantization systems in the HEVC standard are known as Uniform Reconstruction\nQuantization (URQ) and Rate Distortion Optimized Quantization (RDOQ); URQ and\nRDOQ are not perceptually optimized for the coding of RGB 4:4:4 video data. In\nthis paper, we propose a novel spatiotemporal perceptual quantization technique\nnamed SPAQ. With application for RGB 4:4:4 video data, SPAQ exploits HVS\nspectral sensitivity-related color masking in addition to spatial masking and\ntemporal masking; SPAQ operates at the Coding Block (CB) level and the\nPrediction Unit (PU) level. The proposed technique perceptually adjusts the\nQuantization Step Size (QStep) at the CB level if high variance spatial data in\nG, B and R CBs is detected and also if high motion vector magnitudes in PUs are\ndetected. Compared with anchor 1 (HEVC HM 16.17 RExt), SPAQ considerably\nreduces bitrates with a maximum reduction of approximately 80%. The Mean\nOpinion Score (MOS) in the subjective evaluations, in addition to the SSIM\nscores, show that SPAQ successfully achieves perceptually lossless compression\ncompared with anchors.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.07928v1"
    },
    {
        "title": "User-generated Video Quality Assessment: A Subjective and Objective\n  Study",
        "authors": [
            "Yang Li",
            "Shengbin Meng",
            "Xinfeng Zhang",
            "Shiqi Wang",
            "Yue Wang",
            "Siwei Ma"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Recently, we have observed an exponential increase of user-generated content\n(UGC) videos. The distinguished characteristic of UGC videos originates from\nthe video production and delivery chain, as they are usually acquired and\nprocessed by non-professional users before uploading to the hosting platforms\nfor sharing. As such, these videos usually undergo multiple distortion stages\nthat may affect visual quality before ultimately being viewed. Inspired by the\nincreasing consensus that the optimization of the video coding and processing\nshall be fully driven by the perceptual quality, in this paper, we propose to\nstudy the quality of the UGC videos from both objective and subjective\nperspectives. We first construct a UGC video quality assessment (VQA) database,\naiming to provide useful guidance for the UGC video coding and processing in\nthe hosting platform. The database contains source UGC videos uploaded to the\nplatform and their transcoded versions that are ultimately enjoyed by\nend-users, along with their subjective scores. Furthermore, we develop an\nobjective quality assessment algorithm that automatically evaluates the quality\nof the transcoded videos based on the corrupted reference, which is in\naccordance with the application scenarios of UGC video sharing in the hosting\nplatforms. The information from the corrupted reference is well leveraged and\nthe quality is predicted based on the inferred quality maps with deep neural\nnetworks (DNN). Experimental results show that the proposed method yields\nsuperior performance. Both subjective and objective evaluations of the UGC\nvideos also shed lights on the design of perceptual UGC video coding.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.08527v1"
    },
    {
        "title": "Human-Perception-Oriented Pseudo Analog Video Transmissions with Deep\n  Learning",
        "authors": [
            "Xiao-Wei Tang",
            "Xin-Lin Huang",
            "Fei Hu",
            "Qingjiang Shi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Recently, pseudo analog transmission has gained increasing attentions due to\nits ability to alleviate the cliff effect in video multicast scenarios. The\nexisting pseudo analog systems are sorely optimized under the minimum mean\nsquared error criterion without taking the perceptual video quality into\nconsideration. In this paper, we propose a human-perception-based pseudo analog\nvideo transmission system named ROIC-Cast, which aims to intelligently enhance\nthe transmission quality of the region-of-interest (ROI) parts. Firstly, the\nclassic deep learning based saliency detection algorithm is adopted to\ndecompose the continuous video sequences into ROI and non-ROI blocks. Secondly,\nan effective compression method is used to reduce the data amount of side\ninformation generated by the ROI extraction module. Then, the power allocation\nscheme is formulated as a convex problem, and the optimal transmission power\nfor both ROI and non-ROI blocks is derived in a closed form. Finally, the\nsimulations are conducted to validate the proposed system by comparing with a\nfew of existing systems, e.g., KMV-Cast, SoftCast, and DAC-RAN. The proposed\nROIC-Cast can achieve over 4.1dB peak signal- to-noise ratio gains of ROI\ncompared with other systems, given the channel signal-to-noise ratio as -5dB,\n0dB, 5dB, and 10dB, respectively. This significant performance improvement is\ndue to the automatic ROI extraction, high-efficiency data compression as well\nas adaptive power allocation.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09302v1"
    },
    {
        "title": "A Modified Fourier-Mellin Approach for Source Device Identification on\n  Stabilized Videos",
        "authors": [
            "Sara Mandelli",
            "Fabrizio Argenti",
            "Paolo Bestagini",
            "Massimo Iuliani",
            "Alessandro Piva",
            "Stefano Tubaro"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  To decide whether a digital video has been captured by a given device,\nmultimedia forensic tools usually exploit characteristic noise traces left by\nthe camera sensor on the acquired frames. This analysis requires that the noise\npattern characterizing the camera and the noise pattern extracted from video\nframes under analysis are geometrically aligned. However, in many practical\nscenarios this does not occur, thus a re-alignment or synchronization has to be\nperformed. Current solutions often require time consuming search of the\nrealignment transformation parameters. In this paper, we propose to overcome\nthis limitation by searching scaling and rotation parameters in the frequency\ndomain. The proposed algorithm tested on real videos from a well-known\nstate-of-the-art dataset shows promising results.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09984v1"
    },
    {
        "title": "Robust Spatial-spread Deep Neural Image Watermarking",
        "authors": [
            "Marcin Plata",
            "Piotr Syga"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Watermarking is an operation of embedding an information into an image in a\nway that allows to identify ownership of the image despite applying some\ndistortions on it. In this paper, we presented a novel end-to-end solution for\nembedding and recovering the watermark in the digital image using convolutional\nneural networks. The method is based on spreading the message over the spatial\ndomain of the image, hence reducing the \"local bits per pixel\" capacity. To\nobtain the model we used adversarial training and applied noiser layers between\nthe encoder and the decoder. Moreover, we broadened the spectrum of typically\nconsidered attacks on the watermark and by grouping the attacks according to\ntheir scope, we achieved high general robustness, most notably against JPEG\ncompression, Gaussian blurring, subsampling or resizing. To help us in the\nmodels training we also proposed a precise differentiable approximation of\nJPEG.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.11735v2"
    },
    {
        "title": "Memory Assessment of Versatile Video Coding",
        "authors": [
            "Arthur Cerveira",
            "Luciano Agostini",
            "Bruno Zatt",
            "Felipe Sampaio"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper presents a memory assessment of the next-generation Versatile\nVideo Coding (VVC). The memory analyses are performed adopting as a baseline\nthe state-of-the-art High-Efficiency Video Coding (HEVC). The goal is to offer\ninsights and observations of how critical the memory requirements of VVC are\naggravated, compared to HEVC. The adopted methodology consists of two sets of\nexperiments: (1) an overall memory profiling and (2) an inter-prediction\nspecific memory analysis. The results obtained in the memory profiling show\nthat VVC access up to 13.4x more memory than HEVC. Moreover, the\ninter-prediction module remains (as in HEVC) the most resource-intensive\noperation in the encoder: 60%-90% of the memory requirements. The\ninter-prediction specific analysis demonstrates that VVC requires up to 5.3x\nmore memory accesses than HEVC. Furthermore, our analysis indicates that up to\n23% of such growth is due to VVC novel-CU sizes (larger than 64x64).\n",
        "pdf_link": "http://arxiv.org/pdf/2005.13331v2"
    },
    {
        "title": "Image Authentication Based on Neural Networks",
        "authors": [
            "Shiguo Lian"
        ],
        "category": "cs.MM",
        "published_year": "2007",
        "summary": "  Neural network has been attracting more and more researchers since the past\ndecades. The properties, such as parameter sensitivity, random similarity,\nlearning ability, etc., make it suitable for information protection, such as\ndata encryption, data authentication, intrusion detection, etc. In this paper,\nby investigating neural networks' properties, the low-cost authentication\nmethod based on neural networks is proposed and used to authenticate images or\nvideos. The authentication method can detect whether the images or videos are\nmodified maliciously. Firstly, this chapter introduces neural networks'\nproperties, such as parameter sensitivity, random similarity, diffusion\nproperty, confusion property, one-way property, etc. Secondly, the chapter\ngives an introduction to neural network based protection methods. Thirdly, an\nimage or video authentication scheme based on neural networks is presented, and\nits performances, including security, robustness and efficiency, are analyzed.\nFinally, conclusions are drawn, and some open issues in this field are\npresented.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.4524v1"
    },
    {
        "title": "Content-Aware Rate Control for Video Transmission with Buffer\n  Constraints in Multipath Networks",
        "authors": [
            "Mohammad Hassan Hajiesmaili",
            "Ali Sehati",
            "Ahmad Khonsari",
            "Mohammad Sadegh Talebi"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Being an integral part of the network traffic, nowadays it's vital to design\nrobust mechanisms to provide QoS for multimedia applications. The main goal of\nthis paper is to provide an efficient solution to support content-aware video\ntransmission mechanism with buffer underflow avoidance at the receiver in\nmultipath networks. Towards this, we introduce a content-aware time-varying\nutility function, where the quality impacts of video content is incorporated\ninto its definition. Using the proposed utility function, we formulate a\nmultipath Dynamic Network Utility Maximization (DNUM) problem for the rate\nallocation of video streams, where it takes into account QoS demand of video\nstreams in terms of buffer underflow avoidance. Finally, using primal-dual\nmethod, we propose a distributed solution that optimally allocates the shared\nbandwidth to video streams. The numerical examples demonstrate the efficacy of\nthe proposed content-aware rate allocation algorithm for video sources in both\nsingle and multiple path network models.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.6851v2"
    },
    {
        "title": "Investigating Streaming Techniques and Energy Efficiency of Mobile Video\n  Services",
        "authors": [
            "Mohammad Ashraful Hoque",
            "Matti Siekkinen",
            "Jukka K. Nurminen",
            "Mika Aalto"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  We report results from a measurement study of three video streaming services,\nYouTube, Dailymotion and Vimeo on six different smartphones. We measure and\nanalyze the traffic and energy consumption when streaming different quality\nvideos over Wi-Fi and 3G. We identify five different techniques to deliver the\nvideo and show that the use of a particular technique depends on the device,\nplayer, quality, and service. The energy consumption varies dramatically\nbetween devices, services, and video qualities depending on the streaming\ntechnique used. As a consequence, we come up with suggestions on how to improve\nthe energy efficiency of mobile video streaming services.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.2855v1"
    },
    {
        "title": "Attribute-Based Multi-Dimensional Scalable Access Control For Social\n  Media Sharing",
        "authors": [
            "Changsha Ma",
            "Chang Wen Chen"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Media sharing is an extremely popular paradigm of social interaction in\nonline social networks (OSNs) nowadays. The scalable media access control is\nessential to perform information sharing among users with various access\nprivileges. In this paper, we present a multi-dimensional scalable media access\ncontrol (MD-SMAC) system based on the proposed scalable ciphertext policy\nattribute-based encryption (SCP-ABE) algorithm. In the proposed MD-SMAC system,\nfine-grained access control can be performed on the media contents encoded in a\nmulti-dimensional scalable manner based on data consumers' diverse attributes.\nThrough security analysis, we show that the proposed MC-SMAC system is able to\nresist collusion attacks. Additionally, we conduct experiments to evaluate the\nefficiency performance of the proposed system, especially on mobile devices.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.03351v1"
    },
    {
        "title": "A GMM-Based Stair Quality Model for Human Perceived JPEG Images",
        "authors": [
            "Sudeng Hu",
            "Haiqiang Wang",
            "C. -C. Jay Kuo"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Based on the notion of just noticeable differences (JND), a stair quality\nfunction (SQF) was recently proposed to model human perception on JPEG images.\nFurthermore, a k-means clustering algorithm was adopted to aggregate JND data\ncollected from multiple subjects to generate a single SQF. In this work, we\npropose a new method to derive the SQF using the Gaussian Mixture Model (GMM).\nThe newly derived SQF can be interpreted as a way to characterize the mean\nviewer experience. Furthermore, it has a lower information criterion (BIC)\nvalue than the previous one, indicating that it offers a better model. A\nspecific example is given to demonstrate the advantages of the new approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.03398v1"
    },
    {
        "title": "Learning Local Distortion Visibility From Image Quality Data-sets",
        "authors": [
            "Navaneeth Kamballur Kottayil",
            "Giuseppe Valenzise",
            "Frederic Dufaux",
            "Irene Cheng"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Accurate prediction of local distortion visibility thresholds is critical in\nmany image and video processing applications. Existing methods require an\naccurate modeling of the human visual system, and are derived through\npshycophysical experiments with simple, artificial stimuli. These approaches,\nhowever, are difficult to generalize to natural images with complex types of\ndistortion. In this paper, we explore a different perspective, and we\ninvestigate whether it is possible to learn local distortion visibility from\nimage quality scores. We propose a convolutional neural network based\noptimization framework to infer local detection thresholds in a distorted\nimage. Our model is trained on multiple quality datasets, and the results are\ncorrelated with empirical visibility thresholds collected on complex stimuli in\na recent study. Our results are comparable to state-of-the-art mathematical\nmodels that were trained on phsycovisual data directly. This suggests that it\nis possible to predict psychophysical phenomena from visibility information\nembedded in image quality scores.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04053v1"
    },
    {
        "title": "Robust LSB Watermarking Optimized for Local Structural Similarity",
        "authors": [
            "Amin Banitalebi",
            "Said Nader-Esfahani",
            "Alireza Nasiri Avanaki"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Growth of the Internet and networked multimedia systems has emphasized the\nneed for copyright protection of the media. Media can be images, audio clips,\nvideos and etc. Digital watermarking is today extensively used for many\napplications such as authentication of ownership or identification of illegal\ncopies. Digital watermark is an invisible or maybe visible structure added to\nthe original media (known as asset). Images are considered as communication\nchannel when they are subject to a watermark embedding procedure so in the case\nof embedding a digital watermark in an image, the capacity of the channel\nshould be considered. There is a trade-off between imperceptibility, robustness\nand capacity for embedding a watermark in an asset. In the case of image\nwatermarks, it is reasonable that the watermarking algorithm should depend on\nthe content and structure of the image. Conventionally, mean squared error\n(MSE) has been used as a common distortion measure to assess the quality of the\nimages. Newly developed quality metrics proposed some distortion measures that\nare based on human visual system (HVS). These metrics show that MSE is not\nbased on HVS and it has a lack of accuracy when dealing with perceptually\nimportant signals such as images and videos. SSIM or structural similarity is a\nstate of the art HVS based image quality criterion that has recently been of\nmuch interest. In this paper we propose a robust least significant bit (LSB)\nwatermarking scheme which is optimized for structural similarity. The watermark\nis embedded into a host image through an adaptive algorithm. Various attacks\nexamined on the embedding approach and simulation results revealed the fact\nthat the watermarked sequence can be extracted with an acceptable accuracy\nafter all attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04617v1"
    },
    {
        "title": "An Improvement Technique based on Structural Similarity Thresholding for\n  Digital Watermarking",
        "authors": [
            "Amin Banitalebi-Dehkordi",
            "Mehdi Banitalebi-Dehkordi",
            "Jamshid Abouei",
            "Said Nader-Esfahani"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Digital watermarking is extensively used in ownership authentication and\ncopyright protection. In this paper, we propose an efficient thresholding\nscheme to improve the watermark embedding procedure in an image. For the\nproposed algorithm, watermark casting is performed separately in each block of\nan image, and embedding in each block continues until a certain structural\nsimilarity threshold is reached. Numerical evaluations demonstrate that our\nscheme improves the imperceptibility of the watermark when the capacity remains\nfix, and at the same time, robustness against attacks is assured. The proposed\nmethod is applicable to most image watermarking algorithms. We verify this\nissue on watermarking schemes in Discrete Cosine Transform (DCT), wavelet, and\nspatial domain.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04966v1"
    },
    {
        "title": "QoE-Oriented Resource Allocation for 360-degree Video Transmission over\n  Heterogeneous Networks",
        "authors": [
            "Wei Huang",
            "Lianghui Ding",
            "Hung-Yu Wei",
            "Jenq-Neng Hwang",
            "Yiling Xu",
            "Wenjun Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Immersive media streaming, especially virtual reality (VR)/360-degree video\nstreaming which is very bandwidth demanding, has become more and more popular\ndue to the rapid growth of the multimedia and networking deployments. To better\nexplore the usage of resource and achieve better quality of experience (QoE)\nperceived by users, this paper develops an application-layer scheme to jointly\nexploit the available bandwidth from the LTE and Wi-Fi networks in 360-degree\nvideo streaming. This newly proposed scheme and the corresponding solution\nalgorithms utilize the saliency of video, prediction of users' view and the\nstatus information of users to obtain an optimal association of the users with\ndifferent Wi-Fi access points (APs) for maximizing the system's utility.\nBesides, a novel buffer strategy is proposed to mitigate the influence of\nshort-time prediction problem for transmitting 360-degree videos in\ntime-varying networks. The promising performance and low complexity of the\nproposed scheme and algorithms are validated in simulations with various\n360-degree videos.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.07789v1"
    },
    {
        "title": "Digital Cardan Grille: A Modern Approach for Information Hiding",
        "authors": [
            "Jia Liu",
            "Tanping Zhou",
            "Zhuo Zhang",
            "Yan Ke",
            "Yu Lei",
            "Minqing Zhang",
            "Xiaoyuan Yang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this paper, a new framework for construction of Cardan grille for\ninformation hiding is proposed. Based on the semantic image inpainting\ntechnique, the stego image are driven by secret messages directly. A mask\ncalled Digital Cardan Grille (DCG) for determining the hidden location is\nintroduced to hide the message. The message is written to the corrupted region\nthat needs to be filled in the corrupted image in advance. Then the corrupted\nimage with secret message is feeded into a Generative Adversarial Network (GAN)\nfor semantic completion. The adversarial game not only reconstruct the\ncorrupted image , but also generate a stego image which contains the logic\nrationality of image content. The experimental results verify the feasibility\nof the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09219v2"
    },
    {
        "title": "EAST Real-Time VOD System Based on MDSplus",
        "authors": [
            "J. Y. Xia",
            "B. J. Xiao",
            "Fei Yang",
            "Dan Li"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  As with EAST (Experimental Advanced Superconducting Tokamak) experimental\ndata analyzed by more and more collaborators, the experimental videos which\ndirectly reflect the real status of vacuum attract more and more researchers'\nattention. The real time VOD (Video On Demand) system based on MDSplus allows\nusers reading the video frames in real time as same as the signal data which is\nalso stored in the MDSplus database. User can display the plasma discharge\nvideos and analyze videos frame by frame through jScope or our VOD web station.\nThe system mainly includes the frames storing and frames displaying. The frames\nstoring application accepts shot information by using socket TCP communication\nfirstly, then reads video frames through disk mapping, finally stores them into\nMDSplus. The displaying process is implemented through B/S (Browser/Server)\nframework, it uses PHP and JavaScript to realize VOD function and read frames\ninformation from MDSplus. The system offers a unit way to access and backup\nexperimental data and video during the EAST experiment, which is of great\nbenefit to EAST experimenter than the formal VOD system in VOD function and\nreal time performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.03773v1"
    },
    {
        "title": "Few-Shot Adaptation for Multimedia Semantic Indexing",
        "authors": [
            "Nakamasa Inoue",
            "Koichi Shinoda"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  We propose a few-shot adaptation framework, which bridges zero-shot learning\nand supervised many-shot learning, for semantic indexing of image and video\ndata. Few-shot adaptation provides robust parameter estimation with few\ntraining examples, by optimizing the parameters of zero-shot learning and\nsupervised many-shot learning simultaneously. In this method, first we build a\nzero-shot detector, and then update it by using the few examples. Our\nexperiments show the effectiveness of the proposed framework on three datasets:\nTRECVID Semantic Indexing 2010, 2014, and ImageNET. On the ImageNET dataset, we\nshow that our method outperforms recent few-shot learning methods. On the\nTRECVID 2014 dataset, we achieve 15.19% and 35.98% in Mean Average Precision\nunder the zero-shot condition and the supervised condition, respectively. To\nthe best of our knowledge, these are the best results on this dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07203v1"
    },
    {
        "title": "SoniControl - A Mobile Ultrasonic Firewall",
        "authors": [
            "Matthias Zeppelzauer",
            "Alexis Ringot",
            "Florian Taurer"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The exchange of data between mobile devices in the near-ultrasonic frequency\nband is a new promising technology for near field communication (NFC) but also\nraises a number of privacy concerns. We present the first ultrasonic firewall\nthat reliably detects ultrasonic communication and provides the user with\neffective means to prevent hidden data exchange. This demonstration showcases a\nnew media-based communication technology (\"data over audio\") together with its\nrelated privacy concerns. It enables users to (i) interactively test out and\nexperience ultrasonic information exchange and (ii) shows how to protect\noneself against unwanted tracking.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07617v1"
    },
    {
        "title": "Invisible Steganography via Generative Adversarial Networks",
        "authors": [
            "Ru Zhang",
            "Shiqi Dong",
            "Jianyi Liu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Nowadays, there are plenty of works introducing convolutional neural networks\n(CNNs) to the steganalysis and exceeding conventional steganalysis algorithms.\nThese works have shown the improving potential of deep learning in information\nhiding domain. There are also several works based on deep learning to do image\nsteganography, but these works still have problems in capacity, invisibility\nand security. In this paper, we propose a novel CNN architecture named as\n\\isgan to conceal a secret gray image into a color cover image on the sender\nside and exactly extract the secret image out on the receiver side. There are\nthree contributions in our work: (i) we improve the invisibility by hiding the\nsecret image only in the Y channel of the cover image; (ii) We introduce the\ngenerative adversarial networks to strengthen the security by minimizing the\ndivergence between the empirical probability distributions of stego images and\nnatural images. (iii) In order to associate with the human visual system\nbetter, we construct a mixed loss function which is more appropriate for\nsteganography to generate more realistic stego images and reveal out more\nbetter secret images. Experiment results show that ISGAN can achieve\nstart-of-art performances on LFW, Pascal VOC2012 and ImageNet datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.08571v3"
    },
    {
        "title": "Video Storytelling: Textual Summaries for Events",
        "authors": [
            "Junnan Li",
            "Yongkang Wong",
            "Qi Zhao",
            "Mohan S. Kankanhalli"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Bridging vision and natural language is a longstanding goal in computer\nvision and multimedia research. While earlier works focus on generating a\nsingle-sentence description for visual content, recent works have studied\nparagraph generation. In this work, we introduce the problem of video\nstorytelling, which aims at generating coherent and succinct stories for long\nvideos. Video storytelling introduces new challenges, mainly due to the\ndiversity of the story and the length and complexity of the video. We propose\nnovel methods to address the challenges. First, we propose a context-aware\nframework for multimodal embedding learning, where we design a Residual\nBidirectional Recurrent Neural Network to leverage contextual information from\npast and future. Second, we propose a Narrator model to discover the underlying\nstoryline. The Narrator is formulated as a reinforcement learning agent which\nis trained by directly optimizing the textual metric of the generated story. We\nevaluate our method on the Video Story dataset, a new dataset that we have\ncollected to enable the study. We compare our method with multiple\nstate-of-the-art baselines, and show that our method achieves better\nperformance, in terms of quantitative measures and user study.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.09418v3"
    },
    {
        "title": "BlackMarks: Blackbox Multibit Watermarking for Deep Neural Networks",
        "authors": [
            "Huili Chen",
            "Bita Darvish Rouhani",
            "Farinaz Koushanfar"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Deep Neural Networks have created a paradigm shift in our ability to\ncomprehend raw data in various important fields ranging from computer vision\nand natural language processing to intelligence warfare and healthcare. While\nDNNs are increasingly deployed either in a white-box setting where the model\ninternal is publicly known, or a black-box setting where only the model outputs\nare known, a practical concern is protecting the models against Intellectual\nProperty (IP) infringement. We propose BlackMarks, the first end-to-end\nmulti-bit watermarking framework that is applicable in the black-box scenario.\nBlackMarks takes the pre-trained unmarked model and the owner's binary\nsignature as inputs and outputs the corresponding marked model with a set of\nwatermark keys. To do so, BlackMarks first designs a model-dependent encoding\nscheme that maps all possible classes in the task to bit '0' and bit '1' by\nclustering the output activations into two groups. Given the owner's watermark\nsignature (a binary string), a set of key image and label pairs are designed\nusing targeted adversarial attacks. The watermark (WM) is then embedded in the\nprediction behavior of the target DNN by fine-tuning the model with generated\nWM key set. To extract the WM, the remote model is queried by the WM key images\nand the owner's signature is decoded from the corresponding predictions\naccording to the designed encoding scheme. We perform a comprehensive\nevaluation of BlackMarks's performance on MNIST, CIFAR10, ImageNet datasets and\ncorroborate its effectiveness and robustness. BlackMarks preserves the\nfunctionality of the original DNN and incurs negligible WM embedding runtime\noverhead as low as 2.054%.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00344v1"
    },
    {
        "title": "YouTube UGC Dataset for Video Compression Research",
        "authors": [
            "Yilin Wang",
            "Sasi Inguva",
            "Balu Adsumilli"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Non-professional video, commonly known as User Generated Content (UGC) has\nbecome very popular in today's video sharing applications. However, traditional\nmetrics used in compression and quality assessment, like BD-Rate and PSNR, are\ndesigned for pristine originals. Thus, their accuracy drops significantly when\nbeing applied on non-pristine originals (the majority of UGC). Understanding\ndifficulties for compression and quality assessment in the scenario of UGC is\nimportant, but there are few public UGC datasets available for research. This\npaper introduces a large scale UGC dataset (1500 20 sec video clips) sampled\nfrom millions of YouTube videos. The dataset covers popular categories like\nGaming, Sports, and new features like High Dynamic Range (HDR). Besides a novel\nsampling method based on features extracted from encoding, challenges for UGC\ncompression and quality evaluation are also discussed. Shortcomings of\ntraditional reference-based metrics on UGC are addressed. We demonstrate a\npromising way to evaluate UGC quality by no-reference objective quality\nmetrics, and evaluate the current dataset with three no-reference metrics\n(Noise, Banding, and SLEEQ).\n",
        "pdf_link": "http://arxiv.org/pdf/1904.06457v2"
    },
    {
        "title": "Steganographer Identification",
        "authors": [
            "Hanzhou Wu"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Conventional steganalysis detects the presence of steganography within single\nobjects. In the real-world, we may face a complex scenario that one or some of\nmultiple users called actors are guilty of using steganography, which is\ntypically defined as the Steganographer Identification Problem (SIP). One might\nuse the conventional steganalysis algorithms to separate stego objects from\ncover objects and then identify the guilty actors. However, the guilty actors\nmay be lost due to a number of false alarms. To deal with the SIP, most of the\nstate-of-the-arts use unsupervised learning based approaches. In their\nsolutions, each actor holds multiple digital objects, from which a set of\nfeature vectors can be extracted. The well-defined distances between these\nfeature sets are determined to measure the similarity between the corresponding\nactors. By applying clustering or outlier detection, the most suspicious\nactor(s) will be judged as the steganographer(s). Though the SIP needs further\nstudy, the existing works have good ability to identify the steganographer(s)\nwhen non-adaptive steganographic embedding was applied. In this chapter, we\nwill present foundational concepts and review advanced methodologies in SIP.\nThis chapter is self-contained and intended as a tutorial introducing the SIP\nin the context of media steganography.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.07554v1"
    },
    {
        "title": "Deep Learning-Based Video Coding: A Review and A Case Study",
        "authors": [
            "Dong Liu",
            "Yue Li",
            "Jianping Lin",
            "Houqiang Li",
            "Feng Wu"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The past decade has witnessed great success of deep learning technology in\nmany disciplines, especially in computer vision and image processing. However,\ndeep learning-based video coding remains in its infancy. This paper reviews the\nrepresentative works about using deep learning for image/video coding, which\nhas been an actively developing research area since the year of 2015. We divide\nthe related works into two categories: new coding schemes that are built\nprimarily upon deep networks (deep schemes), and deep network-based coding\ntools (deep tools) that shall be used within traditional coding schemes or\ntogether with traditional coding tools. For deep schemes, pixel probability\nmodeling and auto-encoder are the two approaches, that can be viewed as\npredictive coding scheme and transform coding scheme, respectively. For deep\ntools, there have been several proposed techniques using deep learning to\nperform intra-picture prediction, inter-picture prediction, cross-channel\nprediction, probability distribution prediction, transform, post- or in-loop\nfiltering, down- and up-sampling, as well as encoding optimizations. In the\nhope of advocating the research of deep learning-based video coding, we present\na case study of our developed prototype video codec, namely Deep Learning Video\nCoding (DLVC). DLVC features two deep tools that are both based on\nconvolutional neural network (CNN), namely CNN-based in-loop filter (CNN-ILF)\nand CNN-based block adaptive resolution coding (CNN-BARC). Both tools help\nimprove the compression efficiency by a significant margin. With the two deep\ntools as well as other non-deep coding tools, DLVC is able to achieve on\naverage 39.6\\% and 33.0\\% bits saving than HEVC, under random-access and\nlow-delay configurations, respectively. The source code of DLVC has been\nreleased for future researches.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.12462v1"
    },
    {
        "title": "Image Steganography using Gaussian Markov Random Field Model",
        "authors": [
            "Wenkang Su",
            "Jiangqun Ni",
            "Yuanfeng Pan",
            "Xianglei Hu",
            "Yun-Qing Shi"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Recent advances on adaptive steganography show that the performance of image\nsteganographic communication can be improved by incorporating the non-additive\nmodels that capture the dependences among adjacent pixels. In this paper, a\nGaussian Markov Random Field model (GMRF) with four-element cross neighborhood\nis proposed to characterize the interactions among local elements of cover\nimages, and the problem of secure image steganography is formulated as the one\nof minimization of KL-divergence in terms of a series of low-dimensional clique\nstructures associated with GMRF by taking advantages of the conditional\nindependence of GMRF. The adoption of the proposed GMRF tessellates the cover\nimage into two disjoint subimages, and an alternating iterative optimization\nscheme is developed to effectively embed the given payload while minimizing the\ntotal KL-divergence between cover and stego, i.e., the statistical\ndetectability. Experimental results demonstrate that the proposed GMRF\noutperforms the prior arts of model based schemes, e.g., MiPOD, and rivals the\nstate-of-the-art HiLL for practical steganography, where the selection channel\nknowledges are unavailable to steganalyzers.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.01483v1"
    },
    {
        "title": "Cascaded Revision Network for Novel Object Captioning",
        "authors": [
            "Qianyu Feng",
            "Yu Wu",
            "Hehe Fan",
            "Chenggang Yan",
            "Yi Yang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Image captioning, a challenging task where the machine automatically\ndescribes an image by sentences, has drawn significant attention in recent\nyears. Despite the remarkable improvements of recent approaches, however, these\nmethods are built upon a large set of training image-sentence pairs. The\nexpensive labor efforts hence limit the captioning model to describe the wider\nworld. In this paper, we present a novel network structure, Cascaded Revision\nNetwork, which aims at relieving the problem by equipping the model with\nout-of-domain knowledge. CRN first tries its best to describe an image using\nthe existing vocabulary from in-domain knowledge. Due to the lack of\nout-of-domain knowledge, the caption may be inaccurate or include ambiguous\nwords for the image with unknown (novel) objects. We propose to re-edit the\nprimary captioning sentence by a series of cascaded operations. We introduce a\nperplexity predictor to find out which words are most likely to be inaccurate\ngiven the input image. Thereafter, we utilize external knowledge from a\npre-trained object detection model and select more accurate words from\ndetection results by the visual matching module. In the last step, we design a\nsemantic matching module to ensure that the novel object is fit in the right\nposition. By this novel cascaded captioning-revising mechanism, CRN can\naccurately describe images with unseen objects. We validate the proposed method\nwith state-of-the-art performance on the held-out MSCOCO dataset as well as\nscale to ImageNet, demonstrating the effectiveness of this method.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02726v1"
    },
    {
        "title": "A Benchmark of Visual Storytelling in Social Media",
        "authors": [
            "Gonçalo Marcelino",
            "David Semedo",
            "André Mourão",
            "Saverio Blasi",
            "Marta Mrak",
            "João Magalhães"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Media editors in the newsroom are constantly pressed to provide a \"like-being\nthere\" coverage of live events. Social media provides a disorganised collection\nof images and videos that media professionals need to grasp before publishing\ntheir latest news updated. Automated news visual storyline editing with social\nmedia content can be very challenging, as it not only entails the task of\nfinding the right content but also making sure that news content evolves\ncoherently over time. To tackle these issues, this paper proposes a benchmark\nfor assessing social media visual storylines. The SocialStories benchmark,\ncomprised by total of 40 curated stories covering sports and cultural events,\nprovides the experimental setup and introduces novel quantitative metrics to\nperform a rigorous evaluation of visual storytelling with social media data.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.03505v1"
    },
    {
        "title": "False News Detection on Social Media",
        "authors": [
            "Juan Cao",
            "Qiang Sheng",
            "Peng Qi",
            "Lei Zhong",
            "Yanyan Wang",
            "Xueyao Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Social media has become a major information platform where people consume and\nshare news. However, it has also enabled the wide dissemination of false news,\ni.e., news posts published on social media that are verifiably false, causing\nsignificant negative effects on society. In order to help prevent further\npropagation of false news on social media, we set up this competition to\nmotivate the development of automated real-time false news detection\napproaches. Specifically, this competition includes three sub-tasks: false-news\ntext detection, false-news image detection and false-news multi-modal\ndetetcion, which aims to motivate participants to further explore the\nefficiency of multiple modalities in detecting false news and reasonable fusion\napproaches of multi-modal contents. To better support this competition, we also\nconstruct and publicize a multi-modal data repository about False News on Weibo\nSocial platform(MCG-FNeWS}) to help evaluate the performance of different\napproaches from participants.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.10818v1"
    },
    {
        "title": "A steganographic approach based on the chaotic fractional map and in the\n  DCT domain",
        "authors": [
            "A. Soria-Lorente",
            "E. Pérez-Michel",
            "E. Avila-Domenech"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  A steganographic method based on the chaotic fractional map and in the DCT\ndomain is proposed. This method embeds a secret message in some high frequency\ncoefficients of the image using a 128-bit private key and a chaotic fractional\nmap which generate a permutation indicating the positions where the secret bits\nwill be embedded. An experimental work on the validation of the proposed method\nis also presented, showing performance in imperceptibility, quality, similarity\nand security analysis of the steganographic system. The proposed algorithm\nimproved the level of imperceptibility and Cachin's security of stego-system\nanalyzed through the values of Peak Signal-to-Noise Ratio (PSNR) and the\nRelative Entropy (RE).\n",
        "pdf_link": "http://arxiv.org/pdf/1908.10898v1"
    },
    {
        "title": "UGC-VIDEO: perceptual quality assessment of user-generated videos",
        "authors": [
            "Yang Li",
            "Shengbin Meng",
            "Xinfeng Zhang",
            "Shiqi Wang",
            "Yue Wang",
            "Siwei Ma"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Recent years have witnessed an ever-expandingvolume of user-generated content\n(UGC) videos available on the Internet. Nevertheless, progress on perceptual\nquality assessmentof UGC videos still remains quite limited. There are many\ndistinguished characteristics of UGC videos in the complete video production\nand delivery chain, and one important property closely relevant to video\nquality is that there does not exist the pristine source after they are\nuploaded to the hosting platform,such that they often undergo multiple\ncompression stages before ultimately viewed. To facilitate the UGC video\nquality assessment,we created a UGC video perceptual quality assessment\ndatabase. It contains 50 source videos collected from TikTok with diverse\ncontent, along with multiple distortion versions generated bythe compression\nwith different quantization levels and coding standards. Subjective quality\nassessment was conducted to evaluate the video quality. Furthermore, we\nbenchmark the database using existing quality assessment algorithms, and\npotential roomis observed to future improve the accuracy of UGC video quality\nmeasures.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.11517v2"
    },
    {
        "title": "Robust Invisible Video Watermarking with Attention",
        "authors": [
            "Kevin Alex Zhang",
            "Lei Xu",
            "Alfredo Cuesta-Infante",
            "Kalyan Veeramachaneni"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The goal of video watermarking is to embed a message within a video file in a\nway such that it minimally impacts the viewing experience but can be recovered\neven if the video is redistributed and modified, allowing media producers to\nassert ownership over their content. This paper presents RivaGAN, a novel\narchitecture for robust video watermarking which features a custom\nattention-based mechanism for embedding arbitrary data as well as two\nindependent adversarial networks which critique the video quality and optimize\nfor robustness. Using this technique, we are able to achieve state-of-the-art\nresults in deep learning-based video watermarking and produce watermarked\nvideos which have minimal visual distortion and are robust against common video\nprocessing operations.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.01285v1"
    },
    {
        "title": "Binocular Rivalry Oriented Predictive Auto-Encoding Network for Blind\n  Stereoscopic Image Quality Measurement",
        "authors": [
            "Jiahua Xu",
            "Wei Zhou",
            "Zhibo Chen",
            "Suiyi Ling",
            "Patrick Le Callet"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Stereoscopic image quality measurement (SIQM) has become increasingly\nimportant for guiding stereo image processing and commutation systems due to\nthe widespread usage of 3D contents. Compared with conventional methods which\nare relied on hand-crafted features, deep learning oriented measurements have\nachieved remarkable performance in recent years. However, most existing deep\nSIQM evaluators are not specifically built for stereoscopic contents and\nconsider little prior domain knowledge of the 3D human visual system (HVS) in\nnetwork design. In this paper, we develop a Predictive Auto-encoDing Network\n(PAD-Net) for blind/No-Reference stereoscopic image quality measurement. In the\nfirst stage, inspired by the predictive coding theory that the cognition system\ntries to match bottom-up visual signal with top-down predictions, we adopt the\nencoder-decoder architecture to reconstruct the distorted inputs. Besides,\nmotivated by the binocular rivalry phenomenon, we leverage the likelihood and\nprior maps generated from the predictive coding process in the Siamese\nframework for assisting SIQM. In the second stage, quality regression network\nis applied to the fusion image for acquiring the perceptual quality prediction.\nThe performance of PAD-Net has been extensively evaluated on three benchmark\ndatabases and the superiority has been well validated on both symmetrically and\nasymmetrically distorted stereoscopic images under various distortion types.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.01738v3"
    },
    {
        "title": "Camera Fingerprint Extraction via Spatial Domain Averaged Frames",
        "authors": [
            "Samet Taspinar",
            "Manoranjan Mohanty",
            "Nasir Memon"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Photo Response Non-Uniformity (PRNU) based camera attribution is an effective\nmethod to determine the source camera of visual media (an image or a video). To\napply this method, images or videos need to be obtained from a camera to create\na \"camera fingerprint\" which then can be compared against the PRNU of the query\nmedia whose origin is under question. The fingerprint extraction process can be\ntime-consuming when a large number of video frames or images have to be\ndenoised. This may need to be done when the individual images have been\nsubjected to high compression or other geometric processing such as video\nstabilization. This paper investigates a simple, yet effective and efficient\ntechnique to create a camera fingerprint when so many still images need to be\ndenoised. The technique utilizes Spatial Domain Averaged (SDA) frames. An\nSDA-frame is the arithmetic mean of multiple still images. When it is used for\nfingerprint extraction, the number of denoising operations can be significantly\ndecreased with little or no performance loss. Experimental results show that\nthe proposed method can work more than 50 times faster than conventional\nmethods while providing similar matching results.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.04573v1"
    },
    {
        "title": "Image Steganography: Protection of Digital Properties against\n  Eavesdropping",
        "authors": [
            "Ramita Maharjan",
            "Ajay Kumar Shrestha",
            "Rejina Basnet"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Steganography is the art of hiding the fact that communication is taking\nplace, by hiding information in other information. Different types of carrier\nfile formats can be used, but digital images are the most popular ones because\nof their frequency on the internet. For hiding secret information in images,\nthere exists a large variety of steganography techniques. Some are more complex\nthan others and all of them have respective strong and weak points. Many\napplications may require absolute invisibility of the secret information. This\npaper intends to give an overview of image steganography, it's usage and\ntechniques, basically, to store the confidential information within images such\nas details of working strategy, secret missions, criminal and confidential\ninformation in various organizations that work for the national security such\nas army, police, FBI, secret service etc. We develop a desktop application that\nincorporates Advanced Encryption Standard for encryption of the original\nmessage, and Spatially Desynchronized Steganography Algorithm for hiding the\ntext file inside the image.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.04685v1"
    },
    {
        "title": "AdaCompress: Adaptive Compression for Online Computer Vision Services",
        "authors": [
            "Hongshan Li",
            "Yu Guo",
            "Zhi Wang",
            "Shutao Xia",
            "Wenwu Zhu"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  With the growth of computer vision based applications and services, an\nexplosive amount of images have been uploaded to cloud servers which host such\ncomputer vision algorithms, usually in the form of deep learning models. JPEG\nhas been used as the {\\em de facto} compression and encapsulation method before\none uploads the images, due to its wide adaptation. However, standard JPEG\nconfiguration does not always perform well for compressing images that are to\nbe processed by a deep learning model, e.g., the standard quality level of JPEG\nleads to 50\\% of size overhead (compared with the best quality level selection)\non ImageNet under the same inference accuracy in popular computer vision models\nincluding InceptionNet, ResNet, etc. Knowing this, designing a better JPEG\nconfiguration for online computer vision services is still extremely\nchallenging: 1) Cloud-based computer vision models are usually a black box to\nend-users; thus it is difficult to design JPEG configuration without knowing\ntheir model structures. 2) JPEG configuration has to change when different\nusers use it. In this paper, we propose a reinforcement learning based JPEG\nconfiguration framework. In particular, we design an agent that adaptively\nchooses the compression level according to the input image's features and\nbackend deep learning models. Then we train the agent in a reinforcement\nlearning way to adapt it for different deep learning cloud services that act as\nthe {\\em interactive training environment} and feeding a reward with\ncomprehensive consideration of accuracy and data size. In our real-world\nevaluation on Amazon Rekognition, Face++ and Baidu Vision, our approach can\nreduce the size of images by 1/2 -- 1/3 while the overall classification\naccuracy only decreases slightly.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.08148v1"
    },
    {
        "title": "sZoom: A Framework for Automatic Zoom into High Resolution Surveillance\n  Videos",
        "authors": [
            "Mukesh Saini",
            "Benjamin Guthier",
            "Hao Kuang",
            "Dwarikanath Mahapatra",
            "Abdulmotaleb El Saddik"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Current cameras are capable of recording high resolution video. While viewing\non a mobile device, a user can manually zoom into this high resolution video to\nget more detailed view of objects and activities. However, manual zooming is\nnot suitable for surveillance and monitoring. It is tiring to continuously keep\nzooming into various regions of the video. Also, while viewing one region, the\noperator may miss activities in other regions. In this paper, we propose sZoom,\na framework to automatically zoom into a high resolution surveillance video.\nThe proposed framework selectively zooms into the sensitive regions of the\nvideo to present details of the scene, while still preserving the overall\ncontext required for situation assessment. A multi-variate Gaussian penalty is\nintroduced to ensure full coverage of the scene. The method achieves near\nreal-time performance through a number of timing optimizations. An extensive\nuser study shows that, while watching a full HD video on a mobile device, the\nsystem enhances the security operator's efficiency in understanding the details\nof the scene by 99% on the average compared to a scaled version of the original\nhigh resolution video. The produced video achieved 46% higher ratings for\nusefulness in a surveillance task.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.10164v1"
    },
    {
        "title": "Cross-Modal Subspace Learning with Scheduled Adaptive Margin Constraints",
        "authors": [
            "David Semedo",
            "João Magalhães"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Cross-modal embeddings, between textual and visual modalities, aim to\norganise multimodal instances by their semantic correlations. State-of-the-art\napproaches use maximum-margin methods, based on the hinge-loss, to enforce a\nconstant margin m, to separate projections of multimodal instances from\ndifferent categories. In this paper, we propose a novel scheduled adaptive\nmaximum-margin (SAM) formulation that infers triplet-specific constraints\nduring training, therefore organising instances by adaptively enforcing\ninter-category and inter-modality correlations. This is supported by a\nscheduled adaptive margin function, that is smoothly activated, replacing a\nstatic margin by an adaptively inferred one reflecting triplet-specific\nsemantic correlations while accounting for the incremental learning behaviour\nof neural networks to enforce category cluster formation and enforcement.\nExperiments on widely used datasets show that our model improved upon\nstate-of-the-art approaches, by achieving a relative improvement of up to\n~12.5% over the second best method, thus confirming the effectiveness of our\nscheduled adaptive margin formulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.13733v1"
    },
    {
        "title": "Hybrid blind robust image watermarking technique based on DFT-DCT and\n  Arnold transform",
        "authors": [
            "Mohamed Hamidi",
            "Mohamed El Haziti",
            "Hocine Cherifi",
            "Mohammed El Hassouni"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this paper, a robust blind image watermarking method is proposed for\ncopyright protection of digital images. This hybrid method relies on combining\ntwo well-known transforms that are the discrete Fourier transform (DFT) and the\ndiscrete cosine transform (DCT). The motivation behind this combination is to\nenhance the imperceptibility and the robustness. The imperceptibility\nrequirement is achieved by using magnitudes of DFT coefficients while the\nrobustness improvement is ensured by applying DCT to the DFT coefficients\nmagnitude. The watermark is embedded by modifying the coefficients of the\nmiddle band of the DCT using a secret key. The security of the proposed method\nis enhanced by applying Arnold transform (AT) to the watermark before\nembedding. Experiments were conducted on natural and textured images. Results\nshow that, compared with state-of-the-art methods, the proposed method is\nrobust to a wide range of attacks while preserving high imperceptibility.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00753v1"
    },
    {
        "title": "CALPA-NET: Channel-pruning-assisted Deep Residual Network for\n  Steganalysis of Digital Images",
        "authors": [
            "Shunquan Tan",
            "Weilong Wu",
            "Zilong Shao",
            "Qiushi Li",
            "Bin Li",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Over the past few years, detection performance improvements of deep-learning\nbased steganalyzers have been usually achieved through structure expansion.\nHowever, excessive expanded structure results in huge computational cost,\nstorage overheads, and consequently difficulty in training and deployment. In\nthis paper we propose CALPA-NET, a ChAnneL-Pruning-Assisted deep residual\nnetwork architecture search approach to shrink the network structure of\nexisting vast, over-parameterized deep-learning based steganalyzers. We observe\nthat the broad inverted-pyramid structure of existing deep-learning based\nsteganalyzers might contradict the well-established model diversity oriented\nphilosophy, and therefore is not suitable for steganalysis. Then a hybrid\ncriterion combined with two network pruning schemes is introduced to adaptively\nshrink every involved convolutional layer in a data-driven manner. The\nresulting network architecture presents a slender bottleneck-like structure. We\nhave conducted extensive experiments on BOSSBase+BOWS2 dataset, more diverse\nALASKA dataset and even a large-scale subset extracted from ImageNet CLS-LOC\ndataset. The experimental results show that the model structure generated by\nour proposed CALPA-NET can achieve comparative performance with less than two\npercent of parameters and about one third FLOPs compared to the original\nsteganalytic model. The new model possesses even better adaptivity,\ntransferability, and scalability.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.04657v2"
    },
    {
        "title": "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A\n  Survey",
        "authors": [
            "Sicheng Zhao",
            "Shangfei Wang",
            "Mohammad Soleymani",
            "Dhiraj Joshi",
            "Qiang Ji"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The wide popularity of digital photography and social networks has generated\na rapidly growing volume of multimedia data (i.e., image, music, and video),\nresulting in a great demand for managing, retrieving, and understanding these\ndata. Affective computing (AC) of these data can help to understand human\nbehaviors and enable wide applications. In this article, we survey the\nstate-of-the-art AC technologies comprehensively for large-scale heterogeneous\nmultimedia data. We begin this survey by introducing the typical emotion\nrepresentation models from psychology that are widely employed in AC. We\nbriefly describe the available datasets for evaluating AC algorithms. We then\nsummarize and compare the representative methods on AC of different multimedia\ntypes, i.e., images, music, videos, and multimodal data, with the focus on both\nhandcrafted features-based methods and deep learning methods. Finally, we\ndiscuss some challenges and future directions for multimedia affective\ncomputing.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.05609v1"
    },
    {
        "title": "Understanding the Teaching Styles by an Attention based Multi-task\n  Cross-media Dimensional modelling",
        "authors": [
            "Suping Zhou",
            "Jia Jia",
            "Yufeng Yin",
            "Xiang Li",
            "Yang Yao",
            "Ying Zhang",
            "Zeyang Ye",
            "Kehua Lei",
            "Yan Huang",
            "Jialie Shen"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Teaching style plays an influential role in helping students to achieve\nacademic success. In this paper, we explore a new problem of effectively\nunderstanding teachers' teaching styles. Specifically, we study 1) how to\nquantitatively characterize various teachers' teaching styles for various\nteachers and 2) how to model the subtle relationship between cross-media\nteaching related data (speech, facial expressions and body motions, content et\nal.) and teaching styles. Using the adjectives selected from more than 10,000\nfeedback questionnaires provided by an educational enterprise, a novel concept\ncalled Teaching Style Semantic Space (TSSS) is developed based on the\npleasure-arousal dimensional theory to describe teaching styles quantitatively\nand comprehensively. Then a multi-task deep learning based model,\nAttention-based Multi-path Multi-task Deep Neural Network (AMMDNN), is proposed\nto accurately and robustly capture the internal correlations between\ncross-media features and TSSS. Based on the benchmark dataset, we further\ndevelop a comprehensive data set including 4,541 full-annotated cross-modality\nteaching classes. Our experimental results demonstrate that the proposed AMMDNN\noutperforms (+0.0842 in terms of the concordance correlation coefficient (CCC)\non average) baseline methods. To further demonstrate the advantages of the\nproposed TSSS and our model, several interesting case studies are carried out,\nsuch as teaching styles comparison among different teachers and courses, and\nleveraging the proposed method for teaching quality analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.07253v1"
    },
    {
        "title": "A Knowledge-Driven Quality-of-Experience Model for Adaptive Streaming\n  Videos",
        "authors": [
            "Zhengfang Duanmu",
            "Wentao Liu",
            "Diqi Chen",
            "Zhuoran Li",
            "Zhou Wang",
            "Yizhou Wang",
            "Wen Gao"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The fundamental conflict between the enormous space of adaptive streaming\nvideos and the limited capacity for subjective experiment casts significant\nchallenges to objective Quality-of-Experience (QoE) prediction. Existing\nobjective QoE models exhibit complex functional form, failing to generalize\nwell in diverse streaming environments. In this study, we propose an objective\nQoE model namely knowledge-driven streaming quality index (KSQI) to integrate\nprior knowledge on the human visual system and human annotated data in a\nprincipled way. By analyzing the subjective characteristics towards streaming\nvideos from a corpus of subjective studies, we show that a family of QoE\nfunctions lies in a convex set. Using a variant of projected gradient descent,\nwe optimize the objective QoE model over a database of training videos. The\nproposed KSQI demonstrates strong generalizability to diverse streaming\nenvironments, evident by state-of-the-art performance on four publicly\navailable benchmark datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.07944v1"
    },
    {
        "title": "Fine granularity access in interactive compression of 360-degree images\n  based on rate-adaptive channel codes",
        "authors": [
            "Navid Mahmoudian Bidgoli",
            "Thomas Maugey",
            "Aline Roumy"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In this paper, we propose a new interactive compression scheme for\nomnidirectional images. This requires two characteristics: efficient\ncompression of data, to lower the storage cost, and random access ability to\nextract part of the compressed stream requested by the user (for reducing the\ntransmission rate). For efficient compression, data needs to be predicted by a\nseries of references that have been pre-defined and compressed. This contrasts\nwith the spirit of random accessibility. We propose a solution for this problem\nbased on incremental codes implemented by rate-adaptive channel codes. This\nscheme encodes the image while adapting to any user request and leads to an\nefficient coding that is flexible in extracting data depending on the available\ninformation at the decoder. Therefore, only the information that is needed to\nbe displayed at the user's side is transmitted during the user's request, as if\nthe request was already known at the encoder. The experimental results\ndemonstrate that our coder obtains a better transmission rate than the\nstate-of-the-art tile-based methods at a small cost in storage. Moreover, the\ntransmission rate grows gradually with the size of the request and avoids a\nstaircase effect, which shows the perfect suitability of our coder for\ninteractive transmission.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.14239v2"
    },
    {
        "title": "QoE-Driven UAV-Enabled Pseudo-Analog Wireless Video Broadcast: A Joint\n  Optimization of Power and Trajectory",
        "authors": [
            "Xiao-Wei Tang",
            "Xin-Lin Huang",
            "Fei Hu"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The explosive demands for high quality mobile video services have caused\nheavy overload to the existing cellular networks. Although the small cell has\nbeen proposed to alleviate such a problem, the network operators may not be\ninterested in deploying numerous base stations (BSs) due to expensive\ninfrastructure construction and maintenance. The unmanned aerial vehicles\n(UAVs) can provide the low-cost and quick deployment, which can support\nhigh-quality line-of-sight communications and have become promising mobile BSs.\nIn this paper, we propose a quality-of-experience (QoE)-driven UAV-enabled\npseudo-analog wireless video broadcast scheme, which provides mobile video\nbroadcast services for ground users (GUs). Due to limited energy available in\nUAV, the aim of the proposed scheme is to maximize the minimum peak\nsignal-to-noise ratio (PSNR) of GUs' video reconstruction quality by jointly\noptimizing the transmission power allocation strategy and the UAV trajectory.\nFirstly, the reconstructed video quality at GUs is defined under the\nconstraints of the UAV's total energy and motion mechanism, and the proposed\nscheme is formulated as a complex non-convex optimization problem. Then, the\noptimization problem is simplified to obtain a tractable suboptimal solution\nwith the help of the block coordinate descent model and the successive convex\napproximation model. Finally, the experimental results are presented to show\nthe effectiveness of the proposed scheme. Specifically, the proposed scheme can\nachieve over 1.6dB PSNR gains in terms of GUs' minimum PSNR, compared with the\nstate-of-the-art schemes, e.g., DVB, SoftCast, and SharpCast.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.14438v1"
    },
    {
        "title": "Retracing the Flow of the Stream: Investigating Kodi Streaming Services",
        "authors": [
            "Samuel Todd Bromley",
            "John Sheppard",
            "Mark Scanlon",
            "Nhien-An Le-Khac"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Kodi is of one of the world's largest open-source streaming platforms for\nviewing video content. Easily installed Kodi add-ons facilitate access to\nonline pirated videos and streaming content by facilitating the user to search\nand view copyrighted videos with a basic level of technical knowledge. In some\ncountries, there have been paid child sexual abuse organizations\npublishing/streaming child abuse material to an international paying clientele.\nOpen source software used for viewing videos from the Internet, such as Kodi,\nis being exploited by criminals to conduct their activities. In this paper, we\ndescribe a new method to quickly locate Kodi artifacts and gather information\nfor a successful prosecution. We also evaluate our approach on different\nplatforms; Windows, Android and Linux. Our experiments show the file location,\nartifacts and a history of viewed content including their locations from the\nInternet. Our approach will serve as a resource to forensic investigators to\nexamine Kodi or similar streaming platforms.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.01107v1"
    },
    {
        "title": "DIPPAS: A Deep Image Prior PRNU Anonymization Scheme",
        "authors": [
            "Francesco Picetti",
            "Sara Mandelli",
            "Paolo Bestagini",
            "Vincenzo Lipari",
            "Stefano Tubaro"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Source device identification is an important topic in image forensics since\nit allows to trace back the origin of an image. Its forensics counter-part is\nsource device anonymization, that is, to mask any trace on the image that can\nbe useful for identifying the source device. A typical trace exploited for\nsource device identification is the Photo Response Non-Uniformity (PRNU), a\nnoise pattern left by the device on the acquired images. In this paper, we\ndevise a methodology for suppressing such a trace from natural images without\nsignificant impact on image quality. Specifically, we turn PRNU anonymization\ninto an optimization problem in a Deep Image Prior (DIP) framework. In a\nnutshell, a Convolutional Neural Network (CNN) acts as generator and returns an\nimage that is anonymized with respect to the source PRNU, still maintaining\nhigh visual quality. With respect to widely-adopted deep learning paradigms,\nour proposed CNN is not trained on a set of input-target pairs of images.\nInstead, it is optimized to reconstruct the PRNU-free image from the original\nimage under analysis itself. This makes the approach particularly suitable in\nscenarios where large heterogeneous databases are analyzed and prevents any\nproblem due to lack of generalization. Through numerical examples on publicly\navailable datasets, we prove our methodology to be effective compared to\nstate-of-the-art techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.03581v2"
    },
    {
        "title": "Study on the Assessment of the Quality of Experience of Streaming Video",
        "authors": [
            "Aleksandr Ivchenko",
            "Pavel Kononyuk",
            "Alexander Dvorkovich",
            "Liubov Antiufrieva"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Dynamic adaptive streaming over HTTP provides the work of most multimedia\nservices, however, the nature of this technology further complicates the\nassessment of the QoE (Quality of Experience). In this paper, the influence of\nvarious objective factors on the subjective estimation of the QoE of streaming\nvideo is studied. The paper presents standard and handcrafted features, shows\ntheir correlation and p-Value of significance. VQA (Video Quality Assessment)\nmodels based on regression and gradient boosting with SRCC reaching up to\n0.9647 on the validation subsample are proposed. The proposed regression models\nare adapted for applied applications (both with and without a reference video);\nthe Gradient Boosting Regressor model is perspective for further improvement of\nthe quality estimation model. We take SQoE-III database, so far the largest and\nmost realistic of its kind. The VQA (video quality assessment) models are\navailable at https://github.com/AleksandrIvchenko/QoE-assesment\n",
        "pdf_link": "http://arxiv.org/pdf/2012.04623v1"
    },
    {
        "title": "Coverless Video Steganography based on Maximum DC Coefficients",
        "authors": [
            "Laijin Meng",
            "Xinghao Jiang",
            "Zhenzhen Zhang",
            "Zhaohong Li",
            "Tanfeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Coverless steganography has been a great interest in recent years, since it\nis a technology that can absolutely resist the detection of steganalysis by not\nmodifying the carriers. However, most existing coverless steganography\nalgorithms select images as carriers, and few studies are reported on coverless\nvideo steganography. In fact, video is a securer and more informative carrier.\nIn this paper, a novel coverless video steganography algorithm based on maximum\nDirect Current (DC) coefficients is proposed. Firstly, a Gaussian distribution\nmodel of DC coefficients considering video coding process is built, which\nindicates that the distribution of changes for maximum DC coefficients in a\nblock is more stable than the adjacent DC coefficients. Then, a novel hash\nsequence generation method based on the maximum DC coefficients is proposed.\nAfter that, the video index structure is established to speed up the efficiency\nof searching videos. In the process of information hiding, the secret\ninformation is converted into binary segments, and the video whose hash\nsequence equals to secret information segment is selected as the carrier\naccording to the video index structure. Finally, all of the selected videos and\nauxiliary information are sent to the receiver. Especially, the subjective\nsecurity of video carriers, the cost of auxiliary information and the\nrobustness to video compression are considered for the first time in this\npaper. Experimental results and analysis show that the proposed algorithm\nperforms better in terms of capacity, robustness, and security, compared with\nthe state-of-the-art coverless steganography algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.06809v1"
    },
    {
        "title": "An adaptive algorithm for embedding information into compressed JPEG\n  images using the QIM method",
        "authors": [
            "Anna Melman",
            "Pavel Petrov",
            "Alexander Shelupanov"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The widespread use of JPEG images makes them good covers for secret messages\nstoring and transmitting. This paper proposes a new algorithm for embedding\ninformation in JPEG images based on the steganographic QIM method. The main\nproblem of such embedding is the vulnerability to statistical steganalysis. To\nsolve this problem, it is proposed to use a variable quantization step, which\nis adaptively selected for each block of the JPEG cover image. Experimental\nresults show that the proposed approach successfully increases the security of\nembedding.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08742v1"
    },
    {
        "title": "Self-Supervision based Task-Specific Image Collection Summarization",
        "authors": [
            "Anurag Singh",
            "Deepak Kumar Sharma",
            "Sudhir Kumar Sharma"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Successful applications of deep learning (DL) requires large amount of\nannotated data. This often restricts the benefits of employing DL to businesses\nand individuals with large budgets for data-collection and computation.\nSummarization offers a possible solution by creating much smaller\nrepresentative datasets that can allow real-time deep learning and analysis of\nbig data and thus democratize use of DL. In the proposed work, our aim is to\nexplore a novel approach to task-specific image corpus summarization using\nsemantic information and self-supervision. Our method uses a\nclassification-based Wasserstein generative adversarial network (CLSWGAN) as a\nfeature generating network. The model also leverages rotational invariance as\nself-supervision and classification on another task. All these objectives are\nadded on a features from resnet34 to make it discriminative and robust. The\nmodel then generates a summary at inference time by using K-means clustering in\nthe semantic embedding space. Thus, another main advantage of this model is\nthat it does not need to be retrained each time to obtain summaries of\ndifferent lengths which is an issue with current end-to-end models. We also\ntest our model efficacy by means of rigorous experiments both qualitatively and\nquantitatively.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.10657v4"
    },
    {
        "title": "Sub-sampled Cross-component Prediction for Emerging Video Coding\n  Standards",
        "authors": [
            "Junru Li",
            "Meng Wang",
            "Li Zhang",
            "Shiqi Wang",
            "Kai Zhang",
            "Shanshe Wang",
            "Siwei Ma",
            "Wen Gao"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Cross-component linear model (CCLM) prediction has been repeatedly proven to\nbe effective in reducing the inter-channel redundancies in video compression.\nEssentially speaking, the linear model is identically trained by employing\naccessible luma and chroma reference samples at both encoder and decoder,\nelevating the level of operational complexity due to the least square\nregression or max-min based model parameter derivation. In this paper, we\ninvestigate the capability of the linear model in the context of sub-sampled\nbased cross-component correlation mining, as a means of significantly releasing\nthe operation burden and facilitating the hardware and software design for both\nencoder and decoder. In particular, the sub-sampling ratios and positions are\nelaborately designed by exploiting the spatial correlation and the\ninter-channel correlation. Extensive experiments verify that the proposed\nmethod is characterized by its simplicity in operation and robustness in terms\nof rate-distortion performance, leading to the adoption by Versatile Video\nCoding (VVC) standard and the third generation of Audio Video Coding Standard\n(AVS3).\n",
        "pdf_link": "http://arxiv.org/pdf/2012.15067v1"
    },
    {
        "title": "A Machine Learning Approach to Optimal Inverse Discrete Cosine Transform\n  (IDCT) Design",
        "authors": [
            "Yifan Wang",
            "Zhanxuan Mei",
            "Chia-Yang Tsai",
            "Ioannis Katsavounidis",
            "C. -C. Jay Kuo"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The design of the optimal inverse discrete cosine transform (IDCT) to\ncompensate the quantization error is proposed for effective lossy image\ncompression in this work. The forward and inverse DCTs are designed in pair in\ncurrent image/video coding standards without taking the quantization effect\ninto account. Yet, the distribution of quantized DCT coefficients deviate from\nthat of original DCT coefficients. This is particularly obvious when the\nquality factor of JPEG compressed images is small. To address this problem, we\nfirst use a set of training images to learn the compound effect of forward DCT,\nquantization and dequantization in cascade. Then, a new IDCT kernel is learned\nto reverse the effect of such a pipeline. Experiments are conducted to\ndemonstrate that the advantage of the new method, which has a gain of\n0.11-0.30dB over the standard JPEG over a wide range of quality factors.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.00502v1"
    },
    {
        "title": "Optimizing Video Caching at the Edge: A Hybrid Multi-Point Process\n  Approach",
        "authors": [
            "Xianzhi Zhang",
            "Yipeng Zhou",
            "Di Wu",
            "Miao Hu",
            "James Xi Zheng",
            "Min Chen",
            "Song Guo"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  It is always a challenging problem to deliver a huge volume of videos over\nthe Internet. To meet the high bandwidth and stringent playback demand, one\nfeasible solution is to cache video contents on edge servers based on predicted\nvideo popularity. Traditional caching algorithms (e.g., LRU, LFU) are too\nsimple to capture the dynamics of video popularity, especially long-tailed\nvideos. Recent learning-driven caching algorithms (e.g., DeepCache) show\npromising performance, however, such black-box approaches are lack of\nexplainability and interpretability. Moreover, the parameter tuning requires a\nlarge number of historical records, which are difficult to obtain for videos\nwith low popularity. In this paper, we optimize video caching at the edge using\na white-box approach, which is highly efficient and also completely\nexplainable. To accurately capture the evolution of video popularity, we\ndevelop a mathematical model called \\emph{HRS} model, which is the combination\nof multiple point processes, including Hawkes' self-exciting, reactive and\nself-correcting processes. The key advantage of the HRS model is its\nexplainability, and much less number of model parameters. In addition, all its\nmodel parameters can be learned automatically through maximizing the\nLog-likelihood function constructed by past video request events. Next, we\nfurther design an online HRS-based video caching algorithm. To verify its\neffectiveness, we conduct a series of experiments using real video traces\ncollected from Tencent Video, one of the largest online video providers in\nChina. Experiment results demonstrate that our proposed algorithm outperforms\nthe state-of-the-art algorithms, with 12.3\\% improvement on average in terms of\ncache hit rate under realistic settings.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.00646v1"
    },
    {
        "title": "Fake-image detection with Robust Hashing",
        "authors": [
            "Miki Tanaka",
            "Hitoshi Kiya"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In this paper, we investigate whether robust hashing has a possibility to\nrobustly detect fake-images even when multiple manipulation techniques such as\nJPEG compression are applied to images for the first time. In an experiment,\nthe proposed fake detection with robust hashing is demonstrated to outperform\nstate-of-the-art one under the use of various datasets including fake images\ngenerated with GANs.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.01313v1"
    },
    {
        "title": "Extend the FFmpeg Framework to Analyze Media Content",
        "authors": [
            "Xintian Wu",
            "Pengfei Qu",
            "Shaofei Wang",
            "Lin Xie",
            "Jie Dong"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  This paper introduces a new set of video analytics plugins developed for the\nFFmpeg framework. Multimedia applications that increasingly utilize the FFmpeg\nmedia features for its comprehensive media encoding, decoding, muxing, and\ndemuxing capabilities can now additionally analyze the video content based on\nAI models. The plugins are thread optimized for best performance overcoming\ncertain FFmpeg threading limitations. The plugins utilize the Intel OpenVINO\nToolkit inference engine as the backend. The analytics workloads are\naccelerated on different platforms such as CPU, GPU, FPGA or specialized\nanalytics accelerators. With our reference implementation, the feature of\nOpenVINO as inference backend has been pushed into FFmpeg mainstream\nrepository. We plan to submit more patches later.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.03539v1"
    },
    {
        "title": "Multi-view data capture for dynamic object reconstruction using handheld\n  augmented reality mobiles",
        "authors": [
            "M. Bortolon",
            "L. Bazzanella",
            "F. Poiesi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We propose a system to capture nearly-synchronous frame streams from multiple\nand moving handheld mobiles that is suitable for dynamic object 3D\nreconstruction. Each mobile executes Simultaneous Localisation and Mapping\non-board to estimate its pose, and uses a wireless communication channel to\nsend or receive synchronisation triggers. Our system can harvest frames and\nmobile poses in real time using a decentralised triggering strategy and a\ndata-relay architecture that can be deployed either at the Edge or in the\nCloud. We show the effectiveness of our system by employing it for 3D skeleton\nand volumetric reconstructions. Our triggering strategy achieves equal\nperformance to that of an NTP-based synchronisation approach, but offers higher\nflexibility, as it can be adjusted online based on application needs. We\ncreated a challenging new dataset, namely 4DM, that involves six handheld\naugmented reality mobiles recording an actor performing sports actions\noutdoors. We validate our system on 4DM, analyse its strengths and limitations,\nand compare its modules with alternative ones.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.07883v2"
    },
    {
        "title": "A Survey of Multimedia Technologies and Robust Algorithms",
        "authors": [
            "Zijian Kuang",
            "Xinran Tie"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Multimedia technologies are now more practical and deployable in real life,\nand the algorithms are widely used in various researching areas such as deep\nlearning, signal processing, haptics, computer vision, robotics, and medical\nmultimedia processing. This survey provides an overview of multimedia\ntechnologies and robust algorithms in multimedia data processing, medical\nmultimedia processing, human facial expression tracking and pose recognition,\nand multimedia in education and training. This survey will also analyze and\npropose a future research direction based on the overview of current robust\nalgorithms and multimedia technologies. We want to thank the research and\nprevious work done by the Multimedia Research Centre (MRC), the University of\nAlberta, which is the inspiration and starting point for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.13477v2"
    },
    {
        "title": "Product semantics translation from brain activity via adversarial\n  learning",
        "authors": [
            "Pan Wang",
            "Zhifeng Gong",
            "Shuo Wang",
            "Hao Dong",
            "Jialu Fan",
            "Ling Li",
            "Peter Childs",
            "Yike Guo"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  A small change of design semantics may affect a user's satisfaction with a\nproduct. To modify a design semantic of a given product from personalised brain\nactivity via adversarial learning, in this work, we propose a deep generative\ntransformation model to modify product semantics from the brain signal. We\nattempt to accomplish such synthesis: 1) synthesising the product image with\nnew features corresponding to EEG signal; 2) maintaining the other image\nfeatures that irrelevant to EEG signal. We leverage the idea of StarGAN and the\nmodel is designed to synthesise products with preferred design semantics\n(colour & shape) via adversarial learning from brain activity, and is applied\nwith a case study to generate shoes with different design semantics from\nrecorded EEG signals. To verify our proposed cognitive transformation model, a\ncase study has been presented. The results work as a proof-of-concept that our\nframework has the potential to synthesis product semantic from brain activity.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.15602v1"
    },
    {
        "title": "Seeing through a Black Box: Toward High-Quality Terahertz\n  TomographicImaging via Multi-Scale Spatio-Spectral Image Fusion",
        "authors": [
            "Weng-tai Su",
            "Yi-Chun Hung",
            "Ta-Hsuan Chao",
            "Po-Jen Yu",
            "Shang-Hua Yang",
            "Chia-Wen Lin"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Terahertz (THz) imaging has recently attracted significant attention thanks\nto its non-invasive, non-destructive, non-ionizing, material-classification,\nand ultra-fast nature for object exploration and inspection. However, its\nstrong water absorption nature and low noise tolerance lead to undesired blurs\nand distortions of reconstructed THz images. The performances of existing\nrestoration methods are highly constrained by the diffraction-limited THz\nsignals. To address the problem, we propose a novel\nSubspace-and-Attention-guided Restoration Network (SARNet) that fuses\nmulti-spectral features of a THz image for effective restoration. To this end,\nSARNet uses multi-scale branches to extract spatio-spectral features of\namplitude and phase which are then fused via shared subspace projection and\nattention guidance. Here, we experimentally construct ultra-fast THz\ntime-domain spectroscopy system covering a broad frequency range from 0.1 THz\nto 4 THz for building up temporal/spectral/spatial/phase/material THz database\nof hidden 3D objects. Complementary to a quantitative evaluation, we\ndemonstrate the effectiveness of our SARNet model on 3D THz tomographic\nreconstruction\n",
        "pdf_link": "http://arxiv.org/pdf/2103.16932v2"
    },
    {
        "title": "Improved CNN Prediction Based Reversible Data Hiding",
        "authors": [
            "Yingqiang Qiu",
            "Wanli Peng",
            "Xiaodan Lin",
            "Huanqiang Zeng",
            "Zhenxing Qian"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  This letter proposes an improved CNN predictor (ICNNP) for reversible data\nhiding (RDH) in images, which consists of a feature extraction module, a pixel\nprediction module, and a complexity prediction module. Due to predicting the\ncomplexity of each pixel with the ICNNP during the embedding process, the\nproposed method can achieve superior performance than the CNN predictor-based\nmethod. Specifically, an input image does be first split into two different\nsub-images, i.e., the \"Dot\" image and the \"Cross\" image. Meanwhile, each\nsub-image is applied to predict another one. Then, the prediction errors of\npixels are sorted with the predicted pixel complexities. In light of this, some\nsorted prediction errors with less complexity are selected to be efficiently\nused for low-distortion data embedding with a traditional histogram shift\nscheme. Experimental results demonstrate that the proposed method can achieve\nbetter embedding performance than that of the CNN predictor with the same\nhistogram shifting strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01420v1"
    },
    {
        "title": "Text2Poster: Laying out Stylized Texts on Retrieved Images",
        "authors": [
            "Chuhao Jin",
            "Hongteng Xu",
            "Ruihua Song",
            "Zhiwu Lu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Poster generation is a significant task for a wide range of applications,\nwhich is often time-consuming and requires lots of manual editing and artistic\nexperience. In this paper, we propose a novel data-driven framework, called\n\\textit{Text2Poster}, to automatically generate visually-effective posters from\ntextual information. Imitating the process of manual poster editing, our\nframework leverages a large-scale pretrained visual-textual model to retrieve\nbackground images from given texts, lays out the texts on the images\niteratively by cascaded auto-encoders, and finally, stylizes the texts by a\nmatching-based method. We learn the modules of the framework by weakly- and\nself-supervised learning strategies, mitigating the demand for labeled data.\nBoth objective and subjective experiments demonstrate that our Text2Poster\noutperforms state-of-the-art methods, including academic research and\ncommercial software, on the quality of generated posters.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.02363v1"
    },
    {
        "title": "CS-lol: a Dataset of Viewer Comment with Scene in E-sports\n  Live-streaming",
        "authors": [
            "Junjie H. Xu",
            "Yu Nakano",
            "Lingrong Kong",
            "Kojiro Iizuka"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Billions of live-streaming viewers share their opinions on scenes they are\nwatching in real-time and interact with the event, commentators as well as\nother viewers via text comments. Thus, there is necessary to explore viewers'\ncomments with scenes in E-sport live-streaming events. In this paper, we\ndeveloped CS-lol, a new large-scale dataset containing comments from viewers\npaired with descriptions of game scenes in E-sports live-streaming. Moreover,\nwe propose a task, namely viewer comment retrieval, to retrieve the viewer\ncomments for the scene of the live-streaming event. Results on a series of\nbaseline retrieval methods derived from typical IR evaluation methods show our\ntask as a challenging task. Finally, we release CS-lol and baseline\nimplementation to the research community as a resource.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.06876v1"
    },
    {
        "title": "Reduced-Reference Quality Assessment of Point Clouds via\n  Content-Oriented Saliency Projection",
        "authors": [
            "Wei Zhou",
            "Guanghui Yue",
            "Ruizeng Zhang",
            "Yipeng Qin",
            "Hantao Liu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Many dense 3D point clouds have been exploited to represent visual objects\ninstead of traditional images or videos. To evaluate the perceptual quality of\nvarious point clouds, in this letter, we propose a novel and efficient\nReduced-Reference quality metric for point clouds, which is based on\nContent-oriented sAliency Projection (RR-CAP). Specifically, we make the first\nattempt to simplify reference and distorted point clouds into projected\nsaliency maps with a downsampling operation. Through this process, we tackle\nthe issue of transmitting large-volume original point clouds to user-ends for\nquality assessment. Then, motivated by the characteristics of the human visual\nsystem (HVS), the objective quality scores of distorted point clouds are\nproduced by combining content-oriented similarity and statistical correlation\nmeasurements. Finally, extensive experiments are conducted on SJTU-PCQA and WPC\ndatabases. The experimental results demonstrate that our proposed algorithm\noutperforms existing reduced-reference and no-reference quality metrics, and\nsignificantly reduces the performance gap between state-of-the-art\nfull-reference quality assessment methods. In addition, we show the performance\nvariation of each proposed technical component by ablation tests.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.07681v1"
    },
    {
        "title": "The Metaverse from a Multimedia Communications Perspective",
        "authors": [
            "Haiwei Dong",
            "Jeannie S. A. Lee"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  eXtended reality (XR) technologies such as virtual reality and 360{\\deg}\nstereoscopic streaming enable the concept of the Metaverse, an immersive\nvirtual space for collaboration and interaction. To ensure high fidelity\ndisplay of immersive media, the bandwidth, latency and network traffic patterns\nwill need to be considered to ensure a user's Quality of Experience (QoE). In\nthis article, examples and calculations are explored to demonstrate the\nrequirements of the abovementioned parameters. Additionally, future methods\nsuch as network-awareness using reinforcement learning (RL) and XR content\nawareness using spatial or temporal difference in the frames could be explored\nfrom a multimedia communications perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.07740v1"
    },
    {
        "title": "M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing\n  System",
        "authors": [
            "Chenqi Kong",
            "Kexin Zheng",
            "Yibing Liu",
            "Shiqi Wang",
            "Anderson Rocha",
            "Haoliang Li"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Face presentation attacks (FPA), also known as face spoofing, have brought\nincreasing concerns to the public through various malicious applications, such\nas financial fraud and privacy leakage. Therefore, safeguarding face\nrecognition systems against FPA is of utmost importance. Although existing\nlearning-based face anti-spoofing (FAS) models can achieve outstanding\ndetection performance, they lack generalization capability and suffer\nsignificant performance drops in unforeseen environments. Many methodologies\nseek to use auxiliary modality data (e.g., depth and infrared maps) during the\npresentation attack detection (PAD) to address this limitation. However, these\nmethods can be limited since (1) they require specific sensors such as depth\nand infrared cameras for data capture, which are rarely available on commodity\nmobile devices, and (2) they cannot work properly in practical scenarios when\neither modality is missing or of poor quality. In this paper, we devise an\naccurate and robust MultiModal Mobile Face Anti-Spoofing system named M3FAS to\novercome the issues above. The primary innovation of this work lies in the\nfollowing aspects: (1) To achieve robust PAD, our system combines visual and\nauditory modalities using three commonly available sensors: camera, speaker,\nand microphone; (2) We design a novel two-branch neural network with three\nhierarchical feature aggregation modules to perform cross-modal feature fusion;\n(3). We propose a multi-head training strategy, allowing the model to output\npredictions from the vision, acoustic, and fusion heads, resulting in a more\nflexible PAD. Extensive experiments have demonstrated the accuracy, robustness,\nand flexibility of M3FAS under various challenging experimental settings. The\nsource code and dataset are available at: https://github.com/ChenqiKONG/M3FAS/\n",
        "pdf_link": "http://arxiv.org/pdf/2301.12831v3"
    },
    {
        "title": "STIMONT: A core ontology for multimedia stimuli description",
        "authors": [
            "Marko Horvat",
            "Nikola Bogunović",
            "Krešimir Ćosić"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Affective multimedia documents such as images, sounds or videos elicit\nemotional responses in exposed human subjects. These stimuli are stored in\naffective multimedia databases and successfully used for a wide variety of\nresearch in psychology and neuroscience in areas related to attention and\nemotion processing. Although important all affective multimedia databases have\nnumerous deficiencies which impair their applicability. These problems, which\nare brought forward in the paper, result in low recall and precision of\nmultimedia stimuli retrieval which makes creating emotion elicitation\nprocedures difficult and labor-intensive. To address these issues a new core\nontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and\nextends W3C EmotionML format with an expressive and formal representation of\naffective concepts, high-level semantics, stimuli document metadata and the\nelicited physiology. The advantages of ontology in description of affective\nmultimedia stimuli are demonstrated in a document retrieval experiment and\ncompared against contemporary keyword-based querying methods. Also, a software\ntool Intelligent Stimulus Generator for retrieval of affective multimedia and\nconstruction of stimuli sequences is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.2482v1"
    },
    {
        "title": "Image Block Loss Restoration Using Sparsity Pattern as Side Information",
        "authors": [
            "Hossein Hosseini",
            "Ali Goli",
            "Neda Barzegar Marvasti",
            "Masoume Azghani",
            "Farokh Marvasti"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  In this paper, we propose a method for image block loss restoration based on\nthe notion of sparse representation. We use the sparsity pattern as side\ninformation to efficiently restore block losses by iteratively imposing the\nconstraints of spatial and transform domains on the corrupted image. Two novel\nfeatures, including a pre-interpolation and a criterion for stopping the\niterations, are proposed to improve the performance. Also, to deal with\npractical applications, we develop a technique to transmit the side information\nalong with the image. In this technique, we first compress the side information\nand then embed its LDPC coded version in the least significant bits of the\nimage pixels. This technique ensures the error-free transmission of the side\ninformation, while causing only a small perturbation on the transmitted image.\nMathematical analysis and extensive simulations are performed to validate the\nmethod and investigate the efficiency of the proposed techniques. The results\nverify that the proposed method outperforms its counterparts for image block\nloss restoration.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.5966v2"
    },
    {
        "title": "Image Restoration Using Joint Statistical Modeling in Space-Transform\n  Domain",
        "authors": [
            "Jian Zhang",
            "Debin Zhao",
            "Ruiqin Xiong",
            "Siwei Ma",
            "Wen Gao"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  This paper presents a novel strategy for high-fidelity image restoration by\ncharacterizing both local smoothness and nonlocal self-similarity of natural\nimages in a unified statistical manner. The main contributions are three-folds.\nFirst, from the perspective of image statistics, a joint statistical modeling\n(JSM) in an adaptive hybrid space-transform domain is established, which offers\na powerful mechanism of combining local smoothness and nonlocal self-similarity\nsimultaneously to ensure a more reliable and robust estimation. Second, a new\nform of minimization functional for solving image inverse problem is formulated\nusing JSM under regularization-based framework. Finally, in order to make JSM\ntractable and robust, a new Split-Bregman based algorithm is developed to\nefficiently solve the above severely underdetermined inverse problem associated\nwith theoretical proof of convergence. Extensive experiments on image\ninpainting, image deblurring and mixed Gaussian plus salt-and-pepper noise\nremoval applications verify the effectiveness of the proposed algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.3173v1"
    },
    {
        "title": "Steganalysis: Detecting LSB Steganographic Techniques",
        "authors": [
            "Tanmoy Sarkar",
            "Sugata Sanyal"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Steganalysis means analysis of stego images. Like cryptanalysis, steganalysis\nis used to detect messages often encrypted using secret key from stego images\nproduced by steganography techniques. Recently lots of new and improved\nsteganography techniques are developed and proposed by researchers which\nrequire robust steganalysis techniques to detect the stego images having\nminimum false alarm rate. This paper discusses about the different Steganalysis\ntechniques and help to understand how, where and when this techniques can be\nused based on different situations.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.5119v1"
    },
    {
        "title": "Detection Bank: An Object Detection Based Video Representation for\n  Multimedia Event Recognition",
        "authors": [
            "Tim Althoff",
            "Hyun Oh Song",
            "Trevor Darrell"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  While low-level image features have proven to be effective representations\nfor visual recognition tasks such as object recognition and scene\nclassification, they are inadequate to capture complex semantic meaning\nrequired to solve high-level visual tasks such as multimedia event detection\nand recognition. Recognition or retrieval of events and activities can be\nimproved if specific discriminative objects are detected in a video sequence.\nIn this paper, we propose an image representation, called Detection Bank, based\non the detection images from a large number of windowed object detectors where\nan image is represented by different statistics derived from these detections.\nThis representation is extended to video by aggregating the key frame level\nimage representations through mean and max pooling. We empirically show that it\ncaptures complementary information to state-of-the-art representations such as\nSpatial Pyramid Matching and Object Bank. These descriptors combined with our\nDetection Bank representation significantly outperforms any of the\nrepresentations alone on TRECVID MED 2011 data.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.7102v2"
    },
    {
        "title": "Sonic interaction with a virtual orchestra of factory machinery",
        "authors": [
            "Laurent Simon",
            "Florian Nouviale",
            "Ronan Gaugne",
            "Valérie Gouranton"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  This paper presents an immersive application where users receive sound and\nvisual feedbacks on their interactions with a virtual environment. In this\napplication, the users play the part of conductors of an orchestra of factory\nmachines since each of their actions on interaction devices triggers a pair of\nvisual and audio responses. Audio stimuli were spatialized around the listener.\nThe application was exhibited during the 2013 Science and Music day and\ndesigned to be used in a large immersive system with head tracking, shutter\nglasses and a 10.2 loudspeaker configuration.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.2221v1"
    },
    {
        "title": "A Digital Watermarking Approach Based on DCT Domain Combining QR Code\n  and Chaotic Theory",
        "authors": [
            "Qingbo Kang",
            "Ke Li",
            "Jichun Yang"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  This paper proposes a robust watermarking approach based on Discrete Cosine\nTransform domain that combines Quick Response Code and chaotic system.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.7337v1"
    },
    {
        "title": "An adaptive quasi harmonic broadcasting scheme with optimal bandwidth\n  requirement",
        "authors": [
            "Farzana Afrin",
            "Mohammad Saiedur Rahaman"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  The aim of Harmonic Broadcasting protocol is to reduce the bandwidth usage in\nvideo-on-demand service where a video is divided into some equal sized segments\nand every segment is repeatedly transmitted over a number of channels that\nfollows harmonic series for channel bandwidth assignment. As the bandwidth of\nchannels differs from each other and users can join at any time to these\nmulticast channels, they may experience a synchronization problem between\ndownload and playback. To deal with this issue, some schemes have been\nproposed, however, at the cost of additional or wastage of bandwidth or sudden\nextreme bandwidth requirement. In this paper we present an adaptive quasi\nharmonic broadcasting scheme (AQHB) which delivers all data segment on time\nthat is the download and playback synchronization problem is eliminated while\nkeeping the bandwidth consumption as same as traditional harmonic broadcasting\nscheme without cost of any additional or wastage of bandwidth. It also ensures\nthe video server not to increase the channel bandwidth suddenly that is, also\neliminates the sudden buffer requirement at the client side. We present several\nanalytical results to exhibit the efficiency of our proposed broadcasting\nscheme over the existing ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.1474v1"
    },
    {
        "title": "Hiding Sound in Image by K-LSB Mutation",
        "authors": [
            "Ankur Gupta",
            "Ankit Chaudhary"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  In this paper a novel approach to hide sound files in a digital image is\nproposed and implemented such that it becomes difficult to conclude about the\nexistence of the hidden data inside the image. In this approach, we utilize the\nrightmost k-LSB of pixels in an image to embed MP3 sound bits into a pixel. The\npixels are so chosen that the distortion in image would be minimized due to\nembedding. This requires comparing all the possible permutations of pixel\nvalues, which may would lead to exponential time computation. To speed up this,\nCuckoo Search (CS) could be used to find the most optimal solution. The\nadvantage of using proposed CS is that it is easy to implement and is very\neffective at converging in relatively less iterations/generations.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.6592v1"
    },
    {
        "title": "StegExpose - A Tool for Detecting LSB Steganography",
        "authors": [
            "Benedikt Boehm"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  Steganalysis tools play an important part in saving time and providing new\nangles of attack for forensic analysts. StegExpose is a solution designed for\nuse in the real world, and is able to analyse images for LSB steganography in\nbulk using proven attacks in a time efficient manner. When steganalytic methods\nare combined intelligently, they are able generate even more accurate results.\nThis is the prime focus of StegExpose.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.6656v1"
    },
    {
        "title": "Steganography in Modern Smartphones and Mitigation Techniques",
        "authors": [
            "Wojciech Mazurczyk",
            "Luca Caviglione"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  By offering sophisticated services and centralizing a huge volume of personal\ndata, modern smartphones changed the way we socialize, entertain and work. To\nthis aim, they rely upon complex hardware/software frameworks leading to a\nnumber of vulnerabilities, attacks and hazards to profile individuals or gather\nsensitive information. However, the majority of works evaluating the security\ndegree of smartphones neglects steganography, which can be mainly used to: i)\nexfiltrate confidential data via camouflage methods, and ii) conceal valuable\nor personal information into innocent looking carriers.\n  Therefore, this paper surveys the state of the art of steganographic\ntechniques for smartphones, with emphasis on methods developed over the period\n2005 to the second quarter of 2014. The different approaches are grouped\naccording to the portion of the device used to hide information, leading to\nthree different covert channels, i.e., local, object and network. Also, it\nreviews the relevant approaches used to detect and mitigate steganographic\nattacks or threats. Lastly, it showcases the most popular software applications\nto embed secret data into carriers, as well as possible future directions.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.6796v1"
    },
    {
        "title": "Improving image watermarking based on Tabu search by Chaos",
        "authors": [
            "Mohammad Tafaghodi",
            "Meysam Ghaffari",
            "Alimohammad Latif",
            "Seyed Rasoul Mousavi"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  With the fast development of communication and multimedia technology, the\nrights of the owners of multimedia products is vulnerable to the unauthorized\ncopies and watermarking is one of the best known methods for proving the\nownership of a product. In this paper we prosper the previous watermarking\nmethod which was based on Tabu search by Chaos. The modification applied in the\npermutation step of watermarking and the initial population generation of the\nTabu search. We analyze our method on some well known images and experimental\nresults shows the improvement in the quality and speed of the proposed\nwatermarking method.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.01576v3"
    },
    {
        "title": "Minimization of image watermarking side effects through subjective\n  optimization",
        "authors": [
            "Hossein Bakhshi Golestani",
            "Mohammed Ghanbari"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  This paper investigates the use of Structural Similaritys (SSIM) index on the\nminimized side effect to image watermarking. For fast implementation and more\ncompatibility with the standard DCT based codecs, watermark insertion is\ncarried out on the DCT coefficients and hence a SSIM model for DCT based\nwatermarking is developed. For faster implementation, the SSIM index is\nmaximized over independent 4x4 non-overlapped blocks but the disparity between\nthe adjacent blocks reduces the overall image quality. This problem is resolved\nthrough optimization of overlapped blocks, but, the higher image quality is\nachieved at a cost of high computational complexity. To reduce the\ncomputational complexity while preserving the good quality, optimization of\nsemi-overlapped blocks is introduced. We show that while SSIM-based\noptimization over overlapped blocks has as high as 64 times the complexity of\nthe 4x4 non-overlapped method, with semi-overlapped optimization the high\nquality of overlapped method is preserved only at a cost of less than 8 times\nthe non-overlapped method.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.01755v1"
    },
    {
        "title": "Enhance Robustness of Image-in-Image Watermarking through Data\n  Partitioning",
        "authors": [
            "Hossein Bakhshi Golestani",
            "Shahrokh Ghaemmaghami"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Vulnerability of watermarking schemes against intense signal processing\nattacks is generally a major concern, particularly when there are techniques to\nreproduce an acceptable copy of the original signal with no chance for\ndetecting the watermark. In this paper, we propose a two-layer, data\npartitioning (DP) based, image in image watermarking method in the DCT domain\nto improve the watermark detection performance. Truncated singular value\ndecomposition, binary wavelet decomposition and spatial scalability idea in\nH.264/SVC are analyzed and employed as partitioning methods. It is shown that\nthe proposed scheme outperforms its two recent competitors in terms of both\ndata payload and robustness to intense attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.01758v1"
    },
    {
        "title": "Learning Subclass Representations for Visually-varied Image\n  Classification",
        "authors": [
            "Xinchao Li",
            "Peng Xu",
            "Yue Shi",
            "Martha Larson",
            "Alan Hanjalic"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  In this paper, we present a subclass-representation approach that predicts\nthe probability of a social image belonging to one particular class. We explore\nthe co-occurrence of user-contributed tags to find subclasses with a strong\nconnection to the top level class. We then project each image on to the\nresulting subclass space to generate a subclass representation for the image.\nThe novelty of the approach is that subclass representations make use of not\nonly the content of the photos themselves, but also information on the\nco-occurrence of their tags, which determines membership in both subclasses and\ntop-level classes. The novelty is also that the images are classified into\nsmaller classes, which have a chance of being more visually stable and easier\nto model. These subclasses are used as a latent space and images are\nrepresented in this space by their probability of relatedness to all of the\nsubclasses. In contrast to approaches directly modeling each top-level class\nbased on the image content, the proposed method can exploit more information\nfor visually diverse classes. The approach is evaluated on a set of $2$ million\nphotos with 10 classes, released by the Multimedia 2013 Yahoo! Large-scale\nFlickr-tag Image Classification Grand Challenge. Experiments show that the\nproposed system delivers sound performance for visually diverse classes\ncompared with methods that directly model top classes.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.02913v1"
    },
    {
        "title": "Egocentric Activity Recognition with Multimodal Fisher Vector",
        "authors": [
            "Sibo Song",
            "Ngai-Man Cheung",
            "Vijay Chandrasekhar",
            "Bappaditya Mandal",
            "Jie Lin"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  With the increasing availability of wearable devices, research on egocentric\nactivity recognition has received much attention recently. In this paper, we\nbuild a Multimodal Egocentric Activity dataset which includes egocentric videos\nand sensor data of 20 fine-grained and diverse activity categories. We present\na novel strategy to extract temporal trajectory-like features from sensor data.\nWe propose to apply the Fisher Kernel framework to fuse video and temporal\nenhanced sensor features. Experiment results show that with careful design of\nfeature extraction and fusion algorithm, sensor data can enhance\ninformation-rich video data. We make publicly available the Multimodal\nEgocentric Activity dataset to facilitate future research.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.06603v1"
    },
    {
        "title": "Robust Image Watermarking Using Non-Regular Wavelets",
        "authors": [
            "R. J. Cintra",
            "T. V. Cooklev"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  An approach to watermarking digital images using non-regular wavelets is\nadvanced. Non-regular transforms spread the energy in the transform domain. The\nproposed method leads at the same time to increased image quality and increased\nrobustness with respect to lossy compression. The approach provides robust\nwatermarking by suitably creating watermarked messages that have energy\ncompaction and frequency spreading. Our experimental results show that the\napplication of non-regular wavelets, instead of regular ones, can furnish a\nsuperior robust watermarking scheme. The generated watermarked data is more\nimmune against non-intentional JPEG and JPEG2000 attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.07232v1"
    },
    {
        "title": "Geo-distinctive Visual Element Matching for Location Estimation of\n  Images",
        "authors": [
            "Xinchao Li",
            "Martha A. Larson",
            "Alan Hanjalic"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  We propose an image representation and matching approach that substantially\nimproves visual-based location estimation for images. The main novelty of the\napproach, called distinctive visual element matching (DVEM), is its use of\nrepresentations that are specific to the query image whose location is being\npredicted. These representations are based on visual element clouds, which\nrobustly capture the connection between the query and visual evidence from\ncandidate locations. We then maximize the influence of visual elements that are\ngeo-distinctive because they do not occur in images taken at many other\nlocations. We carry out experiments and analysis for both geo-constrained and\ngeo-unconstrained location estimation cases using two large-scale,\npublicly-available datasets: the San Francisco Landmark dataset with $1.06$\nmillion street-view images and the MediaEval '15 Placing Task dataset with\n$5.6$ million geo-tagged images from Flickr. We present examples that\nillustrate the highly-transparent mechanics of the approach, which are based on\ncommon sense observations about the visual patterns in image collections. Our\nresults show that the proposed method delivers a considerable performance\nimprovement compared to the state of the art.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.07884v1"
    },
    {
        "title": "A First Look at Quality of Mobile Live Streaming Experience: the Case of\n  Periscope",
        "authors": [
            "Matti Siekkinen",
            "Enrico Masala",
            "Teemu Kämäräinen"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Live multimedia streaming from mobile devices is rapidly gaining popularity\nbut little is known about the QoE they provide. In this paper, we examine the\nPeriscope service. We first crawl the service in order to understand its usage\npatterns. Then, we study the protocols used, the typical quality of experience\nindicators, such as playback smoothness and latency, video quality, and the\nenergy consumption of the Android application.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.04270v2"
    },
    {
        "title": "Resource Provisioning and Profit Maximization for Transcoding in\n  Information Centric Networking",
        "authors": [
            "Guanyu Gao",
            "Yonggang Wen",
            "Cedric Westphal"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Adaptive bitrate streaming (ABR) has been widely adopted to support video\nstreaming services over heterogeneous devices and varying network conditions.\nWith ABR, each video content is transcoded into multiple representations in\ndifferent bitrates and resolutions. However, video transcoding is computing\nintensive, which requires the transcoding service providers to deploy a large\nnumber of servers for transcoding the video contents published by the content\nproducers. As such, a natural question for the transcoding service provider is\nhow to provision the computing resource for transcoding the video contents\nwhile maximizing service profit. To address this problem, we design a cloud\nvideo transcoding system by taking the advantage of cloud computing technology\nto elastically allocate computing resource. We propose a method for jointly\nconsidering the task scheduling and resource provisioning problem in two\ntimescales, and formulate the service profit maximization as a two-timescale\nstochastic optimization problem. We derive some approximate policies for the\ntask scheduling and resource provisioning. Based on our proposed methods, we\nimplement our open source cloud video transcoding system Morph and evaluate its\nperformance in a real environment. The experiment results demonstrate that our\nproposed method can reduce the resource consumption and achieve a higher profit\ncompared with the baseline schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05758v1"
    },
    {
        "title": "Understanding the Smartrouter-based Peer CDN for Video Streaming",
        "authors": [
            "Ming Ma",
            "Zhi Wang",
            "Ke Su",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Recent years have witnessed a new video delivery paradigm: smartrouter-based\nvideo delivery network, which is enabled by smartrouters deployed at users'\nhomes, together with the conventional video servers deployed in the\ndatacenters. Recently, ChinaCache, a large content delivery network (CDN)\nprovider, and Youku, a video service provider using smartrouters to assist\nvideo delivery, announced their cooperation to create a new paradigm of content\ndelivery based on householders' network resources. This new paradigm is\ndifferent from the conventional peer-to-peer (P2P) approach, because such\ndedicated smartrouters are inherently operated by the centralized video service\nproviders in a coordinative manner. It is intriguing to study the strategies,\nperformance and potential impact on the content delivery ecosystem of such peer\nCDN systems. In this paper, we study the Youku peer CDN, which has deployed\nover 300K smartrouter devices for its video streaming. In our measurement, 78K\nvideos were investigated and 3TB traffic has been analyzed, over controlled\nrouters and players. Our contributions are the following measurement insights.\nFirst, a global replication and caching strategy is essential for the peer CDN\nsystems, and proactively scheduling replication and caching on a daily basis\ncan guarantee their performance. Second, such peer CDN deployment can itself\nform an effective Quality of Service (QoS) monitoring sub-system, which can be\nused for fine-grained user request redirection. We also provide our analysis on\nthe performance issues and potential improvements to the peer CDN systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.07704v1"
    },
    {
        "title": "Understanding Content Placement Strategies in Smartrouter-based Peer CDN\n  for Video Streaming",
        "authors": [
            "Ming Ma",
            "Zhi Wang",
            "Ke Su",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Recent years have witnessed a new video delivery paradigm: smartrouter-based\npeer video content delivery network, which is enabled by smartrouters deployed\nat users' homes. ChinaCache (one of the largest CDN providers in China) and\nYouku (a video provider using smartrouters to assist video delivery) announced\ntheir cooperation in 2015, to create a new paradigm of content delivery based\non householders' network resources. This new paradigm is different from the\nconventional peer-to-peer (P2P) approach, because millions of dedicated\nsmartrouters are operated by the centralized video service providers in a\ncoordinative manner. Thus it is intriguing to study the content placement\nstrategies used in a smartrouter-based content delivery system, as well as its\npotential impact on the content delivery ecosystem. In this paper, we carry out\nmeasurement studies of Youku's peer video CDN, who has deployed over 300K\nsmartrouter devices for its video delivery. In our measurement studies, 104K\nvideos were investigated and 4TB traffic has been analyzed, over controlled\nsmartrouter nodes and players. Our measurement insights are as follows. First,\na global content replication strategy is essential for the peer CDN systems.\nSecond, such peer CDN deployment itself can form an effective sub-system for\nend-to-end QoS monitoring, which can be used for fine-grained request\nredirection (e.g., user-level) and content replication. We also show our\nanalysis on the performance limitations and propose potential improvements to\nthe peer CDN systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.07705v2"
    },
    {
        "title": "Data Analysis in Multimedia Quality Assessment: Revisiting the\n  Statistical Tests",
        "authors": [
            "Manish Narwaria",
            "Lukas Krasula",
            "Patrick Le Callet"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Assessment of multimedia quality relies heavily on subjective assessment, and\nis typically done by human subjects in the form of preferences or continuous\nratings. Such data is crucial for analysis of different multimedia processing\nalgorithms as well as validation of objective (computational) methods for the\nsaid purpose. To that end, statistical testing provides a theoretical framework\ntowards drawing meaningful inferences, and making well grounded conclusions and\nrecommendations. While parametric tests (such as t test, ANOVA, and error\nestimates like confidence intervals) are popular and widely used in the\ncommunity, there appears to be a certain degree of confusion in the application\nof such tests. Specifically, the assumption of normality and homogeneity of\nvariance is often not well understood. Therefore, the main goal of this paper\nis to revisit them from a theoretical perspective and in the process provide\nuseful insights into their practical implications. Experimental results on both\nsimulated and real data are presented to support the arguments made. A software\nimplementing the said recommendations is also made publicly available, in order\nto achieve the goal of reproducible research.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00291v1"
    },
    {
        "title": "Localization of JPEG double compression through multi-domain\n  convolutional neural networks",
        "authors": [
            "Irene Amerini",
            "Tiberio Uricchio",
            "Lamberto Ballan",
            "Roberto Caldelli"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  When an attacker wants to falsify an image, in most of cases she/he will\nperform a JPEG recompression. Different techniques have been developed based on\ndiverse theoretical assumptions but very effective solutions have not been\ndeveloped yet. Recently, machine learning based approaches have been started to\nappear in the field of image forensics to solve diverse tasks such as\nacquisition source identification and forgery detection. In this last case, the\naim ahead would be to get a trained neural network able, given a to-be-checked\nimage, to reliably localize the forged areas. With this in mind, our paper\nproposes a step forward in this direction by analyzing how a single or double\nJPEG compression can be revealed and localized using convolutional neural\nnetworks (CNNs). Different kinds of input to the CNN have been taken into\nconsideration, and various experiments have been carried out trying also to\nevidence potential issues to be further investigated.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01788v1"
    },
    {
        "title": "Modeling Multimodal Clues in a Hybrid Deep Learning Framework for Video\n  Classification",
        "authors": [
            "Yu-Gang Jiang",
            "Zuxuan Wu",
            "Jinhui Tang",
            "Zechao Li",
            "Xiangyang Xue",
            "Shih-Fu Chang"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Videos are inherently multimodal. This paper studies the problem of how to\nfully exploit the abundant multimodal clues for improved video categorization.\nWe introduce a hybrid deep learning framework that integrates useful clues from\nmultiple modalities, including static spatial appearance information, motion\npatterns within a short time window, audio information as well as long-range\ntemporal dynamics. More specifically, we utilize three Convolutional Neural\nNetworks (CNNs) operating on appearance, motion and audio signals to extract\ntheir corresponding features. We then employ a feature fusion network to derive\na unified representation with an aim to capture the relationships among\nfeatures. Furthermore, to exploit the long-range temporal dynamics in videos,\nwe apply two Long Short Term Memory networks with extracted appearance and\nmotion features as inputs. Finally, we also propose to refine the prediction\nscores by leveraging contextual relationships among video semantics. The hybrid\ndeep learning framework is able to exploit a comprehensive set of multimodal\nfeatures for video classification. Through an extensive set of experiments, we\ndemonstrate that (1) LSTM networks which model sequences in an explicitly\nrecurrent manner are highly complementary with CNN models; (2) the feature\nfusion network which produces a fused representation through modeling feature\nrelationships outperforms alternative fusion strategies; (3) the semantic\ncontext of video classes can help further refine the predictions for improved\nperformance. Experimental results on two challenging benchmarks, the UCF-101\nand the Columbia Consumer Videos (CCV), provide strong quantitative evidence\nthat our framework achieves promising results: $93.1\\%$ on the UCF-101 and\n$84.5\\%$ on the CCV, outperforming competing methods with clear margins.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.04508v1"
    },
    {
        "title": "Passive Classification of Source Printer using Text-line-level Geometric\n  Distortion Signatures from Scanned Images of Printed Documents",
        "authors": [
            "Hardik Jain",
            "Gaurav Gupta",
            "Sharad Joshi",
            "Nitin Khanna"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In this digital era, one thing that still holds the convention is a printed\narchive. Printed documents find their use in many critical domains such as\ncontract papers, legal tenders and proof of identity documents. As more\nadvanced printing, scanning and image editing techniques are becoming\navailable, forgeries on these legal tenders pose a serious threat. Ability to\neasily and reliably identify source printer of a printed document can help a\nlot in reducing this menace. During printing procedure, printer hardware\nintroduces certain distortions in printed characters' locations and shapes\nwhich are invisible to naked eyes. These distortions are referred as geometric\ndistortions, their profile (or signature) is generally unique for each printer\nand can be used for printer classification purpose. This paper proposes a set\nof features for characterizing text-line-level geometric distortions, referred\nas geometric distortion signatures and presents a novel system to use them for\nidentification of the origin of a printed document. Detailed experiments\nperformed on a set of thirteen printers demonstrate that the proposed system\nachieves state of the art performance and gives much higher accuracy under\nsmall training size constraint. For four training and six test pages of three\ndifferent fonts, the proposed method gives 99\\% classification accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.06651v1"
    },
    {
        "title": "Single Classifier-based Passive System for Source Printer Classification\n  using Local Texture Features",
        "authors": [
            "Sharad Joshi",
            "Nitin Khanna"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  An important aspect of examining printed documents for potential forgeries\nand copyright infringement is the identification of source printer as it can be\nhelpful for ascertaining the leak and detecting forged documents. This paper\nproposes a system for classification of source printer from scanned images of\nprinted documents using all the printed letters simultaneously. This system\nuses local texture patterns based features and a single classifier for\nclassifying all the printed letters. Letters are extracted from scanned images\nusing connected component analysis followed by morphological filtering without\nthe need of using an OCR. Each letter is sub-divided into a flat region and an\nedge region, and local tetra patterns are estimated separately for these two\nregions. A strategically constructed pooling technique is used to extract the\nfinal feature vectors. The proposed method has been tested on both a publicly\navailable dataset of 10 printers and a new dataset of 18 printers scanned at a\nresolution of 600 dpi as well as 300 dpi printed in four different fonts. The\nresults indicate shape independence property in the proposed method as using a\nsingle classifier it outperforms existing handcrafted feature-based methods and\nneeds much smaller number of training pages by using all the printed letters.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.07422v1"
    },
    {
        "title": "On the usefulness of information hiding techniques for wireless sensor\n  networks security",
        "authors": [
            "Rola Al-Sharif",
            "Christophe Guyeux",
            "Yousra Ahmed Fadil",
            "Abdallah Makhoul",
            "Ali Jaber"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  A wireless sensor network (WSN) typically consists of base stations and a\nlarge number of wireless sensors. The sensory data gathered from the whole\nnetwork at a certain time snapshot can be visualized as an image. As a result,\ninformation hiding techniques can be applied to this \"sensory data image\".\nSteganography refers to the technology of hiding data into digital media\nwithout drawing any suspicion, while steganalysis is the art of detecting the\npresence of steganography. This article provides a brief review of\nsteganography and steganalysis applications for wireless sensor networks\n(WSNs). Then we show that the steganographic techniques are both related to\nsensed data authentication in wireless sensor networks, and when considering\nthe attacker point of view, which has not yet been investigated in the\nliterature. Our simulation results show that the sink level is unable to detect\nan attack carried out by the nsF5 algorithm on sensed data.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.08136v1"
    },
    {
        "title": "From Thumbnails to Summaries - A single Deep Neural Network to Rule Them\n  All",
        "authors": [
            "Hongxiang Gu",
            "Viswanathan Swaminathan"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Video summaries come in many forms, from traditional single-image thumbnails,\nanimated thumbnails, storyboards, to trailer-like video summaries. Content\ncreators use the summaries to display the most attractive portion of their\nvideos; the users use them to quickly evaluate if a video is worth watching.\nAll forms of summaries are essential to video viewers, content creators, and\nadvertisers. Often video content management systems have to generate multiple\nversions of summaries that vary in duration and presentational forms. We\npresent a framework ReconstSum that utilizes LSTM-based autoencoder\narchitecture to extract and select a sparse subset of video frames or keyshots\nthat optimally represent the input video in an unsupervised manner. The encoder\nselects a subset from the input video while the decoder seeks to reconstruct\nthe video from the selection. The goal is to minimize the difference between\nthe original input video and the reconstructed video. Our method is easily\nextendable to generate a variety of applications including static video\nthumbnails, animated thumbnails, storyboards and \"trailer-like\" highlights. We\nspecifically study and evaluate two most popular use cases: thumbnail\ngeneration and storyboard generation. We demonstrate that our methods generate\nbetter results than the state-of-the-art techniques in both use cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.00184v1"
    },
    {
        "title": "Efficient Continuous Top-$k$ Geo-Image Search on Road Network",
        "authors": [
            "Chengyuan Zhang",
            "Kesheng Cheng",
            "Lei Zhu",
            "Ruipeng Chen",
            "Zuping Zhang",
            "Fang Huang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  With the rapid development of mobile Internet and cloud computing technology,\nlarge-scale multimedia data, e.g., texts, images, audio and videos have been\ngenerated, collected, stored and shared. In this paper, we propose a novel\nquery problem named continuous top-$k$ geo-image query on road network which\naims to search out a set of geo-visual objects based on road network distance\nproximity and visual content similarity. Existing approaches for spatial\ntextual query and geo-image query cannot address this problem effectively\nbecause they do not consider both of visual content similarity and road network\ndistance proximity on road network. In order to address this challenge\neffectively and efficiently, firstly we propose the definition of geo-visual\nobjects and continuous top-$k$ geo-visual objects query on road network, then\ndevelop a score function for search. To improve the query efficiency in a\nlarge-scale road network, we propose the search algorithm named geo-visual\nsearch on road network based on a novel hybrid indexing framework called\nVIG-Tree, which combines G-Tree and visual inverted index technique. In\naddition, an important notion named safe interval and results updating rule are\nproposed, and based on them we develop an efficient algorithm named moving\nmonitor algorithm to solve continuous query. Experimental evaluation on real\nmultimedia dataset and road network dataset illustrates that our solution\noutperforms state-of-the-art method.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.02793v1"
    },
    {
        "title": "First Steps Toward CNN based Source Classification of Document Images\n  Shared Over Messaging App",
        "authors": [
            "Sharad Joshi",
            "Suraj Saxena",
            "Nitin Khanna"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Knowledge of source smartphone corresponding to a document image can be\nhelpful in a variety of applications including copyright infringement,\nownership attribution, leak identification and usage restriction. In this\nletter, we investigate a convolutional neural network-based approach to solve\nsource smartphone identification problem for printed text documents which have\nbeen captured by smartphone cameras and shared over messaging platform. In\nabsence of any publicly available dataset addressing this problem, we introduce\na new image dataset consisting of 315 images of documents printed in three\ndifferent fonts, captured using 21 smartphones and shared over WhatsApp.\nExperiments conducted on this dataset demonstrate that, in all scenarios, the\nproposed system performs as well as or better than the state-of-the-art system\nbased on handcrafted features and classification of letters extracted from\ndocument images. The new dataset and code of the proposed system will be made\npublicly available along with this letter's publication, presently they are\nsubmitted for review.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.05941v1"
    },
    {
        "title": "On Evaluating Perceptual Quality of Online User-Generated Videos",
        "authors": [
            "Soobeom Jang",
            "Jong-Seok Lee"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  This paper deals with the issue of the perceptual quality evaluation of\nuser-generated videos shared online, which is an important step toward\ndesigning video-sharing services that maximize users' satisfaction in terms of\nquality. We first analyze viewers' quality perception patterns by applying\ngraph analysis techniques to subjective rating data. We then examine the\nperformance of existing state-of-the-art objective metrics for the quality\nestimation of user-generated videos. In addition, we investigate the\nfeasibility of metadata accompanied with videos in online video-sharing\nservices for quality estimation. Finally, various issues in the quality\nassessment of online user-generated videos are discussed, including\ndifficulties and opportunities.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.05220v1"
    },
    {
        "title": "Intermediate Deep Feature Compression: the Next Battlefield of\n  Intelligent Sensing",
        "authors": [
            "Zhuo Chen",
            "Weisi Lin",
            "Shiqi Wang",
            "Lingyu Duan",
            "Alex C. Kot"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The recent advances of hardware technology have made the intelligent analysis\nequipped at the front-end with deep learning more prevailing and practical. To\nbetter enable the intelligent sensing at the front-end, instead of compressing\nand transmitting visual signals or the ultimately utilized top-layer deep\nlearning features, we propose to compactly represent and convey the\nintermediate-layer deep learning features of high generalization capability, to\nfacilitate the collaborating approach between front and cloud ends. This\nstrategy enables a good balance among the computational load, transmission load\nand the generalization ability for cloud servers when deploying the deep neural\nnetworks for large scale cloud based visual analysis. Moreover, the presented\nstrategy also makes the standardization of deep feature coding more feasible\nand promising, as a series of tasks can simultaneously benefit from the\ntransmitted intermediate layers. We also present the results for evaluation of\nlossless deep feature compression with four benchmark data compression methods,\nwhich provides meaningful investigations and baselines for future research and\nstandardization activities.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.06196v1"
    },
    {
        "title": "Learning to Detect Fake Face Images in the Wild",
        "authors": [
            "Chih-Chung Hsu",
            "Chia-Yen Lee",
            "Yi-Xiu Zhuang"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Although Generative Adversarial Network (GAN) can be used to generate the\nrealistic image, improper use of these technologies brings hidden concerns. For\nexample, GAN can be used to generate a tampered video for specific people and\ninappropriate events, creating images that are detrimental to a particular\nperson, and may even affect that personal safety. In this paper, we will\ndevelop a deep forgery discriminator (DeepFD) to efficiently and effectively\ndetect the computer-generated images. Directly learning a binary classifier is\nrelatively tricky since it is hard to find the common discriminative features\nfor judging the fake images generated from different GANs. To address this\nshortcoming, we adopt contrastive loss in seeking the typical features of the\nsynthesized images generated by different GANs and follow by concatenating a\nclassifier to detect such computer-generated images. Experimental results\ndemonstrate that the proposed DeepFD successfully detected 94.7% fake images\ngenerated by several state-of-the-art GANs.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.08754v3"
    },
    {
        "title": "A Coarse-To-Fine Framework For Video Object Segmentation",
        "authors": [
            "Chi Zhang",
            "Alexander Loui"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this study, we develop an unsupervised coarse-to-fine video analysis\nframework and prototype system to extract a salient object in a video sequence.\nThis framework starts from tracking grid-sampled points along temporal frames,\ntypically using KLT tracking method. The tracking points could be divided into\nseveral groups due to their inconsistent movements. At the same time, the SLIC\nalgorithm is extended into 3D space to generate supervoxels. Coarse\nsegmentation is achieved by combining the categorized tracking points and\nsupervoxels of the corresponding frame in the video sequence. Finally, a\ngraph-based fine segmentation algorithm is used to extract the moving object in\nthe scene. Experimental results reveal that this method outperforms the\nprevious approaches in terms of accuracy and robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.10260v1"
    },
    {
        "title": "Deep Multiple Description Coding by Learning Scalar Quantization",
        "authors": [
            "Lijun Zhao",
            "Huihui Bai",
            "Anhong Wang",
            "Yao Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In this paper, we propose a deep multiple description coding framework, whose\nquantizers are adaptively learned via the minimization of multiple description\ncompressive loss. Firstly, our framework is built upon auto-encoder networks,\nwhich have multiple description multi-scale dilated encoder network and\nmultiple description decoder networks. Secondly, two entropy estimation\nnetworks are learned to estimate the informative amounts of the quantized\ntensors, which can further supervise the learning of multiple description\nencoder network to represent the input image delicately. Thirdly, a pair of\nscalar quantizers accompanied by two importance-indicator maps is automatically\nlearned in an end-to-end self-supervised way. Finally, multiple description\nstructural dissimilarity distance loss is imposed on multiple description\ndecoded images in pixel domain for diversified multiple description generations\nrather than on feature tensors in feature domain, in addition to multiple\ndescription reconstruction loss. Through testing on two commonly used datasets,\nit is verified that our method is beyond several state-of-the-art multiple\ndescription coding approaches in terms of coding efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.01504v3"
    },
    {
        "title": "ADNet: A Deep Network for Detecting Adverts",
        "authors": [
            "Murhaf Hossari",
            "Soumyabrata Dev",
            "Matthew Nicholson",
            "Killian McCabe",
            "Atul Nautiyal",
            "Clare Conran",
            "Jian Tang",
            "Wei Xu",
            "François Pitié"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Online video advertising gives content providers the ability to deliver\ncompelling content, reach a growing audience, and generate additional revenue\nfrom online media. Recently, advertising strategies are designed to look for\noriginal advert(s) in a video frame, and replacing them with new adverts. These\nstrategies, popularly known as product placement or embedded marketing, greatly\nhelp the marketing agencies to reach out to a wider audience. However, in the\nexisting literature, such detection of candidate frames in a video sequence for\nthe purpose of advert integration, is done manually. In this paper, we propose\na deep-learning architecture called ADNet, that automatically detects the\npresence of advertisements in video frames. Our approach is the first of its\nkind that automatically detects the presence of adverts in a video frame, and\nachieves state-of-the-art results on a public dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04115v1"
    },
    {
        "title": "Spherical clustering of users navigating 360° content",
        "authors": [
            "Silvia Rossi",
            "Francesca De Simone",
            "Pascal Frossard",
            "Laura Toni"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  In Virtual Reality (VR) applications, understanding how users explore the\nomnidirectional content is important to optimize content creation, to develop\nuser-centric services, or even to detect disorders in medical applications.\nClustering users based on their common navigation patterns is a first direction\nto understand users behaviour. However, classical clustering techniques fail in\nidentifying these common paths, since they are usually focused on minimizing a\nsimple distance metric. In this paper, we argue that minimizing the distance\nmetric does not necessarily guarantee to identify users that experience similar\nnavigation path in the VR domain. Therefore, we propose a graph-based method to\nidentify clusters of users who are attending the same portion of the spherical\ncontent over time. The proposed solution takes into account the spherical\ngeometry of the content and aims at clustering users based on the actual\noverlap of displayed content among users. Our method is tested on real VR user\nnavigation patterns. Results show that our solution leads to clusters in which\nat least 85% of the content displayed by one user is shared among the other\nusers belonging to the same cluster.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.05185v2"
    },
    {
        "title": "VECTORS: Video communication through opportunistic relays and scalable\n  video coding",
        "authors": [
            "Abhishek Thakur",
            "Arnav Dhamija",
            "Tejeshwar Reddy G"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Crowd-sourced video distribution is frequently of interest in the local\nvicinity. In this paper, we propose a novel design to transfer such content\nover opportunistic networks with adaptive quality encoding to achieve\nreasonable delay bounds. The video segments are transmitted between source and\ndestination in a delay tolerant manner using the Nearby Connections Android\nlibrary. This implementation can be applied to multiple domains, including farm\nmonitoring, wildlife, and environmental tracking, disaster response scenarios,\netc. In this work, we present the design of an opportunistic contact based\nsystem, and we discuss basic results for the trial runs within our institute.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.10826v1"
    },
    {
        "title": "Handcrafted vs Deep Learning Classification for Scalable Video QoE\n  Modeling",
        "authors": [
            "Dasari Mallesham",
            "Christina Vlachou",
            "Shruti Sanadhya",
            "Pranjal Sahu",
            "Yang Qiu",
            "Kyu-Han Kim",
            "Samir R. Das"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Mobile video traffic is dominant in cellular and enterprise wireless\nnetworks. With the advent of diverse applications, network administrators face\nthe challenge to provide high QoE in the face of diverse wireless conditions\nand application contents. Yet, state-of-the-art networks lack analytics for\nQoE, as this requires support from the application or user feedback. While\nthere are existing techniques to map QoS to QoE by training machine learning\nmodels without requiring user feedback, these techniques are limited to only\nfew applications, due to insufficient QoE ground-truth annotation for ML. To\naddress these limitations, we focus on video telephony applications and model\nkey artefacts of spatial and temporal video QoE. Our key contribution is\ndesigning content- and device-independent metrics and training across diverse\nWiFi conditions. We show that our metrics achieve a median 90% accuracy by\ncomparing with mean-opinion-score from more than 200 users and 800 video\nsamples over three popular video telephony applications -- Skype, FaceTime and\nGoogle Hangouts. We further extend our metrics by using deep neural networks,\nmore specifically we use a combined CNN and LSTM model. We achieve a median\naccuracy of 95% by combining our QoE metrics with the deep learning model,\nwhich is a 38% improvement over the state-of-the-art well known techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.03404v1"
    },
    {
        "title": "CBA: Contextual Quality Adaptation for Adaptive Bitrate Video Streaming\n  (Extended Version)",
        "authors": [
            "Bastian Alt",
            "Trevor Ballard",
            "Ralf Steinmetz",
            "Heinz Koeppl",
            "Amr Rizk"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Recent advances in quality adaptation algorithms leave adaptive bitrate (ABR)\nstreaming architectures at a crossroads: When determining the sustainable video\nquality one may either rely on the information gathered at the client vantage\npoint or on server and network assistance. The fundamental problem here is to\ndetermine how valuable either information is for the adaptation decision. This\nproblem becomes particularly hard in future Internet settings such as Named\nData Networking (NDN) where the notion of a network connection does not exist.\n  In this paper, we provide a fresh view on ABR quality adaptation for QoE\nmaximization, which we formalize as a decision problem under uncertainty, and\nfor which we contribute a sparse Bayesian contextual bandit algorithm denoted\nCBA. This allows taking high-dimensional streaming context information,\nincluding client-measured variables and network assistance, to find online the\nmost valuable information for the quality adaptation. Since sparse Bayesian\nestimation is computationally expensive, we develop a fast new inference scheme\nto support online video adaptation. We perform an extensive evaluation of our\nadaptation algorithm in the particularly challenging setting of NDN, where we\nuse an emulation testbed to demonstrate the efficacy of CBA compared to\nstate-of-the-art algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.05712v1"
    },
    {
        "title": "Generalization of Spoofing Countermeasures: a Case Study with ASVspoof\n  2015 and BTAS 2016 Corpora",
        "authors": [
            "Dipjyoti Paul",
            "Md Sahidullah",
            "Goutam Saha"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Voice-based biometric systems are highly prone to spoofing attacks. Recently,\nvarious countermeasures have been developed for detecting different kinds of\nattacks such as replay, speech synthesis (SS) and voice conversion (VC). Most\nof the existing studies are conducted with a specific training set defined by\nthe evaluation protocol. However, for realistic scenarios, selecting\nappropriate training data is an open challenge for the system administrator.\nMotivated by this practical concern, this work investigates the generalization\ncapability of spoofing countermeasures in restricted training conditions where\nspeech from a broad attack types are left out in the training database. We\ndemonstrate that different spoofing types have considerably different\ngeneralization capabilities. For this study, we analyze the performance using\ntwo kinds of features, mel-frequency cepstral coefficients (MFCCs) which are\nconsidered as baseline and recently proposed constant Q cepstral coefficients\n(CQCCs). The experiments are conducted with standard Gaussian mixture model -\nmaximum likelihood (GMM-ML) classifier on two recently released spoofing\ncorpora: ASVspoof 2015 and BTAS 2016 that includes cross-corpora performance\nanalysis. Feature-level analysis suggests that static and dynamic coefficients\nof spectral features, both are important for detecting spoofing attacks in the\nreal-life condition.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.08025v1"
    },
    {
        "title": "A study for Image compression using Re-Pair algorithm",
        "authors": [
            "Pasquale De Luca",
            "Vincenzo Maria Russiello",
            "Raffaele Ciro Sannino",
            "Lorenzo Valente"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The compression is an important topic in computer science which allows we to\nstorage more amount of data on our data storage. There are several techniques\nto compress any file. In this manuscript will be described the most important\nalgorithm to compress images such as JPEG and it will be compared with another\nmethod to retrieve good reason to not use this method on images. So to compress\nthe text the most encoding technique known is the Huffman Encoding which it\nwill be explained in exhaustive way. In this manuscript will showed how to\ncompute a text compression method on images in particular the method and the\nreason to choice a determinate image format against the other. The method\nstudied and analyzed in this manuscript is the Re-Pair algorithm which is\npurely for grammatical context to be compress. At the and it will be showed the\ngood result of this application.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.10744v3"
    },
    {
        "title": "Intrinsic Image Popularity Assessment",
        "authors": [
            "Keyan Ding",
            "Kede Ma",
            "Shiqi Wang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The goal of research in automatic image popularity assessment (IPA) is to\ndevelop computational models that can accurately predict the potential of a\nsocial image to go viral on the Internet. Here, we aim to single out the\ncontribution of visual content to image popularity, i.e., intrinsic image\npopularity. Specifically, we first describe a probabilistic method to generate\nmassive popularity-discriminable image pairs, based on which the first\nlarge-scale image database for intrinsic IPA (I$^2$PA) is established. We then\ndevelop computational models for I$^2$PA based on deep neural networks,\noptimizing for ranking consistency with millions of popularity-discriminable\nimage pairs. Experiments on Instagram and other social platforms demonstrate\nthat the optimized model performs favorably against existing methods, exhibits\nreasonable generalizability on different databases, and even surpasses\nhuman-level performance on Instagram. In addition, we conduct a psychophysical\nexperiment to analyze various aspects of human behavior in I$^2$PA.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.01985v2"
    },
    {
        "title": "Informative Visual Storytelling with Cross-modal Rules",
        "authors": [
            "Jiacheng Li",
            "Haizhou Shi",
            "Siliang Tang",
            "Fei Wu",
            "Yueting Zhuang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Existing methods in the Visual Storytelling field often suffer from the\nproblem of generating general descriptions, while the image contains a lot of\nmeaningful contents remaining unnoticed. The failure of informative story\ngeneration can be concluded to the model's incompetence of capturing enough\nmeaningful concepts. The categories of these concepts include entities,\nattributes, actions, and events, which are in some cases crucial to grounded\nstorytelling. To solve this problem, we propose a method to mine the\ncross-modal rules to help the model infer these informative concepts given\ncertain visual input. We first build the multimodal transactions by\nconcatenating the CNN activations and the word indices. Then we use the\nassociation rule mining algorithm to mine the cross-modal rules, which will be\nused for the concept inference. With the help of the cross-modal rules, the\ngenerated stories are more grounded and informative. Besides, our proposed\nmethod holds the advantages of interpretation, expandability, and\ntransferability, indicating potential for wider application. Finally, we\nleverage these concepts in our encoder-decoder framework with the attention\nmechanism. We conduct several experiments on the VIsual StoryTelling~(VIST)\ndataset, the results of which demonstrate the effectiveness of our approach in\nterms of both automatic metrics and human evaluation. Additional experiments\nare also conducted showing that our mined cross-modal rules as additional\nknowledge helps the model gain better performance when trained on a small\ndataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.03240v2"
    },
    {
        "title": "On the Security and Applicability of Fragile Camera Fingerprints",
        "authors": [
            "Erwin Quiring",
            "Matthias Kirchner",
            "Konrad Rieck"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Camera sensor noise is one of the most reliable device characteristics in\ndigital image forensics, enabling the unique linkage of images to digital\ncameras. This so-called camera fingerprint gives rise to different\napplications, such as image forensics and authentication. However, if images\nare publicly available, an adversary can estimate the fingerprint from her\nvictim and plant it into spurious images. The concept of fragile camera\nfingerprints addresses this attack by exploiting asymmetries in data access:\nWhile the camera owner will always have access to a full fingerprint from\nuncompressed images, the adversary has typically access to compressed images\nand thus only to a truncated fingerprint. The security of this defense,\nhowever, has not been systematically explored yet. This paper provides the\nfirst comprehensive analysis of fragile camera fingerprints under attack. A\nseries of theoretical and practical tests demonstrate that fragile camera\nfingerprints allow a reliable device identification for common compression\nlevels in an adversarial environment.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.04025v1"
    },
    {
        "title": "Towards QoS-Aware Recommendations",
        "authors": [
            "Pavlos Sermpezis",
            "Savvas Kastanakis",
            "João Ismael Pinheiro",
            "Felipe Assis",
            "Mateus Nogueira",
            "Daniel Menasché",
            "Thrasyvoulos Spyropoulos"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this paper we propose that recommendation systems (RSs) for multimedia\nservices should be \"QoS-aware\", i.e., take into account the expected QoS with\nwhich a content can be delivered, to increase the user satisfaction.\nNetwork-aware recommendations have been very recently proposed as a promising\nsolution to improve network performance. However, the idea of QoS-aware RSs has\nbeen studied from the network perspective. Its feasibility and performance\nperformance advantages for the content-provider or user perspective have only\nbeen speculated. Hence, in this paper we aim to provide initial answers for the\nfeasibility of the concept of QoS-aware RS, by investigating its impact on real\nuser experience. To this end, we conduct experiments with real users on a\ntestbed, and present initial experimental results. Our analysis demonstrates\nthe potential of the idea: QoS-aware RSs could be beneficial for both the users\n(better experience) and content providers (higher user engagement). Moreover,\nbased on the collected dataset, we build statistical models to (i) predict the\nuser experience as a function of QoS, relevance of recommendations (QoR) and\nuser interest, and (ii) provide useful insights for the design of QoS-aware\nRSs. We believe that our study is an important first step towards QoS-aware\nrecommendations, by providing experimental evidence for their feasibility and\nbenefits, and can help open a future research direction.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.06392v2"
    },
    {
        "title": "Steganography using a 3 player game",
        "authors": [
            "Mehdi Yedroudj",
            "Frédéric Comby",
            "Marc Chaumont"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Image steganography aims to securely embed secret information into cover\nimages. Until now, adaptive embedding algorithms such as S-UNIWARD or Mi-POD,\nare among the most secure and most used methods for image steganography. With\nthe arrival of deep learning and more specifically the Generative Adversarial\nNetworks (GAN), new techniques have appeared. Among these techniques, there is\nthe 3 player game approaches, where three networks compete against each\nother.In this paper, we propose three different architectures based on the 3\nplayer game. The first-architecture is proposed as a rigorous alternative to\ntwo recent publications. The second takes into account stego noise power.\nFinally, our third architecture enriches the second one with a better\ninteraction between the embedding and extracting networks. Our method achieves\nbetter results compared to the existing works GSIVAT, HiDDeN, and paves the way\nfor future research on this topic.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.06956v2"
    },
    {
        "title": "Dual-Domain Fusion Convolutional Neural Network for Contrast Enhancement\n  Forensics",
        "authors": [
            "Pengpeng Yang",
            "Rongrong Ni",
            "Yao Zhao",
            "Gang Cao",
            "Wei Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Contrast enhancement (CE) forensics techniques have always been of great\ninterest for image forensics community, as they can be an effective tool for\nrecovering image history and identifying tampered images. Although several CE\nforensic algorithms have been proposed, their accuracy and robustness against\nsome kinds of processing are still unsatisfactory. In order to attenuate such\ndeficiency, in this paper we propose a new framework based on dual-domain\nfusion convolutional neural network to fuse the features of pixel and histogram\ndomains for CE forensics. Specifically, we first present a pixel-domain\nconvolutional neural network (P-CNN) to automatically capture the patterns of\ncontrast-enhanced images in the pixel domain. Then, we present a\nhistogram-domain convolutional neural network (H-CNN) to extract the features\nin the histogram domain. The feature representations of pixel and histogram\ndomains are fused and fed into two fully connected layers for the\nclassification of contrast-enhanced images. Experimental results show that the\nproposed method achieve better performance and is robust against pre-JPEG\ncompression and anti-forensics attacks. In addition, a strategy for performance\nimprovement of CNN-based forensics is explored, which could provide guidance\nfor the design of CNN-based forensics tools.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.07992v1"
    },
    {
        "title": "Automated Composition of Picture-Synched Music Soundtracks for Movies",
        "authors": [
            "Vansh Dassani",
            "Jon Bird",
            "Dave Cliff"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  We describe the implementation of and early results from a system that\nautomatically composes picture-synched musical soundtracks for videos and\nmovies. We use the phrase \"picture-synched\" to mean that the structure of the\nautomatically composed music is determined by visual events in the input movie,\ni.e. the final music is synchronised to visual events and features such as cut\ntransitions or within-shot key-frame events. Our system combines automated\nvideo analysis and computer-generated music-composition techniques to create\nunique soundtracks in response to the video input, and can be thought of as an\ninitial step in creating a computerised replacement for a human composer\nwriting music to fit the picture-locked edit of a movie. Working only from the\nvideo information in the movie, key features are extracted from the input\nvideo, using video analysis techniques, which are then fed into a\nmachine-learning-based music generation tool, to compose a piece of music from\nscratch. The resulting soundtrack is tied to video features, such as scene\ntransition markers and scene-level energy values, and is unique to the input\nvideo. Although the system we describe here is only a preliminary\nproof-of-concept, user evaluations of the output of the system have been\npositive.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08773v1"
    },
    {
        "title": "Blind Robust 3-D Mesh Watermarking based on Mesh Saliency and QIM\n  quantization for Copyright Protection",
        "authors": [
            "Mohamed Hamidi",
            "Aladine Chetouani",
            "Mohamed El Haziti",
            "Mohammed El Hassouni",
            "and Hocine Cherifi"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Due to the recent demand of 3-D models in several applications like medical\nimaging, video games, among others, the necessity of implementing 3-D mesh\nwatermarking schemes aiming to protect copyright has increased considerably.\nThe majority of robust 3-D watermarking techniques have essentially focused on\nthe robustness against attacks while the imperceptibility of these techniques\nis still a real issue. In this context, a blind robust 3-D mesh watermarking\nmethod based on mesh saliency and Quantization Index Modulation (QIM) for\nCopyright protection is proposed. The watermark is embedded by quantifying the\nvertex norms of the 3-D mesh using QIM scheme since it offers a good\nrobustness-capacity tradeoff. The choice of the vertices is adjusted by the\nmesh saliency to achieve watermark robustness and to avoid visual distortions.\nThe experimental results show the high imperceptibility of the proposed scheme\nwhile ensuring a good robustness against a wide range of attacks including\nadditive noise, similarity transformations, smoothing, quantization, etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.12828v1"
    },
    {
        "title": "Cloud Rendering-based Volumetric Video Streaming System for Mixed\n  Reality Services",
        "authors": [
            "Serhan Gül",
            "Dimitri Podborski",
            "Jangwoo Son",
            "Gurdeep Singh Bhullar",
            "Thomas Buchholz",
            "Thomas Schierl",
            "Cornelius Hellge"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Volumetric video is an emerging technology for immersive representation of 3D\nspaces that captures objects from all directions using multiple cameras and\ncreates a dynamic 3D model of the scene. However, processing volumetric content\nrequires high amounts of processing power and is still a very demanding task\nfor today's mobile devices. To mitigate this, we propose a volumetric video\nstreaming system that offloads the rendering to a powerful cloud/edge server\nand only sends the rendered 2D view to the client instead of the full\nvolumetric content. We use 6DoF head movement prediction techniques, WebRTC\nprotocol and hardware video encoding to ensure low-latency in different parts\nof the processing chain. We demonstrate our system using both a browser-based\nclient and a Microsoft HoloLens client. Our application contains generic\ninterfaces that allow for easy deployment of various augmented/mixed reality\nclients using the same server implementation.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.02526v2"
    },
    {
        "title": "Soft Video Multicasting Using Adaptive Compressed Sensing",
        "authors": [
            "Hadi Hadizadeh",
            "Ivan V. bajic"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Recently, soft video multicasting has gained a lot of attention, especially\nin broadcast and mobile scenarios where the bit rate supported by the channel\nmay differ across receivers, and may vary quickly over time. Unlike the\nconventional designs that force the source to use a single bit rate according\nto the receiver with the worst channel quality, soft video delivery schemes\ntransmit the video such that the video quality at each receiver is commensurate\nwith its specific instantaneous channel quality. In this paper, we present a\nsoft video multicasting system using an adaptive block-based compressed sensing\n(BCS) method. The proposed system consists of an encoder, a transmission\nsystem, and a decoder. At the encoder side, each block in each frame of the\ninput video is adaptively sampled with a rate that depends on the texture\ncomplexity and visual saliency of the block. The obtained BCS samples are then\nplaced into several packets, and the packets are transmitted via a\nchannel-aware OFDM (orthogonal frequency division multiplexing) transmission\nsystem with a number of subchannels. At the decoder side, the received BCS\nsamples are first used to build an initial approximation of the transmitted\nframe. To further improve the reconstruction quality, an iterative BCS\nreconstruction algorithm is then proposed that uses an adaptive transform and\nan adaptive soft-thresholding operator, which exploits the temporal similarity\nbetween adjacent frames to achieve better reconstruction quality. The extensive\nobjective and subjective experimental results indicate the superiority of the\nproposed system over the state-of-the-art soft video multicasting systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.03092v1"
    },
    {
        "title": "Exploring the Role of Visual Content in Fake News Detection",
        "authors": [
            "Juan Cao",
            "Peng Qi",
            "Qiang Sheng",
            "Tianyun Yang",
            "Junbo Guo",
            "Jintao Li"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The increasing popularity of social media promotes the proliferation of fake\nnews, which has caused significant negative societal effects. Therefore, fake\nnews detection on social media has recently become an emerging research area of\ngreat concern. With the development of multimedia technology, fake news\nattempts to utilize multimedia content with images or videos to attract and\nmislead consumers for rapid dissemination, which makes visual content an\nimportant part of fake news. Despite the importance of visual content, our\nunderstanding of the role of visual content in fake news detection is still\nlimited. This chapter presents a comprehensive review of the visual content in\nfake news, including the basic concepts, effective visual features,\nrepresentative detection methods and challenging issues of multimedia fake news\ndetection. This chapter can help readers to understand the role of visual\ncontent in fake news detection, and effectively utilize visual content to\nassist in detecting multimedia fake news.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05096v1"
    },
    {
        "title": "Hide Secret Information in Blocks: Minimum Distortion Embedding",
        "authors": [
            "Md Amiruzzaman",
            "Rizal Mohd Nor"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In this paper, a new steganographic method is presented that provides minimum\ndistortion in the stego image. The proposed encoding algorithm focuses on DCT\nrounding error and optimizes that in a way to reduce distortion in the stego\nimage, and the proposed algorithm produces less distortion than existing\nmethods (e.g., F5 algorithm). The proposed method is based on DCT rounding\nerror which helps to lower distortion and higher embedding capacity.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.07505v1"
    },
    {
        "title": "Convolutional Neural Networks for Continuous QoE Prediction in Video\n  Streaming Services",
        "authors": [
            "Tho Nguyen Duc",
            "Chanh Minh Tran",
            "Phan Xuan Tan",
            "Eiji Kamioka"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In video streaming services, predicting the continuous user's quality of\nexperience (QoE) plays a crucial role in delivering high quality streaming\ncontents to the user. However, the complexity caused by the temporal\ndependencies in QoE data and the non-linear relationships among QoE influence\nfactors has introduced challenges to continuous QoE prediction. To deal with\nthat, existing studies have utilized the Long Short-Term Memory model (LSTM) to\neffectively capture such complex dependencies, resulting in excellent QoE\nprediction accuracy. However, the high computational complexity of LSTM, caused\nby the sequential processing characteristic in its architecture, raises a\nserious question about its performance on devices with limited computational\npower. Meanwhile, Temporal Convolutional Network (TCN), a variation of\nconvolutional neural networks, has recently been proposed for sequence modeling\ntasks (e.g., speech enhancement), providing a superior prediction performance\nover baseline methods including LSTM in terms of prediction accuracy and\ncomputational complexity. Being inspired of that, in this paper, an improved\nTCN-based model, namely CNN-QoE, is proposed for continuously predicting the\nQoE, which poses characteristics of sequential data. The proposed model\nleverages the advantages of TCN to overcome the computational complexity\ndrawbacks of LSTM-based QoE models, while at the same time introducing the\nimprovements to its architecture to improve QoE prediction accuracy. Based on a\ncomprehensive evaluation, we demonstrate that the proposed CNN-QoE model can\nreach the state-of-the-art performance on both personal computers and mobile\ndevices, outperforming the existing approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.08574v1"
    },
    {
        "title": "Continuous QoE Prediction Based on WaveNet",
        "authors": [
            "Phan Xuan Tan",
            "Tho Nguyen Duc",
            "Chanh Minh Tran",
            "Eiji Kamioka"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Continuous QoE prediction is crucial in the purpose of maximizing viewer\nsatisfaction, by which video service providers could improve the revenue.\nContinuously predicting QoE is challenging since it requires QoE models that\nare capable of capturing the complex dependencies among QoE influence factors.\nThe existing approaches that utilize Long-Short-Term-Memory (LSTM) network\nsuccessfully model such long-term dependencies, providing the superior QoE\nprediction performance. However, the inherent drawback of sequential computing\nof LSTM will result in high computational cost in training and prediction\ntasks. Recently, WaveNet, a deep neural network for generating raw audio\nwaveform, has been introduced. Immediately, it gains a great attention since it\nsuccessfully leverages the characteristic of parallel computing of causal\nconvolution and dilated convolution to deal with time-series data (e.g., audio\nsignal). Being inspired by the success of WaveNet, in this paper, we propose\nWaveNet-based QoE model for continuous QoE prediction in video streaming\nservices. The model is trained and tested upon on two publicly available\ndatabases, namely, LFOVIA Video QoE and LIVE Mobile Stall Video II. The\nexperimental results demonstrate that the proposed model outperforms the\nbaselines models in terms of processing time, while maintaining sufficient\naccuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.09249v1"
    },
    {
        "title": "JPEG Steganography and Synchronization of DCT Coefficients for a Given\n  Development Pipeline",
        "authors": [
            "Théo Taburet",
            "Patrick Bas",
            "Wadih Sawaya",
            "Remi Cogranne"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This short paper proposes to use the statistical analysis of the correlation\nbetween DCT coefficients to design a new synchronization strategy that can be\nused for cost-based steganographic schemes in the JPEG domain. First, an\nanalysis is performed on the covariance matrix of DCT coefficients of\nneighboring blocks after a development similar to the one used to generate\nBossBase. This analysis exhibits groups of uncorrelated coefficients: 4 groups\nper block and 2 groups of uncorrelated diagonal neighbors together with groups\nof mutually correlated coefficients groups of 6 coefficients per blocs and 8\ncoefficients between 2 adjacent blocks. Using the uncorrelated groups, an\nembedding scheme can be designed using only 8 disjoint lattices. The cost map\nfor each lattice is updated firstly by using an implicit underlying Gaussian\ndistribution with a variance directly computed from the embedding costs, and\nsecondly by deriving conditional distributions from multivariate distributions.\nThe covariance matrix of these distributions takes into account both the\ncorrelations exhibited by the analysis of the covariance matrix and the\nvariance derived from the cost. This synchronization scheme enables to obtain a\ngain of PE of 5% at QF 95 for an embedding rate close to 0.3 bnzac coefficient\nusing DCTR feature sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.10082v2"
    },
    {
        "title": "Forensic Analysis of Residual Information in Adobe PDF Files",
        "authors": [
            "Hyunji Chung",
            "Jungheum Park",
            "Sangjin Lee"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In recent years, as electronic files include personal records and business\nactivities, these files can be used as important evidences in a digital\nforensic investigation process. In general, the data that can be verified using\nits own application programs is largely used in the investigation of document\nfiles. However, in the case of the PDF file that has been largely used at the\npresent time, certain data, which include the data before some modifications,\nexist in electronic document files unintentionally. Because such residual\ninformation may present the writing process of a file, it can be usefully used\nin a forensic viewpoint. This paper introduces why the residual information is\nstored inside the PDF file and explains a way to extract the information. In\naddition, we demonstrate the attributes of PDF files can be used to hide data.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.10546v1"
    },
    {
        "title": "From QoS Distributions to QoE Distributions: a System's Perspective",
        "authors": [
            "Tobias Hossfeld",
            "Poul E. Heegaard",
            "Martin Varela",
            "Lea Skorin-Kapov",
            "Markus Fiedler"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In the context of QoE management, network and service providers commonly rely\non models that map system QoS conditions (e.g., system response time, paket\nloss, etc.) to estimated end user QoE values. Observable QoS conditions in the\nsystem may be assumed to follow a certain distribution, meaning that different\nend users will experience different conditions. On the other hand, drawing from\nthe results of subjective user studies, we know that user diversity leads to\ndistributions of user scores for any given test conditions (in this case\nreferring to the QoS parameters of interest). Our previous studies have shown\nthat to correctly derive various QoE metrics (e.g., Mean Opinion Score (MOS),\nquantiles, probability of users rating \"good or better\", etc.) in a system\nunder given conditions, there is a need to consider rating distributions\nobtained from user studies, which are often times not available. In this paper\nwe extend these findings to show how to approximate user rating distributions\ngiven a QoS-to-MOS mapping function and second order statistics. Such a user\nrating distribution may then be combined with a QoS distribution observed in a\nsystem to finally derive corresponding distributions of QoE scores. We provide\ntwo examples to illustrate this process: 1) analytical results using a Web QoE\nmodel relating waiting times to QoE, and 2) numerical results using\nmeasurements relating packet losses to video stall pattern, which are in turn\nmapped to QoE estimates. The results in this paper provide a solution to the\nproblem of understanding the QoE distribution in a system, in cases where the\nnecessary data is not directly available in the form of models going beyond the\nMOS, or where the full details of subjective experiments are not available.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.12742v1"
    },
    {
        "title": "A Simple Model for Subject Behavior in Subjective Experiments",
        "authors": [
            "Zhi Li",
            "Christos G. Bampis",
            "Lukáš Krasula",
            "Lucjan Janowski",
            "Ioannis Katsavounidis"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In a subjective experiment to evaluate the perceptual audiovisual quality of\nmultimedia and television services, raw opinion scores collected from test\nsubjects are often noisy and unreliable. To produce the final mean opinion\nscores (MOS), recommendations such as ITU-R BT.500, ITU-T P.910 and ITU-T P.913\nstandardize post-test screening procedures to clean up the raw opinion scores,\nusing techniques such as subject outlier rejection and bias removal. In this\npaper, we analyze the prior standardized techniques to demonstrate their\nweaknesses. As an alternative, we propose a simple model to account for two of\nthe most dominant behaviors of subject inaccuracy: bias and inconsistency. We\nfurther show that this model can also effectively deal with inattentive\nsubjects that give random scores. We propose to use maximum likelihood\nestimation to jointly solve the model parameters, and present two numeric\nsolvers: the first based on the Newton-Raphson method, and the second based on\nan alternating projection (AP). We show that the AP solver generalizes the\nITU-T P.913 post-test screening procedure by weighing a subject's contribution\nto the true quality score by her consistency (thus, the quality scores\nestimated can be interpreted as bias-subtracted consistency-weighted MOS). We\ncompare the proposed methods with the standardized techniques using real\ndatasets and synthetic simulations, and demonstrate that the proposed methods\nare the most valuable when the test conditions are challenging (for example,\ncrowdsourcing and cross-lab studies), offering advantages such as better\nmodel-data fit, tighter confidence intervals, better robustness against subject\noutliers, the absence of hard coded parameters and thresholds, and auxiliary\ninformation on test subjects. The code for this work is open-sourced at\nhttps://github.com/Netflix/sureal.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.02067v3"
    },
    {
        "title": "Reinforcing Short-Length Hashing",
        "authors": [
            "Xingbo Liu",
            "Xiushan Nie",
            "Qi Dai",
            "Yupan Huang",
            "Yilong Yin"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Due to the compelling efficiency in retrieval and storage,\nsimilarity-preserving hashing has been widely applied to approximate nearest\nneighbor search in large-scale image retrieval. However, existing methods have\npoor performance in retrieval using an extremely short-length hash code due to\nweak ability of classification and poor distribution of hash bit. To address\nthis issue, in this study, we propose a novel reinforcing short-length hashing\n(RSLH). In this proposed RSLH, mutual reconstruction between the hash\nrepresentation and semantic labels is performed to preserve the semantic\ninformation. Furthermore, to enhance the accuracy of hash representation, a\npairwise similarity matrix is designed to make a balance between accuracy and\ntraining expenditure on memory. In addition, a parameter boosting strategy is\nintegrated to reinforce the precision with hash bits fusion. Extensive\nexperiments on three large-scale image benchmarks demonstrate the superior\nperformance of RSLH under various short-length hashing scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11511v1"
    },
    {
        "title": "Bharatanatyam Dance Transcription using Multimedia Ontology and Machine\n  Learning",
        "authors": [
            "Tanwi Mallick",
            "Patha Pratim Das",
            "Arun Kumar Majumdar"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Indian Classical Dance is an over 5000 years' old multi-modal language for\nexpressing emotions. Preservation of dance through multimedia technology is a\nchallenging task. In this paper, we develop a system to generate a parseable\nrepresentation of a dance performance. The system will help to preserve\nintangible heritage, annotate performances for better tutoring, and synthesize\ndance performances. We first attempt to capture the concepts of the basic steps\nof an Indian Classical Dance form, named Bharatanatyam Adavus, in an\nontological model. Next, we build an event-based low-level model that relates\nthe ontology of Adavus to the ontology of multi-modal data streams (RGB-D of\nKinect in this case) for a computationally realizable framework. Finally, the\nontology is used for transcription into Labanotation. We also present a\ntranscription tool for encoding the performances of Bharatanatyam Adavus to\nLabanotation and test it on our recorded data set. Our primary aim is to\ndocument the complex movements of dance in terms of Labanotation using the\nontology.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11994v1"
    },
    {
        "title": "SSIM-Based CTU-Level Joint Optimal Bit Allocation and Rate Distortion\n  Optimization",
        "authors": [
            "Yang Li",
            "Xuanqin Mou"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Structural similarity (SSIM)-based distortion $D_\\text{SSIM}$ is more\nconsistent with human perception than the traditional mean squared error\n$D_\\text{MSE}$. To achieve better video quality, many studies on optimal bit\nallocation (OBA) and rate-distortion optimization (RDO) used $D_\\text{SSIM}$ as\nthe distortion metric. However, many of them failed to optimize OBA and RDO\njointly based on SSIM, thus causing a non-optimal R-$D_\\text{SSIM}$\nperformance. This problem is due to the lack of an accurate R-$D_\\text{SSIM}$\nmodel that can be used uniformly in both OBA and RDO. To solve this problem, we\npropose a $D_\\text{SSIM}$-$D_\\text{MSE}$ model first. Based on this model, the\ncomplex R-$D_\\text{SSIM}$ cost in RDO can be calculated as simpler\nR-$D_\\text{MSE}$ cost with a new SSIM-related Lagrange multiplier. This not\nonly reduces the computation burden of SSIM-based RDO, but also enables the\nR-$D_\\text{SSIM}$ model to be uniformly used in OBA and RDO. Moreover, with the\nnew SSIM-related Lagrange multiplier in hand, the joint relationship of\nR-$D_\\text{SSIM}$-$\\lambda_\\text{SSIM}$ (the negative derivative of\nR-$D_\\text{SSIM}$) can be built, based on which the R-$D_\\text{SSIM}$ model\nparameters can be calculated accurately. With accurate and unified\nR-$D_\\text{SSIM}$ model, SSIM-based OBA and SSIM-based RDO are unified together\nin our scheme, called SOSR. Compared with the HEVC reference encoder HM16.20,\nSOSR saves 4%, 10%, and 14% bitrate under the same SSIM in all-intra,\nhierarchical and non-hierarchical low-delay-B configurations, which is superior\nto other state-of-the-art schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.13369v2"
    },
    {
        "title": "An Automated and Robust Image Watermarking Scheme Based on Deep Neural\n  Networks",
        "authors": [
            "Xin Zhong",
            "Pei-Chi Huang",
            "Spyridon Mastorakis",
            "Frank Y. Shih"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Digital image watermarking is the process of embedding and extracting a\nwatermark covertly on a cover-image. To dynamically adapt image watermarking\nalgorithms, deep learning-based image watermarking schemes have attracted\nincreased attention during recent years. However, existing deep learning-based\nwatermarking methods neither fully apply the fitting ability to learn and\nautomate the embedding and extracting algorithms, nor achieve the properties of\nrobustness and blindness simultaneously. In this paper, a robust and blind\nimage watermarking scheme based on deep learning neural networks is proposed.\nTo minimize the requirement of domain knowledge, the fitting ability of deep\nneural networks is exploited to learn and generalize an automated image\nwatermarking algorithm. A deep learning architecture is specially designed for\nimage watermarking tasks, which will be trained in an unsupervised manner to\navoid human intervention and annotation. To facilitate flexible applications,\nthe robustness of the proposed scheme is achieved without requiring any prior\nknowledge or adversarial examples of possible attacks. A challenging case of\nwatermark extraction from phone camera-captured images demonstrates the\nrobustness and practicality of the proposal. The experiments, evaluation, and\napplication cases confirm the superiority of the proposed scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.02460v1"
    },
    {
        "title": "Cost-Efficient Storage for On-Demand Video Streaming on Cloud",
        "authors": [
            "Mahmoud Darwich",
            "Yasser Ismail",
            "Talal Darwich",
            "Magdy Bayoumi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Video stream is converted to several formats to support the user's device,\nthis conversion process is called video transcoding, which imposes high storage\nand powerful resources. With emerging of cloud technology, video stream\ncompanies adopted to process video on the cloud. Generally, many formats of the\nsame video are made (pre-transcoded) and streamed to the adequate user's\ndevice. However, pre-transcoding demands huge storage space and incurs a\nhigh-cost to the video stream companies. More importantly, the pre-transcoding\nof video streams could be hierarchy carried out through different storage types\nin the cloud. To minimize the storage cost, in this paper, we propose a method\nto store video streams in the hierarchical storage of the cloud. Particularly,\nwe develop a method to decide which video stream should be pre-transcoded in\nits suitable cloud storage to minimize the overall cost. Experimental\nsimulation and results show the effectiveness of our approach, specifically,\nwhen the percentage of frequently accessed videos is high in repositories, the\nproposed approach minimizes the overall cost by up to 40 percent.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.03410v1"
    },
    {
        "title": "Local Geometric Distortions Resilient Watermarking Scheme Based on\n  Symmetry",
        "authors": [
            "Zehua Ma",
            "Weiming Zhang",
            "Han Fang",
            "Xiaoyi Dong",
            "Linfeng Geng",
            "Nenghai Yu"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  As an efficient watermark attack method, geometric distortions destroy the\nsynchronization between watermark encoder and decoder. And the local geometric\ndistortion is a famous challenge in the watermark field. Although a lot of\ngeometric distortions resilient watermarking schemes have been proposed, few of\nthem perform well against local geometric distortion like random bending attack\n(RBA). To address this problem, this paper proposes a novel watermark\nsynchronization process and the corresponding watermarking scheme. In our\nscheme, the watermark bits are represented by random patterns. The message is\nencoded to get a watermark unit, and the watermark unit is flipped to generate\na symmetrical watermark. Then the symmetrical watermark is embedded into the\nspatial domain of the host image in an additive way. In watermark extraction,\nwe first get the theoretically mean-square error minimized estimation of the\nwatermark. Then the auto-convolution function is applied to this estimation to\ndetect the symmetry and get a watermark units map. According to this map, the\nwatermark can be accurately synchronized, and then the extraction can be done.\nExperimental results demonstrate the excellent robustness of the proposed\nwatermarking scheme to local geometric distortions, global geometric\ndistortions, common image processing operations, and some kinds of combined\nattacks.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.10240v1"
    },
    {
        "title": "An authorship protection technology for electronic documents based on\n  image watermarking",
        "authors": [
            "Anna Melman",
            "Oleg Evsutin",
            "Alexander Shelupanov"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In the field of information technology, information security technologies\nhold a special place. They ensure the security of the use of information\ntechnology. One of the urgent tasks is the protection of electronic documents\nduring their transfer in information systems. This paper proposes a technology\nfor protecting electronic documents containing digital images. The main idea is\nthat the electronic document authorship protection can be implemented by\ndigital watermark embedding in the images that are contained in this document.\nThe paper considers three cases of using the proposed technology: full copying\nof an electronic document, copying of images contained in the document, and\ncopying of text. It is shown that in all three cases the authorship\nconfirmation can be successfully implemented. Computational experiments are\nconducted with robust watermarking algorithms that can be used within the\ntechnology. A scenario of technology implementation is proposed, which provides\nfor the joint use of different class algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.00302v1"
    },
    {
        "title": "A Clustering-Based Method for Automatic Educational Video Recommendation\n  Using Deep Face-Features of Lecturers",
        "authors": [
            "Paulo R. C. Mendes",
            "Eduardo S. Vieira",
            "Álan L. V. Guedes",
            "Antonio J. G. Busson",
            "Sérgio Colcher"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Discovering and accessing specific content within educational video bases is\na challenging task, mainly because of the abundance of video content and its\ndiversity. Recommender systems are often used to enhance the ability to find\nand select content. But, recommendation mechanisms, especially those based on\ntextual information, exhibit some limitations, such as being error-prone to\nmanually created keywords or due to imprecise speech recognition. This paper\npresents a method for generating educational video recommendation using deep\nface-features of lecturers without identifying them. More precisely, we use an\nunsupervised face clustering mechanism to create relations among the videos\nbased on the lecturer's presence. Then, for a selected educational video taken\nas a reference, we recommend the ones where the presence of the same lecturers\nis detected. Moreover, we rank these recommended videos based on the amount of\ntime the referenced lecturers were present. For this task, we achieved a mAP\nvalue of 99.165%.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.04676v1"
    },
    {
        "title": "DIME: An Online Tool for the Visual Comparison of Cross-Modal Retrieval\n  Models",
        "authors": [
            "Tony Zhao",
            "Jaeyoung Choi",
            "Gerald Friedland"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Cross-modal retrieval relies on accurate models to retrieve relevant results\nfor queries across modalities such as image, text, and video. In this paper, we\nbuild upon previous work by tackling the difficulty of evaluating models both\nquantitatively and qualitatively quickly. We present DIME (Dataset, Index,\nModel, Embedding), a modality-agnostic tool that handles multimodal datasets,\ntrained models, and data preprocessors to support straightforward model\ncomparison with a web browser graphical user interface. DIME inherently\nsupports building modality-agnostic queryable indexes and extraction of\nrelevant feature embeddings, and thus effectively doubles as an efficient\ncross-modal tool to explore and search through datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.09641v1"
    },
    {
        "title": "Short Video-based Advertisements Evaluation System: Self-Organizing\n  Learning Approach",
        "authors": [
            "Yunjie Zhang",
            "Fei Tao",
            "Xudong Liu",
            "Runze Su",
            "Xiaorong Mei",
            "Weicong Ding",
            "Zhichen Zhao",
            "Lei Yuan",
            "Ji Liu"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  With the rising of short video apps, such as TikTok, Snapchat and Kwai,\nadvertisement in short-term user-generated videos (UGVs) has become a trending\nform of advertising. Prediction of user behavior without specific user profile\nis required by advertisers, as they expect to acquire advertisement performance\nin advance in the scenario of cold start. Current recommender system do not\ntake raw videos as input; additionally, most previous work of Multi-Modal\nMachine Learning may not deal with unconstrained videos like UGVs. In this\npaper, we proposed a novel end-to-end self-organizing framework for user\nbehavior prediction. Our model is able to learn the optimal topology of neural\nnetwork architecture, as well as optimal weights, through training data. We\nevaluate our proposed method on our in-house dataset. The experimental results\nreveal that our model achieves the best performance in all our experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.12662v1"
    },
    {
        "title": "A multi-level approach with visual information for encrypted H.265/HEVC\n  videos",
        "authors": [
            "Wenying Wen",
            "Rongxin Tu",
            "Yushu Zhang",
            "Yuming Fang",
            "Yong Yang"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  High-efficiency video coding (HEVC) encryption has been proposed to encrypt\nsyntax elements for the purpose of video encryption. To achieve high video\nsecurity, to the best of our knowledge, almost all of the existing HEVC\nencryption algorithms mainly encrypt the whole video, such that the user\nwithout permissions cannot obtain any viewable information. However, these\nencryption algorithms cannot meet the needs of customers who need part of the\ninformation but not the full information in the video. In many cases, such as\nprofessional paid videos or video meetings, users would like to observe some\nvisible information in the encrypted video of the original video to satisfy\ntheir requirements in daily life. Aiming at this demand, this paper proposes a\nmulti-level encryption scheme that is composed of lightweight encryption,\nmedium encryption and heavyweight encryption, where each encryption level can\nobtain a different amount of visual information. It is found that both\nencrypting the luma intraprediction model (IPM) and scrambling the syntax\nelement of the DCT coefficient sign can achieve the performance of a distorted\nvideo in which there is still residual visual information, while encrypting\nboth of them can implement the intensity of encryption and one cannot gain any\nvisual information. The experimental results meet our expectations\nappropriately, indicating that there is a different amount of visual\ninformation in each encryption level. Meanwhile, users can flexibly choose the\nencryption level according to their various requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.02620v1"
    },
    {
        "title": "OpenKinoAI: An Open Source Framework for Intelligent Cinematography and\n  Editing of Live Performances",
        "authors": [
            "Rémi Ronfard",
            "Rémi Colin de Verdière"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  OpenKinoAI is an open source framework for post-production of ultra high\ndefinition video which makes it possible to emulate professional multiclip\nediting techniques for the case of single camera recordings. OpenKinoAI\nincludes tools for uploading raw video footage of live performances on a remote\nweb server, detecting, tracking and recognizing the performers in the original\nmaterial, reframing the raw video into a large choice of cinematographic\nrushes, editing the rushes into movies, and annotating rushes and movies for\ndocumentation purposes. OpenKinoAI is made available to promote research in\nmulticlip video editing of ultra high definition video, and to allow performing\nartists and companies to use this research for archiving, documenting and\nsharing their work online in an innovative fashion.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.05203v1"
    },
    {
        "title": "Building Movie Map -- A Tool for Exploring Areas in a City -- and its\n  Evaluation",
        "authors": [
            "Naoki Sugimoto",
            "Yoshihito Ebine",
            "Kiyoharu Aizawa"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  We propose a new Movie Map system, with an interface for exploring cities.\nThe system consists of four stages; acquisition, analysis, management, and\ninteraction. In the acquisition stage, omnidirectional videos are taken along\nstreets in target areas. Frames of the video are localized on the map,\nintersections are detected, and videos are segmented. Turning views at\nintersections are subsequently generated. By connecting the video segments\nfollowing the specified movement in an area, we can view the streets better.\nThe interface allows for easy exploration of a target area, and it can show\nvirtual billboards of stores in the view. We conducted user studies to compare\nour system to the GSV in a scenario where users could freely move and explore\nto find a landmark. The experiment showed that our system had a better user\nexperience than GSV.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.08525v1"
    },
    {
        "title": "Overview of Screen Content Coding in Recently Developed Video Coding\n  Standards",
        "authors": [
            "Xiaozhong Xu",
            "Shan Liu"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In recent years, screen content (SC) video including computer generated text,\ngraphics and animations, have drawn more attention than ever, as many related\napplications become very popular. To address the need for efficient coding of\nsuch contents, a number of coding tools have been specifically developed and\nachieved great advances in terms of coding efficiency. The inclusion of screen\ncontent coding (SCC) features in all the recently developed video coding\nstandards (namely, HEVC SCC, VVC, AVS3, AV1 and EVC) demonstrated the\nimportance of supporting such features. This paper provides an overview and\ncomparative study of screen content coding technologies, with discussions on\nthe performance and complexity aspects for the tools developed in these\nstandards.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.14068v1"
    },
    {
        "title": "CSIS: compressed sensing-based enhanced-embedding capacity image\n  steganography scheme",
        "authors": [
            "Rohit Agrawal",
            "Kapil Ahuja"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Image steganography plays a vital role in securing secret data by embedding\nit in the cover images. Usually, these images are communicated in a compressed\nformat. Existing techniques achieve this but have low embedding capacity.\nEnhancing this capacity causes a deterioration in the visual quality of the\nstego-image. Hence, our goal here is to enhance the embedding capacity while\npreserving the visual quality of the stego-image. We also intend to ensure that\nour scheme is resistant to steganalysis attacks.\n  This paper proposes a Compressed Sensing Image Steganography (CSIS) scheme to\nachieve our goal while embedding binary data in images. The novelty of our\nscheme is the combination of three components in attaining the above-listed\ngoals. First, we use compressed sensing to sparsify cover image block-wise,\nobtain its linear measurements, and then uniquely select permissible\nmeasurements. Further, before embedding the secret data, we encrypt it using\nthe Data Encryption Standard (DES) algorithm, and finally, we embed two bits of\nencrypted data into each permissible measurement. Second, we propose a novel\ndata extraction technique, which is lossless and completely recovers our secret\ndata. Third, for the reconstruction of the stego-image, we use the least\nabsolute shrinkage and selection operator (LASSO) for the resultant\noptimization problem.\n  We perform experiments on several standard grayscale images and a color\nimage, and evaluate embedding capacity, PSNR value, mean SSIM index, NCC\ncoefficients, and entropy. We achieve 1.53 times more embedding capacity as\ncompared to the most recent scheme. We obtain an average of 37.92 dB PSNR\nvalue, and average values close to 1 for both the mean SSIM index and the NCC\ncoefficients, which are considered good. Moreover, the entropy of cover images\nand their corresponding stego-images are nearly the same.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.00690v1"
    },
    {
        "title": "The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset:\n  Collection, Insights and Improvements",
        "authors": [
            "Lukas Stappen",
            "Alice Baird",
            "Lea Schumann",
            "Björn Schuller"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Truly real-life data presents a strong, but exciting challenge for sentiment\nand emotion research. The high variety of possible `in-the-wild' properties\nmakes large datasets such as these indispensable with respect to building\nrobust machine learning models. A sufficient quantity of data covering a deep\nvariety in the challenges of each modality to force the exploratory analysis of\nthe interplay of all modalities has not yet been made available in this\ncontext. In this contribution, we present MuSe-CaR, a first of its kind\nmultimodal dataset. The data is publicly available as it recently served as the\ntesting bed for the 1st Multimodal Sentiment Analysis Challenge, and focused on\nthe tasks of emotion, emotion-target engagement, and trustworthiness\nrecognition by means of comprehensively integrating the audio-visual and\nlanguage modalities. Furthermore, we give a thorough overview of the dataset in\nterms of collection and annotation, including annotation tiers not used in this\nyear's MuSe 2020. In addition, for one of the sub-challenges - predicting the\nlevel of trustworthiness - no participant outperformed the baseline model, and\nso we propose a simple, but highly efficient Multi-Head-Attention network that\nexceeds using multimodal fusion the baseline by around 0.2 CCC (almost 50 %\nimprovement).\n",
        "pdf_link": "http://arxiv.org/pdf/2101.06053v2"
    },
    {
        "title": "A Novel Local Binary Pattern Based Blind Feature Image Steganography",
        "authors": [
            "Soumendu Chakraborty",
            "Anand Singh Jalal"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Steganography methods in general terms tend to embed more and more secret\nbits in the cover images. Most of these methods are designed to embed secret\ninformation in such a way that the change in the visual quality of the\nresulting stego image is not detectable. There exists some methods which\npreserve the global structure of the cover after embedding. However, the\nembedding capacity of these methods is very less. In this paper a novel feature\nbased blind image steganography technique is proposed, which preserves the LBP\n(Local binary pattern) feature of the cover with comparable embedding rates.\nLocal binary pattern is a well known image descriptor used for image\nrepresentation. The proposed scheme computes the local binary pattern to hide\nthe bits of the secret image in such a way that the local relationship that\nexists in the cover are preserved in the resulting stego image. The performance\nof the proposed steganography method has been tested on several images of\ndifferent types to show the robustness. State of the art LSB based\nsteganography methods are compared with the proposed method to show the\neffectiveness of feature based image steganography\n",
        "pdf_link": "http://arxiv.org/pdf/2101.06383v1"
    },
    {
        "title": "Ambiguity of Objective Image Quality Metrics: A New Methodology for\n  Performance Evaluation",
        "authors": [
            "Manri Cheon",
            "Toinon Vigier",
            "Lukáš Krasula",
            "Junghyuk Lee",
            "Patrick Le Callet",
            "Jong-Seok Lee"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Objective image quality metrics try to estimate the perceptual quality of the\ngiven image by considering the characteristics of the human visual system.\nHowever, it is possible that the metrics produce different quality scores even\nfor two images that are perceptually indistinguishable by human viewers, which\nhave not been considered in the existing studies related to objective quality\nassessment. In this paper, we address the issue of ambiguity of objective image\nquality assessment. We propose an approach to obtain an ambiguity interval of\nan objective metric, within which the quality score difference is not\nperceptually significant. In particular, we use the visual difference\npredictor, which can consider viewing conditions that are important for visual\nquality perception. In order to demonstrate the usefulness of the proposed\napproach, we conduct experiments with 33 state-of-the-art image quality metrics\nin the viewpoint of their accuracy and ambiguity for three image quality\ndatabases. The results show that the ambiguity intervals can be applied as an\nadditional figure of merit when conventional performance measurement does not\ndetermine superiority between the metrics. The effect of the viewing distance\non the ambiguity interval is also shown.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.07439v1"
    },
    {
        "title": "Wide Color Gamut Image Content Characterization: Method, Evaluation, and\n  Applications",
        "authors": [
            "Junghyuk Lee",
            "Toinon Vigier",
            "Patrick Le Callet",
            "Jong-Seok Lee"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In this paper, we propose a novel framework to characterize a wide color\ngamut image content based on perceived quality due to the processes that change\ncolor gamut, and demonstrate two practical use cases where the framework can be\napplied. We first introduce the main framework and implementation details.\nThen, we provide analysis for understanding of existing wide color gamut\ndatasets with quantitative characterization criteria on their characteristics,\nwhere four criteria, i.e., coverage, total coverage, uniformity, and total\nuniformity, are proposed. Finally, the framework is applied to content\nselection in a gamut mapping evaluation scenario in order to enhance\nreliability and robustness of the evaluation results. As a result, the\nframework fulfils content characterization for studies where quality of\nexperience of wide color gamut stimuli is involved.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.07451v1"
    },
    {
        "title": "Efficient video integrity analysis through container characterization",
        "authors": [
            "Pengpeng Yang",
            "Daniele Baracchi",
            "Massimo Iuliani",
            "Dasara Shullani",
            "Rongrong Ni",
            "Yao Zhao",
            "Alessandro Piva"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Most video forensic techniques look for traces within the data stream that\nare, however, mostly ineffective when dealing with strongly compressed or low\nresolution videos. Recent research highlighted that useful forensic traces are\nalso left in the video container structure, thus offering the opportunity to\nunderstand the life-cycle of a video file without looking at the media stream\nitself.\n  In this paper we introduce a container-based method to identify the software\nused to perform a video manipulation and, in most cases, the operating system\nof the source device. As opposed to the state of the art, the proposed method\nis both efficient and effective and can also provide a simple explanation for\nits decisions. This is achieved by using a decision-tree-based classifier\napplied to a vectorial representation of the video container structure. We\nconducted an extensive validation on a dataset of 7000 video files including\nboth software manipulated contents (ffmpeg, Exiftool, Adobe Premiere, Avidemux,\nand Kdenlive), and videos exchanged through social media platforms (Facebook,\nTikTok, Weibo and YouTube). This dataset has been made available to the\nresearch community. The proposed method achieves an accuracy of 97.6% in\ndistinguishing pristine from tampered videos and classifying the editing\nsoftware, even when the video is cut without re-encoding or when it is\ndownscaled to the size of a thumbnail. Furthermore, it is capable of correctly\nidentifying the operating system of the source device for most of the tampered\nvideos.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.10795v1"
    },
    {
        "title": "Multimedia Technology Applications and Algorithms: A Survey",
        "authors": [
            "Palak Tiwary",
            "Sanjida Ahmed"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Multimedia related research and development has evolved rapidly in the last\nfew years with advancements in hardware, software and network infrastructures.\nAs a result, multimedia has been integrated into domains like Healthcare and\nMedicine, Human facial feature extraction and tracking, pose recognition,\ndisparity estimation, etc. This survey gives an overview of the various\nmultimedia technologies and algorithms developed in the domains mentioned.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.01301v1"
    },
    {
        "title": "Speech Quality Assessment in Crowdsourcing: Comparison Category Rating\n  Method",
        "authors": [
            "Babak Naderi",
            "Sebastian Möller",
            "Ross Cutler"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Traditionally, Quality of Experience (QoE) for a communication system is\nevaluated through a subjective test. The most common test method for speech QoE\nis the Absolute Category Rating (ACR), in which participants listen to a set of\nstimuli, processed by the underlying test conditions, and rate their perceived\nquality for each stimulus on a specific scale. The Comparison Category Rating\n(CCR) is another standard approach in which participants listen to both\nreference and processed stimuli and rate their quality compared to the other\none. The CCR method is particularly suitable for systems that improve the\nquality of input speech. This paper evaluates an adaptation of the CCR test\nprocedure for assessing speech quality in the crowdsourcing set-up. The CCR\nmethod was introduced in the ITU-T Rec. P.800 for laboratory-based experiments.\nWe adapted the test for the crowdsourcing approach following the guidelines\nfrom ITU-T Rec. P.800 and P.808. We show that the results of the CCR procedure\nvia crowdsourcing are highly reproducible. We also compared the CCR test\nresults with widely used ACR test procedures obtained in the laboratory and\ncrowdsourcing. Our results show that the CCR procedure in crowdsourcing is a\nreliable and valid test method.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.04371v1"
    },
    {
        "title": "A Flexible Lossy Depth Video Coding Scheme Based on Low-rank Tensor\n  Modelling and HEVC Intra Prediction for Free Viewpoint Video",
        "authors": [
            "Mansi Sharma",
            "Santosh Kumar"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The compression quality losses of depth sequences determine quality of view\nsynthesis in free-viewpoint video. The depth map intra prediction in 3D\nextensions of the HEVC applies intra modes with auxiliary depth modeling modes\n(DMMs) to better preserve depth edges and handle motion discontinuities.\nAlthough such modes enable high efficiency compression, but at the cost of very\nhigh encoding complexity. Skipping conventional intra coding modes and DMMs in\ndepth coding limits practical applicability of the HEVC for 3D display\napplications. In this paper, we introduce a novel low-complexity scheme for\ndepth video compression based on low-rank tensor decomposition and HEVC intra\ncoding. The proposed scheme leverages spatial and temporal redundancy by\ncompactly representing the depth sequence as a high-order tensor. Tensor\nfactorization into a set of factor matrices following CANDECOMP PARAFAC (CP)\ndecomposition via alternating least squares give a low-rank approximation of\nthe scene geometry. Further, compression of factor matrices with HEVC intra\nprediction support arbitrary target accuracy by flexible adjustment of bitrate,\nvarying tensor decomposition ranks and quantization parameters. The results\ndemonstrate proposed approach achieves significant rate gains by efficiently\ncompressing depth planes in low-rank approximated representation. The proposed\nalgorithm is applied to encode depth maps of benchmark Ballet and Breakdancing\nsequences. The decoded depth sequences are used for view synthesis in a\nmulti-view video system, maintaining appropriate rendering quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.04678v1"
    },
    {
        "title": "\"Subverting the Jewtocracy\": Online Antisemitism Detection Using\n  Multimodal Deep Learning",
        "authors": [
            "Mohit Chandra",
            "Dheeraj Pailla",
            "Himanshu Bhatia",
            "Aadilmehdi Sanchawala",
            "Manish Gupta",
            "Manish Shrivastava",
            "Ponnurangam Kumaraguru"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The exponential rise of online social media has enabled the creation,\ndistribution, and consumption of information at an unprecedented rate. However,\nit has also led to the burgeoning of various forms of online abuse. Increasing\ncases of online antisemitism have become one of the major concerns because of\nits socio-political consequences. Unlike other major forms of online abuse like\nracism, sexism, etc., online antisemitism has not been studied much from a\nmachine learning perspective. To the best of our knowledge, we present the\nfirst work in the direction of automated multimodal detection of online\nantisemitism. The task poses multiple challenges that include extracting\nsignals across multiple modalities, contextual references, and handling\nmultiple aspects of antisemitism. Unfortunately, there does not exist any\npublicly available benchmark corpus for this critical task. Hence, we collect\nand label two datasets with 3,102 and 3,509 social media posts from Twitter and\nGab respectively. Further, we present a multimodal deep learning system that\ndetects the presence of antisemitic content and its specific antisemitism\ncategory using text and images from posts. We perform an extensive set of\nexperiments on the two datasets to evaluate the efficacy of the proposed\nsystem. Finally, we also present a qualitative analysis of our study.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.05947v3"
    },
    {
        "title": "ANT: Learning Accurate Network Throughput for Better Adaptive Video\n  Streaming",
        "authors": [
            "Jiaoyang Yin",
            "Yiling Xu",
            "Hao Chen",
            "Yunfei Zhang",
            "Steve Appleby",
            "Zhan Ma"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Adaptive Bit Rate (ABR) decision plays a crucial role for ensuring\nsatisfactory Quality of Experience (QoE) in video streaming applications, in\nwhich past network statistics are mainly leveraged for future network bandwidth\nprediction. However, most algorithms, either rules-based or learning-driven\napproaches, feed throughput traces or classified traces based on traditional\nstatistics (i.e., mean/standard deviation) to drive ABR decision, leading to\ncompromised performances in specific scenarios. Given the diverse network\nconnections (e.g., WiFi, cellular and wired link) from time to time, this paper\nthus proposes to learn the ANT (a.k.a., Accurate Network Throughput) model to\ncharacterize the full spectrum of network throughput dynamics in the past for\nderiving the proper network condition associated with a specific cluster of\nnetwork throughput segments (NTS). Each cluster of NTS is then used to generate\na dedicated ABR model, by which we wish to better capture the network dynamics\nfor diverse connections. We have integrated the ANT model with existing\nreinforcement learning (RL)-based ABR decision engine, where different ABR\nmodels are applied to respond to the accurate network sensing for better rate\ndecision. Extensive experiment results show that our approach can significantly\nimprove the user QoE by 65.5% and 31.3% respectively, compared with the\nstate-of-the-art Pensive and Oboe, across a wide range of network scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.12507v2"
    },
    {
        "title": "Adaptive Encoding for Constrained Video Delivery in HEVC, VP9, AV1 and\n  VVC Compression Standards and Adaptation to Video Content",
        "authors": [
            "Gangadharan Esakki"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The dissertation proposes the use of a multi-objective optimization framework\nfor designing and selecting among enhanced GOP configurations in video\ncompression standards. The proposed methods achieve fine optimization over a\nset of general modes that include: (i) maximum video quality, (ii) minimum\nbitrate, (iii) maximum encoding rate (previously minimum encoding time mode)\nand (iv) can be shown to improve upon the YouTube/Netflix default encoder mode\nsettings over a set of opposing constraints to guarantee satisfactory\nperformance. The dissertation describes the implementation of a codec-agnostic\napproach using different video coding standards (x265, VP9, AV1) on a wide\nrange of videos derived from different video datasets. The results demonstrate\nthat the optimal encoding parameters obtained from the Pareto front space can\nprovide significant bandwidth savings without sacrificing video quality. This\nis achieved by the use of effective regression models that allow for the\nselection of video encoding settings that are jointly optimal in the encoding\ntime, bitrate, and video quality space. The dissertation applies the proposed\nmethods to x265, VP9, AV1 and using new GOP configurations in x265, delivering\nover 40% of the optimal encodings in two standard reference videos.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.12770v1"
    },
    {
        "title": "An Estimation of Online Video User Engagement from Features of\n  Continuous Emotions",
        "authors": [
            "Lukas Stappen",
            "Alice Baird",
            "Michelle Lienhart",
            "Annalena Bätz",
            "Björn Schuller"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Portraying emotion and trustworthiness is known to increase the appeal of\nvideo content. However, the causal relationship between these signals and\nonline user engagement is not well understood. This limited understanding is\npartly due to a scarcity in emotionally annotated data and the varied\nmodalities which express user engagement online. In this contribution, we\nutilise a large dataset of YouTube review videos which includes ca. 600 hours\nof dimensional arousal, valence and trustworthiness annotations. We investigate\nfeatures extracted from these signals against various user engagement\nindicators including views, like/dislike ratio, as well as the sentiment of\ncomments. In doing so, we identify the positive and negative influences which\nsingle features have, as well as interpretable patterns in each dimension which\nrelate to user engagement. Our results demonstrate that smaller boundary ranges\nand fluctuations for arousal lead to an increase in user engagement.\nFurthermore, the extracted time-series features reveal significant (p<0.05)\ncorrelations for each dimension, such as, count below signal mean (arousal),\nnumber of peaks (valence), and absolute energy (trustworthiness). From this, an\neffective combination of features is outlined for approaches aiming to\nautomatically predict several user engagement indicators. In a user engagement\nprediction paradigm we compare all features against semi-automatic\n(cross-task), and automatic (task-specific) feature selection methods. These\nselected feature sets appear to outperform the usage of all features, e.g.,\nusing all features achieves 1.55 likes per day (Lp/d) mean absolute error from\nvalence; this improves through semi-automatic and automatic selection to 1.33\nand 1.23 Lp/d, respectively (data mean 9.72 Lp/d with a std. 28.75 Lp/d).\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01633v1"
    },
    {
        "title": "Forensic Analysis of Video Files Using Metadata",
        "authors": [
            "Ziyue Xiang",
            "János Horváth",
            "Sriram Baireddy",
            "Paolo Bestagini",
            "Stefano Tubaro",
            "Edward J. Delp"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The unprecedented ease and ability to manipulate video content has led to a\nrapid spread of manipulated media. The availability of video editing tools\ngreatly increased in recent years, allowing one to easily generate\nphoto-realistic alterations. Such manipulations can leave traces in the\nmetadata embedded in video files. This metadata information can be used to\ndetermine video manipulations, brand of video recording device, the type of\nvideo editing tool, and other important evidence. In this paper, we focus on\nthe metadata contained in the popular MP4 video wrapper/container. We describe\nour method for metadata extractor that uses the MP4's tree structure. Our\napproach for analyzing the video metadata produces a more compact\nrepresentation. We will describe how we construct features from the metadata\nand then use dimensionality reduction and nearest neighbor classification for\nforensic analysis of a video file. Our approach allows one to visually inspect\nthe distribution of metadata features and make decisions. The experimental\nresults confirm that the performance of our approach surpasses other methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.06361v2"
    },
    {
        "title": "Adaptive Video Encoding For Different Video Codecs",
        "authors": [
            "Gangadharan Esakki",
            "Andreas Panayides",
            "Venkatesh Jatla",
            "Marios Pattichis"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  By 2022, we expect video traffic to reach 82% of the total internet traffic.\nUndoubtedly, the abundance of video-driven applications will likely lead\ninternet video traffic percentage to a further increase in the near future,\nenabled by associate advances in video devices' capabilities. In response to\nthis ever-growing demand, the Alliance for Open Media (AOM) and the Joint Video\nExperts Team (JVET) have demonstrated strong and renewed interest in developing\nnew video codecs. In the fast-changing video codecs' landscape, there is thus,\na genuine need to develop adaptive methods that can be universally applied to\ndifferent codecs. In this study, we formulate video encoding as a\nmulti-objective optimization process where video quality (as a function of VMAF\nand PSNR), bitrate demands, and encoding rate (in encoded frames per second)\nare jointly optimized, going beyond the standard video encoding approaches that\nfocus on rate control targeting specific bandwidths. More specifically, we\ncreate a dense video encoding space (offline) and then employ regression to\ngenerate forward prediction models for each one of the afore-described\noptimization objectives, using only Pareto-optimal points. We demonstrate our\nadaptive video encoding approach that leverages the generated forward\nprediction models that qualify for real-time adaptation using different codecs\n(e.g., SVT-AV1 and x265) for a variety of video datasets and resolutions. To\nmotivate our approach and establish the promise for future fast VVC encoders,\nwe also perform a comparative performance evaluation using both subjective and\nobjective metrics and report on bitrate savings among all possible pairs\nbetween VVC, SVT-AV1, x265, and VP9 codecs.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.08191v1"
    },
    {
        "title": "A Decade of Research for Image Compression In Multimedia Laboratory",
        "authors": [
            "Shahrokh Paravarzar",
            "Javaneh Alavi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  With the advancement of technology, we have supercomputers with high\nprocessing power and affordable prices. In addition, using multimedia expanded\nall around the world. This caused a vast use of images and videos in different\nfields. As this kind of data consists of a large amount of information, there\nis a need to use compression methods to store, manage or transfer them better\nand faster. One effective technique, which was introduced is variable\nresolution. This technique stimulates human vision and divides regions in\npictures into two different parts, including the area of interest that needs\nmore detail and periphery parts with less detail. This results in better\ncompression. The variable resolution was used for image, video, and 3D motion\ndata compression. This paper investigates the mentioned technique and some\nother research in this regard.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.09281v1"
    },
    {
        "title": "Robust Watermarking using Diffusion of Logo into Autoencoder Feature\n  Maps",
        "authors": [
            "Maedeh Jamali",
            "Nader Karim",
            "Pejman Khadivi",
            "Shahram Shirani",
            "Shadrokh Samavi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Digital contents have grown dramatically in recent years, leading to\nincreased attention to copyright. Image watermarking has been considered one of\nthe most popular methods for copyright protection. With the recent advancements\nin applying deep neural networks in image processing, these networks have also\nbeen used in image watermarking. Robustness and imperceptibility are two\nchallenging features of watermarking methods that the trade-off between them\nshould be satisfied. In this paper, we propose to use an end-to-end network for\nwatermarking. We use a convolutional neural network (CNN) to control the\nembedding strength based on the image content. Dynamic embedding helps the\nnetwork to have the lowest effect on the visual quality of the watermarked\nimage. Different image processing attacks are simulated as a network layer to\nimprove the robustness of the model. Our method is a blind watermarking\napproach that replicates the watermark string to create a matrix of the same\nsize as the input image. Instead of diffusing the watermark data into the input\nimage, we inject the data into the feature space and force the network to do\nthis in regions that increase the robustness against various attacks.\nExperimental results show the superiority of the proposed method in terms of\nimperceptibility and robustness compared to the state-of-the-art algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.11095v1"
    },
    {
        "title": "Multiview Video Compression Using Advanced HEVC Screen Content Coding",
        "authors": [
            "Jarosław Samelak",
            "Marek Domański"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The paper presents a new approach to multiview video coding using Screen\nContent Coding. It is assumed that for a time instant the frames corresponding\nto all views are packed into a single frame, i.e. the frame-compatible approach\nto multiview coding is applied. For such coding scenario, the paper\ndemonstrates that Screen Content Coding can be efficiently used for multiview\nvideo coding. Two approaches are considered: the first using standard HEVC\nScreen Content Coding, and the second using Advanced Screen Content Coding. The\nlatter is the original proposal of the authors that exploits quarter-pel motion\nvectors and other nonstandard extensions of HEVC Screen Content Coding. The\nexperimental results demonstrate that multiview video coding even using\nstandard HEVC Screen Content Coding is much more efficient than simulcast HEVC\ncoding. The proposed Advanced Screen Content Coding provides virtually the same\ncoding efficiency as MV-HEVC, which is the state-of-the-art multiview video\ncompression technique. The authors suggest that Advanced Screen Content Coding\ncan be efficiently used within the new Versatile Video Coding (VVC) technology.\nNevertheless a reference multiview extension of VVC does not exist yet,\ntherefore, for VVC-based coding, the experimental comparisons are left for\nfuture work.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.13574v1"
    },
    {
        "title": "Parallel and High-Fidelity Text-to-Lip Generation",
        "authors": [
            "Jinglin Liu",
            "Zhiying Zhu",
            "Yi Ren",
            "Wencan Huang",
            "Baoxing Huai",
            "Nicholas Yuan",
            "Zhou Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  As a key component of talking face generation, lip movements generation\ndetermines the naturalness and coherence of the generated talking face video.\nPrior literature mainly focuses on speech-to-lip generation while there is a\npaucity in text-to-lip (T2L) generation. T2L is a challenging task and existing\nend-to-end works depend on the attention mechanism and autoregressive (AR)\ndecoding manner. However, the AR decoding manner generates current lip frame\nconditioned on frames generated previously, which inherently hinders the\ninference speed, and also has a detrimental effect on the quality of generated\nlip frames due to error propagation. This encourages the research of parallel\nT2L generation. In this work, we propose a parallel decoding model for fast and\nhigh-fidelity text-to-lip generation (ParaLip). Specifically, we predict the\nduration of the encoded linguistic features and model the target lip frames\nconditioned on the encoded linguistic features with their duration in a\nnon-autoregressive manner. Furthermore, we incorporate the structural\nsimilarity index loss and adversarial learning to improve perceptual quality of\ngenerated lip frames and alleviate the blurry prediction problem. Extensive\nexperiments conducted on GRID and TCD-TIMIT datasets demonstrate the\nsuperiority of proposed methods. Video samples are available via\n\\url{https://paralip.github.io/}.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.06831v2"
    },
    {
        "title": "Objective video quality metrics application to video codecs comparisons:\n  choosing the best for subjective quality estimation",
        "authors": [
            "Anastasia Antsiferova",
            "Alexander Yakovenko",
            "Nickolay Safonov",
            "Dmitriy Kulikov",
            "Alexander Gushin",
            "Dmitriy Vatolin"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Quality assessment plays a key role in creating and comparing video\ncompression algorithms. Despite the development of a large number of new\nmethods for assessing quality, generally accepted and well-known codecs\ncomparisons mainly use the classical methods like PSNR, SSIM and new method\nVMAF. These methods can be calculated following different rules: they can use\ndifferent frame-by-frame averaging techniques or different summation of color\ncomponents. In this paper, a fundamental comparison of various versions of\ngenerally accepted metrics is carried out to find the most relevant and\nrecommended versions of video quality metrics to be used in codecs comparisons.\nFor comparison, we used a set of videos encoded with video codecs of different\nstandards, and visual quality scores collected for the resulting set of streams\nsince 2018 until 2021\n",
        "pdf_link": "http://arxiv.org/pdf/2107.10220v1"
    },
    {
        "title": "A Point-to-Distribution Joint Geometry and Color Metric for Point Cloud\n  Quality Assessment",
        "authors": [
            "Alireza Javaheri",
            "Catarina Brites",
            "Fernando Pereira",
            "João Ascenso"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Point clouds (PCs) are a powerful 3D visual representation paradigm for many\nemerging application domains, especially virtual and augmented reality, and\nautonomous vehicles. However, the large amount of PC data required for highly\nimmersive and realistic experiences requires the availability of efficient,\nlossy PC coding solutions are critical. Recently, two MPEG PC coding standards\nhave been developed to address the relevant application requirements and\nfurther developments are expected in the future. In this context, the\nassessment of PC quality, notably for decoded PCs, is critical and asks for the\ndesign of efficient objective PC quality metrics. In this paper, a novel\npoint-to-distribution metric is proposed for PC quality assessment considering\nboth the geometry and texture. This new quality metric exploits the\nscale-invariance property of the Mahalanobis distance to assess first the\ngeometry and color point-to-distribution distortions, which are after fused to\nobtain a joint geometry and color quality metric. The proposed quality metric\nsignificantly outperforms the best PC quality assessment metrics in the\nliterature.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.00054v1"
    },
    {
        "title": "Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with\n  Generative Adversarial Affective Expression Learning",
        "authors": [
            "Uttaran Bhattacharya",
            "Elizabeth Childs",
            "Nicholas Rewkowski",
            "Dinesh Manocha"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We present a generative adversarial network to synthesize 3D pose sequences\nof co-speech upper-body gestures with appropriate affective expressions. Our\nnetwork consists of two components: a generator to synthesize gestures from a\njoint embedding space of features encoded from the input speech and the seed\nposes, and a discriminator to distinguish between the synthesized pose\nsequences and real 3D pose sequences. We leverage the Mel-frequency cepstral\ncoefficients and the text transcript computed from the input speech in separate\nencoders in our generator to learn the desired sentiments and the associated\naffective cues. We design an affective encoder using multi-scale\nspatial-temporal graph convolutions to transform 3D pose sequences into latent,\npose-based affective features. We use our affective encoder in both our\ngenerator, where it learns affective features from the seed poses to guide the\ngesture synthesis, and our discriminator, where it enforces the synthesized\ngestures to contain the appropriate affective expressions. We perform extensive\nevaluations on two benchmark datasets for gesture synthesis from the speech,\nthe TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the\nbest baselines, we improve the mean absolute joint error by 10--33%, the mean\nacceleration difference by 8--58%, and the Fr\\'echet Gesture Distance by\n21--34%. We also conduct a user study and observe that compared to the best\ncurrent baselines, around 15.28% of participants indicated our synthesized\ngestures appear more plausible, and around 16.32% of participants felt the\ngestures had more appropriate affective expressions aligned with the speech.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.00262v3"
    },
    {
        "title": "Fighting Game Commentator with Pitch and Loudness Adjustment Utilizing\n  Highlight Cues",
        "authors": [
            "Junjie H. Xu",
            "Zhou Fang",
            "Qihang Chen",
            "Satoru Ohno",
            "Pujana Paliyawan"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  This paper presents a commentator for providing real-time game commentary in\na fighting game. The commentary takes into account highlight cues, obtained by\nanalyzing scenes during gameplay, as input to adjust the pitch and loudness of\ncommentary to be spoken by using a Text-to-Speech (TTS) technology. We\ninvestigate different designs for pitch and loudness adjustment. The proposed\nAI consists of two parts: a dynamic adjuster for controlling pitch and loudness\nof the TTS and a real-time game commentary generator. We conduct a pilot study\non a fighting game, and our result shows that by adjusting the loudness\nsignificantly according to the level of game highlight, the entertainment of\nthe gameplay can be enhanced.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.08112v1"
    },
    {
        "title": "Online Enhanced Semantic Hashing: Towards Effective and Efficient\n  Retrieval for Streaming Multi-Modal Data",
        "authors": [
            "Xiao-Ming Wu",
            "Xin Luo",
            "Yu-Wei Zhan",
            "Chen-Lu Ding",
            "Zhen-Duo Chen",
            "Xin-Shun Xu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  With the vigorous development of multimedia equipment and applications,\nefficient retrieval of large-scale multi-modal data has become a trendy\nresearch topic. Thereinto, hashing has become a prevalent choice due to its\nretrieval efficiency and low storage cost. Although multi-modal hashing has\ndrawn lots of attention in recent years, there still remain some problems. The\nfirst point is that existing methods are mainly designed in batch mode and not\nable to efficiently handle streaming multi-modal data. The second point is that\nall existing online multi-modal hashing methods fail to effectively handle\nunseen new classes which come continuously with streaming data chunks. In this\npaper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS).\nWe design novel semantic-enhanced representation for data, which could help\nhandle the new coming classes, and thereby construct the enhanced semantic\nobjective function. An efficient and effective discrete online optimization\nalgorithm is further proposed for OASIS. Extensive experiments show that our\nmethod can exceed the state-of-the-art models. For good reproducibility and\nbenefiting the community, our code and data are already available in\nsupplementary material and will be made publicly available.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04260v2"
    },
    {
        "title": "MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their\n  Targets",
        "authors": [
            "Shraman Pramanick",
            "Shivam Sharma",
            "Dimitar Dimitrov",
            "Md Shad Akhtar",
            "Preslav Nakov",
            "Tanmoy Chakraborty"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Internet memes have become powerful means to transmit political,\npsychological, and socio-cultural ideas. Although memes are typically humorous,\nrecent days have witnessed an escalation of harmful memes used for trolling,\ncyberbullying, and abuse. Detecting such memes is challenging as they can be\nhighly satirical and cryptic. Moreover, while previous work has focused on\nspecific aspects of memes such as hate speech and propaganda, there has been\nlittle work on harm in general. Here, we aim to bridge this gap. We focus on\ntwo tasks: (i)detecting harmful memes, and (ii)identifying the social entities\nthey target. We further extend a recently released HarMeme dataset, which\ncovered COVID-19, with additional memes and a new topic: US politics. To solve\nthese tasks, we propose MOMENTA (MultimOdal framework for detecting harmful\nMemEs aNd Their tArgets), a novel multimodal deep neural network that uses\nglobal and local perspectives to detect harmful memes. MOMENTA systematically\nanalyzes the local and the global perspective of the input meme (in both\nmodalities) and relates it to the background context. MOMENTA is interpretable\nand generalizable, and our experiments show that it outperforms several strong\nrivaling approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.05184v2"
    },
    {
        "title": "CONQUER: Contextual Query-aware Ranking for Video Corpus Moment\n  Retrieval",
        "authors": [
            "Zhijian Hou",
            "Chong-Wah Ngo",
            "Wing Kwong Chan"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  This paper tackles a recently proposed Video Corpus Moment Retrieval task.\nThis task is essential because advanced video retrieval applications should\nenable users to retrieve a precise moment from a large video corpus. We propose\na novel CONtextual QUery-awarE Ranking~(CONQUER) model for effective moment\nlocalization and ranking. CONQUER explores query context for multi-modal fusion\nand representation learning in two different steps. The first step derives\nfusion weights for the adaptive combination of multi-modal video content. The\nsecond step performs bi-directional attention to tightly couple video and query\nas a single joint representation for moment localization. As query context is\nfully engaged in video representation learning, from feature fusion to\ntransformation, the resulting feature is user-centered and has a larger\ncapacity in capturing multi-modal signals specific to query. We conduct studies\non two datasets, TVR for closed-world TV episodes and DiDeMo for open-world\nuser-generated videos, to investigate the potential advantages of fusing video\nand query online as a joint representation for moment retrieval.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.10016v1"
    },
    {
        "title": "A Deep Learning-based Audio-in-Image Watermarking Scheme",
        "authors": [
            "Arjon Das",
            "Xin Zhong"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  This paper presents a deep learning-based audio-in-image watermarking scheme.\nAudio-in-image watermarking is the process of covertly embedding and extracting\naudio watermarks on a cover-image. Using audio watermarks can open up\npossibilities for different downstream applications. For the purpose of\nimplementing an audio-in-image watermarking that adapts to the demands of\nincreasingly diverse situations, a neural network architecture is designed to\nautomatically learn the watermarking process in an unsupervised manner. In\naddition, a similarity network is developed to recognize the audio watermarks\nunder distortions, therefore providing robustness to the proposed method.\nExperimental results have shown high fidelity and robustness of the proposed\nblind audio-in-image watermarking scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.02436v1"
    },
    {
        "title": "TranSalNet: Towards perceptually relevant visual saliency prediction",
        "authors": [
            "Jianxun Lou",
            "Hanhe Lin",
            "David Marshall",
            "Dietmar Saupe",
            "Hantao Liu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Visual saliency prediction using transformers - Convolutional neural networks\n(CNNs) have significantly advanced computational modelling for saliency\nprediction. However, accurately simulating the mechanisms of visual attention\nin the human cortex remains an academic challenge. It is critical to integrate\nproperties of human vision into the design of CNN architectures, leading to\nperceptually more relevant saliency prediction. Due to the inherent inductive\nbiases of CNN architectures, there is a lack of sufficient long-range\ncontextual encoding capacity. This hinders CNN-based saliency models from\ncapturing properties that emulate viewing behaviour of humans. Transformers\nhave shown great potential in encoding long-range information by leveraging the\nself-attention mechanism. In this paper, we propose a novel saliency model that\nintegrates transformer components to CNNs to capture the long-range contextual\nvisual information. Experimental results show that the transformers provide\nadded value to saliency prediction, enhancing its perceptual relevance in the\nperformance. Our proposed saliency model using transformers has achieved\nsuperior results on public benchmarks and competitions for saliency prediction\nmodels.\n  The source code of our proposed saliency model TranSalNet is available at:\nhttps://github.com/LJOVO/TranSalNet\n",
        "pdf_link": "http://arxiv.org/pdf/2110.03593v3"
    },
    {
        "title": "Real-time FPGA Design for OMP Targeting 8K Image Reconstruction",
        "authors": [
            "Jiayao Xu",
            "Chen Fu",
            "Zhiqiang Zhang",
            "Jinjia Zhou"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  During the past decade, implementing reconstruction algorithms on hardware\nhas been at the center of much attention in the field of real-time\nreconstruction in Compressed Sensing (CS). Orthogonal Matching Pursuit (OMP) is\nthe most widely used reconstruction algorithm on hardware implementation\nbecause OMP obtains good quality reconstruction results under a proper time\ncost. OMP includes Dot Product (DP) and Least Square Problem (LSP). These two\nparts have numerous division calculations and considerable vector-based\nmultiplications, which limit the implementation of real-time reconstruction on\nhardware. In the theory of CS, besides the reconstruction algorithm, the choice\nof sensing matrix affects the quality of reconstruction. It also influences the\nreconstruction efficiency by affecting the hardware architecture. Thus,\ndesigning a real-time hardware architecture of OMP needs to take three factors\ninto consideration. The choice of sensing matrix, the implementation of DP and\nLSP. In this paper, a sensing matrix, which is sparsity and contains zero\nvectors mainly, is adopted to optimize the OMP reconstruction to break the\nbottleneck of reconstruction efficiency. Based on the features of the chosen\nmatrix, the DP and LSP are implemented by simple shift, add and comparing\nprocedures. This work is implemented on the Xilinx Virtex UltraScale+ FPGA\ndevice. To reconstruct a digital signal with 1024 length under 0.25 sampling\nrate, the proposal method costs 0.818us while the state-of-the-art costs\n238$us. Thus, this work speedups the state-of-the-art method 290 times. This\nwork costs 0.026s to reconstruct an 8K gray image, which achieves 30FPS\nreal-time reconstruction.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.04714v1"
    },
    {
        "title": "Delay-Sensitive and Power-Efficient Quality Control of Dynamic Video\n  Streaming using Adaptive Super-Resolution",
        "authors": [
            "Minseok Choi",
            "Won Joon Yun",
            "Joongheon Kim"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In a decade, the adaptive quality control of video streaming and the\nsuper-resolution (SR) technique have been deeply explored. As edge devices\nimproved to have exceptional processing capability than ever before, streaming\nusers can enhance the received image quality to allow the transmitter to\ncompress the images to save its power or pursue network efficiency. In this\nsense, this paper proposes a novel dynamic video streaming algorithm that\nadaptively compresses video chunks at the transmitter and separately enhances\nthe quality at the receiver using SR. In order to allow transmission of video\nchunks with different compression levels and control of the computation burden,\nwe present the adaptive SR network which is optimized by minimizing the\nweighted sum of losses extracted from different layer outputs. for dynamic\nvideo streaming. In addition, we jointly orchestrate video delivery and\nresource usage, and the proposed video delivery scheme balances the tradeoff\nwell among the average video quality, the queuing delay, buffering time,\ntransmit power, and computation power. Simulation results show that the\nproposed scheme pursues the quality-of-services (QoS) of the video streaming\nbetter than the adaptive quality control without the cooperation of the\ntransmitter and the receiver and the non-adaptive SR network.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.05783v1"
    },
    {
        "title": "Power Consumption of Video-Decoders on Various Android Devices",
        "authors": [
            "Roman Kazantsev",
            "Dmitriy Vatolin"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The critical constraint of mobile devices is a limited battery life that is\nsignificantly reduced during video playback. The power efficiency of video\nplayback mainly depends on the used compression standard, video-decoder, and\ndevice model. We propose a software-based method to estimate the power\nconsumption of video-decoders on various Android devices. Experiments on two\ndevices of the same model show a small variation of the power playback\nconsumption and a lack of dependence between the power consumption and the\nbattery level. We have implemented an automatic system that includes the VEQE\nAndroid application to measure the power consumption of decoders and a server\nto collect the power metrics. Our system has collected power-consumption and\ndecoding-speed dataset for video-decoders of six standards (AV1, HEVC, VP9,\nH.264, VP8, and MPEG-4) operating on 285 devices, representing 147 models. We\ndemonstrate some slices of the created dataset: the top 30 models and\nvideo-decoders in terms of power efficiency for playback and for decoding only,\nas well as video-decoder ratings by power consumption and decoding speed for a\ngiven device model.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.06529v1"
    },
    {
        "title": "A QoE Model in Point Cloud Video Streaming",
        "authors": [
            "Jie Li",
            "Xiao Wang",
            "Zhi Liu",
            "Qiyue Li"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Point cloud video has been widely used by augmented reality (AR) and virtual\nreality (VR) applications as it allows users to have an immersive experience of\nsix degrees of freedom (6DoFs). Yet there is still a lack of research on\nquality of experience (QoE) model of point cloud video streaming, which cannot\nprovide optimization metric for streaming systems. Besides, position and color\ninformation contained in each pixel of point cloud video, and viewport distance\neffect caused by 6DoFs viewing procedure make the traditional objective quality\nevaluation metric cannot be directly used in point cloud video streaming\nsystem. In this paper we first analyze the subjective and objective factors\nrelated to QoE model. Then an experimental system to simulate point cloud video\nstreaming is setup and detailed subjective quality evaluation experiments are\ncarried out. Based on collected mean opinion score (MOS) data, we propose a QoE\nmodel for point cloud video streaming. We also verify the model by actual\nsubjective scoring, and the results show that the proposed QoE model can\naccurately reflect users' visual perception. We also make the experimental\ndatabase public to promote the QoE research of point cloud video streaming.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.02985v4"
    },
    {
        "title": "Adaptive Steganography Based on bargain Game",
        "authors": [
            "Behbod Keshavarzi",
            "Hamidreza Navidi",
            "Parvaneh Asghari",
            "Seyyed Hamid Haji Seyyed Javadi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The capacity and security of the confidential message on the channel are two\nimportant challenges in steganography. In this paper, a new block steganography\nmodel is presented using the bargain method so that a competitive model is\nintroduced. In this game, the blocks are the same players. The bargain is\nprovided with the aim of embedding information without reducing capacity as\nwell as increasing security. The proposed model shows that it can be used both\nof the special domain and the transform domain, which are two important methods\nof steganography. For this purpose, an example of a special domain model is\nintroduced in which, In the first step, the image is divided into $n \\times n$\nblocks, and in the second step using the graph coloring algorithm, pixels are\nconsidered to embed confidential information in each block. In the third step,\nregarding the bargaining method in game theory, each block plays the role of a\nplayer, that the competition between players is based on the defined goal\nfunction, and in the best blocks in terms of two criteria of capacity and\nsecurity, which here means each block has a higher security-to-capacity ratio,\nso it has a higher priority, which is determined based on the bargaining model.\nAlso, information embedded in LSB two bits. An example of a conversion domain\nmethod is also shows that security increases without decreasing in capacity.\nThe conclusion is evaluated by three criteria: PSNR, histogram, and\n$\\epsilon-secure$ also, 2000 standard images were evaluated and observed that\nthe proposed method improves the block methods of embedding information.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.04653v2"
    },
    {
        "title": "Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video\n  Retrieval",
        "authors": [
            "Fan Hu",
            "Aozhu Chen",
            "Ziyue Wang",
            "Fangming Zhou",
            "Jianfeng Dong",
            "Xirong Li"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In this paper we revisit feature fusion, an old-fashioned topic, in the new\ncontext of text-to-video retrieval. Different from previous research that\nconsiders feature fusion only at one end, let it be video or text, we aim for\nfeature fusion for both ends within a unified framework. We hypothesize that\noptimizing the convex combination of the features is preferred to modeling\ntheir correlations by computationally heavy multi-head self attention. We\npropose Lightweight Attentional Feature Fusion (LAFF). LAFF performs feature\nfusion at both early and late stages and at both video and text ends, making it\na powerful method for exploiting diverse (off-the-shelf) features. The\ninterpretability of LAFF can be used for feature selection. Extensive\nexperiments on five public benchmark sets (MSR-VTT, MSVD, TGIF, VATEX and\nTRECVID AVS 2016-2020) justify LAFF as a new baseline for text-to-video\nretrieval.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01832v3"
    },
    {
        "title": "Malakai: Music That Adapts to the Shape of Emotions",
        "authors": [
            "Zack Harris",
            "Liam Atticus Clarke",
            "Pietro Gagliano",
            "Dante Camarena",
            "Manal Siddiqui",
            "Pablo S. Castro"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The advent of ML music models such as Google Magenta's MusicVAE now allow us\nto extract and replicate compositional features from otherwise complex\ndatasets. These models allow computational composers to parameterize abstract\nvariables such as style and mood. By leveraging these models and combining them\nwith procedural algorithms from the last few decades, it is possible to create\na dynamic song that composes music in real-time to accompany interactive\nexperiences. Malakai is a tool that helps users of varying skill levels create,\nlisten to, remix and share such dynamic songs. Using Malakai, a Composer can\ncreate a dynamic song that can be interacted with by a Listener\n",
        "pdf_link": "http://arxiv.org/pdf/2112.02070v1"
    },
    {
        "title": "MoCA: Incorporating Multi-stage Domain Pretraining and Cross-guided\n  Multimodal Attention for Textbook Question Answering",
        "authors": [
            "Fangzhi Xu",
            "Qika Lin",
            "Jun Liu",
            "Lingling Zhang",
            "Tianzhe Zhao",
            "Qi Chai",
            "Yudai Pan"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Textbook Question Answering (TQA) is a complex multimodal task to infer\nanswers given large context descriptions and abundant diagrams. Compared with\nVisual Question Answering (VQA), TQA contains a large number of uncommon\nterminologies and various diagram inputs. It brings new challenges to the\nrepresentation capability of language model for domain-specific spans. And it\nalso pushes the multimodal fusion to a more complex level. To tackle the above\nissues, we propose a novel model named MoCA, which incorporates multi-stage\ndomain pretraining and multimodal cross attention for the TQA task. Firstly, we\nintroduce a multi-stage domain pretraining module to conduct unsupervised\npost-pretraining with the span mask strategy and supervised pre-finetune.\nEspecially for domain post-pretraining, we propose a heuristic generation\nalgorithm to employ the terminology corpus. Secondly, to fully consider the\nrich inputs of context and diagrams, we propose cross-guided multimodal\nattention to update the features of text, question diagram and instructional\ndiagram based on a progressive strategy. Further, a dual gating mechanism is\nadopted to improve the model ensemble. The experimental results show the\nsuperiority of our model, which outperforms the state-of-the-art methods by\n2.21% and 2.43% for validation and test split respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.02839v1"
    },
    {
        "title": "RFGAN: RF-Based Human Synthesis",
        "authors": [
            "Cong Yu",
            "Zhi Wu",
            "Dongheng Zhang",
            "Zhi Lu",
            "Yang Hu",
            "Yan Chen"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  This paper demonstrates human synthesis based on the Radio Frequency (RF)\nsignals, which leverages the fact that RF signals can record human movements\nwith the signal reflections off the human body. Different from existing RF\nsensing works that can only perceive humans roughly, this paper aims to\ngenerate fine-grained optical human images by introducing a novel cross-modal\nRFGAN model. Specifically, we first build a radio system equipped with\nhorizontal and vertical antenna arrays to transceive RF signals. Since the\nreflected RF signals are processed as obscure signal projection heatmaps on the\nhorizontal and vertical planes, we design a RF-Extractor with RNN in RFGAN for\nRF heatmap encoding and combining to obtain the human activity information.\nThen we inject the information extracted by the RF-Extractor and RNN as the\ncondition into GAN using the proposed RF-based adaptive normalizations.\nFinally, we train the whole model in an end-to-end manner. To evaluate our\nproposed model, we create two cross-modal datasets (RF-Walk & RF-Activity) that\ncontain thousands of optical human activity frames and corresponding RF\nsignals. Experimental results show that the RFGAN can generate target human\nactivity frames using RF signals. To the best of our knowledge, this is the\nfirst work to generate optical images based on RF signals.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.03727v1"
    },
    {
        "title": "A Multi-user Oriented Live Free-viewpoint Video Streaming System Based\n  On View Interpolation",
        "authors": [
            "Jingchuan Hu",
            "Shuai Guo",
            "Kai Zhou",
            "Yu Dong",
            "Jun Xu",
            "Li Song"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  As an important application form of immersive multimedia services,\nfree-viewpoint video(FVV) enables users with great immersive experience by\nstrong interaction. However, the computational complexity of virtual view\nsynthesis algorithms poses a significant challenge to the real-time performance\nof an FVV system. Furthermore, the individuality of user interaction makes it\ndifficult to serve multiple users simultaneously for a system with conventional\narchitecture. In this paper, we novelly introduce a CNN-based view\ninterpolation algorithm to synthesis dense virtual views in real time. Based on\nthis, we also build an end-to-end live free-viewpoint system with a multi-user\noriented streaming strategy. Our system can utilize a single edge server to\nserve multiple users at the same time without having to bring a large view\nsynthesis load on the client side. We analyze the whole system and show that\nour approaches give the user a pleasant immersive experience, in terms of both\nvisual quality and latency.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.10603v2"
    },
    {
        "title": "A Survey on Perceptually Optimized Video Coding",
        "authors": [
            "Yun Zhang",
            "Linwei Zhu",
            "Gangyi Jiang",
            "Sam Kwong",
            "C. -C. Jay Kuo"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  To provide users with more realistic visual experiences, videos are\ndeveloping in the trends of Ultra High Definition (UHD), High Frame Rate (HFR),\nHigh Dynamic Range (HDR), Wide Color Gammut (WCG) and high clarity. However,\nthe data amount of videos increases exponentially, which requires high\nefficiency video compression for storage and network transmission. Perceptually\noptimized video coding aims to maximize compression efficiency by exploiting\nvisual redundancies. In this paper, we present a broad and systematic survey on\nperceptually optimized video coding. Firstly, we present problem formulation\nand framework of the perceptually optimized video coding, which includes visual\nperception modelling, visual quality assessment and perceptual video coding\noptimization. Secondly, recent advances on visual factors, computational\nperceptual models and quality assessment models are presented. Thirdly, we\nreview perceptual video coding optimizations from four key aspects, including\nperceptually optimized bit allocation, rate-distortion optimization, transform\nand quantization, filtering and enhancement. In each part, problem formulation,\nworking flow, recent advances, advantages and challenges are presented.\nFourthly, perceptual coding performances of the latest coding standards and\ntools are experimentally analyzed. Finally, challenging issues and future\nopportunities are identified.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.12284v2"
    },
    {
        "title": "Centre Symmetric Quadruple Pattern: A Novel Descriptor for Facial Image\n  Recognition and Retrieval",
        "authors": [
            "Soumendu Chakraborty",
            "Satish Kumar Singh",
            "Pavan Chakraborty"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Facial features are defined as the local relationships that exist amongst the\npixels of a facial image. Hand-crafted descriptors identify the relationships\nof the pixels in the local neighbourhood defined by the kernel. Kernel is a two\ndimensional matrix which is moved across the facial image. Distinctive\ninformation captured by the kernel with limited number of pixel achieves\nsatisfactory recognition and retrieval accuracies on facial images taken under\nconstrained environment (controlled variations in light, pose, expressions, and\nbackground). To achieve similar accuracies under unconstrained environment\nlocal neighbourhood has to be increased, in order to encode more pixels.\nIncreasing local neighbourhood also increases the feature length of the\ndescriptor. In this paper we propose a hand-crafted descriptor namely Centre\nSymmetric Quadruple Pattern (CSQP), which is structurally symmetric and encodes\nthe facial asymmetry in quadruple space. The proposed descriptor efficiently\nencodes larger neighbourhood with optimal number of binary bits. It has been\nshown using average entropy, computed over feature images encoded with the\nproposed descriptor, that the CSQP captures more meaningful information as\ncompared to state of the art descriptors. The retrieval and recognition\naccuracies of the proposed descriptor has been compared with state of the art\nhand-crafted descriptors (CSLBP, CSLTP, LDP, LBP, SLBP and LDGP) on bench mark\ndatabases namely; LFW, Colour-FERET, and CASIA-face-v5. Result analysis shows\nthat the proposed descriptor performs well under controlled as well as\nuncontrolled variations in pose, illumination, background and expressions.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.00511v1"
    },
    {
        "title": "ECAS-ML: Edge Computing Assisted Adaptation Scheme with Machine Learning\n  for HTTP Adaptive Streaming",
        "authors": [
            "Jesús Aguilar-Armijo",
            "Ekrem Çetinkaya",
            "Christian Timmerer",
            "Hermann Hellwagner"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  As the video streaming traffic in mobile networks is increasing, improving\nthe content delivery process becomes crucial, e.g., by utilizing edge computing\nsupport. At an edge node, we can deploy adaptive bitrate (ABR) algorithms with\na better understanding of network behavior and access to radio and player\nmetrics. In this work, we present ECAS-ML, Edge Assisted Adaptation Scheme for\nHTTP Adaptive Streaming with Machine Learning. ECAS-ML focuses on managing the\ntradeoff among bitrate, segment switches, and stalls to achieve a higher\nquality of experience (QoE). For that purpose, we use machine learning\ntechniques to analyze radio throughput traces and predict the best parameters\nof our algorithm to achieve better performance. The results show that ECAS-ML\noutperforms other client-based and edge-based ABR algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.04488v1"
    },
    {
        "title": "Nebula: Reliable Low-latency Video Transmission for Mobile Cloud Gaming",
        "authors": [
            "Ahmad Alhilal",
            "Tristan Braud",
            "Bo Han",
            "Pan Hui"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Mobile cloud gaming enables high-end games on constrained devices by\nstreaming the game content from powerful servers through mobile networks.\nMobile networks suffer from highly variable bandwidth, latency, and losses that\naffect the gaming experience. This paper introduces Nebula, an end-to-end cloud\ngaming framework to minimize the impact of network conditions on the user\nexperience. Nebula relies on an end-to-end distortion model adapting the video\nsource rate and the amount of frame-level redundancy based on the measured\nnetwork conditions. As a result, it minimizes the motion-to-photon (MTP)\nlatency while protecting the frames from losses. We fully implement Nebula and\nevaluate its performance against the state of the art techniques and latest\nresearch in real-time mobile cloud gaming transmission on a physical testbed\nover emulated and real wireless networks. Nebula consistently balances MTP\nlatency (<140 ms) and visual quality (>31 dB) even in highly variable\nenvironments. A user experiment confirms that Nebula maximizes the user\nexperience with high perceived video quality, playability, and low user load.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.07738v1"
    },
    {
        "title": "Do You See What I See? Capabilities and Limits of Automated Multimedia\n  Content Analysis",
        "authors": [
            "Carey Shenkman",
            "Dhanaraj Thakur",
            "Emma Llansó"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The ever-increasing amount of user-generated content online has led, in\nrecent years, to an expansion in research and investment in automated content\nanalysis tools. Scrutiny of automated content analysis has accelerated during\nthe COVID-19 pandemic, as social networking services have placed a greater\nreliance on these tools due to concerns about health risks to their moderation\nstaff from in-person work. At the same time, there are important policy debates\naround the world about how to improve content moderation while protecting free\nexpression and privacy. In order to advance these debates, we need to\nunderstand the potential role of automated content analysis tools.\n  This paper explains the capabilities and limitations of tools for analyzing\nonline multimedia content and highlights the potential risks of using these\ntools at scale without accounting for their limitations. It focuses on two main\ncategories of tools: matching models and computer prediction models. Matching\nmodels include cryptographic and perceptual hashing, which compare\nuser-generated content with existing and known content. Predictive models\n(including computer vision and computer audition) are machine learning\ntechniques that aim to identify characteristics of new or previously unknown\ncontent.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.11105v1"
    },
    {
        "title": "Banding vs. Quality: Perceptual Impact and Objective Assessment",
        "authors": [
            "Lukáš Krasula",
            "Zhi Li",
            "Christos G. Bampis",
            "Mariana Afonso",
            "Nil Fons Miret",
            "Joel Sole"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Staircase-like contours introduced to a video by quantization in flat areas,\ncommonly known as banding, have been a long-standing problem in both video\nprocessing and quality assessment communities. The fact that even a relatively\nsmall change of the original pixel values can result in a strong impact on\nperceived quality makes banding especially difficult to be detected by\nobjective quality metrics. In this paper, we study how banding annoyance\ncompares to more commonly studied scaling and compression artifacts with\nrespect to the overall perceptual quality. We further propose a simple\ncombination of VMAF and the recently developed banding index, CAMBI, into a\nbanding-aware video quality metric showing improved correlation with overall\nperceived quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.11038v1"
    },
    {
        "title": "GAME-ON: Graph Attention Network based Multimodal Fusion for Fake News\n  Detection",
        "authors": [
            "Mudit Dhawan",
            "Shakshi Sharma",
            "Aditya Kadam",
            "Rajesh Sharma",
            "Ponnurangam Kumaraguru"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Social media in present times has a significant and growing influence. Fake\nnews being spread on these platforms have a disruptive and damaging impact on\nour lives. Furthermore, as multimedia content improves the visibility of posts\nmore than text data, it has been observed that often multimedia is being used\nfor creating fake content. A plethora of previous multimodal-based work has\ntried to address the problem of modeling heterogeneous modalities in\nidentifying fake content. However, these works have the following limitations:\n(1) inefficient encoding of inter-modal relations by utilizing a simple\nconcatenation operator on the modalities at a later stage in a model, which\nmight result in information loss; (2) training very deep neural networks with a\ndisproportionate number of parameters on small but complex real-life multimodal\ndatasets result in higher chances of overfitting. To address these limitations,\nwe propose GAME-ON, a Graph Neural Network based end-to-end trainable framework\nthat allows granular interactions within and across different modalities to\nlearn more robust data representations for multimodal fake news detection. We\nuse two publicly available fake news datasets, Twitter and Weibo, for\nevaluations. Our model outperforms on Twitter by an average of 11% and keeps\ncompetitive performance on Weibo, within a 2.6% margin, while using 65% fewer\nparameters than the best comparable state-of-the-art baseline.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12478v3"
    },
    {
        "title": "A Brief Survey on Adaptive Video Streaming Quality Assessment",
        "authors": [
            "Wei Zhou",
            "Xiongkuo Min",
            "Hong Li",
            "Qiuping Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Quality of experience (QoE) assessment for adaptive video streaming plays a\nsignificant role in advanced network management systems. It is especially\nchallenging in case of dynamic adaptive streaming schemes over HTTP (DASH)\nwhich has increasingly complex characteristics including additional playback\nissues. In this paper, we provide a brief overview of adaptive video streaming\nquality assessment. Upon our review of related works, we analyze and compare\ndifferent variations of objective QoE assessment models with or without using\nmachine learning techniques for adaptive video streaming. Through the\nperformance analysis, we observe that hybrid models perform better than both\nquality-of-service (QoS) driven QoE approaches and signal fidelity measurement.\nMoreover, the machine learning-based model slightly outperforms the model\nwithout using machine learning for the same setting. In addition, we find that\nexisting video streaming QoE assessment models still have limited performance,\nwhich makes it difficult to be applied in practical communication systems.\nTherefore, based on the success of deep learned feature representations for\ntraditional video quality prediction, we also apply the off-the-shelf deep\nconvolutional neural network (DCNN) to evaluate the perceptual quality of\nstreaming videos, where the spatio-temporal properties of streaming videos are\ntaken into consideration. Experiments demonstrate its superiority, which sheds\nlight on the future development of specifically designed deep learning\nframeworks for adaptive video streaming quality assessment. We believe this\nsurvey can serve as a guideline for QoE assessment of adaptive video streaming.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12987v1"
    },
    {
        "title": "Two-stream Hierarchical Similarity Reasoning for Image-text Matching",
        "authors": [
            "Ran Chen",
            "Hanli Wang",
            "Lei Wang",
            "Sam Kwong"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Reasoning-based approaches have demonstrated their powerful ability for the\ntask of image-text matching. In this work, two issues are addressed for\nimage-text matching. First, for reasoning processing, conventional approaches\nhave no ability to find and use multi-level hierarchical similarity\ninformation. To solve this problem, a hierarchical similarity reasoning module\nis proposed to automatically extract context information, which is then\nco-exploited with local interaction information for efficient reasoning.\nSecond, previous approaches only consider learning single-stream similarity\nalignment (i.e., image-to-text level or text-to-image level), which is\ninadequate to fully use similarity information for image-text matching. To\naddress this issue, a two-stream architecture is developed to decompose\nimage-text matching into image-to-text level and text-to-image level similarity\ncomputation. These two issues are investigated by a unifying framework that is\ntrained in an end-to-end manner, namely two-stream hierarchical similarity\nreasoning network. The extensive experiments performed on the two benchmark\ndatasets of MSCOCO and Flickr30K show the superiority of the proposed approach\nas compared to existing state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.05349v1"
    },
    {
        "title": "Affective Feedback Synthesis Towards Multimodal Text and Image Data",
        "authors": [
            "Puneet Kumar",
            "Gaurav Bhat",
            "Omkar Ingle",
            "Daksh Goyal",
            "Balasubramanian Raman"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this paper, we have defined a novel task of affective feedback synthesis\nthat deals with generating feedback for input text & corresponding image in a\nsimilar way as humans respond towards the multimodal data. A feedback synthesis\nsystem has been proposed and trained using ground-truth human comments along\nwith image-text input. We have also constructed a large-scale dataset\nconsisting of image, text, Twitter user comments, and the number of likes for\nthe comments by crawling the news articles through Twitter feeds. The proposed\nsystem extracts textual features using a transformer-based textual encoder\nwhile the visual features have been extracted using a Faster region-based\nconvolutional neural networks model. The textual and visual features have been\nconcatenated to construct the multimodal features using which the decoder\nsynthesizes the feedback. We have compared the results of the proposed system\nwith the baseline models using quantitative and qualitative measures. The\ngenerated feedbacks have been analyzed using automatic and human evaluation.\nThey have been found to be semantically similar to the ground-truth comments\nand relevant to the given text-image input.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12692v2"
    },
    {
        "title": "Continuous Emotion Recognition using Visual-audio-linguistic\n  information: A Technical Report for ABAW3",
        "authors": [
            "Su Zhang",
            "Ruyi An",
            "Yi Ding",
            "Cuntai Guan"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  We propose a cross-modal co-attention model for continuous emotion\nrecognition using visual-audio-linguistic information. The model consists of\nfour blocks. The visual, audio, and linguistic blocks are used to learn the\nspatial-temporal features of the multi-modal input. A co-attention block is\ndesigned to fuse the learned features with the multi-head co-attention\nmechanism. The visual encoding from the visual block is concatenated with the\nattention feature to emphasize the visual information. To make full use of the\ndata and alleviate over-fitting, cross-validation is carried out on the\ntraining and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. The achieved CCC on the\ntest set is $0.520$ for valence and $0.602$ for arousal, which significantly\noutperforms the baseline method with the corresponding CCC of 0.180 and 0.170\nfor valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW3.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.13031v2"
    },
    {
        "title": "Learn to Understand Negation in Video Retrieval",
        "authors": [
            "Ziyue Wang",
            "Aozhu Chen",
            "Fan Hu",
            "Xirong Li"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Negation is a common linguistic skill that allows human to express what we do\nNOT want. Naturally, one might expect video retrieval to support\nnatural-language queries with negation, e.g., finding shots of kids sitting on\nthe floor and not playing with a dog. However, the state-of-the-art deep\nlearning based video retrieval models lack such ability, as they are typically\ntrained on video description datasets such as MSR-VTT and VATEX that lack\nnegated descriptions. Their retrieved results basically ignore the negator in\nthe sample query, incorrectly returning videos showing kids playing with dog.\nThis paper presents the first study on learning to understand negation in video\nretrieval and make contributions as follows. By re-purposing two existing\ndatasets (MSR-VTT and VATEX), we propose a new evaluation protocol for video\nretrieval with negation. We propose a learning based method for training a\nnegation-aware video retrieval model. The key idea is to first construct a soft\nnegative caption for a specific training video by partially negating its\noriginal caption, and then compute a bidirectionally constrained loss on the\ntriplet. This auxiliary loss is weightedly added to a standard retrieval loss.\nExperiments on the re-purposed benchmarks show that re-training the CLIP\n(Contrastive Language-Image Pre-Training) model by the proposed method clearly\nimproves its ability to handle queries with negation. In addition, the model\nperformance on the original benchmarks is also improved.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00132v2"
    },
    {
        "title": "ConfLab: A Data Collection Concept, Dataset, and Benchmark for Machine\n  Analysis of Free-Standing Social Interactions in the Wild",
        "authors": [
            "Chirag Raman",
            "Jose Vargas-Quiros",
            "Stephanie Tan",
            "Ashraful Islam",
            "Ekin Gedik",
            "Hayley Hung"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Recording the dynamics of unscripted human interactions in the wild is\nchallenging due to the delicate trade-offs between several factors: participant\nprivacy, ecological validity, data fidelity, and logistical overheads. To\naddress these, following a 'datasets for the community by the community' ethos,\nwe propose the Conference Living Lab (ConfLab): a new concept for multimodal\nmultisensor data collection of in-the-wild free-standing social conversations.\nFor the first instantiation of ConfLab described here, we organized a real-life\nprofessional networking event at a major international conference. Involving 48\nconference attendees, the dataset captures a diverse mix of status,\nacquaintance, and networking motivations. Our capture setup improves upon the\ndata fidelity of prior in-the-wild datasets while retaining privacy\nsensitivity: 8 videos (1920x1080, 60 fps) from a non-invasive overhead view,\nand custom wearable sensors with onboard recording of body motion (full 9-axis\nIMU), privacy-preserving low-frequency audio (1250 Hz), and Bluetooth-based\nproximity. Additionally, we developed custom solutions for distributed hardware\nsynchronization at acquisition and time-efficient continuous annotation of body\nkeypoints and actions at high sampling rates. Our benchmarks showcase some of\nthe open research tasks related to in-the-wild privacy-preserving social data\nanalysis: keypoints detection from overhead camera views, skeleton-based\nno-audio speaker detection, and F-formation detection.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.05177v3"
    },
    {
        "title": "Deep Decomposition and Bilinear Pooling Network for Blind Night-Time\n  Image Quality Evaluation",
        "authors": [
            "Qiuping Jiang",
            "Jiawu Xu",
            "Yudong Mao",
            "Wei Zhou",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Blind image quality assessment (BIQA), which aims to accurately predict the\nimage quality without any pristine reference information, has been extensively\nconcerned in the past decades. Especially, with the help of deep neural\nnetworks, great progress has been achieved. However, it remains less\ninvestigated on BIQA for night-time images (NTIs) which usually suffers from\ncomplicated authentic distortions such as reduced visibility, low contrast,\nadditive noises, and color distortions. These diverse authentic degradations\nparticularly challenges the design of effective deep neural network for blind\nNTI quality evaluation (NTIQE). In this paper, we propose a novel deep\ndecomposition and bilinear pooling network (DDB-Net) to better address this\nissue. The DDB-Net contains three modules, i.e., an image decomposition module,\na feature encoding module, and a bilinear pooling module. The image\ndecomposition module is inspired by the Retinex theory and involves decoupling\nthe input NTI into an illumination layer component responsible for illumination\ninformation and a reflection layer component responsible for content\ninformation. Then, the feature encoding module involves learning feature\nrepresentations of degradations that are rooted in the two decoupled components\nseparately. Finally, by modeling illumination-related and content-related\ndegradations as two-factor variations, the two feature sets are bilinearly\npooled together to form a unified representation for quality prediction. The\nsuperiority of the proposed DDB-Net has been well validated by extensive\nexperiments on several benchmark datasets. The source code will be made\navailable soon.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.05880v2"
    },
    {
        "title": "Distilling Knowledge from Object Classification to Aesthetics Assessment",
        "authors": [
            "Jingwen Hou",
            "Henghui Ding",
            "Weisi Lin",
            "Weide Liu",
            "Yuming Fang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this work, we point out that the major dilemma of image aesthetics\nassessment (IAA) comes from the abstract nature of aesthetic labels. That is, a\nvast variety of distinct contents can correspond to the same aesthetic label.\nOn the one hand, during inference, the IAA model is required to relate various\ndistinct contents to the same aesthetic label. On the other hand, when\ntraining, it would be hard for the IAA model to learn to distinguish different\ncontents merely with the supervision from aesthetic labels, since aesthetic\nlabels are not directly related to any specific content. To deal with this\ndilemma, we propose to distill knowledge on semantic patterns for a vast\nvariety of image contents from multiple pre-trained object classification (POC)\nmodels to an IAA model. Expecting the combination of multiple POC models can\nprovide sufficient knowledge on various image contents, the IAA model can\neasier learn to relate various distinct contents to a limited number of\naesthetic labels. By supervising an end-to-end single-backbone IAA model with\nthe distilled knowledge, the performance of the IAA model is significantly\nimproved by 4.8% in SRCC compared to the version trained only with ground-truth\naesthetic labels. On specific categories of images, the SRCC improvement\nbrought by the proposed method can achieve up to 7.2%. Peer comparison also\nshows that our method outperforms 10 previous IAA methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.00809v1"
    },
    {
        "title": "A DTCWT-SVD Based Video Watermarking resistant to frame rate conversion",
        "authors": [
            "Yifei Wang",
            "Qichao Ying",
            "Zhenxing Qian",
            "Sheng Li",
            "Xinpeng Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Videos can be easily tampered, copied and redistributed by attackers for\nillegal and monetary usage. Such behaviors severely jeopardize the interest of\ncontent owners. Despite huge efforts made in digital video watermarking for\ncopyright protection, typical distortions in video transmission including\nsignal attacks, geometric attacks and temporal synchronization attacks can\nstill easily erase the embedded signal. Among them, temporal synchronization\nattacks which include frame dropping, frame insertion and frame rate conversion\nis one of the most prevalent attacks. To address this issue, we present a new\nvideo watermarking based on joint Dual-Tree Cosine Wavelet Transformation\n(DTCWT) and Singular Value Decomposition (SVD), which is resistant to frame\nrate conversion. We first extract a set of candidate coefficient by applying\nSVD decomposition after DTCWT transform. Then, we simulate the watermark\nembedding by adjusting the shape of candidate coefficient. Finally, we perform\ngroup-level watermarking that includes moderate temporal redundancy to resist\ntemporal desynchronization attacks. Extensive experimental results show that\nthe proposed scheme is more resilient to temporal desynchronization attacks and\nperforms better than the existing blind video watermarking schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.01094v1"
    },
    {
        "title": "Block-Parallel Systolic-Array Architecture for 2-D NTT-based Fragile\n  Watermark Embedding",
        "authors": [
            "H. P. L. Arjuna Madanayake",
            "R. J. Cintra",
            "V. S. Dimitrov",
            "L. Bruton"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Number-theoretic transforms (NTTs) have been applied in the fragile\nwatermarking of digital images. A block-parallel systolic-array architecture is\nproposed for watermarking based on the 2-D special Hartley NTT (HNTT). The\nproposed core employs two 2-D special HNTT hardware cores, each using digital\narithmetic over $\\mathrm{GF}(3)$, and processes $4\\times4$ blocks of pixels in\nparallel every clock cycle. Prototypes are operational on a Xilinx Sx35-10ff668\nFPGA device. The maximum estimated throughput of the FPGA circuit is 100\nmillion $4\\times4$ HNTT fragile watermarked blocks per second, when clocked at\n100 MHz. Potential applications exist in high-traffic back-end servers dealing\nwith large amounts of protected digital images requiring authentication, in\nremote-sensing for high-security surveillance applications, in real-time video\nprocessing of information of a sensitive nature or matters of national\nsecurity, in video/photographic content management of corporate clients, in\nauthenticating multimedia for the entertainment industry, in the authentication\nof electronic evidence material, and in real-time news streaming.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.01146v1"
    },
    {
        "title": "What is the Metaverse? An Immersive Cyberspace and Open Challenges",
        "authors": [
            "Lik-Hang Lee",
            "Pengyuan Zhou",
            "Tristan Braud",
            "Pan Hui"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The Metaverse refers to a virtual-physical blended space in which multiple\nusers can concurrently interact with a unified computer-generated environment\nand other users, which can be regarded as the next significant milestone of the\ncurrent cyberspace. This article primarily discusses the development and\nchallenges of the Metaverse. We first briefly describe the development of\ncyberspace and the necessity of technology enablers. Accordingly, our bottom-up\napproach highlights three critical technology enablers for the Metaverse:\nnetworks, systems, and users. Also, we highlight a number of indispensable\nissues, under technological and ecosystem perspectives, that build and sustain\nthe Metaverse.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.03018v1"
    },
    {
        "title": "Blind Surveillance Image Quality Assessment via Deep Neural Network\n  Combined with the Visual Saliency",
        "authors": [
            "Wei Lu",
            "Wei Sun",
            "Wenhan Zhu",
            "Xiongkuo Min",
            "Zicheng Zhang",
            "Tao Wang",
            "Guangtao Zhai"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The intelligent video surveillance system (IVSS) can automatically analyze\nthe content of the surveillance image (SI) and reduce the burden of the manual\nlabour. However, the SIs may suffer quality degradations in the procedure of\nacquisition, compression, and transmission, which makes IVSS hard to understand\nthe content of SIs. In this paper, we first conduct an example experiment (i.e.\nthe face detection task) to demonstrate that the quality of the SIs has a\ncrucial impact on the performance of the IVSS, and then propose a\nsaliency-based deep neural network for the blind quality assessment of the SIs,\nwhich helps IVSS to filter the low-quality SIs and improve the detection and\nrecognition performance. Specifically, we first compute the saliency map of the\nSI to select the most salient local region since the salient regions usually\ncontain rich semantic information for machine vision and thus have a great\nimpact on the overall quality of the SIs. Next, the convolutional neural\nnetwork (CNN) is adopted to extract quality-aware features for the whole image\nand local region, which are then mapped into the global and local quality\nscores through the fully connected (FC) network respectively. Finally, the\noverall quality score is computed as the weighted sum of the global and local\nquality scores. Experimental results on the SI quality database (SIQD) show\nthat the proposed method outperforms all compared state-of-the-art BIQA\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.04318v1"
    },
    {
        "title": "Deep Neural Network for Blind Visual Quality Assessment of 4K Content",
        "authors": [
            "Wei Lu",
            "Wei Sun",
            "Xiongkuo Min",
            "Wenhan Zhu",
            "Quan Zhou",
            "Jun He",
            "Qiyuan Wang",
            "Zicheng Zhang",
            "Tao Wang",
            "Guangtao Zhai"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The 4K content can deliver a more immersive visual experience to consumers\ndue to the huge improvement of spatial resolution. However, existing blind\nimage quality assessment (BIQA) methods are not suitable for the original and\nupscaled 4K contents due to the expanded resolution and specific distortions.\nIn this paper, we propose a deep learning-based BIQA model for 4K content,\nwhich on one hand can recognize true and pseudo 4K content and on the other\nhand can evaluate their perceptual visual quality. Considering the\ncharacteristic that high spatial resolution can represent more abundant\nhigh-frequency information, we first propose a Grey-level Co-occurrence Matrix\n(GLCM) based texture complexity measure to select three representative image\npatches from a 4K image, which can reduce the computational complexity and is\nproven to be very effective for the overall quality prediction through\nexperiments. Then we extract different kinds of visual features from the\nintermediate layers of the convolutional neural network (CNN) and integrate\nthem into the quality-aware feature representation. Finally, two multilayer\nperception (MLP) networks are utilized to map the quality-aware features into\nthe class probability and the quality score for each patch respectively. The\noverall quality index is obtained through the average pooling of patch results.\nThe proposed model is trained through the multi-task learning manner and we\nintroduce an uncertainty principle to balance the losses of the classification\nand regression tasks. The experimental results show that the proposed model\noutperforms all compared BIQA metrics on four 4K content quality assessment\ndatabases.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.04363v1"
    },
    {
        "title": "Unifying Multimodal Source and Propagation Graph for Rumour Detection on\n  Social Media with Missing Features",
        "authors": [
            "Tsun-Hin Cheung",
            "Kin-Man Lam"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  With the rapid development of online social media platforms, the spread of\nrumours has become a critical societal concern. Current methods for rumour\ndetection can be categorized into image-text pair classification and\nsource-reply graph classification. In this paper, we propose a novel approach\nthat combines multimodal source and propagation graph features for rumour\nclassification. We introduce the Unified Multimodal Graph Transformer Network\n(UMGTN) which integrates Transformer encoders to fuse these features. Given\nthat not every message in social media is associated with an image and\ncommunity responses in propagation graphs do not immediately follow source\nmessages, our aim is to build a network architecture that handles missing\nfeatures such as images or replies. To enhance the model's robustness to data\nwith missing features, we adopt a multitask learning framework that\nsimultaneously learns representations between samples with complete and missing\nfeatures. We evaluate our proposed method on four real-world datasets,\naugmenting them by recovering images and replies from Twitter and Weibo.\nExperimental results demonstrate that our UMGTN with multitask learning\nachieves state-of-the-art performance, improving F1-score by 1.0% to 4.0%,\nwhile maintaining detection robustness to missing features within 2% accuracy\nand F1-score compared to models trained without the multitask learning\nframework. We have made our models and datasets publicly available at:\nhttps://thcheung.github.io/umgtn/.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.04832v2"
    },
    {
        "title": "AntPivot: Livestream Highlight Detection via Hierarchical Attention\n  Mechanism",
        "authors": [
            "Yang Zhao",
            "Xuan Lin",
            "Wenqiang Xu",
            "Maozong Zheng",
            "Zhengyong Liu",
            "Zhou Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In recent days, streaming technology has greatly promoted the development in\nthe field of livestream. Due to the excessive length of livestream records,\nit's quite essential to extract highlight segments with the aim of effective\nreproduction and redistribution. Although there are lots of approaches proven\nto be effective in the highlight detection for other modals, the challenges\nexisting in livestream processing, such as the extreme durations, large topic\nshifts, much irrelevant information and so forth, heavily hamper the adaptation\nand compatibility of these methods. In this paper, we formulate a new task\nLivestream Highlight Detection, discuss and analyze the difficulties listed\nabove and propose a novel architecture AntPivot to solve this problem.\nConcretely, we first encode the original data into multiple views and model\ntheir temporal relations to capture clues in a hierarchical attention\nmechanism. Afterwards, we try to convert the detection of highlight clips into\nthe search for optimal decision sequences and use the fully integrated\nrepresentations to predict the final results in a dynamic-programming\nmechanism. Furthermore, we construct a fully-annotated dataset AntHighlight to\ninstantiate this task and evaluate the performance of our model. The extensive\nexperiments indicate the effectiveness and validity of our proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.04888v1"
    },
    {
        "title": "It's Time for Artistic Correspondence in Music and Video",
        "authors": [
            "Didac Suris",
            "Carl Vondrick",
            "Bryan Russell",
            "Justin Salamon"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  We present an approach for recommending a music track for a given video, and\nvice versa, based on both their temporal alignment and their correspondence at\nan artistic level. We propose a self-supervised approach that learns this\ncorrespondence directly from data, without any need of human annotations. In\norder to capture the high-level concepts that are required to solve the task,\nwe propose modeling the long-term temporal context of both the video and the\nmusic signals, using Transformer networks for each modality. Experiments show\nthat this approach strongly outperforms alternatives that do not exploit the\ntemporal context. The combination of our contributions improve retrieval\naccuracy up to 10x over prior state of the art. This strong improvement allows\nus to introduce a wide range of analyses and applications. For instance, we can\ncondition music retrieval based on visually defined attributes.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.07148v1"
    },
    {
        "title": "Bandwidth-Efficient Multi-video Prefetching for Short Video Streaming",
        "authors": [
            "Xutong Zuo",
            "Yishu Li",
            "Mohan Xu",
            "Wei Tsang Ooi",
            "Jiangchuan Liu",
            "Junchen Jiang",
            "Xinggong Zhang",
            "Kai Zheng",
            "Yong Cui"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Applications that allow sharing of user-created short videos exploded in\npopularity in recent years. A typical short video application allows a user to\nswipe away the current video being watched and start watching the next video in\na video queue. Such user interface causes significant bandwidth waste if users\nfrequently swipe a video away before finishing watching. Solutions to reduce\nbandwidth waste without impairing the Quality of Experience (QoE) are needed.\nSolving the problem requires adaptively prefetching of short video chunks,\nwhich is challenging as the download strategy needs to match unknown user\nviewing behavior and network conditions. In our work, we first formulate the\nproblem of adaptive multi-video prefetching in short video streaming. Then, to\nfacilitate the integration and comparison of researchers' algorithms towards\nsolving the problem, we design and implement a discrete-event simulator, which\nwe release as open source. Finally, based on the organization of the Short\nVideo Streaming Grand Challenge at ACM Multimedia 2022, we analyze and\nsummarize the algorithms of the contestants, with the hope of promoting the\nresearch community towards addressing this problem.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.09839v3"
    },
    {
        "title": "Information-Theoretic Bounds for Steganography in Multimedia",
        "authors": [
            "Hassan Y. El Arsh",
            "Amr Abdelaziz",
            "Ahmed Elliethy",
            "Hussein A. Aly",
            "T. Aaron Gulliver"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Steganography in multimedia aims to embed secret data into an innocent\nlooking multimedia cover object. This embedding introduces some distortion to\nthe cover object and produces a corresponding stego object. The embedding\ndistortion is measured by a cost function that determines the detection\nprobability of the existence of the embedded secret data. A cost function\nrelated to the maximum embedding rate is typically employed to evaluate a\nsteganographic system. In addition, the distribution of multimedia sources\nfollows the Gibbs distribution which is a complex statistical model that\nrestricts analysis. Thus, previous multimedia steganographic approaches either\nassume a relaxed distribution or presume a proposition on the maximum embedding\nrate and then try to prove it is correct. Conversely, this paper introduces an\nanalytic approach to determining the maximum embedding rate in multimedia cover\nobjects through a constrained optimization problem concerning the relationship\nbetween the maximum embedding rate and the probability of detection by any\nsteganographic detector. The KL-divergence between the distributions for the\ncover and stego objects is used as the cost function as it upper bounds the\nperformance of the optimal steganographic detector. An equivalence between the\nGibbs and correlated-multivariate-quantized-Gaussian distributions is\nestablished to solve this optimization problem. The solution provides an\nanalytic form for the maximum embedding rate in terms of the WrightOmega\nfunction. Moreover, it is proven that the maximum embedding rate is in\nagreement with the commonly used Square Root Law (SRL) for steganography, but\nthe solution presented here is more accurate. Finally, the theoretical results\nobtained are verified experimentally.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.04521v2"
    },
    {
        "title": "RTN: Reinforced Transformer Network for Coronary CT Angiography\n  Vessel-level Image Quality Assessment",
        "authors": [
            "Yiting Lu",
            "Jun Fu",
            "Xin Li",
            "Wei Zhou",
            "Sen Liu",
            "Xinxin Zhang",
            "Congfu Jia",
            "Ying Liu",
            "Zhibo Chen"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Coronary CT Angiography (CCTA) is susceptible to various distortions (e.g.,\nartifacts and noise), which severely compromise the exact diagnosis of\ncardiovascular diseases. The appropriate CCTA Vessel-level Image Quality\nAssessment (CCTA VIQA) algorithm can be used to reduce the risk of error\ndiagnosis. The primary challenges of CCTA VIQA are that the local part of\ncoronary that determines final quality is hard to locate. To tackle the\nchallenge, we formulate CCTA VIQA as a multiple-instance learning (MIL)\nproblem, and exploit Transformer-based MIL backbone (termed as T-MIL) to\naggregate the multiple instances along the coronary centerline into the final\nquality. However, not all instances are informative for final quality. There\nare some quality-irrelevant/negative instances intervening the exact quality\nassessment(e.g., instances covering only background or the coronary in\ninstances is not identifiable). Therefore, we propose a Progressive\nReinforcement learning based Instance Discarding module (termed as PRID) to\nprogressively remove quality-irrelevant/negative instances for CCTA VIQA. Based\non the above two modules, we propose a Reinforced Transformer Network (RTN) for\nautomatic CCTA VIQA based on end-to-end optimization. Extensive experimental\nresults demonstrate that our proposed method achieves the state-of-the-art\nperformance on the real-world CCTA dataset, exceeding previous MIL methods by a\nlarge margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.06177v1"
    },
    {
        "title": "A Comprehensive Review on Digital Image Watermarking",
        "authors": [
            "Shweta Wadhera",
            "Deepa Kamra",
            "Ankit Rajpal",
            "Aruna Jain",
            "Vishal Jain"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The advent of the Internet led to the easy availability of digital data like\nimages, audio, and video. Easy access to multimedia gives rise to the issues\nsuch as content authentication, security, copyright protection, and ownership\nidentification. Here, we discuss the concept of digital image watermarking with\na focus on the technique used in image watermark embedding and extraction of\nthe watermark. The detailed classification along with the basic\ncharacteristics, namely visual imperceptibility, robustness, capacity, security\nof digital watermarking is also presented in this work. Further, we have also\ndiscussed the recent application areas of digital watermarking such as\nhealthcare, remote education, electronic voting systems, and the military. The\nrobustness is evaluated by examining the effect of image processing attacks on\nthe signed content and the watermark recoverability. The authors believe that\nthe comprehensive survey presented in this paper will help the new researchers\nto gather knowledge in this domain. Further, the comparative analysis can\nenkindle ideas to improve upon the already mentioned techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.06909v1"
    },
    {
        "title": "A Hybrid Deep Animation Codec for Low-bitrate Video Conferencing",
        "authors": [
            "Goluck Konuko",
            "Stéphane Lathuilière",
            "Giuseppe Valenzise"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Deep generative models, and particularly facial animation schemes, can be\nused in video conferencing applications to efficiently compress a video through\na sparse set of keypoints, without the need to transmit dense motion vectors.\nWhile these schemes bring significant coding gains over conventional video\ncodecs at low bitrates, their performance saturates quickly when the available\nbandwidth increases. In this paper, we propose a layered, hybrid coding scheme\nto overcome this limitation. Specifically, we extend a codec based on facial\nanimation by adding an auxiliary stream consisting of a very low bitrate\nversion of the video, obtained through a conventional video codec (e.g., HEVC).\nThe animated and auxiliary videos are combined through a novel fusion module.\nOur results show consistent average BD-Rate gains in excess of -30% on a large\ndataset of video conferencing sequences, extending the operational range of\nbitrates of a facial animation codec alone\n",
        "pdf_link": "http://arxiv.org/pdf/2207.13530v1"
    },
    {
        "title": "A New Scheme for Image Compression and Encryption Using ECIES, Henon\n  Map, and AEGAN",
        "authors": [
            "Mahdi Shariatzadeh",
            "Mahdi Eftekhari",
            "Mohammad Javad Rostami"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Providing security in the transmission of images and other multimedia data\nhas become one of the most important scientific and practical issues. In this\npaper, a method for compressing and encryption images is proposed, which can\nsafely transmit images in low-bandwidth data transmission channels. At first,\nusing the autoencoding generative adversarial network (AEGAN) model, the images\nare mapped to a vector in the latent space with low dimensions. In the next\nstep, the obtained vector is encrypted using public key encryption methods. In\nthe proposed method, Henon chaotic map is used for permutation, which makes\ninformation transfer more secure. To evaluate the results of the proposed\nscheme, three criteria SSIM, PSNR, and execution time have been used.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.07635v2"
    },
    {
        "title": "M2HF: Multi-level Multi-modal Hybrid Fusion for Text-Video Retrieval",
        "authors": [
            "Shuo Liu",
            "Weize Quan",
            "Ming Zhou",
            "Sihong Chen",
            "Jian Kang",
            "Zhe Zhao",
            "Chen Chen",
            "Dong-Ming Yan"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Videos contain multi-modal content, and exploring multi-level cross-modal\ninteractions with natural language queries can provide great prominence to\ntext-video retrieval task (TVR). However, new trending methods applying\nlarge-scale pre-trained model CLIP for TVR do not focus on multi-modal cues in\nvideos. Furthermore, the traditional methods simply concatenating multi-modal\nfeatures do not exploit fine-grained cross-modal information in videos. In this\npaper, we propose a multi-level multi-modal hybrid fusion (M2HF) network to\nexplore comprehensive interactions between text queries and each modality\ncontent in videos. Specifically, M2HF first utilizes visual features extracted\nby CLIP to early fuse with audio and motion features extracted from videos,\nobtaining audio-visual fusion features and motion-visual fusion features\nrespectively. Multi-modal alignment problem is also considered in this process.\nThen, visual features, audio-visual fusion features, motion-visual fusion\nfeatures, and texts extracted from videos establish cross-modal relationships\nwith caption queries in a multi-level way. Finally, the retrieval outputs from\nall levels are late fused to obtain final text-video retrieval results. Our\nframework provides two kinds of training strategies, including an ensemble\nmanner and an end-to-end manner. Moreover, a novel multi-modal balance loss\nfunction is proposed to balance the contributions of each modality for\nefficient end-to-end training. M2HF allows us to obtain state-of-the-art\nresults on various benchmarks, eg, Rank@1 of 64.9\\%, 68.2\\%, 33.2\\%, 57.1\\%,\n57.8\\% on MSR-VTT, MSVD, LSMDC, DiDeMo, and ActivityNet, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.07664v1"
    },
    {
        "title": "Blind Quality Assessment of 3D Dense Point Clouds with Structure Guided\n  Resampling",
        "authors": [
            "Wei Zhou",
            "Qi Yang",
            "Qiuping Jiang",
            "Guangtao Zhai",
            "Weisi Lin"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Objective quality assessment of 3D point clouds is essential for the\ndevelopment of immersive multimedia systems in real-world applications. Despite\nthe success of perceptual quality evaluation for 2D images and videos,\nblind/no-reference metrics are still scarce for 3D point clouds with\nlarge-scale irregularly distributed 3D points. Therefore, in this paper, we\npropose an objective point cloud quality index with Structure Guided Resampling\n(SGR) to automatically evaluate the perceptually visual quality of 3D dense\npoint clouds. The proposed SGR is a general-purpose blind quality assessment\nmethod without the assistance of any reference information. Specifically,\nconsidering that the human visual system (HVS) is highly sensitive to structure\ninformation, we first exploit the unique normal vectors of point clouds to\nexecute regional pre-processing which consists of keypoint resampling and local\nregion construction. Then, we extract three groups of quality-related features,\nincluding: 1) geometry density features; 2) color naturalness features; 3)\nangular consistency features. Both the cognitive peculiarities of the human\nbrain and naturalness regularity are involved in the designed quality-aware\nfeatures that can capture the most vital aspects of distorted 3D point clouds.\nExtensive experiments on several publicly available subjective point cloud\nquality databases validate that our proposed SGR can compete with\nstate-of-the-art full-reference, reduced-reference, and no-reference quality\nassessment algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.14603v1"
    },
    {
        "title": "Deep Live Video Ad Placement on the 5G Edge",
        "authors": [
            "Mohammad Hosseini"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The video broadcasting industry has been growing significantly in the recent\nyears, specially on delivering personalized contents to the end users. While\nvideo broadcasting has continued to grow beyond TV, video adverting has become\na key marketing tool to deliver targeted messages directly to the audience.\nHowever, unfortunately for broadband TV, a key problem is that the TV\ncommercials target the broad audience, therefore lacking user-specific and\npersonalized ad contents.\n  In this paper, we propose a deep edge-cloud ad-placement system, and briefly\ndescribe our methodologies and the architecture of our designed ad placement\nsystem for delivering both the Video on Demand (VoD) and live broadcast TV\ncontents over MMT streaming protocol. The aim of our paper is to showcase how\nto enable targeted, personalized, and user-specific advertising services\ndeployed on the future 5G MEC platforms, which in turn can have high potentials\nto increase ad revenues for the mobile operator industry.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.01421v1"
    },
    {
        "title": "Volumetric video streaming: Current approaches and implementations",
        "authors": [
            "Irene Viola",
            "Pablo Cesar"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The rise of capturing systems for objects and scenes in 3D with increased\nfidelity and immersion has led to the popularity of volumetric video contents\nthat can be seen from any position and angle in 6 degrees of freedom\nnavigation. Such contents need large volumes of data to accurately represent\nthe real world. Thus, novel optimization solutions and delivery systems are\nneeded to enable volumetric video streaming over bandwidth-limited networks. In\nthis chapter, we discuss theoretical approaches to volumetric video streaming\noptimization, through compression solutions, as well as network and user\nadaptation, for high-end and low-powered devices. Moreover, we present an\noverview of existing end-to-end systems, and we point to the future of\nvolumetric video streaming.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.01982v1"
    },
    {
        "title": "Using Computational Approaches in Visual Identity Design: A Visual\n  Identity for the Design and Multimedia Courses of Faculty of Sciences and\n  Technology of University of Coimbra",
        "authors": [
            "Sérgio M. Rebelo",
            "Tiago Martins",
            "Artur Rebelo",
            "João Bicker",
            "Penousal Machado"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Computational approaches are beginning to be used to design dynamic visual\nidentities fuelled by data and generative processes. In this work, we explore\nthese computational approaches in order to generate a visual identity that\ncreates bespoke letterings and images. We achieve this developing a generative\ndesign system that automatically assembles black and white visual modules. This\nsystem generates designs performing two main methods: (i) Assisted generation;\nand (ii) Automatic generation. Assisted generation method produces outputs\nwherein the placement of modules is determined by a configuration file previous\ndefined. On the other hand, the Automatic generation method produces outputs\nwherein the modules are assembled to depict an input image. This system speeds\nup the process of design and deployment of one visual identity design as well\nas it generates outputs visual coherent among them. In this paper, we\ncompressively describe this system and its achievements.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.03420v1"
    },
    {
        "title": "A Survey on Mobile Edge Computing for Video Streaming: Opportunities and\n  Challenges",
        "authors": [
            "Muhammad Asif Khan",
            "Emna Baccour",
            "Zina Chkirbene",
            "Aiman Erbad",
            "Ridha Hamila",
            "Mounir Hamdi",
            "Moncef Gabbouj"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  5G communication brings substantial improvements in the quality of service\nprovided to various applications by achieving higher throughput and lower\nlatency. However, interactive multimedia applications (e.g., ultra high\ndefinition video conferencing, 3D and multiview video streaming, crowd-sourced\nvideo streaming, cloud gaming, virtual and augmented reality) are becoming more\nambitious with high volume and low latency video streams putting strict demands\non the already congested networks. Mobile Edge Computing (MEC) is an emerging\nparadigm that extends cloud computing capabilities to the edge of the network\ni.e., at the base station level. To meet the latency requirements and avoid the\nend-to-end communication with remote cloud data centers, MEC allows to store\nand process video content (e.g., caching, transcoding, pre-processing) at the\nbase stations. Both video on demand and live video streaming can utilize MEC to\nimprove existing services and develop novel use cases, such as video analytics,\nand targeted advertisements. MEC is expected to reshape the future of video\nstreaming by providing ultra-reliable and low latency streaming (e.g., in\naugmented reality, virtual reality, and autonomous vehicles), pervasive\ncomputing (e.g., in real-time video analytics), and blockchain-enabled\narchitecture for secure live streaming. This paper presents a comprehensive\nsurvey of recent developments in MEC-enabled video streaming bringing\nunprecedented improvement to enable novel use cases. A detailed review of the\nstate-of-the-art is presented covering novel caching schemes, optimal\ncomputation offloading, cooperative caching and offloading and the use of\nartificial intelligence (i.e., machine learning, deep learning, and\nreinforcement learning) in MEC-assisted video streaming services.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.05761v1"
    },
    {
        "title": "EmpathicSchool: A multimodal dataset for real-time facial expressions\n  and physiological data analysis under different stress conditions",
        "authors": [
            "Majid Hosseini",
            "Fahad Sohrab",
            "Raju Gottumukkala",
            "Ravi Teja Bhupatiraju",
            "Satya Katragadda",
            "Jenni Raitoharju",
            "Alexandros Iosifidis",
            "Moncef Gabbouj"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Affective computing has garnered researchers' attention and interest in\nrecent years as there is a need for AI systems to better understand and react\nto human emotions. However, analyzing human emotions, such as mood or stress,\nis quite complex. While various stress studies use facial expressions and\nwearables, most existing datasets rely on processing data from a single\nmodality. This paper presents EmpathicSchool, a novel dataset that captures\nfacial expressions and the associated physiological signals, such as heart\nrate, electrodermal activity, and skin temperature, under different stress\nlevels. The data was collected from 20 participants at different sessions for\n26 hours. The data includes nine different signal types, including both\ncomputer vision and physiological features that can be used to detect stress.\nIn addition, various experiments were conducted to validate the signal quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.13542v1"
    },
    {
        "title": "Towards Holographic Video Communications: A Promising AI-driven Solution",
        "authors": [
            "Yakun Huang",
            "Yuanwei Zhu",
            "Xiuquan Qiao",
            "Xiang Su",
            "Schahram Dustdar",
            "Ping Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Real-time holographic video communications enable immersive experiences for\nnext-generation video services in the future metaverse era. However,\nhigh-fidelity holographic videos require high bandwidth and significant\ncomputation resources, which exceed the transferring and computing capacity of\n5G networks. This article reviews state-of-the-art holographic point cloud\nvideo (PCV) transmission techniques and highlights the critical challenges of\ndelivering such immersive services. We further implement a preliminary\nprototype of an AI-driven holographic video communication system and present\ncritical experimental results to evaluate its performance. Finally, we identify\nfuture research directions and discuss potential solutions for providing\nreal-time and high-quality holographic experiences.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.06794v1"
    },
    {
        "title": "Comparison of Popular Video Conferencing Apps Using Client-side\n  Measurements on Different Backhaul Networks",
        "authors": [
            "Rohan Kumar",
            "Dhruv Nagpal",
            "Vinayak Naik",
            "Dipanjan Chakraborty"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Video conferencing platforms have been appropriated during the COVID-19\npandemic for different purposes, including classroom teaching. However, the\nplatforms are not designed for many of these objectives. When users, like\neducationists, select a platform, it is unclear which platform will perform\nbetter given the same network and hardware resources to meet the required\nQuality of Experience (QoE). Similarly, when developers design a new video\nconferencing platform, they do not have clear guidelines for making design\nchoices given the QoE requirements.\n  In this paper, we provide a set of networks and systems measurements, and\nquantitative user studies to measure the performance of video conferencing apps\nin terms of both, Quality of Service (QoS) and QoE. Using those metrics, we\nmeasure the performance of Google Meet, Microsoft Teams, and Zoom, which are\nthree popular platforms in education and business. We find a substantial\ndifference in how the three apps treat video and audio streams. We see that\ntheir choice of treatment affects their consumption of hardware resources. Our\nquantitative user studies confirm the findings of our quantitative\nmeasurements. While each platform has its benefits, we find that no app is\nideal. A user can choose a suitable platform depending on which of the\nfollowing, audio, video, or network bandwidth, CPU, or memory are more\nimportant.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.09651v1"
    },
    {
        "title": "A Multimodal Sensor Fusion Framework Robust to Missing Modalities for\n  Person Recognition",
        "authors": [
            "Vijay John",
            "Yasutomo Kawanishi"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Utilizing the sensor characteristics of the audio, visible camera, and\nthermal camera, the robustness of person recognition can be enhanced. Existing\nmultimodal person recognition frameworks are primarily formulated assuming that\nmultimodal data is always available. In this paper, we propose a novel trimodal\nsensor fusion framework using the audio, visible, and thermal camera, which\naddresses the missing modality problem. In the framework, a novel deep latent\nembedding framework, termed the AVTNet, is proposed to learn multiple latent\nembeddings. Also, a novel loss function, termed missing modality loss, accounts\nfor possible missing modalities based on the triplet loss calculation while\nlearning the individual latent embeddings. Additionally, a joint latent\nembedding utilizing the trimodal data is learnt using the multi-head attention\ntransformer, which assigns attention weights to the different modalities. The\ndifferent latent embeddings are subsequently used to train a deep neural\nnetwork. The proposed framework is validated on the Speaking Faces dataset. A\ncomparative analysis with baseline algorithms shows that the proposed framework\nsignificantly increases the person recognition accuracy while accounting for\nmissing modalities.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.10972v2"
    },
    {
        "title": "End-to-end Transformer for Compressed Video Quality Enhancement",
        "authors": [
            "Li Yu",
            "Wenshuai Chang",
            "Shiyu Wu",
            "Moncef Gabbouj"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Convolutional neural networks have achieved excellent results in compressed\nvideo quality enhancement task in recent years. State-of-the-art methods\nexplore the spatiotemporal information of adjacent frames mainly by deformable\nconvolution. However, offset fields in deformable convolution are difficult to\ntrain, and its instability in training often leads to offset overflow, which\nreduce the efficiency of correlation modeling. In this work, we propose a\ntransformer-based compressed video quality enhancement (TVQE) method,\nconsisting of Swin-AutoEncoder based Spatio-Temporal feature Fusion (SSTF)\nmodule and Channel-wise Attention based Quality Enhancement (CAQE) module. The\nproposed SSTF module learns both local and global features with the help of\nSwin-AutoEncoder, which improves the ability of correlation modeling.\nMeanwhile, the window mechanism-based Swin Transformer and the encoderdecoder\nstructure greatly improve the execution efficiency. On the other hand, the\nproposed CAQE module calculates the channel attention, which aggregates the\ntemporal information between channels in the feature map, and finally achieves\nthe efficient fusion of inter-frame information. Extensive experimental results\non the JCT-VT test sequences show that the proposed method achieves better\nperformance in average for both subjective and objective quality. Meanwhile,\nour proposed method outperforms existing ones in terms of both inference speed\nand GPU consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.13827v1"
    },
    {
        "title": "Leveraging Computer Vision Application in Visual Arts: A Case Study on\n  the Use of Residual Neural Network to Classify and Analyze Baroque Paintings",
        "authors": [
            "Daniel Kvak"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  With the increasing availability of large digitized fine art collections,\nautomated analysis and classification of paintings is becoming an interesting\narea of research. However, due to domain specificity, implicit subjectivity,\nand pervasive nuances that vaguely separate art movements, analyzing art using\nmachine learning techniques poses significant challenges. Residual networks, or\nvariants thereof, are one the most popular tools for image classification\ntasks, which can extract relevant features for well-defined classes. In this\ncase study, we focus on the classification of a selected painting 'Portrait of\nthe Painter Charles Bruni' by Johann Kupetzky and the analysis of the\nperformance of the proposed classifier. We show that the features extracted\nduring residual network training can be useful for image retrieval within\nsearch systems in online art collections.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.15300v1"
    },
    {
        "title": "Improving the Modality Representation with Multi-View Contrastive\n  Learning for Multimodal Sentiment Analysis",
        "authors": [
            "Peipei Liu",
            "Xin Zheng",
            "Hong Li",
            "Jie Liu",
            "Yimo Ren",
            "Hongsong Zhu",
            "Limin Sun"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Modality representation learning is an important problem for multimodal\nsentiment analysis (MSA), since the highly distinguishable representations can\ncontribute to improving the analysis effect. Previous works of MSA have usually\nfocused on multimodal fusion strategies, and the deep study of modal\nrepresentation learning was given less attention. Recently, contrastive\nlearning has been confirmed effective at endowing the learned representation\nwith stronger discriminate ability. Inspired by this, we explore the\nimprovement approaches of modality representation with contrastive learning in\nthis study. To this end, we devise a three-stages framework with multi-view\ncontrastive learning to refine representations for the specific objectives. At\nthe first stage, for the improvement of unimodal representations, we employ the\nsupervised contrastive learning to pull samples within the same class together\nwhile the other samples are pushed apart. At the second stage, a\nself-supervised contrastive learning is designed for the improvement of the\ndistilled unimodal representations after cross-modal interaction. At last, we\nleverage again the supervised contrastive learning to enhance the fused\nmultimodal representation. After all the contrast trainings, we next achieve\nthe classification task based on frozen representations. We conduct experiments\non three open datasets, and results show the advance of our model.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.15824v3"
    },
    {
        "title": "GRACE: Loss-Resilient Real-Time Video Communication Using Data-Scalable\n  Autoencoder",
        "authors": [
            "Yihua Cheng",
            "Anton Arapin",
            "Ziyi Zhang",
            "Qizheng Zhang",
            "Hanchen Li",
            "Nick Feamster",
            "Junchen Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Across many real-time video applications, we see a growing need (especially\nin long delays and dynamic bandwidth) to allow clients to decode each frame\nonce any (non-empty) subset of its packets is received and improve quality with\neach new packet. We call it data-scalable delivery. Unfortunately, existing\ntechniques (e.g., FEC, RS and Fountain Codes) fall short: they require either\ndelivery of a minimum number of packets to decode frames, and/or pad video data\nwith redundancy in anticipation of packet losses, which hurts video quality if\nno packets get lost. This work explores a new approach, inspired by recent\nadvances of neural-network autoencoders, which make data-scalable delivery\npossible. We present Grace, a concrete data-scalable real-time video system.\nWith the same video encoding, Grace's quality is slightly lower than\ntraditional codec without redundancy when no packet is lost, but with each\nmissed packet, its quality degrades much more gracefully than existing\nsolutions, allowing clients to flexibly trade between frame delay and video\nquality. Grace makes two contributions: (1) it trains new custom autoencoders\nto balance compression efficiency and resilience against a wide range of packet\nlosses; and (2) it uses a new transmission scheme to deliver autoencoder-coded\nframes as individually decodable packets. We test Grace (and traditional\nloss-resilient schemes and codecs) on real network traces and videos, and show\nthat while Grace's compression efficiency is slightly worse than heavily\nengineered video codecs, it significantly reduces tail video frame delay (by\n2$\\times$ at the 95th percentile) with the marginally lowered video quality\n",
        "pdf_link": "http://arxiv.org/pdf/2210.16639v1"
    },
    {
        "title": "DaI: Decrypt and Infer the Quality of Real-Time Video Streaming",
        "authors": [
            "Sheng Cheng"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Inferring the quality of network services is the vital basis of optimization\nfor network operators. However, prevailing real-time video streaming\napplications adopt encryption for security, leaving it a problem to extract\nQuality of Service (QoS) indicators of real-time video. In this paper, we\npropose DaI, a traffic-based real-time video quality estimator. DaI can\npartially decrypt the encrypted real-time video data and applies machine\nlearning methods to estimate key objective Quality of Experience (QoE) metrics\nof real-time video. According to the experimental results, DaI can estimate\nobjective QoE metrics with an average accuracy of 79%.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.02240v1"
    },
    {
        "title": "Complete Cross-triplet Loss in Label Space for Audio-visual Cross-modal\n  Retrieval",
        "authors": [
            "Donghuo Zeng",
            "Yanan Wang",
            "Jianming Wu",
            "Kazushi Ikeda"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The heterogeneity gap problem is the main challenge in cross-modal retrieval.\nBecause cross-modal data (e.g. audiovisual) have different distributions and\nrepresentations that cannot be directly compared. To bridge the gap between\naudiovisual modalities, we learn a common subspace for them by utilizing the\nintrinsic correlation in the natural synchronization of audio-visual data with\nthe aid of annotated labels. TNN-CCCA is the best audio-visual cross-modal\nretrieval (AV-CMR) model so far, but the model training is sensitive to hard\nnegative samples when learning common subspace by applying triplet loss to\npredict the relative distance between inputs. In this paper, to reduce the\ninterference of hard negative samples in representation learning, we propose a\nnew AV-CMR model to optimize semantic features by directly predicting labels\nand then measuring the intrinsic correlation between audio-visual data using\ncomplete cross-triple loss. In particular, our model projects audio-visual\nfeatures into label space by minimizing the distance between predicted label\nfeatures after feature projection and ground label representations. Moreover,\nwe adopt complete cross-triplet loss to optimize the predicted label features\nby leveraging the relationship between all possible similarity and\ndissimilarity semantic information across modalities. The extensive\nexperimental results on two audio-visual double-checked datasets have shown an\nimprovement of approximately 2.1% in terms of average MAP over the current\nstate-of-the-art method TNN-CCCA for the AV-CMR task, which indicates the\neffectiveness of our proposed model.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.03434v1"
    },
    {
        "title": "Dehazed Image Quality Evaluation: From Partial Discrepancy to Blind\n  Perception",
        "authors": [
            "Wei Zhou",
            "Ruizeng Zhang",
            "Leida Li",
            "Hantao Liu",
            "Huiyan Chen"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Image dehazing aims to restore spatial details from hazy images. There have\nemerged a number of image dehazing algorithms, designed to increase the\nvisibility of those hazy images. However, much less work has been focused on\nevaluating the visual quality of dehazed images. In this paper, we propose a\nReduced-Reference dehazed image quality evaluation approach based on Partial\nDiscrepancy (RRPD) and then extend it to a No-Reference quality assessment\nmetric with Blind Perception (NRBP). Specifically, inspired by the hierarchical\ncharacteristics of the human perceiving dehazed images, we introduce three\ngroups of features: luminance discrimination, color appearance, and overall\nnaturalness. In the proposed RRPD, the combined distance between a set of\nsender and receiver features is adopted to quantify the perceptually dehazed\nimage quality. By integrating global and local channels from dehazed images,\nthe RRPD is converted to NRBP which does not rely on any information from the\nreferences. Extensive experiment results on several dehazed image quality\ndatabases demonstrate that our proposed methods outperform state-of-the-art\nfull-reference, reduced-reference, and no-reference quality assessment models.\nFurthermore, we show that the proposed dehazed image quality evaluation methods\ncan be effectively applied to tune parameters for potential image dehazing\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.12636v1"
    },
    {
        "title": "Disparity-based Stereo Image Compression with Aligned Cross-View Priors",
        "authors": [
            "Yongqi Zhai",
            "Luyang Tang",
            "Yi Ma",
            "Rui Peng",
            "Ronggang Wang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  With the wide application of stereo images in various fields, the research on\nstereo image compression (SIC) attracts extensive attention from academia and\nindustry. The core of SIC is to fully explore the mutual information between\nthe left and right images and reduce redundancy between views as much as\npossible. In this paper, we propose DispSIC, an end-to-end trainable deep\nneural network, in which we jointly train a stereo matching model to assist in\nthe image compression task. Based on the stereo matching results (i.e.\ndisparity), the right image can be easily warped to the left view, and only the\nresiduals between the left and right views are encoded for the left image. A\nthree-branch auto-encoder architecture is adopted in DispSIC, which encodes the\nright image, the disparity map and the residuals respectively. During training,\nthe whole network can learn how to adaptively allocate bitrates to these three\nparts, achieving better rate-distortion performance at the cost of a lower\ndisparity map bitrates. Moreover, we propose a conditional entropy model with\naligned cross-view priors for SIC, which takes the warped latents of the right\nimage as priors to improve the accuracy of the probability estimation for the\nleft image. Experimental results demonstrate that our proposed method achieves\nsuperior performance compared to other existing SIC methods on the KITTI and\nInStereo2K datasets both quantitatively and qualitatively.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.00459v1"
    },
    {
        "title": "Audio-Visual Activity Guided Cross-Modal Identity Association for Active\n  Speaker Detection",
        "authors": [
            "Rahul Sharma",
            "Shrikanth Narayanan"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Active speaker detection in videos addresses associating a source face,\nvisible in the video frames, with the underlying speech in the audio modality.\nThe two primary sources of information to derive such a speech-face\nrelationship are i) visual activity and its interaction with the speech signal\nand ii) co-occurrences of speakers' identities across modalities in the form of\nface and speech. The two approaches have their limitations: the audio-visual\nactivity models get confused with other frequently occurring vocal activities,\nsuch as laughing and chewing, while the speakers' identity-based methods are\nlimited to videos having enough disambiguating information to establish a\nspeech-face association. Since the two approaches are independent, we\ninvestigate their complementary nature in this work. We propose a novel\nunsupervised framework to guide the speakers' cross-modal identity association\nwith the audio-visual activity for active speaker detection. Through\nexperiments on entertainment media videos from two benchmark datasets, the AVA\nactive speaker (movies) and Visual Person Clustering Dataset (TV shows), we\nshow that a simple late fusion of the two approaches enhances the active\nspeaker detection performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.00539v1"
    },
    {
        "title": "Comprehensive Complexity Assessment of Emerging Learned Image\n  Compression on CPU and GPU",
        "authors": [
            "Farhad Pakdaman",
            "Moncef Gabbouj"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Learned Compression (LC) is the emerging technology for compressing image and\nvideo content, using deep neural networks. Despite being new, LC methods have\nalready gained a compression efficiency comparable to state-of-the-art image\ncompression, such as HEVC or even VVC. However, the existing solutions often\nrequire a huge computational complexity, which discourages their adoption in\ninternational standards or products. This paper provides a comprehensive\ncomplexity assessment of several notable methods, that shed light on the\nmatter, and guide the future development of this field by presenting key\nfindings. To do so, six existing methods have been evaluated for both encoding\nand decoding, on CPU and GPU platforms. Various aspects of complexity such as\nthe overall complexity, share of each coding module, number of operations,\nnumber of parameters, most demanding GPU kernels, and memory requirements have\nbeen measured and compared on Kodak dataset. The reported results (1) quantify\nthe complexity of LC methods, (2) fairly compare different methods, and (3) a\nmajor contribution of the work is identifying and quantifying the key factors\naffecting the complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.05466v1"
    },
    {
        "title": "Towards Blind Watermarking: Combining Invertible and Non-invertible\n  Mechanisms",
        "authors": [
            "Rui Ma",
            "Mengxi Guo",
            "Yi Hou",
            "Fan Yang",
            "Yuan Li",
            "Huizhu Jia",
            "Xiaodong Xie"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Blind watermarking provides powerful evidence for copyright protection, image\nauthentication, and tampering identification. However, it remains a challenge\nto design a watermarking model with high imperceptibility and robustness\nagainst strong noise attacks. To resolve this issue, we present a framework\nCombining the Invertible and Non-invertible (CIN) mechanisms. The CIN is\ncomposed of the invertible part to achieve high imperceptibility and the\nnon-invertible part to strengthen the robustness against strong noise attacks.\nFor the invertible part, we develop a diffusion and extraction module (DEM) and\na fusion and split module (FSM) to embed and extract watermarks symmetrically\nin an invertible way. For the non-invertible part, we introduce a\nnon-invertible attention-based module (NIAM) and the noise-specific selection\nmodule (NSM) to solve the asymmetric extraction under a strong noise attack.\nExtensive experiments demonstrate that our framework outperforms the current\nstate-of-the-art methods of imperceptibility and robustness significantly. Our\nframework can achieve an average of 99.99% accuracy and 67.66 dB PSNR under\nnoise-free conditions, while 96.64% and 39.28 dB combined strong noise attacks.\nThe code will be available in https://github.com/rmpku/CIN.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.12678v1"
    },
    {
        "title": "Blind Omnidirectional Image Quality Assessment: Integrating Local\n  Statistics and Global Semantics",
        "authors": [
            "Wei Zhou",
            "Zhou Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Omnidirectional image quality assessment (OIQA) aims to predict the\nperceptual quality of omnidirectional images that cover the whole\n180$\\times$360$^{\\circ}$ viewing range of the visual environment. Here we\npropose a blind/no-reference OIQA method named S$^2$ that bridges the gap\nbetween low-level statistics and high-level semantics of omnidirectional\nimages. Specifically, statistic and semantic features are extracted in separate\npaths from multiple local viewports and the hallucinated global omnidirectional\nimage, respectively. A quality regression along with a weighting process is\nthen followed that maps the extracted quality-aware features to a perceptual\nquality prediction. Experimental results demonstrate that the proposed S$^2$\nmethod offers highly competitive performance against state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.12393v1"
    },
    {
        "title": "BIRD-PCC: Bi-directional Range Image-based Deep LiDAR Point Cloud\n  Compression",
        "authors": [
            "Chia-Sheng Liu",
            "Jia-Fong Yeh",
            "Hao Hsu",
            "Hung-Ting Su",
            "Ming-Sui Lee",
            "Winston H. Hsu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The large amount of data collected by LiDAR sensors brings the issue of LiDAR\npoint cloud compression (PCC). Previous works on LiDAR PCC have used range\nimage representations and followed the predictive coding paradigm to create a\nbasic prototype of a coding framework. However, their prediction methods give\nan inaccurate result due to the negligence of invalid pixels in range images\nand the omission of future frames in the time step. Moreover, their handcrafted\ndesign of residual coding methods could not fully exploit spatial redundancy.\nTo remedy this, we propose a coding framework BIRD-PCC. Our prediction module\nis aware of the coordinates of invalid pixels in range images and takes a\nbidirectional scheme. Also, we introduce a deep-learned residual coding module\nthat can further exploit spatial redundancy within a residual frame.\nExperiments conducted on SemanticKITTI and KITTI-360 datasets show that\nBIRD-PCC outperforms other methods in most bitrate conditions and generalizes\nwell to unseen environments.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.04027v2"
    },
    {
        "title": "Micro-video Tagging via Jointly Modeling Social Influence and Tag\n  Relation",
        "authors": [
            "Xiao Wang",
            "Tian Gan",
            "Yinwei Wei",
            "Jianlong Wu",
            "Dai Meng",
            "Liqiang Nie"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The last decade has witnessed the proliferation of micro-videos on various\nuser-generated content platforms. According to our statistics, around 85.7\\% of\nmicro-videos lack annotation. In this paper, we focus on annotating\nmicro-videos with tags. Existing methods mostly focus on analyzing video\ncontent, neglecting users' social influence and tag relation. Meanwhile,\nexisting tag relation construction methods suffer from either deficient\nperformance or low tag coverage. To jointly model social influence and tag\nrelation, we formulate micro-video tagging as a link prediction problem in a\nconstructed heterogeneous network. Specifically, the tag relation (represented\nby tag ontology) is constructed in a semi-supervised manner. Then, we combine\ntag relation, video-tag annotation, and user-follow relation to build the\nnetwork. Afterward, a better video and tag representation are derived through\nBehavior Spread modeling and visual and linguistic knowledge aggregation.\nFinally, the semantic similarity between each micro-video and all candidate\ntags is calculated in this video-tag network. Extensive experiments on\nindustrial datasets of three verticals verify the superiority of our model\ncompared with several state-of-the-art baselines.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.08318v1"
    },
    {
        "title": "Progressive Frame Patching for FoV-based Point Cloud Video Streaming",
        "authors": [
            "Tongyu Zong",
            "Yixiang Mao",
            "Chen Li",
            "Yong Liu",
            "Yao Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Many XR applications require the delivery of volumetric video to users with\nsix degrees of freedom (6-DoF) movements. Point Cloud has become a popular\nvolumetric video format. A dense point cloud consumes much higher bandwidth\nthan a 2D/360 degree video frame. User Field of View (FoV) is more dynamic with\n6-DoF movement than 3-DoF movement. To save bandwidth, FoV-adaptive streaming\npredicts a user's FoV and only downloads point cloud data falling in the\npredicted FoV. However, it is vulnerable to FoV prediction errors, which can be\nsignificant when a long buffer is utilized for smoothed streaming. In this\nwork, we propose a multi-round progressive refinement framework for point cloud\nvideo streaming. Instead of sequentially downloading point cloud frames, our\nsolution simultaneously downloads/patches multiple frames falling into a\nsliding time-window, leveraging the inherent scalability of octree-based\npoint-cloud coding. The optimal rate allocation among all tiles of active\nframes are solved analytically using the heterogeneous tile rate-quality\nfunctions calibrated by the predicted user FoV. Multi-frame\ndownloading/patching simultaneously takes advantage of the streaming smoothness\nresulting from long buffer and the FoV prediction accuracy at short buffer\nlength. We evaluate our streaming solution using simulations driven by real\npoint cloud videos, real bandwidth traces, and 6-DoF FoV traces of real users.\nOur solution is robust against the bandwidth/FoV prediction errors, and can\ndeliver high and smooth view quality in the face of bandwidth variations and\ndynamic user and point cloud movements.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.08336v3"
    },
    {
        "title": "Learning a Practical SDR-to-HDRTV Up-conversion using New Dataset and\n  Degradation Models",
        "authors": [
            "Cheng Guo",
            "Leidong Fan",
            "Ziyu Xue",
            "and Xiuhua Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In media industry, the demand of SDR-to-HDRTV up-conversion arises when users\npossess HDR-WCG (high dynamic range-wide color gamut) TVs while most\noff-the-shelf footage is still in SDR (standard dynamic range). The research\ncommunity has started tackling this low-level vision task by learning-based\napproaches. When applied to real SDR, yet, current methods tend to produce dim\nand desaturated result, making nearly no improvement on viewing experience.\nDifferent from other network-oriented methods, we attribute such deficiency to\ntraining set (HDR-SDR pair). Consequently, we propose new HDRTV dataset (dubbed\nHDRTV4K) and new HDR-to-SDR degradation models. Then, it's used to train a\nluminance-segmented network (LSN) consisting of a global mapping trunk, and two\nTransformer branches on bright and dark luminance range. We also update\nassessment criteria by tailored metrics and subjective experiment. Finally,\nablation studies are conducted to prove the effectiveness. Our work is\navailable at: https://github.com/AndreGuo/HDRTVDM.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.13031v1"
    },
    {
        "title": "Factor Decomposed Generative Adversarial Networks for Text-to-Image\n  Synthesis",
        "authors": [
            "Jiguo Li",
            "Xiaobin Liu",
            "Lirong Zheng"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Prior works about text-to-image synthesis typically concatenated the sentence\nembedding with the noise vector, while the sentence embedding and the noise\nvector are two different factors, which control the different aspects of the\ngeneration. Simply concatenating them will entangle the latent factors and\nencumber the generative model.\n  In this paper, we attempt to decompose these two factors and propose Factor\nDecomposed Generative Adversarial Networks~(FDGAN). To achieve this, we firstly\ngenerate images from the noise vector and then apply the sentence embedding in\nthe normalization layer for both generator and discriminators. We also design\nan additive norm layer to align and fuse the text-image features. The\nexperimental results show that decomposing the noise and the sentence embedding\ncan disentangle latent factors in text-to-image synthesis, and make the\ngenerative model more efficient. Compared with the baseline, FDGAN can achieve\nbetter performance, while fewer parameters are used.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.13821v1"
    },
    {
        "title": "XGC-VQA: A unified video quality assessment model for User,\n  Professionally, and Occupationally-Generated Content",
        "authors": [
            "Xinhui Huang",
            "Chunyi Li",
            "Abdelhak Bentaleb",
            "Roger Zimmermann",
            "Guangtao Zhai"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  With the rapid growth of Internet video data amounts and types, a unified\nVideo Quality Assessment (VQA) is needed to inspire video communication with\nperceptual quality. To meet the real-time and universal requirements in\nproviding such inspiration, this study proposes a VQA model from a\nclassification of User Generated Content (UGC), Professionally Generated\nContent (PGC), and Occupationally Generated Content (OGC). In the time domain,\nthis study utilizes non-uniform sampling, as each content type has varying\ntemporal importance based on its perceptual quality. In the spatial domain,\ncentralized downsampling is performed before the VQA process by utilizing a\npatch splicing/sampling mechanism to lower complexity for real-time assessment.\nThe experimental results demonstrate that the proposed method achieves a median\ncorrelation of $0.7$ while limiting the computation time below 5s for three\ncontent types, which ensures that the communication experience of UGC, PGC, and\nOGC can be optimized altogether.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.13859v1"
    },
    {
        "title": "Enhancing Multimodal Entity and Relation Extraction with Variational\n  Information Bottleneck",
        "authors": [
            "Shiyao Cui",
            "Jiangxia Cao",
            "Xin Cong",
            "Jiawei Sheng",
            "Quangang Li",
            "Tingwen Liu",
            "Jinqiao Shi"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  This paper studies the multimodal named entity recognition (MNER) and\nmultimodal relation extraction (MRE), which are important for multimedia social\nplatform analysis. The core of MNER and MRE lies in incorporating evident\nvisual information to enhance textual semantics, where two issues inherently\ndemand investigations. The first issue is modality-noise, where the\ntask-irrelevant information in each modality may be noises misleading the task\nprediction. The second issue is modality-gap, where representations from\ndifferent modalities are inconsistent, preventing from building the semantic\nalignment between the text and image. To address these issues, we propose a\nnovel method for MNER and MRE by Multi-Modal representation learning with\nInformation Bottleneck (MMIB). For the first issue, a refinement-regularizer\nprobes the information-bottleneck principle to balance the predictive evidence\nand noisy information, yielding expressive representations for prediction. For\nthe second issue, an alignment-regularizer is proposed, where a mutual\ninformation-based item works in a contrastive manner to regularize the\nconsistent text-image representations. To our best knowledge, we are the first\nto explore variational IB estimation for MNER and MRE. Experiments show that\nMMIB achieves the state-of-the-art performances on three public benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.02328v1"
    },
    {
        "title": "Improving Adaptive Real-Time Video Communication Via Cross-layer\n  Optimization",
        "authors": [
            "Yueheng Li",
            "Hao Chen",
            "Bowei Xu",
            "Zicheng Zhang",
            "Zhan Ma"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Effective Adaptive BitRate (ABR) algorithm or policy is of paramount\nimportance for Real-Time Video Communication (RTVC) amid this pandemic to\npursue uncompromised quality of experience (QoE). Existing ABR methods mainly\nseparate the network bandwidth estimation and video encoder control, and\nfine-tune video bitrate towards estimated bandwidth, assuming the maximization\nof bandwidth utilization yields the optimal QoE. However, the QoE of a RTVC\nsystem is jointly determined by the quality of compressed video, fluency of\nvideo playback, and interaction delay. Solely maximizing the bandwidth\nutilization without comprehensively considering compound impacts incurred by\nboth network and video application layers, does not assure the satisfactory\nQoE. And the decoupling of network and video layer further exacerbates the user\nexperience due to network-codec incoordination. This work therefore proposes\nthe Palette, a reinforcement learning based ABR scheme that unifies the\nprocessing of network and video application layers to directly maximize the QoE\nformulated as the weighted function of video quality, stalling rate and delay.\nTo this aim, a cross-layer optimization is proposed to derive fine-grained\ncompression factor of upcoming frame(s) using cross-layer observations like\nnetwork conditions, video encoding parameters, and video content complexity. As\na result, Palette manages to resolve the network-codec incoordination and to\nbest catch up with the network fluctuation. Compared with state-of-the-art\nschemes in real-world tests, Palette not only reduces 3.1%-46.3% of the\nstalling rate, 20.2%-50.8% of the delay, but also improves 0.2%-7.2% of the\nvideo quality with comparable bandwidth consumption, under a variety of\napplication scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.03505v2"
    },
    {
        "title": "Improving ABR Performance for Short Video Streaming Using Multi-Agent\n  Reinforcement Learning with Expert Guidance",
        "authors": [
            "Yueheng Li",
            "Qianyuan Zheng",
            "Zicheng Zhang",
            "Hao Chen",
            "Zhan Ma"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In the realm of short video streaming, popular adaptive bitrate (ABR)\nalgorithms developed for classical long video applications suffer from\ncatastrophic failures because they are tuned to solely adapt bitrates. Instead,\nshort video adaptive bitrate (SABR) algorithms have to properly determine which\nvideo at which bitrate level together for content prefetching, without\nsacrificing the users' quality of experience (QoE) and yielding noticeable\nbandwidth wastage jointly. Unfortunately, existing SABR methods are inevitably\nentangled with slow convergence and poor generalization. Thus, in this paper,\nwe propose Incendio, a novel SABR framework that applies Multi-Agent\nReinforcement Learning (MARL) with Expert Guidance to separate the decision of\nvideo ID and video bitrate in respective buffer management and bitrate\nadaptation agents to maximize the system-level utilized score modeled as a\ncompound function of QoE and bandwidth wastage metrics. To train Incendio, it\nis first initialized by imitating the hand-crafted expert rules and then\nfine-tuned through the use of MARL. Results from extensive experiments indicate\nthat Incendio outperforms the current state-of-the-art SABR algorithm with a\n53.2% improvement measured by the utility score while maintaining low training\ncomplexity and inference time.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.04637v1"
    },
    {
        "title": "Movie Box Office Prediction With Self-Supervised and Visually Grounded\n  Pretraining",
        "authors": [
            "Qin Chao",
            "Eunsoo Kim",
            "Boyang Li"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Investments in movie production are associated with a high level of risk as\nmovie revenues have long-tailed and bimodal distributions. Accurate prediction\nof box-office revenue may mitigate the uncertainty and encourage investment.\nHowever, learning effective representations for actors, directors, and\nuser-generated content-related keywords remains a challenging open problem. In\nthis work, we investigate the effects of self-supervised pretraining and\npropose visual grounding of content keywords in objects from movie posters as a\npertaining objective. Experiments on a large dataset of 35,794 movies\ndemonstrate significant benefits of self-supervised training and visual\ngrounding. In particular, visual grounding pretraining substantially improves\nlearning on movies with content keywords and achieves 14.5% relative\nperformance gains compared to a finetuned BERT model with identical\narchitecture.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.10311v1"
    },
    {
        "title": "NFT Marketplace",
        "authors": [
            "Piyush Batra",
            "Gagan Raj Singh",
            "Ritik Gandhi"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In an increasingly digitized world, the secure management and trade of\ndigital assets have become a pressing issue. This project aims to address this\nchallenge by developing a decentralized application (dApp) that leverages\nblockchain technology and deep learning models to provide secure and efficient\ndigital asset management, with a focus on NFTs. The dApp includes features such\nas secure wallet connections, NFT image generation, minting, marketplace, and\nprofile management. The back-end of the dApp is implemented using the Goerli\ntestnet with Solidity-based smart contracts, while IPFS and ReactJS/EtherJS are\nused for decentralized storage and front-end development, respectively.\nAdditionally, the OpenAI API is integrated to generate unique NFT images based\non user input. The project demonstrates the practical application of blockchain\ntechnology and deep learning models in developing dApps for secure and\ndecentralized digital asset management. Overall, the project contributes to the\nongoing research on blockchain-based solutions for secure digital asset\nmanagement, while highlighting the potential of blockchain and deep learning\ntechnologies to transform the way we manage and trade digital assets.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.10632v1"
    },
    {
        "title": "Images Within Images? A Multi-image Paradigm with Novel Key-Value Graph\n  Oriented Steganography",
        "authors": [
            "Subhrangshu Adhikary"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Steganographic methods have been in the limelight of research and development\nfor concealing secret data within a cover media without being noticed through\ngeneral visualization. The Least Significant Bits (LSBs) of 8-bit color code\nfor the RGB image arises the possibility of replacing the last two bits with\nthe bits of the encrypted message. Several procedures have been developed to\nhide an image within another image however in most cases the payload image has\nto be within the accommodatable range of the cover image and very little\nliterature have shown methods to hide multiple images within multiple images.\nThis paper presents a novel approach to split the image into JSON styled\ndictionary of key-value pairs and using a metadata graph to locate different\nparts and positions of the payload images in the entire cluster of cover\nimages. The model could be easily used in the real world scenario for privately\nsharing secret data over public communication channels without being noticed.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.11720v1"
    },
    {
        "title": "Generative Steganography Diffusion",
        "authors": [
            "Ping Wei",
            "Qing Zhou",
            "Zichi Wang",
            "Zhenxing Qian",
            "Xinpeng Zhang",
            "Sheng Li"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Generative steganography (GS) is an emerging technique that generates stego\nimages directly from secret data. Various GS methods based on GANs or Flow have\nbeen developed recently. However, existing GAN-based GS methods cannot\ncompletely recover the hidden secret data due to the lack of network\ninvertibility, while Flow-based methods produce poor image quality due to the\nstringent reversibility restriction in each module. To address this issue, we\npropose a novel GS scheme called \"Generative Steganography Diffusion\" (GSD) by\ndevising an invertible diffusion model named \"StegoDiffusion\". It not only\ngenerates realistic stego images but also allows for 100\\% recovery of the\nhidden secret data. The proposed StegoDiffusion model leverages a non-Markov\nchain with a fast sampling technique to achieve efficient stego image\ngeneration. By constructing an ordinary differential equation (ODE) based on\nthe transition probability of the generation process in StegoDiffusion, secret\ndata and stego images can be converted to each other through the approximate\nsolver of ODE -- Euler iteration formula, enabling the use of irreversible but\nmore expressive network structures to achieve model invertibility. Our proposed\nGSD has the advantages of both reversibility and high performance,\nsignificantly outperforming existing GS methods in all metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03472v2"
    },
    {
        "title": "DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for\n  Identifying Large Language Model Generated Text",
        "authors": [
            "Travis Munyer",
            "Abdullah Tanvir",
            "Arjon Das",
            "Xin Zhong"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of text generators. With the potential for misuse\nescalating, the importance of discerning whether texts are human-authored or\ngenerated by LLMs has become paramount. Several preceding studies have ventured\nto address this challenge by employing binary classifiers to differentiate\nbetween human-written and LLM-generated text. Nevertheless, the reliability of\nthese classifiers has been subject to question. Given that consequential\ndecisions may hinge on the outcome of such classification, it is imperative\nthat text source detection is of high caliber. In light of this, the present\npaper introduces DeepTextMark, a deep learning-driven text watermarking\nmethodology devised for text source identification. By leveraging Word2Vec and\nSentence Encoding for watermark insertion, alongside a transformer-based\nclassifier for watermark detection, DeepTextMark epitomizes a blend of\nblindness, robustness, imperceptibility, and reliability. As elaborated within\nthe paper, these attributes are crucial for universal text source detection,\nwith a particular emphasis in this paper on text produced by LLMs. DeepTextMark\noffers a viable \"add-on\" solution to prevailing text generation frameworks,\nrequiring no direct access or alterations to the underlying text generation\nmechanism. Experimental evaluations underscore the high imperceptibility,\nelevated detection accuracy, augmented robustness, reliability, and swift\nexecution of DeepTextMark.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.05773v2"
    },
    {
        "title": "Circular Rectifiction of 3D Video and Efficient Modification of 3D-HEVC",
        "authors": [
            "Jarosław Samelak",
            "Marek Domański"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Video acquired from multiple cameras located along a line is often rectified\nto video virtually obtained from cameras with ideally parallel optical axes\ncollocated on a single plane and principal points on a line. Such an approach\nsimplifies video processing including depth estimation and compression.\nNowadays, for many application video, like virtual reality or virtual\nnavigation, the content is often acquired by cameras located nearly on a circle\nor on a part of that. Therefore, we introduce new operation of circular\nrectification that results in multiview video virtually obtained from cameras\nlocated on an ideal arc and with optical axes that are collocated on a single\nplane and they intersect in a single point. For the circularly rectified video,\ndepth estimation and compression are simplified. The standard 3DHEVC codec was\ndesigned for rectified video and its efficiency is limited for video acquired\nfrom cameras located on an arc. Therefore, we developed a 3-D HEVC codec\nmodified in order to compress efficiently circularly rectified video. The\nexperiments demonstrate its better performance than for the standard 3D-HEVC\ncodec.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.06285v1"
    },
    {
        "title": "Towards Balanced Active Learning for Multimodal Classification",
        "authors": [
            "Meng Shen",
            "Yizheng Huang",
            "Jianxiong Yin",
            "Heqing Zou",
            "Deepu Rajan",
            "Simon See"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Training multimodal networks requires a vast amount of data due to their\nlarger parameter space compared to unimodal networks. Active learning is a\nwidely used technique for reducing data annotation costs by selecting only\nthose samples that could contribute to improving model performance. However,\ncurrent active learning strategies are mostly designed for unimodal tasks, and\nwhen applied to multimodal data, they often result in biased sample selection\nfrom the dominant modality. This unfairness hinders balanced multimodal\nlearning, which is crucial for achieving optimal performance. To address this\nissue, we propose three guidelines for designing a more balanced multimodal\nactive learning strategy. Following these guidelines, a novel approach is\nproposed to achieve more fair data selection by modulating the gradient\nembedding with the dominance degree among modalities. Our studies demonstrate\nthat the proposed method achieves more balanced multimodal learning by avoiding\ngreedy sample selection from the dominant modality. Our approach outperforms\nexisting active learning strategies on a variety of multimodal classification\ntasks. Overall, our work highlights the importance of balancing sample\nselection in multimodal active learning and provides a practical solution for\nachieving more balanced active learning for multimodal classification.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.08306v2"
    },
    {
        "title": "Training Multimedia Event Extraction With Generated Images and Captions",
        "authors": [
            "Zilin Du",
            "Yunxin Li",
            "Xu Guo",
            "Yidan Sun",
            "Boyang Li"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Contemporary news reporting increasingly features multimedia content,\nmotivating research on multimedia event extraction. However, the task lacks\nannotated multimodal training data and artificially generated training data\nsuffer from distribution shift from real-world data. In this paper, we propose\nCross-modality Augmented Multimedia Event Learning (CAMEL), which successfully\nutilizes artificially generated multimodal training data and achieves\nstate-of-the-art performance. We start with two labeled unimodal datasets in\ntext and image respectively, and generate the missing modality using\noff-the-shelf image generators like Stable Diffusion and image captioners like\nBLIP. After that, we train the network on the resultant multimodal datasets. In\norder to learn robust features that are effective across domains, we devise an\niterative and gradual training strategy. Substantial experiments show that\nCAMEL surpasses state-of-the-art (SOTA) baselines on the M2E2 benchmark. On\nmultimedia events in particular, we outperform the prior SOTA by 4.2% F1 on\nevent mention identification and by 9.8% F1 on argument identification, which\nindicates that CAMEL learns synergistic representations from the two\nmodalities. Our work demonstrates a recipe to unleash the power of synthetic\ntraining data in structured prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.08966v2"
    },
    {
        "title": "FLiCR: A Fast and Lightweight LiDAR Point Cloud Compression Based on\n  Lossy RI",
        "authors": [
            "Jin Heo",
            "Christopher Phillips",
            "Ada Gavrilovska"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Light detection and ranging (LiDAR) sensors are becoming available on modern\nmobile devices and provide a 3D sensing capability. This new capability is\nbeneficial for perceptions in various use cases, but it is challenging for\nresource-constrained mobile devices to use the perceptions in real-time because\nof their high computational complexity. In this context, edge computing can be\nused to enable LiDAR online perceptions, but offloading the perceptions on the\nedge server requires a low-latency, lightweight, and efficient compression due\nto the large volume of LiDAR point clouds data.\n  This paper presents FLiCR, a fast and lightweight LiDAR point cloud\ncompression method for enabling edge-assisted online perceptions. FLiCR is\nbased on range images (RI) as an intermediate representation (IR), and\ndictionary coding for compressing RIs. FLiCR achieves its benefits by\nleveraging lossy RIs, and we show the efficiency of bytestream compression is\nlargely improved with quantization and subsampling. In addition, we identify\nthe limitation of current quality metrics for presenting the entropy of a point\ncloud, and introduce a new metric that reflects both point-wise and\nentropy-wise qualities for lossy IRs. The evaluation results show FLiCR is more\nsuitable for edge-assisted real-time perceptions than the existing LiDAR\ncompressions, and we demonstrate the effectiveness of our compression and\nmetric with the evaluations on 3D object detection and LiDAR SLAM.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15005v1"
    },
    {
        "title": "Visual Captioning at Will: Describing Images and Videos Guided by a Few\n  Stylized Sentences",
        "authors": [
            "Dingyi Yang",
            "Hongyu Chen",
            "Xinglin Hou",
            "Tiezheng Ge",
            "Yuning Jiang",
            "Qin Jin"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Stylized visual captioning aims to generate image or video descriptions with\nspecific styles, making them more attractive and emotionally appropriate. One\nmajor challenge with this task is the lack of paired stylized captions for\nvisual content, so most existing works focus on unsupervised methods that do\nnot rely on parallel datasets. However, these approaches still require training\nwith sufficient examples that have style labels, and the generated captions are\nlimited to predefined styles. To address these limitations, we explore the\nproblem of Few-Shot Stylized Visual Captioning, which aims to generate captions\nin any desired style, using only a few examples as guidance during inference,\nwithout requiring further training. We propose a framework called FS-StyleCap\nfor this task, which utilizes a conditional encoder-decoder language model and\na visual projection module. Our two-step training scheme proceeds as follows:\nfirst, we train a style extractor to generate style representations on an\nunlabeled text-only corpus. Then, we freeze the extractor and enable our\ndecoder to generate stylized descriptions based on the extracted style vector\nand projected visual content vectors. During inference, our model can generate\ndesired stylized captions by deriving the style representation from\nuser-supplied examples. Our automatic evaluation results for few-shot\nsentimental visual captioning outperform state-of-the-art approaches and are\ncomparable to models that are fully trained on labeled style corpora. Human\nevaluations further confirm our model s ability to handle multiple styles.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.16399v1"
    },
    {
        "title": "An Efficient Recommendation System in E-commerce using Passer learning\n  optimization based on Bi-LSTM",
        "authors": [
            "Hemn Barzan Abdalla",
            "Awder Ahmed",
            "Bahtiyar Mehmed",
            "Mehdi Gheisari",
            "Maryam Cheraghy"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Recommendation system services have become crucial for users to access\npersonalized goods or services as the global e-commerce market expands. They\ncan increase business sales growth and lower the cost of user information\nexploration. Recent years have seen a signifi-cant increase in researchers\nactively using user reviews to solve standard recommender system research\nissues. Reviews may, however, contain information that does not help consumers\nde-cide what to buy, such as advertising or fictitious or fake reviews. Using\nsuch reviews to offer suggestion services may reduce the effectiveness of those\nrecommendations. In this research, the recommendation in e-commerce is\ndeveloped using passer learning optimization based on Bi-LSTM to solve that\nissue (PL optimized Bi-LSTM). Data is first obtained from the product\nrecommendation dataset and pre-processed to remove any values that are missing\nor incon-sistent. Then, feature extraction is performed using TF-IDF features\nand features that support graph embedding. Before submitting numerous features\nwith the same dimensions to the Bi-LSTM classifier for analysis, they are\nintegrated using the feature concatenation approach. The Collaborative Bi-LSTM\nmethod employs these features to determine if the model is a recommended\nproduct. The PL optimization approach, which efficiently adjusts the\nclassifier's parameters and produces an extract output that measures the\nf1-score, MSE, precision, and recall, is the basis of this research's\ncontributions. As compared to earlier methods, the pro-posed PL-optimized\nBi-LSTM achieved values of 88.58%, 1.24%, 92.69%, and 92.69% for dataset 1,\n88.46%, 0.48%, 92.43%, and 93.47% for dataset 2, and 92.51%, 1.58%, 91.90%, and\n90.76% for dataset 3.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.00137v2"
    },
    {
        "title": "PromptCARE: Prompt Copyright Protection by Watermark Injection and\n  Verification",
        "authors": [
            "Hongwei Yao",
            "Jian Lou",
            "Kui Ren",
            "Zhan Qin"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Large language models (LLMs) have witnessed a meteoric rise in popularity\namong the general public users over the past few months, facilitating diverse\ndownstream tasks with human-level accuracy and proficiency. Prompts play an\nessential role in this success, which efficiently adapt pre-trained LLMs to\ntask-specific applications by simply prepending a sequence of tokens to the\nquery texts. However, designing and selecting an optimal prompt can be both\nexpensive and demanding, leading to the emergence of Prompt-as-a-Service\nproviders who profit by providing well-designed prompts for authorized use.\nWith the growing popularity of prompts and their indispensable role in\nLLM-based services, there is an urgent need to protect the copyright of prompts\nagainst unauthorized use.\n  In this paper, we propose PromptCARE, the first framework for prompt\ncopyright protection through watermark injection and verification. Prompt\nwatermarking presents unique challenges that render existing watermarking\ntechniques developed for model and dataset copyright verification ineffective.\nPromptCARE overcomes these hurdles by proposing watermark injection and\nverification schemes tailor-made for prompts and NLP characteristics. Extensive\nexperiments on six well-known benchmark datasets, using three prevalent\npre-trained LLMs (BERT, RoBERTa, and Facebook OPT-1.3b), demonstrate the\neffectiveness, harmlessness, robustness, and stealthiness of PromptCARE.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.02816v2"
    },
    {
        "title": "Optimizing Adaptive Video Streaming with Human Feedback",
        "authors": [
            "Tianchi Huang",
            "Rui-Xiao Zhang",
            "Chenglei Wu",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Quality of Experience~(QoE)-driven adaptive bitrate (ABR) algorithms are\ntypically optimized using QoE models that are based on the mean opinion\nscore~(MOS), while such principles may not account for user heterogeneity on\nrating scales, resulting in unexpected behaviors. In this paper, we propose\nJade, which leverages reinforcement learning with human feedback~(RLHF)\ntechnologies to better align the users' opinion scores. Jade's rank-based QoE\nmodel considers relative values of user ratings to interpret the subjective\nperception of video sessions. We implement linear-based and Deep Neural Network\n(DNN)-based architectures for satisfying both accuracy and generalization\nability. We further propose entropy-aware reinforced mechanisms for training\npolicies with the integration of the proposed QoE models. Experimental results\ndemonstrate that Jade performs favorably on conventional metrics, such as\nquality and stall ratio, and improves QoE by 8.09%-38.13% in different network\nconditions, emphasizing the importance of user heterogeneity in QoE modeling\nand the potential of combining linear-based and DNN-based models for\nperformance improvement.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.04132v2"
    },
    {
        "title": "LSCD: A Large-Scale Screen Content Dataset for Video Compression",
        "authors": [
            "Yuhao Cheng",
            "Siru Zhang",
            "Yiqiang Yan",
            "Rong Chen",
            "Yun Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Multimedia compression allows us to watch videos, see pictures and hear\nsounds within a limited bandwidth, which helps the flourish of the internet.\nDuring the past decades, multimedia compression has achieved great success\nusing hand-craft features and systems. With the development of artificial\nintelligence and video compression, there emerges a lot of research work\nrelated to using the neural network on the video compression task to get rid of\nthe complicated system. Not only producing the advanced algorithms, but\nresearchers also spread the compression to different content, such as User\nGenerated Content(UGC). With the rapid development of mobile devices, screen\ncontent videos become an important part of multimedia data. In contrast, we\nfind community lacks a large-scale dataset for screen content video\ncompression, which impedes the fast development of the corresponding\nlearning-based algorithms. In order to fulfill this blank and accelerate the\nresearch of this special type of videos, we propose the Large-scale Screen\nContent Dataset(LSCD), which contains 714 source sequences. Meanwhile, we\nprovide the analysis of the proposed dataset to show some features of screen\ncontent videos, which will help researchers have a better understanding of how\nto explore new algorithms. Besides collecting and post-processing the data to\norganize the dataset, we also provide a benchmark containing the performance of\nboth traditional codec and learning-based methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.09332v1"
    },
    {
        "title": "MMAPS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product\n  Summarization",
        "authors": [
            "Tao Chen",
            "Ze Lin",
            "Hui Li",
            "Jiayi Ji",
            "Yiyi Zhou",
            "Guanbin Li",
            "Rongrong Ji"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Given the long textual product information and the product image, Multi-modal\nProduct Summarization (MPS) aims to increase customers' desire to purchase by\nhighlighting product characteristics with a short textual summary. Existing MPS\nmethods can produce promising results. Nevertheless, they still 1) lack\nend-to-end product summarization, 2) lack multi-grained multi-modal modeling,\nand 3) lack multi-modal attribute modeling. To improve MPS, we propose an\nend-to-end multi-grained multi-modal attribute-aware product summarization\nmethod (MMAPS) for generating high-quality product summaries in e-commerce.\nMMAPS jointly models product attributes and generates product summaries. We\ndesign several multi-grained multi-modal tasks to better guide the multi-modal\nlearning of MMAPS. Furthermore, we model product attributes based on both text\nand image modalities so that multi-modal product characteristics can be\nmanifested in the generated summaries. Extensive experiments on a real\nlarge-scale Chinese e-commence dataset demonstrate that our model outperforms\nstate-of-the-art product summarization methods w.r.t. several summarization\nmetrics. Our code is publicly available at: https://github.com/KDEGroup/MMAPS.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.11351v2"
    },
    {
        "title": "UMMAFormer: A Universal Multimodal-adaptive Transformer Framework for\n  Temporal Forgery Localization",
        "authors": [
            "Rui Zhang",
            "Hongxia Wang",
            "Mingshan Du",
            "Hanqing Liu",
            "Yang Zhou",
            "Qiang Zeng"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The emergence of artificial intelligence-generated content (AIGC) has raised\nconcerns about the authenticity of multimedia content in various fields.\nHowever, existing research for forgery content detection has focused mainly on\nbinary classification tasks of complete videos, which has limited applicability\nin industrial settings. To address this gap, we propose UMMAFormer, a novel\nuniversal transformer framework for temporal forgery localization (TFL) that\npredicts forgery segments with multimodal adaptation. Our approach introduces a\nTemporal Feature Abnormal Attention (TFAA) module based on temporal feature\nreconstruction to enhance the detection of temporal differences. We also design\na Parallel Cross-Attention Feature Pyramid Network (PCA-FPN) to optimize the\nFeature Pyramid Network (FPN) for subtle feature enhancement. To evaluate the\nproposed method, we contribute a novel Temporal Video Inpainting Localization\n(TVIL) dataset specifically tailored for video inpainting scenes. Our\nexperiments show that our approach achieves state-of-the-art performance on\nbenchmark datasets, including Lav-DF, TVIL, and Psynd, significantly\noutperforming previous methods. The code and data are available at\nhttps://github.com/ymhzyj/UMMAFormer/.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.14395v1"
    },
    {
        "title": "Spatial Perceptual Quality Aware Adaptive Volumetric Video Streaming",
        "authors": [
            "Xi Wang",
            "Wei Liu",
            "Huitong Liu",
            "Peng Yang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Volumetric video offers a highly immersive viewing experience, but poses\nchallenges in ensuring quality of experience (QoE) due to its high bandwidth\nrequirements. In this paper, we explore the effect of viewing distance\nintroduced by six degrees of freedom (6DoF) spatial navigation on user's\nperceived quality. By considering human visual resolution limitations, we\npropose a visual acuity model that describes the relationship between the\nvirtual viewing distance and the tolerable boundary point cloud density. The\nproposed model satisfies spatial visual requirements during 6DoF exploration.\nAdditionally, it dynamically adjusts quality levels to balance perceptual\nquality and bandwidth consumption. Furthermore, we present a QoE model to\nrepresent user's perceived quality at different viewing distances precisely.\nExtensive experimental results demonstrate that, the proposed scheme can\neffectively improve the overall average QoE by up to 26% over real networks and\nuser traces, compared to existing baselines.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.05026v1"
    },
    {
        "title": "MPAI-EEV: Standardization Efforts of Artificial Intelligence based\n  End-to-End Video Coding",
        "authors": [
            "Chuanmin Jia",
            "Feng Ye",
            "Fanke Dong",
            "Kai Lin",
            "Leonardo Chiariglione",
            "Siwei Ma",
            "Huifang Sun",
            "Wen Gao"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The rapid advancement of artificial intelligence (AI) technology has led to\nthe prioritization of standardizing the processing, coding, and transmission of\nvideo using neural networks. To address this priority area, the Moving Picture,\nAudio, and Data Coding by Artificial Intelligence (MPAI) group is developing a\nsuite of standards called MPAI-EEV for \"end-to-end optimized neural video\ncoding.\" The aim of this AI-based video standard project is to compress the\nnumber of bits required to represent high-fidelity video data by utilizing\ndata-trained neural coding technologies. This approach is not constrained by\nhow data coding has traditionally been applied in the context of a hybrid\nframework. This paper presents an overview of recent and ongoing\nstandardization efforts in this area and highlights the key technologies and\ndesign philosophy of EEV. It also provides a comparison and report on some\nprimary efforts such as the coding efficiency of the reference model.\nAdditionally, it discusses emerging activities such as learned\nUnmanned-Aerial-Vehicles (UAVs) video coding which are currently planned, under\ndevelopment, or in the exploration phase. With a focus on UAV video signals,\nthis paper addresses the current status of these preliminary efforts. It also\nindicates development timelines, summarizes the main technical details, and\nprovides pointers to further points of reference. The exploration experiment\nshows that the EEV model performs better than the state-of-the-art video coding\nstandard H.266/VVC in terms of perceptual evaluation metric.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.07589v1"
    },
    {
        "title": "CLIP-Hand3D: Exploiting 3D Hand Pose Estimation via Context-Aware\n  Prompting",
        "authors": [
            "Shaoxiang Guo",
            "Qing Cai",
            "Lin Qi",
            "Junyu Dong"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Contrastive Language-Image Pre-training (CLIP) starts to emerge in many\ncomputer vision tasks and has achieved promising performance. However, it\nremains underexplored whether CLIP can be generalized to 3D hand pose\nestimation, as bridging text prompts with pose-aware features presents\nsignificant challenges due to the discrete nature of joint positions in 3D\nspace. In this paper, we make one of the first attempts to propose a novel 3D\nhand pose estimator from monocular images, dubbed as CLIP-Hand3D, which\nsuccessfully bridges the gap between text prompts and irregular detailed pose\ndistribution. In particular, the distribution order of hand joints in various\n3D space directions is derived from pose labels, forming corresponding text\nprompts that are subsequently encoded into text representations.\nSimultaneously, 21 hand joints in the 3D space are retrieved, and their spatial\ndistribution (in x, y, and z axes) is encoded to form pose-aware features.\nSubsequently, we maximize semantic consistency for a pair of pose-text features\nfollowing a CLIP-based contrastive learning paradigm. Furthermore, a\ncoarse-to-fine mesh regressor is designed, which is capable of effectively\nquerying joint-aware cues from the feature pyramid. Extensive experiments on\nseveral public hand benchmarks show that the proposed model attains a\nsignificantly faster inference speed while achieving state-of-the-art\nperformance compared to methods utilizing the similar scale backbone.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.16140v1"
    },
    {
        "title": "Redistributing the Precision and Content in 3D-LUT-based Inverse\n  Tone-mapping for HDR/WCG Display",
        "authors": [
            "Cheng Guo",
            "Leidong Fan",
            "Qian Zhang",
            "Hanyuan Liu",
            "Kanglin Liu",
            "Xiuhua Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  ITM(inverse tone-mapping) converts SDR (standard dynamic range) footage to\nHDR/WCG (high dynamic range /wide color gamut) for media production. It happens\nnot only when remastering legacy SDR footage in front-end content provider, but\nalso adapting on-theair SDR service on user-end HDR display. The latter\nrequires more efficiency, thus the pre-calculated LUT (look-up table) has\nbecome a popular solution. Yet, conventional fixed LUT lacks adaptability, so\nwe learn from research community and combine it with AI. Meanwhile,\nhigher-bit-depth HDR/WCG requires larger LUT than SDR, so we consult\ntraditional ITM for an efficiency-performance trade-off: We use 3 smaller LUTs,\neach has a non-uniform packing (precision) respectively denser in dark, middle\nand bright luma range. In this case, their results will have less error only in\ntheir own range, so we use a contribution map to combine their best parts to\nfinal result. With the guidance of this map, the elements (content) of 3 LUTs\nwill also be redistributed during training. We conduct ablation studies to\nverify method's effectiveness, and subjective and objective experiments to show\nits practicability. Code is available at: https://github.com/AndreGuo/ITMLUT.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.17160v2"
    },
    {
        "title": "Robust Image Watermarking based on Cross-Attention and Invariant Domain\n  Learning",
        "authors": [
            "Agnibh Dasgupta",
            "Xin Zhong"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Image watermarking involves embedding and extracting watermarks within a\ncover image, with deep learning approaches emerging to bolster generalization\nand robustness. Predominantly, current methods employ convolution and\nconcatenation for watermark embedding, while also integrating conceivable\naugmentation in the training process. This paper explores a robust image\nwatermarking methodology by harnessing cross-attention and invariant domain\nlearning, marking two novel, significant advancements. First, we design a\nwatermark embedding technique utilizing a multi-head cross attention mechanism,\nenabling information exchange between the cover image and watermark to identify\nsemantically suitable embedding locations. Second, we advocate for learning an\ninvariant domain representation that encapsulates both semantic and\nnoise-invariant information concerning the watermark, shedding light on\npromising avenues for enhancing image watermarking techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.05395v1"
    },
    {
        "title": "Motion Vector-Domain Video Steganalysis Exploiting Skipped Macroblocks",
        "authors": [
            "Jun Li",
            "Minqing Zhang",
            "Ke Niu",
            "Yingnan Zhang",
            "Xiaoyuan Yang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Video steganography has the potential to be used to convey illegal\ninformation, and video steganalysis is a vital tool to detect the presence of\nthis illicit act. Currently, all the motion vector (MV)-based video\nsteganalysis algorithms extract feature sets directly on the MVs, but ignoring\nthe steganograhic operation may perturb the statistics distribution of other\nvideo encoding elements, such as the skipped macroblocks (no direct MVs). This\npaper proposes a novel 11-dimensional feature set to detect MV-based video\nsteganography based on the above observation. The proposed feature is extracted\nbased on the skipped macroblocks by recompression calibration. Specifically,\nthe feature consists of two components. The first is the probability\ndistribution of motion vector prediction (MVP) difference, and the second is\nthe probability distribution of partition state transfer. Extensive experiments\non different conditions demonstrate that the proposed feature set achieves good\ndetection accuracy, especially in lower embedding capacity. In addition, the\nloss of detection performance caused by recompression calibration using\nmismatched quantization parameters (QP) is within the acceptable range, so the\nproposed method can be used in practical scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.07121v1"
    },
    {
        "title": "Exploring Sparse Spatial Relation in Graph Inference for Text-Based VQA",
        "authors": [
            "Sheng Zhou",
            "Dan Guo",
            "Jia Li",
            "Xun Yang",
            "Meng Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Text-based visual question answering (TextVQA) faces the significant\nchallenge of avoiding redundant relational inference. To be specific, a large\nnumber of detected objects and optical character recognition (OCR) tokens\nresult in rich visual relationships. Existing works take all visual\nrelationships into account for answer prediction. However, there are three\nobservations: (1) a single subject in the images can be easily detected as\nmultiple objects with distinct bounding boxes (considered repetitive objects).\nThe associations between these repetitive objects are superfluous for answer\nreasoning; (2) two spatially distant OCR tokens detected in the image\nfrequently have weak semantic dependencies for answer reasoning; and (3) the\nco-existence of nearby objects and tokens may be indicative of important visual\ncues for predicting answers. Rather than utilizing all of them for answer\nprediction, we make an effort to identify the most important connections or\neliminate redundant ones. We propose a sparse spatial graph network (SSGN) that\nintroduces a spatially aware relation pruning technique to this task. As\nspatial factors for relation measurement, we employ spatial distance, geometric\ndimension, overlap area, and DIoU for spatially aware pruning. We consider\nthree visual relationships for graph learning: object-object, OCR-OCR tokens,\nand object-OCR token relationships. SSGN is a progressive graph learning\narchitecture that verifies the pivotal relations in the correlated object-token\nsparse graph, and then in the respective object-based sparse graph and\ntoken-based sparse graph. Experiment results on TextVQA and ST-VQA datasets\ndemonstrate that SSGN achieves promising performances. And some visualization\nresults further demonstrate the interpretability of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.09147v1"
    },
    {
        "title": "Soccer on Social Media",
        "authors": [
            "Mehdi Houshmand Sarkhoosh",
            "Sayed Mohammad Majidi Dorcheh",
            "Sushant Gautam",
            "Cise Midoglu",
            "Saeed Shafiee Sabet",
            "Pål Halvorsen"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In the era of digitalization, social media has become an integral part of our\nlives, serving as a significant hub for individuals and businesses to share\ninformation, communicate, and engage. This is also the case for professional\nsports, where leagues, clubs and players are using social media to reach out to\ntheir fans. In this respect, a huge amount of time is spent curating multimedia\ncontent for various social media platforms and their target users. With the\nemergence of Artificial Intelligence (AI), AI-based tools for automating\ncontent generation and enhancing user experiences on social media have become\nwidely popular. However, to effectively utilize such tools, it is imperative to\ncomprehend the demographics and preferences of users on different platforms,\nunderstand how content providers post information in these channels, and how\ndifferent types of multimedia are consumed by audiences. This report presents\nan analysis of social media platforms, in terms of demographics, supported\nmultimedia modalities, and distinct features and specifications for different\nmodalities, followed by a comparative case study of select European soccer\nleagues and teams, in terms of their social media practices. Through this\nanalysis, we demonstrate that social media, while being very important for and\nwidely used by supporters from all ages, also requires a fine-tuned effort on\nthe part of soccer professionals, in order to elevate fan experiences and\nfoster engagement.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.12328v1"
    },
    {
        "title": "Generating Robust Adversarial Examples against Online Social Networks\n  (OSNs)",
        "authors": [
            "Jun Liu",
            "Jiantao Zhou",
            "Haiwei Wu",
            "Weiwei Sun",
            "Jinyu Tian"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Online Social Networks (OSNs) have blossomed into prevailing transmission\nchannels for images in the modern era. Adversarial examples (AEs) deliberately\ndesigned to mislead deep neural networks (DNNs) are found to be fragile against\nthe inevitable lossy operations conducted by OSNs. As a result, the AEs would\nlose their attack capabilities after being transmitted over OSNs. In this work,\nwe aim to design a new framework for generating robust AEs that can survive the\nOSN transmission; namely, the AEs before and after the OSN transmission both\npossess strong attack capabilities. To this end, we first propose a\ndifferentiable network termed SImulated OSN (SIO) to simulate the various\noperations conducted by an OSN. Specifically, the SIO network consists of two\nmodules: 1) a differentiable JPEG layer for approximating the ubiquitous JPEG\ncompression and 2) an encoder-decoder subnetwork for mimicking the remaining\noperations. Based upon the SIO network, we then formulate an optimization\nframework to generate robust AEs by enforcing model outputs with and without\npassing through the SIO to be both misled. Extensive experiments conducted over\nFacebook, WeChat and QQ demonstrate that our attack methods produce more robust\nAEs than existing approaches, especially under small distortion constraints;\nthe performance gain in terms of Attack Success Rate (ASR) could be more than\n60%. Furthermore, we build a public dataset containing more than 10,000 pairs\nof AEs processed by Facebook, WeChat or QQ, facilitating future research in the\nrobust AEs generation. The dataset and code are available at\nhttps://github.com/csjunjun/RobustOSNAttack.git.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.12708v1"
    },
    {
        "title": "A survey of manifold learning and its applications for multimedia",
        "authors": [
            "Hannes Fassold"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Manifold learning is an emerging research domain of machine learning. In this\nwork, we give an introduction into manifold learning and how it is employed for\nimportant application fields in multimedia.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.12986v1"
    },
    {
        "title": "Perceptual impact of the loss function on deep-learning image coding\n  performance",
        "authors": [
            "Shima Mohammadi",
            "Joao Ascenso"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Nowadays, deep-learning image coding solutions have shown similar or better\ncompression efficiency than conventional solutions based on hand-crafted\ntransforms and spatial prediction techniques. These deep-learning codecs\nrequire a large training set of images and a training methodology to obtain a\nsuitable model (set of parameters) for efficient compression. The training is\nperformed with an optimization algorithm which provides a way to minimize the\nloss function. Therefore, the loss function plays a key role in the overall\nperformance and includes a differentiable quality metric that attempts to mimic\nhuman perception. The main objective of this paper is to study the perceptual\nimpact of several image quality metrics that can be used in the loss function\nof the training process, through a crowdsourcing subjective image quality\nassessment study. From this study, it is possible to conclude that the choice\nof the quality metric is critical for the perceptual performance of the\ndeep-learning codec and that can vary depending on the image content.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.06084v1"
    },
    {
        "title": "Vision-Language Instruction Tuning: A Review and Analysis",
        "authors": [
            "Chen Li",
            "Yixiao Ge",
            "Dian Li",
            "Ying Shan"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Instruction tuning is a crucial supervised training phase in Large Language\nModels (LLMs), aiming to enhance the LLM's ability to generalize instruction\nexecution and adapt to user preferences. With the increasing integration of\nmulti-modal data into LLMs, there is growing interest in Vision-Language\nInstruction Tuning (VLIT), which presents more complex characteristics compared\nto pure text instruction tuning. In this paper, we systematically review the\nlatest VLIT settings and corresponding datasets in multi-modal LLMs and provide\ninsights into the intrinsic motivations behind their design. For the first\ntime, we offer a detailed multi-perspective categorization for existing VLIT\ndatasets and identify the characteristics that high-quality VLIT data should\npossess. By incorporating these characteristics as guiding principles into the\nexisting VLIT data construction process, we conduct extensive experiments and\nverify their positive impact on the performance of tuned multi-modal LLMs.\nFurthermore, we discuss the current challenges and future research directions\nof VLIT, providing insights for the continuous development of this field. The\ncode and dataset related to this paper have been open-sourced at\nhttps://github.com/palchenli/VL-Instruction-Tuning.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.08172v2"
    },
    {
        "title": "RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection",
        "authors": [
            "Stefanos-Iordanis Papadopoulos",
            "Christos Koutlis",
            "Symeon Papadopoulos",
            "Panagiotis C. Petrantonakis"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Online misinformation is often multimodal in nature, i.e., it is caused by\nmisleading associations between texts and accompanying images. To support the\nfact-checking process, researchers have been recently developing automatic\nmultimodal methods that gather and analyze external information, evidence,\nrelated to the image-text pairs under examination. However, prior works assumed\nall external information collected from the web to be relevant. In this study,\nwe introduce a \"Relevant Evidence Detection\" (RED) module to discern whether\neach piece of evidence is relevant, to support or refute the claim.\nSpecifically, we develop the \"Relevant Evidence Detection Directed Transformer\"\n(RED-DOT) and explore multiple architectural variants (e.g., single or\ndual-stage) and mechanisms (e.g., \"guided attention\"). Extensive ablation and\ncomparative experiments demonstrate that RED-DOT achieves significant\nimprovements over the state-of-the-art (SotA) on the VERITE benchmark by up to\n33.7%. Furthermore, our evidence re-ranking and element-wise modality fusion\nled to RED-DOT surpassing the SotA on NewsCLIPings+ by up to 3% without the\nneed for numerous evidence or multiple backbone encoders. We release our code\nat: https://github.com/stevejpapad/relevant-evidence-detection\n",
        "pdf_link": "http://arxiv.org/pdf/2311.09939v2"
    },
    {
        "title": "mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large\n  Language Model",
        "authors": [
            "Anwen Hu",
            "Yaya Shi",
            "Haiyang Xu",
            "Jiabo Ye",
            "Qinghao Ye",
            "Ming Yan",
            "Chenliang Li",
            "Qi Qian",
            "Ji Zhang",
            "Fei Huang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Recently, the strong text creation ability of Large Language Models(LLMs) has\ngiven rise to many tools for assisting paper reading or even writing. However,\nthe weak diagram analysis abilities of LLMs or Multimodal LLMs greatly limit\ntheir application scenarios, especially for scientific academic paper writing.\nIn this work, towards a more versatile copilot for academic paper writing, we\nmainly focus on strengthening the multi-modal diagram analysis ability of\nMultimodal LLMs. By parsing Latex source files of high-quality papers, we\ncarefully build a multi-modal diagram understanding dataset M-Paper. By\naligning diagrams in the paper with related paragraphs, we construct\nprofessional diagram analysis samples for training and evaluation. M-Paper is\nthe first dataset to support joint comprehension of multiple scientific\ndiagrams, including figures and tables in the format of images or Latex codes.\nBesides, to better align the copilot with the user's intention, we introduce\nthe `outline' as the control signal, which could be directly given by the user\nor revised based on auto-generated ones. Comprehensive experiments with a\nstate-of-the-art Mumtimodal LLM demonstrate that training on our dataset shows\nstronger scientific diagram understanding performance, including diagram\ncaptioning, diagram analysis, and outline recommendation. The dataset, code,\nand model are available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.18248v2"
    },
    {
        "title": "High-Quality Live Video Streaming via Transcoding Time Prediction and\n  Preset Selection",
        "authors": [
            "Zahra Nabizadeh Shahre-Babak",
            "Nader Karimi",
            "Krishna Rapaka",
            "Tarek Amara",
            "Shadrokh Samavi",
            "Shahram Shirani"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Video streaming often requires transcoding content into different resolutions\nand bitrates to match the recipient's internet speed and screen capabilities.\nVideo encoders like x264 offer various presets, each with different tradeoffs\nbetween transcoding time and rate-distortion performance. Choosing the best\npreset for video transcoding is difficult, especially for live streaming, as\ntrying all the presets and choosing the best one is not feasible. One solution\nis to predict each preset's transcoding time and select the preset that ensures\nthe highest quality while adhering to live streaming time constraints.\nPrediction of video transcoding time is also critical in minimizing streaming\ndelays, deploying resource management algorithms, and load balancing. We\npropose a learning-based framework for predicting the transcoding time of\nvideos across various presets. Our predictor's features for video transcoding\ntime prediction are derived directly from the ingested stream, primarily from\nthe header or metadata. As a result, only minimal additional delay is incurred\nfor feature extraction, rendering our approach ideal for live-streaming\napplications. We evaluated our learning-based transcoding time prediction using\na dataset of videos. The results demonstrate that our framework can accurately\npredict the transcoding time for different presets, with a mean absolute\npercentage error (MAPE) of nearly 5.0%. Leveraging these predictions, we then\nselect the most suitable transcoding preset for live video streaming. Utilizing\nour transcoding time prediction-based preset selection improved Peak\nSignal-to-Noise Ratio (PSNR) of up to 5 dB.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.05348v1"
    },
    {
        "title": "Accelerated Event-Based Feature Detection and Compression for\n  Surveillance Video Systems",
        "authors": [
            "Andrew C. Freeman",
            "Ketan Mayer-Patel",
            "Montek Singh"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The strong temporal consistency of surveillance video enables compelling\ncompression performance with traditional methods, but downstream vision\napplications operate on decoded image frames with a high data rate. Since it is\nnot straightforward for applications to extract information on temporal\nredundancy from the compressed video representations, we propose a novel system\nwhich conveys temporal redundancy within a sparse decompressed representation.\nWe leverage a video representation framework called ADDER to transcode framed\nvideos to sparse, asynchronous intensity samples. We introduce mechanisms for\ncontent adaptation, lossy compression, and asynchronous forms of classical\nvision algorithms. We evaluate our system on the VIRAT surveillance video\ndataset, and we show a median 43.7% speed improvement in FAST feature detection\ncompared to OpenCV. We run the same algorithm as OpenCV, but only process\npixels that receive new asynchronous events, rather than process every pixel in\nan image frame. Our work paves the way for upcoming neuromorphic sensors and is\namenable to future applications with spiking neural networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.08213v2"
    },
    {
        "title": "Preparing VVC for Streaming: A Fast Multi-Rate Encoding Approach",
        "authors": [
            "Yiqun Liu",
            "Hadi Amirpour",
            "Mohsen Abdoli",
            "Christian Timmerer",
            "Thomas Guionnet"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The integration of advanced video codecs into the streaming pipeline is\ngrowing in response to the increasing demand for high quality video content.\nHowever, the significant computational demand for advanced codecs like\nVersatile Video Coding (VVC) poses challenges for service providers, including\nlonger encoding time and higher encoding cost. This challenge becomes even more\npronounced in streaming, as the same content needs to be encoded at multiple\nbitrates (also known as representations) to accommodate different network\nconditions. To accelerate the encoding process of multiple representations of\nthe same content in VVC, we employ the encoding map of a single representation,\nknown as the reference representation, and utilize its partitioning structure\nto accelerate the encoding of the remaining representations, referred to as\ndependent representations. To ensure compatibility with parallel processing, we\ndesignate the lowest bitrate representation as the reference representation.\nThe experimental results indicate a substantial improvement in the encoding\ntime for the dependent representations, achieving an average reduction of 40%,\nwhile maintaining a minimal average quality drop of only 0.43 in Video\nMulti-method Assessment Fusion (VMAF). This improvement is observed when\nutilizing Versatile Video Encoder (VVenC), an open and optimized VVC encoder\nimplementation.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.08330v1"
    },
    {
        "title": "CARAT: Contrastive Feature Reconstruction and Aggregation for\n  Multi-Modal Multi-Label Emotion Recognition",
        "authors": [
            "Cheng Peng",
            "Ke Chen",
            "Lidan Shou",
            "Gang Chen"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Multi-modal multi-label emotion recognition (MMER) aims to identify relevant\nemotions from multiple modalities. The challenge of MMER is how to effectively\ncapture discriminative features for multiple labels from heterogeneous data.\nRecent studies are mainly devoted to exploring various fusion strategies to\nintegrate multi-modal information into a unified representation for all labels.\nHowever, such a learning scheme not only overlooks the specificity of each\nmodality but also fails to capture individual discriminative features for\ndifferent labels. Moreover, dependencies of labels and modalities cannot be\neffectively modeled. To address these issues, this paper presents ContrAstive\nfeature Reconstruction and AggregaTion (CARAT) for the MMER task. Specifically,\nwe devise a reconstruction-based fusion mechanism to better model fine-grained\nmodality-to-label dependencies by contrastively learning modal-separated and\nlabel-specific features. To further exploit the modality complementarity, we\nintroduce a shuffle-based aggregation strategy to enrich co-occurrence\ncollaboration among labels. Experiments on two benchmark datasets CMU-MOSEI and\nM3ED demonstrate the effectiveness of CARAT over state-of-the-art methods. Code\nis available at https://github.com/chengzju/CARAT.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.10201v3"
    },
    {
        "title": "Frequency Spectrum is More Effective for Multimodal Representation and\n  Fusion: A Multimodal Spectrum Rumor Detector",
        "authors": [
            "An Lao",
            "Qi Zhang",
            "Chongyang Shi",
            "Longbing Cao",
            "Kun Yi",
            "Liang Hu",
            "Duoqian Miao"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Multimodal content, such as mixing text with images, presents significant\nchallenges to rumor detection in social media. Existing multimodal rumor\ndetection has focused on mixing tokens among spatial and sequential locations\nfor unimodal representation or fusing clues of rumor veracity across\nmodalities. However, they suffer from less discriminative unimodal\nrepresentation and are vulnerable to intricate location dependencies in the\ntime-consuming fusion of spatial and sequential tokens. This work makes the\nfirst attempt at multimodal rumor detection in the frequency domain, which\nefficiently transforms spatial features into the frequency spectrum and obtains\nhighly discriminative spectrum features for multimodal representation and\nfusion. A novel Frequency Spectrum Representation and fUsion network (FSRU)\nwith dual contrastive learning reveals the frequency spectrum is more effective\nfor multimodal representation and fusion, extracting the informative components\nfor rumor detection. FSRU involves three novel mechanisms: utilizing the\nFourier transform to convert features in the spatial domain to the frequency\ndomain, the unimodal spectrum compression, and the cross-modal spectrum\nco-selection module in the frequency domain. Substantial experiments show that\nFSRU achieves satisfactory multimodal rumor detection performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.11023v1"
    },
    {
        "title": "Coffee: Cost-Effective Edge Caching for 360 Degree Live Video Streaming",
        "authors": [
            "Chen Li",
            "Tingwei Ye",
            "Tongyu Zong",
            "Liyang Sun",
            "Houwei Cao",
            "Yong Liu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  While live 360 degree video streaming delivers immersive viewing experience,\nit poses significant bandwidth and latency challenges for content delivery\nnetworks. Edge servers are expected to play an important role in facilitating\nlive streaming of 360 degree videos. In this paper, we propose a novel\npredictive edge caching algorithm (Coffee) for live 360 degree video that\nemploy collaborative FoV prediction and predictive tile prefetching to reduce\nbandwidth consumption, streaming cost and improve the streaming quality and\nrobustness. Our light-weight caching algorithms exploit the unique tile\nconsumption patterns of live 360 degree video streaming to achieve high tile\ncaching gains. Through extensive experiments driven by real 360 degree video\nstreaming traces, we demonstrate that edge caching algorithms specifically\ndesigned for live 360 degree video streaming can achieve high streaming cost\nsavings with small edge cache space consumption. Coffee, guided by viewer FoV\npredictions, significantly reduces back-haul traffic up to 76% compared to\nstate-of-the-art edge caching algorithms. Furthermore, we develop a\ntranscoding-aware variant (TransCoffee) and evaluate it using comprehensive\nexperiments, which demonstrate that TransCoffee can achieve 63\\% lower cost\ncompared to state-of-the-art transcoding-aware approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.13470v2"
    },
    {
        "title": "Token-Level Contrastive Learning with Modality-Aware Prompting for\n  Multimodal Intent Recognition",
        "authors": [
            "Qianrui Zhou",
            "Hua Xu",
            "Hao Li",
            "Hanlei Zhang",
            "Xiaohan Zhang",
            "Yifan Wang",
            "Kai Gao"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Multimodal intent recognition aims to leverage diverse modalities such as\nexpressions, body movements and tone of speech to comprehend user's intent,\nconstituting a critical task for understanding human language and behavior in\nreal-world multimodal scenarios. Nevertheless, the majority of existing methods\nignore potential correlations among different modalities and own limitations in\neffectively learning semantic features from nonverbal modalities. In this\npaper, we introduce a token-level contrastive learning method with\nmodality-aware prompting (TCL-MAP) to address the above challenges. To\nestablish an optimal multimodal semantic environment for text modality, we\ndevelop a modality-aware prompting module (MAP), which effectively aligns and\nfuses features from text, video and audio modalities with similarity-based\nmodality alignment and cross-modality attention mechanism. Based on the\nmodality-aware prompt and ground truth labels, the proposed token-level\ncontrastive learning framework (TCL) constructs augmented samples and employs\nNT-Xent loss on the label token. Specifically, TCL capitalizes on the optimal\ntextual semantic insights derived from intent labels to guide the learning\nprocesses of other modalities in return. Extensive experiments show that our\nmethod achieves remarkable improvements compared to state-of-the-art methods.\nAdditionally, ablation analyses demonstrate the superiority of the\nmodality-aware prompt over the handcrafted prompt, which holds substantial\nsignificance for multimodal prompt learning. The codes are released at\nhttps://github.com/thuiar/TCL-MAP.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.14667v2"
    },
    {
        "title": "An Open Software Suite for Event-Based Video",
        "authors": [
            "Andrew C. Freeman"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  While traditional video representations are organized around discrete image\nframes, event-based video is a new paradigm that forgoes image frames\naltogether. Rather, pixel samples are temporally asynchronous and independent\nof one another. Until now, researchers have lacked a cohesive software\nframework for exploring the representation, compression, and applications of\nevent-based video. I present the AD$\\Delta$ER software suite to fill this gap.\nThis framework includes utilities for transcoding framed and multimodal\nevent-based video sources to a common representation, rate control mechanisms,\nlossy compression, application support, and an interactive GUI for transcoding\nand playback. In this paper, I describe these various software components and\ntheir usage.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.17151v1"
    },
    {
        "title": "Performance Evaluation of Associative Watermarking Using Statistical\n  Neurodynamics",
        "authors": [
            "Ryoto Kanegae",
            "Masaki Kawamura"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  We theoretically evaluated the performance of our proposed associative\nwatermarking method in which the watermark is not embedded directly into the\nimage. We previously proposed a watermarking method that extends the\nzero-watermarking model by applying associative memory models. In this model,\nthe hetero-associative memory model is introduced to the mapping process\nbetween image features and watermarks, and the auto-associative memory model is\napplied to correct watermark errors. We herein show that the associative\nwatermarking model outperforms the zero-watermarking model through computer\nsimulations using actual images. In this paper, we describe how we derive the\nmacroscopic state equation for the associative watermarking model using the\nOkada theory. The theoretical results obtained by the fourth-order theory were\nin good agreement with those obtained by computer simulations. Furthermore, the\nperformance of the associative watermarking model was evaluated using the bit\nerror rate of the watermark, both theoretically and using computer simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.05508v2"
    },
    {
        "title": "BDIQA: A New Dataset for Video Question Answering to Explore Cognitive\n  Reasoning through Theory of Mind",
        "authors": [
            "Yuanyuan Mao",
            "Xin Lin",
            "Qin Ni",
            "Liang He"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  As a foundational component of cognitive intelligence, theory of mind (ToM)\ncan make AI more closely resemble human thought processes, thereby enhancing\ntheir interaction and collaboration with human. In particular, it can\nsignificantly improve a model's comprehension of videos in complex scenes.\nHowever, current video question answer (VideoQA) datasets focus on studying\ncausal reasoning within events few of them genuinely incorporating human ToM.\nConsequently, there is a lack of development in ToM reasoning tasks within the\narea of VideoQA. This paper presents BDIQA, the first benchmark to explore the\ncognitive reasoning capabilities of VideoQA models in the context of ToM. BDIQA\nis inspired by the cognitive development of children's ToM and addresses the\ncurrent deficiencies in machine ToM within datasets and tasks. Specifically, it\noffers tasks at two difficulty levels, assessing Belief, Desire and Intention\n(BDI) reasoning in both simple and complex scenarios. We conduct evaluations on\nseveral mainstream methods of VideoQA and diagnose their capabilities with zero\nshot, few shot and supervised learning. We find that the performance of\npre-trained models on cognitive reasoning tasks remains unsatisfactory. To\ncounter this challenge, we undertake thorough analysis and experimentation,\nultimately presenting two guidelines to enhance cognitive reasoning derived\nfrom ablation analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.07402v1"
    },
    {
        "title": "Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image\n  Data",
        "authors": [
            "Puneet Kumar",
            "Sarthak Malik",
            "Balasubramanian Raman",
            "Xiaobai Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The ability to generate sentiment-controlled feedback in response to\nmultimodal inputs comprising text and images addresses a critical gap in\nhuman-computer interaction. This capability allows systems to provide\nempathetic, accurate, and engaging responses, with useful applications in\neducation, healthcare, marketing, and customer service. To this end, we have\nconstructed a large-scale Controllable Multimodal Feedback Synthesis (CMFeed)\ndataset and propose a controllable feedback synthesis system. The system\nfeatures an encoder, decoder, and controllability block for textual and visual\ninputs. It extracts features using a transformer and Faster R-CNN networks,\ncombining them to generate feedback. The CMFeed dataset includes images, texts,\nreactions to the posts, human comments with relevance scores, and reactions to\nthese comments. These reactions train the model to produce feedback with\nspecified sentiments, achieving a sentiment classification accuracy of 77.23\\%,\nwhich is 18.82\\% higher than the accuracy without controllability. The system\nalso incorporates a similarity module for assessing feedback relevance through\nrank-based metrics and an interpretability technique to analyze the\ncontributions of textual and visual features during feedback generation. Access\nto the CMFeed dataset and the system's code is available at\nhttps://github.com/MIntelligence-Group/CMFeed.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.07640v3"
    },
    {
        "title": "LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement\n  Learning",
        "authors": [
            "Adithya Raman",
            "Bekir Turkkan",
            "Tevfik Kosar"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Over the recent years, research and development in adaptive bitrate (ABR)\nalgorithms for live video streaming have been successful in improving users'\nquality of experience (QoE) by reducing latency to near real-time levels while\ndelivering higher bitrate videos with minimal rebuffering time. However, the\nQoE models used by these ABR algorithms do not take into account that a large\nportion of live video streaming clients use mobile devices where a higher\nbitrate does not necessarily translate into higher perceived quality. Ignoring\nperceived quality results in playing videos at higher bitrates without a\nsignificant increase in perceptual video quality and becomes a burden for\nbattery-constrained mobile devices due to higher energy consumption. In this\npaper, we propose LL-GABR, a deep reinforcement learning approach that models\nthe QoE using perceived video quality instead of bitrate and uses energy\nconsumption along with other metrics like latency, rebuffering events, and\nsmoothness. LL-GABR makes no assumptions about the underlying video,\nenvironment, or network settings and can operate flexibly on different video\ntitles, each having a different bitrate encoding ladder without additional\nre-training, unlike existing learning-based ABRs. Trace-driven experimental\nresults show that LL-GABR outperforms the state-of-the-art approaches by up to\n44% in terms of perceptual QoE and a 73% increase in energy efficiency as a\nresult of reducing net energy consumption by 11%.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09392v1"
    },
    {
        "title": "SpaceMeta: Global-Scale Massive Multi-User Virtual Interaction over LEO\n  Satellite Constellations",
        "authors": [
            "Jiahe Huang",
            "Yifei Zhu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Low latency and high synchronization among users are critical for emerging\nmulti-user virtual interaction applications. However, the existing ground-based\ncloud solutions are naturally limited by the complex ground topology and fiber\nspeeds, making it difficult to pace with the requirement of multi-user virtual\ninteraction. The growth of low earth orbit (LEO) satellite constellations\nbecomes a promising alternative to ground solutions. To fully exploit the\npotential of the LEO satellite, in this paper, we study the satellite server\nselection problem for global-scale multi-user interaction applications over LEO\nconstellations. We propose an effective server selection framework, called\nSpaceMeta, that jointly selects the ingress satellite servers and relay servers\non the communication path to minimize latency and latency discrepancy among\nusers. Extensive experiments using real-world Starlink topology demonstrate\nthat SpaceMeta reduces the latency by 6.72% and the interquartile range (IQR)\nof user latency by 39.50% compared with state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09720v1"
    },
    {
        "title": "An Experimental Study of Low-Latency Video Streaming over 5G",
        "authors": [
            "Imran Khan",
            "Tuyen X. Tran",
            "Matti Hiltunen",
            "Theodore Karagioules",
            "Dimitrios Koutsonikolas"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Low-latency video streaming over 5G has become rapidly popular over the last\nfew years due to its increased usage in hosting virtual events, online\neducation, webinars, and all-hands meetings. Our work aims to address the\nabsence of studies that reveal the real-world behavior of low-latency video\nstreaming. To that end, we provide an experimental methodology and\nmeasurements, collected in a US metropolitan area over a commercial 5G network,\nthat correlates application-level QoE and lower-layer metrics on the devices,\nsuch as RSRP, RSRQ, handover records, etc., under both static and mobility\nscenarios. We find that RAN-side information, which is readily available on\nevery cellular device, has the potential to enhance throughput estimation\nmodules of video streaming clients, ultimately making low-latency streaming\nmore resilient against network perturbations and handover events.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00752v1"
    },
    {
        "title": "Optimizing Mobile-Friendly Viewport Prediction for Live 360-Degree Video\n  Streaming",
        "authors": [
            "Lei Zhang",
            "Tao Long",
            "Weizhen Xu",
            "Laizhong Cui",
            "Jiangchuan Liu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Viewport prediction is the crucial task for adaptive 360-degree video\nstreaming, as the bitrate control algorithms usually require the knowledge of\nthe user's viewing portions of the frames. Various methods are studied and\nadopted for viewport prediction from less accurate statistic tools to highly\ncalibrated deep neural networks. Conventionally, it is difficult to implement\nsophisticated deep learning methods on mobile devices, which have limited\ncomputation capability. In this work, we propose an advanced learning-based\nviewport prediction approach and carefully design it to introduce minimal\ntransmission and computation overhead for mobile terminals. We also propose a\nmodel-agnostic meta-learning (MAML) based saliency prediction network trainer,\nwhich provides a few-sample fast training solution to obtain the prediction\nmodel by utilizing the information from the past models. We further discuss how\nto integrate this mobile-friendly viewport prediction (MFVP) approach into a\ntypical 360-degree video live streaming system by formulating and solving the\nbitrate adaptation problem. Extensive experiment results show that our\nprediction approach can work in real-time for live video streaming and can\nachieve higher accuracies compared to other existing prediction methods on\nmobile end, which, together with our bitrate adaptation algorithm,\nsignificantly improves the streaming QoE from various aspects. We observe the\naccuracy of MFVP is 8.1$\\%$ to 28.7$\\%$ higher than other algorithms and\nachieves 3.73$\\%$ to 14.96$\\%$ higher average quality level and 49.6$\\%$ to\n74.97$\\%$ less quality level change than other algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.02693v1"
    },
    {
        "title": "AMUSE: Adaptive Multi-Segment Encoding for Dataset Watermarking",
        "authors": [
            "Saeed Ranjbar Alvar",
            "Mohammad Akbari",
            "David Ming Xuan Yue",
            "Yong Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Curating high quality datasets that play a key role in the emergence of new\nAI applications requires considerable time, money, and computational resources.\nSo, effective ownership protection of datasets is becoming critical. Recently,\nto protect the ownership of an image dataset, imperceptible watermarking\ntechniques are used to store ownership information (i.e., watermark) into the\nindividual image samples. Embedding the entire watermark into all samples leads\nto significant redundancy in the embedded information which damages the\nwatermarked dataset quality and extraction accuracy. In this paper, a\nmulti-segment encoding-decoding method for dataset watermarking (called AMUSE)\nis proposed to adaptively map the original watermark into a set of shorter\nsub-messages and vice versa. Our message encoder is an adaptive method that\nadjusts the length of the sub-messages according to the protection requirements\nfor the target dataset. Existing image watermarking methods are then employed\nto embed the sub-messages into the original images in the dataset and also to\nextract them from the watermarked images. Our decoder is then used to\nreconstruct the original message from the extracted sub-messages. The proposed\nencoder and decoder are plug-and-play modules that can easily be added to any\nwatermarking method. To this end, extensive experiments are preformed with\nmultiple watermarking solutions which show that applying AMUSE improves the\noverall message extraction accuracy upto 28% for the same given dataset\nquality. Furthermore, the image dataset quality is enhanced by a PSNR of\n$\\approx$2 dB on average, while improving the extraction accuracy for one of\nthe tested image watermarking methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.05628v2"
    },
    {
        "title": "MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent\n  Recognition and Out-of-scope Detection in Conversations",
        "authors": [
            "Hanlei Zhang",
            "Xin Wang",
            "Hua Xu",
            "Qianrui Zhou",
            "Kai Gao",
            "Jianhua Su",
            "jinyue Zhao",
            "Wenrui Li",
            "Yanting Chen"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal intent recognition poses significant challenges, requiring the\nincorporation of non-verbal modalities from real-world contexts to enhance the\ncomprehension of human intentions. Existing benchmark datasets are limited in\nscale and suffer from difficulties in handling out-of-scope samples that arise\nin multi-turn conversational interactions. We introduce MIntRec2.0, a\nlarge-scale benchmark dataset for multimodal intent recognition in multi-party\nconversations. It contains 1,245 dialogues with 15,040 samples, each annotated\nwithin a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope\nsamples, it also includes 5,736 out-of-scope samples appearing in multi-turn\ncontexts, which naturally occur in real-world scenarios. Furthermore, we\nprovide comprehensive information on the speakers in each utterance, enriching\nits utility for multi-party conversational research. We establish a general\nframework supporting the organization of single-turn and multi-turn dialogue\ndata, modality feature extraction, multimodal fusion, as well as in-scope\nclassification and out-of-scope detection. Evaluation benchmarks are built\nusing classic multimodal fusion methods, ChatGPT, and human evaluators. While\nexisting methods incorporating nonverbal information yield improvements,\neffectively leveraging context information and detecting out-of-scope samples\nremains a substantial challenge. Notably, large language models exhibit a\nsignificant performance gap compared to humans, highlighting the limitations of\nmachine learning methods in the cognitive intent understanding task. We believe\nthat MIntRec2.0 will serve as a valuable resource, providing a pioneering\nfoundation for research in human-machine conversational interactions, and\nsignificantly facilitating related applications. The full dataset and codes are\navailable at https://github.com/thuiar/MIntRec2.0.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.10943v4"
    },
    {
        "title": "NewsCaption: Named-Entity aware Captioning for Out-of-Context Media",
        "authors": [
            "Anurag Singh",
            "Shivangi Aneja"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  With the increasing influence of social media, online misinformation has\ngrown to become a societal issue. The motivation for our work comes from the\nthreat caused by cheapfakes, where an unaltered image is described using a news\ncaption in a new but false-context. The main challenge in detecting such\nout-of-context multimedia is the unavailability of large-scale datasets.\nSeveral detection methods employ randomly selected captions to generate\nout-of-context training inputs. However, these randomly matched captions are\nnot truly representative of out-of-context scenarios due to inconsistencies\nbetween the image description and the matched caption. We aim to address these\nlimitations by introducing a novel task of out-of-context caption generation.\nIn this work, we propose a new method that generates a realistic out-of-context\ncaption given visual and textual context. We also demonstrate that the\nsemantics of the generated captions can be controlled using the textual\ncontext. We also evaluate our method against several baselines and our method\nimproves over the image captioning baseline by 6.2% BLUE-4, 2.96% CiDEr, 11.5%\nROUGE, and 7.3% METEOR\n",
        "pdf_link": "http://arxiv.org/pdf/2403.12618v1"
    },
    {
        "title": "Not All Attention is Needed: Parameter and Computation Efficient\n  Transfer Learning for Multi-modal Large Language Models",
        "authors": [
            "Qiong Wu",
            "Weihao Ye",
            "Yiyi Zhou",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we propose a novel parameter and computation efficient tuning\nmethod for Multi-modal Large Language Models (MLLMs), termed Efficient\nAttention Skipping (EAS). Concretely, we first reveal that multi-head\nattentions (MHAs), the main computational overhead of MLLMs, are often\nredundant to downstream tasks. Based on this observation, EAS evaluates the\nattention redundancy and skips the less important MHAs to speed up inference.\nBesides, we also propose a novel propagation-of-information adapter (PIA) to\nserve the attention skipping of EAS and keep parameter efficiency, which can be\nfurther re-parameterized into feed-forward networks (FFNs) for zero-extra\nlatency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN\nand a classic VL pre-trained model called METER, and conduct extensive\nexperiments on a set of benchmarks. The experiments show that EAS not only\nretains high performance and parameter efficiency, but also greatly speeds up\ninference speed. For instance, LaVIN-EAS can obtain 89.98\\% accuracy on\nScineceQA while speeding up inference by 2.2 times to LaVIN\n",
        "pdf_link": "http://arxiv.org/pdf/2403.15226v2"
    },
    {
        "title": "Experimental Studies of Metaverse Streaming",
        "authors": [
            "Haopeng Wang",
            "Roberto Martinez-Velazquez",
            "Haiwei Dong",
            "Abdulmotaleb El Saddik"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Metaverse aims to construct a large, unified, immersive, and shared digital\nrealm by combining various technologies, namely XR (extended reality),\nblockchain, and digital twin, among others. This article explores the Metaverse\nfrom the perspective of multimedia communication by conducting and analyzing\nreal-world experiments on four different Metaverse platforms: VR (virtual\nreality) Vircadia, VR Mozilla Hubs, VRChat, and MR (mixed reality) Virtual\nCity. We first investigate the traffic patterns and network performance in the\nthree VR platforms. After raising the challenges of the Metaverse streaming and\ninvestigating the potential methods to enhance Metaverse performance, we\npropose a remote rendering architecture and verify its advantages through a\nprototype involving the campus network and MR multimodal interaction by\ncomparison with local rendering.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.15256v1"
    },
    {
        "title": "Network-Assisted Delivery of Adaptive Video Streaming Services through\n  CDN, SDN, and MEC",
        "authors": [
            "Reza Farahani"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimedia applications, mainly video streaming services, are currently the\ndominant source of network load worldwide. In recent Video-on-Demand (VoD) and\nlive video streaming services, traditional streaming delivery techniques have\nbeen replaced by adaptive solutions based on the HTTP protocol. Current trends\ntoward high-resolution (e.g., 8K) and/or low-latency VoD and live video\nstreaming pose new challenges to end-to-end (E2E) bandwidth demand and have\nstringent delay requirements. To do this, video providers typically rely on\nContent Delivery Networks (CDNs) to ensure that they provide scalable video\nstreaming services. To support future streaming scenarios involving millions of\nusers, it is necessary to increase the CDNs' efficiency. It is widely agreed\nthat these requirements may be satisfied by adopting emerging networking\ntechniques to present Network-Assisted Video Streaming (NAVS) methods.\nMotivated by this, this thesis goes one step beyond traditional pure\nclient-based HAS algorithms by incorporating (an) in-network component(s) with\na broader view of the network to present completely transparent NAVS solutions\nfor HAS clients.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.16951v1"
    },
    {
        "title": "TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment\n  Analysis",
        "authors": [
            "Ming Zhou",
            "Weize Quan",
            "Ziqi Zhou",
            "Kai Wang",
            "Tong Wang",
            "Dong-Ming Yan"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment\nby leveraging language, visual, and acoustic modalities. Despite the remarkable\nperformance exhibited by previous MSA approaches, the presence of inherent\nmultimodal heterogeneities poses a challenge, with the contribution of\ndifferent modalities varying considerably. Past research predominantly focused\non improving representation learning techniques and feature fusion strategies.\nHowever, many of these efforts overlooked the variation in semantic richness\namong different modalities, treating each modality uniformly. This approach may\nlead to underestimating the significance of strong modalities while\noveremphasizing the importance of weak ones. Motivated by these insights, we\nintroduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the\npredominant role of the text modality in MSA. Specifically, for each multimodal\nsample, by taking unaligned sequences of the three modalities as inputs, we\ninitially allocate the extracted unimodal features into a visual-text and an\nacoustic-text pair. Subsequently, we implement self-attention on the text\nmodality and apply text-queried cross-attention to the visual and acoustic\nmodalities. To mitigate the influence of noise signals and redundant features,\nwe incorporate a gated control mechanism into the framework. Additionally, we\nintroduce unimodal joint learning to gain a deeper understanding of homogeneous\nemotional tendencies across diverse modalities through backpropagation.\nExperimental results demonstrate that TCAN consistently outperforms\nstate-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI).\n",
        "pdf_link": "http://arxiv.org/pdf/2404.04545v1"
    },
    {
        "title": "Multimodal Emotion Recognition by Fusing Video Semantic in MOOC Learning\n  Scenarios",
        "authors": [
            "Yuan Zhang",
            "Xiaomei Tao",
            "Hanxu Ai",
            "Tao Chen",
            "Yanling Gan"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In the Massive Open Online Courses (MOOC) learning scenario, the semantic\ninformation of instructional videos has a crucial impact on learners' emotional\nstate. Learners mainly acquire knowledge by watching instructional videos, and\nthe semantic information in the videos directly affects learners' emotional\nstates. However, few studies have paid attention to the potential influence of\nthe semantic information of instructional videos on learners' emotional states.\nTo deeply explore the impact of video semantic information on learners'\nemotions, this paper innovatively proposes a multimodal emotion recognition\nmethod by fusing video semantic information and physiological signals. We\ngenerate video descriptions through a pre-trained large language model (LLM) to\nobtain high-level semantic information about instructional videos. Using the\ncross-attention mechanism for modal interaction, the semantic information is\nfused with the eye movement and PhotoPlethysmoGraphy (PPG) signals to obtain\nthe features containing the critical information of the three modes. The\naccurate recognition of learners' emotional states is realized through the\nemotion classifier. The experimental results show that our method has\nsignificantly improved emotion recognition performance, providing a new\nperspective and efficient method for emotion recognition research in MOOC\nlearning scenarios. The method proposed in this paper not only contributes to a\ndeeper understanding of the impact of instructional videos on learners'\nemotional states but also provides a beneficial reference for future research\non emotion recognition in MOOC learning scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.07484v1"
    },
    {
        "title": "Video Compression Beyond VVC: Quantitative Analysis of Intra Coding\n  Tools in Enhanced Compression Model (ECM)",
        "authors": [
            "Mohsen Abdoli",
            "Ramin G. Youvalari",
            "Karam Naser",
            "Kevin Reuzé",
            "Fabrice Le Léannec"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  A quantitative analysis of post-VVC luma and chroma intra tools is presented,\nfocusing on their statistical behaviors, in terms of block selection rate under\ndifferent conditions. The aim is to provide insights to the standardization\ncommunity, offering a clearer understanding of interactions between tools and\nassisting in the design of an optimal combination of these novel tools when the\nJVET enters the standardization phase. Specifically, this paper examines the\nselection rate of intra tools as function of 1) the version of the ECM, 2)\nvideo resolution, and 3) video bitrate. Additionally, tests have been conducted\non sequences beyond the JVET CTC database. The statistics show several trends\nand interactions, with various strength, between coding tools of both luma and\nchroma.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.07872v1"
    },
    {
        "title": "Arena: A Patch-of-Interest ViT Inference Acceleration System for\n  Edge-Assisted Video Analytics",
        "authors": [
            "Haosong Peng",
            "Wei Feng",
            "Hao Li",
            "Yufeng Zhan",
            "Ren Jin",
            "Yuanqing Xia"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The advent of edge computing has made real-time intelligent video analytics\nfeasible. Previous works, based on traditional model architecture (e.g., CNN,\nRNN, etc.), employ various strategies to filter out non-region-of-interest\ncontent to minimize bandwidth and computation consumption but show inferior\nperformance in adverse environments. Recently, visual foundation models based\non transformers have shown great performance in adverse environments due to\ntheir amazing generalization capability. However, they require a large amount\nof computation power, which limits their applications in real-time intelligent\nvideo analytics. In this paper, we find visual foundation models like Vision\nTransformer (ViT) also have a dedicated acceleration mechanism for video\nanalytics. To this end, we introduce Arena, an end-to-end edge-assisted video\ninference acceleration system based on ViT. We leverage the capability of ViT\nthat can be accelerated through token pruning by only offloading and feeding\nPatches-of-Interest to the downstream models. Additionally, we design an\nadaptive keyframe inference switching algorithm tailored to different videos,\ncapable of adapting to the current video content to jointly optimize accuracy\nand bandwidth. Through extensive experiments, our findings reveal that Arena\ncan boost inference speeds by up to 1.58\\(\\times\\) and 1.82\\(\\times\\) on\naverage while consuming only 47\\% and 31\\% of the bandwidth, respectively, all\nwith high inference accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.09245v2"
    },
    {
        "title": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion",
        "authors": [
            "Yingxuan Li",
            "Ryota Hinami",
            "Kiyoharu Aizawa",
            "Yusuke Matsui"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.13993v4"
    },
    {
        "title": "MorphText: Deep Morphology Regularized Arbitrary-shape Scene Text\n  Detection",
        "authors": [
            "Chengpei Xu",
            "Wenjing Jia",
            "Ruomei Wang",
            "Xiaonan Luo",
            "Xiangjian He"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Bottom-up text detection methods play an important role in arbitrary-shape\nscene text detection but there are two restrictions preventing them from\nachieving their great potential, i.e., 1) the accumulation of false text\nsegment detections, which affects subsequent processing, and 2) the difficulty\nof building reliable connections between text segments. Targeting these two\nproblems, we propose a novel approach, named ``MorphText\", to capture the\nregularity of texts by embedding deep morphology for arbitrary-shape text\ndetection. Towards this end, two deep morphological modules are designed to\nregularize text segments and determine the linkage between them. First, a Deep\nMorphological Opening (DMOP) module is constructed to remove false text segment\ndetections generated in the feature extraction process. Then, a Deep\nMorphological Closing (DMCL) module is proposed to allow text instances of\nvarious shapes to stretch their morphology along their most significant\norientation while deriving their connections. Extensive experiments conducted\non four challenging benchmark datasets (CTW1500, Total-Text, MSRA-TD500 and\nICDAR2017) demonstrate that our proposed MorphText outperforms both top-down\nand bottom-up state-of-the-art arbitrary-shape scene text detection approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.17151v1"
    },
    {
        "title": "fMRI Exploration of Visual Quality Assessment",
        "authors": [
            "Yiming Zhang",
            "Ying Hu",
            "Xiongkuo Min",
            "Yan Zhou",
            "Guangtao Zhai"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Despite significant strides in visual quality assessment, the neural\nmechanisms underlying visual quality perception remain insufficiently explored.\nThis study employed fMRI to examine brain activity during image quality\nassessment and identify differences in human processing of images with varying\nquality. Fourteen healthy participants underwent tasks assessing both image\nquality and content classification while undergoing functional MRI scans. The\ncollected behavioral data was statistically analyzed, and univariate and\nfunctional connectivity analyses were conducted on the imaging data. The\nfindings revealed that quality assessment is a more complex task than content\nclassification, involving enhanced activation in high-level cognitive brain\nregions for fine-grained visual analysis. Moreover, the research showed the\nbrain's adaptability to different visual inputs, adopting different strategies\ndepending on the input's quality. In response to high-quality images, the brain\nprimarily uses specialized visual areas for precise analysis, whereas with\nlow-quality images, it recruits additional resources including higher-order\nvisual cortices and related cognitive and attentional networks to decode and\nrecognize complex, ambiguous signals effectively. This study pioneers the\nintersection of neuroscience and image quality research, providing empirical\nevidence through fMRI linking image quality to neural processing. It\ncontributes novel insights into the human visual system's response to diverse\nimage qualities, thereby paving the way for advancements in objective image\nquality assessment algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.18162v1"
    },
    {
        "title": "G-Refine: A General Quality Refiner for Text-to-Image Generation",
        "authors": [
            "Chunyi Li",
            "Haoning Wu",
            "Hongkun Hao",
            "Zicheng Zhang",
            "Tengchaun Kou",
            "Chaofeng Chen",
            "Lei Bai",
            "Xiaohong Liu",
            "Weisi Lin",
            "Guangtao Zhai"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  With the evolution of Text-to-Image (T2I) models, the quality defects of\nAI-Generated Images (AIGIs) pose a significant barrier to their widespread\nadoption. In terms of both perception and alignment, existing models cannot\nalways guarantee high-quality results. To mitigate this limitation, we\nintroduce G-Refine, a general image quality refiner designed to enhance\nlow-quality images without compromising the integrity of high-quality ones. The\nmodel is composed of three interconnected modules: a perception quality\nindicator, an alignment quality indicator, and a general quality enhancement\nmodule. Based on the mechanisms of the Human Visual System (HVS) and syntax\ntrees, the first two indicators can respectively identify the perception and\nalignment deficiencies, and the last module can apply targeted quality\nenhancement accordingly. Extensive experimentation reveals that when compared\nto alternative optimization methods, AIGIs after G-Refine outperform in 10+\nquality metrics across 4 databases. This improvement significantly contributes\nto the practical application of contemporary T2I models, paving the way for\ntheir broader adoption. The code will be released on\nhttps://github.com/Q-Future/Q-Refine.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.18343v1"
    },
    {
        "title": "Enhancing Interactive Image Retrieval With Query Rewriting Using Large\n  Language Models and Vision Language Models",
        "authors": [
            "Hongyi Zhu",
            "Jia-Hong Huang",
            "Stevan Rudinac",
            "Evangelos Kanoulas"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Image search stands as a pivotal task in multimedia and computer vision,\nfinding applications across diverse domains, ranging from internet search to\nmedical diagnostics. Conventional image search systems operate by accepting\ntextual or visual queries, retrieving the top-relevant candidate results from\nthe database. However, prevalent methods often rely on single-turn procedures,\nintroducing potential inaccuracies and limited recall. These methods also face\nthe challenges, such as vocabulary mismatch and the semantic gap, constraining\ntheir overall effectiveness. To address these issues, we propose an interactive\nimage retrieval system capable of refining queries based on user relevance\nfeedback in a multi-turn setting. This system incorporates a vision language\nmodel (VLM) based image captioner to enhance the quality of text-based queries,\nresulting in more informative queries with each iteration. Moreover, we\nintroduce a large language model (LLM) based denoiser to refine text-based\nquery expansions, mitigating inaccuracies in image descriptions generated by\ncaptioning models. To evaluate our system, we curate a new dataset by adapting\nthe MSR-VTT video retrieval dataset to the image retrieval task, offering\nmultiple relevant ground truth images for each query. Through comprehensive\nexperiments, we validate the effectiveness of our proposed system against\nbaseline methods, achieving state-of-the-art performance with a notable 10\\%\nimprovement in terms of recall. Our contributions encompass the development of\nan innovative interactive image retrieval system, the integration of an\nLLM-based denoiser, the curation of a meticulously designed evaluation dataset,\nand thorough experimental validation.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.18746v1"
    },
    {
        "title": "MVBIND: Self-Supervised Music Recommendation For Videos Via Embedding\n  Space Binding",
        "authors": [
            "Jiajie Teng",
            "Huiyu Duan",
            "Yucheng Zhu",
            "Sijing Wu",
            "Guangtao Zhai"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recent years have witnessed the rapid development of short videos, which\nusually contain both visual and audio modalities. Background music is important\nto the short videos, which can significantly influence the emotions of the\nviewers. However, at present, the background music of short videos is generally\nchosen by the video producer, and there is a lack of automatic music\nrecommendation methods for short videos. This paper introduces MVBind, an\ninnovative Music-Video embedding space Binding model for cross-modal retrieval.\nMVBind operates as a self-supervised approach, acquiring inherent knowledge of\nintermodal relationships directly from data, without the need of manual\nannotations. Additionally, to compensate the lack of a corresponding\nmusical-visual pair dataset for short videos, we construct a dataset,\nSVM-10K(Short Video with Music-10K), which mainly consists of meticulously\nselected short videos. On this dataset, MVBind manifests significantly improved\nperformance compared to other baseline methods. The constructed dataset and\ncode will be released to facilitate future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.09286v1"
    },
    {
        "title": "Enhancing Student Feedback Using Predictive Models in Visual Literacy\n  Courses",
        "authors": [
            "Alon Friedman",
            "Kevin Hawley",
            "Paul Rosen",
            "Md Dilshadur Rahman"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Peer review is a popular feedback mechanism in higher education that actively\nengages students and provides researchers with a means to assess student\nengagement. However, there is little empirical support for the durability of\npeer review, particularly when using data predictive modeling to analyze\nstudent comments. This study uses Na\\\"ive Bayes modeling to analyze peer review\ndata obtained from an undergraduate visual literacy course over five years. We\nexpand on the research of Friedman and Rosen and Beasley et al. by focusing on\nthe Na\\\"ive Bayes model of students' remarks. Our findings highlight the\nutility of Na\\\"ive Bayes modeling, particularly in the analysis of student\ncomments based on parts of speech, where nouns emerged as the prominent\ncategory. Additionally, when examining students' comments using the visual peer\nreview rubric, the lie factor emerged as the predominant factor. Comparing\nNa\\\"ive Bayes model to Beasley's approach, we found both help instructors map\ndirections taken in the class, but the Na\\\"ive Bayes model provides a more\nspecific outline for forecasting with a more detailed framework for identifying\ncore topics within the course, enhancing the forecasting of educational\ndirections. Through the application of the Holdout Method and $\\mathrm{k}$-fold\ncross-validation with continuity correction, we have validated the model's\npredictive accuracy, underscoring its effectiveness in offering deep insights\ninto peer review mechanisms. Our study findings suggest that using predictive\nmodeling to assess student comments can provide a new way to better serve the\nstudents' classroom comments on their visual peer work. This can benefit\ncourses by inspiring changes to course content, reinforcement of course\ncontent, modification of projects, or modifications to the rubric itself.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.15026v1"
    },
    {
        "title": "AIM: Let Any Multi-modal Large Language Models Embrace Efficient\n  In-Context Learning",
        "authors": [
            "Jun Gao",
            "Qian Qiao",
            "Ziqiang Cao",
            "Zili Wang",
            "Wenjie Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In-context learning (ICL) facilitates Large Language Models (LLMs) exhibiting\nemergent ability on downstream tasks without updating billions of parameters.\nHowever, in the area of multi-modal Large Language Models (MLLMs), two problems\nhinder the application of multi-modal ICL: (1) Most primary MLLMs are only\ntrained on single-image datasets, making them unable to read multi-modal\ndemonstrations. (2) With the demonstrations increasing, thousands of visual\ntokens highly challenge hardware and degrade ICL performance. During\npreliminary explorations, we discovered that the inner LLM tends to focus more\non the linguistic modality within multi-modal demonstrations to generate\nresponses. Therefore, we propose a general and light-weighted framework\n\\textbf{AIM} to tackle the mentioned problems through \\textbf{A}ggregating\n\\textbf{I}mage information of \\textbf{M}ultimodal demonstrations to the dense\nlatent space of the corresponding linguistic part. Specifically, AIM first uses\nthe frozen backbone MLLM to read each image-text demonstration and extracts the\nvector representations on top of the text. These vectors naturally fuse the\ninformation of the image-text pair, and AIM transforms them into fused virtual\ntokens acceptable for the inner LLM via a trainable projection layer.\nUltimately, these fused tokens function as variants of multi-modal\ndemonstrations, fed into the MLLM to direct its response to the current query\nas usual. Because these fused tokens stem from the textual component of the\nimage-text pair, a multi-modal demonstration is nearly reduced to a pure\ntextual demonstration, thus seamlessly applying to any MLLMs. With its de facto\nMLLM frozen, AIM is parameter-efficient and we train it on public multi-modal\nweb corpora which have nothing to do with downstream test tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.07588v2"
    },
    {
        "title": "Enhancing Cross-Prompt Transferability in Vision-Language Models through\n  Contextual Injection of Target Tokens",
        "authors": [
            "Xikang Yang",
            "Xuehai Tang",
            "Fuqing Zhu",
            "Jizhong Han",
            "Songlin Hu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Vision-language models (VLMs) seamlessly integrate visual and textual data to\nperform tasks such as image classification, caption generation, and visual\nquestion answering. However, adversarial images often struggle to deceive all\nprompts effectively in the context of cross-prompt migration attacks, as the\nprobability distribution of the tokens in these images tends to favor the\nsemantics of the original image rather than the target tokens. To address this\nchallenge, we propose a Contextual-Injection Attack (CIA) that employs\ngradient-based perturbation to inject target tokens into both visual and\ntextual contexts, thereby improving the probability distribution of the target\ntokens. By shifting the contextual semantics towards the target tokens instead\nof the original image semantics, CIA enhances the cross-prompt transferability\nof adversarial images.Extensive experiments on the BLIP2, InstructBLIP, and\nLLaVA models show that CIA outperforms existing methods in cross-prompt\ntransferability, demonstrating its potential for more effective adversarial\nstrategies in VLMs.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.13294v1"
    },
    {
        "title": "Convex-hull Estimation using XPSNR for Versatile Video Coding",
        "authors": [
            "Vignesh V Menon",
            "Christian R. Helmrich",
            "Adam Wieckowski",
            "Benjamin Bross",
            "Detlev Marpe"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  As adaptive streaming becomes crucial for delivering high-quality video\ncontent across diverse network conditions, accurate metrics to assess\nperceptual quality are essential. This paper explores using the eXtended Peak\nSignal-to-Noise Ratio (XPSNR) metric as an alternative to the popular Video\nMultimethod Assessment Fusion (VMAF) metric for determining optimized\nbitrate-resolution pairs in the context of Versatile Video Coding (VVC). Our\nstudy is rooted in the observation that XPSNR shows a superior correlation with\nsubjective quality scores for VVC-coded Ultra-High Definition (UHD) content\ncompared to VMAF. We predict the average XPSNR of VVC-coded bitstreams using\nspatiotemporal complexity features of the video and the target encoding\nconfiguration and then determine the convex-hull online. On average, the\nproposed convex-hull using XPSNR (VEXUS) achieves an overall quality\nimprovement of 5.84 dB PSNR and 0.62 dB XPSNR while maintaining the same\nbitrate, compared to the default UHD encoding using the VVenC encoder,\naccompanied by an encoding time reduction of 44.43% and a decoding time\nreduction of 65.46%. This shift towards XPSNR as a guiding metric shall enhance\nthe effectiveness of adaptive streaming algorithms, ensuring an optimal balance\nbetween bitrate efficiency and perceptual fidelity with advanced video coding\nstandards.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.13712v1"
    },
    {
        "title": "Contrast then Memorize: Semantic Neighbor Retrieval-Enhanced Inductive\n  Multimodal Knowledge Graph Completion",
        "authors": [
            "Yu Zhao",
            "Ying Zhang",
            "Baohang Zhou",
            "Xinying Qian",
            "Kehui Song",
            "Xiangrui Cai"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  A large number of studies have emerged for Multimodal Knowledge Graph\nCompletion (MKGC) to predict the missing links in MKGs. However, fewer studies\nhave been proposed to study the inductive MKGC (IMKGC) involving emerging\nentities unseen during training. Existing inductive approaches focus on\nlearning textual entity representations, which neglect rich semantic\ninformation in visual modality. Moreover, they focus on aggregating structural\nneighbors from existing KGs, which of emerging entities are usually limited.\nHowever, the semantic neighbors are decoupled from the topology linkage and\nusually imply the true target entity. In this paper, we propose the IMKGC task\nand a semantic neighbor retrieval-enhanced IMKGC framework CMR, where the\ncontrast brings the helpful semantic neighbors close, and then the memorize\nsupports semantic neighbor retrieval to enhance inference. Specifically, we\nfirst propose a unified cross-modal contrastive learning to simultaneously\ncapture the textual-visual and textual-textual correlations of query-entity\npairs in a unified representation space. The contrastive learning increases the\nsimilarity of positive query-entity pairs, therefore making the representations\nof helpful semantic neighbors close. Then, we explicitly memorize the knowledge\nrepresentations to support the semantic neighbor retrieval. At test time, we\nretrieve the nearest semantic neighbors and interpolate them to the\nquery-entity similarity distribution to augment the final prediction. Extensive\nexperiments validate the effectiveness of CMR on three inductive MKGC datasets.\nCodes are available at https://github.com/OreOZhao/CMR.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.02867v1"
    },
    {
        "title": "User Digital Twin-Driven Video Streaming for Customized Preferences and\n  Adaptive Transcoding",
        "authors": [
            "Stephen Jimmy",
            "Kalkidan Berhane",
            "Kevin Muhammad"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In the rapidly evolving field of multimedia services, video streaming has\nbecome increasingly prevalent, demanding innovative solutions to enhance user\nexperience and system efficiency. This paper introduces a novel approach that\nintegrates user digital twins-a dynamic digital representation of a user's\npreferences and behaviors-with traditional video streaming systems. We explore\nthe potential of this integration to dynamically adjust video preferences and\noptimize transcoding processes according to real-time data. The methodology\nleverages advanced machine learning algorithms to continuously update the\nuser's digital twin, which in turn informs the transcoding service to adapt\nvideo parameters for optimal quality and minimal buffering. Experimental\nresults show that our approach not only improves the personalization of content\ndelivery but also significantly enhances the overall efficiency of video\nstreaming services by reducing bandwidth usage and improving video playback\nquality. The implications of such advancements suggest a shift towards more\nadaptive, user-centric multimedia services, potentially transforming how video\ncontent is consumed and delivered.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.09766v1"
    },
    {
        "title": "A Multimodal Transformer for Live Streaming Highlight Prediction",
        "authors": [
            "Jiaxin Deng",
            "Shiyao Wang",
            "Dong Shen",
            "Liqin Zhao",
            "Fan Yang",
            "Guorui Zhou",
            "Gaofeng Meng"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recently, live streaming platforms have gained immense popularity.\nTraditional video highlight detection mainly focuses on visual features and\nutilizes both past and future content for prediction. However, live streaming\nrequires models to infer without future frames and process complex multimodal\ninteractions, including images, audio and text comments. To address these\nissues, we propose a multimodal transformer that incorporates historical\nlook-back windows. We introduce a novel Modality Temporal Alignment Module to\nhandle the temporal shift of cross-modal signals. Additionally, using existing\ndatasets with limited manual annotations is insufficient for live streaming\nwhose topics are constantly updated and changed. Therefore, we propose a novel\nBorder-aware Pairwise Loss to learn from a large-scale dataset and utilize user\nimplicit feedback as a weak supervision signal. Extensive experiments show our\nmodel outperforms various strong baselines on both real-world scenarios and\npublic datasets. And we will release our dataset and code to better assess this\ntopic.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.12002v1"
    },
    {
        "title": "VCEval: Rethinking What is a Good Educational Video and How to\n  Automatically Evaluate It",
        "authors": [
            "Xiaoxuan Zhu",
            "Zhouhong Gu",
            "Sihang Jiang",
            "Zhixu Li",
            "Hongwei Feng",
            "Yanghua Xiao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Online courses have significantly lowered the barrier to accessing education,\nyet the varying content quality of these videos poses challenges. In this work,\nwe focus on the task of automatically evaluating the quality of video course\ncontent. We have constructed a dataset with a substantial collection of video\ncourses and teaching materials. We propose three evaluation principles and\ndesign a new evaluation framework, \\textit{VCEval}, based on these principles.\nThe task is modeled as a multiple-choice question-answering task, with a\nlanguage model serving as the evaluator. Our method effectively distinguishes\nvideo courses of different content quality and produces a range of\ninterpretable results.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.12005v2"
    },
    {
        "title": "PG-Attack: A Precision-Guided Adversarial Attack Framework Against\n  Vision Foundation Models for Autonomous Driving",
        "authors": [
            "Jiyuan Fu",
            "Zhaoyu Chen",
            "Kaixun Jiang",
            "Haijing Guo",
            "Shuyong Gao",
            "Wenqiang Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Vision foundation models are increasingly employed in autonomous driving\nsystems due to their advanced capabilities. However, these models are\nsusceptible to adversarial attacks, posing significant risks to the reliability\nand safety of autonomous vehicles. Adversaries can exploit these\nvulnerabilities to manipulate the vehicle's perception of its surroundings,\nleading to erroneous decisions and potentially catastrophic consequences. To\naddress this challenge, we propose a novel Precision-Guided Adversarial Attack\n(PG-Attack) framework that combines two techniques: Precision Mask Perturbation\nAttack (PMP-Attack) and Deceptive Text Patch Attack (DTP-Attack). PMP-Attack\nprecisely targets the attack region to minimize the overall perturbation while\nmaximizing its impact on the target object's representation in the model's\nfeature space. DTP-Attack introduces deceptive text patches that disrupt the\nmodel's understanding of the scene, further enhancing the attack's\neffectiveness. Our experiments demonstrate that PG-Attack successfully deceives\na variety of advanced multi-modal large models, including GPT-4V, Qwen-VL, and\nimp-V1. Additionally, we won First-Place in the CVPR 2024 Workshop Challenge:\nBlack-box Adversarial Attacks on Vision Foundation Models and codes are\navailable at https://github.com/fuhaha824/PG-Attack.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.13111v1"
    },
    {
        "title": "Multimodal Unlearnable Examples: Protecting Data against Multimodal\n  Contrastive Learning",
        "authors": [
            "Xinwei Liu",
            "Xiaojun Jia",
            "Yuan Xun",
            "Siyuan Liang",
            "Xiaochun Cao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal contrastive learning (MCL) has shown remarkable advances in\nzero-shot classification by learning from millions of image-caption pairs\ncrawled from the Internet. However, this reliance poses privacy risks, as\nhackers may unauthorizedly exploit image-text data for model training,\npotentially including personal and privacy-sensitive information. Recent works\npropose generating unlearnable examples by adding imperceptible perturbations\nto training images to build shortcuts for protection. However, they are\ndesigned for unimodal classification, which remains largely unexplored in MCL.\nWe first explore this context by evaluating the performance of existing methods\non image-caption pairs, and they do not generalize effectively to multimodal\ndata and exhibit limited impact to build shortcuts due to the lack of labels\nand the dispersion of pairs in MCL. In this paper, we propose Multi-step Error\nMinimization (MEM), a novel optimization process for generating multimodal\nunlearnable examples. It extends the Error-Minimization (EM) framework to\noptimize both image noise and an additional text trigger, thereby enlarging the\noptimized space and effectively misleading the model to learn the shortcut\nbetween the noise features and the text trigger. Specifically, we adopt\nprojected gradient descent to solve the noise minimization problem and use\nHotFlip to approximate the gradient and replace words to find the optimal text\ntrigger. Extensive experiments demonstrate the effectiveness of MEM, with\npost-protection retrieval results nearly half of random guessing, and its high\ntransferability across different models. Our code is available on the\nhttps://github.com/thinwayliu/Multimodal-Unlearnable-Examples\n",
        "pdf_link": "http://arxiv.org/pdf/2407.16307v2"
    },
    {
        "title": "Start from Video-Music Retrieval: An Inter-Intra Modal Loss for Cross\n  Modal Retrieval",
        "authors": [
            "Zeyu Chen",
            "Pengfei Zhang",
            "Kai Ye",
            "Wei Dong",
            "Xin Feng",
            "Yana Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The burgeoning short video industry has accelerated the advancement of\nvideo-music retrieval technology, assisting content creators in selecting\nappropriate music for their videos. In self-supervised training for\nvideo-to-music retrieval, the video and music samples in the dataset are\nseparated from the same video work, so they are all one-to-one matches. This\ndoes not match the real situation. In reality, a video can use different music\nas background music, and a music can be used as background music for different\nvideos. Many videos and music that are not in a pair may be compatible, leading\nto false negative noise in the dataset. A novel inter-intra modal (II) loss is\nproposed as a solution. By reducing the variation of feature distribution\nwithin the two modalities before and after the encoder, II loss can reduce the\nmodel's overfitting to such noise without removing it in a costly and laborious\nway. The video-music retrieval framework, II-CLVM (Contrastive Learning for\nVideo-Music Retrieval), incorporating the II Loss, achieves state-of-the-art\nperformance on the YouTube8M dataset. The framework II-CLVTM shows better\nperformance when retrieving music using multi-modal video information (such as\ntext in videos). Experiments are designed to show that II loss can effectively\nalleviate the problem of false negative noise in retrieval tasks. Experiments\nalso show that II loss improves various self-supervised and supervised\nuni-modal and cross-modal retrieval tasks, and can obtain good retrieval models\nwith a small amount of training samples.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19415v1"
    },
    {
        "title": "AxiomVision: Accuracy-Guaranteed Adaptive Visual Model Selection for\n  Perspective-Aware Video Analytics",
        "authors": [
            "Xiangxiang Dai",
            "Zeyu Zhang",
            "Peng Yang",
            "Yuedong Xu",
            "Xutong Liu",
            "John C. S. Lui"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The rapid evolution of multimedia and computer vision technologies requires\nadaptive visual model deployment strategies to effectively handle diverse tasks\nand varying environments. This work introduces AxiomVision, a novel framework\nthat can guarantee accuracy by leveraging edge computing to dynamically select\nthe most efficient visual models for video analytics under diverse scenarios.\nUtilizing a tiered edge-cloud architecture, AxiomVision enables the deployment\nof a broad spectrum of visual models, from lightweight to complex DNNs, that\ncan be tailored to specific scenarios while considering camera source impacts.\nIn addition, AxiomVision provides three core innovations: (1) a dynamic visual\nmodel selection mechanism utilizing continual online learning, (2) an efficient\nonline method that efficiently takes into account the influence of the camera's\nperspective, and (3) a topology-driven grouping approach that accelerates the\nmodel selection process. With rigorous theoretical guarantees, these\nadvancements provide a scalable and effective solution for visual tasks\ninherent to multimedia systems, such as object detection, classification, and\ncounting. Empirically, AxiomVision achieves a 25.7\\% improvement in accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.20124v2"
    },
    {
        "title": "Open-Vocabulary Audio-Visual Semantic Segmentation",
        "authors": [
            "Ruohao Guo",
            "Liao Qu",
            "Dantong Niu",
            "Yanyu Qi",
            "Wenzhen Yue",
            "Ji Shi",
            "Bowei Xing",
            "Xianghua Ying"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Audio-visual semantic segmentation (AVSS) aims to segment and classify\nsounding objects in videos with acoustic cues. However, most approaches operate\non the close-set assumption and only identify pre-defined categories from\ntraining data, lacking the generalization ability to detect novel categories in\npractical applications. In this paper, we introduce a new task: open-vocabulary\naudio-visual semantic segmentation, extending AVSS task to open-world scenarios\nbeyond the annotated label space. This is a more challenging task that requires\nrecognizing all categories, even those that have never been seen nor heard\nduring training. Moreover, we propose the first open-vocabulary AVSS framework,\nOV-AVSS, which mainly consists of two parts: 1) a universal sound source\nlocalization module to perform audio-visual fusion and locate all potential\nsounding objects and 2) an open-vocabulary classification module to predict\ncategories with the help of the prior knowledge from large-scale pre-trained\nvision-language models. To properly evaluate the open-vocabulary AVSS, we split\nzero-shot training and testing subsets based on the AVSBench-semantic\nbenchmark, namely AVSBench-OV. Extensive experiments demonstrate the strong\nsegmentation and zero-shot generalization ability of our model on all\ncategories. On the AVSBench-OV dataset, OV-AVSS achieves 55.43% mIoU on base\ncategories and 29.14% mIoU on novel categories, exceeding the state-of-the-art\nzero-shot method by 41.88%/20.61% and open-vocabulary method by 10.2%/11.6%.\nThe code is available at https://github.com/ruohaoguo/ovavss.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.21721v1"
    },
    {
        "title": "An Empirical Comparison of Video Frame Sampling Methods for Multi-Modal\n  RAG Retrieval",
        "authors": [
            "Mahesh Kandhare",
            "Thibault Gisselbrecht"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Numerous video frame sampling methodologies detailed in the literature\npresent a significant challenge in determining the optimal video frame method\nfor Video RAG pattern without a comparative side-by-side analysis. In this\nwork, we investigate the trade-offs in frame sampling methods for Video & Frame\nRetrieval using natural language questions. We explore the balance between the\nquantity of sampled frames and the retrieval recall score, aiming to identify\nefficient video frame sampling strategies that maintain high retrieval efficacy\nwith reduced storage and processing demands. Our study focuses on the storage\nand retrieval of image data (video frames) within a vector database required by\nVideo RAG pattern, comparing the effectiveness of various frame sampling\ntechniques. Our investigation indicates that the recall@k metric for both\ntext-to-video and text-to-frame retrieval tasks using various methods covered\nas part of this work is comparable to or exceeds that of storing each frame\nfrom the video. Our findings are intended to inform the selection of frame\nsampling methods for practical Video RAG implementations, serving as a\nspringboard for innovative research in this domain.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.03340v1"
    },
    {
        "title": "Rethinking Video with a Universal Event-Based Representation",
        "authors": [
            "Andrew Freeman"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Traditionally, video is structured as a sequence of discrete image frames.\nRecently, however, a novel video sensing paradigm has emerged which eschews\nvideo frames entirely. These \"event\" sensors aim to mimic the human vision\nsystem with asynchronous sensing, where each pixel has an independent, sparse\ndata stream. While these cameras enable high-speed and high-dynamic-range\nsensing, researchers often revert to a framed representation of the event data\nfor existing applications, or build bespoke applications for a particular\ncamera's event data type. At the same time, classical video systems have\nsignificant computational redundancy at the application layer, since pixel\nsamples are repeated across frames in the uncompressed domain.\n  To address the shortcomings of existing systems, I introduce Address,\nDecimation, {\\Delta}t Event Representation (AD{\\Delta}ER, pronounced \"adder\"),\na novel intermediate video representation and system framework. The framework\ntranscodes a variety of framed and event camera sources into a single\nevent-based representation, which supports source-modeled lossy compression and\nbackward compatibility with traditional frame-based applications. I demonstrate\nthat AD{\\Delta}ER achieves state-of-the-art application speed and compression\nperformance for scenes with high temporal redundancy. Crucially, I describe how\nAD{\\Delta}ER unlocks an entirely new control mechanism for computer vision:\napplication speed can correlate with both the scene content and the level of\nlossy compression. Finally, I discuss the implications for event-based video on\nlarge-scale video surveillance and resource-constrained sensing.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.06248v1"
    },
    {
        "title": "SpeechCraft: A Fine-grained Expressive Speech Dataset with Natural\n  Language Description",
        "authors": [
            "Zeyu Jin",
            "Jia Jia",
            "Qixin Wang",
            "Kehan Li",
            "Shuoyi Zhou",
            "Songtao Zhou",
            "Xiaoyu Qin",
            "Zhiyong Wu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Speech-language multi-modal learning presents a significant challenge due to\nthe fine nuanced information inherent in speech styles. Therefore, a\nlarge-scale dataset providing elaborate comprehension of speech style is\nurgently needed to facilitate insightful interplay between speech audio and\nnatural language. However, constructing such datasets presents a major\ntrade-off between large-scale data collection and high-quality annotation. To\ntackle this challenge, we propose an automatic speech annotation system for\nexpressiveness interpretation that annotates in-the-wild speech clips with\nexpressive and vivid human language descriptions. Initially, speech audios are\nprocessed by a series of expert classifiers and captioning models to capture\ndiverse speech characteristics, followed by a fine-tuned LLaMA for customized\nannotation generation. Unlike previous tag/templet-based annotation frameworks\nwith limited information and diversity, our system provides in-depth\nunderstandings of speech style through tailored natural language descriptions,\nthereby enabling accurate and voluminous data generation for large model\ntraining. With this system, we create SpeechCraft, a fine-grained bilingual\nexpressive speech dataset. It is distinguished by highly descriptive natural\nlanguage style prompts, containing approximately 2,000 hours of audio data and\nencompassing over two million speech clips. Extensive experiments demonstrate\nthat the proposed dataset significantly boosts speech-language task performance\nin stylist speech synthesis and speech style understanding.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.13608v1"
    },
    {
        "title": "Digital Fingerprinting on Multimedia: A Survey",
        "authors": [
            "Wendi Chen",
            "Wensheng Gan",
            "Philip S. Yu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The explosive growth of multimedia content in the digital economy era has\nbrought challenges in content recognition, copyright protection, and data\nmanagement. As an emerging content management technology, perceptual hash-based\ndigital fingerprints, serving as compact summaries of multimedia content, have\nbeen widely adopted for efficient multimedia content identification and\nretrieval across different modalities (e.g., text, image, video, audio),\nattracting significant attention from both academia and industry. Despite the\nincreasing applications of digital fingerprints, there is a lack of systematic\nand comprehensive literature review on multimedia digital fingerprints. This\nsurvey aims to fill this gap and provide an important resource for researchers\nstudying the details and related advancements of multimedia digital\nfingerprints. The survey first introduces the definition, characteristics, and\nrelated concepts (including hash functions, granularity, similarity measures,\netc.) of digital fingerprints. It then focuses on analyzing and summarizing the\nalgorithms for extracting unimodal fingerprints of different types of digital\ncontent, including text fingerprints, image fingerprints, video fingerprints,\nand audio fingerprints. Particularly, it provides an in-depth review and\nsummary of deep learning-based fingerprints. Additionally, the survey\nelaborates on the various practical applications of digital fingerprints and\noutlines the challenges and potential future research directions. The goal is\nto promote the continued development of multimedia digital fingerprint\nresearch.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.14155v1"
    },
    {
        "title": "Multimodal Multi-turn Conversation Stance Detection: A Challenge Dataset\n  and Effective Model",
        "authors": [
            "Fuqiang Niu",
            "Zebang Cheng",
            "Xianghua Fu",
            "Xiaojiang Peng",
            "Genan Dai",
            "Yin Chen",
            "Hu Huang",
            "Bowen Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Stance detection, which aims to identify public opinion towards specific\ntargets using social media data, is an important yet challenging task. With the\nproliferation of diverse multimodal social media content including text, and\nimages multimodal stance detection (MSD) has become a crucial research area.\nHowever, existing MSD studies have focused on modeling stance within individual\ntext-image pairs, overlooking the multi-party conversational contexts that\nnaturally occur on social media. This limitation stems from a lack of datasets\nthat authentically capture such conversational scenarios, hindering progress in\nconversational MSD. To address this, we introduce a new multimodal multi-turn\nconversational stance detection dataset (called MmMtCSD). To derive stances\nfrom this challenging dataset, we propose a novel multimodal large language\nmodel stance detection framework (MLLM-SD), that learns joint stance\nrepresentations from textual and visual modalities. Experiments on MmMtCSD show\nstate-of-the-art performance of our proposed MLLM-SD approach for multimodal\nstance detection. We believe that MmMtCSD will contribute to advancing\nreal-world applications of stance detection research.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.00597v1"
    },
    {
        "title": "SwinGS: Sliding Window Gaussian Splatting for Volumetric Video Streaming\n  with Arbitrary Length",
        "authors": [
            "Bangya Liu",
            "Suman Banerjee"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recent advances in 3D Gaussian Splatting (3DGS) have garnered significant\nattention in computer vision and computer graphics due to its high rendering\nspeed and remarkable quality. While extant research has endeavored to extend\nthe application of 3DGS from static to dynamic scenes, such efforts have been\nconsistently impeded by excessive model sizes, constraints on video duration,\nand content deviation. These limitations significantly compromise the\nstreamability of dynamic 3D Gaussian models, thereby restricting their utility\nin downstream applications, including volumetric video, autonomous vehicle, and\nimmersive technologies such as virtual, augmented, and mixed reality.\n  This paper introduces SwinGS, a novel framework for training, delivering, and\nrendering volumetric video in a real-time streaming fashion. To address the\naforementioned challenges and enhance streamability, SwinGS integrates\nspacetime Gaussian with Markov Chain Monte Carlo (MCMC) to adapt the model to\nfit various 3D scenes across frames, in the meantime employing a sliding window\ncaptures Gaussian snapshots for each frame in an accumulative way. We implement\na prototype of SwinGS and demonstrate its streamability across various datasets\nand scenes. Additionally, we develop an interactive WebGL viewer enabling\nreal-time volumetric video playback on most devices with modern browsers,\nincluding smartphones and tablets. Experimental results show that SwinGS\nreduces transmission costs by 83.6% compared to previous work with ignorable\ncompromise in PSNR. Moreover, SwinGS easily scales to long video sequences\nwithout compromising quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.07759v2"
    },
    {
        "title": "IWN: Image Watermarking Based on Idempotency",
        "authors": [
            "Kaixin Deng"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In the expanding field of digital media, maintaining the strength and\nintegrity of watermarking technology is becoming increasingly challenging. This\npaper, inspired by the Idempotent Generative Network (IGN), explores the\nprospects of introducing idempotency into image watermark processing and\nproposes an innovative neural network model - the Idempotent Watermarking\nNetwork (IWN). The proposed model, which focuses on enhancing the recovery\nquality of color image watermarks, leverages idempotency to ensure superior\nimage reversibility. This feature ensures that, even if color image watermarks\nare attacked or damaged, they can be effectively projected and mapped back to\ntheir original state. Therefore, the extracted watermarks have unquestionably\nincreased quality. The IWN model achieves a balance between embedding capacity\nand robustness, alleviating to some extent the inherent contradiction between\nthese two factors in traditional watermarking techniques and steganography\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.19506v1"
    },
    {
        "title": "Maximum entropy and quantized metric models for absolute category\n  ratings",
        "authors": [
            "Dietmar Saupe",
            "Krzysztof Rusek",
            "David Hägele",
            "Daniel Weiskopf",
            "Lucjan Janowski"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The datasets of most image quality assessment studies contain ratings on a\ncategorical scale with five levels, from bad (1) to excellent (5). For each\nstimulus, the number of ratings from 1 to 5 is summarized and given in the form\nof the mean opinion score. In this study, we investigate families of\nmultinomial probability distributions parameterized by mean and variance that\nare used to fit the empirical rating distributions. To this end, we consider\nquantized metric models based on continuous distributions that model perceived\nstimulus quality on a latent scale. The probabilities for the rating categories\nare determined by quantizing the corresponding random variables using threshold\nvalues. Furthermore, we introduce a novel discrete maximum entropy distribution\nfor a given mean and variance. We compare the performance of these models and\nthe state of the art given by the generalized score distribution for two large\ndata sets, KonIQ-10k and VQEG HDTV. Given an input distribution of ratings, our\nfitted two-parameter models predict unseen ratings better than the empirical\ndistribution. In contrast to empirical ACR distributions and their discrete\nmodels, our continuous models can provide fine-grained estimates of quantiles\nof quality of experience that are relevant to service providers to satisfy a\ntarget fraction of the user population.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.00817v1"
    },
    {
        "title": "ChatVTG: Video Temporal Grounding via Chat with Video Dialogue Large\n  Language Models",
        "authors": [
            "Mengxue Qu",
            "Xiaodong Chen",
            "Wu Liu",
            "Alicia Li",
            "Yao Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Video Temporal Grounding (VTG) aims to ground specific segments within an\nuntrimmed video corresponding to the given natural language query. Existing VTG\nmethods largely depend on supervised learning and extensive annotated data,\nwhich is labor-intensive and prone to human biases. To address these\nchallenges, we present ChatVTG, a novel approach that utilizes Video Dialogue\nLarge Language Models (LLMs) for zero-shot video temporal grounding. Our\nChatVTG leverages Video Dialogue LLMs to generate multi-granularity segment\ncaptions and matches these captions with the given query for coarse temporal\ngrounding, circumventing the need for paired annotation data. Furthermore, to\nobtain more precise temporal grounding results, we employ moment refinement for\nfine-grained caption proposals. Extensive experiments on three mainstream VTG\ndatasets, including Charades-STA, ActivityNet-Captions, and TACoS, demonstrate\nthe effectiveness of ChatVTG. Our ChatVTG surpasses the performance of current\nzero-shot methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12813v1"
    },
    {
        "title": "RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping\n  Language-Image Pre-training",
        "authors": [
            "Muhe Ding",
            "Yang Ma",
            "Pengda Qin",
            "Jianlong Wu",
            "Yuhong Li",
            "Liqiang Nie"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal Large Language Models (MLLMs) have recently received substantial\ninterest, which shows their emerging potential as general-purpose models for\nvarious vision-language tasks. MLLMs involve significant external knowledge\nwithin their parameters; however, it is challenging to continually update these\nmodels with the latest knowledge, which involves huge computational costs and\npoor interpretability. Retrieval augmentation techniques have proven to be\neffective plugins for both LLMs and MLLMs. In this study, we propose multimodal\nadaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training\n(RA-BLIP), a novel retrieval-augmented framework for various MLLMs. Considering\nthe redundant information within vision modality, we first leverage the\nquestion to instruct the extraction of visual information through interactions\nwith one set of learnable queries, minimizing irrelevant interference during\nretrieval and generation. Besides, we introduce a pre-trained multimodal\nadaptive fusion module to achieve question text-to-multimodal retrieval and\nintegration of multimodal knowledge by projecting visual and language\nmodalities into a unified semantic space. Furthermore, we present an Adaptive\nSelection Knowledge Generation (ASKG) strategy to train the generator to\nautonomously discern the relevance of retrieved knowledge, which realizes\nexcellent denoising performance. Extensive experiments on open multimodal\nquestion-answering datasets demonstrate that RA-BLIP achieves significant\nperformance and surpasses the state-of-the-art retrieval-augmented models.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.14154v1"
    },
    {
        "title": "Diverse Sign Language Translation",
        "authors": [
            "Xin Shen",
            "Lei Shen",
            "Shaozu Yuan",
            "Heming Du",
            "Haiyang Sun",
            "Xin Yu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Like spoken languages, a single sign language expression could correspond to\nmultiple valid textual interpretations. Hence, learning a rigid one-to-one\nmapping for sign language translation (SLT) models might be inadequate,\nparticularly in the case of limited data. In this work, we introduce a Diverse\nSign Language Translation (DivSLT) task, aiming to generate diverse yet\naccurate translations for sign language videos. Firstly, we employ large\nlanguage models (LLM) to generate multiple references for the widely-used\nCSL-Daily and PHOENIX14T SLT datasets. Here, native speakers are only invited\nto touch up inaccurate references, thus significantly improving the annotation\nefficiency. Secondly, we provide a benchmark model to spur research in this\ntask. Specifically, we investigate multi-reference training strategies to\nenable our DivSLT model to achieve diverse translations. Then, to enhance\ntranslation accuracy, we employ the max-reward-driven reinforcement learning\nobjective that maximizes the reward of the translated result. Additionally, we\nutilize multiple metrics to assess the accuracy, diversity, and semantic\nprecision of the DivSLT task. Experimental results on the enriched datasets\ndemonstrate that our DivSLT method achieves not only better translation\nperformance but also diverse translation results.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.19586v1"
    },
    {
        "title": "Rate-aware Compression for NeRF-based Volumetric Video",
        "authors": [
            "Zhiyu Zhang",
            "Guo Lu",
            "Huanxiong Liang",
            "Zhengxue Cheng",
            "Anni Tang",
            "Li Song"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The neural radiance fields (NeRF) have advanced the development of 3D\nvolumetric video technology, but the large data volumes they involve pose\nsignificant challenges for storage and transmission. To address these problems,\nthe existing solutions typically compress these NeRF representations after the\ntraining stage, leading to a separation between representation training and\ncompression. In this paper, we try to directly learn a compact NeRF\nrepresentation for volumetric video in the training stage based on the proposed\nrate-aware compression framework. Specifically, for volumetric video, we use a\nsimple yet effective modeling strategy to reduce temporal redundancy for the\nNeRF representation. Then, during the training phase, an implicit entropy model\nis utilized to estimate the bitrate of the NeRF representation. This entropy\nmodel is then encoded into the bitstream to assist in the decoding of the NeRF\nrepresentation. This approach enables precise bitrate estimation, thereby\nleading to a compact NeRF representation. Furthermore, we propose an adaptive\nquantization strategy and learn the optimal quantization step for the NeRF\nrepresentations. Finally, the NeRF representation can be optimized by using the\nrate-distortion trade-off. Our proposed compression framework can be used for\ndifferent representations and experimental results demonstrate that our\napproach significantly reduces the storage size with marginal distortion and\nachieves state-of-the-art rate-distortion performance for volumetric video on\nthe HumanRF and ReRF datasets. Compared to the previous state-of-the-art method\nTeTriRF, we achieved an approximately -80% BD-rate on the HumanRF dataset and\n-60% BD-rate on the ReRF dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05322v1"
    },
    {
        "title": "Rendering-Oriented 3D Point Cloud Attribute Compression using Sparse\n  Tensor-based Transformer",
        "authors": [
            "Xiao Huo",
            "Junhui Hou",
            "Shuai Wan",
            "Fuzheng Yang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The evolution of 3D visualization techniques has fundamentally transformed\nhow we interact with digital content. At the forefront of this change is point\ncloud technology, offering an immersive experience that surpasses traditional\n2D representations. However, the massive data size of point clouds presents\nsignificant challenges in data compression. Current methods for lossy point\ncloud attribute compression (PCAC) generally focus on reconstructing the\noriginal point clouds with minimal error. However, for point cloud\nvisualization scenarios, the reconstructed point clouds with distortion still\nneed to undergo a complex rendering process, which affects the final\nuser-perceived quality. In this paper, we propose an end-to-end deep learning\nframework that seamlessly integrates PCAC with differentiable rendering,\ndenoted as rendering-oriented PCAC (RO-PCAC), directly targeting the quality of\nrendered multiview images for viewing. In a differentiable manner, the impact\nof the rendering process on the reconstructed point clouds is taken into\naccount. Moreover, we characterize point clouds as sparse tensors and propose a\nsparse tensor-based transformer, called SP-Trans. By aligning with the local\ndensity of the point cloud and utilizing an enhanced local attention mechanism,\nSP-Trans captures the intricate relationships within the point cloud, further\nimproving feature analysis and synthesis within the framework. Extensive\nexperiments demonstrate that the proposed RO-PCAC achieves state-of-the-art\ncompression performance, compared to existing reconstruction-oriented methods,\nincluding traditional, learning-based, and hybrid methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07899v3"
    },
    {
        "title": "CMATH: Cross-Modality Augmented Transformer with Hierarchical\n  Variational Distillation for Multimodal Emotion Recognition in Conversation",
        "authors": [
            "Xiaofei Zhu",
            "Jiawei Cheng",
            "Zhou Yang",
            "Zhuo Chen",
            "Qingyang Wang",
            "Jianfeng Yao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal emotion recognition in conversation (MER) aims to accurately\nidentify emotions in conversational utterances by integrating multimodal\ninformation. Previous methods usually treat multimodal information as equal\nquality and employ symmetric architectures to conduct multimodal fusion.\nHowever, in reality, the quality of different modalities usually varies\nconsiderably, and utilizing a symmetric architecture is difficult to accurately\nrecognize conversational emotions when dealing with uneven modal information.\nFurthermore, fusing multi-modality information in a single granularity may fail\nto adequately integrate modal information, exacerbating the inaccuracy in\nemotion recognition. In this paper, we propose a novel Cross-Modality Augmented\nTransformer with Hierarchical Variational Distillation, called CMATH, which\nconsists of two major components, i.e., Multimodal Interaction Fusion and\nHierarchical Variational Distillation. The former is comprised of two\nsubmodules, including Modality Reconstruction and Cross-Modality Augmented\nTransformer (CMA-Transformer), where Modality Reconstruction focuses on\nobtaining high-quality compressed representation of each modality, and\nCMA-Transformer adopts an asymmetric fusion strategy which treats one modality\nas the central modality and takes others as auxiliary modalities. The latter\nfirst designs a variational fusion network to fuse the fine-grained\nrepresentations learned by CMA- Transformer into a coarse-grained\nrepresentations. Then, it introduces a hierarchical distillation framework to\nmaintain the consistency between modality representations with different\ngranularities. Experiments on the IEMOCAP and MELD datasets demonstrate that\nour proposed model outperforms previous state-of-the-art baselines.\nImplementation codes can be available at https://github.com/ cjw-MER/CMATH.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.10060v1"
    },
    {
        "title": "MUFM: A Mamba-Enhanced Feedback Model for Micro Video Popularity\n  Prediction",
        "authors": [
            "Jiacheng Lu",
            "Mingyuan Xiao",
            "Weijian Wang",
            "Yuxin Du",
            "Yi Cui",
            "Jingnan Zhao",
            "Cheng Hua"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The surge in micro-videos is transforming the concept of popularity. As\nresearchers delve into vast multi-modal datasets, there is a growing interest\nin understanding the origins of this popularity and the forces driving its\nrapid expansion. Recent studies suggest that the virality of short videos is\nnot only tied to their inherent multi-modal content but is also heavily\ninfluenced by the strength of platform recommendations driven by audience\nfeedback. In this paper, we introduce a framework for capturing long-term\ndependencies in user feedback and dynamic event interactions, based on the\nMamba Hawkes process. Our experiments on the large-scale open-source\nmulti-modal dataset show that our model significantly outperforms\nstate-of-the-art approaches across various metrics by 23.2%. We believe our\nmodel's capability to map the relationships within user feedback behavior\nsequences will not only contribute to the evolution of next-generation\nrecommendation algorithms and platform applications but also enhance our\nunderstanding of micro video dissemination and its broader societal impact.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.15455v1"
    },
    {
        "title": "Advanced Learning-Based Inter Prediction for Future Video Coding",
        "authors": [
            "Yanchen Zhao",
            "Wenhong Duan",
            "Chuanmin Jia",
            "Shanshe Wang",
            "Siwei Ma"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In the fourth generation Audio Video coding Standard (AVS4), the Inter\nPrediction Filter (INTERPF) reduces discontinuities between prediction and\nadjacent reconstructed pixels in inter prediction. The paper proposes a low\ncomplexity learning-based inter prediction (LLIP) method to replace the\ntraditional INTERPF. LLIP enhances the filtering process by leveraging a\nlightweight neural network model, where parameters can be exported for\nefficient inference. Specifically, we extract pixels and coordinates utilized\nby the traditional INTERPF to form the training dataset. Subsequently, we\nexport the weights and biases of the trained neural network model and implement\nthe inference process without any third-party dependency, enabling seamless\nintegration into video codec without relying on Libtorch, thus achieving faster\ninference speed. Ultimately, we replace the traditional handcraft filtering\nparameters in INTERPF with the learned optimal filtering parameters. This\npractical solution makes the combination of deep learning encoding tools with\ntraditional video encoding schemes more efficient. Experimental results show\nthat our approach achieves 0.01%, 0.31%, and 0.25% coding gain for the Y, U,\nand V components under the random access (RA) configuration on average.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.15759v1"
    },
    {
        "title": "Subjective and Objective Quality Assessment Methods of Stereoscopic\n  Videos with Visibility Affecting Distortions",
        "authors": [
            "Sria Biswas",
            "Balasubramanyam Appina",
            "Priyanka Kokil",
            "Sumohana S Channappayya"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  We present two major contributions in this work: 1) we create a full HD\nresolution stereoscopic (S3D) video dataset comprised of 12 reference and 360\ndistorted videos. The test stimuli are produced by simulating the five levels\nof fog and haze ambiances on the pristine left and right video sequences. We\nperform subjective analysis on the created video dataset with 24 viewers and\ncompute Difference Mean Opinion Scores (DMOS) as quality representative of the\ndataset, 2) an Opinion Unaware (OU) and Distortion Unaware (DU) video quality\nassessment model is developed for S3D videos. We construct cyclopean frames\nfrom the individual views of an S3D video and partition them into\nnonoverlapping blocks. We analyze the Natural Scene Statistics (NSS) of all\npatches of pristine and test videos, and empirically model the NSS features\nwith Univariate Generalized Gaussian Distribution (UGGD). We compute UGGD model\nparameters ({\\alpha}, \\b{eta}) at multiple spatial scales and multiple\norientations of spherical steerable pyramid decomposition and show that the\nUGGD parameters are distortion discriminable. Further, we perform Multivariate\nGaussian (MVG) modeling on the pristine and distorted video feature sets and\ncompute the corresponding mean vectors and covariance matrices of MVG fits. We\ncompute the Bhattacharyya distance measure between mean vectors and covariance\nmatrices to estimate the perceptual deviation of a test video from pristine\nvideo set. Finally, we pool both distance measures to estimate the overall\nquality score of an S3D video. The performance of the proposed objective\nalgorithm is verified on the popular S3D video datasets such as IRCCYN,\nLFOVIAS3DPh1, LFOVIAS3DPh2 and the proposed VAD stereo dataset. The algorithm\ndelivers consistent performance across all datasets and shows competitive\nperformance against off-the-shelf 2D and 3D image and video quality assessment\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.19522v1"
    },
    {
        "title": "Hybrid Local-Global Context Learning for Neural Video Compression",
        "authors": [
            "Yongqi Zhai",
            "Jiayu Yang",
            "Wei Jiang",
            "Chunhui Yang",
            "Luyang Tang",
            "Ronggang Wang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In neural video codecs, current state-of-the-art methods typically adopt\nmulti-scale motion compensation to handle diverse motions. These methods\nestimate and compress either optical flow or deformable offsets to reduce\ninter-frame redundancy. However, flow-based methods often suffer from\ninaccurate motion estimation in complicated scenes. Deformable\nconvolution-based methods are more robust but have a higher bit cost for motion\ncoding. In this paper, we propose a hybrid context generation module, which\ncombines the advantages of the above methods in an optimal way and achieves\naccurate compensation at a low bit cost. Specifically, considering the\ncharacteristics of features at different scales, we adopt flow-guided\ndeformable compensation at largest-scale to produce accurate alignment in\ndetailed regions. For smaller-scale features, we perform flow-based warping to\nsave the bit cost for motion coding. Furthermore, we design a local-global\ncontext enhancement module to fully explore the local-global information of\nprevious reconstructed signals. Experimental results demonstrate that our\nproposed Hybrid Local-Global Context learning (HLGC) method can significantly\nenhance the state-of-the-art methods on standard test datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.00446v1"
    },
    {
        "title": "Multimodal Sentiment Analysis Based on Causal Reasoning",
        "authors": [
            "Fuhai Chen",
            "Pengpeng Huang",
            "Xuri Ge",
            "Jie Huang",
            "Zishuo Bao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  With the rapid development of multimedia, the shift from unimodal textual\nsentiment analysis to multimodal image-text sentiment analysis has obtained\nacademic and industrial attention in recent years. However, multimodal\nsentiment analysis is affected by unimodal data bias, e.g., text sentiment is\nmisleading due to explicit sentiment semantic, leading to low accuracy in the\nfinal sentiment classification. In this paper, we propose a novel\nCounterFactual Multimodal Sentiment Analysis framework (CF-MSA) using causal\ncounterfactual inference to construct multimodal sentiment causal inference.\nCF-MSA mitigates the direct effect from unimodal bias and ensures heterogeneity\nacross modalities by differentiating the treatment variables between\nmodalities. In addition, considering the information complementarity and bias\ndifferences between modalities, we propose a new optimisation objective to\neffectively integrate different modalities and reduce the inherent bias from\neach modality. Experimental results on two public datasets, MVSA-Single and\nMVSA-Multiple, demonstrate that the proposed CF-MSA has superior debiasing\ncapability and achieves new state-of-the-art performances. We will release the\ncode and datasets to facilitate future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.07292v1"
    },
    {
        "title": "Towards Open-Vocabulary Video Semantic Segmentation",
        "authors": [
            "Xinhao Li",
            "Yun Liu",
            "Guolei Sun",
            "Min Wu",
            "Le Zhang",
            "Ce Zhu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Semantic segmentation in videos has been a focal point of recent research.\nHowever, existing models encounter challenges when faced with unfamiliar\ncategories. To address this, we introduce the Open Vocabulary Video Semantic\nSegmentation (OV-VSS) task, designed to accurately segment every pixel across a\nwide range of open-vocabulary categories, including those that are novel or\npreviously unexplored. To enhance OV-VSS performance, we propose a robust\nbaseline, OV2VSS, which integrates a spatial-temporal fusion module, allowing\nthe model to utilize temporal relationships across consecutive frames.\nAdditionally, we incorporate a random frame enhancement module, broadening the\nmodel's understanding of semantic context throughout the entire video sequence.\nOur approach also includes video text encoding, which strengthens the model's\ncapability to interpret textual information within the video context.\nComprehensive evaluations on benchmark datasets such as VSPW and Cityscapes\nhighlight OV-VSS's zero-shot generalization capabilities, especially in\nhandling novel categories. The results validate OV2VSS's effectiveness,\ndemonstrating improved performance in semantic segmentation tasks across\ndiverse video datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.09329v1"
    },
    {
        "title": "Patch-level Sounding Object Tracking for Audio-Visual Question Answering",
        "authors": [
            "Zhangbin Li",
            "Jinxing Zhou",
            "Jing Zhang",
            "Shengeng Tang",
            "Kun Li",
            "Dan Guo"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Answering questions related to audio-visual scenes, i.e., the AVQA task, is\nbecoming increasingly popular. A critical challenge is accurately identifying\nand tracking sounding objects related to the question along the timeline. In\nthis paper, we present a new Patch-level Sounding Object Tracking (PSOT)\nmethod. It begins with a Motion-driven Key Patch Tracking (M-KPT) module, which\nrelies on visual motion information to identify salient visual patches with\nsignificant movements that are more likely to relate to sounding objects and\nquestions. We measure the patch-wise motion intensity map between neighboring\nvideo frames and utilize it to construct and guide a motion-driven graph\nnetwork. Meanwhile, we design a Sound-driven KPT (S-KPT) module to explicitly\ntrack sounding patches. This module also involves a graph network, with the\nadjacency matrix regularized by the audio-visual correspondence map. The M-KPT\nand S-KPT modules are performed in parallel for each temporal segment, allowing\nbalanced tracking of salient and sounding objects. Based on the tracked\npatches, we further propose a Question-driven KPT (Q-KPT) module to retain\npatches highly relevant to the question, ensuring the model focuses on the most\ninformative clues. The audio-visual-question features are updated during the\nprocessing of these modules, which are then aggregated for final answer\nprediction. Extensive experiments on standard datasets demonstrate the\neffectiveness of our method, achieving competitive performance even compared to\nrecent large-scale pretraining-based approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.10749v1"
    },
    {
        "title": "ChartAdapter: Large Vision-Language Model for Chart Summarization",
        "authors": [
            "Peixin Xu",
            "Yujuan Ding",
            "Wenqi Fan"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Chart summarization, which focuses on extracting key information from charts\nand interpreting it in natural language, is crucial for generating and\ndelivering insights through effective and accessible data analysis. Traditional\nmethods for chart understanding and summarization often rely on multi-stage\npipelines, which may produce suboptimal semantic alignment between visual and\ntextual information. In comparison, recently developed LLM-based methods are\nmore dependent on the capability of foundation images or languages, while\nignoring the characteristics of chart data and its relevant challenges. To\naddress these limitations, we propose ChartAdapter, a novel lightweight\ntransformer module designed to bridge the gap between charts and textual\nsummaries. ChartAdapter employs learnable query vectors to extract implicit\nsemantics from chart data and incorporates a cross-modal alignment projector to\nenhance vision-to-language generative learning. By integrating ChartAdapter\nwith an LLM, we enable end-to-end training and efficient chart summarization.\nTo further enhance the training, we introduce a three-stage hierarchical\ntraining procedure and develop a large-scale dataset specifically curated for\nchart summarization, comprising 190,618 samples. Experimental results on the\nstandard Chart-to-Text testing set demonstrate that our approach significantly\noutperforms existing methods, including state-of-the-art models, in generating\nhigh-quality chart summaries. Ablation studies further validate the\neffectiveness of key components in ChartAdapter. This work highlights the\npotential of tailored LLM-based approaches to advance chart understanding and\nsets a strong foundation for future research in this area.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.20715v1"
    },
    {
        "title": "MSM-BD: Multimodal Social Media Bot Detection Using Heterogeneous\n  Information",
        "authors": [
            "Tingxuan Wu",
            "Zhaorui Ma",
            "Yanjun Cui",
            "Ziyi Zhou",
            "Eric Wang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Although social bots can be engineered for constructive applications, their\npotential for misuse in manipulative schemes and malware distribution cannot be\noverlooked. This dichotomy underscores the critical need to detect social bots\non social media platforms. Advances in artificial intelligence have improved\nthe abilities of social bots, allowing them to generate content that is almost\nindistinguishable from human-created content. These advancements require the\ndevelopment of more advanced detection techniques to accurately identify these\nautomated entities. Given the heterogeneous information landscape on social\nmedia, spanning images, texts, and user statistical features, we propose\nMSM-BD, a Multimodal Social Media Bot Detection approach using heterogeneous\ninformation. MSM-BD incorporates specialized encoders for heterogeneous\ninformation and introduces a cross-modal fusion technology, Cross-Modal\nResidual Cross-Attention (CMRCA), to enhance detection accuracy. We validate\nthe effectiveness of our model through extensive experiments using the\nTwiBot-22 dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.00204v1"
    },
    {
        "title": "FlexCache: Flexible Approximate Cache System for Video Diffusion",
        "authors": [
            "Desen Sun",
            "Henry Tian",
            "Tim Lu",
            "Sihang Liu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Text-to-Video applications receive increasing attention from the public.\nAmong these, diffusion models have emerged as the most prominent approach,\noffering impressive quality in visual content generation. However, it still\nsuffers from substantial computational complexity, often requiring several\nminutes to generate a single video. While prior research has addressed the\ncomputational overhead in text-to-image diffusion models, the techniques\ndeveloped are not directly suitable for video diffusion models due to the\nsignificantly larger cache requirements and enhanced computational demands\nassociated with video generation.\n  We present FlexCache, a flexible approximate cache system that addresses the\nchallenges in two main designs. First, we compress the caches before saving\nthem to storage. Our compression strategy can reduce 6.7 times consumption on\naverage. Then we find that the approximate cache system can achieve higher hit\nrate and computation savings by decoupling the object and background. We\nfurther design a tailored cache replacement policy to support the two\ntechniques mentioned above better. Through our evaluation, FlexCache reaches\n1.26 times higher throughput and 25% lower cost compared to the\nstate-of-the-art diffusion approximate cache system.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.04012v1"
    },
    {
        "title": "Zero-shot Video Moment Retrieval via Off-the-shelf Multimodal Large\n  Language Models",
        "authors": [
            "Yifang Xu",
            "Yunzhuo Sun",
            "Benxiang Zhai",
            "Ming Li",
            "Wenxin Liang",
            "Yang Li",
            "Sidan Du"
        ],
        "category": "cs.MM",
        "published_year": "2025",
        "summary": "  The target of video moment retrieval (VMR) is predicting temporal spans\nwithin a video that semantically match a given linguistic query. Existing VMR\nmethods based on multimodal large language models (MLLMs) overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. Although some\nrecent studies introduce a zero-shot setting to avoid fine-tuning, they\noverlook inherent language bias in the query, leading to erroneous\nlocalization. To tackle the aforementioned challenges, this paper proposes\nMoment-GPT, a tuning-free pipeline for zero-shot VMR utilizing frozen MLLMs.\nSpecifically, we first employ LLaMA-3 to correct and rephrase the query to\nmitigate language bias. Subsequently, we design a span generator combined with\nMiniGPT-v2 to produce candidate spans adaptively. Finally, to leverage the\nvideo comprehension capabilities of MLLMs, we apply VideoChatGPT and span\nscorer to select the most appropriate spans. Our proposed method substantially\noutperforms the state-ofthe-art MLLM-based and zero-shot models on several\npublic datasets, including QVHighlights, ActivityNet-Captions, and\nCharades-STA.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.07972v1"
    },
    {
        "title": "On the Superdistribution of Digital Goods",
        "authors": [
            "Andreas U. Schmidt"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  Business models involving buyers of digital goods in the distribution process\nare called superdistribution schemes. We review the state-of-the art of\nresearch and application of superdistribution and propose systematic approach\nto market mechanisms using super-distribution and technical system\narchitectures supporting it. The limiting conditions on such markets are of\neconomic, legal, technical, and psychological nature.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.1543v1"
    },
    {
        "title": "Scalar Quantization for Audio Data Coding",
        "authors": [
            "Boris D. Kudryashov",
            "Anton V. Porov",
            "Eunmi L. Oh"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  This paper is concerned with scalar quantization of transform coefficients in\nan audio codec. The generalized Gaussian distribution (GGD) is used as an\napproximation of one-dimensional probability density function for transform\ncoefficients obtained by modulated lapped transform (MLT) or modified cosine\ntransform (MDCT) filterbank. The rationale of the model is provided in\ncomparison with theoretically achievable rate-distortion function. The\nrate-distortion function computed for the random sequence obtained from a real\nsequence of samples from a large database is compared with that computed for\nrandom sequence obtained by a GGD random generator. A simple algorithm of\nconstructing the Extended Zero Zone (EZZ) quantizer is proposed. Simulation\nresults show that the EZZ quantizer yields a negligible loss in terms of coding\nefficiency compared to optimal scalar quantizers. Furthermore, we describe an\nadaptive version of the EZZ quantizer which works efficiently with low bitrate\nrequirements for transmitting side information\n",
        "pdf_link": "http://arxiv.org/pdf/0806.4293v1"
    },
    {
        "title": "Approximate Sparse Decomposition Based on Smoothed L0-Norm",
        "authors": [
            "Hamed Firouzi",
            "Masoud Farivar",
            "Massoud Babaie-Zadeh",
            "Christian Jutten"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  In this paper, we propose a method to address the problem of source\nestimation for Sparse Component Analysis (SCA) in the presence of additive\nnoise. Our method is a generalization of a recently proposed method (SL0),\nwhich has the advantage of directly minimizing the L0-norm instead of L1-norm,\nwhile being very fast. SL0 is based on minimization of the smoothed L0-norm\nsubject to As=x. In order to better estimate the source vector for noisy\nmixtures, we suggest then to remove the constraint As=x, by relaxing exact\nequality to an approximation (we call our method Smoothed L0-norm Denoising or\nSL0DN). The final result can then be obtained by minimization of a proper\nlinear combination of the smoothed L0-norm and a cost function for the\napproximation. Experimental results emphasize on the significant enhancement of\nthe modified method in noisy cases.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.2868v1"
    },
    {
        "title": "Wide spread spectrum watermarking with side information and interference\n  cancellation",
        "authors": [
            "Gaëtan Le Guelvouit",
            "Stéphane Pateux"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  Nowadays, a popular method used for additive watermarking is wide spread\nspectrum. It consists in adding a spread signal into the host document. This\nsignal is obtained by the sum of a set of carrier vectors, which are modulated\nby the bits to be embedded. To extract these embedded bits, weighted\ncorrelations between the watermarked document and the carriers are computed.\nUnfortunately, even without any attack, the obtained set of bits can be\ncorrupted due to the interference with the host signal (host interference) and\nalso due to the interference with the others carriers (inter-symbols\ninterference (ISI) due to the non-orthogonality of the carriers). Some recent\nwatermarking algorithms deal with host interference using side informed\nmethods, but inter-symbols interference problem is still open. In this paper,\nwe deal with interference cancellation methods, and we propose to consider ISI\nas side information and to integrate it into the host signal. This leads to a\ngreat improvement of extraction performance in term of signal-to-noise ratio\nand/or watermark robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.4483v1"
    },
    {
        "title": "Trellis-coded quantization for public-key steganography",
        "authors": [
            "Gaëtan Le Guelvouit"
        ],
        "category": "cs.MM",
        "published_year": "2008",
        "summary": "  This paper deals with public-key steganography in the presence of a passive\nwarden. The aim is to hide secret messages within cover-documents without\nmaking the warden suspicious, and without any preliminar secret key sharing.\nWhereas a practical attempt has been already done to provide a solution to this\nproblem, it suffers of poor flexibility (since embedding and decoding steps\nhighly depend on cover-signals statistics) and of little capacity compared to\nrecent data hiding techniques. Using the same framework, this paper explores\nthe use of trellis-coded quantization techniques (TCQ and turbo TCQ) to design\na more efficient public-key scheme. Experiments on audio signals show great\nimprovements considering Cachin's security criterion.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.4700v1"
    },
    {
        "title": "Efficiently Learning a Detection Cascade with Sparse Eigenvectors",
        "authors": [
            "Chunhua Shen",
            "Sakrapee Paisitkriangkrai",
            "Jian Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  In this work, we first show that feature selection methods other than\nboosting can also be used for training an efficient object detector. In\nparticular, we introduce Greedy Sparse Linear Discriminant Analysis (GSLDA)\n\\cite{Moghaddam2007Fast} for its conceptual simplicity and computational\nefficiency; and slightly better detection performance is achieved compared with\n\\cite{Viola2004Robust}. Moreover, we propose a new technique, termed Boosted\nGreedy Sparse Linear Discriminant Analysis (BGSLDA), to efficiently train a\ndetection cascade. BGSLDA exploits the sample re-weighting property of boosting\nand the class-separability criterion of GSLDA.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.3103v1"
    },
    {
        "title": "Structural Solutions for Cross-Layer Optimization of Wireless Multimedia\n  Transmission",
        "authors": [
            "Fangwen Fu",
            "Mihaela van der Schaar"
        ],
        "category": "cs.MM",
        "published_year": "2009",
        "summary": "  In this paper, we propose a systematic solution to the problem of cross-layer\noptimization for delay-sensitive media transmission over time-varying wireless\nchannels as well as investigate the structures and properties of this solution,\nsuch that it can be easily implemented in various multimedia systems and\napplications. Specifically, we formulate this problem as a finite-horizon\nMarkov decision process (MDP) by explicitly considering the users'\nheterogeneous multimedia traffic characteristics (e.g. delay deadlines,\ndistortion impacts and dependencies etc.), time-varying network conditions as\nwell as, importantly, their ability to adapt their cross-layer transmission\nstrategies in response to these dynamics. Based on the heterogeneous\ncharacteristics of the media packets, we are able to express the transmission\npriorities between packets as a new type of directed acyclic graph (DAG). This\nDAG provides the necessary structure for determining the optimal cross-layer\nactions in each time slot: the root packet in the DAG will always be selected\nfor transmission since it has the highest positive marginal utility; and the\ncomplexity of the proposed cross-layer solution is demonstrated to linearly\nincrease w.r.t. the number of disconnected packet pairs in the DAG and\nexponentially increase w.r.t. the number of packets on which the current\npackets depend on. The simulation results demonstrate that the proposed\nsolution significantly outperforms existing state-of-the-art cross-layer\nsolutions. Moreover, we show that our solution provides the upper bound\nperformance for the cross-layer optimization solutions with delayed feedback\nsuch as the well-known RaDiO framework.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.4087v1"
    },
    {
        "title": "Structural Solutions to Dynamic Scheduling for Multimedia Transmission\n  in Unknown Wireless Environments",
        "authors": [
            "Fangwen Fu",
            "Mihaela van der Schaar"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  In this paper, we propose a systematic solution to the problem of scheduling\ndelay-sensitive media data for transmission over time-varying wireless\nchannels. We first formulate the dynamic scheduling problem as a Markov\ndecision process (MDP) that explicitly considers the users' heterogeneous\nmultimedia data characteristics (e.g. delay deadlines, distortion impacts and\ndependencies etc.) and time-varying channel conditions, which are not\nsimultaneously considered in state-of-the-art packet scheduling algorithms.\nThis formulation allows us to perform foresighted decisions to schedule\nmultiple data units for transmission at each time in order to optimize the\nlong-term utilities of the multimedia applications. The heterogeneity of the\nmedia data enables us to express the transmission priorities between the\ndifferent data units as a priority graph, which is a directed acyclic graph\n(DAG). This priority graph provides us with an elegant structure to decompose\nthe multi-data unit foresighted decision at each time into multiple single-data\nunit foresighted decisions which can be performed sequentially, from the high\npriority data units to the low priority data units, thereby significantly\nreducing the computation complexity. When the statistical knowledge of the\nmultimedia data characteristics and channel conditions is unknown a priori, we\ndevelop a low-complexity online learning algorithm to update the value\nfunctions which capture the impact of the current decision on the future\nutility. The simulation results show that the proposed solution significantly\noutperforms existing state-of-the-art scheduling solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.4406v1"
    },
    {
        "title": "M-Learning: A New Paradigm of Learning Mathematics in Malaysia",
        "authors": [
            "Saipunidzam Mahamad",
            "Mohammad Noor Ibrahim",
            "Shakirah Mohd Taib"
        ],
        "category": "cs.MM",
        "published_year": "2010",
        "summary": "  M-Learning is a new learning paradigm of the new social structure with mobile\nand wireless technologies.Smart school is one of the four flagship applications\nfor Multimedia Super Corridor (MSC) under Malaysian government initiative to\nimprove education standard in the country. With the advances of mobile devices\ntechnologies, mobile learning could help the government in realizing the\ninitiative. This paper discusses the prospect of implementing mobile learning\nfor primary school students. It indicates significant and challenges and\nanalysis of user perceptions on potential mobile applications through a survey\ndone in primary school context. The authors propose the m-Learning for\nmathematics by allowing the extension of technology in the traditional\nclassroom in term of learning and teaching.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.1170v1"
    },
    {
        "title": "Selection of network coding nodes for minimal playback delay in\n  streaming overlays",
        "authors": [
            "Nicolae Cleju",
            "Nikolaos Thomos",
            "Pascal Frossard"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Network coding permits to deploy distributed packet delivery algorithms that\nlocally adapt to the network availability in media streaming applications.\nHowever, it may also increase delay and computational complexity if it is not\nimplemented efficiently. We address here the effective placement of nodes that\nimplement randomized network coding in overlay networks, so that the goodput is\nkept high while the delay for decoding stays small in streaming applications.\nWe first estimate the decoding delay at each client, which depends on the\ninnovative rate in the network. This estimation permits to identify the nodes\nthat have to perform coding for a reduced decoding delay. We then propose two\niterative algorithms for selecting the nodes that should perform network\ncoding. The first algorithm relies on the knowledge of the full network\nstatistics. The second algorithm uses only local network statistics at each\nnode. Simulation results show that large performance gains can be achieved with\nthe selection of only a few network coding nodes. Moreover, the second\nalgorithm performs very closely to the central estimation strategy, which\ndemonstrates that the network coding nodes can be selected efficiently in a\ndistributed manner. Our scheme shows large gains in terms of achieved\nthroughput, delay and video quality in realistic overlay networks when compared\nto methods that employ traditional streaming strategies as well as random\nnetwork nodes selection algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.3979v1"
    },
    {
        "title": "Compression and Quantitative Analysis of Buffer Map Message in P2P\n  Streaming System",
        "authors": [
            "Chunxi Li",
            "Changjia Chen",
            "DahMing Chiu"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  BM compression is a straightforward and operable way to reduce buffer message\nlength as well as to improve system performance. In this paper, we thoroughly\ndiscuss the principles and protocol progress of different compression schemes,\nand for the first time present an original compression scheme which can nearly\nremove all redundant information from buffer message. Theoretical limit of\ncompression rates are deduced in the theory of information. Through the\nanalysis of information content and simulation with our measured BM trace of\nUUSee, the validity and superiority of our compression scheme are validated in\nterm of compression ratio.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.6290v2"
    },
    {
        "title": "Buffer Map Message Compression Based on Relevant Window in P2P Streaming\n  Media System",
        "authors": [
            "Chunxi Li",
            "Changjia Chen",
            "DahMing Chiu"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Popular peer to peer streaming media systems such as PPLive and UUSee rely on\nperiodic buffer-map exchange between peers for proper operation. The buffer-map\nexchange contains redundant information which causes non-negligible overhead.\nIn this paper we present a theoretical framework to study how the overhead can\nbe lowered. Differentiating from the traditional data compression approach, we\ndo not treat each buffer-map as an isolated data block, but consider the\ncorrelations between the sequentially exchanged buffer-maps. Under this\nframework, two buffer-map compression schemes are proposed and the correctness\nof the schemes is proved mathematically. Moreover, we derive the theoretical\nlimit of compression gain based on probability theory and information theory.\nBased on the system parameters of UUSee (a popular P2P streaming platform), our\nsimulations show that the buffer-map sizes are reduced by 86% and 90% (from 456\nbits down to only 66 bits and 46 bits) respectively after applying our schemes.\nFurthermore, by combining with the traditional compression methods (on\nindividual blocks), the sizes are decreased by 91% and 95% (to 42 bits and 24\nbits) respectively. Our study provides a guideline for developing practical\ncompression algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.6293v3"
    },
    {
        "title": "Storage Balancing in Self-organizing Multimedia Delivery Systems",
        "authors": [
            "Anita Sobe",
            "Wilfried Elmenreich",
            "Laszlo Böszörmenyi"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  Many of the current bio-inspired delivery networks set their focus on search,\ne.g., by using artificial ants. If the network size and, therefore, the search\nspace gets too large, the users experience high delays until the requested\ncontent can be consumed. In previous work, we proposed different replication\nstrategies to reduce the search space. In this report we further evaluate\nmeasures for storage load balancing, because peers are most likely limited in\nspace. We periodically apply clean-ups if a certain storage level is reached.\nFor our evaluations we combine the already introduced replication measures with\nleast recently used (LRU), least frequently used (LFU) and a hormone-based\nclean-up. The goal is to elaborate a combination that leads to low delays while\nthe replica utilization is high.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.0242v1"
    },
    {
        "title": "A Hybrid Image Cryptosystem Based On OMFLIP Permutation Cipher",
        "authors": [
            "G. Sudheer",
            "B. V. S. Renuka Devi"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  The protection of confidential image data from unauthorized access is an\nimportant area of research in network communication. This paper presents a\nhigh-level security encryption scheme for gray scale images. The gray level\nimage is first decomposed into binary images using bit scale decomposition.\nEach binary image is then compressed by selecting a good scanning path that\nminimizes the total number of bits needed to encode the bit sequence along the\nscanning path using two dimensional run encoding. The compressed bit string is\nthen scrambled iteratively using a pseudo-random number generator and finally\nencrypted using a bit level permutation OMFLIP. The performance is tested,\nillustrated and discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.2147v1"
    },
    {
        "title": "Compressed Sensing for Moving Imagery in Medical Imaging",
        "authors": [
            "Cagdas Bilen",
            "Yao Wang",
            "Ivan Selesnick"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Numerous applications in signal processing have benefited from the theory of\ncompressed sensing which shows that it is possible to reconstruct signals\nsampled below the Nyquist rate when certain conditions are satisfied. One of\nthese conditions is that there exists a known transform that represents the\nsignal with a sufficiently small number of non-zero coefficients. However when\nthe signal to be reconstructed is composed of moving images or volumes, it is\nchallenging to form such regularization constraints with traditional transforms\nsuch as wavelets. In this paper, we present a motion compensating prior for\nsuch signals that is derived directly from the optical flow constraint and can\nutilize the motion information during compressed sensing reconstruction.\nProposed regularization method can be used in a wide variety of applications\ninvolving compressed sensing and images or volumes of moving and deforming\nobjects. It is also shown that it is possible to estimate the signal and the\nmotion jointly or separately. Practical examples from magnetic resonance\nimaging has been presented to demonstrate the benefit of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.5772v1"
    },
    {
        "title": "Simplification Resilient LDPC-Coded Sparse-QIM Watermarking for\n  3D-Meshes",
        "authors": [
            "Bata Vasic",
            "Bane Vasic"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  We propose a blind watermarking scheme for 3-D meshes which combines sparse\nquantization index modulation (QIM) with deletion correction codes. The QIM\noperates on the vertices in rough concave regions of the surface thus ensuring\nimpeccability, while the deletion correction code recovers the data hidden in\nthe vertices which is removed by mesh optimization and/or simplification. The\nproposed scheme offers two orders of magnitude better performance in terms of\nrecovered watermark bit error rate compared to the existing schemes of similar\npayloads and fidelity constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.2214v3"
    },
    {
        "title": "Signal and Image Processing with Sinlets",
        "authors": [
            "Alexander Y. Davydov"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  This paper presents a new family of localized orthonormal bases - sinlets -\nwhich are well suited for both signal and image processing and analysis.\nOne-dimensional sinlets are related to specific solutions of the time-dependent\nharmonic oscillator equation. By construction, each sinlet is infinitely\ndifferentiable and has a well-defined and smooth instantaneous frequency known\nin analytical form. For square-integrable transient signals with infinite\nsupport, one-dimensional sinlet basis provides an advantageous alternative to\nthe Fourier transform by rendering accurate signal representation via a\ncountable set of real-valued coefficients. The properties of sinlets make them\nsuitable for analyzing many real-world signals whose frequency content changes\nwith time including radar and sonar waveforms, music, speech, biological\necholocation sounds, biomedical signals, seismic acoustic waves, and signals\nemployed in wireless communication systems. One-dimensional sinlet bases can be\nused to construct two- and higher-dimensional bases with variety of potential\napplications including image analysis and representation.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.0692v3"
    },
    {
        "title": "Sampling and Reconstruction of Spatial Fields using Mobile Sensors",
        "authors": [
            "Jayakrishnan Unnikrishnan",
            "Martin Vetterli"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Spatial sampling is traditionally studied in a static setting where static\nsensors scattered around space take measurements of the spatial field at their\nlocations. In this paper we study the emerging paradigm of sampling and\nreconstructing spatial fields using sensors that move through space. We show\nthat mobile sensing offers some unique advantages over static sensing in\nsensing time-invariant bandlimited spatial fields. Since a moving sensor\nencounters such a spatial field along its path as a time-domain signal, a\ntime-domain anti-aliasing filter can be employed prior to sampling the signal\nreceived at the sensor. Such a filtering procedure, when used by a\nconfiguration of sensors moving at constant speeds along equispaced parallel\nlines, leads to a complete suppression of spatial aliasing in the direction of\nmotion of the sensors. We analytically quantify the advantage of using such a\nsampling scheme over a static sampling scheme by computing the reduction in\nsampling noise due to the filter. We also analyze the effects of non-uniform\nsensor speeds on the reconstruction accuracy. Using simulation examples we\ndemonstrate the advantages of mobile sampling over static sampling in practical\nproblems.\n  We extend our analysis to sampling and reconstruction schemes for monitoring\ntime-varying bandlimited fields using mobile sensors. We demonstrate that in\nsome situations we require a lower density of sensors when using a mobile\nsensing scheme instead of the conventional static sensing scheme. The exact\nadvantage is quantified for a problem of sampling and reconstructing an audio\nfield.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.0135v1"
    },
    {
        "title": "Bounding Lossy Compression using Lossless Codes at Reduced Precision",
        "authors": [
            "John Scoville"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  An alternative approach to two-part 'critical compression' is presented.\nWhereas previous results were based on summing a lossless code at reduced\nprecision with a lossy-compressed error or noise term, the present approach\nuses a similar lossless code at reduced precision to establish absolute bounds\nwhich constrain an arbitrary lossy data compression algorithm applied to the\noriginal data.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.0026v1"
    },
    {
        "title": "A new compressive video sensing framework for mobile broadcast",
        "authors": [
            "Chengbo Li",
            "Hong Jiang",
            "Paul Wilford",
            "Yin Zhang",
            "Mike Scheutzow"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  A new video coding method based on compressive sampling is proposed. In this\nmethod, a video is coded using compressive measurements on video cubes. Video\nreconstruction is performed by minimization of total variation (TV) of the\npixelwise DCT coefficients along the temporal direction. A new reconstruction\nalgorithm is developed from TVAL3, an efficient TV minimization algorithm based\non the alternating minimization and augmented Lagrangian methods. Video coding\nwith this method is inherently scalable, and has applications in mobile\nbroadcast.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.1947v1"
    },
    {
        "title": "An Optical Watermarking Solution for Color Personal Identification\n  Pictures",
        "authors": [
            "Tan Yi-zhou",
            "Liu Hai-bo",
            "Huang Shui-hua",
            "Sheng Ben-jian",
            "Pan Zhong-ming"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  This paper presents a new approach for embedding authentication information\ninto image on printed materials based on optical projection technique. Our\nexperimental setup consists of two parts, one is a common camera, and the other\nis a LCD projector, which project a pattern on personnel's body (especially on\nthe face). The pattern, generated by a computer, act as the illumination light\nsource with sinusoidal distribution and it is also the watermark signal. For a\ncolor image, the watermark is embedded into the blue channel. While we take\npictures (256 *256 and 512*512, 567*390 pixels, respectively), an invisible\nmark is embedded directly into magnitude oefficients of Discrete Fourier\ntransform (DFT) at exposure moment. Both optical an d digital correlation is\nsuitable for detection of this type of watermark. The decoded watermark is a\nset of concentric circles or sectors in the DFT domain (middle frequencies\nregion) which is robust to photographing, printing and scanning. The unlawful\npeople modify or replace the original photograph, and make fake passport\n(drivers' license and so on). Experiments show, it is difficult to forge\ncertificates in which a watermark was embedded by our projector-camera\ncombination based on analogue watermark method rather than classical digital\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.4784v1"
    },
    {
        "title": "Anticipatory Buffer Control and Resource Allocation for Wireless Video\n  Streaming",
        "authors": [
            "Sanam Sadr",
            "Stefan Valentin"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  This paper describes a new approach for allocating resources to video\nstreaming traffic. Assuming that the future channel state can be predicted for\na certain time, we minimize the fraction of the bandwidth consumed for smooth\nstreaming by jointly allocating wireless channel resources and play-out buffer\nsize. To formalize this idea, we introduce a new model to capture the dynamic\nof a video streaming buffer and the allocated spectrum in an optimization\nproblem. The result is a Linear Program that allows to trade off buffer size\nand allocated bandwidth. Based on this tractable model, our simulation results\nshow that anticipating poor channel states and pre-loading the buffer\naccordingly allows to serve more users at perfect video quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3056v1"
    },
    {
        "title": "Arabic Text Recognition in Video Sequences",
        "authors": [
            "M. Ben Halima",
            "H. Karray",
            "A. M. Alimi"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  In this paper, we propose a robust approach for text extraction and\nrecognition from Arabic news video sequence. The text included in video\nsequences is an important needful for indexing and searching system. However,\nthis text is difficult to detect and recognize because of the variability of\nits size, their low resolution characters and the complexity of the\nbackgrounds. To solve these problems, we propose a system performing in two\nmain tasks: extraction and recognition of text. Our system is tested on a\nvaried database composed of different Arabic news programs and the obtained\nresults are encouraging and show the merits of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.3243v1"
    },
    {
        "title": "Chaotic Arithmetic Coding for Secure Video Multicast",
        "authors": [
            "Gaurav Pande"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Arithmetic Coding (AC) is widely used for the entropy coding of text and\nvideo data. It involves recursive partitioning of the range [0,1) in accordance\nwith the relative probabilities of occurrence of the input symbols. A data\n(image or video) encryption scheme based on arithmetic coding called as Chaotic\nArithmetic Coding (CAC) has been presented in previous works. In CAC, a large\nnumber of chaotic maps can be used to perform coding, each achieving Shannon\noptimal compression performance. The exact choice of map is governed by a key.\nCAC has the effect of scrambling the intervals without making any changes to\nthe width of interval in which the codeword must lie, thereby allowing\nencryption without sacrificing any coding efficiency. In this paper, we use a\nredundancy in CAC procedure for secure multicast of videos where multiple users\nare distributed with different keys to decode same encrypted file. By\nencrypting once, we can generate multiple keys, either of which can be used to\ndecrypt the encoded file. This is very suitable for video distribution over\nInternet where a single video can be distributed to multiple clients in a\nprivacy preserving manner.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.3220v1"
    },
    {
        "title": "A Multiplierless Pruned DCT-like Transformation for Image and Video\n  Compression that Requires 10 Additions Only",
        "authors": [
            "V. A. Coutinho",
            "R. J. Cintra",
            "F. M. Bayer",
            "S. Kulasekera",
            "A. Madanayake"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  A multiplierless pruned approximate 8-point discrete cosine transform (DCT)\nrequiring only 10 additions is introduced. The proposed algorithm was assessed\nin image and video compression, showing competitive performance with\nstate-of-the-art methods. Digital implementation in 45 nm CMOS technology up to\nplace-and-route level indicates clock speed of 288 MHz at a 1.1 V supply. The\n8x8 block rate is 36 MHz.The DCT approximation was embedded into HEVC reference\nsoftware; resulting video frames, at up to 327 Hz for 8-bit RGB HEVC, presented\nnegligible image degradation.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.5979v2"
    },
    {
        "title": "A DCT Approximation for Image Compression",
        "authors": [
            "R. J. Cintra",
            "F. M. Bayer"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  An orthogonal approximation for the 8-point discrete cosine transform (DCT)\nis introduced. The proposed transformation matrix contains only zeros and ones;\nmultiplications and bit-shift operations are absent. Close spectral behavior\nrelative to the DCT was adopted as design criterion. The proposed algorithm is\nsuperior to the signed discrete cosine transform. It could also outperform\nstate-of-the-art algorithms in low and high image compression scenarios,\nexhibiting at the same time a comparable computational complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.6034v1"
    },
    {
        "title": "Saving Energy in Mobile Devices for On-Demand Multimedia Streaming -- A\n  Cross-Layer Approach",
        "authors": [
            "Mohammad Ashraful Hoque",
            "Matti Siekkinen",
            "Jukka K. Nurminen",
            "Sasu Tarkoma",
            "Mika Aalto"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  This paper proposes a novel energy-efficient multimedia delivery system\ncalled EStreamer. First, we study the relationship between buffer size at the\nclient, burst-shaped TCP-based multimedia traffic, and energy consumption of\nwireless network interfaces in smartphones. Based on the study, we design and\nimplement EStreamer for constant bit rate and rate-adaptive streaming.\nEStreamer can improve battery lifetime by 3x, 1.5x and 2x while streaming over\nWi-Fi, 3G and 4G respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.3710v1"
    },
    {
        "title": "The Art of Data Hiding with Reed-Solomon Error Correcting Codes",
        "authors": [
            "Fredrick R. Ishengoma"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  With the tremendous advancements in technology and the Internet, data\nsecurity has become a major issue around the globe. To guarantee that data is\nprotected and does not go to an unintended endpoint, the art of data hiding\n(steganography) emerged. Steganography is the art of hiding information such\nthat it is not detectable to the naked eye. Various techniques have been\nproposed for hiding a secret message in a carrier document. In this paper, we\npresent a novel design that applies Reed-Solomon (RS) error correcting codes in\nsteganographic applications. The model works by substituting the redundant RS\ncodes with the steganographic message. The experimental results show that the\nproposed design is satisfactory with the percentage of decoded information 100%\nand percentage of decoded secret message 97. 36%. The proposed model proved\nthat it could be applied in various steganographic applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.4790v1"
    },
    {
        "title": "Fragile Watermarking Using Finite Field Trigonometrical Transforms",
        "authors": [
            "R. J. Cintra",
            "V. S. Dimitrov",
            "H. M. de Oliveira",
            "R. M. Campello de Souza"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Fragile digital watermarking has been applied for authentication and\nalteration detection in images. Utilizing the cosine and Hartley transforms\nover finite fields, a new transform domain fragile watermarking scheme is\nintroduced. A watermark is embedded into a host image via a blockwise\napplication of two-dimensional finite field cosine or Hartley transforms.\nAdditionally, the considered finite field transforms are adjusted to be number\ntheoretic transforms, appropriate for error-free calculation. The employed\ntechnique can provide invisible fragile watermarking for authentication systems\nwith tamper location capability. It is shown that the choice of the finite\nfield characteristic is pivotal to obtain perceptually invisible watermarked\nimages. It is also shown that the generated watermarked images can be used as\npublicly available signature data for authentication purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00296v1"
    },
    {
        "title": "Compressive Sensing of Large-Scale Images: An Assumption-Free Approach",
        "authors": [
            "Wei-Jie Liang",
            "Gang-Xuan Lin",
            "Chun-Shien Lu"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Cost-efficient compressive sensing of big media data with fast reconstructed\nhigh-quality results is very challenging. In this paper, we propose a new\nlarge-scale image compressive sensing method, composed of operator-based\nstrategy in the context of fixed point continuation method and weighted LASSO\nwith tree structure sparsity pattern. The main characteristic of our method is\nfree from any assumptions and restrictions. The feasibility of our method is\nverified via simulations and comparisons with state-of-the-art algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.05407v1"
    },
    {
        "title": "Tree-based Visualization and Optimization for Image Collection",
        "authors": [
            "Xintong Han",
            "Chongyang Zhang",
            "Weiyao Lin",
            "Mingliang Xu",
            "Bin Sheng",
            "Tao Mei"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The visualization of an image collection is the process of displaying a\ncollection of images on a screen under some specific layout requirements. This\npaper focuses on an important problem that is not well addressed by the\nprevious methods: visualizing image collections into arbitrary layout shapes\nwhile arranging images according to user-defined semantic or visual\ncorrelations (e.g., color or object category). To this end, we first propose a\nproperty-based tree construction scheme to organize images of a collection into\na tree structure according to user-defined properties. In this way, images can\nbe adaptively placed with the desired semantic or visual correlations in the\nfinal visualization layout. Then, we design a two-step visualization\noptimization scheme to further optimize image layouts. As a result, multiple\nlayout effects including layout shape and image overlap ratio can be\neffectively controlled to guarantee a satisfactory visualization. Finally, we\nalso propose a tree-transfer scheme such that visualization layouts can be\nadaptively changed when users select different \"images of interest\". We\ndemonstrate the effectiveness of our proposed approach through the comparisons\nwith state-of-the-art visualization techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04913v1"
    },
    {
        "title": "Approaching Maximum Embedding Efficiency on Small Covers Using\n  Staircase-Generator Codes",
        "authors": [
            "Simona Samardjiska",
            "Danilo Gligoroski"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  We introduce a new family of binary linear codes suitable for steganographic\nmatrix embedding. The main characteristic of the codes is the staircase random\nblock structure of the generator matrix. We propose an efficient list decoding\nalgorithm for the codes that finds a close codeword to a given random word. We\nprovide both theoretical analysis of the performance and stability of the\ndecoding algorithm, as well as practical results. Used for matrix embedding,\nthese codes achieve almost the upper theoretical bound of the embedding\nefficiency for covers in the range of 1000 - 1500 bits, which is at least an\norder of magnitude smaller than the values reported in related works.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.02284v1"
    },
    {
        "title": "Dot-Diffused Halftoning with Improved Homogeneity",
        "authors": [
            "Yun-Fu Liu",
            "Jing-Ming Guo"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Compared to the error diffusion, dot diffusion provides an additional\npixel-level parallelism for digital halftoning. However, even though its\nperiodic and blocking artifacts had been eased by previous works, it was still\nfar from satisfactory in terms of the blue noise spectrum perspective. In this\nwork, we strengthen the relationship among the pixel locations of the same\nprocessing order by an iterative halftoning method, and the results demonstrate\na significant improvement. Moreover, a new approach of deriving the averaged\npower spectrum density (APSD) is proposed to avoid the regular sampling of the\nwell-known Bartlett's procedure which inaccurately presents the halftone\nperiodicity of certain halftoning techniques with parallelism. As a result, the\nproposed dot diffusion is substantially superior to the state-of-the-art\nparallel halftoning methods in terms of visual quality and artifact-free\nproperty, and competitive runtime to the theoretical fastest ordered dithering\nis offered simultaneously.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05373v1"
    },
    {
        "title": "Reversible Denoising and Lifting Based Color Component Transformation\n  for Lossless Image Compression",
        "authors": [
            "Roman Starosolski"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  An undesirable side effect of reversible color space transformation, which\nconsists of lifting steps (LSs), is that while removing correlation it\ncontaminates transformed components with noise from other components. Noise\naffects particularly adversely the compression ratios of lossless compression\nalgorithms. To remove correlation without increasing noise, a reversible\ndenoising and lifting step (RDLS) was proposed that integrates denoising\nfilters into LS. Applying RDLS to color space transformation results in a new\nimage component transformation that is perfectly reversible despite involving\nthe inherently irreversible denoising; the first application of such a\ntransformation is presented in this paper. For the JPEG-LS, JPEG 2000, and JPEG\nXR standard algorithms in lossless mode, the application of RDLS to the RDgDb\ncolor space transformation with simple denoising filters is especially\neffective for images in the native optical resolution of acquisition devices.\nIt results in improving compression ratios of all those images in cases when\nunmodified color space transformation either improves or worsens ratios\ncompared with the untransformed image. The average improvement is 5.0-6.0\\% for\ntwo out of the three sets of such images, whereas average ratios of images from\nstandard test-sets are improved by up to 2.2\\%. For the efficient\nimage-adaptive determination of filters for RDLS, a couple of fast\nentropy-based estimators of compression effects that may be used independently\nof the actual compression algorithm are investigated and an immediate filter\nselection method based on the detector precision characteristic model driven by\nimage acquisition parameters is introduced.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.06106v4"
    },
    {
        "title": "Real Time Video Quality Representation Classification of Encrypted HTTP\n  Adaptive Video Streaming - the Case of Safari",
        "authors": [
            "Ran Dubin",
            "Amit Dvir",
            "Ofir Pele",
            "Ofer Hadar",
            "Itay Richman",
            "Ofir Trabelsi"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The increasing popularity of HTTP adaptive video streaming services has\ndramatically increased bandwidth requirements on operator networks, which\nattempt to shape their traffic through Deep Packet Inspection (DPI). However,\nGoogle and certain content providers have started to encrypt their video\nservices. As a result, operators often encounter difficulties in shaping their\nencrypted video traffic via DPI. This highlights the need for new traffic\nclassification methods for encrypted HTTP adaptive video streaming to enable\nsmart traffic shaping. These new methods will have to effectively estimate the\nquality representation layer and playout buffer. We present a new method and\nshow for the first time that video quality representation classification for\n(YouTube) encrypted HTTP adaptive streaming is possible. We analyze the\nperformance of this classification method with Safari over HTTPS. Based on a\nlarge number of offline and online traffic classification experiments, we\ndemonstrate that it can independently classify, in real time, every video\nsegment into one of the quality representation layers with 97.18% average\naccuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.00489v2"
    },
    {
        "title": "I Know What You Saw Last Minute - Encrypted HTTP Adaptive Video\n  Streaming Title Classification",
        "authors": [
            "Ran Dubin",
            "Amit Dvir",
            "Ofir Pele",
            "Ofer Hadar"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Desktops and laptops can be maliciously exploited to violate privacy. There\nare two main types of attack scenarios: active and passive. In this paper, we\nconsider the passive scenario where the adversary does not interact actively\nwith the device, but he is able to eavesdrop on the network traffic of the\ndevice from the network side. Most of the Internet traffic is encrypted and\nthus passive attacks are challenging. Previous research has shown that\ninformation can be extracted from encrypted multimedia streams. This includes\nvideo title classification of non HTTP adaptive streams (non-HAS). This paper\npresents an algorithm for encrypted HTTP adaptive video streaming title\nclassification. We show that an external attacker can identify the video title\nfrom video HTTP adaptive streams (HAS) sites such as YouTube. To the best of\nour knowledge, this is the first work that shows this. We provide a large data\nset of 10000 YouTube video streams of 100 popular video titles (each title\ndownloaded 100 times) as examples for this task. The dataset was collected\nunder real-world network conditions. We present several machine algorithms for\nthe task and run a through set of experiments, which shows that our\nclassification accuracy is more than 95%. We also show that our algorithms are\nable to classify video titles that are not in the training set as unknown and\nsome of the algorithms are also able to eliminate false prediction of video\ntitles and instead report unknown. Finally, we evaluate our algorithms\nrobustness to delays and packet losses at test time and show that a solution\nthat uses SVM is the most robust against these changes given enough training\ndata. We provide the dataset and the crawler for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.00490v2"
    },
    {
        "title": "Joint Data Detection and Phase Noise Mitigation for Light Field Video\n  Transmission in MIMO-OFDM Systems",
        "authors": [
            "Omar H. Salim",
            "Wei Xiang",
            "Ali A. Nasi",
            "Gengkun Wang",
            "Hani Mehrpouyan"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Previous studies in the literature for video transmission over wireless\ncommunication systems focused on combating the effects of additive channel\nnoise and fading channels without taking the impairments in the physical layer\nsuch as phase noise (PHN) into account. Oscillator phase noise impairs the\nperformance of multi-input multi-output- orthogonal frequency division\nmultiplexing (MIMO-OFDM) systems in providing high data rates for video\napplications and may lead to decoding failure. In this paper, we propose a\nlight field (LF) video transmission system in wireless channels, and analyze\njoint data detection and phase mitigation in MIMO-OFDM systems for LF video\ntransmission. The signal model and rate-distortion (RD) model for LF video\ntransmission in the presence of multiple PHNs are discussed. Moreover, we\npropose an iterative algorithm based on the extended Kalman filter for joint\ndata detection and PHN tracking. Numerical results show that the proposed\ndetector can significantly improve the average bit-error rate (BER) and\npeak-to-noise ratio (PSNR) performance for LF video transmission compared to\nexisting algorithms. Moreover, the BER and PSNR performance of the proposed\nsystem is closer to that of the ideal case of perfect PHN estimation. Finally,\nit is demonstrated that the proposed system model and algorithm are well suited\nfor LF video transmission in wireless channels.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.02834v1"
    },
    {
        "title": "VLSI Friendly Framework for Scalable Video Coding based on Compressed\n  Sensing",
        "authors": [
            "B. K. N. Srinivasarao",
            "Vinay Chakravarthi Gogineni",
            "Subrahmanyam Mula",
            "Indrajit Chakrabarti"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This paper presents a new VLSI friendly framework for scalable video coding\nbased on Compressed Sensing (CS). It achieves scalability through 3-Dimensional\nDiscrete Wavelet Transform (3-D DWT) and better compression ratio by exploiting\nthe inherent sparsity of the high-frequency wavelet sub-bands through CS. By\nusing 3-D DWT and a proposed adaptive measurement scheme called AMS at the\nencoder, one can succeed in improving the compression ratio and reducing the\ncomplexity of the decoder. The proposed video codec uses only 7% of the total\nnumber of multipliers needed in a conventional CS-based video coding system. A\ncodebook of Bernoulli matrices with different sizes corresponding to the\npredefined sparsity levels is maintained at both the encoder and the decoder.\nBased on the calculated l0-norm of the input vector, one of the sixteen\npossible Bernoulli matrices will be selected for taking the CS measurements and\nits index will be transmitted along with the measurements. Based on this index,\nthe corresponding Bernoulli matrix has been used in CS reconstruction algorithm\nto get back the high-frequency wavelet sub-bands at the decoder. At the\ndecoder, a new Enhanced Approximate Message Passing (EAMP) algorithm has been\nproposed to reconstruct the wavelet coefficients and apply the inverse wavelet\ntransform for restoring back the video frames. Simulation results have\nestablished the superiority of the proposed framework over the existing schemes\nand have increased its suitability for VLSI implementation. Moreover, the coded\nvideo is found to be scalable with an increase in a number of levels of wavelet\ndecomposition.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.07453v1"
    },
    {
        "title": "Social- and Mobility-Aware Device-to-Device Content Delivery",
        "authors": [
            "Zhi Wang",
            "Lifeng Sun",
            "Miao Zhang",
            "Haitian Pang",
            "Erfang Tian",
            "Wenwu Zhu"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Mobile online social network services have seen a rapid increase, in which\nthe huge amount of user-generated social media contents propagating between\nusers via social connections has significantly challenged the traditional\ncontent delivery paradigm: First, replicating all of the contents generated by\nusers to edge servers that well \"fit\" the receivers becomes difficult due to\nthe limited bandwidth and storage capacities. Motivated by device-to-device\n(D2D) communication that allows users with smart devices to transfer content\ndirectly, we propose replicating bandwidth-intensive social contents in a\ndevice-to-device manner. Based on large-scale measurement studies on social\ncontent propagation and user mobility patterns in edge-network regions, we\nobserve that (1) Device-to-device replication can significantly help users\ndownload social contents from nearby neighboring peers; (2) Both social\npropagation and mobility patterns affect how contents should be replicated; (3)\nThe replication strategies depend on regional characteristics ({\\em e.g.}, how\nusers move across regions).\n  Using these measurement insights, we propose a joint \\emph{propagation- and\nmobility-aware} content replication strategy for edge-network regions, in which\nsocial contents are assigned to users in edge-network regions according to a\njoint consideration of social graph, content propagation and user mobility. We\nformulate the replication scheduling as an optimization problem and design\ndistributed algorithm only using historical, local and partial information to\nsolve it. Trace-driven experiments further verify the superiority of our\nproposal: compared with conventional pure movement-based and popularity-based\napproach, our design can significantly ($2-4$ times) improve the amount of\nsocial contents successfully delivered by device-to-device replication.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.04195v1"
    },
    {
        "title": "Label Tree Embeddings for Acoustic Scene Classification",
        "authors": [
            "Huy Phan",
            "Lars Hertel",
            "Marco Maass",
            "Philipp Koch",
            "Alfred Mertins"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  We present in this paper an efficient approach for acoustic scene\nclassification by exploring the structure of class labels. Given a set of class\nlabels, a category taxonomy is automatically learned by collectively optimizing\na clustering of the labels into multiple meta-classes in a tree structure. An\nacoustic scene instance is then embedded into a low-dimensional feature\nrepresentation which consists of the likelihoods that it belongs to the\nmeta-classes. We demonstrate state-of-the-art results on two different datasets\nfor the acoustic scene classification task, including the DCASE 2013 and LITIS\nRouen datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.07908v2"
    },
    {
        "title": "Minimum-latency Time-frequency Analysis Using Asymmetric Window\n  Functions",
        "authors": [
            "Li Su",
            "Hau-tieng Wu"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  We study the real-time dynamics retrieval from a time series via the\ntime-frequency (TF) analysis with the minimal latency guarantee. While\ndifferent from the well-known intrinsic latency definition in the filter\ndesign, a rigorous definition of intrinsic latency for different time-frequency\nrepresentations (TFR) is provided, including the short time Fourier transform\n(STFT), synchrosqeezing transform (SST) and reassignment method (RM). To\nachieve the minimal latency, a systematic method is proposed to construct an\nasymmetric window from a well-designed symmetric one based on the concept of\nminimum-phase, if the window satisfies some weak conditions. We theoretically\nshow that the TFR determined by SST with the constructed asymmetric window does\nhave a smaller intrinsic latency. Finally, the music onset detection problem is\nstudied to show the strength of the proposed algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.09047v1"
    },
    {
        "title": "City-Identification of Flickr Videos Using Semantic Acoustic Features",
        "authors": [
            "Benjamin Elizalde",
            "Guan-Lin Chao",
            "Ming Zeng",
            "Ian Lane"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  City-identification of videos aims to determine the likelihood of a video\nbelonging to a set of cities. In this paper, we present an approach using only\naudio, thus we do not use any additional modality such as images, user-tags or\ngeo-tags. In this manner, we show to what extent the city-location of videos\ncorrelates to their acoustic information. Success in this task suggests\nimprovements can be made to complement the other modalities. In particular, we\npresent a method to compute and use semantic acoustic features to perform\ncity-identification and the features show semantic evidence of the\nidentification. The semantic evidence is given by a taxonomy of urban sounds\nand expresses the potential presence of these sounds in the city- soundtracks.\nWe used the MediaEval Placing Task set, which contains Flickr videos labeled by\ncity. In addition, we used the UrbanSound8K set containing audio clips labeled\nby sound- type. Our method improved the state-of-the-art performance and\nprovides a novel semantic approach to this task\n",
        "pdf_link": "http://arxiv.org/pdf/1607.03257v1"
    },
    {
        "title": "Low-complexity feedback-channel-free distributed video coding using\n  Local Rank Transform",
        "authors": [
            "P Raj Bhagath",
            "Kallol Mallick",
            "Jayanta Mukherjee",
            "Sudipta Mukopadhayay"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  In this paper, we propose a new feedback-channel-free Distributed Video\nCoding (DVC) algorithm using Local Rank Transform (LRT). The encoder computes\nLRT by considering selected neighborhood pixels of Wyner-Ziv frame. The ranks\nfrom the modified LRT are merged, and their positions are entropy coded and\nsent to the decoder. In addition, means of each block of Wyner-Ziv frame are\nalso transmitted to assist motion estimation. Using these measurements, the\ndecoder generates side information (SI) by implementing motion estimation and\ncompensation in LRT domain. An iterative algorithm is executed on SI using LRT\nto reconstruct the Wyner-Ziv frame. Experimental results show that the coding\nefficiency of our codec is close to the efficiency of pixel domain distributed\nvideo coders based on Low-Density Parity Check and Accumulate (LDPCA) or turbo\ncodes, with less encoder complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.07697v1"
    },
    {
        "title": "Semi-Fragile Image Authentication based on CFD and 3-Bit Quantization",
        "authors": [
            "Aleksey Zhuvikin",
            "Valery Korzhik",
            "Guillermo Morales-Luna"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  There is a great adventure of watermarking usage in the context of\nconventional authentication since it does not require additional storage space\nfor supplementary metadata. However JPEG compression, being a conventional\nmethod to compress images, leads to exact authentication breaking. We discuss a\nsemi-fragile watermarking system for digital images tolerant to JPEG/JPEG2000\ncompression. Recently we have published a selective authentication method based\non Zernike moments. But unfortunately it has large computational complexity and\nnot sufficiently good detection of small image modifications. In the current\npaper it is proposed (in contrast to Zernike moments approach) the usage of\nimage finite differences and 3-bit quantization as the main technique. In order\nto embed a watermark (WM) into the image, some areas of the Haar wavelet\ntransform coefficients are used. Simulation results show a good resistance of\nthis method to JPEG compression with $\\mbox{\\rm CR}\\leq 30\\%$ (Compression\nRatio), high probability of small image modification recognition, image quality\nassessments $\\mbox{\\rm PSNR}\\geq 40$ (Peak signal-to-noise ratio) dB and\n$\\mbox{\\rm SSIM}\\geq 0.98$ (Structural Similarity Index Measure) after\nembedding and lower computation complexity of WM embedding and extraction. All\nthese properties qualify this approach as effective.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.02291v1"
    },
    {
        "title": "Towards Music Captioning: Generating Music Playlist Descriptions",
        "authors": [
            "Keunwoo Choi",
            "George Fazekas",
            "Brian McFee",
            "Kyunghyun Cho",
            "Mark Sandler"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Descriptions are often provided along with recommendations to help users'\ndiscovery. Recommending automatically generated music playlists (e.g.\npersonalised playlists) introduces the problem of generating descriptions. In\nthis paper, we propose a method for generating music playlist descriptions,\nwhich is called as music captioning. In the proposed method, audio content\nanalysis and natural language processing are adopted to utilise the information\nof each track.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04868v2"
    },
    {
        "title": "On the Efficiency and Fairness of Multiplayer HTTP-based Adaptive Video\n  Streaming",
        "authors": [
            "Xiaoqi Yin",
            "Mihovil Bartulović",
            "Vyas Sekar",
            "Bruno Sinopoli"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  User-perceived quality-of-experience (QoE) is critical in internet video\ndelivery systems. Extensive prior work has studied the design of client-side\nbitrate adaptation algorithms to maximize single-player QoE. However,\nmultiplayer QoE fairness becomes critical as the growth of video traffic makes\nit more likely that multiple players share a bottleneck in the network. Despite\nseveral recent proposals, there is still a series of open questions. In this\npaper, we bring the problem space to light from a control theory perspective by\nformalizing the multiplayer QoE fairness problem and addressing two key\nquestions in the broader problem space. First, we derive the sufficient\nconditions of convergence to steady state QoE fairness under TCP-based\nbandwidth sharing scheme. Based on the insight from this analysis that\nin-network active bandwidth allocation is needed, we propose a non-linear\nMPC-based, router-assisted bandwidth allocation algorithm that regards each\nplayer as closed-loop systems. We use trace-driven simulation to show the\nimprovement over existing approaches. We identify several research directions\nenabled by the control theoretic modeling and envision that control theory can\nplay an important role on guiding real system design in adaptive video\nstreaming.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.08469v1"
    },
    {
        "title": "Steganographic Generative Adversarial Networks",
        "authors": [
            "Denis Volkhonskiy",
            "Ivan Nazarov",
            "Evgeny Burnaev"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Steganography is collection of methods to hide secret information (\"payload\")\nwithin non-secret information \"container\"). Its counterpart, Steganalysis, is\nthe practice of determining if a message contains a hidden payload, and\nrecovering it if possible. Presence of hidden payloads is typically detected by\na binary classifier. In the present study, we propose a new model for\ngenerating image-like containers based on Deep Convolutional Generative\nAdversarial Networks (DCGAN). This approach allows to generate more\nsetganalysis-secure message embedding using standard steganography algorithms.\nExperiment results demonstrate that the new model successfully deceives the\nsteganography analyzer, and for this reason, can be used in steganographic\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.05502v2"
    },
    {
        "title": "Performance Analysis of Reliable Video Streaming with Strict Playout\n  Deadline in Multi-Hop Wireless Networks",
        "authors": [
            "Hussein Al-Zubaidy",
            "Viktoria Fodor",
            "György Dán",
            "Markus Flierl"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Motivated by emerging vision-based intelligent services, we consider the\nproblem of rate adaptation for high quality and low delay visual information\ndelivery over wireless networks using scalable video coding. Rate adaptation in\nthis setting is inherently challenging due to the interplay between the\nvariability of the wireless channels, the queuing at the network nodes and the\nframe-based decoding and playback of the video content at the receiver at very\nshort time scales. To address the problem, we propose a low-complexity,\nmodel-based rate adaptation algorithm for scalable video streaming systems,\nbuilding on a novel performance model based on stochastic network calculus. We\nvalidate the model using extensive simulations. We show that it allows fast,\nnear optimal rate adaptation for fixed transmission paths, as well as\ncross-layer optimized routing and video rate adaptation in mesh networks, with\nless than $10$\\% quality degradation compared to the best achievable\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.02790v1"
    },
    {
        "title": "Cross-media Similarity Metric Learning with Unified Deep Networks",
        "authors": [
            "Jinwei Qi",
            "Xin Huang",
            "Yuxin Peng"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  As a highlighting research topic in the multimedia area, cross-media\nretrieval aims to capture the complex correlations among multiple media types.\nLearning better shared representation and distance metric for multimedia data\nis important to boost the cross-media retrieval. Motivated by the strong\nability of deep neural network in feature representation and comparison\nfunctions learning, we propose the Unified Network for Cross-media Similarity\nMetric (UNCSM) to associate cross-media shared representation learning with\ndistance metric in a unified framework. First, we design a two-pathway deep\nnetwork pretrained with contrastive loss, and employ double triplet similarity\nloss for fine-tuning to learn the shared representation for each media type by\nmodeling the relative semantic similarity. Second, the metric network is\ndesigned for effectively calculating the cross-media similarity of the shared\nrepresentation, by modeling the pairwise similar and dissimilar constraints.\nCompared to the existing methods which mostly ignore the dissimilar constraints\nand only use sample distance metric as Euclidean distance separately, our UNCSM\napproach unifies the representation learning and distance metric to preserve\nthe relative similarity as well as embrace more complex similarity functions\nfor further improving the cross-media retrieval accuracy. The experimental\nresults show that our UNCSM approach outperforms 8 state-of-the-art methods on\n4 widely-used cross-media datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.04333v1"
    },
    {
        "title": "Attacking Automatic Video Analysis Algorithms: A Case Study of Google\n  Cloud Video Intelligence API",
        "authors": [
            "Hossein Hosseini",
            "Baicen Xiao",
            "Andrew Clark",
            "Radha Poovendran"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Due to the growth of video data on Internet, automatic video analysis has\ngained a lot of attention from academia as well as companies such as Facebook,\nTwitter and Google. In this paper, we examine the robustness of video analysis\nalgorithms in adversarial settings. Specifically, we propose targeted attacks\non two fundamental classes of video analysis algorithms, namely video\nclassification and shot detection. We show that an adversary can subtly\nmanipulate a video in such a way that a human observer would perceive the\ncontent of the original video, but the video analysis algorithm will return the\nadversary's desired outputs.\n  We then apply the attacks on the recently released Google Cloud Video\nIntelligence API. The API takes a video file and returns the video labels\n(objects within the video), shot changes (scene changes within the video) and\nshot labels (description of video events over time). Through experiments, we\nshow that the API generates video and shot labels by processing only the first\nframe of every second of the video. Hence, an adversary can deceive the API to\noutput only her desired video and shot labels by periodically inserting an\nimage into the video at the rate of one frame per second. We also show that the\npattern of shot changes returned by the API can be mostly recovered by an\nalgorithm that compares the histograms of consecutive frames. Based on our\nequivalent model, we develop a method for slightly modifying the video frames,\nin order to deceive the API into generating our desired pattern of shot\nchanges. We perform extensive experiments with different videos and show that\nour attacks are consistently successful across videos with different\ncharacteristics. At the end, we propose introducing randomness to video\nanalysis algorithms as a countermeasure to our attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04301v1"
    },
    {
        "title": "MHTN: Modal-adversarial Hybrid Transfer Network for Cross-modal\n  Retrieval",
        "authors": [
            "Xin Huang",
            "Yuxin Peng",
            "Mingkuan Yuan"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Cross-modal retrieval has drawn wide interest for retrieval across different\nmodalities of data. However, existing methods based on DNN face the challenge\nof insufficient cross-modal training data, which limits the training\neffectiveness and easily leads to overfitting. Transfer learning is for\nrelieving the problem of insufficient training data, but it mainly focuses on\nknowledge transfer only from large-scale datasets as single-modal source domain\nto single-modal target domain. Such large-scale single-modal datasets also\ncontain rich modal-independent semantic knowledge that can be shared across\ndifferent modalities. Besides, large-scale cross-modal datasets are very\nlabor-consuming to collect and label, so it is significant to fully exploit the\nknowledge in single-modal datasets for boosting cross-modal retrieval. This\npaper proposes modal-adversarial hybrid transfer network (MHTN), which to the\nbest of our knowledge is the first work to realize knowledge transfer from\nsingle-modal source domain to cross-modal target domain, and learn cross-modal\ncommon representation. It is an end-to-end architecture with two subnetworks:\n(1) Modal-sharing knowledge transfer subnetwork is proposed to jointly transfer\nknowledge from a large-scale single-modal dataset in source domain to all\nmodalities in target domain with a star network structure, which distills\nmodal-independent supplementary knowledge for promoting cross-modal common\nrepresentation learning. (2) Modal-adversarial semantic learning subnetwork is\nproposed to construct an adversarial training mechanism between common\nrepresentation generator and modality discriminator, making the common\nrepresentation discriminative for semantics but indiscriminative for modalities\nto enhance cross-modal semantic consistency during transfer process.\nComprehensive experiments on 4 widely-used datasets show its effectiveness and\ngenerality.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04308v1"
    },
    {
        "title": "An improved watermarking scheme for Internet applications",
        "authors": [
            "Christophe Guyeux",
            "Jacques M. Bahi"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In this paper, a data hiding scheme ready for Internet applications is\nproposed. An existing scheme based on chaotic iterations is improved, to\nrespond to some major Internet security concerns, such as digital rights\nmanagement, communication over hidden channels, and social search engines. By\nusing Reed Solomon error correcting codes and wavelets domain, we show that\nthis data hiding scheme can be improved to solve issues and requirements raised\nby these Internet fields.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.05970v1"
    },
    {
        "title": "Protest Activity Detection and Perceived Violence Estimation from Social\n  Media Images",
        "authors": [
            "Donghyeon Won",
            "Zachary C. Steinert-Threlkeld",
            "Jungseock Joo"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  We develop a novel visual model which can recognize protesters, describe\ntheir activities by visual attributes and estimate the level of perceived\nviolence in an image. Studies of social media and protests use natural language\nprocessing to track how individuals use hashtags and links, often with a focus\non those items' diffusion. These approaches, however, may not be effective in\nfully characterizing actual real-world protests (e.g., violent or peaceful) or\nestimating the demographics of participants (e.g., age, gender, and race) and\ntheir emotions. Our system characterizes protests along these dimensions. We\nhave collected geotagged tweets and their images from 2013-2017 and analyzed\nmultiple major protest events in that period. A multi-task convolutional neural\nnetwork is employed in order to automatically classify the presence of\nprotesters in an image and predict its visual attributes, perceived violence\nand exhibited emotions. We also release the UCLA Protest Image Dataset, our\nnovel dataset of 40,764 images (11,659 protest images and hard negatives) with\nvarious annotations of visual attributes and sentiments. Using this dataset, we\ntrain our model and demonstrate its effectiveness. We also present experimental\nresults from various analysis on geotagged image data in several prevalent\nprotest events. Our dataset will be made accessible at\nhttps://www.sscnet.ucla.edu/comm/jjoo/mm-protest/.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.06204v1"
    },
    {
        "title": "Similarity measures for vocal-based drum sample retrieval using deep\n  convolutional auto-encoders",
        "authors": [
            "Adib Mehrabi",
            "Keunwoo Choi",
            "Simon Dixon",
            "Mark Sandler"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  The expressive nature of the voice provides a powerful medium for\ncommunicating sonic ideas, motivating recent research on methods for query by\nvocalisation. Meanwhile, deep learning methods have demonstrated\nstate-of-the-art results for matching vocal imitations to imitated sounds, yet\nlittle is known about how well learned features represent the perceptual\nsimilarity between vocalisations and queried sounds. In this paper, we address\nthis question using similarity ratings between vocal imitations and imitated\ndrum sounds. We use a linear mixed effect regression model to show how features\nlearned by convolutional auto-encoders (CAEs) perform as predictors for\nperceptual similarity between sounds. Our experiments show that CAEs outperform\nthree baseline feature sets (spectrogram-based representations, MFCCs, and\ntemporal features) at predicting the subjective similarity ratings. We also\ninvestigate how the size and shape of the encoded layer effects the predictive\npower of the learned features. The results show that preservation of temporal\ninformation is more important than spectral resolution for this application.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.05178v1"
    },
    {
        "title": "ReDMark: Framework for Residual Diffusion Watermarking on Deep Networks",
        "authors": [
            "Mahdi Ahmadi",
            "Alireza Norouzi",
            "S. M. Reza Soroushmehr",
            "Nader Karimi",
            "Kayvan Najarian",
            "Shadrokh Samavi",
            "Ali Emami"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Due to the rapid growth of machine learning tools and specifically deep\nnetworks in various computer vision and image processing areas, application of\nConvolutional Neural Networks for watermarking have recently emerged. In this\npaper, we propose a deep end-to-end diffusion watermarking framework (ReDMark)\nwhich can be adapted for any desired transform space. The framework is composed\nof two Fully Convolutional Neural Networks with the residual structure for\nembedding and extraction. The whole deep network is trained end-to-end to\nconduct a blind secure watermarking. The framework is customizable for the\nlevel of robustness vs. imperceptibility. It is also adjustable for the\ntrade-off between capacity and robustness. The proposed framework simulates\nvarious attacks as a differentiable network layer to facilitate end-to-end\ntraining. For JPEG attack, a differentiable approximation is utilized, which\ndrastically improves the watermarking robustness to this attack. Another\nimportant characteristic of the proposed framework, which leads to improved\nsecurity and robustness, is its capability to diffuse watermark information\namong a relatively wide area of the image. Comparative results versus recent\nstate-of-the-art researches highlight the superiority of the proposed framework\nin terms of imperceptibility and robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.07248v3"
    },
    {
        "title": "The Prefetch Aggressiveness Tradeoff in 360$^{\\circ}$ Video Streaming",
        "authors": [
            "Mathias Almquist",
            "Viktor Almquist",
            "Vengatanathan Krishnamoorthi",
            "Niklas Carlsson",
            "Derek Eager"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  With 360$^{\\circ}$ video, only a limited fraction of the full view is\ndisplayed at each point in time. This has prompted the design of streaming\ndelivery techniques that allow alternative playback qualities to be delivered\nfor each candidate viewing direction. However, while prefetching based on the\nuser's expected viewing direction is best done close to playback deadlines,\nlarge buffers are needed to protect against shortfalls in future available\nbandwidth. This results in conflicting goals and an important prefetch\naggressiveness tradeoff problem regarding how far ahead in time from the\ncurrent playpoint prefetching should be done. This paper presents the first\ncharacterization of this tradeoff. The main contributions include an empirical\ncharacterization of head movement behavior based on data from viewing sessions\nof four different categories of 360$^{\\circ}$ video, an optimization-based\ncomparison of the prefetch aggressiveness tradeoffs seen for these video\ncategories, and a data-driven discussion of further optimizations, which\ninclude a novel system design that allows both tradeoff objectives to be\ntargeted simultaneously. By qualitatively and quantitatively analyzing the\nabove tradeoffs, we provide insights into how to best design tomorrow's\ndelivery systems for 360$^{\\circ}$ videos, allowing content providers to reduce\nbandwidth costs and improve users' playback experiences.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07277v1"
    },
    {
        "title": "A Smart Security System with Face Recognition",
        "authors": [
            "Trung Nguyen",
            "Barth Lakshmanan",
            "Weihua Sheng"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Web-based technology has improved drastically in the past decade. As a\nresult, security technology has become a major help to protect our daily life.\nIn this paper, we propose a robust security based on face recognition system\n(SoF). In particular, we develop this system to giving access into a home for\nauthenticated users. The classifier is trained by using a new adaptive learning\nmethod. The training data are initially collected from social networks. The\naccuracy of the classifier is incrementally improved as the user starts using\nthe system. A novel method has been introduced to improve the classifier model\nby human interaction and social media. By using a deep learning framework -\nTensorFlow, it will be easy to reuse the framework to adopt with many devices\nand applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09127v1"
    },
    {
        "title": "Scene Graph Reasoning with Prior Visual Relationship for Visual Question\n  Answering",
        "authors": [
            "Zhuoqian Yang",
            "Zengchang Qin",
            "Jing Yu",
            "Yue Hu"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  One of the key issues of Visual Question Answering (VQA) is to reason with\nsemantic clues in the visual content under the guidance of the question, how to\nmodel relational semantics still remains as a great challenge. To fully capture\nvisual semantics, we propose to reason over a structured visual representation\n- scene graph, with embedded objects and inter-object relationships. This shows\ngreat benefit over vanilla vector representations and implicit visual\nrelationship learning. Based on existing visual relationship models, we propose\na visual relationship encoder that projects visual relationships into a learned\ndeep semantic space constrained by visual context and language priors. Upon the\nconstructed graph, we propose a Scene Graph Convolutional Network (SceneGCN) to\njointly reason the object properties and relational semantics for the correct\nanswer. We demonstrate the model's effectiveness and interpretability on the\nchallenging GQA dataset and the classical VQA 2.0 dataset, remarkably achieving\nstate-of-the-art 54.56% accuracy on GQA compared to the existing best model.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09681v2"
    },
    {
        "title": "Camera Obscurer: Generative Art for Design Inspiration",
        "authors": [
            "Dilpreet Singh",
            "Nina Rajcic",
            "Simon Colton",
            "Jon McCormack"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  We investigate using generated decorative art as a source of inspiration for\ndesign tasks. Using a visual similarity search for image retrieval, the\n\\emph{Camera Obscurer} app enables rapid searching of tens of thousands of\ngenerated abstract images of various types. The seed for a visual similarity\nsearch is a given image, and the retrieved generated images share some visual\nsimilarity with the seed. Implemented in a hand-held device, the app empowers\nusers to use photos of their surroundings to search through the archive of\ngenerated images and other image archives. Being abstract in nature, the\nretrieved images supplement the seed image rather than replace it, providing\ndifferent visual stimuli including shapes, colours, textures and\njuxtapositions, in addition to affording their own interpretations. This\napproach can therefore be used to provide inspiration for a design task, with\nthe abstract images suggesting new ideas that might give direction to a graphic\ndesign project. We describe a crowdsourcing experiment with the app to estimate\nuser confidence in retrieved images, and we describe a pilot study where Camera\nObscurer provided inspiration for a design task. These experiments have enabled\nus to describe future improvements, and to begin to understand sources of\nvisual inspiration for design tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.02165v1"
    },
    {
        "title": "Look, Listen, and Attend: Co-Attention Network for Self-Supervised\n  Audio-Visual Representation Learning",
        "authors": [
            "Ying Cheng",
            "Ruize Wang",
            "Zhihao Pan",
            "Rui Feng",
            "Yuejie Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  When watching videos, the occurrence of a visual event is often accompanied\nby an audio event, e.g., the voice of lip motion, the music of playing\ninstruments. There is an underlying correlation between audio and visual\nevents, which can be utilized as free supervised information to train a neural\nnetwork by solving the pretext task of audio-visual synchronization. In this\npaper, we propose a novel self-supervised framework with co-attention mechanism\nto learn generic cross-modal representations from unlabelled videos in the\nwild, and further benefit downstream tasks. Specifically, we explore three\ndifferent co-attention modules to focus on discriminative visual regions\ncorrelated to the sounds and introduce the interactions between them.\nExperiments show that our model achieves state-of-the-art performance on the\npretext task while having fewer parameters compared with existing methods. To\nfurther evaluate the generalizability and transferability of our approach, we\napply the pre-trained model on two downstream tasks, i.e., sound source\nlocalization and action recognition. Extensive experiments demonstrate that our\nmodel provides competitive results with other self-supervised methods, and also\nindicate that our approach can tackle the challenging scenes which contain\nmultiple sound sources.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.05789v1"
    },
    {
        "title": "WAN: Watermarking Attack Network",
        "authors": [
            "Seung-Hun Nam",
            "In-Jae Yu",
            "Seung-Min Mun",
            "Daesik Kim",
            "Wonhyuk Ahn"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Multi-bit watermarking (MW) has been developed to improve robustness against\nsignal processing operations and geometric distortions. To this end, benchmark\ntools that test robustness by applying simulated attacks on watermarked images\nare available. However, limitations in these general attacks exist since they\ncannot exploit specific characteristics of the targeted MW. In addition, these\nattacks are usually devised without consideration of visual quality, which\nrarely occurs in the real world. To address these limitations, we propose a\nwatermarking attack network (WAN), a fully trainable watermarking benchmark\ntool that utilizes the weak points of the target MW and induces an inversion of\nthe watermark bit, thereby considerably reducing the watermark extractability.\nTo hinder the extraction of hidden information while ensuring high visual\nquality, we utilize a residual dense blocks-based architecture specialized in\nlocal and global feature learning. A novel watermarking attack loss is\nintroduced to break the MW systems. We empirically demonstrate that the WAN can\nsuccessfully fool various block-based MW systems. Moreover, we show that\nexisting MW methods can be improved with the help of the WAN as an add-on\nmodule.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06255v3"
    },
    {
        "title": "NANCY: Neural Adaptive Network Coding methodologY for video distribution\n  over wireless networks",
        "authors": [
            "Paresh Saxena",
            "Mandan Naresh",
            "Manik Gupta",
            "Anirudh Achanta",
            "Sastri Kota",
            "Smrati Gupta"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper presents NANCY, a system that generates adaptive bit rates (ABR)\nfor video and adaptive network coding rates (ANCR) using reinforcement learning\n(RL) for video distribution over wireless networks. NANCY trains a neural\nnetwork model with rewards formulated as quality of experience (QoE) metrics.\nIt performs joint optimization in order to select: (i) adaptive bit rates for\nfuture video chunks to counter variations in available bandwidth and (ii)\nadaptive network coding rates to encode the video chunk slices to counter\npacket losses in wireless networks. We present the design and implementation of\nNANCY, and evaluate its performance compared to state-of-the-art video rate\nadaptation algorithms including Pensieve and robustMPC. Our results show that\nNANCY provides 29.91% and 60.34% higher average QoE than Pensieve and\nrobustMPC, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.09559v1"
    },
    {
        "title": "Rate distortion optimization over large scale video corpus with machine\n  learning",
        "authors": [
            "Sam John",
            "Akshay Gadde",
            "Balu Adsumilli"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  We present an efficient codec-agnostic method for bitrate allocation over a\nlarge scale video corpus with the goal of minimizing the average bitrate\nsubject to constraints on average and minimum quality. Our method clusters the\nvideos in the corpus such that videos within one cluster have similar\nrate-distortion (R-D) characteristics. We train a support vector machine\nclassifier to predict the R-D cluster of a video using simple video complexity\nfeatures that are computationally easy to obtain. The model allows us to\nclassify a large sample of the corpus in order to estimate the distribution of\nthe number of videos in each of the clusters. We use this distribution to find\nthe optimal encoder operating point for each R-D cluster. Experiments with AV1\nencoder show that our method can achieve the same average quality over the\ncorpus with $22\\%$ less average bitrate.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.12408v1"
    },
    {
        "title": "Multimedia Semantic Integrity Assessment Using Joint Embedding Of Images\n  And Text",
        "authors": [
            "Ayush Jaiswal",
            "Ekraam Sabir",
            "Wael AbdAlmageed",
            "Premkumar Natarajan"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Real world multimedia data is often composed of multiple modalities such as\nan image or a video with associated text (e.g. captions, user comments, etc.)\nand metadata. Such multimodal data packages are prone to manipulations, where a\nsubset of these modalities can be altered to misrepresent or repurpose data\npackages, with possible malicious intent. It is, therefore, important to\ndevelop methods to assess or verify the integrity of these multimedia packages.\nUsing computer vision and natural language processing methods to directly\ncompare the image (or video) and the associated caption to verify the integrity\nof a media package is only possible for a limited set of objects and scenes. In\nthis paper, we present a novel deep learning-based approach for assessing the\nsemantic integrity of multimedia packages containing images and captions, using\na reference set of multimedia packages. We construct a joint embedding of\nimages and captions with deep multimodal representation learning on the\nreference dataset in a framework that also provides image-caption consistency\nscores (ICCSs). The integrity of query media packages is assessed as the\ninlierness of the query ICCSs with respect to the reference dataset. We present\nthe MultimodAl Information Manipulation dataset (MAIM), a new dataset of media\npackages from Flickr, which we make available to the research community. We use\nboth the newly created dataset as well as Flickr30K and MS COCO datasets to\nquantitatively evaluate our proposed approach. The reference dataset does not\ncontain unmanipulated versions of tampered query packages. Our method is able\nto achieve F1 scores of 0.75, 0.89 and 0.94 on MAIM, Flickr30K and MS COCO,\nrespectively, for detecting semantically incoherent media packages.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.01606v4"
    },
    {
        "title": "Multi-Representation Knowledge Distillation For Audio Classification",
        "authors": [
            "Liang Gao",
            "Kele Xu",
            "Huaimin Wang",
            "Yuxing Peng"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  As an important component of multimedia analysis tasks, audio classification\naims to discriminate between different audio signal types and has received\nintensive attention due to its wide applications. Generally speaking, the raw\nsignal can be transformed into various representations (such as Short Time\nFourier Transform and Mel Frequency Cepstral Coefficients), and information\nimplied in different representations can be complementary. Ensembling the\nmodels trained on different representations can greatly boost the\nclassification performance, however, making inference using a large number of\nmodels is cumbersome and computationally expensive. In this paper, we propose a\nnovel end-to-end collaborative learning framework for the audio classification\ntask. The framework takes multiple representations as the input to train the\nmodels in parallel. The complementary information provided by different\nrepresentations is shared by knowledge distillation. Consequently, the\nperformance of each model can be significantly promoted without increasing the\ncomputational overhead in the inference stage. Extensive experimental results\ndemonstrate that the proposed approach can improve the classification\nperformance and achieve state-of-the-art results on both acoustic scene\nclassification tasks and general audio tagging tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.09607v1"
    },
    {
        "title": "Model Watermarking for Image Processing Networks",
        "authors": [
            "Jie Zhang",
            "Dongdong Chen",
            "Jing Liao",
            "Han Fang",
            "Weiming Zhang",
            "Wenbo Zhou",
            "Hao Cui",
            "Nenghai Yu"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Deep learning has achieved tremendous success in numerous industrial\napplications. As training a good model often needs massive high-quality data\nand computation resources, the learned models often have significant business\nvalues. However, these valuable deep models are exposed to a huge risk of\ninfringements. For example, if the attacker has the full information of one\ntarget model including the network structure and weights, the model can be\neasily finetuned on new datasets. Even if the attacker can only access the\noutput of the target model, he/she can still train another similar surrogate\nmodel by generating a large scale of input-output training pairs. How to\nprotect the intellectual property of deep models is a very important but\nseriously under-researched problem. There are a few recent attempts at\nclassification network protection only. In this paper, we propose the first\nmodel watermarking framework for protecting image processing models. To achieve\nthis goal, we leverage the spatial invisible watermarking mechanism.\nSpecifically, given a black-box target model, a unified and invisible watermark\nis hidden into its outputs, which can be regarded as a special task-agnostic\nbarrier. In this way, when the attacker trains one surrogate model by using the\ninput-output pairs of the target model, the hidden watermark will be learned\nand extracted afterward. To enable watermarks from binary bits to\nhigh-resolution images, both traditional and deep spatial invisible\nwatermarking mechanism are considered. Experiments demonstrate the robustness\nof the proposed watermarking mechanism, which can resist surrogate models\nlearned with different network structures and objective functions. Besides deep\nmodels, the proposed method is also easy to be extended to protect data and\ntraditional image processing algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.11088v1"
    },
    {
        "title": "Multiview Hessian Discriminative Sparse Coding for Image Annotation",
        "authors": [
            "Weifeng Liu",
            "Dacheng Tao",
            "Jun Cheng",
            "Yuanyan Tang"
        ],
        "category": "cs.MM",
        "published_year": "2013",
        "summary": "  Sparse coding represents a signal sparsely by using an overcomplete\ndictionary, and obtains promising performance in practical computer vision\napplications, especially for signal restoration tasks such as image denoising\nand image inpainting. In recent years, many discriminative sparse coding\nalgorithms have been developed for classification problems, but they cannot\nnaturally handle visual data represented by multiview features. In addition,\nexisting sparse coding algorithms use graph Laplacian to model the local\ngeometry of the data distribution. It has been identified that Laplacian\nregularization biases the solution towards a constant function which possibly\nleads to poor extrapolating power. In this paper, we present multiview Hessian\ndiscriminative sparse coding (mHDSC) which seamlessly integrates Hessian\nregularization with discriminative sparse coding for multiview learning\nproblems. In particular, mHDSC exploits Hessian regularization to steer the\nsolution which varies smoothly along geodesics in the manifold, and treats the\nlabel information as an additional view of feature for incorporating the\ndiscriminative power for image annotation. We conduct extensive experiments on\nPASCAL VOC'07 dataset and demonstrate the effectiveness of mHDSC for image\nannotation.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.3811v1"
    },
    {
        "title": "Steganography - coding and intercepting the information from encoded\n  pictures in the absence of any initial information",
        "authors": [
            "Monika Kwiatkowska",
            "Lukasz Swierczewski"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  The work includes implementation and extraction algorithms capabilities test,\nwithout any additional data (starting position, the number of bits used, gap\nbetween the amount of data encoded) information from encoded files (mostly\nimages). The software is written using OpenMP standard [1], which allowed them\nto run on parallel computers. Performance tests were carried out on computers,\nBlue Gene/P [2], Blue Gene/Q [3] and the system consisting of four AMD Opteron\n6272 [4]. Source code is available under GNU GPL v3 license and are available\nin a repository OLib [5].\n",
        "pdf_link": "http://arxiv.org/pdf/1404.2237v1"
    },
    {
        "title": "OR-Benchmark: An Open and Reconfigurable Digital Watermarking\n  Benchmarking Framework",
        "authors": [
            "Hui Wang",
            "Anthony TS Ho",
            "Shujun Li"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Benchmarking digital watermarking algorithms is not an easy task because\ndifferent applications of digital watermarking often have very different sets\nof requirements and trade-offs between conflicting requirements. While there\nhave been some general-purpose digital watermarking benchmarking systems\navailable, they normally do not support complicated benchmarking tasks and\ncannot be easily reconfigured to work with different watermarking algorithms\nand testing conditions. In this paper, we propose OR-Benchmark, an open and\nhighly reconfigurable general-purpose digital watermarking benchmarking\nframework, which has the following two key features: 1) all the interfaces are\npublic and general enough to support all watermarking applications and\nbenchmarking tasks we can think of; 2) end users can easily extend the\nfunctionalities and freely configure what watermarking algorithms are tested,\nwhat system components are used, how the benchmarking process runs, and what\nresults should be produced. We implemented a prototype of this framework as a\nMATLAB software package and used it to benchmark a number of digital\nwatermarking algorithms involving two types of watermarks for content\nauthentication and self-restoration purposes. The benchmarking results\ndemonstrated the advantages of the proposed benchmarking framework, and also\ngave us some useful insights about existing image authentication and\nself-restoration watermarking algorithms which are an important but less\nstudied topic in digital watermarking.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.00243v2"
    },
    {
        "title": "AENet: Learning Deep Audio Features for Video Analysis",
        "authors": [
            "Naoya Takahashi",
            "Michael Gygli",
            "Luc Van Gool"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  We propose a new deep network for audio event recognition, called AENet. In\ncontrast to speech, sounds coming from audio events may be produced by a wide\nvariety of sources. Furthermore, distinguishing them often requires analyzing\nan extended time period due to the lack of clear sub-word units that are\npresent in speech. In order to incorporate this long-time frequency structure\nof audio events, we introduce a convolutional neural network (CNN) operating on\na large temporal input. In contrast to previous works this allows us to train\nan audio event detection system end-to-end. The combination of our network\narchitecture and a novel data augmentation outperforms previous methods for\naudio event detection by 16%. Furthermore, we perform transfer learning and\nshow that our model learnt generic audio features, similar to the way CNNs\nlearn generic features on vision tasks. In video analysis, combining visual\nfeatures and traditional audio features such as MFCC typically only leads to\nmarginal improvements. Instead, combining visual features with our AENet\nfeatures, which can be computed efficiently on a GPU, leads to significant\nperformance improvements on action recognition and video highlight detection.\nIn video highlight detection, our audio features improve the performance by\nmore than 8% over visual features alone.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.00599v2"
    },
    {
        "title": "A Watermarking Technique Using Discrete Curvelet Transform for Security\n  of Multiple Biometric Features",
        "authors": [
            "Rohit M. Thanki",
            "Ved Vyas Dwivedi",
            "Komal R. Borisagar"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The robustness and security of the biometric watermarking approach can be\nimproved by using a multiple watermarking. This multiple watermarking proposed\nfor improving security of biometric features and data. When the imposter tries\nto create the spoofed biometric feature, the invisible biometric watermark\nfeatures can provide appropriate protection to multimedia data. In this paper,\na biometric watermarking technique with multiple biometric watermarks are\nproposed in which biometric features of fingerprint, face, iris and signature\nis embedded in the image. Before embedding, fingerprint, iris, face and\nsignature features are extracted using Shen-Castan edge detection and Principal\nComponent Analysis. These all biometric watermark features are embedded into\nvarious mid band frequency curvelet coefficients of host image. All four\nfingerprint features, iris features, facial features and signature features are\nthe biometric characteristics of the individual and they are used for cross\nverification and copyright protection if any manipulation occurs. The proposed\ntechnique is fragile enough; features cannot be extracted from the watermarked\nimage when an imposter tries to remove watermark features illegally. It can use\nfor multiple copyright authentication and verification.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.04185v1"
    },
    {
        "title": "CM-GANs: Cross-modal Generative Adversarial Networks for Common\n  Representation Learning",
        "authors": [
            "Yuxin Peng",
            "Jinwei Qi",
            "Yuxin Yuan"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  It is known that the inconsistent distribution and representation of\ndifferent modalities, such as image and text, cause the heterogeneity gap that\nmakes it challenging to correlate such heterogeneous data. Generative\nadversarial networks (GANs) have shown its strong ability of modeling data\ndistribution and learning discriminative representation, existing GANs-based\nworks mainly focus on generative problem to generate new data. We have\ndifferent goal, aim to correlate heterogeneous data, by utilizing the power of\nGANs to model cross-modal joint distribution. Thus, we propose Cross-modal GANs\nto learn discriminative common representation for bridging heterogeneity gap.\nThe main contributions are: (1) Cross-modal GANs architecture is proposed to\nmodel joint distribution over data of different modalities. The inter-modality\nand intra-modality correlation can be explored simultaneously in generative and\ndiscriminative models. Both of them beat each other to promote cross-modal\ncorrelation learning. (2) Cross-modal convolutional autoencoders with\nweight-sharing constraint are proposed to form generative model. They can not\nonly exploit cross-modal correlation for learning common representation, but\nalso preserve reconstruction information for capturing semantic consistency\nwithin each modality. (3) Cross-modal adversarial mechanism is proposed, which\nutilizes two kinds of discriminative models to simultaneously conduct\nintra-modality and inter-modality discrimination. They can mutually boost to\nmake common representation more discriminative by adversarial training process.\nTo the best of our knowledge, our proposed CM-GANs approach is the first to\nutilize GANs to perform cross-modal common representation learning. Experiments\nare conducted to verify the performance of our proposed approach on cross-modal\nretrieval paradigm, compared with 10 methods on 3 cross-modal datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.05106v2"
    },
    {
        "title": "Learning Social Image Embedding with Deep Multimodal Attention Networks",
        "authors": [
            "Feiran Huang",
            "Xiaoming Zhang",
            "Zhoujun Li",
            "Tao Mei",
            "Yueying He",
            "Zhonghua Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  Learning social media data embedding by deep models has attracted extensive\nresearch interest as well as boomed a lot of applications, such as link\nprediction, classification, and cross-modal search. However, for social images\nwhich contain both link information and multimodal contents (e.g., text\ndescription, and visual content), simply employing the embedding learnt from\nnetwork structure or data content results in sub-optimal social image\nrepresentation. In this paper, we propose a novel social image embedding\napproach called Deep Multimodal Attention Networks (DMAN), which employs a deep\nmodel to jointly embed multimodal contents and link information. Specifically,\nto effectively capture the correlations between multimodal contents, we propose\na multimodal attention network to encode the fine-granularity relation between\nimage regions and textual words. To leverage the network structure for\nembedding learning, a novel Siamese-Triplet neural network is proposed to model\nthe links among images. With the joint deep model, the learnt embedding can\ncapture both the multimodal contents and the nonlinear network information.\nExtensive experiments are conducted to investigate the effectiveness of our\napproach in the applications of multi-label classification and cross-modal\nsearch. Compared to state-of-the-art image embeddings, our proposed DMAN\nachieves significant improvement in the tasks of multi-label classification and\ncross-modal search.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.06582v1"
    },
    {
        "title": "Cross-user Similarities in Viewing Behavior for 360$^{\\circ}$ Video and\n  Caching Implications",
        "authors": [
            "Niklas Carlsson",
            "Derek Eager"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The demand and usage of 360$^{\\circ}$ video services are expected to\nincrease. However, despite these services being highly bandwidth intensive, not\nmuch is known about the potential value that basic bandwidth saving techniques\nsuch as server or edge-network on-demand caching (e.g., in a CDN) could have\nwhen used for delivery of such services. This problem is both important and\ncomplicated as client-side solutions have been developed that split the full\n360$^{\\circ}$ view into multiple tiles, and adapt the quality of the downloaded\ntiles based on the user's expected viewing direction and bandwidth conditions.\nThis paper presents new trace-based analysis methods that incorporate users'\nviewports (the area of the full 360$^{\\circ}$ view the user actually sees), a\nfirst characterization of the cross-user similarities of the users' viewports,\nand a trace-based analysis of the potential bandwidth savings that\ncaching-based techniques may offer under different conditions. Our analysis\ntakes into account differences in the time granularity over which viewport\noverlaps can be beneficial for resource saving techniques, compares and\ncontrasts differences between video categories, and accounts for uncertainties\nin the network conditions and the prediction of the future viewing direction\nwhen prefetching. The results provide substantial insight into the conditions\nunder which overlap can be considerable and caching effective, and inform the\ndesign of new caching system policies tailored for 360$^{\\circ}$ video.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.09779v2"
    },
    {
        "title": "QoE-based MAC Layer Optimization for Video Teleconferencing over WiFi",
        "authors": [
            "Tianyi Xu",
            "Liangping Ma",
            "Gregory Sternberg"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  In IEEE 802.11, the retry limit is set the same value for all packets. In\nthis paper, we dynamically classify video teleconferencing packets based on the\ntype of the video frame that a packet carries and the packet loss events that\nhave happened in the network, and assign them different retry limits. We\nconsider the IPPP video encoding structure with instantaneous decoder refresh\n(IDR) frame insertion based on packet loss feedback. The loss of a single frame\ncauses error propagation for a period of time equal to the packet loss feedback\ndelay. To optimize the video quality, we propose a method to concentrate the\npacket losses to small segments of the entire video sequence, and study the\nperformance by an analytic model. Our proposed method is implemented only on\nthe stations interested in enhanced video quality, and is compatible with\nunmodified IEEE 802.11 stations and access points in terms of performance.\nSimulation results show that the performance gain can be significant compared\nto the IEEE 802.11 standard without negatively affecting cross traffic.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.00869v1"
    },
    {
        "title": "Real-Time Steganalysis for Stream Media Based on Multi-channel\n  Convolutional Sliding Windows",
        "authors": [
            "Zhongliang Yang",
            "Hao Yang",
            "Yuting Hu",
            "Yongfeng Huang",
            "Yu-Jin Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Previous VoIP steganalysis methods face great challenges in detecting speech\nsignals at low embedding rates, and they are also generally difficult to\nperform real-time detection, making them hard to truly maintain cyberspace\nsecurity. To solve these two challenges, in this paper, combined with the\nsliding window detection algorithm and Convolution Neural Network we propose a\nreal-time VoIP steganalysis method which based on multi-channel convolution\nsliding windows. In order to analyze the correlations between frames and\ndifferent neighborhood frames in a VoIP signal, we define multi channel sliding\ndetection windows. Within each sliding window, we design two feature extraction\nchannels which contain multiple convolution layers with multiple convolution\nkernels each layer to extract correlation features of the input signal. Then\nbased on these extracted features, we use a forward fully connected network for\nfeature fusion. Finally, by analyzing the statistical distribution of these\nfeatures, the discriminator will determine whether the input speech signal\ncontains covert information or not.We designed several experiments to test the\nproposed model's detection ability under various conditions, including\ndifferent embedding rates, different speech length, etc. Experimental results\nshowed that the proposed model outperforms all the previous methods, especially\nin the case of low embedding rate, which showed state-of-the-art performance.\nIn addition, we also tested the detection efficiency of the proposed model, and\nthe results showed that it can achieve almost real-time detection of VoIP\nspeech signals.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.01286v1"
    },
    {
        "title": "PixelSteganalysis: Pixel-wise Hidden Information Removal with Low Visual\n  Degradation",
        "authors": [
            "Dahuin Jung",
            "Ho Bae",
            "Hyun-Soo Choi",
            "Sungroh Yoon"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Recently, the field of steganography has experienced rapid developments based\non deep learning (DL). DL based steganography distributes secret information\nover all the available bits of the cover image, thereby posing difficulties in\nusing conventional steganalysis methods to detect, extract or remove hidden\nsecret images. However, our proposed framework is the first to effectively\ndisable covert communications and transactions that use DL based steganography.\nWe propose a DL based steganalysis technique that effectively removes secret\nimages by restoring the distribution of the original images. We formulate a\nproblem and address it by exploiting sophisticated pixel distributions and an\nedge distribution of images by using a deep neural network. Based on the given\ninformation, we remove the hidden secret information at the pixel level. We\nevaluate our technique by comparing it with conventional steganalysis methods\nusing three public benchmarks. As the decoding method of DL based steganography\nis approximate (lossy) and is different from the decoding method of\nconventional steganography, we also introduce a new quantitative metric called\nthe destruction rate (DT). The experimental results demonstrate performance\nimprovements of 10-20% in both the decoded rate and the DT.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10905v3"
    },
    {
        "title": "PixelSteganalysis: Destroying Hidden Information with a Low Degree of\n  Visual Degradation",
        "authors": [
            "Dahuin Jung",
            "Ho Bae",
            "Hyun-Soo Choi",
            "Sungroh Yoon"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Steganography is the science of unnoticeably concealing a secret message\nwithin a certain image, called a cover image. The cover image with the secret\nmessage is called a stego image. Steganography is commonly used for illegal\npurposes such as terrorist activities and pornography. To thwart covert\ncommunications and transactions, attacking algorithms against steganography,\ncalled steganalysis, exist. Currently, there are many studies implementing deep\nlearning to the steganography algorithm. However, conventional steganalysis is\nno longer effective for deep learning based steganography algorithms. Our\nframework is the first one to disturb covert communications and transactions\nvia the recent deep learning-based steganography algorithms. We first extract a\nsophisticated pixel distribution of the potential stego image from the\nauto-regressive model induced by deep learning. Using the extracted pixel\ndistributions, we detect whether an image is the stego or not at the pixel\nlevel. Each pixel value is adjusted as required and the adjustment induces an\neffective removal of the secret image. Because the decoding method of deep\nlearning-based steganography algorithms is approximate (lossy), which is\ndifferent from the conventional steganography, we propose a new quantitative\nmetric that is more suitable for measuring the accurate effect. We evaluate our\nmethod using three public benchmarks in comparison with a conventional\nsteganalysis method and show up to a 20% improvement in terms of decoding rate.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.11113v2"
    },
    {
        "title": "An Attention-Based Speaker Naming Method for Online Adaptation in\n  Non-Fixed Scenarios",
        "authors": [
            "Jungwoo Pyo",
            "Joohyun Lee",
            "Youngjune Park",
            "Tien-Cuong Bui",
            "Sang Kyun Cha"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  A speaker naming task, which finds and identifies the active speaker in a\ncertain movie or drama scene, is crucial for dealing with high-level video\nanalysis applications such as automatic subtitle labeling and video\nsummarization. Modern approaches have usually exploited biometric features with\na gradient-based method instead of rule-based algorithms. In a certain\nsituation, however, a naive gradient-based method does not work efficiently.\nFor example, when new characters are added to the target identification list,\nthe neural network needs to be frequently retrained to identify new people and\nit causes delays in model preparation. In this paper, we present an\nattention-based method which reduces the model setup time by updating the newly\nadded data via online adaptation without a gradient update process. We\ncomparatively analyzed with three evaluation metrics(accuracy, memory usage,\nsetup time) of the attention-based method and existing gradient-based methods\nunder various controlled settings of speaker naming. Also, we applied existing\nspeaker naming models and the attention-based model to real video to prove that\nour approach shows comparable accuracy to the existing state-of-the-art models\nand even higher accuracy in some cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.00649v1"
    },
    {
        "title": "KonVid-150k: A Dataset for No-Reference Video Quality Assessment of\n  Videos in-the-Wild",
        "authors": [
            "Franz Götz-Hahn",
            "Vlad Hosu",
            "Hanhe Lin",
            "Dietmar Saupe"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Video quality assessment (VQA) methods focus on particular degradation types,\nusually artificially induced on a small set of reference videos. Hence, most\ntraditional VQA methods under-perform in-the-wild. Deep learning approaches\nhave had limited success due to the small size and diversity of existing VQA\ndatasets, either artificial or authentically distorted. We introduce a new\nin-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k.\nIt consists of a coarsely annotated set of 153,841 videos having five quality\nratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally,\nwe propose new efficient VQA approaches (MLSP-VQA) relying on multi-level\nspatially pooled deep-features (MLSP). They are exceptionally well suited for\ntraining at scale, compared to deep transfer learning approaches. Our best\nmethod, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient\n(SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark\ndataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC)\nand hand-crafted feature-based method (0.78 SRCC). We further investigate how\nalternative approaches perform under different levels of label noise, and\ndataset size, showing that MLSP-VQA-FF is the overall best method for videos\nin-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k\nsets the new state-of-the-art for cross-test performance on KoNViD-1k,\nLIVE-VQC, and LIVE-Qualcomm with a 0.83, 0.75, and 0.64 SRCC, respectively. For\nboth KoNViD-1k and LIVE-VQC this inter-dataset testing outperforms\nintra-dataset experiments, showing excellent generalization.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.07966v2"
    },
    {
        "title": "Leveraging Topics and Audio Features with Multimodal Attention for Audio\n  Visual Scene-Aware Dialog",
        "authors": [
            "Shachi H Kumar",
            "Eda Okur",
            "Saurav Sahay",
            "Jonathan Huang",
            "Lama Nachman"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  With the recent advancements in Artificial Intelligence (AI), Intelligent\nVirtual Assistants (IVA) such as Alexa, Google Home, etc., have become a\nubiquitous part of many homes. Currently, such IVAs are mostly audio-based, but\ngoing forward, we are witnessing a confluence of vision, speech and dialog\nsystem technologies that are enabling the IVAs to learn audio-visual groundings\nof utterances. This will enable agents to have conversations with users about\nthe objects, activities and events surrounding them. In this work, we present\nthree main architectural explorations for the Audio Visual Scene-Aware Dialog\n(AVSD): 1) investigating `topics' of the dialog as an important contextual\nfeature for the conversation, 2) exploring several multimodal attention\nmechanisms during response generation, 3) incorporating an end-to-end audio\nclassification ConvNet, AclNet, into our architecture. We discuss detailed\nanalysis of the experimental results and show that our model variations\noutperform the baseline system presented for the AVSD task.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10131v1"
    },
    {
        "title": "Structural characterization of musical harmonies",
        "authors": [
            "Maria Rojo González",
            "Simone Santini"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Understanding the structural characteristics of harmony is essential for an\neffective use of music as a communication medium. Of the three expressive axes\nof music (melody, rhythm, harmony), harmony is the foundation on which the\nemotional content is built, and its understanding is important in areas such as\nmultimedia and affective computing. The common tool for studying this kind of\nstructure in computing science is the formal grammar but, in the case of music,\ngrammars run into problems due to the ambiguous nature of some of the concepts\ndefined in music theory. In this paper, we consider one of such constructs:\nmodulation, that is, the change of key in the middle of a musical piece, an\nimportant tool used by many authors to enhance the capacity of music to express\nemotions. We develop a hybrid method in which an evidence-gathering numerical\nmethod detects modulation and then, based on the detected tonalities, a\nnon-ambiguous grammar can be used for analyzing the structure of each tonal\ncomponent. Experiments with music from the XVII and XVIII centuries show that\nwe can detect the precise point of modulation with an error of at most two\nchords in almost 97% of the cases. Finally, we show examples of complete\nmodulation and structural analysis of musical harmonies.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12362v1"
    },
    {
        "title": "Adaptive Control of Embedding Strength in Image Watermarking using\n  Neural Networks",
        "authors": [
            "Mahnoosh Bagheri",
            "Majid Mohrekesh",
            "Nader Karimi",
            "Shadrokh Samavi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Digital image watermarking has been widely used in different applications\nsuch as copyright protection of digital media, such as audio, image, and video\nfiles. Two opposing criteria of robustness and transparency are the goals of\nwatermarking methods. In this paper, we propose a framework for determining the\nappropriate embedding strength factor. The framework can use most DWT and DCT\nbased blind watermarking approaches. We use Mask R-CNN on the COCO dataset to\nfind a good strength factor for each sub-block. Experiments show that this\nmethod is robust against different attacks and has good transparency.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.03251v1"
    },
    {
        "title": "Distortion Agnostic Deep Watermarking",
        "authors": [
            "Xiyang Luo",
            "Ruohan Zhan",
            "Huiwen Chang",
            "Feng Yang",
            "Peyman Milanfar"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Watermarking is the process of embedding information into an image that can\nsurvive under distortions, while requiring the encoded image to have little or\nno perceptual difference from the original image. Recently, deep learning-based\nmethods achieved impressive results in both visual quality and message payload\nunder a wide variety of image distortions. However, these methods all require\ndifferentiable models for the image distortions at training time, and may\ngeneralize poorly to unknown distortions. This is undesirable since the types\nof distortions applied to watermarked images are usually unknown and\nnon-differentiable. In this paper, we propose a new framework for\ndistortion-agnostic watermarking, where the image distortion is not explicitly\nmodeled during training. Instead, the robustness of our system comes from two\nsources: adversarial training and channel coding. Compared to training on a\nfixed set of distortions and noise levels, our method achieves comparable or\nbetter results on distortions available during training, and better performance\non unknown distortions.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.04580v1"
    },
    {
        "title": "AMP: Authentication of Media via Provenance",
        "authors": [
            "Paul England",
            "Henrique S. Malvar",
            "Eric Horvitz",
            "Jack W. Stokes",
            "Cédric Fournet",
            "Rebecca Burke-Aguero",
            "Amaury Chamayou",
            "Sylvan Clebsch",
            "Manuel Costa",
            "John Deutscher",
            "Shabnam Erfani",
            "Matt Gaylor",
            "Andrew Jenks",
            "Kevin Kane",
            "Elissa Redmiles",
            "Alex Shamis",
            "Isha Sharma",
            "Sam Wenker",
            "Anika Zaman"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Advances in graphics and machine learning have led to the general\navailability of easy-to-use tools for modifying and synthesizing media. The\nproliferation of these tools threatens to cast doubt on the veracity of all\nmedia. One approach to thwarting the flow of fake media is to detect modified\nor synthesized media through machine learning methods. While detection may help\nin the short term, we believe that it is destined to fail as the quality of\nfake media generation continues to improve. Soon, neither humans nor algorithms\nwill be able to reliably distinguish fake versus real content. Thus, pipelines\nfor assuring the source and integrity of media will be required---and\nincreasingly relied upon. We propose AMP, a system that ensures the\nauthentication of media via certifying provenance. AMP creates one or more\npublisher-signed manifests for a media instance uploaded by a content provider.\nThese manifests are stored in a database allowing fast lookup from applications\nsuch as browsers. For reference, the manifests are also registered and signed\nby a permissioned ledger, implemented using the Confidential Consortium\nFramework (CCF). CCF employs both software and hardware techniques to ensure\nthe integrity and transparency of all registered manifests. AMP, through its\nuse of CCF, enables a consortium of media providers to govern the service while\nmaking all its operations auditable. The authenticity of the media can be\ncommunicated to the user via visual elements in the browser, indicating that an\nAMP manifest has been successfully located and verified.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.07886v3"
    },
    {
        "title": "NAViDAd: A No-Reference Audio-Visual Quality Metric Based on a Deep\n  Autoencoder",
        "authors": [
            "Helard Martinez",
            "M. C. Farias",
            "A. Hines"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The development of models for quality prediction of both audio and video\nsignals is a fairly mature field. But, although several multimodal models have\nbeen proposed, the area of audio-visual quality prediction is still an emerging\narea. In fact, despite the reasonable performance obtained by combination and\nparametric metrics, currently there is no reliable pixel-based audio-visual\nquality metric. The approach presented in this work is based on the assumption\nthat autoencoders, fed with descriptive audio and video features, might produce\na set of features that is able to describe the complex audio and video\ninteractions. Based on this hypothesis, we propose a No-Reference Audio-Visual\nQuality Metric Based on a Deep Autoencoder (NAViDAd). The model visual features\nare natural scene statistics (NSS) and spatial-temporal measures of the video\ncomponent. Meanwhile, the audio features are obtained by computing the\nspectrogram representation of the audio component. The model is formed by a\n2-layer framework that includes a deep autoencoder layer and a classification\nlayer. These two layers are stacked and trained to build the deep neural\nnetwork model. The model is trained and tested using a large set of stimuli,\ncontaining representative audio and video artifacts. The model performed well\nwhen tested against the UnB-AV and the LiveNetflix-II databases. %Results shows\nthat this type of approach produces quality scores that are highly correlated\nto subjective quality scores.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.11406v2"
    },
    {
        "title": "Towards Deep Learning Methods for Quality Assessment of\n  Computer-Generated Imagery",
        "authors": [
            "Markus Utke",
            "Saman Zadtootaghaj",
            "Steven Schmidt",
            "Sebastian Möller"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Video gaming streaming services are growing rapidly due to new services such\nas passive video streaming, e.g. Twitch.tv, and cloud gaming, e.g. Nvidia\nGeforce Now. In contrast to traditional video content, gaming content has\nspecial characteristics such as extremely high motion for some games, special\nmotion patterns, synthetic content and repetitive content, which makes the\nstate-of-the-art video and image quality metrics perform weaker for this\nspecial computer generated content. In this paper, we outline our plan to build\na deep learningbased quality metric for video gaming quality assessment. In\naddition, we present initial results by training the network based on VMAF\nvalues as a ground truth to give some insights on how to build a metric in\nfuture. The paper describes the method that is used to choose an appropriate\nConvolutional Neural Network architecture. Furthermore, we estimate the size of\nthe required subjective quality dataset which achieves a sufficiently high\nperformance. The results show that by taking around 5k images for training of\nthe last six modules of Xception, we can obtain a relatively high performance\nmetric to assess the quality of distorted video games.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.00836v1"
    },
    {
        "title": "Cross-media Structured Common Space for Multimedia Event Extraction",
        "authors": [
            "Manling Li",
            "Alireza Zareian",
            "Qi Zeng",
            "Spencer Whitehead",
            "Di Lu",
            "Heng Ji",
            "Shih-Fu Chang"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  We introduce a new task, MultiMedia Event Extraction (M2E2), which aims to\nextract events and their arguments from multimedia documents. We develop the\nfirst benchmark and collect a dataset of 245 multimedia news articles with\nextensively annotated events and arguments. We propose a novel method, Weakly\nAligned Structured Embedding (WASE), that encodes structured representations of\nsemantic information from textual and visual data into a common embedding\nspace. The structures are aligned across modalities by employing a weakly\nsupervised training strategy, which enables exploiting available resources\nwithout explicit cross-media annotation. Compared to uni-modal state-of-the-art\nmethods, our approach achieves 4.0% and 9.8% absolute F-score gains on text\nevent argument role labeling and visual event extraction. Compared to\nstate-of-the-art multimedia unstructured representations, we achieve 8.3% and\n5.0% absolute F-score gains on multimedia event extraction and argument role\nlabeling, respectively. By utilizing images, we extract 21.4% more event\nmentions than traditional text-only methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.02472v1"
    },
    {
        "title": "Comment on \"No-Reference Video Quality Assessment Based on the Temporal\n  Pooling of Deep Features\"",
        "authors": [
            "Franz Götz-Hahn",
            "Vlad Hosu",
            "Dietmar Saupe"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In Neural Processing Letters 50,3 (2019) a machine learning approach to blind\nvideo quality assessment was proposed. It is based on temporal pooling of\nfeatures of video frames, taken from the last pooling layer of deep\nconvolutional neural networks. The method was validated on two established\nbenchmark datasets and gave results far better than the previous\nstate-of-the-art. In this letter we report the results from our careful\nreimplementations. The performance results, claimed in the paper, cannot be\nreached, and are even below the state-of-the-art by a large margin. We show\nthat the originally reported wrong performance results are a consequence of two\ncases of data leakage. Information from outside the training dataset was used\nin the fine-tuning stage and in the model evaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.04400v1"
    },
    {
        "title": "Complexity Analysis Of Next-Generation VVC Encoding and Decoding",
        "authors": [
            "Farhad Pakdaman",
            "Mohammad Ali Adelimanesh",
            "Moncef Gabbouj",
            "Mahmoud Reza Hashemi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  While the next generation video compression standard, Versatile Video Coding\n(VVC), provides a superior compression efficiency, its computational complexity\ndramatically increases. This paper thoroughly analyzes this complexity for both\nencoder and decoder of VVC Test Model 6, by quantifying the complexity\nbreak-down for each coding tool and measuring the complexity and memory\nrequirements for VVC encoding/decoding. These extensive analyses are performed\nfor six video sequences of 720p, 1080p, and 2160p, under Low-Delay (LD),\nRandom-Access (RA), and All-Intra (AI) conditions (a total of 320\nencoding/decoding). Results indicate that the VVC encoder and decoder are 5x\nand 1.5x more complex compared to HEVC in LD, and 31x and 1.8x in AI,\nrespectively. Detailed analysis of coding tools reveals that in LD on average,\nmotion estimation tools with 53%, transformation and quantization with 22%, and\nentropy coding with 7% dominate the encoding complexity. In decoding, loop\nfilters with 30%, motion compensation with 20%, and entropy decoding with 16%,\nare the most complex modules. Moreover, the required memory bandwidth for VVC\nencoding/decoding are measured through memory profiling, which are 30x and 3x\nof HEVC. The reported results and insights are a guide for future research and\nimplementations of energy-efficient VVC encoder/decoder.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.10801v1"
    },
    {
        "title": "Latent Semantic Learning with Structured Sparse Representation for Human\n  Action Recognition",
        "authors": [
            "Zhiwu Lu",
            "Yuxin Peng"
        ],
        "category": "cs.MM",
        "published_year": "2011",
        "summary": "  This paper proposes a novel latent semantic learning method for extracting\nhigh-level features (i.e. latent semantics) from a large vocabulary of abundant\nmid-level features (i.e. visual keywords) with structured sparse\nrepresentation, which can help to bridge the semantic gap in the challenging\ntask of human action recognition. To discover the manifold structure of\nmidlevel features, we develop a spectral embedding approach to latent semantic\nlearning based on L1-graph, without the need to tune any parameter for graph\nconstruction as a key step of manifold learning. More importantly, we construct\nthe L1-graph with structured sparse representation, which can be obtained by\nstructured sparse coding with its structured sparsity ensured by novel L1-norm\nhypergraph regularization over mid-level features. In the new embedding space,\nwe learn latent semantics automatically from abundant mid-level features\nthrough spectral clustering. The learnt latent semantics can be readily used\nfor human action recognition with SVM by defining a histogram intersection\nkernel. Different from the traditional latent semantic analysis based on topic\nmodels, our latent semantic learning method can explore the manifold structure\nof mid-level features in both L1-graph construction and spectral embedding,\nwhich results in compact but discriminative high-level features. The\nexperimental results on the commonly used KTH action dataset and unconstrained\nYouTube action dataset show the superior performance of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.4979v1"
    },
    {
        "title": "Approximate Similarity Search for Online Multimedia Services on\n  Distributed CPU-GPU Platforms",
        "authors": [
            "George Teodoro",
            "Eduardo Valle",
            "Nathan Mariano",
            "Ricardo Torres",
            "Wagner Meira Jr",
            "Joel H. Saltz"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  Similarity search in high-dimentional spaces is a pivotal operation found a\nvariety of database applications. Recently, there has been an increase interest\nin similarity search for online content-based multimedia services. Those\nservices, however, introduce new challenges with respect to the very large\nvolumes of data that have to be indexed/searched, and the need to minimize\nresponse times observed by the end-users. Additionally, those users dynamically\ninteract with the systems creating fluctuating query request rates, requiring\nthe search algorithm to adapt in order to better utilize the underline hardware\nto reduce response times. In order to address these challenges, we introduce\nhypercurves, a flexible framework for answering approximate k-nearest neighbor\n(kNN) queries for very large multimedia databases, aiming at online\ncontent-based multimedia services. Hypercurves executes on hybrid CPU--GPU\nenvironments, and is able to employ those devices cooperatively to support\nmassive query request rates. In order to keep the response times optimal as the\nrequest rates vary, it employs a novel dynamic scheduler to partition the work\nbetween CPU and GPU. Hypercurves was throughly evaluated using a large database\nof multimedia descriptors. Its cooperative CPU--GPU execution achieved\nperformance improvements of up to 30x when compared to the single CPU-core\nversion. The dynamic work partition mechanism reduces the observed query\nresponse times in about 50% when compared to the best static CPU--GPU task\npartition configuration. In addition, Hypercurves achieves superlinear\nscalability in distributed (multi-node) executions, while keeping a high\nguarantee of equivalence with its sequential version --- thanks to the proof of\nprobabilistic equivalence, which supported its aggressive parallelization\ndesign.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.0410v1"
    },
    {
        "title": "Recovering Missing Coefficients in DCT-Transformed Images",
        "authors": [
            "Shujun Li",
            "Andreas Karrenbauer",
            "Dietmar Saupe",
            "C. -C. Jay Kuo"
        ],
        "category": "cs.MM",
        "published_year": "2012",
        "summary": "  A general method for recovering missing DCT coefficients in DCT-transformed\nimages is presented in this work. We model the DCT coefficients recovery\nproblem as an optimization problem and recover all missing DCT coefficients via\nlinear programming. The visual quality of the recovered image gradually\ndecreases as the number of missing DCT coefficients increases. For some images,\nthe quality is surprisingly good even when more than 10 most significant DCT\ncoefficients are missing. When only the DC coefficient is missing, the proposed\nalgorithm outperforms existing methods according to experimental results\nconducted on 200 test images. The proposed recovery method can be used for\ncryptanalysis of DCT based selective encryption schemes and other applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.1673v1"
    },
    {
        "title": "Deep learning is a good steganalysis tool when embedding key is reused\n  for different images, even if there is a cover source-mismatch",
        "authors": [
            "Lionel Pibre",
            "Pasquet Jérôme",
            "Dino Ienco",
            "Marc Chaumont"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Since the BOSS competition, in 2010, most steganalysis approaches use a\nlearning methodology involving two steps: feature extraction, such as the Rich\nModels (RM), for the image representation, and use of the Ensemble Classifier\n(EC) for the learning step. In 2015, Qian et al. have shown that the use of a\ndeep learning approach that jointly learns and computes the features, is very\npromising for the steganalysis. In this paper, we follow-up the study of Qian\net al., and show that, due to intrinsic joint minimization, the results\nobtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural\nNetwork (FNN), if well parameterized, surpass the conventional use of a RM with\nan EC. First, numerous experiments were conducted in order to find the best \"\nshape \" of the CNN. Second, experiments were carried out in the clairvoyant\nscenario in order to compare the CNN and FNN to an RM with an EC. The results\nshow more than 16% reduction in the classification error with our CNN or FNN.\nThird, experiments were also performed in a cover-source mismatch setting. The\nresults show that the CNN and FNN are naturally robust to the mismatch problem.\nIn Addition to the experiments, we provide discussions on the internal\nmechanisms of a CNN, and weave links with some previously stated ideas, in\norder to understand the impressive results we obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.04855v2"
    },
    {
        "title": "Fine-Grain Annotation of Cricket Videos",
        "authors": [
            "Rahul Anand Sharma",
            "Pramod Sankar K",
            "CV Jawahar"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The recognition of human activities is one of the key problems in video\nunderstanding. Action recognition is challenging even for specific categories\nof videos, such as sports, that contain only a small set of actions.\nInterestingly, sports videos are accompanied by detailed commentaries available\nonline, which could be used to perform action annotation in a weakly-supervised\nsetting. For the specific case of Cricket videos, we address the challenge of\ntemporal segmentation and annotation of ctions with semantic descriptions. Our\nsolution consists of two stages. In the first stage, the video is segmented\ninto \"scenes\", by utilizing the scene category information extracted from\ntext-commentary. The second stage consists of classifying video-shots as well\nas the phrases in the textual description into various categories. The relevant\nphrases are then suitably mapped to the video-shots. The novel aspect of this\nwork is the fine temporal scale at which semantic information is assigned to\nthe video. As a result of our approach, we enable retrieval of specific actions\nthat last only a few seconds, from several hours of video. This solution yields\na large number of labeled exemplars, with no manual effort, that could be used\nby machine learning algorithms to learn complex actions.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.07607v2"
    },
    {
        "title": "Creativity in Mind: Evaluating and Maintaining Advances in Network\n  Steganographic Research",
        "authors": [
            "Steffen Wendzel",
            "Carolin Palmer"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  The research discipline of network steganography deals with the hiding of\ninformation within network transmissions, e.g. to transfer illicit information\nin networks with Internet censorship. The last decades of research on network\nsteganography led to more than hundred techniques for hiding data in network\ntransmissions. However, previous research has shown that most of these hiding\ntechniques are either based on the same idea or introduce limited novelty,\nenabling the application of existing countermeasures. In this paper, we provide\na link between the field of creativity and network steganographic research. We\npropose a framework and a metric to help evaluating the creativity bound to a\ngiven hiding technique. This way, we support two sides of the scientific peer\nreview process as both authors and reviewers can use our framework to analyze\nthe novelty and applicability of hiding techniques. At the same time, we\ncontribute to a uniform terminology in network steganography.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.08507v1"
    },
    {
        "title": "Learning Affective Correspondence between Music and Image",
        "authors": [
            "Gaurav Verma",
            "Eeshan Gunesh Dhekane",
            "Tanaya Guha"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  We introduce the problem of learning affective correspondence between audio\n(music) and visual data (images). For this task, a music clip and an image are\nconsidered similar (having true correspondence) if they have similar emotion\ncontent. In order to estimate this crossmodal, emotion-centric similarity, we\npropose a deep neural network architecture that learns to project the data from\nthe two modalities to a common representation space, and performs a binary\nclassification task of predicting the affective correspondence (true or false).\nTo facilitate the current study, we construct a large scale database containing\nmore than $3,500$ music clips and $85,000$ images with three emotion classes\n(positive, neutral, negative). The proposed approach achieves $61.67\\%$\naccuracy for the affective correspondence prediction task on this database,\noutperforming two relevant and competitive baselines. We also demonstrate that\nour network learns modality-specific representations of emotion (without\nexplicitly being trained with emotion labels), which are useful for emotion\nrecognition in individual modalities.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00150v2"
    },
    {
        "title": "MMED: A Multi-domain and Multi-modality Event Dataset",
        "authors": [
            "Zhenguo Yang",
            "Zehang Lin",
            "Min Cheng",
            "Qing Li",
            "Wenyin Liu"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In this work, we construct and release a multi-domain and multi-modality\nevent dataset (MMED), containing 25,165 textual news articles collected from\nhundreds of news media sites (e.g., Yahoo News, Google News, CNN News.) and\n76,516 image posts shared on Flickr social media, which are annotated according\nto 412 real-world events. The dataset is collected to explore the problem of\norganizing heterogeneous data contributed by professionals and amateurs in\ndifferent data domains, and the problem of transferring event knowledge\nobtained from one data domain to heterogeneous data domain, thus summarizing\nthe data with different contributors. We hope that the release of the MMED\ndataset can stimulate innovate research on related challenging problems, such\nas event discovery, cross-modal (event) retrieval, and visual question\nanswering, etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.02354v2"
    },
    {
        "title": "Quality Assessment of In-the-Wild Videos",
        "authors": [
            "Dingquan Li",
            "Tingting Jiang",
            "Ming Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Quality assessment of in-the-wild videos is a challenging problem because of\nthe absence of reference videos and shooting distortions. Knowledge of the\nhuman visual system can help establish methods for objective quality assessment\nof in-the-wild videos. In this work, we show two eminent effects of the human\nvisual system, namely, content-dependency and temporal-memory effects, could be\nused for this purpose. We propose an objective no-reference video quality\nassessment method by integrating both effects into a deep neural network. For\ncontent-dependency, we extract features from a pre-trained image classification\nneural network for its inherent content-aware property. For temporal-memory\neffects, long-term dependencies, especially the temporal hysteresis, are\nintegrated into the network with a gated recurrent unit and a\nsubjectively-inspired temporal pooling layer. To validate the performance of\nour method, experiments are conducted on three publicly available in-the-wild\nvideo quality assessment databases: KoNViD-1k, CVD2014, and LIVE-Qualcomm,\nrespectively. Experimental results demonstrate that our proposed method\noutperforms five state-of-the-art methods by a large margin, specifically,\n12.39%, 15.71%, 15.45%, and 18.09% overall performance improvements over the\nsecond-best method VBLIINDS, in terms of SROCC, KROCC, PLCC and RMSE,\nrespectively. Moreover, the ablation study verifies the crucial role of both\nthe content-aware features and the modeling of temporal-memory effects. The\nPyTorch implementation of our method is released at\nhttps://github.com/lidq92/VSFA.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.00375v3"
    },
    {
        "title": "Comyco: Quality-Aware Adaptive Video Streaming via Imitation Learning",
        "authors": [
            "Tianchi Huang",
            "Chao Zhou",
            "Rui-Xiao Zhang",
            "Chenglei Wu",
            "Xin Yao",
            "Lifeng Sun"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Learning-based Adaptive Bit Rate~(ABR) method, aiming to learn outstanding\nstrategies without any presumptions, has become one of the research hotspots\nfor adaptive streaming. However, it typically suffers from several issues,\ni.e., low sample efficiency and lack of awareness of the video quality\ninformation. In this paper, we propose Comyco, a video quality-aware ABR\napproach that enormously improves the learning-based methods by tackling the\nabove issues. Comyco trains the policy via imitating expert trajectories given\nby the instant solver, which can not only avoid redundant exploration but also\nmake better use of the collected samples. Meanwhile, Comyco attempts to pick\nthe chunk with higher perceptual video qualities rather than video bitrates. To\nachieve this, we construct Comyco's neural network architecture, video datasets\nand QoE metrics with video quality features. Using trace-driven and real-world\nexperiments, we demonstrate significant improvements of Comyco's sample\nefficiency in comparison to prior work, with 1700x improvements in terms of the\nnumber of samples required and 16x improvements on training time required.\nMoreover, results illustrate that Comyco outperforms previously proposed\nmethods, with the improvements on average QoE of 7.5% - 16.79%. Especially,\nComyco also surpasses state-of-the-art approach Pensieve by 7.37% on average\nvideo quality under the same rebuffering time.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02270v2"
    },
    {
        "title": "A Robust Image Watermarking System Based on Deep Neural Networks",
        "authors": [
            "Xin Zhong",
            "Frank Y. Shih"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Digital image watermarking is the process of embedding and extracting\nwatermark covertly on a carrier image. Incorporating deep learning networks\nwith image watermarking has attracted increasing attention during recent years.\nHowever, existing deep learning-based watermarking systems cannot achieve\nrobustness, blindness, and automated embedding and extraction simultaneously.\nIn this paper, a fully automated image watermarking system based on deep neural\nnetworks is proposed to generalize the image watermarking processes. An\nunsupervised deep learning structure and a novel loss computation are proposed\nto achieve high capacity and high robustness without any prior knowledge of\npossible attacks. Furthermore, a challenging application of watermark\nextraction from camera-captured images is provided to validate the practicality\nas well as the robustness of the proposed system. Experimental results show the\nsuperiority performance of the proposed system as comparing against several\ncurrently available techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.11331v1"
    },
    {
        "title": "MG-VAE: Deep Chinese Folk Songs Generation with Specific Regional Style",
        "authors": [
            "Jing Luo",
            "Xinyu Yang",
            "Shulei Ji",
            "Juan Li"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Regional style in Chinese folk songs is a rich treasure that can be used for\nethnic music creation and folk culture research. In this paper, we propose\nMG-VAE, a music generative model based on VAE (Variational Auto-Encoder) that\nis capable of capturing specific music style and generating novel tunes for\nChinese folk songs (Min Ge) in a manipulatable way. Specifically, we\ndisentangle the latent space of VAE into four parts in an adversarial training\nway to control the information of pitch and rhythm sequence, as well as of\nmusic style and content. In detail, two classifiers are used to separate style\nand content latent space, and temporal supervision is utilized to disentangle\nthe pitch and rhythm sequence. The experimental results show that the\ndisentanglement is successful and our model is able to create novel folk songs\nwith controllable regional styles. To our best knowledge, this is the first\nstudy on applying deep generative model and adversarial training for Chinese\nmusic generation.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.13287v1"
    },
    {
        "title": "Towards Multimodal Understanding of Passenger-Vehicle Interactions in\n  Autonomous Vehicles: Intent/Slot Recognition Utilizing Audio-Visual Data",
        "authors": [
            "Eda Okur",
            "Shachi H Kumar",
            "Saurav Sahay",
            "Lama Nachman"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Understanding passenger intents from spoken interactions and car's vision\n(both inside and outside the vehicle) are important building blocks towards\ndeveloping contextual dialog systems for natural interactions in autonomous\nvehicles (AV). In this study, we continued exploring AMIE (Automated-vehicle\nMultimodal In-cabin Experience), the in-cabin agent responsible for handling\ncertain multimodal passenger-vehicle interactions. When the passengers give\ninstructions to AMIE, the agent should parse such commands properly considering\navailable three modalities (language/text, audio, video) and trigger the\nappropriate functionality of the AV system. We had collected a multimodal\nin-cabin dataset with multi-turn dialogues between the passengers and AMIE\nusing a Wizard-of-Oz scheme via realistic scavenger hunt game. In our previous\nexplorations, we experimented with various RNN-based models to detect\nutterance-level intents (set destination, change route, go faster, go slower,\nstop, park, pull over, drop off, open door, and others) along with intent\nkeywords and relevant slots (location, position/direction, object,\ngesture/gaze, time-guidance, person) associated with the action to be performed\nin our AV scenarios. In this recent work, we propose to discuss the benefits of\nmultimodal understanding of in-cabin utterances by incorporating\nverbal/language input (text and speech embeddings) together with the\nnon-verbal/acoustic and visual input from inside and outside the vehicle (i.e.,\npassenger gestures and gaze from in-cabin video stream, referred objects\noutside of the vehicle from the road view camera stream). Our experimental\nresults outperformed text-only baselines and with multimodality, we achieved\nimproved performances for utterance-level intent detection and slot filling.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.13714v1"
    },
    {
        "title": "Learning a Representation for Cover Song Identification Using\n  Convolutional Neural Network",
        "authors": [
            "Zhesong Yu",
            "Xiaoshuo Xu",
            "Xiaoou Chen",
            "Deshun Yang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Cover song identification represents a challenging task in the field of Music\nInformation Retrieval (MIR) due to complex musical variations between query\ntracks and cover versions. Previous works typically utilize hand-crafted\nfeatures and alignment algorithms for the task. More recently, further\nbreakthroughs are achieved employing neural network approaches. In this paper,\nwe propose a novel Convolutional Neural Network (CNN) architecture based on the\ncharacteristics of the cover song task. We first train the network through\nclassification strategies; the network is then used to extract music\nrepresentation for cover song identification. A scheme is designed to train\nrobust models against tempo changes. Experimental results show that our\napproach outperforms state-of-the-art methods on all public datasets, improving\nthe performance especially on the large dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00334v1"
    },
    {
        "title": "Robustness and Imperceptibility Enhancement in Watermarked Images by\n  Color Transformation",
        "authors": [
            "Maedeh Jamali",
            "Mahnoosh Bagheri",
            "Nader Karimi",
            "Shadrokh Samavi"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  One of the effective methods for the preservation of copyright ownership of\ndigital media is watermarking. Different watermarking techniques try to set a\ntradeoff between robustness and transparency of the process. In this research\nwork, we have used color space conversion and frequency transform to achieve\nhigh robustness and transparency. Due to the distribution of image information\nin the RGB domain, we use the YUV color space, which concentrates the visual\ninformation in the Y channel. Embedding of the watermark is performed in the\nDCT coefficients of the specific wavelet subbands. Experimental results show\nhigh transparency and robustness of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00772v1"
    },
    {
        "title": "Parametric Graph-based Separable Transforms for Video Coding",
        "authors": [
            "Hilmi E. Egilmez",
            "Oguzhan Teke",
            "Amir Said",
            "Vadim Seregin",
            "Marta Karczewicz"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In many video coding systems, separable transforms (such as two-dimensional\nDCT-2) have been used to code block residual signals obtained after prediction.\nThis paper proposes a parametric approach to build graph-based separable\ntransforms (GBSTs) for video coding. Specifically, a GBST is derived from a\npair of line graphs, whose weights are determined based on two non-negative\nparameters. As certain choices of those parameters correspond to the discrete\nsine and cosine transform types used in recent video coding standards\n(including DCT-2, DST-7 and DCT-8), this paper further optimizes these graph\nparameters to better capture residual block statistics and improve video coding\nefficiency. The proposed GBSTs are tested on the Versatile Video Coding (VVC)\nreference software, and the experimental results show that about 0.4% average\ncoding gain is achieved over the existing set of separable transforms\nconstructed based on DCT-2, DST-7 and DCT-8 in VVC.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.06981v3"
    },
    {
        "title": "A Graph-based Ranking Approach to Extract Key-frames for Static Video\n  Summarization",
        "authors": [
            "Saikat Chakraborty"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Video abstraction has become one of the efficient approaches to grasp the\ncontent of a video without seeing it entirely. Key frame-based static video\nsummarization falls under this category. In this paper, we propose a\ngraph-based approach which summarizes the video with best user satisfaction. We\ntreated each video frame as a node of the graph and assigned a rank to each\nnode by our proposed VidRank algorithm. We developed three different models of\nVidRank algorithm and performed a comparative study on those models. A\ncomprehensive evaluation of 50 videos from open video database using objective\nand semi-objective measures indicates the superiority of our static video\nsummary generation method.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.13279v1"
    },
    {
        "title": "Robust watermarking with double detector-discriminator approach",
        "authors": [
            "Marcin Plata",
            "Piotr Syga"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In this paper we present a novel deep framework for a watermarking - a\ntechnique of embedding a transparent message into an image in a way that allows\nretrieving the message from a (perturbed) copy, so that copyright infringement\ncan be tracked. For this technique, it is essential to extract the information\nfrom the image even after imposing some digital processing operations on it.\nOur framework outperforms recent methods in the context of robustness against\nnot only spectrum of attacks (e.g. rotation, resizing, Gaussian smoothing) but\nalso against compression, especially JPEG. The bit accuracy of our method is at\nleast 0.86 for all types of distortions. We also achieved 0.90 bit accuracy for\nJPEG while recent methods provided at most 0.83. Our method retains high\ntransparency and capacity as well. Moreover, we present our double\ndetector-discriminator approach - a scheme to detect and discriminate if the\nimage contains the embedded message or not, which is crucial for real-life\nwatermarking systems and up to now was not investigated using neural networks.\nWith this, we design a testing formula to validate our extended approach and\ncompared it with a common procedure. We also present an alternative method of\nbalancing between image quality and robustness on attacks which is easily\napplicable to the framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.03921v1"
    },
    {
        "title": "Hysia: Serving DNN-Based Video-to-Retail Applications in Cloud",
        "authors": [
            "Huaizheng Zhang",
            "Yuanming Li",
            "Qiming Ai",
            "Yong Luo",
            "Yonggang Wen",
            "Yichao Jin",
            "Nguyen Binh Duong Ta"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Combining \\underline{v}ideo streaming and online \\underline{r}etailing (V2R)\nhas been a growing trend recently. In this paper, we provide practitioners and\nresearchers in multimedia with a cloud-based platform named Hysia for easy\ndevelopment and deployment of V2R applications. The system consists of: 1) a\nback-end infrastructure providing optimized V2R related services including data\nengine, model repository, model serving and content matching; and 2) an\napplication layer which enables rapid V2R application prototyping. Hysia\naddresses industry and academic needs in large-scale multimedia by: 1)\nseamlessly integrating state-of-the-art libraries including NVIDIA video SDK,\nFacebook faiss, and gRPC; 2) efficiently utilizing GPU computation; and 3)\nallowing developers to bind new models easily to meet the rapidly changing deep\nlearning (DL) techniques. On top of that, we implement an orchestrator for\nfurther optimizing DL model serving performance. Hysia has been released as an\nopen source project on GitHub, and attracted considerable attention. We have\npublished Hysia to DockerHub as an official image for seamless integration and\ndeployment in current cloud environments.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.05117v1"
    },
    {
        "title": "Capturing Video Frame Rate Variations via Entropic Differencing",
        "authors": [
            "Pavan C. Madhusudana",
            "Neil Birkbeck",
            "Yilin Wang",
            "Balu Adsumilli",
            "Alan C. Bovik"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  High frame rate videos are increasingly getting popular in recent years,\ndriven by the strong requirements of the entertainment and streaming industries\nto provide high quality of experiences to consumers. To achieve the best\ntrade-offs between the bandwidth requirements and video quality in terms of\nframe rate adaptation, it is imperative to understand the effects of frame rate\non video quality. In this direction, we devise a novel statistical entropic\ndifferencing method based on a Generalized Gaussian Distribution model\nexpressed in the spatial and temporal band-pass domains, which measures the\ndifference in quality between reference and distorted videos. The proposed\ndesign is highly generalizable and can be employed when the reference and\ndistorted sequences have different frame rates. Our proposed model correlates\nvery well with subjective scores in the recently proposed LIVE-YT-HFR database\nand achieves state of the art performance when compared with existing\nmethodologies.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.11424v2"
    },
    {
        "title": "On Addressing the Impact of ISO Speed upon PRNU and Forgery Detection",
        "authors": [
            "Yijun Quan",
            "Chang-Tsun Li"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Photo Response Non-Uniformity (PRNU) has been used as a powerful device\nfingerprint for image forgery detection because image forgeries can be revealed\nby finding the absence of the PRNU in the manipulated areas. The correlation\nbetween an image's noise residual with the device's reference PRNU is often\ncompared with a decision threshold to check the existence of the PRNU. A PRNU\ncorrelation predictor is usually used to determine this decision threshold\nassuming the correlation is content-dependent. However, we found that not only\nthe correlation is content-dependent, but it also depends on the camera\nsensitivity setting. \\textit{Camera sensitivity}, commonly known by the name of\n\\textit{ISO speed}, is an important attribute in digital photography. In this\nwork, we will show the PRNU correlation's dependency on ISO speed. Due to such\ndependency, we postulate that a correlation predictor is ISO speed-specific,\ni.e. \\textit{reliable correlation predictions can only be made when a\ncorrelation predictor is trained with images of similar ISO speeds to the image\nin question}. We report the experiments we conducted to validate the postulate.\nIt is realized that in the real-world, information about the ISO speed may not\nbe available in the metadata to facilitate the implementation of our postulate\nin the correlation prediction process. We hence propose a method called\nContent-based Inference of ISO Speeds (CINFISOS) to infer the ISO speed from\nthe image content.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.11539v1"
    },
    {
        "title": "Leveraging Audio Gestalt to Predict Media Memorability",
        "authors": [
            "Lorin Sweeney",
            "Graham Healy",
            "Alan F. Smeaton"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Memorability determines what evanesces into emptiness, and what worms its way\ninto the deepest furrows of our minds. It is the key to curating more\nmeaningful media content as we wade through daily digital torrents. The\nPredicting Media Memorability task in MediaEval 2020 aims to address the\nquestion of media memorability by setting the task of automatically predicting\nvideo memorability. Our approach is a multimodal deep learning-based late\nfusion that combines visual, semantic, and auditory features. We used audio\ngestalt to estimate the influence of the audio modality on overall video\nmemorability, and accordingly inform which combination of features would best\npredict a given video's memorability scores.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.15635v1"
    },
    {
        "title": "Investigating Memorability of Dynamic Media",
        "authors": [
            "Phuc H. Le-Khac",
            "Ayush K. Rai",
            "Graham Healy",
            "Alan F. Smeaton",
            "Noel E. O'Connor"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The Predicting Media Memorability task in MediaEval'20 has some challenging\naspects compared to previous years. In this paper we identify the high-dynamic\ncontent in videos and dataset of limited size as the core challenges for the\ntask, we propose directions to overcome some of these challenges and we present\nour initial result in these directions.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.15641v1"
    },
    {
        "title": "Overview of MediaEval 2020 Predicting Media Memorability Task: What\n  Makes a Video Memorable?",
        "authors": [
            "Alba García Seco De Herrera",
            "Rukiye Savran Kiziltepe",
            "Jon Chamberlain",
            "Mihai Gabriel Constantin",
            "Claire-Hélène Demarty",
            "Faiyaz Doctor",
            "Bogdan Ionescu",
            "Alan F. Smeaton"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper describes the MediaEval 2020 \\textit{Predicting Media\nMemorability} task. After first being proposed at MediaEval 2018, the\nPredicting Media Memorability task is in its 3rd edition this year, as the\nprediction of short-term and long-term video memorability (VM) remains a\nchallenging task. In 2020, the format remained the same as in previous\neditions. This year the videos are a subset of the TRECVid 2019 Video-to-Text\ndataset, containing more action rich video content as compared with the 2019\ntask. In this paper a description of some aspects of this task is provided,\nincluding its main characteristics, a description of the collection, the ground\ntruth dataset, evaluation metrics and the requirements for participants' run\nsubmissions.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.15650v1"
    },
    {
        "title": "Visual Framing of Science Conspiracy Videos: Integrating Machine\n  Learning with Communication Theories to Study the Use of Color and Brightness",
        "authors": [
            "Kaiping Chen",
            "Sang Jung Kim",
            "Qiantong Gao",
            "Sebastian Raschka"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Recent years have witnessed an explosion of science conspiracy videos on the\nInternet, challenging science epistemology and public understanding of science.\nScholars have started to examine the persuasion techniques used in conspiracy\nmessages such as uncertainty and fear yet, little is understood about the\nvisual narratives, especially how visual narratives differ in videos that\ndebunk conspiracies versus those that propagate conspiracies. This paper\naddresses this gap in understanding visual framing in conspiracy videos through\nanalyzing millions of frames from conspiracy and counter-conspiracy YouTube\nvideos using computational methods. We found that conspiracy videos tended to\nuse lower color variance and brightness, especially in thumbnails and earlier\nparts of the videos. This paper also demonstrates how researchers can integrate\ntextual and visual features in machine learning models to study conspiracies on\nsocial media and discusses the implications of computational modeling for\nscholars interested in studying visual manipulation in the digital era. The\nanalysis of visual and textual features presented in this paper could be useful\nfor future studies focused on designing systems to identify conspiracy content\non the Internet.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.01163v3"
    },
    {
        "title": "A Survey on 360-Degree Video: Coding, Quality of Experience and\n  Streaming",
        "authors": [
            "Federico Chiariotti"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The commercialization of Virtual Reality (VR) headsets has made immersive and\n360-degree video streaming the subject of intense interest in the industry and\nresearch communities. While the basic principles of video streaming are the\nsame, immersive video presents a set of specific challenges that need to be\naddressed. In this survey, we present the latest developments in the relevant\nliterature on four of the most important ones: (i) omnidirectional video coding\nand compression, (ii) subjective and objective Quality of Experience (QoE) and\nthe factors that can affect it, (iii) saliency measurement and Field of View\n(FoV) prediction, and (iv) the adaptive streaming of immersive 360-degree\nvideos. The final objective of the survey is to provide an overview of the\nresearch on all the elements of an immersive video streaming system, giving the\nreader an understanding of their interplay and performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.08192v1"
    },
    {
        "title": "Frame-rate Up-conversion Detection Based on Convolutional Neural Network\n  for Learning Spatiotemporal Features",
        "authors": [
            "Minseok Yoon",
            "Seung-Hun Nam",
            "In-Jae Yu",
            "Wonhyuk Ahn",
            "Myung-Joon Kwon",
            "Heung-Kyu Lee"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  With the advance in user-friendly and powerful video editing tools, anyone\ncan easily manipulate videos without leaving prominent visual traces.\nFrame-rate up-conversion (FRUC), a representative temporal-domain operation,\nincreases the motion continuity of videos with a lower frame-rate and is used\nby malicious counterfeiters in video tampering such as generating fake\nframe-rate video without improving the quality or mixing temporally spliced\nvideos. FRUC is based on frame interpolation schemes and subtle artifacts that\nremain in interpolated frames are often difficult to distinguish. Hence,\ndetecting such forgery traces is a critical issue in video forensics. This\npaper proposes a frame-rate conversion detection network (FCDNet) that learns\nforensic features caused by FRUC in an end-to-end fashion. The proposed network\nuses a stack of consecutive frames as the input and effectively learns\ninterpolation artifacts using network blocks to learn spatiotemporal features.\nThis study is the first attempt to apply a neural network to the detection of\nFRUC. Moreover, it can cover the following three types of frame interpolation\nschemes: nearest neighbor interpolation, bilinear interpolation, and\nmotion-compensated interpolation. In contrast to existing methods that exploit\nall frames to verify integrity, the proposed approach achieves a high detection\nspeed because it observes only six frames to test its authenticity. Extensive\nexperiments were conducted with conventional forensic methods and neural\nnetworks for video forensic tasks to validate our research. The proposed\nnetwork achieved state-of-the-art performance in terms of detecting the\ninterpolated artifacts of FRUC. The experimental results also demonstrate that\nour trained model is robust for an unseen dataset, unlearned frame-rate, and\nunlearned quality factor.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.13674v1"
    },
    {
        "title": "MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning\n  Framework for Universal Non-additive Steganography",
        "authors": [
            "Xianbo Mo",
            "Shunquan Tan",
            "Bin Li",
            "Jiwu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.13689v2"
    },
    {
        "title": "Depression Diagnosis and Analysis via Multimodal Multi-order Factor\n  Fusion",
        "authors": [
            "Chengbo Yuan",
            "Qianhui Xu",
            "Yong Luo"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Depression is a leading cause of death worldwide, and the diagnosis of\ndepression is nontrivial. Multimodal learning is a popular solution for\nautomatic diagnosis of depression, and the existing works suffer two main\ndrawbacks: 1) the high-order interactions between different modalities can not\nbe well exploited; and 2) interpretability of the models are weak. To remedy\nthese drawbacks, we propose a multimodal multi-order factor fusion (MMFF)\nmethod. Our method can well exploit the high-order interactions between\ndifferent modalities by extracting and assembling modality factors under the\nguide of a shared latent proxy. We conduct extensive experiments on two recent\nand popular datasets, E-DAIC-WOZ and CMDC, and the results show that our method\nachieve significantly better performance compared with other existing\napproaches. Besides, by analyzing the process of factor assembly, our model can\nintuitively show the contribution of each factor. This helps us understand the\nfusion mechanism.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.00254v1"
    },
    {
        "title": "Ring That Bell: A Corpus and Method for Multimodal Metaphor Detection in\n  Videos",
        "authors": [
            "Khalid Alnajjar",
            "Mika Hämäläinen",
            "Shuo Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  We present the first openly available multimodal metaphor annotated corpus.\nThe corpus consists of videos including audio and subtitles that have been\nannotated by experts. Furthermore, we present a method for detecting metaphors\nin the new dataset based on the textual content of the videos. The method\nachieves a high F1-score (62\\%) for metaphorical labels. We also experiment\nwith other modalities and multimodal methods; however, these methods did not\nout-perform the text-based model. In our error analysis, we do identify that\nthere are cases where video could help in disambiguating metaphors, however,\nthe visual cues are too subtle for our model to capture. The data is available\non Zenodo.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01134v1"
    },
    {
        "title": "Dance2MIDI: Dance-driven multi-instruments music generation",
        "authors": [
            "Bo Han",
            "Yuheng Li",
            "Yixuan Shen",
            "Yi Ren",
            "Feilin Han"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Dance-driven music generation aims to generate musical pieces conditioned on\ndance videos. Previous works focus on monophonic or raw audio generation, while\nthe multi-instruments scenario is under-explored. The challenges associated\nwith the dance-driven multi-instrument music (MIDI) generation are twofold: 1)\nno publicly available multi-instruments MIDI and video paired dataset and 2)\nthe weak correlation between music and video. To tackle these challenges, we\nbuild the first multi-instruments MIDI and dance paired dataset (D2MIDI). Based\non our proposed dataset, we introduce a multi-instruments MIDI generation\nframework (Dance2MIDI) conditioned on dance video. Specifically, 1) to capture\nthe relationship between dance and music, we employ the Graph Convolutional\nNetwork to encode the dance motion. This allows us to extract features related\nto dance movement and dance style, 2) to generate a harmonious rhythm, we\nutilize a Transformer model to decode the drum track sequence, leveraging a\ncross-attention mechanism, and 3) we model the task of generating the remaining\ntracks based on the drum track as a sequence understanding and completion task.\nA BERT-like model is employed to comprehend the context of the entire music\npiece through self-supervised learning. We evaluate the generated music of our\nframework trained on the D2MIDI dataset and demonstrate that our method\nachieves State-of-the-Art performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.09080v7"
    },
    {
        "title": "Wireless Video Multicast with Cooperative and Incremental Transmission\n  of Parity Packets",
        "authors": [
            "Zhili Guo",
            "Yao Wang",
            "Elza Erkip",
            "Shivendra Panwar"
        ],
        "category": "cs.MM",
        "published_year": "2014",
        "summary": "  In this paper, a cooperative multicast scheme that uses Randomized\nDistributed Space Time Codes (R-DSTC), along with packet level Forward Error\nCorrection (FEC), is studied. Instead of sending source packets and/or parity\npackets through two hops using R-DSTC as proposed in our prior work, the new\nscheme delivers both source packets and parity packets using only one hop.\nAfter the source station (access point, AP) first sends all the source packets,\nthe AP as well as all nodes that have received all source packets together send\nthe parity packets using R-DSTC. As more parity packets are transmitted, more\nnodes can recover all source packets and join the parity packet transmission.\nThe process continues until all nodes acknowledge the receipt of enough packets\nfor recovering the source packets. For each given node distribution, the\noptimum transmission rates for source and parity packets are determined such\nthat the video rate that can be sustained at all nodes is maximized. This new\nscheme can support significantly higher video rates, and correspondingly higher\nPSNR of decoded video, than the prior approaches. Three suboptimal approaches,\nwhich do not require full information about user distribution or the feedback,\nand hence are more feasible in practice are also presented. The proposed\nsuboptimal scheme with only the node count information and without feedback\nstill outperforms our prior approach that assumes full channel information and\nno feedback.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.3674v1"
    },
    {
        "title": "Improved 8-point Approximate DCT for Image and Video Compression\n  Requiring Only 14 Additions",
        "authors": [
            "U. S. Potluri",
            "A. Madanayake",
            "R. J. Cintra",
            "F. M. Bayer",
            "S. Kulasekera",
            "A. Edirisuriya"
        ],
        "category": "cs.MM",
        "published_year": "2015",
        "summary": "  Video processing systems such as HEVC requiring low energy consumption needed\nfor the multimedia market has lead to extensive development in fast algorithms\nfor the efficient approximation of 2-D DCT transforms. The DCT is employed in a\nmultitude of compression standards due to its remarkable energy compaction\nproperties. Multiplier-free approximate DCT transforms have been proposed that\noffer superior compression performance at very low circuit complexity. Such\napproximations can be realized in digital VLSI hardware using additions and\nsubtractions only, leading to significant reductions in chip area and power\nconsumption compared to conventional DCTs and integer transforms. In this\npaper, we introduce a novel 8-point DCT approximation that requires only 14\naddition operations and no multiplications. The proposed transform possesses\nlow computational complexity and is compared to state-of-the-art DCT\napproximations in terms of both algorithm complexity and peak signal-to-noise\nratio. The proposed DCT approximation is a candidate for reconfigurable video\nstandards such as HEVC. The proposed transform and several other DCT\napproximations are mapped to systolic-array digital architectures and\nphysically realized as digital prototype circuits using FPGA technology and\nmapped to 45 nm CMOS technology.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.02995v1"
    },
    {
        "title": "Impact Analysis of Baseband Quantizer on Coding Efficiency for HDR Video",
        "authors": [
            "Chau-Wai Wong",
            "Guan-Ming Su",
            "Min Wu"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  Digitally acquired high dynamic range (HDR) video baseband signal can take 10\nto 12 bits per color channel. It is economically important to be able to reuse\nthe legacy 8 or 10-bit video codecs to efficiently compress the HDR video.\nLinear or nonlinear mapping on the intensity can be applied to the baseband\nsignal to reduce the dynamic range before the signal is sent to the codec, and\nwe refer to this range reduction step as a baseband quantization. We show\nanalytically and verify using test sequences that the use of the baseband\nquantizer lowers the coding efficiency. Experiments show that as the baseband\nquantizer is strengthened by 1.6 bits, the drop of PSNR at a high bitrate is up\nto 1.60dB. Our result suggests that in order to achieve high coding efficiency,\ninformation reduction of videos in terms of quantization error should be\nintroduced in the video codec instead of on the baseband signal.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.02980v3"
    },
    {
        "title": "Delay-aware Fountain Codes for Video Streaming with Optimal Sampling\n  Strategy",
        "authors": [
            "Kairan Sun",
            "Huazi Zhang",
            "Dapeng Wu"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The explosive demand of on-line video from smart mobile devices poses\nunprecedented challenges to delivering high quality of experience (QoE) over\nwireless networks. Streaming high-definition video with low delay is difficult\nmainly due to (i) the stochastic nature of wireless channels and (ii) the\nfluctuating videos bit rate. To address this, we propose a novel delay-aware\nfountain coding (DAF) technique that integrates channel coding and video\ncoding. In this paper, we reveal that the fluctuation of video bit rate can\nalso be exploited to further improve fountain codes for wireless video\nstreaming. Specifically, we develop two coding techniques: the time-based\nsliding window and the optimal window-wise sampling strategy. By adaptively\nselecting the window length and optimally adjusting the sampling pattern\naccording to the ongoing video bit rate, the proposed schemes deliver\nsignificantly higher video quality than existing schemes, with low delay and\nconstant data rate. To validate our design, we implement the protocols of DAF,\nDAF-L (a low-complexity version) and the existing delay-aware video streaming\nschemes by streaming H.264/AVC standard videos over an 802.11b network on CORE\nemulation platform. The results show that the decoding ratio of our scheme is\n15% to 100% higher than the state of the art techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.03236v1"
    },
    {
        "title": "Going Deeper for Multilingual Visual Sentiment Detection",
        "authors": [
            "Brendan Jou",
            "Shih-Fu Chang"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  This technical report details several improvements to the visual concept\ndetector banks built on images from the Multilingual Visual Sentiment Ontology\n(MVSO). The detector banks are trained to detect a total of 9,918\nsentiment-biased visual concepts from six major languages: English, Spanish,\nItalian, French, German and Chinese. In the original MVSO release,\nadjective-noun pair (ANP) detectors were trained for the six languages using an\nAlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through a\nmore extensive set of experiments, parameter tuning, and training runs, we\ndetail and release higher accuracy models for detecting ANPs across six\nlanguages from the same image pool and setting as in the original release using\na more modern architecture, GoogLeNet, providing comparable or better\nperformance with reduced network parameter cost.\n  In addition, since the image pool in MVSO can be corrupted by user noise from\nsocial interactions, we partitioned out a sub-corpus of MVSO images based on\ntag-restricted queries for higher fidelity labels. We show that as a result of\nthese higher fidelity labels, higher performing AlexNet-styled ANP detectors\ncan be trained using the tag-restricted image subset as compared to the models\nin full corpus. We release all these newly trained models for public research\nuse along with the list of tag-restricted images from the MVSO dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.09211v1"
    },
    {
        "title": "Putting a Face to the Voice: Fusing Audio and Visual Signals Across a\n  Video to Determine Speakers",
        "authors": [
            "Ken Hoover",
            "Sourish Chaudhuri",
            "Caroline Pantofaru",
            "Malcolm Slaney",
            "Ian Sturdy"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  In this paper, we present a system that associates faces with voices in a\nvideo by fusing information from the audio and visual signals. The thesis\nunderlying our work is that an extremely simple approach to generating (weak)\nspeech clusters can be combined with visual signals to effectively associate\nfaces and voices by aggregating statistics across a video. This approach does\nnot need any training data specific to this task and leverages the natural\ncoherence of information in the audio and visual streams. It is particularly\napplicable to tracking speakers in videos on the web where a priori information\nabout the environment (e.g., number of speakers, spatial signals for\nbeamforming) is not available. We performed experiments on a real-world dataset\nusing this analysis framework to determine the speaker in a video. Given a\nground truth labeling determined by human rater consensus, our approach had\n~71% accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00079v1"
    },
    {
        "title": "Cross-modal Common Representation Learning by Hybrid Transfer Network",
        "authors": [
            "Xin Huang",
            "Yuxin Peng",
            "Mingkuan Yuan"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  DNN-based cross-modal retrieval is a research hotspot to retrieve across\ndifferent modalities as image and text, but existing methods often face the\nchallenge of insufficient cross-modal training data. In single-modal scenario,\nsimilar problem is usually relieved by transferring knowledge from large-scale\nauxiliary datasets (as ImageNet). Knowledge from such single-modal datasets is\nalso very useful for cross-modal retrieval, which can provide rich general\nsemantic information that can be shared across different modalities. However,\nit is challenging to transfer useful knowledge from single-modal (as image)\nsource domain to cross-modal (as image/text) target domain. Knowledge in source\ndomain cannot be directly transferred to both two different modalities in\ntarget domain, and the inherent cross-modal correlation contained in target\ndomain provides key hints for cross-modal retrieval which should be preserved\nduring transfer process. This paper proposes Cross-modal Hybrid Transfer\nNetwork (CHTN) with two subnetworks: Modal-sharing transfer subnetwork utilizes\nthe modality in both source and target domains as a bridge, for transferring\nknowledge to both two modalities simultaneously; Layer-sharing correlation\nsubnetwork preserves the inherent cross-modal semantic correlation to further\nadapt to cross-modal retrieval task. Cross-modal data can be converted to\ncommon representation by CHTN for retrieval, and comprehensive experiment on 3\ndatasets shows its effectiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00153v2"
    },
    {
        "title": "Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval",
        "authors": [
            "Niluthpol Chowdhury Mithun",
            "Rameswar Panda",
            "Evangelos E. Papalexakis",
            "Amit K. Roy-Chowdhury"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Cross-modal retrieval between visual data and natural language description\nremains a long-standing challenge in multimedia. While recent image-text\nretrieval methods offer great promise by learning deep representations aligned\nacross modalities, most of these methods are plagued by the issue of training\nwith small-scale datasets covering a limited number of images with ground-truth\nsentences. Moreover, it is extremely expensive to create a larger dataset by\nannotating millions of images with sentences and may lead to a biased model.\nInspired by the recent success of webly supervised learning in deep neural\nnetworks, we capitalize on readily-available web images with noisy annotations\nto learn robust image-text joint representation. Specifically, our main idea is\nto leverage web images and corresponding tags, along with fully annotated\ndatasets, in training for learning the visual-semantic joint embedding. We\npropose a two-stage approach for the task that can augment a typical supervised\npair-wise ranking loss based formulation with weakly-annotated web images to\nlearn a more robust visual-semantic embedding. Experiments on two standard\nbenchmark datasets demonstrate that our method achieves a significant\nperformance gain in image-text retrieval compared to state-of-the-art\napproaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.07793v1"
    },
    {
        "title": "Towards Machine Learning-Based Optimal HAS",
        "authors": [
            "Christian Sieber",
            "Korbinian Hagn",
            "Christian Moldovan",
            "Tobias Hoßfeld",
            "Wolfgang Kellerer"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Mobile video consumption is increasing and sophisticated video quality\nadaptation strategies are required to deal with mobile throughput fluctuations.\nThese adaptation strategies have to keep the switching frequency low, the\naverage quality high and prevent stalling occurrences to ensure customer\nsatisfaction. This paper proposes a novel methodology for the design of machine\nlearning-based adaptation logics named HASBRAIN. Furthermore, the performance\nof a trained neural network against two algorithms from the literature is\nevaluated. We first use a modified existing optimization formulation to\ncalculate optimal adaptation paths with a minimum number of quality switches\nfor a wide range of videos and for challenging mobile throughput patterns.\nAfterwards we use the resulting optimal adaptation paths to train and compare\ndifferent machine learning models. The evaluation shows that an artificial\nneural network-based model can reach a high average quality with a low number\nof switches in the mobile scenario. The proposed methodology is general enough\nto be extended for further designs of machine learning-based algorithms and the\nprovided model can be deployed in on-demand streaming scenarios or be further\nrefined using reward-based mechanisms such as reinforcement learning. All\ntools, models and datasets created during the work are provided as open-source\nsoftware.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.08065v1"
    },
    {
        "title": "Representation Learning for Image-based Music Recommendation",
        "authors": [
            "Chih-Chun Hsia",
            "Kwei-Herng Lai",
            "Yian Chen",
            "Chuan-Ju Wang",
            "Ming-Feng Tsai"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Image perception is one of the most direct ways to provide contextual\ninformation about a user concerning his/her surrounding environment; hence\nimages are a suitable proxy for contextual recommendation. We propose a novel\nrepresentation learning framework for image-based music recommendation that\nbridges the heterogeneity gap between music and image data; the proposed method\nis a key component for various contextual recommendation tasks. Preliminary\nexperiments show that for an image-to-song retrieval task, the proposed method\nretrieves relevant or conceptually similar songs for input images.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.09198v2"
    },
    {
        "title": "FIVR: Fine-grained Incident Video Retrieval",
        "authors": [
            "Giorgos Kordopatis-Zilos",
            "Symeon Papadopoulos",
            "Ioannis Patras",
            "Ioannis Kompatsiaris"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  This paper introduces the problem of Fine-grained Incident Video Retrieval\n(FIVR). Given a query video, the objective is to retrieve all associated\nvideos, considering several types of associations that range from duplicate\nvideos to videos from the same incident. FIVR offers a single framework that\ncontains several retrieval tasks as special cases. To address the benchmarking\nneeds of all such tasks, we construct and present a large-scale annotated video\ndataset, which we call FIVR-200K, and it comprises 225,960 videos. To create\nthe dataset, we devise a process for the collection of YouTube videos based on\nmajor news events from recent years crawled from Wikipedia and deploy a\nretrieval pipeline for the automatic selection of query videos based on their\nestimated suitability as benchmarks. We also devise a protocol for the\nannotation of the dataset with respect to the four types of video associations\ndefined by FIVR. Finally, we report the results of an experimental study on the\ndataset comparing five state-of-the-art methods developed based on a variety of\nvisual descriptors, highlighting the challenges of the current problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.04094v2"
    },
    {
        "title": "Adaptive Music Composition for Games",
        "authors": [
            "Patrick Hutchings",
            "Jon McCormack"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  The generation of music that adapts dynamically to content and actions has an\nimportant role in building more immersive, memorable and emotive game\nexperiences. To date, the development of adaptive music systems for video games\nis limited by both the nature of algorithms used for real-time music generation\nand the limited modelling of player action, game world context and emotion in\ncurrent games. We propose that these issues must be addressed in tandem for the\nquality and flexibility of adaptive game music to significantly improve.\nCognitive models of knowledge organisation and emotional affect are integrated\nwith multi-modal, multi-agent composition techniques to produce a novel\nAdaptive Music System (AMS). The system is integrated into two stylistically\ndistinct games. Gamers reported an overall higher immersion and correlation of\nmusic with game-world concepts with the AMS than with the original game\nsoundtracks in both games.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.01154v1"
    },
    {
        "title": "Heard More Than Heard: An Audio Steganography Method Based on GAN",
        "authors": [
            "Dengpan Ye",
            "Shunzhi Jiang",
            "Jiaqin Huang"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Audio steganography is a collection of techniques for concealing the\nexistence of information by embedding it within a non-secret audio, which is\nreferred to as carrier. Distinct from cryptography, the steganography put\nemphasis on the hiding of the secret existence. The existing audio\nsteganography methods mainly depend on human handcraft, while we proposed an\naudio steganography algorithm which automatically generated from adversarial\ntraining. The method consists of three neural networks: encoder which embeds\nthe secret message in the carrier, decoder which extracts the message, and\ndiscriminator which determine the carriers contain secret messages. All the\nnetworks are simultaneously trained to create embedding, extracting and\ndiscriminating process. The system is trained with different training settings\non two datasets. Competed the majority of audio steganographic schemes, the\nproposed scheme could produce high fidelity steganographic audio which contains\nsecret audio. Besides, the additional experiments verify the robustness and\nsecurity of our algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.04986v1"
    },
    {
        "title": "SMP Challenge: An Overview of Social Media Prediction Challenge 2019",
        "authors": [
            "Bo Wu",
            "Wen-Huang Cheng",
            "Peiye Liu",
            "Bei Liu",
            "Zhaoyang Zeng",
            "Jiebo Luo"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  \"SMP Challenge\" aims to discover novel prediction tasks for numerous data on\nsocial multimedia and seek excellent research teams. Making predictions via\nsocial multimedia data (e.g. photos, videos or news) is not only helps us to\nmake better strategic decisions for the future, but also explores advanced\npredictive learning and analytic methods on various problems and scenarios,\nsuch as multimedia recommendation, advertising system, fashion analysis etc.\n  In the SMP Challenge at ACM Multimedia 2019, we introduce a novel prediction\ntask Temporal Popularity Prediction, which focuses on predicting future\ninteraction or attractiveness (in terms of clicks, views or likes etc.) of new\nonline posts in social media feeds before uploading. We also collected and\nreleased a large-scale SMPD benchmark with over 480K posts from 69K users. In\nthis paper, we define the challenge problem, give an overview of the dataset,\npresent statistics of rich information for data and annotation and design the\naccuracy and correlation evaluation metrics for temporal popularity prediction\nto the challenge.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.01795v2"
    },
    {
        "title": "Hate Speech in Pixels: Detection of Offensive Memes towards Automatic\n  Moderation",
        "authors": [
            "Benet Oriol Sabat",
            "Cristian Canton Ferrer",
            "Xavier Giro-i-Nieto"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  This work addresses the challenge of hate speech detection in Internet memes,\nand attempts using visual information to automatically detect hate speech,\nunlike any previous work of our knowledge. Memes are pixel-based multimedia\ndocuments that contain photos or illustrations together with phrases which,\nwhen combined, usually adopt a funny meaning. However, hate memes are also used\nto spread hate through social networks, so their automatic detection would help\nreduce their harmful societal impact. Our results indicate that the model can\nlearn to detect some of the memes, but that the task is far from being solved\nwith this simple architecture. While previous work focuses on linguistic hate\nspeech, our experiments indicate how the visual modality can be much more\ninformative for hate speech detection than the linguistic one in memes. In our\nexperiments, we built a dataset of 5,020 memes to train and evaluate a\nmulti-layer perceptron over the visual and language representations, whether\nindependently or fused. The source code and mode and models are available\nhttps://github.com/imatge-upc/hate-speech-detection .\n",
        "pdf_link": "http://arxiv.org/pdf/1910.02334v1"
    },
    {
        "title": "Multi-modal Deep Analysis for Multimedia",
        "authors": [
            "Wenwu Zhu",
            "Xin Wang",
            "Hongzhi Li"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  With the rapid development of Internet and multimedia services in the past\ndecade, a huge amount of user-generated and service provider-generated\nmultimedia data become available. These data are heterogeneous and multi-modal\nin nature, imposing great challenges for processing and analyzing them.\nMulti-modal data consist of a mixture of various types of data from different\nmodalities such as texts, images, videos, audios etc. In this article, we\npresent a deep and comprehensive overview for multi-modal analysis in\nmultimedia. We introduce two scientific research problems, data-driven\ncorrelational representation and knowledge-guided fusion for multimedia\nanalysis. To address the two scientific problems, we investigate them from the\nfollowing aspects: 1) multi-modal correlational representation: multi-modal\nfusion of data across different modalities, and 2) multi-modal data and\nknowledge fusion: multi-modal fusion of data with domain knowledge. More\nspecifically, on data-driven correlational representation, we highlight three\nimportant categories of methods, such as multi-modal deep representation,\nmulti-modal transfer learning, and multi-modal hashing. On knowledge-guided\nfusion, we discuss the approaches for fusing knowledge with data and four\nexemplar applications that require various kinds of domain knowledge, including\nmulti-modal visual question answering, multi-modal video summarization,\nmulti-modal visual pattern mining and multi-modal recommendation. Finally, we\nbring forward our insights and future research directions.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.04964v2"
    },
    {
        "title": "Hide the Image in FC-DenseNets to another Image",
        "authors": [
            "Duan Xintao",
            "Liu Nao"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  In the past, steganography was to embed text in a carrier, the sender Alice\nand the recipient Bob share the key, and the text is extracted by Bob through\nthe key. If more information is embedded, the image is easily distorted. In\ncontrast, if there is less embedded information, the image maintains good\nvisual integrity, but does not meet our requirements for steganographic\ncapacity. In this paper, we focus on tackling these challenges and limitations\nto improve steganographic capacity. An image steganography method based on\nFully Convolutional Dense Network(FC-DenseNet) was proposed by us. The hidden\nnetwork and the extracted network are trained at the same time. The dataset of\nthe deep neural network is derived from various natural images of ImageNet. The\nexperimental results show that the stego-image after steganography and the\nsecret image extracted from stego-imge have a visually good effect, and the\nstego-image has high capacity and high peak signal to noise ratio.\nImage-to-image full size hiding is implemented.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08341v1"
    },
    {
        "title": "Automatic Reminiscence Therapy for Dementia",
        "authors": [
            "Mariona Caros",
            "Maite Garolera",
            "Petia Radeva",
            "Xavier Giro-i-Nieto"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  With people living longer than ever, the number of cases with dementia such\nas Alzheimer's disease increases steadily. It affects more than 46 million\npeople worldwide, and it is estimated that in 2050 more than 100 million will\nbe affected. While there are not effective treatments for these terminal\ndiseases, therapies such as reminiscence, that stimulate memories from the past\nare recommended. Currently, reminiscence therapy takes place in care homes and\nis guided by a therapist or a carer. In this work, we present an AI-based\nsolution to automatize the reminiscence therapy, which consists in a dialogue\nsystem that uses photos as input to generate questions. We run a usability case\nstudy with patients diagnosed of mild cognitive impairment that shows they\nfound the system very entertaining and challenging. Overall, this paper\npresents how reminiscence therapy can be automatized by using machine learning,\nand deployed to smartphones and laptops, making the therapy more accessible to\nevery person affected by dementia.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.11949v2"
    },
    {
        "title": "DRST: Deep Residual Shearlet Transform for Densely Sampled Light Field\n  Reconstruction",
        "authors": [
            "Yuan Gao",
            "Robert Bregovic",
            "Reinhard Koch",
            "Atanas Gotchev"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The Image-Based Rendering (IBR) approach using Shearlet Transform (ST) is one\nof the most effective methods for Densely-Sampled Light Field (DSLF)\nreconstruction. The ST-based DSLF reconstruction typically relies on an\niterative thresholding algorithm for Epipolar-Plane Image (EPI) sparse\nregularization in shearlet domain, involving dozens of transformations between\nimage domain and shearlet domain, which are in general time-consuming. To\novercome this limitation, a novel learning-based ST approach, referred to as\nDeep Residual Shearlet Transform (DRST), is proposed in this paper.\nSpecifically, for an input sparsely-sampled EPI, DRST employs a deep fully\nConvolutional Neural Network (CNN) to predict the residuals of the shearlet\ncoefficients in shearlet domain in order to reconstruct a densely-sampled EPI\nin image domain. The DRST network is trained on synthetic Sparsely-Sampled\nLight Field (SSLF) data only by leveraging elaborately-designed masks.\nExperimental results on three challenging real-world light field evaluation\ndatasets with varying moderate disparity ranges (8 - 16 pixels) demonstrate the\nsuperiority of the proposed learning-based DRST approach over the\nnon-learning-based ST method for DSLF reconstruction. Moreover, DRST provides a\n2.4x speedup over ST, at least.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.08865v1"
    },
    {
        "title": "How deep is your encoder: an analysis of features descriptors for an\n  autoencoder-based audio-visual quality metric",
        "authors": [
            "Helard Martinez",
            "Andrew Hines",
            "Mylene C. Q. Farias"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The development of audio-visual quality assessment models poses a number of\nchallenges in order to obtain accurate predictions. One of these challenges is\nthe modelling of the complex interaction that audio and visual stimuli have and\nhow this interaction is interpreted by human users. The No-Reference\nAudio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with\nthis problem from a machine learning perspective. The metric receives two sets\nof audio and video features descriptors and produces a low-dimensional set of\nfeatures used to predict the audio-visual quality. A basic implementation of\nNAViDAd was able to produce accurate predictions tested with a range of\ndifferent audio-visual databases. The current work performs an ablation study\non the base architecture of the metric. Several modules are removed or\nre-trained using different configurations to have a better understanding of the\nmetric functionality. The results presented in this study provided important\nfeedback that allows us to understand the real capacity of the metric's\narchitecture and eventually develop a much better audio-visual quality metric.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.11100v1"
    },
    {
        "title": "Unsupervised Cross-Modal Audio Representation Learning from Unstructured\n  Multilingual Text",
        "authors": [
            "Alexander Schindler",
            "Sergiu Gordea",
            "Peter Knees"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  We present an approach to unsupervised audio representation learning. Based\non a triplet neural network architecture, we harnesses semantically related\ncross-modal information to estimate audio track-relatedness. By applying Latent\nSemantic Indexing (LSI) we embed corresponding textual information into a\nlatent vector space from which we derive track relatedness for online triplet\nselection. This LSI topic modelling facilitates fine-grained selection of\nsimilar and dissimilar audio-track pairs to learn the audio representation\nusing a Convolution Recurrent Neural Network (CRNN). By this we directly\nproject the semantic context of the unstructured text modality onto the learned\nrepresentation space of the audio modality without deriving structured\nground-truth annotations from it. We evaluate our approach on the Europeana\nSounds collection and show how to improve search in digital audio libraries by\nharnessing the multilingual meta-data provided by numerous European digital\nlibraries. We show that our approach is invariant to the variety of annotation\nstyles as well as to the different languages of this collection. The learned\nrepresentations perform comparable to the baseline of handcrafted features,\nrespectively exceeding this baseline in similarity retrieval precision at\nhigher cut-offs with only 15\\% of the baseline's feature vector length.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.12265v1"
    },
    {
        "title": "A General Approach for Using Deep Neural Network for Digital\n  Watermarking",
        "authors": [
            "Yurui Ming",
            "Weiping Ding",
            "Zehong Cao",
            "Chin-Teng Lin"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Technologies of the Internet of Things (IoT) facilitate digital contents such\nas images being acquired in a massive way. However, consideration from the\nprivacy or legislation perspective still demands the need for intellectual\ncontent protection. In this paper, we propose a general deep neural network\n(DNN) based watermarking method to fulfill this goal. Instead of training a\nneural network for protecting a specific image, we train on an image set and\nuse the trained model to protect a distinct test image set in a bulk manner.\nRespective evaluations both from the subjective and objective aspects confirm\nthe supremacy and practicability of our proposed method. To demonstrate the\nrobustness of this general neural watermarking mechanism, commonly used\nmanipulations are applied to the watermarked image to examine the corresponding\nextracted watermark, which still retains sufficient recognizable traits. To the\nbest of our knowledge, we are the first to propose a general way to perform\nwatermarking using DNN. Considering its performance and economy, it is\nconcluded that subsequent studies that generalize our work on utilizing DNN for\nintellectual content protection is a promising research trend.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.12428v1"
    },
    {
        "title": "Deep Residual Neural Networks for Image in Speech Steganography",
        "authors": [
            "Shivam Agarwal",
            "Siddarth Venkatraman"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Steganography is the art of hiding a secret message inside a publicly visible\ncarrier message. Ideally, it is done without modifying the carrier, and with\nminimal loss of information in the secret message. Recently, various deep\nlearning based approaches to steganography have been applied to different\nmessage types. We propose a deep learning based technique to hide a source RGB\nimage message inside finite length speech segments without perceptual loss. To\nachieve this, we train three neural networks; an encoding network to hide the\nmessage in the carrier, a decoding network to reconstruct the message from the\ncarrier and an additional image enhancer network to further improve the\nreconstructed message. We also discuss future improvements to the algorithm\nproposed.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13217v1"
    },
    {
        "title": "Social-Sensor Composition for Tapestry Scenes",
        "authors": [
            "Tooba Aamir",
            "Hai Dong",
            "Athman Bouguettaya"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The extensive use of social media platforms and overwhelming amounts of\nimagery data creates unique opportunities for sensing, gathering and sharing\ninformation about events. One of its potential applications is to leverage\ncrowdsourced social media images to create a tapestry scene for scene analysis\nof designated locations and time intervals. The existing attempts however\nignore the temporal-semantic relevance and spatio-temporal evolution of the\nimages and direction-oriented scene reconstruction. We propose a novel\nsocial-sensor cloud (SocSen) service composition approach to form tapestry\nscenes for scene analysis. The novelty lies in utilising images and image\nmeta-information to bypass expensive traditional image processing techniques to\nreconstruct scenes. Metadata, such as geolocation, time and angle of view of an\nimage are modelled as non-functional attributes of a SocSen service. Our major\ncontribution lies on proposing a context and direction-aware spatio-temporal\nclustering and recommendation approach for selecting a set of temporally and\nsemantically similar services to compose the best available SocSen services.\nAnalytical results based on real datasets are presented to demonstrate the\nperformance of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13684v1"
    },
    {
        "title": "Direct Speech-to-image Translation",
        "authors": [
            "Jiguo Li",
            "Xinfeng Zhang",
            "Chuanmin Jia",
            "Jizheng Xu",
            "Li Zhang",
            "Yue Wang",
            "Siwei Ma",
            "Wen Gao"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Direct speech-to-image translation without text is an interesting and useful\ntopic due to the potential applications in human-computer interaction, art\ncreation, computer-aided design. etc. Not to mention that many languages have\nno writing form. However, as far as we know, it has not been well-studied how\nto translate the speech signals into images directly and how well they can be\ntranslated. In this paper, we attempt to translate the speech signals into the\nimage signals without the transcription stage. Specifically, a speech encoder\nis designed to represent the input speech signals as an embedding feature, and\nit is trained with a pretrained image encoder using teacher-student learning to\nobtain better generalization ability on new classes. Subsequently, a stacked\ngenerative adversarial network is used to synthesize high-quality images\nconditioned on the embedding feature. Experimental results on both synthesized\nand real data show that our proposed method is effective to translate the raw\nspeech signals into images without the middle text representation. Ablation\nstudy gives more insights about our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.03413v2"
    },
    {
        "title": "MIDI-Sheet Music Alignment Using Bootleg Score Synthesis",
        "authors": [
            "Thitaree Tanprasert",
            "Teerapat Jenrungrot",
            "Meinard Mueller",
            "T. J. Tsai"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  MIDI-sheet music alignment is the task of finding correspondences between a\nMIDI representation of a piece and its corresponding sheet music images. Rather\nthan using optical music recognition to bridge the gap between sheet music and\nMIDI, we explore an alternative approach: projecting the MIDI data into pixel\nspace and performing alignment in the image domain. Our method converts the\nMIDI data into a crude representation of the score that only contains\nrectangular floating notehead blobs, a process we call bootleg score synthesis.\nFurthermore, we project sheet music images into the same bootleg space by\napplying a deep watershed notehead detector and filling in the bounding boxes\naround each detected notehead. Finally, we align the bootleg representations\nusing a simple variant of dynamic time warping. On a dataset of 68 real scanned\npiano scores from IMSLP and corresponding MIDI performances, our method\nachieves a 97.3% accuracy at an error tolerance of one second, outperforming\nseveral baseline systems that employ optical music recognition.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.10345v1"
    },
    {
        "title": "MIDI Passage Retrieval Using Cell Phone Pictures of Sheet Music",
        "authors": [
            "Daniel Yang",
            "Thitaree Tanprasert",
            "Teerapat Jenrungrot",
            "Mengyi Shan",
            "TJ Tsai"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper investigates a cross-modal retrieval problem in which a user would\nlike to retrieve a passage of music from a MIDI file by taking a cell phone\npicture of a physical page of sheet music. While audio-sheet music retrieval\nhas been explored by a number of works, this scenario is novel in that the\nquery is a cell phone picture rather than a digital scan. To solve this\nproblem, we introduce a mid-level feature representation called a bootleg score\nwhich explicitly encodes the rules of Western musical notation. We convert both\nthe MIDI and the sheet music into bootleg scores using deterministic rules of\nmusic and classical computer vision techniques for detecting simple geometric\nshapes. Once the MIDI and cell phone image have been converted into bootleg\nscores, we estimate the alignment using dynamic programming. The most notable\ncharacteristic of our system is that it does test-time adaptation and has no\ntrainable weights at all -- only a set of about 30 hyperparameters. On a\ndataset containing 1000 cell phone pictures taken of 100 scores of classical\npiano music, our system achieves an F measure score of .869 and outperforms\nbaseline systems based on commercial optical music recognition software.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.10347v1"
    },
    {
        "title": "Upgrading the Newsroom: An Automated Image Selection System for News\n  Articles",
        "authors": [
            "Fangyu Liu",
            "Rémi Lebret",
            "Didier Orel",
            "Philippe Sordet",
            "Karl Aberer"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  We propose an automated image selection system to assist photo editors in\nselecting suitable images for news articles. The system fuses multiple textual\nsources extracted from news articles and accepts multilingual inputs. It is\nequipped with char-level word embeddings to help both modeling morphologically\nrich languages, e.g. German, and transferring knowledge across nearby\nlanguages. The text encoder adopts a hierarchical self-attention mechanism to\nattend more to both keywords within a piece of text and informative components\nof a news article. We extensively experiment with our system on a large-scale\ntext-image database containing multimodal multilingual news articles collected\nfrom Swiss local news media websites. The system is compared with multiple\nbaselines with ablation studies and is shown to beat existing text-image\nretrieval methods in a weakly-supervised learning setting. Besides, we also\noffer insights on the advantage of using multiple textual sources and\nmultilingual data.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11449v1"
    },
    {
        "title": "Using Cell Phone Pictures of Sheet Music To Retrieve MIDI Passages",
        "authors": [
            "TJ Tsai",
            "Daniel Yang",
            "Mengyi Shan",
            "Thitaree Tanprasert",
            "Teerapat Jenrungrot"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This article investigates a cross-modal retrieval problem in which a user\nwould like to retrieve a passage of music from a MIDI file by taking a cell\nphone picture of several lines of sheet music. This problem is challenging for\ntwo reasons: it has a significant runtime constraint since it is a user-facing\napplication, and there is very little relevant training data containing cell\nphone images of sheet music. To solve this problem, we introduce a novel\nfeature representation called a bootleg score which encodes the position of\nnoteheads relative to staff lines in sheet music. The MIDI representation can\nbe converted into a bootleg score using deterministic rules of Western musical\nnotation, and the sheet music image can be converted into a bootleg score using\nclassical computer vision techniques for detecting simple geometrical shapes.\nOnce the MIDI and cell phone image have been converted into bootleg scores, we\ncan estimate the alignment using dynamic programming. The most notable\ncharacteristic of our system is that it has no trainable weights at all -- only\na set of about 40 hyperparameters. With a training set of just 400 images, we\nshow that our system generalizes well to a much larger set of 1600 test images\nfrom 160 unseen musical scores. Our system achieves a test F measure score of\n0.89, has an average runtime of 0.90 seconds, and outperforms baseline systems\nbased on music object detection and sheet-audio alignment. We provide extensive\nexperimental validation and analysis of our system.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11724v1"
    },
    {
        "title": "DWT-GBT-SVD-based Robust Speech Steganography",
        "authors": [
            "Noshin Amiri",
            "Iman Naderi"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Steganography is a method that can improve network security and make\ncommunications safer. In this method, a secret message is hidden in content\nlike audio signals that should not be perceptible by listening to the audio or\nseeing the signal waves. Also, it should be robust against different common\nattacks such as noise and compression. In this paper, we propose a new speech\nsteganography method based on a combination of Discrete Wavelet Transform,\nGraph-based Transform, and Singular Value Decomposition (SVD). In this method,\nwe first find voiced frames based on energy and zero-crossing counts of the\nframes and then embed a binary message into voiced frames. Experimental results\non the NOIZEUS database show that the proposed method is imperceptible and also\nrobust against Gaussian noise, re-sampling, re-quantization, high pass filter,\nand low pass filter. Also, it is robust against MP3 compression and scaling for\nwatermarking applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12569v1"
    },
    {
        "title": "Deep Convolutional Neural Network for Identifying Seam-Carving Forgery",
        "authors": [
            "Seung-Hun Nam",
            "Wonhyuk Ahn",
            "In-Jae Yu",
            "Myung-Joon Kwon",
            "Minseok Son",
            "Heung-Kyu Lee"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Seam carving is a representative content-aware image retargeting approach to\nadjust the size of an image while preserving its visually prominent content. To\nmaintain visually important content, seam-carving algorithms first calculate\nthe connected path of pixels, referred to as the seam, according to a defined\ncost function and then adjust the size of an image by removing and duplicating\nrepeatedly calculated seams. Seam carving is actively exploited to overcome\ndiversity in the resolution of images between applications and devices; hence,\ndetecting the distortion caused by seam carving has become important in image\nforensics. In this paper, we propose a convolutional neural network (CNN)-based\napproach to classifying seam-carving-based image retargeting for reduction and\nexpansion. To attain the ability to learn low-level features, we designed a CNN\narchitecture comprising five types of network blocks specialized for capturing\nsubtle signals. An ensemble module is further adopted to both enhance\nperformance and comprehensively analyze the features in the local areas of the\ngiven image. To validate the effectiveness of our work, extensive experiments\nbased on various CNN-based baselines were conducted. Compared to the baselines,\nour work exhibits state-of-the-art performance in terms of three-class\nclassification (original, seam inserted, and seam removed). In addition, our\nmodel with the ensemble module is robust for various unseen cases. The\nexperimental results also demonstrate that our method can be applied to\nlocalize both seam-removed and seam-inserted areas.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.02393v2"
    },
    {
        "title": "Subjective and Objective Quality Assessment of High Frame Rate Videos",
        "authors": [
            "Pavan C. Madhusudana",
            "Xiangxu Yu",
            "Neil Birkbeck",
            "Yilin Wang",
            "Balu Adsumilli",
            "Alan C. Bovik"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  High frame rate (HFR) videos are becoming increasingly common with the\ntremendous popularity of live, high-action streaming content such as sports.\nAlthough HFR contents are generally of very high quality, high bandwidth\nrequirements make them challenging to deliver efficiently, while simultaneously\nmaintaining their quality. To optimize trade-offs between bandwidth\nrequirements and video quality, in terms of frame rate adaptation, it is\nimperative to understand the intricate relationship between frame rate and\nperceptual video quality. Towards advancing progression in this direction we\ndesigned a new subjective resource, called the LIVE-YouTube-HFR (LIVE-YT-HFR)\ndataset, which is comprised of 480 videos having 6 different frame rates,\nobtained from 16 diverse contents. In order to understand the combined effects\nof compression and frame rate adjustment, we also processed videos at 5\ncompression levels at each frame rate. To obtain subjective labels on the\nvideos, we conducted a human study yielding 19,000 human quality ratings\nobtained from a pool of 85 human subjects. We also conducted a holistic\nevaluation of existing state-of-the-art Full and No-Reference video quality\nalgorithms, and statistically benchmarked their performance on the new\ndatabase. The LIVE-YT-HFR database has been made available online for public\nuse and evaluation purposes, with hopes that it will help advance research in\nthis exciting video technology direction. It may be obtained at\n\\url{https://live.ece.utexas.edu/research/LIVE_YT_HFR/LIVE_YT_HFR/index.html}\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11634v2"
    },
    {
        "title": "Kalman Filter-based Head Motion Prediction for Cloud-based Mixed Reality",
        "authors": [
            "Serhan Gül",
            "Sebastian Bosse",
            "Dimitri Podborski",
            "Thomas Schierl",
            "Cornelius Hellge"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Volumetric video allows viewers to experience highly-realistic 3D content\nwith six degrees of freedom in mixed reality (MR) environments. Rendering\ncomplex volumetric videos can require a prohibitively high amount of\ncomputational power for mobile devices. A promising technique to reduce the\ncomputational burden on mobile devices is to perform the rendering at a cloud\nserver. However, cloud-based rendering systems suffer from an increased\ninteraction (motion-to-photon) latency that may cause registration errors in MR\nenvironments. One way of reducing the effective latency is to predict the\nviewer's head pose and render the corresponding view from the volumetric video\nin advance. In this paper, we design a Kalman filter for head motion prediction\nin our cloud-based volumetric video streaming system. We analyze the\nperformance of our approach using recorded head motion traces and compare its\nperformance to an autoregression model for different prediction intervals\n(look-ahead times). Our results show that the Kalman filter can predict head\norientations 0.5 degrees more accurately than the autoregression model for a\nlook-ahead time of 60 ms.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.14084v1"
    },
    {
        "title": "Improved Handling of Repeats and Jumps in Audio-Sheet Image\n  Synchronization",
        "authors": [
            "Mengyi Shan",
            "TJ Tsai"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper studies the problem of automatically generating piano score\nfollowing videos given an audio recording and raw sheet music images. Whereas\nprevious works focus on synthetic sheet music where the data has been cleaned\nand preprocessed, we instead focus on developing a system that can cope with\nthe messiness of raw, unprocessed sheet music PDFs from IMSLP. We investigate\nhow well existing systems cope with real scanned sheet music, filler pages and\nunrelated pieces or movements, and discontinuities due to jumps and repeats. We\nfind that a significant bottleneck in system performance is handling jumps and\nrepeats correctly. In particular, we find that a previously proposed Jump DTW\nalgorithm does not perform robustly when jump locations are unknown a priori.\nWe propose a novel alignment algorithm called Hierarchical DTW that can handle\njumps and repeats even when jump locations are not known. It first performs\nalignment at the feature level on each sheet music line, and then performs a\nsecond alignment at the segment level. By operating at the segment level, it is\nable to encode domain knowledge about how likely a particular jump is. Through\ncarefully controlled experiments on unprocessed sheet music PDFs from IMSLP, we\nshow that Hierarachical DTW significantly outperforms Jump DTW in handling\nvarious types of jumps.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.14580v1"
    },
    {
        "title": "Revenue and Energy Efficiency-Driven Delay Constrained Computing Task\n  Offloading and Resource Allocation in a Vehicular Edge Computing Network: A\n  Deep Reinforcement Learning Approach",
        "authors": [
            "Xinyu Huang",
            "Lijun He",
            "Xing Chen",
            "Liejun Wang",
            "Fan Li"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  For in-vehicle application,task type and vehicle state information, i.e.,\nvehicle speed, bear a significant impact on the task delay requirement.\nHowever, the joint impact of task type and vehicle speed on the task delay\nconstraint has not been studied, and this lack of study may cause a mismatch\nbetween the requirement of the task delay and allocated computation and\nwireless resources. In this paper, we propose a joint task type and vehicle\nspeed-aware task offloading and resource allocation strategy to decrease the\nvehicl's energy cost for executing tasks and increase the revenue of the\nvehicle for processing tasks within the delay constraint. First, we establish\nthe joint task type and vehicle speed-aware delay constraint model. Then, the\ndelay, energy cost and revenue for task execution in the vehicular edge\ncomputing (VEC) server, local terminal and terminals of other vehicles are\ncalculated. Based on the energy cost and revenue from task execution,the\nutility function of the vehicle is acquired. Next, we formulate a joint\noptimization of task offloading and resource allocation to maximize the utility\nlevel of the vehicles subject to the constraints of task delay, computation\nresources and wireless resources. To obtain a near-optimal solution of the\nformulated problem, a joint offloading and resource allocation based on the\nmulti-agent deep deterministic policy gradient (JORA-MADDPG) algorithm is\nproposed to maximize the utility level of vehicles. Simulation results show\nthat our algorithm can achieve superior performance in task completion delay,\nvehicles' energy cost and processing revenue.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08119v1"
    },
    {
        "title": "Ensemble Chinese End-to-End Spoken Language Understanding for Abnormal\n  Event Detection from audio stream",
        "authors": [
            "Haoran Wei",
            "Fei Tao",
            "Runze Su",
            "Sen Yang",
            "Ji Liu"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Conventional spoken language understanding (SLU) consist of two stages, the\nfirst stage maps speech to text by automatic speech recognition (ASR), and the\nsecond stage maps text to intent by natural language understanding (NLU).\nEnd-to-end SLU maps speech directly to intent through a single deep learning\nmodel. Previous end-to-end SLU models are primarily used for English\nenvironment due to lacking large scale SLU dataset in Chines, and use only one\nASR model to extract features from speech. With the help of Kuaishou\ntechnology, a large scale SLU dataset in Chinese is collected to detect\nabnormal event in their live audio stream. Based on this dataset, this paper\nproposed a ensemble end-to-end SLU model used for Chinese environment. This\nensemble SLU models extracted hierarchies features using multiple pre-trained\nASR models, leading to better representation of phoneme level and word level\ninformation. This proposed approached achieve 9.7% increase of accuracy\ncompared to previous end-to-end SLU model.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.09235v2"
    },
    {
        "title": "Effect of Language Proficiency on Subjective Evaluation of Noise\n  Suppression Algorithms",
        "authors": [
            "Babak Naderi",
            "Gabriel Mittag",
            "Rafael Zequeira Jim\\a'enez",
            "Sebastian Möller"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Speech communication systems based on Voice-over-IP technology are frequently\nused by native as well as non-native speakers of a target language, e.g. in\ninternational phone calls or telemeetings. Frequently, such calls also occur in\na noisy environment, making noise suppression modules necessary to increase\nperceived quality of experience. Whereas standard tests for assessing perceived\nquality make use of native listeners, we assume that noise-reduced speech and\nresidual noise may affect native and non-native listeners of a target language\nin different ways. To test this assumption, we report results of two subjective\ntests conducted with English and German native listeners who judge the quality\nof speech samples recorded by native English, German, and Mandarin speakers,\nwhich are degraded with different background noise levels and noise suppression\neffects. The experiments were conducted following the standardized ITU-T Rec.\nP.835 approach, however implemented in a crowdsourcing setting according to\nITU-T Rec. P.808. Our results show a significant influence of language on\nspeech signal ratings and, consequently, on the overall perceived quality in\nspecific conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.13260v1"
    },
    {
        "title": "ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate\n  Dependent Video Quality Prediction",
        "authors": [
            "Pavan C. Madhusudana",
            "Neil Birkbeck",
            "Yilin Wang",
            "Balu Adsumilli",
            "Alan C. Bovik"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  We consider the problem of conducting frame rate dependent video quality\nassessment (VQA) on videos of diverse frame rates, including high frame rate\n(HFR) videos. More generally, we study how perceptual quality is affected by\nframe rate, and how frame rate and compression combine to affect perceived\nquality. We devise an objective VQA model called Space-Time GeneRalized\nEntropic Difference (GREED) which analyzes the statistics of spatial and\ntemporal band-pass video coefficients. A generalized Gaussian distribution\n(GGD) is used to model band-pass responses, while entropy variations between\nreference and distorted videos under the GGD model are used to capture video\nquality variations arising from frame rate changes. The entropic differences\nare calculated across multiple temporal and spatial subbands, and merged using\na learned regressor. We show through extensive experiments that GREED achieves\nstate-of-the-art performance on the LIVE-YT-HFR Database when compared with\nexisting VQA models. The features used in GREED are highly generalizable and\nobtain competitive performance even on standard, non-HFR VQA databases. The\nimplementation of GREED has been made available online:\nhttps://github.com/pavancm/GREED\n",
        "pdf_link": "http://arxiv.org/pdf/2010.13715v2"
    },
    {
        "title": "MEG: Multi-Evidence GNN for Multimodal Semantic Forensics",
        "authors": [
            "Ekraam Sabir",
            "Ayush Jaiswal",
            "Wael AbdAlmageed",
            "Prem Natarajan"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Fake news often involves semantic manipulations across modalities such as\nimage, text, location etc and requires the development of multimodal semantic\nforensics for its detection. Recent research has centered the problem around\nimages, calling it image repurposing -- where a digitally unmanipulated image\nis semantically misrepresented by means of its accompanying multimodal metadata\nsuch as captions, location, etc. The image and metadata together comprise a\nmultimedia package. The problem setup requires algorithms to perform multimodal\nsemantic forensics to authenticate a query multimedia package using a reference\ndataset of potentially related packages as evidences. Existing methods are\nlimited to using a single evidence (retrieved package), which ignores potential\nperformance improvement from the use of multiple evidences. In this work, we\nintroduce a novel graph neural network based model for multimodal semantic\nforensics, which effectively utilizes multiple retrieved packages as evidences\nand is scalable with the number of evidences. We compare the scalability and\nperformance of our model against existing methods. Experimental results show\nthat the proposed model outperforms existing state-of-the-art algorithms with\nan error reduction of up to 25%.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.11286v1"
    },
    {
        "title": "SoMin.ai: Personality-Driven Content Generation Platform",
        "authors": [
            "Aleksandr Farseev",
            "Qi Yang",
            "Andrey Filchenkov",
            "Kirill Lepikhin",
            "Yu-Yi Chu-Farseeva",
            "Daron-Benjamin Loo"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  In this technical demonstration, we showcase the World's first\npersonality-driven marketing content generation platform, called SoMin.ai. The\nplatform combines deep multi-view personality profiling framework and style\ngenerative adversarial networks facilitating the automatic creation of content\nthat appeals to different human personality types. The platform can be used for\nthe enhancement of the social networking user experience as well as for content\nmarketing routines. Guided by the MBTI personality type, automatically derived\nfrom a user social network content, SoMin.ai generates new social media content\nbased on the preferences of other users with a similar personality type aiming\nat enhancing the user experience on social networking venues as well\ndiversifying the efforts of marketers when crafting new content for digital\nmarketing campaigns. The real-time user feedback to the platform via the\nplatform's GUI fine-tunes the content generation model and the evaluation\nresults demonstrate the promising performance of the proposed multi-view\npersonality profiling framework when being applied in the content generation\nscenario. By leveraging content generation at a large scale, marketers will be\nable to execute more effective digital marketing campaigns at a lower cost.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.14615v2"
    },
    {
        "title": "End-to-End Video Question-Answer Generation with Generator-Pretester\n  Network",
        "authors": [
            "Hung-Ting Su",
            "Chen-Hsi Chang",
            "Po-Wei Shen",
            "Yu-Siang Wang",
            "Ya-Liang Chang",
            "Yu-Cheng Chang",
            "Pu-Jen Cheng",
            "Winston H. Hsu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We study a novel task, Video Question-Answer Generation (VQAG), for\nchallenging Video Question Answering (Video QA) task in multimedia. Due to\nexpensive data annotation costs, many widely used, large-scale Video QA\ndatasets such as Video-QA, MSVD-QA and MSRVTT-QA are automatically annotated\nusing Caption Question Generation (CapQG) which inputs captions instead of the\nvideo itself. As captions neither fully represent a video, nor are they always\npractically available, it is crucial to generate question-answer pairs based on\na video via Video Question-Answer Generation (VQAG). Existing video-to-text\n(V2T) approaches, despite taking a video as the input, only generate a question\nalone. In this work, we propose a novel model Generator-Pretester Network that\nfocuses on two components: (1) The Joint Question-Answer Generator (JQAG) which\ngenerates a question with its corresponding answer to allow Video Question\n\"Answering\" training. (2) The Pretester (PT) verifies a generated question by\ntrying to answer it and checks the pretested answer with both the model's\nproposed answer and the ground truth answer. We evaluate our system with the\nonly two available large-scale human-annotated Video QA datasets and achieves\nstate-of-the-art question generation performances. Furthermore, using our\ngenerated QA pairs only on the Video QA task, we can surpass some supervised\nbaselines. We apply our generated questions to Video QA applications and\nsurpasses some supervised baselines using generated questions only. As a\npre-training strategy, we outperform both CapQG and transfer learning\napproaches when employing semi-supervised (20%) or fully supervised learning\nwith annotated data. These experimental results suggest the novel perspectives\nfor Video QA training.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.01447v1"
    },
    {
        "title": "Designing a mobile game to generate player data -- lessons learned",
        "authors": [
            "William Wallis",
            "William Kavanagh",
            "Alice Miller",
            "Tim Storer"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  User friendly tools have lowered the requirements of high-quality game design\nto the point where researchers without development experience can release their\nown games. However, there is no established best-practice as few games have\nbeen produced for research purposes. Having developed a mobile game without the\nguidance of similar projects, we realised the need to share our experience so\nfuture researchers have a path to follow. Research into game balancing and\nsystem simulation required an experimental case study, which inspired the\ncreation of \"RPGLite\", a multiplayer mobile game. In creating RPGLitewith no\ndevelopment expertise we learned a series of lessons about effective amateur\ngame development for research purposes. In this paper we reflect on the entire\ndevelopment process and present these lessons.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.07144v1"
    },
    {
        "title": "Latent Factor Modeling of Users Subjective Perception for Stereoscopic\n  3D Video Recommendation",
        "authors": [
            "Balasubramanyam Appina",
            "Mansi Sharma",
            "Santosh Kumar"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Numerous stereoscopic 3D movies are released every year to theaters and\ncreated large revenues. Despite the improvement in stereo capturing and 3D\nvideo post-production technology, stereoscopic artifacts which cause viewer\ndiscomfort continue to appear even in high-budget films. Existing automatic 3D\nvideo quality measurement tools can detect distortions in stereoscopic images\nor videos, but they fail to consider the viewer's subjective perception of\nthose artifacts, and how these distortions affect their choices. In this paper,\nwe introduce a novel recommendation system for stereoscopic 3D movies based on\na latent factor model that meticulously analyse the viewer's subjective ratings\nand influence of 3D video distortions on their preferences. To the best of our\nknowledge, this is a first-of-its-kind model that recommends 3D movies based on\nstereo-film quality ratings accounting correlation between the viewer's visual\ndiscomfort and stereoscopic-artifact perception. The proposed model is trained\nand tested on benchmark Nama3ds1-cospad1 and LFOVIAS3DPh2 S3D video quality\nassessment datasets. The experiments revealed that resulting\nmatrix-factorization based recommendation system is able to generalize\nconsiderably better for the viewer's subjective ratings.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.10039v1"
    },
    {
        "title": "Exploring Deep Learning for Joint Audio-Visual Lip Biometrics",
        "authors": [
            "Meng Liu",
            "Longbiao Wang",
            "Kong Aik Lee",
            "Hanyi Zhang",
            "Chang Zeng",
            "Jianwu Dang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Audio-visual (AV) lip biometrics is a promising authentication technique that\nleverages the benefits of both the audio and visual modalities in speech\ncommunication. Previous works have demonstrated the usefulness of AV lip\nbiometrics. However, the lack of a sizeable AV database hinders the exploration\nof deep-learning-based audio-visual lip biometrics. To address this problem, we\ncompile a moderate-size database using existing public databases. Meanwhile, we\nestablish the DeepLip AV lip biometrics system realized with a convolutional\nneural network (CNN) based video module, a time-delay neural network (TDNN)\nbased audio module, and a multimodal fusion module. Our experiments show that\nDeepLip outperforms traditional speaker recognition models in context modeling\nand achieves over 50% relative improvements compared with our best single\nmodality baseline, with an equal error rate of 0.75% and 1.11% on the test\ndatasets, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.08510v1"
    },
    {
        "title": "Detection of Audio-Video Synchronization Errors Via Event Detection",
        "authors": [
            "Joshua P. Ebenezer",
            "Yongjun Wu",
            "Hai Wei",
            "Sriram Sethuraman",
            "Zongyi Liu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We present a new method and a large-scale database to detect audio-video\nsynchronization(A/V sync) errors in tennis videos. A deep network is trained to\ndetect the visual signature of the tennis ball being hit by the racquet in the\nvideo stream. Another deep network is trained to detect the auditory signature\nof the same event in the audio stream. During evaluation, the audio stream is\nsearched by the audio network for the audio event of the ball being hit. If the\nevent is found in audio, the neighboring interval in video is searched for the\ncorresponding visual signature. If the event is not found in the video stream\nbut is found in the audio stream, A/V sync error is flagged. We developed a\nlarge-scaled database of 504,300 frames from 6 hours of videos of tennis\nevents, simulated A/V sync errors, and found our method achieves high accuracy\non the task.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.10116v1"
    },
    {
        "title": "The Influence of Audio on Video Memorability with an Audio Gestalt\n  Regulated Video Memorability System",
        "authors": [
            "Lorin Sweeney",
            "Graham Healy",
            "Alan F. Smeaton"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Memories are the tethering threads that tie us to the world, and memorability\nis the measure of their tensile strength. The threads of memory are spun from\nfibres of many modalities, obscuring the contribution of a single fibre to a\nthread's overall tensile strength. Unfurling these fibres is the key to\nunderstanding the nature of their interaction, and how we can ultimately create\nmore meaningful media content. In this paper, we examine the influence of audio\non video recognition memorability, finding evidence to suggest that it can\nfacilitate overall video recognition memorability rich in high-level (gestalt)\naudio features. We introduce a novel multimodal deep learning-based late-fusion\nsystem that uses audio gestalt to estimate the influence of a given video's\naudio on its overall short-term recognition memorability, and selectively\nleverages audio features to make a prediction accordingly. We benchmark our\naudio gestalt based system on the Memento10k short-term video memorability\ndataset, achieving top-2 state-of-the-art results.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.11568v1"
    },
    {
        "title": "Spatial Privacy-aware VR streaming",
        "authors": [
            "Xing Wei",
            "Chenyang Yang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Proactive tile-based virtual reality (VR) video streaming employs the current\ntracking data of a user to predict future requested tiles, then renders and\ndelivers the predicted tiles before playback. Very recently, privacy protection\nin proactive VR video streaming starts to raise concerns. However, existing\nprivacy protection may fail even with privacy-preserve federated learning. This\nis because when the future requested tiles can be predicted accurately, the\nuser-behavior-related data can still be recovered from the predicted tiles. In\nthis paper, we consider how to protect privacy even with accurate predictors\nand investigate the impact of privacy requirement on the quality of experience\n(QoE). To this end, we first add extra \\textit{camouflaged} tile requests to\nthe real tile requests and model the privacy requirement as the \\textit{spatial\ndegree of privacy} (sDoP). By ensuring sDoP, the real tile requests can be\nhidden and privacy can be protected. Then, we jointly optimize the durations\nfor prediction, computing, and transmitting, aimed at maximizing the\nprivacy-aware QoE given arbitrary predictor and configured resources. From the\nobtained optimal closed-form solution, we find that the impacts of sDoP on the\nQoE are two sides of the same coin. On the one side the increase of sDoP\nimproves the capability of communication and computing hence improves QoE. On\nthe other side it degrades the prediction performance hence degrades the QoE.\nThe overall impact depends on which factor dominates the QoE. Simulation with\ntwo predictors on a real dataset verifies the analysis and shows that the\noverall impact of sDoP is to improve the QoE.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14170v2"
    },
    {
        "title": "Naturalistic audio-visual volumetric sequences dataset of sounding\n  actions for six degree-of-freedom interaction",
        "authors": [
            "Hanne Stenzel",
            "Davide Berghi",
            "Marco Volino",
            "Philip J. B. Jackson"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  As audio-visual systems increasingly bring immersive and interactive\ncapabilities into our work and leisure activities, so the need for naturalistic\ntest material grows. New volumetric datasets have captured high-quality 3D\nvideo, but accompanying audio is often neglected, making it hard to test an\nintegrated bimodal experience. Designed to cover diverse sound types and\nfeatures, the presented volumetric dataset was constructed from audio and video\nstudio recordings of scenes to yield forty short action sequences. Potential\nuses in technical and scientific tests are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.00641v1"
    },
    {
        "title": "Generic Reversible Visible Watermarking Via Regularized Graph Fourier\n  Transform Coding",
        "authors": [
            "Wenfa Qi",
            "Sirui Guo",
            "Wei Hu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Reversible visible watermarking (RVW) is an active copyright protection\nmechanism. It not only transparently superimposes copyright patterns on\nspecific positions of digital images or video frames to declare the copyright\nownership information, but also completely erases the visible watermark image\nand thus enables restoring the original host image without any distortion.\nHowever, existing RVW algorithms mostly construct the reversible mapping\nmechanism for a specific visible watermarking scheme, which is not versatile.\nHence, we propose a generic RVW framework to accommodate various visible\nwatermarking schemes. In particular, we obtain a reconstruction data packet --\nthe compressed difference image between the watermarked image and the original\nhost image, which is embedded into the watermarked image via any conventional\nreversible data hiding method to facilitate the blind recovery of the host\nimage. The key is to achieve compact compression of the difference image for\nefficient embedding of the reconstruction data packet. To this end, we propose\nregularized Graph Fourier Transform (GFT) coding, where the difference image is\nsmoothed via the graph Laplacian regularizer for more efficient compression and\nthen encoded by multi-resolution GFTs in an approximately optimal manner.\nExperimental results show that the proposed framework has much better\nversatility than state-of-the-art methods. Due to the small amount of auxiliary\ninformation to be embedded, the visual quality of the watermarked image is also\nhigher.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.08350v2"
    },
    {
        "title": "SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and\n  Images",
        "authors": [
            "Dimitar Dimitrov",
            "Bishr Bin Ali",
            "Shaden Shaar",
            "Firoj Alam",
            "Fabrizio Silvestri",
            "Hamed Firooz",
            "Preslav Nakov",
            "Giovanni Da San Martino"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in\nTexts and Images: the data, the annotation guidelines, the evaluation setup,\nthe results, and the participating systems. The task focused on memes and had\nthree subtasks: (i) detecting the techniques in the text, (ii) detecting the\ntext spans where the techniques are used, and (iii) detecting techniques in the\nentire meme, i.e., both in the text and in the image. It was a popular task,\nattracting 71 registrations, and 22 teams that eventually made an official\nsubmission on the test set. The evaluation results for the third subtask\nconfirmed the importance of both modalities, the text and the image. Moreover,\nsome teams reported benefits when not just combining the two modalities, e.g.,\nby using early or late fusion, but rather modeling the interaction between them\nin a joint model.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.09284v1"
    },
    {
        "title": "Deep Learning for Predictive Analytics in Reversible Steganography",
        "authors": [
            "Ching-Chun Chang",
            "Xu Wang",
            "Sisheng Chen",
            "Isao Echizen",
            "Victor Sanchez",
            "Chang-Tsun Li"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Deep learning is regarded as a promising solution for reversible\nsteganography. There is an accelerating trend of representing a reversible\nsteo-system by monolithic neural networks, which bypass intermediate operations\nin traditional pipelines of reversible steganography. This end-to-end paradigm,\nhowever, suffers from imperfect reversibility. By contrast, the modular\nparadigm that incorporates neural networks into modules of traditional\npipelines can stably guarantee reversibility with mathematical explainability.\nPrediction-error modulation is a well-established reversible steganography\npipeline for digital images. It consists of a predictive analytics module and a\nreversible coding module. Given that reversibility is governed independently by\nthe coding module, we narrow our focus to the incorporation of neural networks\ninto the analytics module, which serves the purpose of predicting pixel\nintensities and a pivotal role in determining capacity and imperceptibility.\nThe objective of this study is to evaluate the impacts of different training\nconfigurations upon predictive accuracy of neural networks and provide\npractical insights. In particular, we investigate how different initialisation\nstrategies for input images may affect the learning process and how different\ntraining strategies for dual-layer prediction respond to the problem of\ndistributional shift. Furthermore, we compare steganographic performance of\nvarious model architectures with different loss functions.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.06924v3"
    },
    {
        "title": "Detect and remove watermark in deep neural networks via generative\n  adversarial networks",
        "authors": [
            "Haoqi Wang",
            "Mingfu Xue",
            "Shichang Sun",
            "Yushu Zhang",
            "Jian Wang",
            "Weiqiang Liu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Deep neural networks (DNN) have achieved remarkable performance in various\nfields. However, training a DNN model from scratch requires a lot of computing\nresources and training data. It is difficult for most individual users to\nobtain such computing resources and training data. Model copyright infringement\nis an emerging problem in recent years. For instance, pre-trained models may be\nstolen or abuse by illegal users without the authorization of the model owner.\nRecently, many works on protecting the intellectual property of DNN models have\nbeen proposed. In these works, embedding watermarks into DNN based on backdoor\nis one of the widely used methods. However, when the DNN model is stolen, the\nbackdoor-based watermark may face the risk of being detected and removed by an\nadversary. In this paper, we propose a scheme to detect and remove watermark in\ndeep neural networks via generative adversarial networks (GAN). We demonstrate\nthat the backdoor-based DNN watermarks are vulnerable to the proposed GAN-based\nwatermark removal attack. The proposed attack method includes two phases. In\nthe first phase, we use the GAN and few clean images to detect and reverse the\nwatermark in the DNN model. In the second phase, we fine-tune the watermarked\nDNN based on the reversed backdoor images. Experimental evaluations on the\nMNIST and CIFAR10 datasets demonstrate that, the proposed method can\neffectively remove about 98% of the watermark in DNN models, as the watermark\nretention rate reduces from 100% to less than 2% after applying the proposed\nattack. In the meantime, the proposed attack hardly affects the model's\nperformance. The test accuracy of the watermarked DNN on the MNIST and the\nCIFAR10 datasets drops by less than 1% and 3%, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.08104v1"
    },
    {
        "title": "PixInWav: Residual Steganography for Hiding Pixels in Audio",
        "authors": [
            "Margarita Geleta",
            "Cristina Punti",
            "Kevin McGuinness",
            "Jordi Pons",
            "Cristian Canton",
            "Xavier Giro-i-Nieto"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Steganography comprises the mechanics of hiding data in a host media that may\nbe publicly available. While previous works focused on unimodal setups (e.g.,\nhiding images in images, or hiding audio in audio), PixInWav targets the\nmultimodal case of hiding images in audio. To this end, we propose a novel\nresidual architecture operating on top of short-time discrete cosine transform\n(STDCT) audio spectrograms. Among our results, we find that the residual audio\nsteganography setup we propose allows independent encoding of the hidden image\nfrom the host audio without compromising quality. Accordingly, while previous\nworks require both host and hidden signals to hide a signal, PixInWav can\nencode images offline -- which can be later hidden, in a residual fashion, into\nany audio signal. Finally, we test our scheme in a lab setting to transmit\nimages over airwaves from a loudspeaker to a microphone verifying our\ntheoretical insights and obtaining promising results.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.09814v1"
    },
    {
        "title": "Multi-Contextual Design of Convolutional Neural Network for Steganalysis",
        "authors": [
            "Brijesh Singh",
            "Arijit Sur",
            "Pinaki Mitra"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In recent times, deep learning-based steganalysis classifiers became popular\ndue to their state-of-the-art performance. Most deep steganalysis classifiers\nusually extract noise residuals using high-pass filters as preprocessing steps\nand feed them to their deep model for classification. It is observed that\nrecent steganographic embedding does not always restrict their embedding in the\nhigh-frequency zone; instead, they distribute it as per embedding policy.\nTherefore, besides noise residual, learning the embedding zone is another\nchallenging task. In this work, unlike the conventional approaches, the\nproposed model first extracts the noise residual using learned denoising\nkernels to boost the signal-to-noise ratio. After preprocessing, the sparse\nnoise residuals are fed to a novel Multi-Contextual Convolutional Neural\nNetwork (M-CNET) that uses heterogeneous context size to learn the sparse and\nlow-amplitude representation of noise residuals. The model performance is\nfurther improved by incorporating the Self-Attention module to focus on the\nareas prone to steganalytic embedding. A set of comprehensive experiments is\nperformed to show the proposed scheme's efficacy over the prior arts. Besides,\nan ablation study is given to justify the contribution of various modules of\nthe proposed architecture.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.10430v2"
    },
    {
        "title": "Cross-Modal Knowledge Distillation Method for Automatic Cued Speech\n  Recognition",
        "authors": [
            "Jianrong Wang",
            "Ziyue Tang",
            "Xuewei Li",
            "Mei Yu",
            "Qiang Fang",
            "Li Liu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Cued Speech (CS) is a visual communication system for the deaf or hearing\nimpaired people. It combines lip movements with hand cues to obtain a complete\nphonetic repertoire. Current deep learning based methods on automatic CS\nrecognition suffer from a common problem, which is the data scarcity. Until\nnow, there are only two public single speaker datasets for French (238\nsentences) and British English (97 sentences). In this work, we propose a\ncross-modal knowledge distillation method with teacher-student structure, which\ntransfers audio speech information to CS to overcome the limited data problem.\nFirstly, we pretrain a teacher model for CS recognition with a large amount of\nopen source audio speech data, and simultaneously pretrain the feature\nextractors for lips and hands using CS data. Then, we distill the knowledge\nfrom teacher model to the student model with frame-level and sequence-level\ndistillation strategies. Importantly, for frame-level, we exploit multi-task\nlearning to weigh losses automatically, to obtain the balance coefficient.\nBesides, we establish a five-speaker British English CS dataset for the first\ntime. The proposed method is evaluated on French and British English CS\ndatasets, showing superior CS recognition performance to the state-of-the-art\n(SOTA) by a large margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.13686v1"
    },
    {
        "title": "DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering",
        "authors": [
            "Jianyu Wang",
            "Bing-Kun Bao",
            "Changsheng Xu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Video question answering is a challenging task, which requires agents to be\nable to understand rich video contents and perform spatial-temporal reasoning.\nHowever, existing graph-based methods fail to perform multi-step reasoning\nwell, neglecting two properties of VideoQA: (1) Even for the same video,\ndifferent questions may require different amount of video clips or objects to\ninfer the answer with relational reasoning; (2) During reasoning, appearance\nand motion features have complicated interdependence which are correlated and\ncomplementary to each other. Based on these observations, we propose a\nDual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an\nend-to-end fashion. The first contribution of our DualVGR is the design of an\nexplainable Query Punishment Module, which can filter out irrelevant visual\nfeatures through multiple cycles of reasoning. The second contribution is the\nproposed Video-based Multi-view Graph Attention Network, which captures the\nrelations between appearance and motion features. Our DualVGR network achieves\nstate-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and\ndemonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is\navailable at https://github.com/MMIR/DualVGR-VideoQA.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.04768v1"
    },
    {
        "title": "RCLC: ROI-based joint conventional and learning video compression",
        "authors": [
            "Trinh Man Hoang",
            "Jinjia Zhou"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  COVID-19 leads to the high demand for remote interactive systems ever seen.\nOne of the key elements of these systems is video streaming, which requires a\nvery high network bandwidth due to its specific real-time demand, especially\nwith high-resolution video. Existing video compression methods are struggling\nin the trade-off between video quality and the speed requirement. Addressed\nthat the background information rarely changes in most remote meeting cases, we\nintroduce a Region-Of-Interests (ROI) based video compression framework (named\nRCLC) that leverages the cutting-edge learning-based and conventional\ntechnologies. In RCLC, each coming frame is marked as a background-updating\n(BU) or ROI-updating (RU) frame. By applying the conventional video codec, the\nBU frame is compressed with low-quality and high-compression, while the ROI\nfrom RU-frame is compressed with high-quality and low-compression. The\nlearning-based methods are applied to detect the ROI, blend background-ROI, and\nenhance video quality. The experimental results show that our RCLC can reduce\nup to 32.55\\% BD-rate for the ROI region compared to H.265 video codec under a\nsimilar compression time with 1080p resolution.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.06492v1"
    },
    {
        "title": "Transformer with Peak Suppression and Knowledge Guidance for\n  Fine-grained Image Recognition",
        "authors": [
            "Xinda Liu",
            "Lili Wang",
            "Xiaoguang Han"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Fine-grained image recognition is challenging because discriminative clues\nare usually fragmented, whether from a single image or multiple images. Despite\ntheir significant improvements, most existing methods still focus on the most\ndiscriminative parts from a single image, ignoring informative details in other\nregions and lacking consideration of clues from other associated images. In\nthis paper, we analyze the difficulties of fine-grained image recognition from\na new perspective and propose a transformer architecture with the peak\nsuppression module and knowledge guidance module, which respects the\ndiversification of discriminative features in a single image and the\naggregation of discriminative clues among multiple images. Specifically, the\npeak suppression module first utilizes a linear projection to convert the input\nimage into sequential tokens. It then blocks the token based on the attention\nresponse generated by the transformer encoder. This module penalizes the\nattention to the most discriminative parts in the feature learning process,\ntherefore, enhancing the information exploitation of the neglected regions. The\nknowledge guidance module compares the image-based representation generated\nfrom the peak suppression module with the learnable knowledge embedding set to\nobtain the knowledge response coefficients. Afterwards, it formalizes the\nknowledge learning as a classification problem using response coefficients as\nthe classification scores. Knowledge embeddings and image-based representations\nare updated during training so that the knowledge embedding includes\ndiscriminative clues for different images. Finally, we incorporate the acquired\nknowledge embeddings into the image-based representations as comprehensive\nrepresentations, leading to significantly higher performance. Extensive\nevaluations on the six popular datasets demonstrate the advantage of the\nproposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.06538v2"
    },
    {
        "title": "DHNet: Double MPEG-4 Compression Detection via Multiple DCT Histograms",
        "authors": [
            "Seung-Hun Nam",
            "Wonhyuk Ahn",
            "Myung-Joon Kwon",
            "Jihyeon Kang",
            "In-Jae Yu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In this article, we aim to detect the double compression of MPEG-4, a\nuniversal video codec that is built into surveillance systems and shooting\ndevices. Double compression is accompanied by various types of video\nmanipulation, and its traces can be exploited to determine whether a video is a\nforgery. To this end, we present a neural network-based approach with\ndiscriminant features for capturing peculiar artifacts in the discrete cosine\ntransform (DCT) domain caused by double MPEG-4 compression. By analyzing the\nintra-coding process of MPEG-4, which performs block-DCT-based quantization, we\nexploit multiple DCT histograms as features to focus on the statistical\nproperties of DCT coefficients on multiresolution blocks. Furthermore, we\nimprove detection performance using a vectorized feature of the quantization\ntable on dense layers as auxiliary information. Compared with neural\nnetwork-based approaches suitable for exploring subtle manipulations, the\nexperimental results reveal that this work achieves high performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.08939v2"
    },
    {
        "title": "The CORSMAL benchmark for the prediction of the properties of containers",
        "authors": [
            "Alessio Xompero",
            "Santiago Donaher",
            "Vladimir Iashin",
            "Francesca Palermo",
            "Gökhan Solak",
            "Claudio Coppola",
            "Reina Ishikawa",
            "Yuichi Nagao",
            "Ryo Hachiuma",
            "Qi Liu",
            "Fan Feng",
            "Chuanlin Lan",
            "Rosa H. M. Chan",
            "Guilherme Christmann",
            "Jyun-Ting Song",
            "Gonuguntla Neeharika",
            "Chinnakotla Krishna Teja Reddy",
            "Dinesh Jain",
            "Bakhtawar Ur Rehman",
            "Andrea Cavallaro"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The contactless estimation of the weight of a container and the amount of its\ncontent manipulated by a person are key pre-requisites for safe human-to-robot\nhandovers. However, opaqueness and transparencies of the container and the\ncontent, and variability of materials, shapes, and sizes, make this estimation\ndifficult. In this paper, we present a range of methods and an open framework\nto benchmark acoustic and visual perception for the estimation of the capacity\nof a container, and the type, mass, and amount of its content. The framework\nincludes a dataset, specific tasks and performance measures. We conduct an\nin-depth comparative analysis of methods that used this framework and\naudio-only or vision-only baselines designed from related works. Based on this\nanalysis, we can conclude that audio-only and audio-visual classifiers are\nsuitable for the estimation of the type and amount of the content using\ndifferent types of convolutional neural networks, combined with either\nrecurrent neural networks or a majority voting strategy, whereas computer\nvision methods are suitable to determine the capacity of the container using\nregression and geometric approaches. Classifying the content type and level\nusing only audio achieves a weighted average F1-score up to 81% and 97%,\nrespectively. Estimating the container capacity with vision-only approaches and\nestimating the filling mass with audio-visual multi-stage approaches reach up\nto 65% weighted average capacity and mass scores. These results show that there\nis still room for improvement on the design of new methods. These new methods\ncan be ranked and compared on the individual leaderboards provided by our open\nframework.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.12719v3"
    },
    {
        "title": "Angel's Girl for Blind Painters: an Efficient Painting Navigation System\n  Validated by Multimodal Evaluation Approach",
        "authors": [
            "Hang Liu",
            "Menghan Hu",
            "Yuzhen Chen",
            "Qingli Li",
            "Guangtao Zhai",
            "Simon X. Yang",
            "Xiao-Ping Zhang",
            "Xiaokang Yang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  For people who ardently love painting but unfortunately have visual\nimpairments, holding a paintbrush to create a work is a very difficult task.\nPeople in this special group are eager to pick up the paintbrush, like Leonardo\nda Vinci, to create and make full use of their own talents. Therefore, to\nmaximally bridge this gap, we propose a painting navigation system to assist\nblind people in painting and artistic creation. The proposed system is composed\nof cognitive system and guidance system. The system adopts drawing board\npositioning based on QR code, brush navigation based on target detection and\nbush real-time positioning. Meanwhile, this paper uses human-computer\ninteraction on the basis of voice and a simple but efficient position\ninformation coding rule. In addition, we design a criterion to efficiently\njudge whether the brush reaches the target or not. According to the\nexperimental results, the thermal curves extracted from the faces of testers\nshow that it is relatively well accepted by blindfolded and even blind testers.\nWith the prompt frequency of 1s, the painting navigation system performs best\nwith the completion degree of 89% with SD of 8.37% and overflow degree of 347%\nwith SD of 162.14%. Meanwhile, the excellent and good types of brush tip\ntrajectory account for 74%, and the relative movement distance is 4.21 with SD\nof 2.51. This work demonstrates that it is practicable for the blind people to\nfeel the world through the brush in their hands. In the future, we plan to\ndeploy Angle's Eyes on the phone to make it more portable. The demo video of\nthe proposed painting navigation system is available at:\nhttps://doi.org/10.6084/m9.figshare.9760004.v1.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.12921v1"
    },
    {
        "title": "An Efficient Digital Watermarking Algorithm Based on DCT and BCH Error\n  Correcting Code",
        "authors": [
            "Saeideh Nabipour",
            "Javad Javidan",
            "Majid Khorrami",
            "Jila Azimzadeh"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Watermarking is a technique for hiding of data in a medium coverage so that\nits presence is not detectable by a human eye and is recoverable only by the\nauthorized recipient. Two of the most important features of watermarked image\nare transparency and robustness which are largely related to the security of\nwatermarking algorithms. In this paper, an image watermarking scheme based on\nBCH error correction code in Discrete Cosine Transformation (DCT) domain is\nconsidered. Before embedding process, the watermark is encoded through BCH\ncoding. Then it is embedded into the Discrete Cosine Transformation (DCT)\ncoefficients of cover image. In order to decrease embedding complexity and\nspeed up the process of finding the best position to insert a watermark signal,\nlookup table method is utilized. The key features of proposed method include\nthe reduction of time required in the process embedding of information,\nsecurity and ability to correct the error caused by variety of attacks and\ndestructions as well. Watermarked image robustness has been investigated\nagainst different kinds attacks and the simulation results indicate that the\nproposed algorithm outperforms the existing methods in terms of\nimperceptibility, robustness and security.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.01612v3"
    },
    {
        "title": "Grid-VLP: Revisiting Grid Features for Vision-Language Pre-training",
        "authors": [
            "Ming Yan",
            "Haiyang Xu",
            "Chenliang Li",
            "Bin Bi",
            "Junfeng Tian",
            "Min Gui",
            "Wei Wang"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Existing approaches to vision-language pre-training (VLP) heavily rely on an\nobject detector based on bounding boxes (regions), where salient objects are\nfirst detected from images and then a Transformer-based model is used for\ncross-modal fusion. Despite their superior performance, these approaches are\nbounded by the capability of the object detector in terms of both effectiveness\nand efficiency. Besides, the presence of object detection imposes unnecessary\nconstraints on model designs and makes it difficult to support end-to-end\ntraining. In this paper, we revisit grid-based convolutional features for\nvision-language pre-training, skipping the expensive region-related steps. We\npropose a simple yet effective grid-based VLP method that works surprisingly\nwell with the grid features. By pre-training only with in-domain datasets, the\nproposed Grid-VLP method can outperform most competitive region-based VLP\nmethods on three examined vision-language understanding tasks. We hope that our\nfindings help to further advance the state of the art of vision-language\npre-training, and provide a new direction towards effective and efficient VLP.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.09479v1"
    },
    {
        "title": "Towards Robust Mispronunciation Detection and Diagnosis for L2 English\n  Learners with Accent-Modulating Methods",
        "authors": [
            "Shao-Wei Fan Jiang",
            "Bi-Cheng Yan",
            "Tien-Hong Lo",
            "Fu-An Chao",
            "Berlin Chen"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  With the acceleration of globalization, more and more people are willing or\nrequired to learn second languages (L2). One of the major remaining challenges\nfacing current mispronunciation and diagnosis (MDD) models for use in\ncomputer-assisted pronunciation training (CAPT) is to handle speech from L2\nlearners with a diverse set of accents. In this paper, we set out to mitigate\nthe adverse effects of accent variety in building an L2 English MDD system with\nend-to-end (E2E) neural models. To this end, we first propose an effective\nmodeling framework that infuses accent features into an E2E MDD model, thereby\nmaking the model more accent-aware. Going a step further, we design and present\ndisparate accent-aware modules to perform accent-aware modulation of acoustic\nfeatures in a finer-grained manner, so as to enhance the discriminating\ncapability of the resulting MDD model. Extensive sets of experiments conducted\non the L2-ARCTIC benchmark dataset show the merits of our MDD model, in\ncomparison to some existing E2E-based strong baselines and the celebrated\npronunciation scoring based method.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.11627v2"
    },
    {
        "title": "Fusion with Hierarchical Graphs for Mulitmodal Emotion Recognition",
        "authors": [
            "Shuyun Tang",
            "Zhaojie Luo",
            "Guoshun Nan",
            "Yuichiro Yoshikawa",
            "Ishiguro Hiroshi"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Automatic emotion recognition (AER) based on enriched multimodal inputs,\nincluding text, speech, and visual clues, is crucial in the development of\nemotionally intelligent machines. Although complex modality relationships have\nbeen proven effective for AER, they are still largely underexplored because\nprevious works predominantly relied on various fusion mechanisms with simply\nconcatenated features to learn multimodal representations for emotion\nclassification. This paper proposes a novel hierarchical fusion graph\nconvolutional network (HFGCN) model that learns more informative multimodal\nrepresentations by considering the modality dependencies during the feature\nfusion procedure. Specifically, the proposed model fuses multimodality inputs\nusing a two-stage graph construction approach and encodes the modality\ndependencies into the conversation representation. We verified the\ninterpretable capabilities of the proposed method by projecting the emotional\nstates to a 2D valence-arousal (VA) subspace. Extensive experiments showed the\neffectiveness of our proposed model for more accurate AER, which yielded\nstate-of-the-art results on two public datasets, IEMOCAP and MELD.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.07149v1"
    },
    {
        "title": "Graph Fourier Transform based Audio Zero-watermarking",
        "authors": [
            "Longting Xu",
            "Daiyu Huang",
            "Syed Faham Ali Zaidi",
            "Abdul Rauf",
            "Rohan Kumar Das"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The frequent exchange of multimedia information in the present era projects\nan increasing demand for copyright protection. In this work, we propose a novel\naudio zero-watermarking technology based on graph Fourier transform for\nenhancing the robustness with respect to copyright protection. In this\napproach, the combined shift operator is used to construct the graph signal,\nupon which the graph Fourier analysis is performed. The selected maximum\nabsolute graph Fourier coefficients representing the characteristics of the\naudio segment are then encoded into a feature binary sequence using K-means\nalgorithm. Finally, the resultant feature binary sequence is XOR-ed with the\nwatermark binary sequence to realize the embedding of the zero-watermarking.\nThe experimental studies show that the proposed approach performs more\neffectively in resisting common or synchronization attacks than the existing\nstate-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.08007v1"
    },
    {
        "title": "Multi-Level Visual Similarity Based Personalized Tourist Attraction\n  Recommendation Using Geo-Tagged Photos",
        "authors": [
            "Ling Chen",
            "Dandan Lyu",
            "Shanshan Yu",
            "Gencai Chen"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Geo-tagged photo based tourist attraction recommendation can discover users'\ntravel preferences from their taken photos, so as to recommend suitable tourist\nattractions to them. However, existing visual content based methods cannot\nfully exploit the user and tourist attraction information of photos to extract\nvisual features, and do not differentiate the significances of different\nphotos. In this paper, we propose multi-level visual similarity based\npersonalized tourist attraction recommendation using geo-tagged photos (MEAL).\nMEAL utilizes the visual contents of photos and interaction behavior data to\nobtain the final embeddings of users and tourist attractions, which are then\nused to predict the visit probabilities. Specifically, by crossing the user and\ntourist attraction information of photos, we define four visual similarity\nlevels and introduce a corresponding quintuplet loss to embed the visual\ncontents of photos. In addition, to capture the significances of different\nphotos, we exploit the self-attention mechanism to obtain the visual\nrepresentations of users and tourist attractions. We conducted experiments on a\ndataset crawled from Flickr, and the experimental results proved the advantage\nof this method.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.08275v2"
    },
    {
        "title": "High Frame Rate Video Quality Assessment using VMAF and Entropic\n  Differences",
        "authors": [
            "Pavan C Madhusudana",
            "Neil Birkbeck",
            "Yilin Wang",
            "Balu Adsumilli",
            "Alan C. Bovik"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The popularity of streaming videos with live, high-action content has led to\nan increased interest in High Frame Rate (HFR) videos. In this work we address\nthe problem of frame rate dependent Video Quality Assessment (VQA) when the\nvideos to be compared have different frame rate and compression factor. The\ncurrent VQA models such as VMAF have superior correlation with perceptual\njudgments when videos to be compared have same frame rates and contain\nconventional distortions such as compression, scaling etc. However this\nframework requires additional pre-processing step when videos with different\nframe rates need to be compared, which can potentially limit its overall\nperformance. Recently, Generalized Entropic Difference (GREED) VQA model was\nproposed to account for artifacts that arise due to changes in frame rate, and\nshowed superior performance on the LIVE-YT-HFR database which contains frame\nrate dependent artifacts such as judder, strobing etc. In this paper we propose\na simple extension, where the features from VMAF and GREED are fused in order\nto exploit the advantages of both models. We show through various experiments\nthat the proposed fusion framework results in more efficient features for\npredicting frame rate dependent video quality. We also evaluate the fused\nfeature set on standard non-HFR VQA databases and obtain superior performance\nthan both GREED and VMAF, indicating the combined feature set captures\ncomplimentary perceptual quality information.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.12785v1"
    },
    {
        "title": "Audio-to-Image Cross-Modal Generation",
        "authors": [
            "Maciej Żelaszczyk",
            "Jacek Mańdziuk"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Cross-modal representation learning allows to integrate information from\ndifferent modalities into one representation. At the same time, research on\ngenerative models tends to focus on the visual domain with less emphasis on\nother domains, such as audio or text, potentially missing the benefits of\nshared representations. Studies successfully linking more than one modality in\nthe generative setting are rare. In this context, we verify the possibility to\ntrain variational autoencoders (VAEs) to reconstruct image archetypes from\naudio data. Specifically, we consider VAEs in an adversarial training framework\nin order to ensure more variability in the generated data and find that there\nis a trade-off between the consistency and diversity of the generated images -\nthis trade-off can be governed by scaling the reconstruction loss up or down,\nrespectively. Our results further suggest that even in the case when the\ngenerated images are relatively inconsistent (diverse), features that are\ncritical for proper image classification are preserved.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.13354v1"
    },
    {
        "title": "FiLMing Multimodal Sarcasm Detection with Attention",
        "authors": [
            "Sundesh Gupta",
            "Aditya Shah",
            "Miten Shah",
            "Laribok Syiemlieh",
            "Chandresh Maurya"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Sarcasm detection identifies natural language expressions whose intended\nmeaning is different from what is implied by its surface meaning. It finds\napplications in many NLP tasks such as opinion mining, sentiment analysis, etc.\nToday, social media has given rise to an abundant amount of multimodal data\nwhere users express their opinions through text and images. Our paper aims to\nleverage multimodal data to improve the performance of the existing systems for\nsarcasm detection. So far, various approaches have been proposed that uses text\nand image modality and a fusion of both. We propose a novel architecture that\nuses the RoBERTa model with a co-attention layer on top to incorporate context\nincongruity between input text and image attributes. Further, we integrate\nfeature-wise affine transformation by conditioning the input image through\nFiLMed ResNet blocks with the textual features using the GRU network to capture\nthe multimodal information. The output from both the models and the CLS token\nfrom RoBERTa is concatenated and used for the final prediction. Our results\ndemonstrate that our proposed model outperforms the existing state-of-the-art\nmethod by 6.14% F1 score on the public Twitter multimodal sarcasm detection\ndataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.00416v1"
    },
    {
        "title": "Graph Representation Learning for Spatial Image Steganalysis",
        "authors": [
            "Qiyun Liu",
            "Hanzhou Wu"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In this paper, we introduce a graph representation learning architecture for\nspatial image steganalysis, which is motivated by the assumption that\nsteganographic modifications unavoidably distort the statistical\ncharacteristics of the hidden graph features derived from cover images. In the\ndetailed architecture, we translate each image to a graph, where nodes\nrepresent the patches of the image and edges indicate the local relationships\nbetween the patches. Each node is associated with a feature vector determined\nfrom the corresponding patch by a shallow convolutional neural network (CNN)\nstructure. By feeding the graph to an attention network, the discriminative\nfeatures can be learned for efficient steganalysis. Experiments indicate that\nthe reported architecture achieves a competitive performance compared to the\nbenchmark CNN model, which has shown the potential of graph learning for\nsteganalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.00957v3"
    },
    {
        "title": "Multimodal Fusion Based Attentive Networks for Sequential Music\n  Recommendation",
        "authors": [
            "Kunal Vaswani",
            "Yudhik Agrawal",
            "Vinoo Alluri"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Music has the power to evoke intense emotional experiences and regulate the\nmood of an individual. With the advent of online streaming services, research\nin music recommendation services has seen tremendous progress. Modern methods\nleveraging the listening histories of users for session-based song\nrecommendations have overlooked the significance of features extracted from\nlyrics and acoustic content. We address the task of song prediction through\nmultiple modalities, including tags, lyrics, and acoustic content. In this\npaper, we propose a novel deep learning approach by refining Attentive Neural\nNetworks using representations derived via a Transformer model for lyrics and\nVariational Autoencoder for acoustic features. Our model achieves significant\nimprovement in performance over existing state-of-the-art models using lyrical\nand acoustic features alone. Furthermore, we conduct a study to investigate the\nimpact of users' psychological health on our model's performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.01001v1"
    },
    {
        "title": "NoisyActions2M: A Multimedia Dataset for Video Understanding from Noisy\n  Labels",
        "authors": [
            "Mohit Sharma",
            "Raj Patra",
            "Harshal Desai",
            "Shruti Vyas",
            "Yogesh Rawat",
            "Rajiv Ratn Shah"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Deep learning has shown remarkable progress in a wide range of problems.\nHowever, efficient training of such models requires large-scale datasets, and\ngetting annotations for such datasets can be challenging and costly. In this\nwork, we explore the use of user-generated freely available labels from web\nvideos for video understanding. We create a benchmark dataset consisting of\naround 2 million videos with associated user-generated annotations and other\nmeta information. We utilize the collected dataset for action classification\nand demonstrate its usefulness with existing small-scale annotated datasets,\nUCF101 and HMDB51. We study different loss functions and two pretraining\nstrategies, simple and self-supervised learning. We also show how a network\npretrained on the proposed dataset can help against video corruption and label\nnoise in downstream datasets. We present this as a benchmark dataset in noisy\nlearning for video understanding. The dataset, code, and trained models will be\npublicly available for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.06827v1"
    },
    {
        "title": "Mixed Reality using Illumination-aware Gradient Mixing in Surgical\n  Telepresence: Enhanced Multi-layer Visualization",
        "authors": [
            "Nirakar Puri",
            "Abeer Alsadoon",
            "P. W. C. Prasad",
            "Nada Alsalami",
            "Tarik A. Rashid"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Background and aim: Surgical telepresence using augmented perception has been\napplied, but mixed reality is still being researched and is only theoretical.\nThe aim of this work is to propose a solution to improve the visualization in\nthe final merged video by producing globally consistent videos when the\nintensity of illumination in the input source and target video varies.\nMethodology: The proposed system uses an enhanced multi-layer visualization\nwith illumination-aware gradient mixing using Illumination Aware Video\nComposition algorithm. Particle Swarm Optimization Algorithm is used to find\nthe best sample pair from foreground and background region and image pixel\ncorrelation to estimate the alpha matte. Particle Swarm Optimization algorithm\nhelps to get the original colour and depth of the unknown pixel in the unknown\nregion. Result: Our results showed improved accuracy caused by reducing the\nMean squared Error for selecting the best sample pair for unknown region in 10\neach sample for bowel, jaw and breast. The amount of this reduction is 16.48%\nfrom the state of art system. As a result, the visibility accuracy is improved\nfrom 89.4 to 97.7% which helped to clear the hand vision even in the difference\nof light. Conclusion: Illumination effect and alpha pixel correlation improves\nthe visualization accuracy and produces a globally consistent composition\nresults and maintains the temporal coherency when compositing two videos with\nhigh and inverse illumination effect. In addition, this paper provides a\nsolution for selecting the best sampling pair for the unknown region to obtain\nthe original colour and depth.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.09318v1"
    },
    {
        "title": "Hierarchical Deep Residual Reasoning for Temporal Moment Localization",
        "authors": [
            "Ziyang Ma",
            "Xianjing Han",
            "Xuemeng Song",
            "Yiran Cui",
            "Liqiang Nie"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Temporal Moment Localization (TML) in untrimmed videos is a challenging task\nin the field of multimedia, which aims at localizing the start and end points\nof the activity in the video, described by a sentence query. Existing methods\nmainly focus on mining the correlation between video and sentence\nrepresentations or investigating the fusion manner of the two modalities. These\nworks mainly understand the video and sentence coarsely, ignoring the fact that\na sentence can be understood from various semantics, and the dominant words\naffecting the moment localization in the semantics are the action and object\nreference. Toward this end, we propose a Hierarchical Deep Residual Reasoning\n(HDRR) model, which decomposes the video and sentence into multi-level\nrepresentations with different semantics to achieve a finer-grained\nlocalization. Furthermore, considering that videos with different resolution\nand sentences with different length have different difficulty in understanding,\nwe design the simple yet effective Res-BiGRUs for feature fusion, which is able\nto grasp the useful information in a self-adapting manner. Extensive\nexperiments conducted on Charades-STA and ActivityNet-Captions datasets\ndemonstrate the superiority of our HDRR model compared with other\nstate-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.00417v1"
    },
    {
        "title": "Distantly Supervised Semantic Text Detection and Recognition for\n  Broadcast Sports Videos Understanding",
        "authors": [
            "Avijit Shah",
            "Topojoy Biswas",
            "Sathish Ramadoss",
            "Deven Santosh Shah"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Comprehensive understanding of key players and actions in multiplayer sports\nbroadcast videos is a challenging problem. Unlike in news or finance videos,\nsports videos have limited text. While both action recognition for multiplayer\nsports and detection of players has seen robust research, understanding\ncontextual text in video frames still remains one of the most impactful avenues\nof sports video understanding. In this work we study extremely accurate\nsemantic text detection and recognition in sports clocks, and challenges\ntherein. We observe unique properties of sports clocks, which makes it hard to\nutilize general-purpose pre-trained detectors and recognizers, so that text can\nbe accurately understood to the degree of being used to align to external\nknowledge. We propose a novel distant supervision technique to automatically\nbuild sports clock datasets. Along with suitable data augmentations, combined\nwith any state-of-the-art text detection and recognition model architectures,\nwe extract extremely accurate semantic text. Finally, we share our\ncomputational architecture pipeline to scale this system in industrial setting\nand proposed a robust dataset for the same to validate our results.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.00629v1"
    },
    {
        "title": "Video Background Music Generation with Controllable Music Transformer",
        "authors": [
            "Shangzhe Di",
            "Zeren Jiang",
            "Si Liu",
            "Zhaokai Wang",
            "Leyan Zhu",
            "Zexin He",
            "Hongming Liu",
            "Shuicheng Yan"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In this work, we address the task of video background music generation. Some\nprevious works achieve effective music generation but are unable to generate\nmelodious music tailored to a particular video, and none of them considers the\nvideo-music rhythmic consistency. To generate the background music that matches\nthe given video, we first establish the rhythmic relations between video and\nbackground music. In particular, we connect timing, motion speed, and motion\nsaliency from video with beat, simu-note density, and simu-note strength from\nmusic, respectively. We then propose CMT, a Controllable Music Transformer that\nenables local control of the aforementioned rhythmic features and global\ncontrol of the music genre and instruments. Objective and subjective\nevaluations show that the generated background music has achieved satisfactory\ncompatibility with the input videos, and at the same time, impressive music\nquality. Code and models are available at\nhttps://github.com/wzk1015/video-bgm-generation.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.08380v1"
    },
    {
        "title": "FNR: A Similarity and Transformer-Based Approach to Detect Multi-Modal\n  Fake News in Social Media",
        "authors": [
            "Faeze Ghorbanpour",
            "Maryam Ramezani",
            "Mohammad A. Fazli",
            "Hamid R. Rabiee"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The availability and interactive nature of social media have made them the\nprimary source of news around the globe. The popularity of social media tempts\ncriminals to pursue their immoral intentions by producing and disseminating\nfake news using seductive text and misleading images. Therefore, verifying\nsocial media news and spotting fakes is crucial. This work aims to analyze\nmulti-modal features from texts and images in social media for detecting fake\nnews. We propose a Fake News Revealer (FNR) method that utilizes transform\nlearning to extract contextual and semantic features and contrastive loss to\ndetermine the similarity between image and text. We applied FNR on two real\nsocial media datasets. The results show the proposed method achieves higher\naccuracies in detecting fake news compared to the previous works.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01131v1"
    },
    {
        "title": "Cross-modal Knowledge Distillation for Vision-to-Sensor Action\n  Recognition",
        "authors": [
            "Jianyuan Ni",
            "Raunak Sarbajna",
            "Yang Liu",
            "Anne H. H. Ngu",
            "Yan Yan"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Human activity recognition (HAR) based on multi-modal approach has been\nrecently shown to improve the accuracy performance of HAR. However, restricted\ncomputational resources associated with wearable devices, i.e., smartwatch,\nfailed to directly support such advanced methods. To tackle this issue, this\nstudy introduces an end-to-end Vision-to-Sensor Knowledge Distillation (VSKD)\nframework. In this VSKD framework, only time-series data, i.e., accelerometer\ndata, is needed from wearable devices during the testing phase. Therefore, this\nframework will not only reduce the computational demands on edge devices, but\nalso produce a learning model that closely matches the performance of the\ncomputational expensive multi-modal approach. In order to retain the local\ntemporal relationship and facilitate visual deep learning models, we first\nconvert time-series data to two-dimensional images by applying the Gramian\nAngular Field ( GAF) based encoding method. We adopted ResNet18 and multi-scale\nTRN with BN-Inception as teacher and student network in this study,\nrespectively. A novel loss function, named Distance and Angle-wised Semantic\nKnowledge loss (DASK), is proposed to mitigate the modality variations between\nthe vision and the sensor domain. Extensive experimental results on UTD-MHAD,\nMMAct, and Berkeley-MHAD datasets demonstrate the effectiveness and\ncompetitiveness of the proposed VSKD model which can deployed on wearable\nsensors.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01849v1"
    },
    {
        "title": "Expert and Crowd-Guided Affect Annotation and Prediction",
        "authors": [
            "Ramanathan Subramanian",
            "Yan Yan",
            "Nicu Sebe"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  We employ crowdsourcing to acquire time-continuous affective annotations for\nmovie clips, and refine noisy models trained from these crowd annotations\nincorporating expert information within a Multi-task Learning (MTL) framework.\nWe propose a novel \\textbf{e}xpert \\textbf{g}uided MTL (EG-MTL) algorithm,\nwhich minimizes the loss with respect to both crowd and expert labels to learn\na set of weights corresponding to each movie clip for which crowd annotations\nare acquired. We employ EG-MTL to solve two problems, namely,\n\\textbf{\\texttt{P1}}: where dynamic annotations acquired from both experts and\ncrowdworkers for the \\textbf{Validation} set are used to train a regression\nmodel with audio-visual clip descriptors as features, and predict dynamic\narousal and valence levels on 5--15 second snippets derived from the clips; and\n\\textbf{\\texttt{P2}}: where a classification model trained on the\n\\textbf{Validation} set using dynamic crowd and expert annotations (as\nfeatures) and static affective clip labels is used for binary emotion\nrecognition on the \\textbf{Evaluation} set for which only dynamic crowd\nannotations are available. Observed experimental results confirm the\neffectiveness of the EG-MTL algorithm, which is reflected via improved arousal\nand valence estimation for \\textbf{\\texttt{P1}}, and higher recognition\naccuracy for \\textbf{\\texttt{P2}}.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.08432v1"
    },
    {
        "title": "Perceptual Evaluation of 360 Audiovisual Quality and Machine Learning\n  Predictions",
        "authors": [
            "Randy Frans Fela",
            "Nick Zacharov",
            "Søren Forchhammer"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  In an earlier study, we gathered perceptual evaluations of the audio, video,\nand audiovisual quality for 360 audiovisual content. This paper investigates\nperceived audiovisual quality prediction based on objective quality metrics and\nsubjective scores of 360 video and spatial audio content. Thirteen objective\nvideo quality metrics and three objective audio quality metrics were evaluated\nfor five stimuli for each coding parameter. Four regression-based machine\nlearning models were trained and tested here, i.e., multiple linear regression,\ndecision tree, random forest, and support vector machine. Each model was\nconstructed using a combination of audio and video quality metrics and two\ncross-validation methods (k-Fold and Leave-One-Out) were investigated and\nproduced 312 predictive models. The results indicate that the model based on\nthe evaluation of VMAF and AMBIQUAL is better than other combinations of\naudio-video quality metric. In this study, support vector machine provides\nhigher performance using k-Fold (PCC = 0.909, SROCC = 0.914, and RMSE = 0.416).\nThese results can provide insights for the design of multimedia quality metrics\nand the development of predictive models for audiovisual omnidirectional media.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.12273v1"
    },
    {
        "title": "BLINC: Lightweight Bimodal Learning for Low-Complexity VVC Intra Coding",
        "authors": [
            "Farhad Pakdaman",
            "Mohammad Ali Adelimanesh",
            "Mahmoud Reza Hashemi"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The latest video coding standard, Versatile Video Coding (VVC), achieves\nalmost twice coding efficiency compared to its predecessor, the High Efficiency\nVideo Coding (HEVC). However, achieving this efficiency (for intra coding)\nrequires 31x computational complexity compared to HEVC, making it challenging\nfor low power and real-time applications. This paper, proposes a novel machine\nlearning approach that jointly and separately employs two modalities of\nfeatures, to simplify the intra coding decision. First a set of features are\nextracted that use the existing DCT core of VVC, to assess the texture\ncharacteristics, and forms the first modality of data. This produces high\nquality features with almost no overhead. The distribution of intra modes at\nthe neighboring blocks is also used to form the second modality of data, which\nprovides statistical information about the frame. Second, a two-step feature\nreduction method is designed that reduces the size of feature set, such that a\nlightweight model with a limited number of parameters can be used to learn the\nintra mode decision task. Third, three separate training strategies are\nproposed (1) an offline training strategy using the first (single) modality of\ndata, (2) an online training strategy that uses the second (single) modality,\nand (3) a mixed online-offline strategy that uses bimodal learning. Finally, a\nlow-complexity encoding algorithms is proposed based on the proposed learning\nstrategies. Extensive experimental results show that the proposed methods can\nreduce up to 24% of encoding time, with a negligible loss of coding efficiency.\nMoreover, it is demonstrated how a bimodal learning strategy can boost the\nperformance of learning. Lastly, the proposed method has a very low\ncomputational overhead (0.2%), and uses existing components of a VVC encoder,\nwhich makes it much more practical compared to competing solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.07823v1"
    },
    {
        "title": "A Pre-trained Audio-Visual Transformer for Emotion Recognition",
        "authors": [
            "Minh Tran",
            "Mohammad Soleymani"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this paper, we introduce a pretrained audio-visual Transformer trained on\nmore than 500k utterances from nearly 4000 celebrities from the VoxCeleb2\ndataset for human behavior understanding. The model aims to capture and extract\nuseful information from the interactions between human facial and auditory\nbehaviors, with application in emotion recognition. We evaluate the model\nperformance on two datasets, namely CREMAD-D (emotion classification) and\nMSP-IMPROV (continuous emotion regression). Experimental results show that\nfine-tuning the pre-trained model helps improving emotion classification\naccuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous\nemotion recognition by 0.03-0.09 compared to the same model trained from\nscratch. We also demonstrate the robustness of finetuning the pre-trained model\nin a low-resource setting. With only 10% of the original training set provided,\nfine-tuning the pre-trained model can lead to at least 10% better emotion\nrecognition accuracy and a CCC score improvement by at least 0.1 for continuous\nemotion recognition.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.09165v1"
    },
    {
        "title": "Image Difference Captioning with Pre-training and Contrastive Learning",
        "authors": [
            "Linli Yao",
            "Weiying Wang",
            "Qin Jin"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The Image Difference Captioning (IDC) task aims to describe the visual\ndifferences between two similar images with natural language. The major\nchallenges of this task lie in two aspects: 1) fine-grained visual differences\nthat require learning stronger vision and language association and 2) high-cost\nof manual annotations that leads to limited supervised data. To address these\nchallenges, we propose a new modeling framework following the\npre-training-finetuning paradigm. Specifically, we design three self-supervised\ntasks and contrastive learning strategies to align visual differences and text\ndescriptions at a fine-grained level. Moreover, we propose a data expansion\nstrategy to utilize extra cross-task supervision information, such as data for\nfine-grained image classification, to alleviate the limitation of available\nsupervised IDC data. Extensive experiments on two IDC benchmark datasets,\nCLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed\nmodeling framework. The codes and models will be released at\nhttps://github.com/yaolinli/IDC.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.04298v1"
    },
    {
        "title": "Privacy Leakage in Proactive VR Streaming: Modeling and Tradeoff",
        "authors": [
            "Xing Wei",
            "Chenyang Yang",
            "Chengjian Sun"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Proactive tile-based virtual reality (VR) video streaming employs the\nviewpoint of a user to predict the tiles to be requested, renders and delivers\nthe predicted tiles before playback. Recently, it has been found that the\nidentity and preference of the user can be inferred from the trace of viewpoint\nuploaded for proactive streaming, which indicates that viewpoint leakage incurs\nprivacy leakage. In this paper, we strive to answer the following questions\nregarding viewpoint leakage during proactive VR video streaming. When is the\nviewpoint leaked? Can privacy-preserving approaches (e.g., federated or\nindividual training, using predictors with no need for training, or predicting\nlocally) avoid viewpoint leakage? We find that if the prediction error or the\nquality of experience (QoE) metric is uploaded for adaptive streaming, the real\nviewpoint can be inferred even with the privacy-preserving approaches. Then, we\ndefine viewpoint leakage probability to characterize the accuracy of the\ninferred viewpoint, and respectively derive the probability when uploading\nprediction error and QoE metric. We find that the viewpoint leakage probability\ncan be reduced by sacrificing QoE or increasing resources. Simulation with the\nstate-of-the-art predictor over a real dataset shows that such a tradeoff does\nnot exist only in rare cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.03107v2"
    },
    {
        "title": "A study on joint modeling and data augmentation of multi-modalities for\n  audio-visual scene classification",
        "authors": [
            "Qing Wang",
            "Jun Du",
            "Siyuan Zheng",
            "Yunqing Li",
            "Yajian Wang",
            "Yuzhong Wu",
            "Hu Hu",
            "Chao-Han Huck Yang",
            "Sabato Marco Siniscalchi",
            "Yannan Wang",
            "Chin-Hui Lee"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this paper, we propose two techniques, namely joint modeling and data\naugmentation, to improve system performances for audio-visual scene\nclassification (AVSC). We employ pre-trained networks trained only on image\ndata sets to extract video embedding; whereas for audio embedding models, we\ndecide to train them from scratch. We explore different neural network\narchitectures for joint modeling to effectively combine the video and audio\nmodalities. Moreover, data augmentation strategies are investigated to increase\naudio-visual training set size. For the video modality the effectiveness of\nseveral operations in RandAugment is verified. An audio-video joint mixup\nscheme is proposed to further improve AVSC performances. Evaluated on the\ndevelopment set of TAU Urban Audio Visual Scenes 2021, our final system can\nachieve the best accuracy of 94.2% among all single AVSC systems submitted to\nDCASE 2021 Task 1b.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.04114v3"
    },
    {
        "title": "Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning",
        "authors": [
            "Zhenhailong Wang",
            "Hang Yu",
            "Manling Li",
            "Han Zhao",
            "Heng Ji"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models still fall short of few-shot transfer ability on\ndomain-specific problems. Classical fine-tuning often fails to prevent highly\nexpressive models from exploiting spurious correlations. Although\nmodel-agnostic meta-learning (MAML) presents as a natural alternative for\nfew-shot transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use on large-scale vision-language models\nsuch as CLIP. While much literature has been devoted to exploring alternative\noptimization strategies, we identify another essential aspect towards effective\nfew-shot transfer learning, task sampling, which is previously only be viewed\nas part of data pre-processing in MAML. To show the impact of task sampling, we\npropose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which\ndifferentiates classical fine-tuning only on uniformly sampling multiple tasks.\nDespite its simplicity, we show that MAMF consistently outperforms classical\nfine-tuning on five few-shot vision-language classification tasks. We further\nshow that the effectiveness of the bi-level optimization in MAML is highly\nsensitive to the zero-shot performance of a task in the context of few-shot\nvision-language classification. The goal of this paper is to provide new\ninsights on what makes few-shot learning work, and encourage more research into\ninvestigating better task sampling strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.04904v3"
    },
    {
        "title": "UofA-Truth at Factify 2022 : Transformer And Transfer Learning Based\n  Multi-Modal Fact-Checking",
        "authors": [
            "Abhishek Dhankar",
            "Osmar R. Zaïane",
            "Francois Bolduc"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Identifying fake news is a very difficult task, especially when considering\nthe multiple modes of conveying information through text, image, video and/or\naudio. We attempted to tackle the problem of automated\nmisinformation/disinformation detection in multi-modal news sources (including\ntext and images) through our simple, yet effective, approach in the FACTIFY\nshared task at De-Factify@AAAI2022. Our model produced an F1-weighted score of\n74.807%, which was the fourth best out of all the submissions. In this paper we\nwill explain our approach to undertake the shared task.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.07990v1"
    },
    {
        "title": "Steganalysis of Image with Adaptively Parametric Activation",
        "authors": [
            "Hai Su",
            "Meiyin Han",
            "Junle Liang",
            "Songsen Yu"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Steganalysis as a method to detect whether image contains se-cret message, is\na crucial study avoiding the imperils from abus-ing steganography. The point of\nsteganalysis is to detect the weak embedding signals which is hardly learned by\nconvolution-al layer and easily suppressed. In this paper, to enhance\nembed-ding signals, we study the insufficiencies of activation function,\nfilters and loss function from the aspects of reduce embedding signal loss and\nenhance embedding signal capture ability. Adap-tive Parametric Activation\nModule is designed to reserve nega-tive embedding signal. For embedding signal\ncapture ability enhancement, we add constraints on the high-pass filters to\nim-prove residual diversity which enables the filters extracts rich embedding\nsignals. Besides, a loss function based on contrastive learning is applied to\novercome the limitations of cross-entropy loss by maximum inter-class distance.\nIt helps the network make a distinction between embedding signals and semantic\nedges. We use images from BOSSbase 1.01 and make stegos by WOW and S-UNIWARD\nfor experiments. Compared to state-of-the-art methods, our method has a\ncompetitive performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12843v1"
    },
    {
        "title": "SeCo: Separating Unknown Musical Visual Sounds with Consistency Guidance",
        "authors": [
            "Xinchi Zhou",
            "Dongzhan Zhou",
            "Wanli Ouyang",
            "Hang Zhou",
            "Ziwei Liu",
            "Di Hu"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Recent years have witnessed the success of deep learning on the visual sound\nseparation task. However, existing works follow similar settings where the\ntraining and testing datasets share the same musical instrument categories,\nwhich to some extent limits the versatility of this task. In this work, we\nfocus on a more general and challenging scenario, namely the separation of\nunknown musical instruments, where the categories in training and testing\nphases have no overlap with each other. To tackle this new setting, we propose\nthe Separation-with-Consistency (SeCo) framework, which can accomplish the\nseparation on unknown categories by exploiting the consistency constraints.\nFurthermore, to capture richer characteristics of the novel melodies, we devise\nan online matching strategy, which can bring stable enhancements with no cost\nof extra parameters. Experiments demonstrate that our SeCo framework exhibits\nstrong adaptation ability on the novel musical categories and outperforms the\nbaseline methods by a significant margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.13535v1"
    },
    {
        "title": "A Cross-Domain Approach for Continuous Impression Recognition from\n  Dyadic Audio-Visual-Physio Signals",
        "authors": [
            "Yuanchao Li",
            "Catherine Lai"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  The impression we make on others depends not only on what we say, but also,\nto a large extent, on how we say it. As a sub-branch of affective computing and\nsocial signal processing, impression recognition has proven critical in both\nhuman-human conversations and spoken dialogue systems. However, most research\nhas studied impressions only from the signals expressed by the emitter,\nignoring the response from the receiver. In this paper, we perform impression\nrecognition using a proposed cross-domain architecture on the dyadic IMPRESSION\ndataset. This improved architecture makes use of cross-domain attention and\nregularization. The cross-domain attention consists of intra- and\ninter-attention mechanisms, which capture intra- and inter-domain relatedness,\nrespectively. The cross-domain regularization includes knowledge distillation\nand similarity enhancement losses, which strengthen the feature connections\nbetween the emitter and receiver. The experimental evaluation verified the\neffectiveness of our approach. Our approach achieved a concordance correlation\ncoefficient of 0.770 in competence dimension and 0.748 in warmth dimension.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.13932v1"
    },
    {
        "title": "Using Active Speaker Faces for Diarization in TV shows",
        "authors": [
            "Rahul Sharma",
            "Shrikanth Narayanan"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Speaker diarization is one of the critical components of computational media\nintelligence as it enables a character-level analysis of story portrayals and\nmedia content understanding. Automated audio-based speaker diarization of\nentertainment media poses challenges due to the diverse acoustic conditions\npresent in media content, be it background music, overlapping speakers, or\nsound effects. At the same time, speaking faces in the visual modality provide\ncomplementary information and not prone to the errors seen in the audio\nmodality. In this paper, we address the problem of speaker diarization in TV\nshows using the active speaker faces. We perform face clustering on the active\nspeaker faces and show superior speaker diarization performance compared to the\nstate-of-the-art audio-based diarization methods. We additionally report a\nsystematic analysis of the impact of active speaker face detection quality on\nthe diarization performance. We also observe that a moderately well-performing\nactive speaker system could outperform the audio-based diarization systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.15961v1"
    },
    {
        "title": "Network state Estimation using Raw Video Analysis: vQoS-GAN based\n  non-intrusive Deep Learning Approach",
        "authors": [
            "Renith G",
            "Harikrishna Warrier",
            "Yogesh Gupta"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Content based providers transmits real time complex signal such as video data\nfrom one region to another. During this transmission process, the signals\nusually end up distorted or degraded where the actual information present in\nthe video is lost. This normally happens in the streaming video services\napplications. Hence there is a need to know the level of degradation that\nhappened in the receiver side. This video degradation can be estimated by\nnetwork state parameters like data rate and packet loss values. Our proposed\nsolution vQoS GAN (video Quality of Service Generative Adversarial Network) can\nestimate the network state parameters from the degraded received video data\nusing a deep learning approach of semi supervised generative adversarial\nnetwork algorithm. A robust and unique design of deep learning network model\nhas been trained with the video data along with data rate and packet loss class\nlabels and achieves over 95 percent of training accuracy. The proposed semi\nsupervised generative adversarial network can additionally reconstruct the\ndegraded video data to its original form for a better end user experience.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.07062v1"
    },
    {
        "title": "hate-alert@DravidianLangTech-ACL2022: Ensembling Multi-Modalities for\n  Tamil TrollMeme Classification",
        "authors": [
            "Mithun Das",
            "Somnath Banerjee",
            "Animesh Mukherjee"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Social media platforms often act as breeding grounds for various forms of\ntrolling or malicious content targeting users or communities. One way of\ntrolling users is by creating memes, which in most cases unites an image with a\nshort piece of text embedded on top of it. The situation is more complex for\nmultilingual(e.g., Tamil) memes due to the lack of benchmark datasets and\nmodels. We explore several models to detect Troll memes in Tamil based on the\nshared task, \"Troll Meme Classification in DravidianLangTech2022\" at ACL-2022.\nWe observe while the text-based model MURIL performs better for Non-troll meme\nclassification, the image-based model VGG16 performs better for Troll-meme\nclassification. Further fusing these two modalities help us achieve stable\noutcomes in both classes. Our fusion model achieved a 0.561 weighted average F1\nscore and ranked second in this task.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12587v1"
    },
    {
        "title": "Perceptual Evaluation on Audio-visual Dataset of 360 Content",
        "authors": [
            "Randy F Fela",
            "Andréas Pastor",
            "Patrick Le Callet",
            "Nick Zacharov",
            "Toinon Vigier",
            "Søren Forchhammer"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  To open up new possibilities to assess the multimodal perceptual quality of\nomnidirectional media formats, we proposed a novel open source 360 audiovisual\n(AV) quality dataset. The dataset consists of high-quality 360 video clips in\nequirectangular (ERP) format and higher-order ambisonic (4th order) along with\nthe subjective scores. Three subjective quality experiments were conducted for\naudio, video, and AV with the procedures detailed in this paper. Using the data\nfrom subjective tests, we demonstrated that this dataset can be used to\nquantify perceived audio, video, and audiovisual quality. The diversity and\ndiscriminability of subjective scores were also analyzed. Finally, we\ninvestigated how our dataset correlates with various objective quality metrics\nof audio and video. Evidence from the results of this study implies that the\nproposed dataset can benefit future studies on multimodal quality evaluation of\n360 content.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.08007v1"
    },
    {
        "title": "3D-VFD: A Victim-free Detector against 3D Adversarial Point Clouds",
        "authors": [
            "Jiahao Zhu",
            "Huajun Zhou",
            "Zixuan Chen",
            "Yi Zhou",
            "Xiaohua Xie"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  3D deep models consuming point clouds have achieved sound application effects\nin computer vision. However, recent studies have shown they are vulnerable to\n3D adversarial point clouds. In this paper, we regard these malicious point\nclouds as 3D steganography examples and present a new perspective, 3D\nsteganalysis, to counter such examples. Specifically, we propose 3D-VFD, a\nvictim-free detector against 3D adversarial point clouds. Its core idea is to\ncapture the discrepancies between residual geometric feature distributions of\nbenign point clouds and adversarial point clouds and map these point clouds to\na lower dimensional space where we can efficiently distinguish them. Unlike\nexisting detection techniques against 3D adversarial point clouds, 3D-VFD does\nnot rely on the victim 3D deep model's outputs for discrimination. Extensive\nexperiments demonstrate that 3D-VFD achieves state-of-the-art detection and can\neffectively detect 3D adversarial attacks based on point adding and point\nperturbation while keeping fast detection speed.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.08738v3"
    },
    {
        "title": "Seeing Sounds, Hearing Shapes: a gamified study to evaluate\n  sound-sketches",
        "authors": [
            "Sebastian Löbbers",
            "György Fazekas"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Sound-shape associations, a subset of cross-modal associations between the\nauditory and visual domain, have been studied mainly in the context of matching\na set of purposefully crafted shapes to sounds. Recent studies have explored\nhow humans represent sound through free-form sketching and how a graphical\nsketch input could be used for sound production. In this paper, the potential\nof communicating sound characteristics through these free-form sketches is\ninvestigated in a gamified study that was conducted with eighty-two\nparticipants at two online exhibition events. The results show that\nparticipants managed to recognise sounds at a higher rate than the random\nbaseline would suggest, however it appeared difficult to visually encode\nnuanced timbral differences.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.08866v1"
    },
    {
        "title": "Let the paintings play",
        "authors": [
            "Paola Gervasio",
            "Alfio Quarteroni",
            "Daniele Cassani"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this paper, we introduce a mathematical method to extract similarities\nbetween paintings and musical tracks. Our approach is based on the\ndigitalization of both paintings and musical tracks by means of finite\nexpansions in terms of orthogonal basis functions (with both Fourier and\nwavelet bases). The best fit between a specific painting and a sample of\nmusical tracks from a given composer is achieved via an $L^2$ projection upon a\nfinite-dimensional subspace. Several examples are provided for the analysis of\na collection of works of art by the Italian artist Marcello Morandini. Finally,\nwe have developed an original applet that implements the process above and\nwhich can be freely downloaded from the site\nhttps://github.com/pgerva/playing-paintings.git\n",
        "pdf_link": "http://arxiv.org/pdf/2206.14142v3"
    },
    {
        "title": "Unsupervised Recurrent Federated Learning for Edge Popularity Prediction\n  in Privacy-Preserving Mobile Edge Computing Networks",
        "authors": [
            "Chong Zheng",
            "Shengheng Liu",
            "Yongming Huang",
            "Wei Zhang",
            "Luxi Yang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Nowadays wireless communication is rapidly reshaping entire industry sectors.\nIn particular, mobile edge computing (MEC) as an enabling technology for\nindustrial Internet of things (IIoT) brings powerful computing/storage\ninfrastructure closer to the mobile terminals and, thereby, significant lowers\nthe response latency. To reap the benefit of proactive caching at the network\nedge, precise knowledge on the popularity pattern among the end devices is\nessential. However, the complex and dynamic nature of the content popularity\nover space and time as well as the data-privacy requirements in many IIoT\nscenarios pose tough challenges to its acquisition. In this article, we propose\nan unsupervised and privacy-preserving popularity prediction framework for\nMEC-enabled IIoT. The concepts of local and global popularities are introduced\nand the time-varying popularity of each user is modelled as a model-free Markov\nchain. On this basis, a novel unsupervised recurrent federated learning (URFL)\nalgorithm is proposed to predict the distributed popularity while achieve\nprivacy preservation and unsupervised training. Simulations indicate that the\nproposed framework can enhance the prediction accuracy in terms of a reduced\nroot-mean-squared error by up to $60.5\\%-68.7\\%$. Additionally, manual labeling\nand violation of users' data privacy are both avoided.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00755v2"
    },
    {
        "title": "Dynamic Contrastive Distillation for Image-Text Retrieval",
        "authors": [
            "Jun Rao",
            "Liang Ding",
            "Shuhan Qi",
            "Meng Fang",
            "Yang Liu",
            "Li Shen",
            "Dacheng Tao"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Although the vision-and-language pretraining (VLP) equipped cross-modal\nimage-text retrieval (ITR) has achieved remarkable progress in the past two\nyears, it suffers from a major drawback: the ever-increasing size of VLP models\nrestricts its deployment to real-world search scenarios (where the high latency\nis unacceptable). To alleviate this problem, we present a novel plug-in dynamic\ncontrastive distillation (DCD) framework to compress the large VLP models for\nthe ITR task. Technically, we face the following two challenges: 1) the typical\nuni-modal metric learning approach is difficult to directly apply to the\ncross-modal tasks, due to the limited GPU memory to optimize too many negative\nsamples during handling cross-modal fusion features. 2) it is inefficient to\nstatic optimize the student network from different hard samples, which have\ndifferent effects on distillation learning and student network optimization. We\ntry to overcome these challenges from two points. First, to achieve multi-modal\ncontrastive learning, and balance the training costs and effects, we propose to\nuse a teacher network to estimate the difficult samples for students, making\nthe students absorb the powerful knowledge from pre-trained teachers, and\nmaster the knowledge from hard samples. Second, to dynamic learn from hard\nsample pairs, we propose dynamic distillation to dynamically learn samples of\ndifferent difficulties, from the perspective of better balancing the difficulty\nof knowledge and students' self-learning ability. We successfully apply our\nproposed DCD strategy to two state-of-the-art vision-language pretrained\nmodels, i.e. ViLT and METER. Extensive experiments on MS-COCO and Flickr30K\nbenchmarks show the effectiveness and efficiency of our DCD framework.\nEncouragingly, we can speed up the inference at least 129$\\times$ compared to\nthe existing ITR models.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.01426v1"
    },
    {
        "title": "Lip-Listening: Mixing Senses to Understand Lips using Cross Modality\n  Knowledge Distillation for Word-Based Models",
        "authors": [
            "Hadeel Mabrouk",
            "Omar Abugabal",
            "Nourhan Sakr",
            "Hesham M. Eraqi"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this work, we propose a technique to transfer speech recognition\ncapabilities from audio speech recognition systems to visual speech\nrecognizers, where our goal is to utilize audio data during lipreading model\ntraining. Impressive progress in the domain of speech recognition has been\nexhibited by audio and audio-visual systems. Nevertheless, there is still much\nto be explored with regards to visual speech recognition systems due to the\nvisual ambiguity of some phonemes. To this end, the development of visual\nspeech recognition models is crucial given the instability of audio models. The\nmain contributions of this work are i) building on recent state-of-the-art\nword-based lipreading models by integrating sequence-level and frame-level\nKnowledge Distillation (KD) to their systems; ii) leveraging audio data during\ntraining visual models, a feat which has not been utilized in prior word-based\nwork; iii) proposing the Gaussian-shaped averaging in frame-level KD, as an\nefficient technique that aids the model in distilling knowledge at the sequence\nmodel encoder. This work proposes a novel and competitive architecture for\nlip-reading, as we demonstrate a noticeable improvement in performance, setting\na new benchmark equals to 88.64% on the LRW dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.05692v1"
    },
    {
        "title": "CubeMLP: An MLP-based Model for Multimodal Sentiment Analysis and\n  Depression Estimation",
        "authors": [
            "Hao Sun",
            "Hongyi Wang",
            "Jiaqing Liu",
            "Yen-Wei Chen",
            "Lanfen Lin"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Multimodal sentiment analysis and depression estimation are two important\nresearch topics that aim to predict human mental states using multimodal data.\nPrevious research has focused on developing effective fusion strategies for\nexchanging and integrating mind-related information from different modalities.\nSome MLP-based techniques have recently achieved considerable success in a\nvariety of computer vision tasks. Inspired by this, we explore multimodal\napproaches with a feature-mixing perspective in this study. To this end, we\nintroduce CubeMLP, a multimodal feature processing framework based entirely on\nMLP. CubeMLP consists of three independent MLP units, each of which has two\naffine transformations. CubeMLP accepts all relevant modality features as input\nand mixes them across three axes. After extracting the characteristics using\nCubeMLP, the mixed multimodal features are flattened for task predictions. Our\nexperiments are conducted on sentiment analysis datasets: CMU-MOSI and\nCMU-MOSEI, and depression estimation dataset: AVEC2019. The results show that\nCubeMLP can achieve state-of-the-art performance with a much lower computing\ncost.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.14087v3"
    },
    {
        "title": "Debiased Cross-modal Matching for Content-based Micro-video Background\n  Music Recommendation",
        "authors": [
            "Jinng Yi",
            "Zhenzhong Chen"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Micro-video background music recommendation is a complicated task where the\nmatching degree between videos and uploader-selected background music is a\nmajor issue. However, the selection of the user-generated content (UGC) is\nbiased caused by knowledge limitations and historical preferences among music\nof each uploader. In this paper, we propose a Debiased Cross-Modal (DebCM)\nmatching model to alleviate the influence of such selection bias. Specifically,\nwe design a teacher-student network to utilize the matching of segments of\nmusic videos, which is professional-generated content (PGC) with specialized\nmusic-matching techniques, to better alleviate the bias caused by insufficient\nknowledge of users. The PGC data is captured by a teacher network to guide the\nmatching of uploader-selected UGC data of the student network by KL-based\nknowledge transfer. In addition, uploaders' personal preferences of music\ngenres are identified as confounders that spuriously correlate music embeddings\nand background music selections, resulting in the learned recommender system to\nover-recommend music from the majority groups. To resolve such confounders in\nthe UGC data of the student network, backdoor adjustment is utilized to\ndeconfound the spurious correlation between music embeddings and prediction\nscores. We further utilize Monte Carlo (MC) estimator with batch-level average\nas the approximations to avoid integrating the entire confounder space\ncalculated by the adjustment. Extensive experiments on the TT-150k-genre\ndataset demonstrate the effectiveness of the proposed method towards the\nselection bias. The code is publicly available on:\n\\url{https://github.com/jing-1/DebCM}.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03633v1"
    },
    {
        "title": "Predict-and-Update Network: Audio-Visual Speech Recognition Inspired by\n  Human Speech Perception",
        "authors": [
            "Jiadong Wang",
            "Xinyuan Qian",
            "Haizhou Li"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Audio and visual signals complement each other in human speech perception, so\ndo they in speech recognition. The visual hint is less evident than the\nacoustic hint, but more robust in a complex acoustic environment, as far as\nspeech perception is concerned. It remains a challenge how we effectively\nexploit the interaction between audio and visual signals for automatic speech\nrecognition. There have been studies to exploit visual signals as redundant or\ncomplementary information to audio input in a synchronous manner. Human studies\nsuggest that visual signal primes the listener in advance as to when and on\nwhich frequency to attend to. We propose a Predict-and-Update Network (P&U\nnet), to simulate such a visual cueing mechanism for Audio-Visual Speech\nRecognition (AVSR). In particular, we first predict the character posteriors of\nthe spoken words, i.e. the visual embedding, based on the visual signals. The\naudio signal is then conditioned on the visual embedding via a novel\ncross-modal Conformer, that updates the character posteriors. We validate the\neffectiveness of the visual cueing mechanism through extensive experiments. The\nproposed P&U net outperforms the state-of-the-art AVSR methods on both LRS2-BBC\nand LRS3-BBC datasets, with the relative reduced Word Error Rate (WER)s\nexceeding 10% and 40% under clean and noisy conditions, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.01768v1"
    },
    {
        "title": "CCOM-HuQin: an Annotated Multimodal Chinese Fiddle Performance Dataset",
        "authors": [
            "Yu Zhang",
            "Ziya Zhou",
            "Xiaobing Li",
            "Feng Yu",
            "Maosong Sun"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  HuQin is a family of traditional Chinese bowed string instruments. Playing\ntechniques(PTs) embodied in various playing styles add abundant emotional\ncoloring and aesthetic feelings to HuQin performance. The complex applied\ntechniques make HuQin music a challenging source for fundamental MIR tasks such\nas pitch analysis, transcription and score-audio alignment. In this paper, we\npresent a multimodal performance dataset of HuQin music that contains\naudio-visual recordings of 11,992 single PT clips and 57 annotated musical\npieces of classical excerpts. We systematically describe the HuQin PT taxonomy\nbased on musicological theory and practical use cases. Then we introduce the\ndataset creation methodology and highlight the annotation principles featuring\nPTs. We analyze the statistics in different aspects to demonstrate the variety\nof PTs played in HuQin subcategories and perform preliminary experiments to\nshow the potential applications of the dataset in various MIR tasks and\ncross-cultural music studies. Finally, we propose future work to be extended on\nthe dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.06496v2"
    },
    {
        "title": "AutoLV: Automatic Lecture Video Generator",
        "authors": [
            "Wenbin Wang",
            "Yang Song",
            "Sanjay Jha"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  We propose an end-to-end lecture video generation system that can generate\nrealistic and complete lecture videos directly from annotated slides,\ninstructor's reference voice and instructor's reference portrait video. Our\nsystem is primarily composed of a speech synthesis module with few-shot speaker\nadaptation and an adversarial learning-based talking-head generation module. It\nis capable of not only reducing instructors' workload but also changing the\nlanguage and accent which can help the students follow the lecture more easily\nand enable a wider dissemination of lecture contents. Our experimental results\nshow that the proposed model outperforms other current approaches in terms of\nauthenticity, naturalness and accuracy. Here is a video demonstration of how\nour system works, and the outcomes of the evaluation and comparison:\nhttps://youtu.be/cY6TYkI0cog.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.08795v1"
    },
    {
        "title": "Recipe Generation from Unsegmented Cooking Videos",
        "authors": [
            "Taichi Nishimura",
            "Atsushi Hashimoto",
            "Yoshitaka Ushiku",
            "Hirotaka Kameko",
            "Shinsuke Mori"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  This paper tackles recipe generation from unsegmented cooking videos, a task\nthat requires agents to (1) extract key events in completing the dish and (2)\ngenerate sentences for the extracted events. Our task is similar to dense video\ncaptioning (DVC), which aims at detecting events thoroughly and generating\nsentences for them. However, unlike DVC, in recipe generation, recipe story\nawareness is crucial, and a model should extract an appropriate number of\nevents in the correct order and generate accurate sentences based on them. We\nanalyze the output of the DVC model and confirm that although (1) several\nevents are adoptable as a recipe story, (2) the generated sentences for such\nevents are not grounded in the visual content. Based on this, we set our goal\nto obtain correct recipes by selecting oracle events from the output events and\nre-generating sentences for them. To achieve this, we propose a\ntransformer-based multimodal recurrent approach of training an event selector\nand sentence generator for selecting oracle events from the DVC's events and\ngenerating sentences for them. In addition, we extend the model by including\ningredients to generate more accurate recipes. The experimental results show\nthat the proposed method outperforms state-of-the-art DVC models. We also\nconfirm that, by modeling the recipe in a story-aware manner, the proposed\nmodel outputs the appropriate number of events in the correct order.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.10134v2"
    },
    {
        "title": "Contrastive Audio-Visual Masked Autoencoder",
        "authors": [
            "Yuan Gong",
            "Andrew Rouditchenko",
            "Alexander H. Liu",
            "David Harwath",
            "Leonid Karlinsky",
            "Hilde Kuehne",
            "James Glass"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this paper, we first extend the recent Masked Auto-Encoder (MAE) model\nfrom a single modality to audio-visual multi-modalities. Subsequently, we\npropose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining\ncontrastive learning and masked data modeling, two major self-supervised\nlearning frameworks, to learn a joint and coordinated audio-visual\nrepresentation. Our experiments show that the contrastive audio-visual\ncorrespondence learning objective not only enables the model to perform\naudio-visual retrieval tasks, but also helps the model learn a better joint\nrepresentation. As a result, our fully self-supervised pretrained CAV-MAE\nachieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with the\nprevious best supervised pretrained model on AudioSet in the audio-visual event\nclassification task. Code and pretrained models are at\nhttps://github.com/yuangongnd/cav-mae.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.07839v4"
    },
    {
        "title": "MMGA: Multimodal Learning with Graph Alignment",
        "authors": [
            "Xuan Yang",
            "Quanjin Tao",
            "Xiao Feng",
            "Donghong Cai",
            "Xiang Ren",
            "Yang Yang"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Multimodal pre-training breaks down the modality barriers and allows the\nindividual modalities to be mutually augmented with information, resulting in\nsignificant advances in representation learning. However, graph modality, as a\nvery general and important form of data, cannot be easily interacted with other\nmodalities because of its non-regular nature. In this paper, we propose MMGA\n(Multimodal learning with Graph Alignment), a novel multimodal pre-training\nframework to incorporate information from graph (social network), image and\ntext modalities on social media to enhance user representation learning. In\nMMGA, a multi-step graph alignment mechanism is proposed to add the\nself-supervision from graph modality to optimize the image and text encoders,\nwhile using the information from the image and text modalities to guide the\ngraph encoder learning. We conduct experiments on the dataset crawled from\nInstagram. The experimental results show that MMGA works well on the dataset\nand improves the fans prediction task's performance. We release our dataset,\nthe first social media multimodal dataset with graph, of 60,000 users labeled\nwith specific topics based on 2 million posts to facilitate future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.09946v2"
    },
    {
        "title": "Using Set Covering to Generate Databases for Holistic Steganalysis",
        "authors": [
            "Rony Abecidan",
            "Vincent Itier",
            "Jérémie Boulanger",
            "Patrick Bas",
            "Tomáš Pevný"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Within an operational framework, covers used by a steganographer are likely\nto come from different sensors and different processing pipelines than the ones\nused by researchers for training their steganalysis models. Thus, a performance\ngap is unavoidable when it comes to out-of-distributions covers, an extremely\nfrequent scenario called Cover Source Mismatch (CSM). Here, we explore a grid\nof processing pipelines to study the origins of CSM, to better understand it,\nand to better tackle it. A set-covering greedy algorithm is used to select\nrepresentative pipelines minimizing the maximum regret between the\nrepresentative and the pipelines within the set. Our main contribution is a\nmethodology for generating relevant bases able to tackle operational CSM.\nExperimental validation highlights that, for a given number of training\nsamples, our set covering selection is a better strategy than selecting random\npipelines or using all the available pipelines. Our analysis also shows that\nparameters as denoising, sharpening, and downsampling are very important to\nfoster diversity. Finally, different benchmarks for classical and wild\ndatabases show the good generalization property of the extracted databases.\nAdditional resources are available at\ngithub.com/RonyAbecidan/HolisticSteganalysisWithSetCovering.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.03447v2"
    },
    {
        "title": "Errorless Robust JPEG Steganography using Outputs of JPEG Coders",
        "authors": [
            "Jan Butora",
            "Pauline Puteaux",
            "Patrick Bas"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Robust steganography is a technique of hiding secret messages in images so\nthat the message can be recovered after additional image processing. One of the\nmost popular processing operations is JPEG recompression. Unfortunately, most\nof today's steganographic methods addressing this issue only provide a\nprobabilistic guarantee of recovering the secret and are consequently not\nerrorless. That is unacceptable since even a single unexpected change can make\nthe whole message unreadable if it is encrypted. We propose to create a robust\nset of DCT coefficients by inspecting their behavior during recompression,\nwhich requires access to the targeted JPEG compressor. This is done by dividing\nthe DCT coefficients into 64 non-overlapping lattices because one embedding\nchange can potentially affect many other coefficients from the same DCT block\nduring recompression. The robustness is then combined with standard\nsteganographic costs creating a lattice embedding scheme robust against JPEG\nrecompression. Through experiments, we show that the size of the robust set and\nthe scheme's security depends on the ordering of lattices during embedding. We\nverify the validity of the proposed method with three typical JPEG compressors\nand the {\\it Slack} instant messaging application. We benchmark its security\nfor various embedding payloads, three different ways of ordering the lattices,\nand a range of Quality Factors. Finally, this method is errorless by\nconstruction, meaning the embedded message will always be readable.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.04750v2"
    },
    {
        "title": "Multimodal Dyadic Impression Recognition via Listener Adaptive\n  Cross-Domain Fusion",
        "authors": [
            "Yuanchao Li",
            "Peter Bell",
            "Catherine Lai"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  As a sub-branch of affective computing, impression recognition, e.g.,\nperception of speaker characteristics such as warmth or competence, is\npotentially a critical part of both human-human conversations and spoken\ndialogue systems. Most research has studied impressions only from the behaviors\nexpressed by the speaker or the response from the listener, yet ignored their\nlatent connection. In this paper, we perform impression recognition using a\nproposed listener adaptive cross-domain architecture, which consists of a\nlistener adaptation function to model the causality between speaker and\nlistener behaviors and a cross-domain fusion function to strengthen their\nconnection. The experimental evaluation on the dyadic IMPRESSION dataset\nverified the efficacy of our method, producing concordance correlation\ncoefficients of 78.8% and 77.5% in the competence and warmth dimensions,\noutperforming previous studies. The proposed method is expected to be\ngeneralized to similar dyadic interaction scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.05163v4"
    },
    {
        "title": "Side-Informed Steganography for JPEG Images by Modeling Decompressed\n  Images",
        "authors": [
            "Jan Butora",
            "Patrick Bas"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Side-informed steganography has always been among the most secure approaches\nin the field. However, a majority of existing methods for JPEG images use the\nside information, here the rounding error, in a heuristic way. For the first\ntime, we show that the usefulness of the rounding error comes from its\ncovariance with the embedding changes. Unfortunately, this covariance between\ncontinuous and discrete variables is not analytically available. An estimate of\nthe covariance is proposed, which allows to model steganography as a change in\nthe variance of DCT coefficients. Since steganalysis today is best performed in\nthe spatial domain, we derive a likelihood ratio test to preserve a model of a\ndecompressed JPEG image. The proposed method then bounds the power of this test\nby minimizing the Kullback-Leibler divergence between the cover and stego\ndistributions. We experimentally demonstrate in two popular datasets that it\nachieves state-of-the-art performance against deep learning detectors.\nMoreover, by considering a different pixel variance estimator for images\ncompressed with Quality Factor 100, even greater improvements are obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.05530v1"
    },
    {
        "title": "The Birds Need Attention Too: Analysing usage of Self Attention in\n  identifying bird calls in soundscapes",
        "authors": [
            "Chandra Kanth Nagesh",
            "Abhishek Purushothama"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Birds are vital parts of ecosystems across the world and are an excellent\nmeasure of the quality of life on earth. Many bird species are endangered while\nothers are already extinct. Ecological efforts in understanding and monitoring\nbird populations are important to conserve their habitat and species, but this\nmostly relies on manual methods in rough terrains. Recent advances in Machine\nLearning and Deep Learning have made automatic bird recognition in diverse\nenvironments possible. Birdcall recognition till now has been performed using\nconvolutional neural networks. In this work, we try and understand how\nself-attention can aid in this endeavor. With that we build an pre-trained\nAttention-based Spectrogram Transformer baseline for BirdCLEF 2022 and compare\nthe results against the pre-trained Convolution-based baseline. Our results\nshow that the transformer models outperformed the convolutional model and we\nfurther validate our results by building baselines and analyzing the results\nfor the previous year BirdCLEF 2021 challenge. Source code available at\nhttps://github.com/ck090/BirdCLEF-22\n",
        "pdf_link": "http://arxiv.org/pdf/2211.07722v1"
    },
    {
        "title": "Rate-Distortion Modeling for Bit Rate Constrained Point Cloud\n  Compression",
        "authors": [
            "Pan Gao",
            "Shengzhou Luo",
            "Manoranjan Paul"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  As being one of the main representation formats of 3D real world and\nwell-suited for virtual reality and augmented reality applications, point\nclouds have gained a lot of popularity. In order to reduce the huge amount of\ndata, a considerable amount of research on point cloud compression has been\ndone. However, given a target bit rate, how to properly choose the color and\ngeometry quantization parameters for compressing point clouds is still an open\nissue. In this paper, we propose a rate-distortion model based quantization\nparameter selection scheme for bit rate constrained point cloud compression.\nFirstly, to overcome the measurement uncertainty in evaluating the distortion\nof the point clouds, we propose a unified model to combine the geometry\ndistortion and color distortion. In this model, we take into account the\ncorrelation between geometry and color variables of point clouds and derive a\ndimensionless quantity to represent the overall quality degradation. Then, we\nderive the relationships of overall distortion and bit rate with the\nquantization parameters. Finally, we formulate the bit rate constrained point\ncloud compression as a constrained minimization problem using the derived\npolynomial models and deduce the solution via an iterative numerical method.\nExperimental results show that the proposed algorithm can achieve optimal\ndecoded point cloud quality at various target bit rates, and substantially\noutperform the video-rate-distortion model based point cloud compression\nscheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.10646v1"
    },
    {
        "title": "VarietySound: Timbre-Controllable Video to Sound Generation via\n  Unsupervised Information Disentanglement",
        "authors": [
            "Chenye Cui",
            "Yi Ren",
            "Jinglin Liu",
            "Rongjie Huang",
            "Zhou Zhao"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Video to sound generation aims to generate realistic and natural sound given\na video input. However, previous video-to-sound generation methods can only\ngenerate a random or average timbre without any controls or specializations of\nthe generated sound timbre, leading to the problem that people cannot obtain\nthe desired timbre under these methods sometimes. In this paper, we pose the\ntask of generating sound with a specific timbre given a video input and a\nreference audio sample. To solve this task, we disentangle each target sound\naudio into three components: temporal information, acoustic information, and\nbackground information. We first use three encoders to encode these components\nrespectively: 1) a temporal encoder to encode temporal information, which is\nfed with video frames since the input video shares the same temporal\ninformation as the original audio; 2) an acoustic encoder to encode timbre\ninformation, which takes the original audio as input and discards its temporal\ninformation by a temporal-corrupting operation; and 3) a background encoder to\nencode the residual or background sound, which uses the background part of the\noriginal audio as input. To make the generated result achieve better quality\nand temporal alignment, we also adopt a mel discriminator and a temporal\ndiscriminator for the adversarial training. Our experimental results on the VAS\ndataset demonstrate that our method can generate high-quality audio samples\nwith good synchronization with events in video and high timbre similarity with\nthe reference audio.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.10666v1"
    },
    {
        "title": "Training Data Improvement for Image Forgery Detection using Comprint",
        "authors": [
            "Hannes Mareen",
            "Dante Vanden Bussche",
            "Glenn Van Wallendael",
            "Luisa Verdoliva",
            "Peter Lambert"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Manipulated images are a threat to consumers worldwide, when they are used to\nspread disinformation. Therefore, Comprint enables forgery detection by\nutilizing JPEG-compression fingerprints. This paper evaluates the impact of the\ntraining set on Comprint's performance. Most interestingly, we found that\nincluding images compressed with low quality factors during training does not\nhave a significant effect on the accuracy, whereas incorporating recompression\nboosts the robustness. As such, consumers can use Comprint on their smartphones\nto verify the authenticity of images.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.14079v1"
    },
    {
        "title": "Parameter Efficient Transfer Learning for Various Speech Processing\n  Tasks",
        "authors": [
            "Shinta Otake",
            "Rei Kawakami",
            "Nakamasa Inoue"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Fine-tuning of self-supervised models is a powerful transfer learning method\nin a variety of fields, including speech processing, since it can utilize\ngeneric feature representations obtained from large amounts of unlabeled data.\nFine-tuning, however, requires a new parameter set for each downstream task,\nwhich is parameter inefficient. Adapter architecture is proposed to partially\nsolve this issue by inserting lightweight learnable modules into a frozen\npre-trained model. However, existing adapter architectures fail to adaptively\nleverage low- to high-level features stored in different layers, which is\nnecessary for solving various kinds of speech processing tasks. Thus, we\npropose a new adapter architecture to acquire feature representations more\nflexibly for various speech tasks. In experiments, we applied this adapter to\nWavLM on four speech tasks. It performed on par or better than naive\nfine-tuning, with only 11% of learnable parameters. It also outperformed an\nexisting adapter architecture.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.02780v1"
    },
    {
        "title": "JPEG Steganalysis Based on Steganographic Feature Enhancement and Graph\n  Attention Learning",
        "authors": [
            "Qiyun Liu",
            "Zhiguang Yang",
            "Hanzhou Wu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The purpose of image steganalysis is to determine whether the carrier image\ncontains hidden information or not. Since JEPG is the most commonly used image\nformat over social networks, steganalysis in JPEG images is also the most\nurgently needed to be explored. However, in order to detect whether secret\ninformation is hidden within JEPG images, the majority of existing algorithms\nare designed in conjunction with the popular computer vision related networks,\nwithout considering the key characteristics appeared in image steganalysis. It\nis crucial that the steganographic signal, as an extremely weak signal, can be\nenhanced during its representation learning process. Motivated by this insight,\nin this paper, we introduce a novel representation learning algorithm for JPEG\nsteganalysis that is mainly consisting of a graph attention learning module and\na feature enhancement module. The graph attention learning module is designed\nto avoid global feature loss caused by the local feature learning of\nconvolutional neural network and reliance on depth stacking to extend the\nperceptual domain. The feature enhancement module is applied to prevent the\nstacking of convolutional layers from weakening the steganographic information.\nIn addition, pretraining as a way to initialize the network weights with a\nlarge-scale dataset is utilized to enhance the ability of the network to\nextract discriminative features. We advocate pretraining with ALASKA2 for the\nmodel trained with BOSSBase+BOWS2. The experimental results indicate that the\nproposed algorithm outperforms previous arts in terms of detection accuracy,\nwhich has verified the superiority and applicability of the proposed work.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.02276v1"
    },
    {
        "title": "SSVMR: Saliency-based Self-training for Video-Music Retrieval",
        "authors": [
            "Xuxin Cheng",
            "Zhihong Zhu",
            "Hongxiang Li",
            "Yaowei Li",
            "Yuexian Zou"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  With the rise of short videos, the demand for selecting appropriate\nbackground music (BGM) for a video has increased significantly, video-music\nretrieval (VMR) task gradually draws much attention by research community. As\nother cross-modal learning tasks, existing VMR approaches usually attempt to\nmeasure the similarity between the video and music in the feature space.\nHowever, they (1) neglect the inevitable label noise; (2) neglect to enhance\nthe ability to capture critical video clips. In this paper, we propose a novel\nsaliency-based self-training framework, which is termed SSVMR. Specifically, we\nfirst explore to fully make use of the information containing in the training\ndataset by applying a semi-supervised method to suppress the adverse impact of\nlabel noise problem, where a self-training approach is adopted. In addition, we\npropose to capture the saliency of the video by mixing two videos at span level\nand preserving the locality of the two original videos. Inspired by back\ntranslation in NLP, we also conduct back retrieval to obtain more training\ndata. Experimental results on MVD dataset show that our SSVMR achieves the\nstate-of-the-art performance by a large margin, obtaining a relative\nimprovement of 34.8% over the previous best model in terms of R@1.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.09328v1"
    },
    {
        "title": "Understanding Social Media Cross-Modality Discourse in Linguistic Space",
        "authors": [
            "Chunpu Xu",
            "Hanzhuo Tan",
            "Jing Li",
            "Piji Li"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The multimedia communications with texts and images are popular on social\nmedia. However, limited studies concern how images are structured with texts to\nform coherent meanings in human cognition. To fill in the gap, we present a\nnovel concept of cross-modality discourse, reflecting how human readers couple\nimage and text understandings. Text descriptions are first derived from images\n(named as subtitles) in the multimedia contexts. Five labels -- entity-level\ninsertion, projection and concretization and scene-level restatement and\nextension -- are further employed to shape the structure of subtitles and texts\nand present their joint meanings. As a pilot study, we also build the very\nfirst dataset containing 16K multimedia tweets with manually annotated\ndiscourse labels. The experimental results show that the multimedia encoder\nbased on multi-head attention with captions is able to obtain\nthe-state-of-the-art results.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.13311v1"
    },
    {
        "title": "Synthetic Misinformers: Generating and Combating Multimodal\n  Misinformation",
        "authors": [
            "Stefanos-Iordanis Papadopoulos",
            "Christos Koutlis",
            "Symeon Papadopoulos",
            "Panagiotis C. Petrantonakis"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  With the expansion of social media and the increasing dissemination of\nmultimedia content, the spread of misinformation has become a major concern.\nThis necessitates effective strategies for multimodal misinformation detection\n(MMD) that detect whether the combination of an image and its accompanying text\ncould mislead or misinform. Due to the data-intensive nature of deep neural\nnetworks and the labor-intensive process of manual annotation, researchers have\nbeen exploring various methods for automatically generating synthetic\nmultimodal misinformation - which we refer to as Synthetic Misinformers - in\norder to train MMD models. However, limited evaluation on real-world\nmisinformation and a lack of comparisons with other Synthetic Misinformers\nmakes difficult to assess progress in the field. To address this, we perform a\ncomparative study on existing and new Synthetic Misinformers that involves (1)\nout-of-context (OOC) image-caption pairs, (2) cross-modal named entity\ninconsistency (NEI) as well as (3) hybrid approaches and we evaluate them\nagainst real-world misinformation; using the COSMOS benchmark. The comparative\nstudy showed that our proposed CLIP-based Named Entity Swapping can lead to MMD\nmodels that surpass other OOC and NEI Misinformers in terms of multimodal\naccuracy and that hybrid approaches can lead to even higher detection accuracy.\nNevertheless, after alleviating information leakage from the COSMOS evaluation\nprotocol, low Sensitivity scores indicate that the task is significantly more\nchallenging than previous studies suggested. Finally, our findings showed that\nNEI-based Synthetic Misinformers tend to suffer from a unimodal bias, where\ntext-only MMDs can outperform multimodal ones.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.01217v1"
    },
    {
        "title": "IPA-CLIP: Integrating Phonetic Priors into Vision and Language\n  Pretraining",
        "authors": [
            "Chihaya Matsuhira",
            "Marc A. Kastner",
            "Takahiro Komamizu",
            "Takatsugu Hirayama",
            "Keisuke Doman",
            "Yasutomo Kawanishi",
            "Ichiro Ide"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Recently, large-scale Vision and Language (V\\&L) pretraining has become the\nstandard backbone of many multimedia systems. While it has shown remarkable\nperformance even in unseen situations, it often performs in ways not intuitive\nto humans. Particularly, they usually do not consider the pronunciation of the\ninput, which humans would utilize to understand language, especially when it\ncomes to unknown words. Thus, this paper inserts phonetic prior into\nContrastive Language-Image Pretraining (CLIP), one of the V\\&L pretrained\nmodels, to make it consider the pronunciation similarity among its\npronunciation inputs. To achieve this, we first propose a phoneme embedding\nthat utilizes the phoneme relationships provided by the International Phonetic\nAlphabet (IPA) chart as a phonetic prior. Next, by distilling the frozen CLIP\ntext encoder, we train a pronunciation encoder employing the IPA-based\nembedding. The proposed model named IPA-CLIP comprises this pronunciation\nencoder and the original CLIP encoders (image and text). Quantitative\nevaluation reveals that the phoneme distribution on the embedding space\nrepresents phonetic relationships more accurately when using the proposed\nphoneme embedding. Furthermore, in some multimodal retrieval tasks, we confirm\nthat the proposed pronunciation encoder enhances the performance of the text\nencoder and that the pronunciation encoder handles nonsense words in a more\nphonetic manner than the text encoder. Finally, qualitative evaluation verifies\nthe correlation between the pronunciation encoder and human perception\nregarding pronunciation similarity.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.03144v1"
    },
    {
        "title": "FSVVD: A Dataset of Full Scene Volumetric Video",
        "authors": [
            "Kaiyuan Hu",
            "Yili Jin",
            "Haowen Yang",
            "Junhua Liu",
            "Fangxin Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Recent years have witnessed a rapid development of immersive multimedia which\nbridges the gap between the real world and virtual space. Volumetric videos, as\nan emerging representative 3D video paradigm that empowers extended reality,\nstand out to provide unprecedented immersive and interactive video watching\nexperience. Despite the tremendous potential, the research towards 3D\nvolumetric video is still in its infancy, relying on sufficient and complete\ndatasets for further exploration. However, existing related volumetric video\ndatasets mostly only include a single object, lacking details about the scene\nand the interaction between them. In this paper, we focus on the current most\nwidely used data format, point cloud, and for the first time release a\nfull-scene volumetric video dataset that includes multiple people and their\ndaily activities interacting with the external environments. Comprehensive\ndataset description and analysis are conducted, with potential usage of this\ndataset. The dataset and additional tools can be accessed via the following\nwebsite: https://cuhksz-inml.github.io/full_scene_volumetric_video_dataset/.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.03599v2"
    },
    {
        "title": "Investigating the Characteristics and Performance of Augmented Reality\n  Applications on Head-Mounted Displays: A Study of the Hololens Application\n  Store",
        "authors": [
            "Pubudu Wijesooriya",
            "Sheikh Muhammad Farjad",
            "Nikolaos Stergiou",
            "Spyridon Mastorakis"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Augmented Reality (AR) based on Head-Mounted Displays (HMDs) has gained\nsignificant traction over the recent years. Nevertheless, it remains unclear\nwhat AR HMD-based applications have been developed over the years and what\ntheir system performance is when they are run on HMDs. In this paper, we aim to\nshed light into this direction. Our study focuses on the applications available\non the Microsoft Hololens application store given the wide use of the Hololens\nheadset. Our study has two major parts: (i) we collect metadata about the\napplications available on the Microsoft Hololens application store to\nunderstand their characteristics (e.g., categories, pricing, permissions\nrequested, hardware and software compatibility); and (ii) we interact with\nthese applications while running on a Hololens 2 headset and collect data about\nsystems-related metrics (e.g., memory and storage usage, time spent on CPU and\nGPU related operations) to investigate the systems performance of applications.\nOur study has resulted in several interesting findings, which we share with the\nresearch community.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.07523v1"
    },
    {
        "title": "Multimodal Continuous Emotion Recognition: A Technical Report for ABAW5",
        "authors": [
            "Su Zhang",
            "Ziyuan Zhao",
            "Cuntai Guan"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  We used two multimodal models for continuous valence-arousal recognition\nusing visual, audio, and linguistic information. The first model is the same as\nwe used in ABAW2 and ABAW3, which employs the leader-follower attention. The\nsecond model has the same architecture for spatial and temporal encoding. As\nfor the fusion block, it employs a compact and straightforward channel\nattention, borrowed from the End2You toolkit. Unlike our previous attempts that\nuse Vggish feature directly as the audio feature, this time we feed the\npre-trained VGG model using logmel-spectrogram and finetune it during the\ntraining. To make full use of the data and alleviate over-fitting,\ncross-validation is carried out. The code is available at\nhttps://github.com/sucv/ABAW3.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10335v2"
    },
    {
        "title": "Music-Driven Group Choreography",
        "authors": [
            "Nhat Le",
            "Thang Pham",
            "Tuong Do",
            "Erman Tjiputra",
            "Quang D. Tran",
            "Anh Nguyen"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Music-driven choreography is a challenging problem with a wide variety of\nindustrial applications. Recently, many methods have been proposed to\nsynthesize dance motions from music for a single dancer. However, generating\ndance motion for a group remains an open problem. In this paper, we present\n$\\rm AIOZ-GDANCE$, a new large-scale dataset for music-driven group dance\ngeneration. Unlike existing datasets that only support single dance, our new\ndataset contains group dance videos, hence supporting the study of group\nchoreography. We propose a semi-autonomous labeling method with humans in the\nloop to obtain the 3D ground truth for our dataset. The proposed dataset\nconsists of 16.7 hours of paired music and 3D motion from in-the-wild videos,\ncovering 7 dance styles and 16 music genres. We show that naively applying\nsingle dance generation technique to creating group dance motion may lead to\nunsatisfactory results, such as inconsistent movements and collisions between\ndancers. Based on our new dataset, we propose a new method that takes an input\nmusic sequence and a set of 3D positions of dancers to efficiently produce\nmultiple group-coherent choreographies. We propose new evaluation metrics for\nmeasuring group dance quality and perform intensive experiments to demonstrate\nthe effectiveness of our method. Our project facilitates future research on\ngroup dance generation and is available at:\nhttps://aioz-ai.github.io/AIOZ-GDANCE/\n",
        "pdf_link": "http://arxiv.org/pdf/2303.12337v2"
    },
    {
        "title": "Latency Target based Analysis of the DASH.js Player",
        "authors": [
            "Piers O'Hanlon",
            "Adil Aslam"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  We analyse the low latency performance of the three Adaptive Bitrate (ABR)\nalgorithms in the dash.js Dynamic Adaptive Streaming over HTTP (DASH) player\nwith respect to a range of latency targets and configuration options. We\nperform experiments on our DASH Testbed which allows for testing with a range\nof real world derived network profiles. Our experiments enable a better\nunderstanding of how latency targets affect quality of experience (QoE), and\nhow well the different algorithms adhere to their targets. We find that with\ndash.js v4.5.0 the default Dynamic algorithm achieves the best overall QoE. We\nshow that whilst the other algorithms can achieve higher video quality at lower\nlatencies, they do so only at the expense of increased stalling. We analyse the\npoor performance of L2A-LL in our tests and develop modifications which\ndemonstrate significant improvements. We also highlight how some low latency\nconfiguration settings can be detrimental to performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13551v1"
    },
    {
        "title": "Interpretability of Machine Learning: Recent Advances and Future\n  Prospects",
        "authors": [
            "Lei Gao",
            "Ling Guan"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The proliferation of machine learning (ML) has drawn unprecedented interest\nin the study of various multimedia contents such as text, image, audio and\nvideo, among others. Consequently, understanding and learning ML-based\nrepresentations have taken center stage in knowledge discovery in intelligent\nmultimedia research and applications. Nevertheless, the black-box nature of\ncontemporary ML, especially in deep neural networks (DNNs), has posed a primary\nchallenge for ML-based representation learning. To address this black-box\nproblem, the studies on interpretability of ML have attracted tremendous\ninterests in recent years. This paper presents a survey on recent advances and\nfuture prospects on interpretability of ML, with several application examples\npertinent to multimedia computing, including text-image cross-modal\nrepresentation learning, face recognition, and the recognition of objects. It\nis evidently shown that the study of interpretability of ML promises an\nimportant research direction, one which is worth further investment in.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.00537v1"
    },
    {
        "title": "Learn how to Prune Pixels for Multi-view Neural Image-based Synthesis",
        "authors": [
            "Marta Milovanović",
            "Enzo Tartaglione",
            "Marco Cagnazzo",
            "Félix Henry"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Image-based rendering techniques stand at the core of an immersive experience\nfor the user, as they generate novel views given a set of multiple input\nimages. Since they have shown good performance in terms of objective and\nsubjective quality, the research community devotes great effort to their\nimprovement. However, the large volume of data necessary to render at the\nreceiver's side hinders applications in limited bandwidth environments or\nprevents their employment in real-time applications. We present LeHoPP, a\nmethod for input pixel pruning, where we examine the importance of each input\npixel concerning the rendered view, and we avoid the use of irrelevant pixels.\nEven without retraining the image-based rendering network, our approach shows a\ngood trade-off between synthesis quality and pixel rate. When tested in the\ngeneral neural rendering framework, compared to other pruning baselines, LeHoPP\ngains between $0.9$ dB and $3.6$ dB on average.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03572v1"
    },
    {
        "title": "Interpretable Multimodal Misinformation Detection with Logic Reasoning",
        "authors": [
            "Hui Liu",
            "Wenya Wang",
            "Haoliang Li"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Multimodal misinformation on online social platforms is becoming a critical\nconcern due to increasing credibility and easier dissemination brought by\nmultimedia content, compared to traditional text-only information. While\nexisting multimodal detection approaches have achieved high performance, the\nlack of interpretability hinders these systems' reliability and practical\ndeployment. Inspired by NeuralSymbolic AI which combines the learning ability\nof neural networks with the explainability of symbolic learning, we propose a\nnovel logic-based neural model for multimodal misinformation detection which\nintegrates interpretable logic clauses to express the reasoning process of the\ntarget task. To make learning effective, we parameterize symbolic logical\nelements using neural representations, which facilitate the automatic\ngeneration and evaluation of meaningful logic clauses. Additionally, to make\nour framework generalizable across diverse misinformation sources, we introduce\nfive meta-predicates that can be instantiated with different correlations.\nResults on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the\nfeasibility and versatility of our model.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.05964v2"
    },
    {
        "title": "GRACE: Loss-Resilient Real-Time Video through Neural Codecs",
        "authors": [
            "Yihua Cheng",
            "Ziyi Zhang",
            "Hanchen Li",
            "Anton Arapin",
            "Yue Zhang",
            "Qizheng Zhang",
            "Yuhan Liu",
            "Xu Zhang",
            "Francis Y. Yan",
            "Amrita Mazumdar",
            "Nick Feamster",
            "Junchen Jiang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  In real-time video communication, retransmitting lost packets over\nhigh-latency networks is not viable due to strict latency requirements. To\ncounter packet losses without retransmission, two primary strategies are\nemployed -- encoder-based forward error correction (FEC) and decoder-based\nerror concealment. The former encodes data with redundancy before transmission,\nyet determining the optimal redundancy level in advance proves challenging. The\nlatter reconstructs video from partially received frames, but dividing a frame\ninto independently coded partitions inherently compromises compression\nefficiency, and the lost information cannot be effectively recovered by the\ndecoder without adapting the encoder. We present a loss-resilient real-time\nvideo system called GRACE, which preserves the user's quality of experience\n(QoE) across a wide range of packet losses through a new neural video codec.\nCentral to GRACE's enhanced loss resilience is its joint training of the neural\nencoder and decoder under a spectrum of simulated packet losses. In lossless\nscenarios, GRACE achieves video quality on par with conventional codecs (e.g.,\nH.265). As the loss rate escalates, GRACE exhibits a more graceful, less\npronounced decline in quality, consistently outperforming other loss-resilient\nschemes. Through extensive evaluation on various videos and real network\ntraces, we demonstrate that GRACE reduces undecodable frames by 95% and stall\nduration by 90% compared with FEC, while markedly boosting video quality over\nerror concealment methods. In a user study with 240 crowdsourced participants\nand 960 subjective ratings, GRACE registers a 38% higher mean opinion score\n(MOS) than other baselines.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.12333v4"
    },
    {
        "title": "CPNet: Exploiting CLIP-based Attention Condenser and Probability Map\n  Guidance for High-fidelity Talking Face Generation",
        "authors": [
            "Jingning Xu",
            "Benlai Tang",
            "Mingjie Wang",
            "Minghao Li",
            "Meirong Ma"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Recently, talking face generation has drawn ever-increasing attention from\nthe research community in computer vision due to its arduous challenges and\nwidespread application scenarios, e.g. movie animation and virtual anchor.\nAlthough persevering efforts have been undertaken to enhance the fidelity and\nlip-sync quality of generated talking face videos, there is still large room\nfor further improvements of synthesis quality and efficiency. Actually, these\nattempts somewhat ignore the explorations of fine-granularity feature\nextraction/integration and the consistency between probability distributions of\nlandmarks, thereby recurring the issues of local details blurring and degraded\nfidelity. To mitigate these dilemmas, in this paper, a novel CLIP-based\nAttention and Probability Map Guided Network (CPNet) is delicately designed for\ninferring high-fidelity talking face videos. Specifically, considering the\ndemands of fine-grained feature recalibration, a clip-based attention condenser\nis exploited to transfer knowledge with rich semantic priors from the\nprevailing CLIP model. Moreover, to guarantee the consistency in probability\nspace and suppress the landmark ambiguity, we creatively propose the density\nmap of facial landmark as auxiliary supervisory signal to guide the landmark\ndistribution learning of generated frame. Extensive experiments on the\nwidely-used benchmark dataset demonstrate the superiority of our CPNet against\nstate of the arts in terms of image and lip-sync quality. In addition, a cohort\nof studies are also conducted to ablate the impacts of the individual pivotal\ncomponents.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.13962v1"
    },
    {
        "title": "Video Compression with Arbitrary Rescaling Network",
        "authors": [
            "Mengxi Guo",
            "Shijie Zhao",
            "Hao Jiang",
            "Junlin Li",
            "Li Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Most video platforms provide video streaming services with different\nqualities, and the quality of the services is usually adjusted by the\nresolution of the videos. So high-resolution videos need to be downsampled for\ncompression. In order to solve the problem of video coding at different\nresolutions, we propose a rate-guided arbitrary rescaling network (RARN) for\nvideo resizing before encoding. To help the RARN be compatible with standard\ncodecs and generate compression-friendly results, an iteratively optimized\ntransformer-based virtual codec (TVC) is introduced to simulate the key\ncomponents of video encoding and perform bitrate estimation. By iteratively\ntraining the TVC and the RARN, we achieved 5%-29% BD-Rate reduction anchored by\nlinear interpolation under different encoding configurations and resolutions,\nexceeding the previous methods on most test videos. Furthermore, the\nlightweight RARN structure can process FHD (1080p) content at real-time speed\n(91 FPS) and obtain a considerable rate reduction.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.04202v1"
    },
    {
        "title": "DiffWA: Diffusion Models for Watermark Attack",
        "authors": [
            "Xinyu Li"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  With the rapid development of deep neural networks(DNNs), many robust blind\nwatermarking algorithms and frameworks have been proposed and achieved good\nresults. At present, the watermark attack algorithm can not compete with the\nwatermark addition algorithm. And many watermark attack algorithms only care\nabout interfering with the normal extraction of the watermark, and the\nwatermark attack will cause great visual loss to the image. To this end, we\npropose DiffWA, a conditional diffusion model with distance guidance for\nwatermark attack, which can restore the image while removing the embedded\nwatermark. The core of our method is training an image-to-image conditional\ndiffusion model on unwatermarked images and guiding the conditional model using\na distance guidance when sampling so that the model will generate unwatermarked\nimages which is similar to original images. We conducted experiments on\nCIFAR-10 using our proposed models. The results shows that the model can remove\nthe watermark with good effect and make the bit error rate of watermark\nextraction higher than 0.4. At the same time, the attacked image will maintain\ngood visual effect with PSNR more than 31 and SSIM more than 0.97 compared with\nthe original image.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12790v1"
    },
    {
        "title": "TACOformer:Token-channel compounded Cross Attention for Multimodal\n  Emotion Recognition",
        "authors": [
            "Xinda Li"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Recently, emotion recognition based on physiological signals has emerged as a\nfield with intensive research. The utilization of multi-modal, multi-channel\nphysiological signals has significantly improved the performance of emotion\nrecognition systems, due to their complementarity. However, effectively\nintegrating emotion-related semantic information from different modalities and\ncapturing inter-modal dependencies remains a challenging issue. Many existing\nmultimodal fusion methods ignore either token-to-token or channel-to-channel\ncorrelations of multichannel signals from different modalities, which limits\nthe classification capability of the models to some extent. In this paper, we\npropose a comprehensive perspective of multimodal fusion that integrates\nchannel-level and token-level cross-modal interactions. Specifically, we\nintroduce a unified cross attention module called Token-chAnnel COmpound (TACO)\nCross Attention to perform multimodal fusion, which simultaneously models\nchannel-level and token-level dependencies between modalities. Additionally, we\npropose a 2D position encoding method to preserve information about the spatial\ndistribution of EEG signal channels, then we use two transformer encoders ahead\nof the fusion module to capture long-term temporal dependencies from the EEG\nsignal and the peripheral physiological signal, respectively.\nSubject-independent experiments on emotional dataset DEAP and Dreamer\ndemonstrate that the proposed model achieves state-of-the-art performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.13592v2"
    },
    {
        "title": "AV-SepFormer: Cross-Attention SepFormer for Audio-Visual Target Speaker\n  Extraction",
        "authors": [
            "Jiuxin Lin",
            "Xinyu Cai",
            "Heinrich Dinkel",
            "Jun Chen",
            "Zhiyong Yan",
            "Yongqing Wang",
            "Junbo Zhang",
            "Zhiyong Wu",
            "Yujun Wang",
            "Helen Meng"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Visual information can serve as an effective cue for target speaker\nextraction (TSE) and is vital to improving extraction performance. In this\npaper, we propose AV-SepFormer, a SepFormer-based attention dual-scale model\nthat utilizes cross- and self-attention to fuse and model features from audio\nand visual. AV-SepFormer splits the audio feature into a number of chunks,\nequivalent to the length of the visual feature. Then self- and cross-attention\nare employed to model and fuse the multi-modal features. Furthermore, we use a\nnovel 2D positional encoding, that introduces the positional information\nbetween and within chunks and provides significant gains over the traditional\npositional encoding. Our model has two key advantages: the time granularity of\naudio chunked feature is synchronized to the visual feature, which alleviates\nthe harm caused by the inconsistency of audio and video sampling rate; by\ncombining self- and cross-attention, feature fusion and speech extraction\nprocesses are unified within an attention paradigm. The experimental results\nshow that AV-SepFormer significantly outperforms other existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.14170v1"
    },
    {
        "title": "Classification of Infant Sleep/Wake States: Cross-Attention among Large\n  Scale Pretrained Transformer Networks using Audio, ECG, and IMU Data",
        "authors": [
            "Kai Chieh Chang",
            "Mark Hasegawa-Johnson",
            "Nancy L. McElwain",
            "Bashima Islam"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Infant sleep is critical to brain and behavioral development. Prior studies\non infant sleep/wake classification have been largely limited to reliance on\nexpensive and burdensome polysomnography (PSG) tests in the laboratory or\nwearable devices that collect single-modality data. To facilitate data\ncollection and accuracy of detection, we aimed to advance this field of study\nby using a multi-modal wearable device, LittleBeats (LB), to collect audio,\nelectrocardiogram (ECG), and inertial measurement unit (IMU) data among a\ncohort of 28 infants. We employed a 3-branch (audio/ECG/IMU) large scale\ntransformer-based neural network (NN) to demonstrate the potential of such\nmulti-modal data. We pretrained each branch independently with its respective\nmodality, then finetuned the model by fusing the pretrained transformer layers\nwith cross-attention. We show that multi-modal data significantly improves\nsleep/wake classification (accuracy = 0.880), compared with use of a single\nmodality (accuracy = 0.732). Our approach to multi-modal mid-level fusion may\nbe adaptable to a diverse range of architectures and tasks, expanding future\ndirections of infant behavioral research.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.15808v1"
    },
    {
        "title": "A Unified Framework for Modality-Agnostic Deepfakes Detection",
        "authors": [
            "Cai Yu",
            "Peng Chen",
            "Jiahe Tian",
            "Jin Liu",
            "Jiao Dai",
            "Xi Wang",
            "Yesheng Chai",
            "Shan Jia",
            "Siwei Lyu",
            "Jizhong Han"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  As AI-generated content (AIGC) thrives, deepfakes have expanded from\nsingle-modality falsification to cross-modal fake content creation, where\neither audio or visual components can be manipulated. While using two unimodal\ndetectors can detect audio-visual deepfakes, cross-modal forgery clues could be\noverlooked. Existing multimodal deepfake detection methods typically establish\ncorrespondence between the audio and visual modalities for binary real/fake\nclassification, and require the co-occurrence of both modalities. However, in\nreal-world multi-modal applications, missing modality scenarios may occur where\neither modality is unavailable. In such cases, audio-visual detection methods\nare less practical than two independent unimodal methods. Consequently, the\ndetector can not always obtain the number or type of manipulated modalities\nbeforehand, necessitating a fake-modality-agnostic audio-visual detector. In\nthis work, we introduce a comprehensive framework that is agnostic to fake\nmodalities, which facilitates the identification of multimodal deepfakes and\nhandles situations with missing modalities, regardless of the manipulations\nembedded in audio, video, or even cross-modal forms. To enhance the modeling of\ncross-modal forgery clues, we employ audio-visual speech recognition (AVSR) as\na preliminary task. This efficiently extracts speech correlations across\nmodalities, a feature challenging for deepfakes to replicate. Additionally, we\npropose a dual-label detection approach that follows the structure of AVSR to\nsupport the independent detection of each modality. Extensive experiments on\nthree audio-visual datasets show that our scheme outperforms state-of-the-art\ndetection methods with promising performance on modality-agnostic audio/video\ndeepfakes.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14491v2"
    },
    {
        "title": "Self-Supervised Visual Acoustic Matching",
        "authors": [
            "Arjun Somayazulu",
            "Changan Chen",
            "Kristen Grauman"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Acoustic matching aims to re-synthesize an audio clip to sound as if it were\nrecorded in a target acoustic environment. Existing methods assume access to\npaired training data, where the audio is observed in both source and target\nenvironments, but this limits the diversity of training data or requires the\nuse of simulated data or heuristics to create paired samples. We propose a\nself-supervised approach to visual acoustic matching where training samples\ninclude only the target scene image and audio -- without acoustically\nmismatched source audio for reference. Our approach jointly learns to\ndisentangle room acoustics and re-synthesize audio into the target environment,\nvia a conditional GAN framework and a novel metric that quantifies the level of\nresidual acoustic information in the de-biased audio. Training with either\nin-the-wild web data or simulated data, we demonstrate it outperforms the\nstate-of-the-art on multiple challenging datasets and a wide variety of\nreal-world audio and environments.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15064v2"
    },
    {
        "title": "Improving Social Media Popularity Prediction with Multiple Post\n  Dependencies",
        "authors": [
            "Zhizhen Zhang",
            "Xiaohui Xie",
            "Mengyu Yang",
            "Ye Tian",
            "Yong Jiang",
            "Yong Cui"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Social Media Popularity Prediction has drawn a lot of attention because of\nits profound impact on many different applications, such as recommendation\nsystems and multimedia advertising. Despite recent efforts to leverage the\ncontent of social media posts to improve prediction accuracy, many existing\nmodels fail to fully exploit the multiple dependencies between posts, which are\nimportant to comprehensively extract content information from posts. To tackle\nthis problem, we propose a novel prediction framework named Dependency-aware\nSequence Network (DSN) that exploits both intra- and inter-post dependencies.\nFor intra-post dependency, DSN adopts a multimodal feature extractor with an\nefficient fine-tuning strategy to obtain task-specific representations from\nimages and textual information of posts. For inter-post dependency, DSN uses a\nhierarchical information propagation method to learn category representations\nthat could better describe the difference between posts. DSN also exploits\nrecurrent networks with a series of gating layers for more flexible local\ntemporal processing abilities and multi-head attention for long-term\ndependencies. The experimental results on the Social Media Popularity Dataset\ndemonstrate the superiority of our method compared to existing state-of-the-art\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15413v1"
    },
    {
        "title": "Emotion recognition based on multi-modal electrophysiology multi-head\n  attention Contrastive Learning",
        "authors": [
            "Yunfei Guo",
            "Tao Zhang",
            "Wu Huang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Emotion recognition is an important research direction in artificial\nintelligence, helping machines understand and adapt to human emotional states.\nMultimodal electrophysiological(ME) signals, such as EEG, GSR,\nrespiration(Resp), and temperature(Temp), are effective biomarkers for\nreflecting changes in human emotions. However, using electrophysiological\nsignals for emotion recognition faces challenges such as data scarcity,\ninconsistent labeling, and difficulty in cross-individual generalization. To\naddress these issues, we propose ME-MHACL, a self-supervised contrastive\nlearning-based multimodal emotion recognition method that can learn meaningful\nfeature representations from unlabeled electrophysiological signals and use\nmulti-head attention mechanisms for feature fusion to improve recognition\nperformance. Our method includes two stages: first, we use the Meiosis method\nto group sample and augment unlabeled electrophysiological signals and design a\nself-supervised contrastive learning task; second, we apply the trained feature\nextractor to labeled electrophysiological signals and use multi-head attention\nmechanisms for feature fusion. We conducted experiments on two public datasets,\nDEAP and MAHNOB-HCI, and our method outperformed existing benchmark methods in\nemotion recognition tasks and had good cross-individual generalization ability.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01919v1"
    },
    {
        "title": "A Brief Yet In-Depth Survey of Deep Learning-Based Image Watermarking",
        "authors": [
            "Xin Zhong",
            "Arjon Das",
            "Fahad Alrasheedi",
            "Abdullah Tanvir"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  This paper presents a comprehensive survey on deep learning-based image\nwatermarking, a technique that entails the invisible embedding and extraction\nof watermarks within a cover image, aiming to offer a seamless blend of\nrobustness and adaptability. We navigate the complex landscape of this\ninterdisciplinary domain, linking historical foundations, current innovations,\nand prospective developments. Unlike existing literature, our study\nconcentrates exclusively on image watermarking with deep learning, delivering\nan in-depth, yet brief analysis enriched by three fundamental contributions.\nFirst, we introduce a refined categorization, segmenting the field into\nEmbedder-Extractor, Deep Networks as a Feature Transformation, and Hybrid\nMethods. This taxonomy, inspired by the varied roles of deep learning across\nstudies, is designed to infuse clarity, offering readers technical insights and\ndirectional guidance. Second, our exploration dives into representative\nmethodologies, encapsulating the diverse research directions and inherent\nchallenges within each category to provide a consolidated perspective. Lastly,\nwe venture beyond established boundaries to outline emerging frontiers,\noffering a detailed insight into prospective research avenues.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.04603v3"
    },
    {
        "title": "Audio-Visual Spatial Integration and Recursive Attention for Robust\n  Sound Source Localization",
        "authors": [
            "Sung Jin Um",
            "Dongjin Kim",
            "Jung Uk Kim"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The objective of the sound source localization task is to enable machines to\ndetect the location of sound-making objects within a visual scene. While the\naudio modality provides spatial cues to locate the sound source, existing\napproaches only use audio as an auxiliary role to compare spatial regions of\nthe visual modality. Humans, on the other hand, utilize both audio and visual\nmodalities as spatial cues to locate sound sources. In this paper, we propose\nan audio-visual spatial integration network that integrates spatial cues from\nboth modalities to mimic human behavior when detecting sound-making objects.\nAdditionally, we introduce a recursive attention network to mimic human\nbehavior of iterative focusing on objects, resulting in more accurate attention\nregions. To effectively encode spatial information from both modalities, we\npropose audio-visual pair matching loss and spatial region alignment loss. By\nutilizing the spatial cues of audio-visual modalities and recursively focusing\nobjects, our method can perform more robust sound source localization.\nComprehensive experimental results on the Flickr SoundNet and VGG-Sound Source\ndatasets demonstrate the superiority of our proposed method over existing\napproaches. Our code is available at: https://github.com/VisualAIKHU/SIRA-SSL\n",
        "pdf_link": "http://arxiv.org/pdf/2308.06087v2"
    },
    {
        "title": "WMFormer++: Nested Transformer for Visible Watermark Removal via Implict\n  Joint Learning",
        "authors": [
            "Dongjian Huo",
            "Zehong Zhang",
            "Hanjing Su",
            "Guanbin Li",
            "Chaowei Fang",
            "Qingyao Wu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Watermarking serves as a widely adopted approach to safeguard media\ncopyright. In parallel, the research focus has extended to watermark removal\ntechniques, offering an adversarial means to enhance watermark robustness and\nfoster advancements in the watermarking field. Existing watermark removal\nmethods mainly rely on UNet with task-specific decoder branches--one for\nwatermark localization and the other for background image restoration. However,\nwatermark localization and background restoration are not isolated tasks;\nprecise watermark localization inherently implies regions necessitating\nrestoration, and the background restoration process contributes to more\naccurate watermark localization. To holistically integrate information from\nboth branches, we introduce an implicit joint learning paradigm. This empowers\nthe network to autonomously navigate the flow of information between implicit\nbranches through a gate mechanism. Furthermore, we employ cross-channel\nattention to facilitate local detail restoration and holistic structural\ncomprehension, while harnessing nested structures to integrate multi-scale\ninformation. Extensive experiments are conducted on various challenging\nbenchmarks to validate the effectiveness of our proposed method. The results\ndemonstrate our approach's remarkable superiority, surpassing existing\nstate-of-the-art methods by a large margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.10195v2"
    },
    {
        "title": "Hey That's Mine Imperceptible Watermarks are Preserved in Diffusion\n  Generated Outputs",
        "authors": [
            "Luke Ditria",
            "Tom Drummond"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Generative models have seen an explosion in popularity with the release of\nhuge generative Diffusion models like Midjourney and Stable Diffusion to the\npublic. Because of this new ease of access, questions surrounding the automated\ncollection of data and issues regarding content ownership have started to\nbuild. In this paper we present new work which aims to provide ways of\nprotecting content when shared to the public. We show that a generative\nDiffusion model trained on data that has been imperceptibly watermarked will\ngenerate new images with these watermarks present. We further show that if a\ngiven watermark is correlated with a certain feature of the training data, the\ngenerated images will also have this correlation. Using statistical tests we\nshow that we are able to determine whether a model has been trained on marked\ndata, and what data was marked. As a result our system offers a solution to\nprotect intellectual property when sharing content online.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.11123v2"
    },
    {
        "title": "Emotion-Aligned Contrastive Learning Between Images and Music",
        "authors": [
            "Shanti Stewart",
            "Kleanthis Avramidis",
            "Tiantian Feng",
            "Shrikanth Narayanan"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Traditional music search engines rely on retrieval methods that match natural\nlanguage queries with music metadata. There have been increasing efforts to\nexpand retrieval methods to consider the audio characteristics of music itself,\nusing queries of various modalities including text, video, and speech. While\nmost approaches aim to match general music semantics to the input queries, only\na few focus on affective qualities. In this work, we address the task of\nretrieving emotionally-relevant music from image queries by learning an\naffective alignment between images and music audio. Our approach focuses on\nlearning an emotion-aligned joint embedding space between images and music.\nThis embedding space is learned via emotion-supervised contrastive learning,\nusing an adapted cross-modal version of the SupCon loss. We evaluate the joint\nembeddings through cross-modal retrieval tasks (image-to-music and\nmusic-to-image) based on emotion labels. Furthermore, we investigate the\ngeneralizability of the learned music embeddings via automatic music tagging.\nOur experiments show that the proposed approach successfully aligns images and\nmusic, and that the learned embedding space is effective for cross-modal\nretrieval applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.12610v3"
    },
    {
        "title": "Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language\n  Pretraining?",
        "authors": [
            "Fei Wang",
            "Liang Ding",
            "Jun Rao",
            "Ye Liu",
            "Li Shen",
            "Changxing Ding"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The multimedia community has shown a significant interest in perceiving and\nrepresenting the physical world with multimodal pretrained neural network\nmodels, and among them, the visual-language pertaining (VLP) is, currently, the\nmost captivating topic. However, there have been few endeavors dedicated to the\nexploration of 1) whether essential linguistic knowledge (e.g., semantics and\nsyntax) can be extracted during VLP, and 2) how such linguistic knowledge\nimpact or enhance the multimodal alignment. In response, here we aim to\nelucidate the impact of comprehensive linguistic knowledge, including semantic\nexpression and syntactic structure, on multimodal alignment. Specifically, we\ndesign and release the SNARE, the first large-scale multimodal alignment\nprobing benchmark, to detect the vital linguistic components, e.g., lexical,\nsemantic, and syntax knowledge, containing four tasks: Semantic structure,\nNegation logic, Attribute ownership, and Relationship composition. Based on our\nproposed probing benchmarks, our holistic analyses of five advanced VLP models\nillustrate that the VLP model: i) shows insensitivity towards complex syntax\nstructures and relies on content words for sentence comprehension; ii)\ndemonstrates limited comprehension of combinations between sentences and\nnegations; iii) faces challenges in determining the presence of actions or\nspatial relationships within visual information and struggles with verifying\nthe correctness of triple combinations. We make our benchmark and code\navailable at \\url{https://github.com/WangFei-2019/SNARE/}.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.12898v2"
    },
    {
        "title": "From Capture to Display: A Survey on Volumetric Video",
        "authors": [
            "Yili Jin",
            "Kaiyuan Hu",
            "Junhua Liu",
            "Fangxin Wang",
            "Xue Liu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Volumetric video, which offers immersive viewing experiences, is gaining\nincreasing prominence. With its six degrees of freedom, it provides viewers\nwith greater immersion and interactivity compared to traditional videos.\nDespite their potential, volumetric video services pose significant challenges.\nThis survey conducts a comprehensive review of the existing literature on\nvolumetric video. We firstly provide a general framework of volumetric video\nservices, followed by a discussion on prerequisites for volumetric video,\nencompassing representations, open datasets, and quality assessment metrics.\nThen we delve into the current methodologies for each stage of the volumetric\nvideo service pipeline, detailing capturing, compression, transmission,\nrendering, and display techniques. Lastly, we explore various applications\nenabled by this pioneering technology and we present an array of research\nchallenges and opportunities in the domain of volumetric video services. This\nsurvey aspires to provide a holistic understanding of this burgeoning field and\nshed light on potential future research trajectories, aiming to bring the\nvision of volumetric video to fruition.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.05658v2"
    },
    {
        "title": "Audio Visual Speaker Localization from EgoCentric Views",
        "authors": [
            "Jinzheng Zhao",
            "Yong Xu",
            "Xinyuan Qian",
            "Wenwu Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The use of audio and visual modality for speaker localization has been well\nstudied in the literature by exploiting their complementary characteristics.\nHowever, most previous works employ the setting of static sensors mounted at\nfixed positions. Unlike them, in this work, we explore the ego-centric setting,\nwhere the heterogeneous sensors are embodied and could be moving with a human\nto facilitate speaker localization. Compared to the static scenario, the\nego-centric setting is more realistic for smart-home applications e.g., a\nservice robot. However, this also brings new challenges such as blurred images,\nfrequent speaker disappearance from the field of view of the wearer, and\nocclusions. In this paper, we study egocentric audio-visual speaker DOA\nestimation and deal with the challenges mentioned above. Specifically, we\npropose a transformer-based audio-visual fusion method to estimate the relative\nDOA of the speaker to the wearer, and design a training strategy to mitigate\nthe problem of the speaker disappearing from the camera's view. We also develop\na new dataset for simulating the out-of-view scenarios, by creating a scene\nwith a camera wearer walking around while a speaker is moving at the same time.\nThe experimental results show that our proposed method offers promising\nperformance in this new dataset in terms of tracking accuracy. Finally, we\nadapt the proposed method for the multi-speaker scenario. Experiments on\nEasyCom show the effectiveness of the proposed model for multiple speakers in\nreal scenarios, which achieves state-of-the-art results in the sphere active\nspeaker detection task and the wearer activity prediction task. The simulated\ndataset and related code are available at\nhttps://github.com/KawhiZhao/Egocentric-Audio-Visual-Speaker-Localization.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.16308v1"
    },
    {
        "title": "CNN-based Prediction of Partition Path for VVC Fast Inter Partitioning\n  Using Motion Fields",
        "authors": [
            "Yiqun Liu",
            "Marc Riviere",
            "Thomas Guionnet",
            "Aline Roumy",
            "Christine Guillemot"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The Versatile Video Coding (VVC) standard has been recently finalized by the\nJoint Video Exploration Team (JVET). Compared to the High Efficiency Video\nCoding (HEVC) standard, VVC offers about 50% compression efficiency gain, in\nterms of Bjontegaard Delta-Rate (BD-rate), at the cost of a 10-fold increase in\nencoding complexity. In this paper, we propose a method based on Convolutional\nNeural Network (CNN) to speed up the inter partitioning process in VVC.\nFirstly, a novel representation for the quadtree with nested multi-type tree\n(QTMT) partition is introduced, derived from the partition path. Secondly, we\ndevelop a U-Net-based CNN taking a multi-scale motion vector field as input at\nthe Coding Tree Unit (CTU) level. The purpose of CNN inference is to predict\nthe optimal partition path during the Rate-Distortion Optimization (RDO)\nprocess. To achieve this, we divide CTU into grids and predict the Quaternary\nTree (QT) depth and Multi-type Tree (MT) split decisions for each cell of the\ngrid. Thirdly, an efficient partition pruning algorithm is introduced to employ\nthe CNN predictions at each partitioning level to skip RDO evaluations of\nunnecessary partition paths. Finally, an adaptive threshold selection scheme is\ndesigned, making the trade-off between complexity and efficiency scalable.\nExperiments show that the proposed method can achieve acceleration ranging from\n16.5% to 60.2% under the RandomAccess Group Of Picture 32 (RAGOP32)\nconfiguration with a reasonable efficiency drop ranging from 0.44% to 4.59% in\nterms of BD-rate, which surpasses other state-of-the-art solutions.\nAdditionally, our method stands out as one of the lightest approaches in the\nfield, which ensures its applicability to other encoders.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.13838v1"
    },
    {
        "title": "Audio-Visual Speaker Tracking: Progress, Challenges, and Future\n  Directions",
        "authors": [
            "Jinzheng Zhao",
            "Yong Xu",
            "Xinyuan Qian",
            "Davide Berghi",
            "Peipei Wu",
            "Meng Cui",
            "Jianyuan Sun",
            "Philip J. B. Jackson",
            "Wenwu Wang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Audio-visual speaker tracking has drawn increasing attention over the past\nfew years due to its academic values and wide application. Audio and visual\nmodalities can provide complementary information for localization and tracking.\nWith audio and visual information, the Bayesian-based filter can solve the\nproblem of data association, audio-visual fusion and track management. In this\npaper, we conduct a comprehensive overview of audio-visual speaker tracking. To\nour knowledge, this is the first extensive survey over the past five years. We\nintroduce the family of Bayesian filters and summarize the methods for\nobtaining audio-visual measurements. In addition, the existing trackers and\ntheir performance on AV16.3 dataset are summarized. In the past few years, deep\nlearning techniques have thrived, which also boosts the development of audio\nvisual speaker tracking. The influence of deep learning techniques in terms of\nmeasurement extraction and state estimation is also discussed. At last, we\ndiscuss the connections between audio-visual speaker tracking and other areas\nsuch as speech separation and distributed speaker tracking.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.14778v3"
    },
    {
        "title": "Intuitive Multilingual Audio-Visual Speech Recognition with a\n  Single-Trained Model",
        "authors": [
            "Joanna Hong",
            "Se Jin Park",
            "Yong Man Ro"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  We present a novel approach to multilingual audio-visual speech recognition\ntasks by introducing a single model on a multilingual dataset. Motivated by a\nhuman cognitive system where humans can intuitively distinguish different\nlanguages without any conscious effort or guidance, we propose a model that can\ncapture which language is given as an input speech by distinguishing the\ninherent similarities and differences between languages. To do so, we design a\nprompt fine-tuning technique into the largely pre-trained audio-visual\nrepresentation model so that the network can recognize the language class as\nwell as the speech with the corresponding language. Our work contributes to\ndeveloping robust and efficient multilingual audio-visual speech recognition\nsystems, reducing the need for language-specific models.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.14946v1"
    },
    {
        "title": "Enabling Acoustic Audience Feedback in Large Virtual Events",
        "authors": [
            "Tamay Aykut",
            "Markus Hofbauer",
            "Christopher Kuhn",
            "Eckehard Steinbach",
            "Bernd Girod"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The COVID-19 pandemic shifted many events in our daily lives into the virtual\ndomain. While virtual conference systems provide an alternative to physical\nmeetings, larger events require a muted audience to avoid an accumulation of\nbackground noise and distorted audio. However, performing artists strongly rely\non the feedback of their audience. We propose a concept for a virtual audience\nframework which supports all participants with the ambience of a real audience.\nAudience feedback is collected locally, allowing users to express enthusiasm or\ndiscontent by selecting means such as clapping, whistling, booing, and\nlaughter. This feedback is sent as abstract information to a virtual audience\nserver. We broadcast the combined virtual audience feedback information to all\nparticipants, which can be synthesized as a single acoustic feedback by the\nclient. The synthesis can be done by turning the collective audience feedback\ninto a prompt that is fed to state-of-the-art models such as AudioGen. This\nway, each user hears a single acoustic feedback sound of the entire virtual\nevent, without requiring to unmute or risk hearing distorted, unsynchronized\nfeedback.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.18099v1"
    },
    {
        "title": "Sound of Story: Multi-modal Storytelling with Audio",
        "authors": [
            "Jaeyeon Bae",
            "Seokhoon Jeong",
            "Seokun Kang",
            "Namgi Han",
            "Jae-Yon Lee",
            "Hyounghun Kim",
            "Taehwan Kim"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Storytelling is multi-modal in the real world. When one tells a story, one\nmay use all of the visualizations and sounds along with the story itself.\nHowever, prior studies on storytelling datasets and tasks have paid little\nattention to sound even though sound also conveys meaningful semantics of the\nstory. Therefore, we propose to extend story understanding and telling areas by\nestablishing a new component called \"background sound\" which is story\ncontext-based audio without any linguistic information. For this purpose, we\nintroduce a new dataset, called \"Sound of Story (SoS)\", which has paired image\nand text sequences with corresponding sound or background music for a story. To\nthe best of our knowledge, this is the largest well-curated dataset for\nstorytelling with sound. Our SoS dataset consists of 27,354 stories with 19.6\nimages per story and 984 hours of speech-decoupled audio such as background\nmusic and other sounds. As benchmark tasks for storytelling with sound and the\ndataset, we propose retrieval tasks between modalities, and audio generation\ntasks from image-text sequences, introducing strong baselines for them. We\nbelieve the proposed dataset and tasks may shed light on the multi-modal\nunderstanding of storytelling in terms of sound. Downloading the dataset and\nbaseline codes for each task will be released in the link:\nhttps://github.com/Sosdatasets/SoS_Dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.19264v1"
    },
    {
        "title": "Are Words Enough? On the semantic conditioning of affective music\n  generation",
        "authors": [
            "Jorge Forero",
            "Gilberto Bernardes",
            "Mónica Mendes"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Music has been commonly recognized as a means of expressing emotions. In this\nsense, an intense debate emerges from the need to verbalize musical emotions.\nThis concern seems highly relevant today, considering the exponential growth of\nnatural language processing using deep learning models where it is possible to\nprompt semantic propositions to generate music automatically. This scoping\nreview aims to analyze and discuss the possibilities of music generation\nconditioned by emotions. To address this topic, we propose a historical\nperspective that encompasses the different disciplines and methods contributing\nto this topic. In detail, we review two main paradigms adopted in automatic\nmusic generation: rules-based and machine-learning models. Of note are the deep\nlearning architectures that aim to generate high-fidelity music from textual\ndescriptions. These models raise fundamental questions about the expressivity\nof music, including whether emotions can be represented with words or expressed\nthrough them. We conclude that overcoming the limitation and ambiguity of\nlanguage to express emotions through music, some of the use of deep learning\nwith natural language has the potential to impact the creative industries by\nproviding powerful tools to prompt and generate new musical works.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.03624v1"
    },
    {
        "title": "DKiS: Decay weight invertible image steganography with private key",
        "authors": [
            "Hang Yang",
            "Yitian Xu",
            "Xuhua Liu"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Image steganography, defined as the practice of concealing information within\nanother image, traditionally encounters security challenges when its methods\nbecome publicly known or are under attack. To address this, a novel private\nkey-based image steganography technique has been introduced. This approach\nensures the security of the hidden information, as access requires a\ncorresponding private key, regardless of the public knowledge of the\nsteganography method. Experimental evidence has been presented, demonstrating\nthe effectiveness of our method and showcasing its real-world applicability.\nFurthermore, a critical challenge in the invertible image steganography process\nhas been identified by us: the transfer of non-essential, or `garbage',\ninformation from the secret to the host pipeline. To tackle this issue, the\ndecay weight has been introduced to control the information transfer,\neffectively filtering out irrelevant data and enhancing the performance of\nimage steganography. The code for this technique is publicly accessible at\nhttps://github.com/yanghangAI/DKiS, and a practical demonstration can be found\nat http://yanghang.site/hidekey.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.18243v2"
    },
    {
        "title": "Multi-Modal Video Topic Segmentation with Dual-Contrastive Domain\n  Adaptation",
        "authors": [
            "Linzi Xing",
            "Quan Tran",
            "Fabian Caba",
            "Franck Dernoncourt",
            "Seunghyun Yoon",
            "Zhaowen Wang",
            "Trung Bui",
            "Giuseppe Carenini"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Video topic segmentation unveils the coarse-grained semantic structure\nunderlying videos and is essential for other video understanding tasks. Given\nthe recent surge in multi-modal, relying solely on a single modality is\narguably insufficient. On the other hand, prior solutions for similar tasks\nlike video scene/shot segmentation cater to short videos with clear visual\nshifts but falter for long videos with subtle changes, such as livestreams. In\nthis paper, we introduce a multi-modal video topic segmenter that utilizes both\nvideo transcripts and frames, bolstered by a cross-modal attention mechanism.\nFurthermore, we propose a dual-contrastive learning framework adhering to the\nunsupervised domain adaptation paradigm, enhancing our model's adaptability to\nlonger, more semantically complex videos. Experiments on short and long video\ncorpora demonstrate that our proposed solution, significantly surpasses\nbaseline methods in terms of both accuracy and transferability, in both intra-\nand cross-domain settings.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.00220v1"
    },
    {
        "title": "More than Vanilla Fusion: a Simple, Decoupling-free, Attention Module\n  for Multimodal Fusion Based on Signal Theory",
        "authors": [
            "Peiwen Sun",
            "Yifan Zhang",
            "Zishan Liu",
            "Donghao Chen",
            "Honggang Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  The vanilla fusion methods still dominate a large percentage of mainstream\naudio-visual tasks. However, the effectiveness of vanilla fusion from a\ntheoretical perspective is still worth discussing. Thus, this paper reconsiders\nthe signal fused in the multimodal case from a bionics perspective and proposes\na simple, plug-and-play, attention module for vanilla fusion based on\nfundamental signal theory and uncertainty theory. In addition, previous work on\nmultimodal dynamic gradient modulation still relies on decoupling the\nmodalities. So, a decoupling-free gradient modulation scheme has been designed\nin conjunction with the aforementioned attention module, which has various\nadvantages over the decoupled one. Experiment results show that just a few\nlines of code can achieve up to 2.0% performance improvements to several\nmultimodal classification methods. Finally, quantitative evaluation of other\nfusion tasks reveals the potential for additional application scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.07212v1"
    },
    {
        "title": "Removing Interference and Recovering Content Imaginatively for Visible\n  Watermark Removal",
        "authors": [
            "Yicheng Leng",
            "Chaowei Fang",
            "Gen Li",
            "Yixiang Fang",
            "Guanbin Li"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Visible watermarks, while instrumental in protecting image copyrights,\nfrequently distort the underlying content, complicating tasks like scene\ninterpretation and image editing. Visible watermark removal aims to eliminate\nthe interference of watermarks and restore the background content. However,\nexisting methods often implement watermark component removal and background\nrestoration tasks within a singular branch, leading to residual watermarks in\nthe predictions and ignoring cases where watermarks heavily obscure the\nbackground. To address these limitations, this study introduces the Removing\nInterference and Recovering Content Imaginatively (RIRCI) framework. RIRCI\nembodies a two-stage approach: the initial phase centers on discerning and\nsegregating the watermark component, while the subsequent phase focuses on\nbackground content restoration. To achieve meticulous background restoration,\nour proposed model employs a dual-path network capable of fully exploring the\nintrinsic background information beneath semi-transparent watermarks and\nperipheral contextual information from unaffected regions. Moreover, a Global\nand Local Context Interaction module is built upon multi-layer perceptrons and\nbidirectional feature transformation for comprehensive representation modeling\nin the background restoration phase. The efficacy of our approach is\nempirically validated across two large-scale datasets, and our findings reveal\na marked enhancement over existing watermark removal techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.14383v1"
    },
    {
        "title": "Human-Centric Resource Allocation for the Metaverse With Multiaccess\n  Edge Computing",
        "authors": [
            "Zijian Long",
            "Haiwei Dong",
            "Abdulmotaleb El Saddik"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Multi-access edge computing (MEC) is a promising solution to the\ncomputation-intensive, low-latency rendering tasks of the metaverse. However,\nhow to optimally allocate limited communication and computation resources at\nthe edge to a large number of users in the metaverse is quite challenging. In\nthis paper, we propose an adaptive edge resource allocation method based on\nmulti-agent soft actor-critic with graph convolutional networks (SAC-GCN).\nSpecifically, SAC-GCN models the multi-user metaverse environment as a graph\nwhere each agent is denoted by a node. Each agent learns the interplay between\nagents by graph convolutional networks with self-attention mechanism to further\ndetermine the resource usage for one user in the metaverse. The effectiveness\nof SAC-GCN is demonstrated through the analysis of user experience, balance of\nresource allocation, and resource utilization rate by taking a virtual city\npark metaverse as an example. Experimental results indicate that SAC-GCN\noutperforms other resource allocation methods in improving overall user\nexperience, balancing resource allocation, and increasing resource utilization\nrate by at least 27%, 11%, and 8%, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.15313v1"
    },
    {
        "title": "Efficient Bitrate Ladder Construction using Transfer Learning and\n  Spatio-Temporal Features",
        "authors": [
            "Ali Falahati",
            "Mohammad Karim Safavi",
            "Ardavan Elahi",
            "Farhad Pakdaman",
            "Moncef Gabbouj"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Providing high-quality video with efficient bitrate is a main challenge in\nvideo industry. The traditional one-size-fits-all scheme for bitrate ladders is\ninefficient and reaching the best content-aware decision computationally\nimpractical due to extensive encodings required. To mitigate this, we propose a\nbitrate and complexity efficient bitrate ladder prediction method using\ntransfer learning and spatio-temporal features. We propose: (1) using feature\nmaps from well-known pre-trained DNNs to predict rate-quality behavior with\nlimited training data; and (2) improving highest quality rung efficiency by\npredicting minimum bitrate for top quality and using it for the top rung. The\nmethod tested on 102 video scenes demonstrates 94.1% reduction in complexity\nversus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning\nwas thoroughly studied through four networks and ablation studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.03195v2"
    },
    {
        "title": "Bjøntegaard Delta (BD): A Tutorial Overview of the Metric, Evolution,\n  Challenges, and Recommendations",
        "authors": [
            "Nabajeet Barman",
            "Maria G. Martini",
            "Yuriy Reznik"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The Bj{\\o}ntegaard Delta (BD) method proposed in 2001 has become a popular\ntool for comparing video codec compression efficiency. It was initially\nproposed to compute bitrate and quality differences between two Rate-Distortion\ncurves using PSNR as a distortion metric. Over the years, many works have\ncalculated and reported BD results using other objective quality metrics such\nas SSIM, VMAF and, in some cases, even subjective ratings (mean opinion\nscores). However, the lack of consolidated literature explaining the metric,\nits evolution over the years, and a systematic evaluation of the same under\ndifferent test conditions can result in a wrong interpretation of the BD\nresults thus obtained.\n  Towards this end, this paper presents a detailed tutorial describing the BD\nmethod and example cases where the metric might fail. We also provide a\ndetailed history of its evolution, including a discussion of various proposed\nimprovements and variations over the last 20 years. In addition, we evaluate\nthe various BD methods and their open-source implementations, considering\ndifferent objective quality metrics and subjective ratings taking into account\ndifferent RD characteristics. Based on our results, we present a set of\nrecommendations on using existing BD metrics and various insights for possible\nexploration towards developing more effective tools for codec compression\nefficiency evaluation and comparison.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.04039v1"
    },
    {
        "title": "SonicVisionLM: Playing Sound with Vision Language Models",
        "authors": [
            "Zhifeng Xie",
            "Shengye Yu",
            "Qile He",
            "Mengtian Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  There has been a growing interest in the task of generating sound for silent\nvideos, primarily because of its practicality in streamlining video\npost-production. However, existing methods for video-sound generation attempt\nto directly create sound from visual representations, which can be challenging\ndue to the difficulty of aligning visual representations with audio\nrepresentations. In this paper, we present SonicVisionLM, a novel framework\naimed at generating a wide range of sound effects by leveraging vision-language\nmodels(VLMs). Instead of generating audio directly from video, we use the\ncapabilities of powerful VLMs. When provided with a silent video, our approach\nfirst identifies events within the video using a VLM to suggest possible sounds\nthat match the video content. This shift in approach transforms the challenging\ntask of aligning image and audio into more well-studied sub-problems of\naligning image-to-text and text-to-audio through the popular diffusion models.\nTo improve the quality of audio recommendations with LLMs, we have collected an\nextensive dataset that maps text descriptions to specific sound effects and\ndeveloped a time-controlled audio adapter. Our approach surpasses current\nstate-of-the-art methods for converting video to audio, enhancing\nsynchronization with the visuals, and improving alignment between audio and\nvideo components. Project page:\nhttps://yusiissy.github.io/SonicVisionLM.github.io/\n",
        "pdf_link": "http://arxiv.org/pdf/2401.04394v3"
    },
    {
        "title": "Optimal Transcoding Resolution Prediction for Efficient Per-Title\n  Bitrate Ladder Estimation",
        "authors": [
            "Jinhai Yang",
            "Mengxi Guo",
            "Shijie Zhao",
            "Junlin Li",
            "Li Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Adaptive video streaming requires efficient bitrate ladder construction to\nmeet heterogeneous network conditions and end-user demands. Per-title optimized\nencoding typically traverses numerous encoding parameters to search the\nPareto-optimal operating points for each video. Recently, researchers have\nattempted to predict the content-optimized bitrate ladder for pre-encoding\noverhead reduction. However, existing methods commonly estimate the encoding\nparameters on the Pareto front and still require subsequent pre-encodings. In\nthis paper, we propose to directly predict the optimal transcoding resolution\nat each preset bitrate for efficient bitrate ladder construction. We adopt a\nTemporal Attentive Gated Recurrent Network to capture spatial-temporal features\nand predict transcoding resolutions as a multi-task classification problem. We\ndemonstrate that content-optimized bitrate ladders can thus be efficiently\ndetermined without any pre-encoding. Our method well approximates the\nground-truth bitrate-resolution pairs with a slight Bj{\\o}ntegaard Delta rate\nloss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.04405v1"
    },
    {
        "title": "Learning Audio Concepts from Counterfactual Natural Language",
        "authors": [
            "Ali Vosoughi",
            "Luca Bondi",
            "Ho-Hsiang Wu",
            "Chenliang Xu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Conventional audio classification relied on predefined classes, lacking the\nability to learn from free-form text. Recent methods unlock learning joint\naudio-text embeddings from raw audio-text pairs describing audio in natural\nlanguage. Despite recent advancements, there is little exploration of\nsystematic methods to train models for recognizing sound events and sources in\nalternative scenarios, such as distinguishing fireworks from gunshots at\noutdoor events in similar situations. This study introduces causal reasoning\nand counterfactual analysis in the audio domain. We use counterfactual\ninstances and include them in our model across different aspects. Our model\nconsiders acoustic characteristics and sound source information from\nhuman-annotated reference texts. To validate the effectiveness of our model, we\nconducted pre-training utilizing multiple audio captioning datasets. We then\nevaluate with several common downstream tasks, demonstrating the merits of the\nproposed method as one of the first works leveraging counterfactual information\nin audio domain. Specifically, the top-1 accuracy in open-ended language-based\naudio retrieval task increased by more than 43%.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.04935v1"
    },
    {
        "title": "ScripTONES: Sentiment-Conditioned Music Generation for Movie Scripts",
        "authors": [
            "Vishruth Veerendranath",
            "Vibha Masti",
            "Utkarsh Gupta",
            "Hrishit Chaudhuri",
            "Gowri Srinivasa"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Film scores are considered an essential part of the film cinematic\nexperience, but the process of film score generation is often expensive and\ninfeasible for small-scale creators. Automating the process of film score\ncomposition would provide useful starting points for music in small projects.\nIn this paper, we propose a two-stage pipeline for generating music from a\nmovie script. The first phase is the Sentiment Analysis phase where the\nsentiment of a scene from the film script is encoded into the valence-arousal\ncontinuous space. The second phase is the Conditional Music Generation phase\nwhich takes as input the valence-arousal vector and conditionally generates\npiano MIDI music to match the sentiment. We study the efficacy of various music\ngeneration architectures by performing a qualitative user survey and propose\nmethods to improve sentiment-conditioning in VAE architectures.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.07084v1"
    },
    {
        "title": "LATENTPATCH: A Non-Parametric Approach for Face Generation and Editing",
        "authors": [
            "Benjamin Samuth",
            "Julien Rabin",
            "David Tschumperlé",
            "Frédéric Jurie"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This paper presents LatentPatch, a new method for generating realistic images\nfrom a small dataset of only a few images. We use a lightweight model with only\na few thousand parameters. Unlike traditional few-shot generation methods that\nfinetune pre-trained large-scale generative models, our approach is computed\ndirectly on the latent distribution by sequential feature matching, and is\nexplainable by design. Avoiding large models based on transformers, recursive\nnetworks, or self-attention, which are not suitable for small datasets, our\nmethod is inspired by non-parametric texture synthesis and style transfer\nmodels, and ensures that generated image features are sampled from the source\ndistribution. We extend previous single-image models to work with a few images\nand demonstrate that our method can generate realistic images, as well as\nenable conditional sampling and image editing. We conduct experiments on face\ndatasets and show that our simplistic model is effective and versatile.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.16830v1"
    },
    {
        "title": "Detecting Multimedia Generated by Large AI Models: A Survey",
        "authors": [
            "Li Lin",
            "Neeraj Gupta",
            "Yue Zhang",
            "Hainan Ren",
            "Chun-Hao Liu",
            "Feng Ding",
            "Xin Wang",
            "Xin Li",
            "Luisa Verdoliva",
            "Shu Hu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The rapid advancement of Large AI Models (LAIMs), particularly diffusion\nmodels and large language models, has marked a new era where AI-generated\nmultimedia is increasingly integrated into various aspects of daily life.\nAlthough beneficial in numerous fields, this content presents significant\nrisks, including potential misuse, societal disruptions, and ethical concerns.\nConsequently, detecting multimedia generated by LAIMs has become crucial, with\na marked rise in related research. Despite this, there remains a notable gap in\nsystematic surveys that focus specifically on detecting LAIM-generated\nmultimedia. Addressing this, we provide the first survey to comprehensively\ncover existing research on detecting multimedia (such as text, images, videos,\naudio, and multimodal content) created by LAIMs. Specifically, we introduce a\nnovel taxonomy for detection methods, categorized by media modality, and\naligned with two perspectives: pure detection (aiming to enhance detection\nperformance) and beyond detection (adding attributes like generalizability,\nrobustness, and interpretability to detectors). Additionally, we have presented\na brief overview of generation mechanisms, public datasets, and online\ndetection tools to provide a valuable resource for researchers and\npractitioners in this field. Furthermore, we identify current challenges in\ndetection and propose directions for future research that address unexplored,\nongoing, and emerging issues in detecting multimedia generated by LAIMs. Our\naim for this survey is to fill an academic gap and contribute to global AI\nsecurity efforts, helping to ensure the integrity of information in the digital\nrealm. The project link is\nhttps://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00045v3"
    },
    {
        "title": "Perceptual Video Quality Assessment: A Survey",
        "authors": [
            "Xiongkuo Min",
            "Huiyu Duan",
            "Wei Sun",
            "Yucheng Zhu",
            "Guangtao Zhai"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Perceptual video quality assessment plays a vital role in the field of video\nprocessing due to the existence of quality degradations introduced in various\nstages of video signal acquisition, compression, transmission and display. With\nthe advancement of internet communication and cloud service technology, video\ncontent and traffic are growing exponentially, which further emphasizes the\nrequirement for accurate and rapid assessment of video quality. Therefore,\nnumerous subjective and objective video quality assessment studies have been\nconducted over the past two decades for both generic videos and specific videos\nsuch as streaming, user-generated content (UGC), 3D, virtual and augmented\nreality (VR and AR), high frame rate (HFR), audio-visual, etc. This survey\nprovides an up-to-date and comprehensive review of these video quality\nassessment studies. Specifically, we first review the subjective video quality\nassessment methodologies and databases, which are necessary for validating the\nperformance of video quality metrics. Second, the objective video quality\nassessment algorithms for general purposes are surveyed and concluded according\nto the methodologies utilized in the quality measures. Third, we overview the\nobjective video quality assessment measures for specific applications and\nemerging topics. Finally, the performances of the state-of-the-art video\nquality assessment measures are compared and analyzed. This survey provides a\nsystematic overview of both classical works and recent progresses in the realm\nof video quality assessment, which can help other researchers quickly access\nthe field and conduct relevant research.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.03413v1"
    },
    {
        "title": "Blind Deep-Learning-Based Image Watermarking Robust Against Geometric\n  Transformations",
        "authors": [
            "Hannes Mareen",
            "Lucas Antchougov",
            "Glenn Van Wallendael",
            "Peter Lambert"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Digital watermarking enables protection against copyright infringement of\nimages. Although existing methods embed watermarks imperceptibly and\ndemonstrate robustness against attacks, they typically lack resilience against\ngeometric transformations. Therefore, this paper proposes a new watermarking\nmethod that is robust against geometric attacks. The proposed method is based\non the existing HiDDeN architecture that uses deep learning for watermark\nencoding and decoding. We add new noise layers to this architecture, namely for\na differentiable JPEG estimation, rotation, rescaling, translation, shearing\nand mirroring. We demonstrate that our method outperforms the state of the art\nwhen it comes to geometric robustness. In conclusion, the proposed method can\nbe used to protect images when viewed on consumers' devices.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09062v1"
    },
    {
        "title": "Television Discourse Decoded: Comprehensive Multimodal Analytics at\n  Scale",
        "authors": [
            "Anmol Agarwal",
            "Pratyush Priyadarshi",
            "Shiven Sinha",
            "Shrey Gupta",
            "Hitkul Jangra",
            "Ponnurangam Kumaraguru",
            "Kiran Garimella"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we tackle the complex task of analyzing televised debates,\nwith a focus on a prime time news debate show from India. Previous methods,\nwhich often relied solely on text, fall short in capturing the multimodal\nessence of these debates. To address this gap, we introduce a comprehensive\nautomated toolkit that employs advanced computer vision and speech-to-text\ntechniques for large-scale multimedia analysis. Utilizing state-of-the-art\ncomputer vision algorithms and speech-to-text methods, we transcribe, diarize,\nand analyze thousands of YouTube videos of a prime-time television debate show\nin India. These debates are a central part of Indian media but have been\ncriticized for compromised journalistic integrity and excessive dramatization.\nOur toolkit provides concrete metrics to assess bias and incivility, capturing\na comprehensive multimedia perspective that includes text, audio utterances,\nand video frames. Our findings reveal significant biases in topic selection and\npanelist representation, along with alarming levels of incivility. This work\noffers a scalable, automated approach for future research in multimedia\nanalysis, with profound implications for the quality of public discourse and\ndemocratic debate. To catalyze further research in this area, we also release\nthe code, dataset collected and supplemental pdf.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.12629v2"
    },
    {
        "title": "A User-Friendly Framework for Generating Model-Preferred Prompts in\n  Text-to-Image Synthesis",
        "authors": [
            "Nailei Hei",
            "Qianyu Guo",
            "Zihao Wang",
            "Yan Wang",
            "Haofen Wang",
            "Wenqiang Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Well-designed prompts have demonstrated the potential to guide text-to-image\nmodels in generating amazing images. Although existing prompt engineering\nmethods can provide high-level guidance, it is challenging for novice users to\nachieve the desired results by manually entering prompts due to a discrepancy\nbetween novice-user-input prompts and the model-preferred prompts. To bridge\nthe distribution gap between user input behavior and model training datasets,\nwe first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and\npropose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG)\nfor automated prompt optimization. For CFP, we construct a novel dataset for\ntext-to-image tasks that combines coarse and fine-grained prompts to facilitate\nthe development of automated prompt generation methods. For UF-FGTG, we propose\na novel framework that automatically translates user-input prompts into\nmodel-preferred prompts. Specifically, we propose a prompt refiner that\ncontinually rewrites prompts to empower users to select results that align with\ntheir unique needs. Meanwhile, we integrate image-related loss functions from\nthe text-to-image model into the training process of text generation to\ngenerate model-preferred prompts. Additionally, we propose an adaptive feature\nextraction module to ensure diversity in the generated results. Experiments\ndemonstrate that our approach is capable of generating more visually appealing\nand diverse images than previous state-of-the-art methods, achieving an average\nimprovement of 5% across six quality and aesthetic metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.12760v1"
    },
    {
        "title": "Investigating the Generalizability of Physiological Characteristics of\n  Anxiety",
        "authors": [
            "Emily Zhou",
            "Mohammad Soleymani",
            "Maja J. Matarić"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recent works have demonstrated the effectiveness of machine learning (ML)\ntechniques in detecting anxiety and stress using physiological signals, but it\nis unclear whether ML models are learning physiological features specific to\nstress. To address this ambiguity, we evaluated the generalizability of\nphysiological features that have been shown to be correlated with anxiety and\nstress to high-arousal emotions. Specifically, we examine features extracted\nfrom electrocardiogram (ECG) and electrodermal (EDA) signals from the following\nthree datasets: Anxiety Phases Dataset (APD), Wearable Stress and Affect\nDetection (WESAD), and the Continuously Annotated Signals of Emotion (CASE)\ndataset. We aim to understand whether these features are specific to anxiety or\ngeneral to other high-arousal emotions through a statistical regression\nanalysis, in addition to a within-corpus, cross-corpus, and\nleave-one-corpus-out cross-validation across instances of stress and arousal.\nWe used the following classifiers: Support Vector Machines, LightGBM, Random\nForest, XGBoost, and an ensemble of the aforementioned models. We found that\nmodels trained on an arousal dataset perform relatively well on a previously\nunseen stress dataset, and vice versa. Our experimental results suggest that\nthe evaluated models may be identifying emotional arousal instead of stress.\nThis work is the first cross-corpus evaluation across stress and arousal from\nECG and EDA signals, contributing new findings about the generalizability of\nstress detection.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.15513v1"
    },
    {
        "title": "Towards Accurate Lip-to-Speech Synthesis in-the-Wild",
        "authors": [
            "Sindhu Hegde",
            "Rudrabha Mukhopadhyay",
            "C. V. Jawahar",
            "Vinay Namboodiri"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we introduce a novel approach to address the task of\nsynthesizing speech from silent videos of any in-the-wild speaker solely based\non lip movements. The traditional approach of directly generating speech from\nlip videos faces the challenge of not being able to learn a robust language\nmodel from speech alone, resulting in unsatisfactory outcomes. To overcome this\nissue, we propose incorporating noisy text supervision using a state-of-the-art\nlip-to-text network that instills language information into our model. The\nnoisy text is generated using a pre-trained lip-to-text model, enabling our\napproach to work without text annotations during inference. We design a visual\ntext-to-speech network that utilizes the visual stream to generate accurate\nspeech, which is in-sync with the silent input video. We perform extensive\nexperiments and ablation studies, demonstrating our approach's superiority over\nthe current state-of-the-art methods on various benchmark datasets. Further, we\ndemonstrate an essential practical application of our method in assistive\ntechnology by generating speech for an ALS patient who has lost the voice but\ncan make mouth movements. Our demo video, code, and additional details can be\nfound at\n\\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw}.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.01087v1"
    },
    {
        "title": "Enhancing Expressiveness in Dance Generation via Integrating Frequency\n  and Music Style Information",
        "authors": [
            "Qiaochu Huang",
            "Xu He",
            "Boshi Tang",
            "Haolin Zhuang",
            "Liyang Chen",
            "Shuochen Gao",
            "Zhiyong Wu",
            "Haozhi Huang",
            "Helen Meng"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Dance generation, as a branch of human motion generation, has attracted\nincreasing attention. Recently, a few works attempt to enhance dance\nexpressiveness, which includes genre matching, beat alignment, and dance\ndynamics, from certain aspects. However, the enhancement is quite limited as\nthey lack comprehensive consideration of the aforementioned three factors. In\nthis paper, we propose ExpressiveBailando, a novel dance generation method\ndesigned to generate expressive dances, concurrently taking all three factors\ninto account. Specifically, we mitigate the issue of speed homogenization by\nincorporating frequency information into VQ-VAE, thus improving dance dynamics.\nAdditionally, we integrate music style information by extracting genre- and\nbeat-related features with a pre-trained music model, hence achieving\nimprovements in the other two factors. Extensive experimental results\ndemonstrate that our proposed method can generate dances with high\nexpressiveness and outperforms existing methods both qualitatively and\nquantitatively.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.05834v1"
    },
    {
        "title": "Efficient Feature Extraction and Late Fusion Strategy for Audiovisual\n  Emotional Mimicry Intensity Estimation",
        "authors": [
            "Jun Yu",
            "Wangyuan Zhu",
            "Jichao Zhu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we present the solution to the Emotional Mimicry Intensity\n(EMI) Estimation challenge, which is part of 6th Affective Behavior Analysis\nin-the-wild (ABAW) Competition.The EMI Estimation challenge task aims to\nevaluate the emotional intensity of seed videos by assessing them from a set of\npredefined emotion categories (i.e., \"Admiration\", \"Amusement\",\n\"Determination\", \"Empathic Pain\", \"Excitement\" and \"Joy\"). To tackle this\nchallenge, we extracted rich dual-channel visual features based on ResNet18 and\nAUs for the video modality and effective single-channel features based on\nWav2Vec2.0 for the audio modality. This allowed us to obtain comprehensive\nemotional features for the audiovisual modality. Additionally, leveraging a\nlate fusion strategy, we averaged the predictions of the visual and acoustic\nmodels, resulting in a more accurate estimation of audiovisual emotional\nmimicry intensity. Experimental results validate the effectiveness of our\napproach, with the average Pearson's correlation Coefficient($\\rho$) across the\n6 emotion dimensionson the validation set achieving 0.3288.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.11757v2"
    },
    {
        "title": "Robust Active Speaker Detection in Noisy Environments",
        "authors": [
            "Siva Sai Nagender Vasireddy",
            "Chenxu Zhang",
            "Xiaohu Guo",
            "Yapeng Tian"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This paper addresses the issue of active speaker detection (ASD) in noisy\nenvironments and formulates a robust active speaker detection (rASD) problem.\nExisting ASD approaches leverage both audio and visual modalities, but\nnon-speech sounds in the surrounding environment can negatively impact\nperformance. To overcome this, we propose a novel framework that utilizes\naudio-visual speech separation as guidance to learn noise-free audio features.\nThese features are then utilized in an ASD model, and both tasks are jointly\noptimized in an end-to-end framework. Our proposed framework mitigates residual\nnoise and audio quality reduction issues that can occur in a naive cascaded\ntwo-stage framework that directly uses separated speech for ASD, and enables\nthe two tasks to be optimized simultaneously. To further enhance the robustness\nof the audio features and handle inherent speech noises, we propose a dynamic\nweighted loss approach to train the speech separator. We also collected a\nreal-world noise audio dataset to facilitate investigations. Experiments\ndemonstrate that non-speech audio noises significantly impact ASD models, and\nour proposed approach improves ASD performance in noisy environments. The\nframework is general and can be applied to different ASD approaches to improve\ntheir robustness. Our code, models, and data will be released.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.19002v2"
    },
    {
        "title": "Guided Masked Self-Distillation Modeling for Distributed Multimedia\n  Sensor Event Analysis",
        "authors": [
            "Masahiro Yasuda",
            "Noboru Harada",
            "Yasunori Ohishi",
            "Shoichiro Saito",
            "Akira Nakayama",
            "Nobutaka Ono"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Observations with distributed sensors are essential in analyzing a series of\nhuman and machine activities (referred to as 'events' in this paper) in complex\nand extensive real-world environments. This is because the information obtained\nfrom a single sensor is often missing or fragmented in such an environment;\nobservations from multiple locations and modalities should be integrated to\nanalyze events comprehensively. However, a learning method has yet to be\nestablished to extract joint representations that effectively combine such\ndistributed observations. Therefore, we propose Guided Masked sELf-Distillation\nmodeling (Guided-MELD) for inter-sensor relationship modeling. The basic idea\nof Guided-MELD is to learn to supplement the information from the masked sensor\nwith information from other sensors needed to detect the event. Guided-MELD is\nexpected to enable the system to effectively distill the fragmented or\nredundant target event information obtained by the sensors without being overly\ndependent on any specific sensors. To validate the effectiveness of the\nproposed method in novel tasks of distributed multimedia sensor event analysis,\nwe recorded two new datasets that fit the problem setting: MM-Store and\nMM-Office. These datasets consist of human activities in a convenience store\nand an office, recorded using distributed cameras and microphones. Experimental\nresults on these datasets show that the proposed Guided-MELD improves event\ntagging and detection performance and outperforms conventional inter-sensor\nrelationship modeling methods. Furthermore, the proposed method performed\nrobustly even when sensors were reduced.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.08264v1"
    },
    {
        "title": "HyDiscGAN: A Hybrid Distributed cGAN for Audio-Visual Privacy\n  Preservation in Multimodal Sentiment Analysis",
        "authors": [
            "Zhuojia Wu",
            "Qi Zhang",
            "Duoqian Miao",
            "Kun Yi",
            "Wei Fan",
            "Liang Hu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal Sentiment Analysis (MSA) aims to identify speakers' sentiment\ntendencies in multimodal video content, raising serious concerns about privacy\nrisks associated with multimodal data, such as voiceprints and facial images.\nRecent distributed collaborative learning has been verified as an effective\nparadigm for privacy preservation in multimodal tasks. However, they often\noverlook the privacy distinctions among different modalities, struggling to\nstrike a balance between performance and privacy preservation. Consequently, it\nposes an intriguing question of maximizing multimodal utilization to improve\nperformance while simultaneously protecting necessary modalities. This paper\nforms the first attempt at modality-specified (i.e., audio and visual) privacy\npreservation in MSA tasks. We propose a novel Hybrid Distributed cross-modality\ncGAN framework (HyDiscGAN), which learns multimodality alignment to generate\nfake audio and visual features conditioned on shareable de-identified textual\ndata. The objective is to leverage the fake features to approximate real audio\nand visual content to guarantee privacy preservation while effectively\nenhancing performance. Extensive experiments show that compared with the\nstate-of-the-art MSA model, HyDiscGAN can achieve superior or competitive\nperformance while preserving privacy.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.11938v1"
    },
    {
        "title": "Deep Learning-based Text-in-Image Watermarking",
        "authors": [
            "Bishwa Karki",
            "Chun-Hua Tsai",
            "Pei-Chi Huang",
            "Xin Zhong"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this work, we introduce a novel deep learning-based approach to\ntext-in-image watermarking, a method that embeds and extracts textual\ninformation within images to enhance data security and integrity. Leveraging\nthe capabilities of deep learning, specifically through the use of\nTransformer-based architectures for text processing and Vision Transformers for\nimage feature extraction, our method sets new benchmarks in the domain. The\nproposed method represents the first application of deep learning in\ntext-in-image watermarking that improves adaptivity, allowing the model to\nintelligently adjust to specific image characteristics and emerging threats.\nThrough testing and evaluation, our method has demonstrated superior robustness\ncompared to traditional watermarking techniques, achieving enhanced\nimperceptibility that ensures the watermark remains undetectable across various\nimage contents.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.13134v1"
    },
    {
        "title": "Beyond Alignment: Blind Video Face Restoration via Parsing-Guided\n  Temporal-Coherent Transformer",
        "authors": [
            "Kepeng Xu",
            "Li Xu",
            "Gang He",
            "Wenxin Yu",
            "Yunsong Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multiple complex degradations are coupled in low-quality video faces in the\nreal world. Therefore, blind video face restoration is a highly challenging\nill-posed problem, requiring not only hallucinating high-fidelity details but\nalso enhancing temporal coherence across diverse pose variations. Restoring\neach frame independently in a naive manner inevitably introduces temporal\nincoherence and artifacts from pose changes and keypoint localization errors.\nTo address this, we propose the first blind video face restoration approach\nwith a novel parsing-guided temporal-coherent transformer (PGTFormer) without\npre-alignment. PGTFormer leverages semantic parsing guidance to select optimal\nface priors for generating temporally coherent artifact-free results.\nSpecifically, we pre-train a temporal-spatial vector quantized auto-encoder on\nhigh-quality video face datasets to extract expressive context-rich priors.\nThen, the temporal parse-guided codebook predictor (TPCP) restores faces in\ndifferent poses based on face parsing context cues without performing face\npre-alignment. This strategy reduces artifacts and mitigates jitter caused by\ncumulative errors from face pre-alignment. Finally, the temporal fidelity\nregulator (TFR) enhances fidelity through temporal feature interaction and\nimproves video temporal consistency. Extensive experiments on face videos show\nthat our method outperforms previous face restoration baselines. The code will\nbe released on\n\\href{https://github.com/kepengxu/PGTFormer}{https://github.com/kepengxu/PGTFormer}.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.13640v1"
    },
    {
        "title": "Pegasus-v1 Technical Report",
        "authors": [
            "Raehyuk Jung",
            "Hyojun Go",
            "Jaehyuk Yi",
            "Jiho Jang",
            "Daniel Kim",
            "Jay Suh",
            "Aiden Lee",
            "Cooper Han",
            "Jae Lee",
            "Jeff Kim",
            "Jin-Young Kim",
            "Junwan Kim",
            "Kyle Park",
            "Lucas Lee",
            "Mars Ha",
            "Minjoon Seo",
            "Abraham Jo",
            "Ed Park",
            "Hassan Kianinejad",
            "SJ Kim",
            "Tony Moon",
            "Wade Jeong",
            "Andrei Popescu",
            "Esther Kim",
            "EK Yoon",
            "Genie Heo",
            "Henry Choi",
            "Jenna Kang",
            "Kevin Han",
            "Noah Seo",
            "Sunny Nguyen",
            "Ryan Won",
            "Yeonhoo Park",
            "Anthony Giuliani",
            "Dave Chung",
            "Hans Yoon",
            "James Le",
            "Jenny Ahn",
            "June Lee",
            "Maninder Saini",
            "Meredith Sanders",
            "Soyoung Lee",
            "Sue Kim",
            "Travis Couture"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This technical report introduces Pegasus-1, a multimodal language model\nspecialized in video content understanding and interaction through natural\nlanguage. Pegasus-1 is designed to address the unique challenges posed by video\ndata, such as interpreting spatiotemporal information, to offer nuanced video\ncontent comprehension across various lengths. This technical report overviews\nPegasus-1's architecture, training strategies, and its performance in\nbenchmarks on video conversation, zero-shot video question answering, and video\nsummarization. We also explore qualitative characteristics of Pegasus-1 ,\ndemonstrating its capabilities as well as its limitations, in order to provide\nreaders a balanced view of its current state and its future direction.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.14687v1"
    },
    {
        "title": "Semantically consistent Video-to-Audio Generation using Multimodal\n  Language Large Model",
        "authors": [
            "Gehui Chen",
            "Guan'an Wang",
            "Xiaowen Huang",
            "Jitao Sang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Existing works have made strides in video generation, but the lack of sound\neffects (SFX) and background music (BGM) hinders a complete and immersive\nviewer experience. We introduce a novel semantically consistent v ideo-to-audio\ngeneration framework, namely SVA, which automatically generates audio\nsemantically consistent with the given video content. The framework harnesses\nthe power of multimodal large language model (MLLM) to understand video\nsemantics from a key frame and generate creative audio schemes, which are then\nutilized as prompts for text-to-audio models, resulting in video-to-audio\ngeneration with natural language as an interface. We show the satisfactory\nperformance of SVA through case study and discuss the limitations along with\nthe future research direction. The project page is available at\nhttps://huiz-a.github.io/audio4video.github.io/.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.16305v2"
    },
    {
        "title": "Picking watermarks from noise (PWFN): an improved robust watermarking\n  model against intensive distortions",
        "authors": [
            "Sijing Xie",
            "Chengxin Zhao",
            "Nan Sun",
            "Wei Li",
            "Hefei Ling"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Digital watermarking is the process of embedding secret information by\naltering images in an undetectable way to the human eye. To increase the\nrobustness of the model, many deep learning-based watermarking methods use the\nencoder-noise-decoder architecture by adding different noises to the noise\nlayer. The decoder then extracts the watermarked information from the distorted\nimage. However, this method can only resist weak noise attacks. To improve the\nrobustness of the decoder against stronger noise, this paper proposes to\nintroduce a denoise module between the noise layer and the decoder. The module\naims to reduce noise and recover some of the information lost caused by\ndistortion. Additionally, the paper introduces the SE module to fuse the\nwatermarking information pixel-wise and channel dimensions-wise, improving the\nencoder's efficiency. Experimental results show that our proposed method is\ncomparable to existing models and outperforms state-of-the-art under different\nnoise intensities. In addition, ablation experiments show the superiority of\nour proposed module.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.05170v2"
    },
    {
        "title": "Quality of Experience Optimization for Real-time XR Video Transmission\n  with Energy Constraints",
        "authors": [
            "Guangjin Pan",
            "Shugong Xu",
            "Shunqing Zhang",
            "Xiaojing Chen",
            "Yanzan Sun"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Extended Reality (XR) is an important service in the 5G network and in future\n6G networks. In contrast to traditional video on demand services, real-time XR\nvideo is transmitted frame-by-frame, requiring low latency and being highly\nsensitive to network fluctuations. In this paper, we model the quality of\nexperience (QoE) for real-time XR video transmission on a frame-by-frame basis.\nBased on the proposed QoE model, we formulate an optimization problem that\nmaximizes QoE with constraints on wireless resources and long-term energy\nconsumption. We utilize Lyapunov optimization to transform the original problem\ninto a single-frame optimization problem and then allocate wireless\nsubchannels. We propose an adaptive XR video bitrate algorithm that employs a\nLong Short Term Memory (LSTM) based Deep Q-Network (DQN) algorithm for video\nbitrate selection. Through numerical results, we show that our proposed\nalgorithm outperforms the baseline algorithms, with the average QoE\nimprovements of 0.04 to 0.46. Specifically, compared to baseline algorithms,\nthe proposed algorithm reduces average video quality variations by 29% to 50%\nand improves the frame transmission success rate by 5% to 48%.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07689v1"
    },
    {
        "title": "MADRL-Based Rate Adaptation for 360° Video Streaming with\n  Multi-Viewpoint Prediction",
        "authors": [
            "Haopeng Wang",
            "Zijian Long",
            "Haiwei Dong",
            "Abdulmotaleb El Saddik"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Over the last few years, 360{\\deg} video traffic on the network has grown\nsignificantly. A key challenge of 360{\\deg} video playback is ensuring a high\nquality of experience (QoE) with limited network bandwidth. Currently, most\nstudies focus on tile-based adaptive bitrate (ABR) streaming based on single\nviewport prediction to reduce bandwidth consumption. However, the performance\nof models for single-viewpoint prediction is severely limited by the inherent\nuncertainty in head movement, which can not cope with the sudden movement of\nusers very well. This paper first presents a multimodal spatial-temporal\nattention transformer to generate multiple viewpoint trajectories with their\nprobabilities given a historical trajectory. The proposed method models\nviewpoint prediction as a classification problem and uses attention mechanisms\nto capture the spatial and temporal characteristics of input video frames and\nviewpoint trajectories for multi-viewpoint prediction. After that, a\nmulti-agent deep reinforcement learning (MADRL)-based ABR algorithm utilizing\nmulti-viewpoint prediction for 360{\\deg} video streaming is proposed for\nmaximizing different QoE objectives under various network conditions. We\nformulate the ABR problem as a decentralized partially observable Markov\ndecision process (Dec-POMDP) problem and present a MAPPO algorithm based on\ncentralized training and decentralized execution (CTDE) framework to solve the\nproblem. The experimental results show that our proposed method improves the\ndefined QoE metric by up to 85.5% compared to existing ABR methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07759v2"
    },
    {
        "title": "Automatic Recognition of Food Ingestion Environment from the AIM-2\n  Wearable Sensor",
        "authors": [
            "Yuning Huang",
            "Mohamed Abul Hassan",
            "Jiangpeng He",
            "Janine Higgins",
            "Megan McCrory",
            "Heather Eicher-Miller",
            "Graham Thomas",
            "Edward O Sazonov",
            "Fengqing Maggie Zhu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Detecting an ingestion environment is an important aspect of monitoring\ndietary intake. It provides insightful information for dietary assessment.\nHowever, it is a challenging problem where human-based reviewing can be\ntedious, and algorithm-based review suffers from data imbalance and perceptual\naliasing problems. To address these issues, we propose a neural network-based\nmethod with a two-stage training framework that tactfully combines fine-tuning\nand transfer learning techniques. Our method is evaluated on a newly collected\ndataset called ``UA Free Living Study\", which uses an egocentric wearable\ncamera, AIM-2 sensor, to simulate food consumption in free-living conditions.\nThe proposed training framework is applied to common neural network backbones,\ncombined with approaches in the general imbalanced classification field.\nExperimental results on the collected dataset show that our proposed method for\nautomatic ingestion environment recognition successfully addresses the\nchallenging data imbalance problem in the dataset and achieves a promising\noverall classification accuracy of 96.63%.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07827v1"
    },
    {
        "title": "SMP Challenge: An Overview and Analysis of Social Media Prediction\n  Challenge",
        "authors": [
            "Bo Wu",
            "Peiye Liu",
            "Wen-Huang Cheng",
            "Bei Liu",
            "Zhaoyang Zeng",
            "Jia Wang",
            "Qiushi Huang",
            "Jiebo Luo"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Social Media Popularity Prediction (SMPP) is a crucial task that involves\nautomatically predicting future popularity values of online posts, leveraging\nvast amounts of multimodal data available on social media platforms. Studying\nand investigating social media popularity becomes central to various online\napplications and requires novel methods of comprehensive analysis, multimodal\ncomprehension, and accurate prediction.\n  SMP Challenge is an annual research activity that has spurred academic\nexploration in this area. This paper summarizes the challenging task, data, and\nresearch progress. As a critical resource for evaluating and benchmarking\npredictive models, we have released a large-scale SMPD benchmark encompassing\napproximately half a million posts authored by around 70K users. The research\nprogress analysis provides an overall analysis of the solutions and trends in\nrecent years. The SMP Challenge website (www.smp-challenge.com) provides the\nlatest information and news.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.10497v1"
    },
    {
        "title": "Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal\n  Utterances",
        "authors": [
            "Hanlei Zhang",
            "Hua Xu",
            "Fei Long",
            "Xin Wang",
            "Kai Gao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Discovering the semantics of multimodal utterances is essential for\nunderstanding human language and enhancing human-machine interactions. Existing\nmethods manifest limitations in leveraging nonverbal information for discerning\ncomplex semantics in unsupervised scenarios. This paper introduces a novel\nunsupervised multimodal clustering method (UMC), making a pioneering\ncontribution to this field. UMC introduces a unique approach to constructing\naugmentation views for multimodal data, which are then used to perform\npre-training to establish well-initialized representations for subsequent\nclustering. An innovative strategy is proposed to dynamically select\nhigh-quality samples as guidance for representation learning, gauged by the\ndensity of each sample's nearest neighbors. Besides, it is equipped to\nautomatically determine the optimal value for the top-$K$ parameter in each\ncluster to refine sample selection. Finally, both high- and low-quality samples\nare used to learn representations conducive to effective clustering. We build\nbaselines on benchmark multimodal intent and dialogue act datasets. UMC shows\nremarkable improvements of 2-6\\% scores in clustering metrics over\nstate-of-the-art methods, marking the first successful endeavor in this domain.\nThe complete code and data are available at https://github.com/thuiar/UMC.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12775v1"
    },
    {
        "title": "Intelligent Text-Conditioned Music Generation",
        "authors": [
            "Zhouyao Xie",
            "Nikhil Yadala",
            "Xinyi Chen",
            "Jing Xi Liu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  CLIP (Contrastive Language-Image Pre-Training) is a multimodal neural network\ntrained on (text, image) pairs to predict the most relevant text caption given\nan image. It has been used extensively in image generation by connecting its\noutput with a generative model such as VQGAN, with the most notable example\nbeing OpenAI's DALLE-2. In this project, we apply a similar approach to bridge\nthe gap between natural language and music. Our model is split into two steps:\nfirst, we train a CLIP-like model on pairs of text and music over contrastive\nloss to align a piece of music with its most probable text caption. Then, we\ncombine the alignment model with a music decoder to generate music. To the best\nof our knowledge, this is the first attempt at text-conditioned deep music\ngeneration. Our experiments show that it is possible to train the text-music\nalignment model using contrastive loss and train a decoder to generate music\nfrom text prompts.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.00626v1"
    },
    {
        "title": "Advancing Grounded Multimodal Named Entity Recognition via LLM-Based\n  Reformulation and Box-Based Segmentation",
        "authors": [
            "Jinyuan Li",
            "Ziyan Li",
            "Han Li",
            "Jianfei Yu",
            "Rui Xia",
            "Di Sun",
            "Gang Pan"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Grounded Multimodal Named Entity Recognition (GMNER) task aims to identify\nnamed entities, entity types and their corresponding visual regions. GMNER task\nexhibits two challenging attributes: 1) The tenuous correlation between images\nand text on social media contributes to a notable proportion of named entities\nbeing ungroundable. 2) There exists a distinction between coarse-grained noun\nphrases used in similar tasks (e.g., phrase localization) and fine-grained\nnamed entities. In this paper, we propose RiVEG, a unified framework that\nreformulates GMNER into a joint MNER-VE-VG task by leveraging large language\nmodels (LLMs) as connecting bridges. This reformulation brings two benefits: 1)\nIt enables us to optimize the MNER module for optimal MNER performance and\neliminates the need to pre-extract region features using object detection\nmethods, thus naturally addressing the two major limitations of existing GMNER\nmethods. 2) The introduction of Entity Expansion Expression module and Visual\nEntailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG).\nThis endows the proposed framework with unlimited data and model scalability.\nFurthermore, to address the potential ambiguity stemming from the\ncoarse-grained bounding box output in GMNER, we further construct the new\nSegmented Multimodal Named Entity Recognition (SMNER) task and corresponding\nTwitter-SMNER dataset aimed at generating fine-grained segmentation masks, and\nexperimentally demonstrate the feasibility and effectiveness of using box\nprompt-based Segment Anything Model (SAM) to empower any GMNER model with the\nability to accomplish the SMNER task. Extensive experiments demonstrate that\nRiVEG significantly outperforms SoTA methods on four datasets across the MNER,\nGMNER, and SMNER tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.07268v1"
    },
    {
        "title": "Enhancing Fake News Detection in Social Media via Label Propagation on\n  Cross-modal Tweet Graph",
        "authors": [
            "Wanqing Zhao",
            "Yuta Nakashima",
            "Haiyuan Chen",
            "Noboru Babaguchi"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Fake news detection in social media has become increasingly important due to\nthe rapid proliferation of personal media channels and the consequential\ndissemination of misleading information. Existing methods, which primarily rely\non multimodal features and graph-based techniques, have shown promising\nperformance in detecting fake news. However, they still face a limitation,\ni.e., sparsity in graph connections, which hinders capturing possible\ninteractions among tweets. This challenge has motivated us to explore a novel\nmethod that densifies the graph's connectivity to capture denser interaction\nbetter. Our method constructs a cross-modal tweet graph using CLIP, which\nencodes images and text into a unified space, allowing us to extract potential\nconnections based on similarities in text and images. We then design a Feature\nContextualization Network with Label Propagation (FCN-LP) to model the\ninteraction among tweets as well as positive or negative correlations between\npredicted labels of connected tweets. The propagated labels from the graph are\nweighted and aggregated for the final detection. To enhance the model's\ngeneralization ability to unseen events, we introduce a domain generalization\nloss that ensures consistent features between tweets on seen and unseen events.\nWe use three publicly available fake news datasets, Twitter, PHEME, and Weibo,\nfor evaluation. Our method consistently improves the performance over the\nstate-of-the-art methods on all benchmark datasets and effectively demonstrates\nits aptitude for generalizing fake news detection in social media.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.09884v1"
    },
    {
        "title": "Multimodal Chaptering for Long-Form TV Newscast Video",
        "authors": [
            "Khalil Guetari",
            "Yannis Tevissen",
            "Frédéric Petitpont"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  We propose a novel approach for automatic chaptering of TV newscast videos,\naddressing the challenge of structuring and organizing large collections of\nunsegmented broadcast content. Our method integrates both audio and visual cues\nthrough a two-stage process involving frozen neural networks and a trained LSTM\nnetwork. The first stage extracts essential features from separate modalities,\nwhile the LSTM effectively fuses these features to generate accurate segment\nboundaries. Our proposed model has been evaluated on a diverse dataset\ncomprising over 500 TV newscast videos of an average of 41 minutes gathered\nfrom TF1, a French TV channel, with varying lengths and topics. Experimental\nresults demonstrate that this innovative fusion strategy achieves state of the\nart performance, yielding a high precision rate of 82% at IoU of 90%.\nConsequently, this approach significantly enhances analysis, indexing and\nstorage capabilities for TV newscast archives, paving the way towards efficient\nmanagement and utilization of vast audiovisual resources.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.17590v1"
    },
    {
        "title": "AIMDiT: Modality Augmentation and Interaction via Multimodal Dimension\n  Transformation for Emotion Recognition in Conversations",
        "authors": [
            "Sheng Wu",
            "Jiaxing Liu",
            "Longbiao Wang",
            "Dongxiao He",
            "Xiaobao Wang",
            "Jianwu Dang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Emotion Recognition in Conversations (ERC) is a popular task in natural\nlanguage processing, which aims to recognize the emotional state of the speaker\nin conversations. While current research primarily emphasizes contextual\nmodeling, there exists a dearth of investigation into effective multimodal\nfusion methods. We propose a novel framework called AIMDiT to solve the problem\nof multimodal fusion of deep features. Specifically, we design a Modality\nAugmentation Network which performs rich representation learning through\ndimension transformation of different modalities and parameter-efficient\ninception block. On the other hand, the Modality Interaction Network performs\ninteraction fusion of extracted inter-modal features and intra-modal features.\nExperiments conducted using our AIMDiT framework on the public benchmark\ndataset MELD reveal 2.34% and 2.87% improvements in terms of the Acc-7 and w-F1\nmetrics compared to the state-of-the-art (SOTA) models.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.00743v1"
    },
    {
        "title": "Relating CNN-Transformer Fusion Network for Change Detection",
        "authors": [
            "Yuhao Gao",
            "Gensheng Pei",
            "Mengmeng Sheng",
            "Zeren Sun",
            "Tao Chen",
            "Yazhou Yao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  While deep learning, particularly convolutional neural networks (CNNs), has\nrevolutionized remote sensing (RS) change detection (CD), existing approaches\noften miss crucial features due to neglecting global context and incomplete\nchange learning. Additionally, transformer networks struggle with low-level\ndetails. RCTNet addresses these limitations by introducing \\textbf{(1)} an\nearly fusion backbone to exploit both spatial and temporal features early on,\n\\textbf{(2)} a Cross-Stage Aggregation (CSA) module for enhanced temporal\nrepresentation, \\textbf{(3)} a Multi-Scale Feature Fusion (MSF) module for\nenriched feature extraction in the decoder, and \\textbf{(4)} an Efficient\nSelf-deciphering Attention (ESA) module utilizing transformers to capture\nglobal information and fine-grained details for accurate change detection.\nExtensive experiments demonstrate RCTNet's clear superiority over traditional\nRS image CD methods, showing significant improvement and an optimal balance\nbetween accuracy and computational cost.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.03178v1"
    },
    {
        "title": "Unsupervised Video Summarization via Reinforcement Learning and a\n  Trained Evaluator",
        "authors": [
            "Mehryar Abbasi",
            "Hadi Hadizadeh",
            "Parvaneh Saeedi"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This paper presents a novel approach for unsupervised video summarization\nusing reinforcement learning. It aims to address the existing limitations of\ncurrent unsupervised methods, including unstable training of adversarial\ngenerator-discriminator architectures and reliance on hand-crafted reward\nfunctions for quality evaluation. The proposed method is based on the concept\nthat a concise and informative summary should result in a reconstructed video\nthat closely resembles the original. The summarizer model assigns an importance\nscore to each frame and generates a video summary. In the proposed scheme,\nreinforcement learning, coupled with a unique reward generation pipeline, is\nemployed to train the summarizer model. The reward generation pipeline trains\nthe summarizer to create summaries that lead to improved reconstructions. It\ncomprises a generator model capable of reconstructing masked frames from a\npartially masked video, along with a reward mechanism that compares the\nreconstructed video from the summary against the original. The video generator\nis trained in a self-supervised manner to reconstruct randomly masked frames,\nenhancing its ability to generate accurate summaries. This training pipeline\nresults in a summarizer model that better mimics human-generated video\nsummaries compared to methods relying on hand-crafted rewards. The training\nprocess consists of two stable and isolated training steps, unlike adversarial\narchitectures. Experimental results demonstrate promising performance, with\nF-scores of 62.3 and 54.5 on TVSum and SumMe datasets, respectively.\nAdditionally, the inference stage is 300 times faster than our previously\nreported state-of-the-art method.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.04258v1"
    },
    {
        "title": "Spiking Tucker Fusion Transformer for Audio-Visual Zero-Shot Learning",
        "authors": [
            "Wenrui Li",
            "Penghong Wang",
            "Ruiqin Xiong",
            "Xiaopeng Fan"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The spiking neural networks (SNNs) that efficiently encode temporal sequences\nhave shown great potential in extracting audio-visual joint feature\nrepresentations. However, coupling SNNs (binary spike sequences) with\ntransformers (float-point sequences) to jointly explore the temporal-semantic\ninformation still facing challenges. In this paper, we introduce a novel\nSpiking Tucker Fusion Transformer (STFT) for audio-visual zero-shot learning\n(ZSL). The STFT leverage the temporal and semantic information from different\ntime steps to generate robust representations. The time-step factor (TSF) is\nintroduced to dynamically synthesis the subsequent inference information. To\nguide the formation of input membrane potentials and reduce the spike noise, we\npropose a global-local pooling (GLP) which combines the max and average pooling\noperations. Furthermore, the thresholds of the spiking neurons are dynamically\nadjusted based on semantic and temporal cues. Integrating the temporal and\nsemantic information extracted by SNNs and Transformers are difficult due to\nthe increased number of parameters in a straightforward bilinear model. To\naddress this, we introduce a temporal-semantic Tucker fusion module, which\nachieves multi-scale fusion of SNN and Transformer outputs while maintaining\nfull second-order interactions. Our experimental results demonstrate the\neffectiveness of the proposed approach in achieving state-of-the-art\nperformance in three benchmark datasets. The harmonic mean (HM) improvement of\nVGGSound, UCF101 and ActivityNet are around 15.4\\%, 3.9\\%, and 14.9\\%,\nrespectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.08130v1"
    },
    {
        "title": "Enhancing Emotion Recognition in Incomplete Data: A Novel Cross-Modal\n  Alignment, Reconstruction, and Refinement Framework",
        "authors": [
            "Haoqin Sun",
            "Shiwan Zhao",
            "Shaokai Li",
            "Xiangyu Kong",
            "Xuechen Wang",
            "Aobo Kong",
            "Jiaming Zhou",
            "Yong Chen",
            "Wenjia Zeng",
            "Yong Qin"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Multimodal emotion recognition systems rely heavily on the full availability\nof modalities, suffering significant performance declines when modal data is\nincomplete. To tackle this issue, we present the Cross-Modal Alignment,\nReconstruction, and Refinement (CM-ARR) framework, an innovative approach that\nsequentially engages in cross-modal alignment, reconstruction, and refinement\nphases to handle missing modalities and enhance emotion recognition. This\nframework utilizes unsupervised distribution-based contrastive learning to\nalign heterogeneous modal distributions, reducing discrepancies and modeling\nsemantic uncertainty effectively. The reconstruction phase applies normalizing\nflow models to transform these aligned distributions and recover missing\nmodalities. The refinement phase employs supervised point-based contrastive\nlearning to disrupt semantic correlations and accentuate emotional traits,\nthereby enriching the affective content of the reconstructed representations.\nExtensive experiments on the IEMOCAP and MSP-IMPROV datasets confirm the\nsuperior performance of CM-ARR under conditions of both missing and complete\nmodalities. Notably, averaged across six scenarios of missing modalities,\nCM-ARR achieves absolute improvements of 2.11% in WAR and 2.12% in UAR on the\nIEMOCAP dataset, and 1.71% and 1.96% in WAR and UAR, respectively, on the\nMSP-IMPROV dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.09029v1"
    },
    {
        "title": "Aligning Sight and Sound: Advanced Sound Source Localization Through\n  Audio-Visual Alignment",
        "authors": [
            "Arda Senocak",
            "Hyeonggon Ryu",
            "Junsik Kim",
            "Tae-Hyun Oh",
            "Hanspeter Pfister",
            "Joon Son Chung"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recent studies on learning-based sound source localization have mainly\nfocused on the localization performance perspective. However, prior work and\nexisting benchmarks overlook a crucial aspect: cross-modal interaction, which\nis essential for interactive sound source localization. Cross-modal interaction\nis vital for understanding semantically matched or mismatched audio-visual\nevents, such as silent objects or off-screen sounds. In this paper, we first\ncomprehensively examine the cross-modal interaction of existing methods,\nbenchmarks, evaluation metrics, and cross-modal understanding tasks. Then, we\nidentify the limitations of previous studies and make several contributions to\novercome the limitations. First, we introduce a new synthetic benchmark for\ninteractive sound source localization. Second, we introduce new evaluation\nmetrics to rigorously assess sound source localization methods, focusing on\naccurately evaluating both localization performance and cross-modal interaction\nability. Third, we propose a learning framework with a cross-modal alignment\nstrategy to enhance cross-modal interaction. Lastly, we evaluate both\ninteractive sound source localization and auxiliary cross-modal retrieval tasks\ntogether to thoroughly assess cross-modal interaction capabilities and\nbenchmark competing methods. Our new benchmarks and evaluation metrics reveal\npreviously overlooked issues in sound source localization studies. Our proposed\nnovel method, with enhanced cross-modal alignment, shows superior sound source\nlocalization performance. This work provides the most comprehensive analysis of\nsound source localization to date, with extensive validation of competing\nmethods on both existing and new benchmarks using new and standard evaluation\nmetrics.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.13676v1"
    },
    {
        "title": "Coarse-to-Fine Proposal Refinement Framework for Audio Temporal Forgery\n  Detection and Localization",
        "authors": [
            "Junyan Wu",
            "Wei Lu",
            "Xiangyang Luo",
            "Rui Yang",
            "Qian Wang",
            "Xiaochun Cao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recently, a novel form of audio partial forgery has posed challenges to its\nforensics, requiring advanced countermeasures to detect subtle forgery\nmanipulations within long-duration audio. However, existing countermeasures\nstill serve a classification purpose and fail to perform meaningful analysis of\nthe start and end timestamps of partial forgery segments. To address this\nchallenge, we introduce a novel coarse-to-fine proposal refinement framework\n(CFPRF) that incorporates a frame-level detection network (FDN) and a proposal\nrefinement network (PRN) for audio temporal forgery detection and localization.\nSpecifically, the FDN aims to mine informative inconsistency cues between real\nand fake frames to obtain discriminative features that are beneficial for\nroughly indicating forgery regions. The PRN is responsible for predicting\nconfidence scores and regression offsets to refine the coarse-grained proposals\nderived from the FDN. To learn robust discriminative features, we devise a\ndifference-aware feature learning (DAFL) module guided by contrastive\nrepresentation learning to enlarge the sensitive differences between different\nframes induced by minor manipulations. We further design a boundary-aware\nfeature enhancement (BAFE) module to capture the contextual information of\nmultiple transition boundaries and guide the interaction between boundary\ninformation and temporal features via a cross-attention mechanism. Extensive\nexperiments show that our CFPRF achieves state-of-the-art performance on\nvarious datasets, including LAV-DF, ASVS2019PS, and HAD.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.16554v1"
    },
    {
        "title": "Revolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken\n  Generation",
        "authors": [
            "Yongqi Li",
            "Hongru Cai",
            "Wenjie Wang",
            "Leigang Qu",
            "Yinwei Wei",
            "Wenjie Li",
            "Liqiang Nie",
            "Tat-Seng Chua"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Text-to-image retrieval is a fundamental task in multimedia processing,\naiming to retrieve semantically relevant cross-modal content. Traditional\nstudies have typically approached this task as a discriminative problem,\nmatching the text and image via the cross-attention mechanism (one-tower\nframework) or in a common embedding space (two-tower framework). Recently,\ngenerative cross-modal retrieval has emerged as a new research line, which\nassigns images with unique string identifiers and generates the target\nidentifier as the retrieval target. Despite its great potential, existing\ngenerative approaches are limited due to the following issues: insufficient\nvisual information in identifiers, misalignment with high-level semantics, and\nlearning gap towards the retrieval target. To address the above issues, we\npropose an autoregressive voken generation method, named AVG. AVG tokenizes\nimages into vokens, i.e., visual tokens, and innovatively formulates the\ntext-to-image retrieval task as a token-to-voken generation problem. AVG\ndiscretizes an image into a sequence of vokens as the identifier of the image,\nwhile maintaining the alignment with both the visual information and high-level\nsemantics of the image. Additionally, to bridge the learning gap between\ngenerative training and the retrieval target, we incorporate discriminative\ntraining to modify the learning direction during token-to-voken training.\nExtensive experiments demonstrate that AVG achieves superior results in both\neffectiveness and efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.17274v1"
    },
    {
        "title": "ReCorD: Reasoning and Correcting Diffusion for HOI Generation",
        "authors": [
            "Jian-Yu Jiang-Lin",
            "Kang-Yang Huang",
            "Ling Lo",
            "Yi-Ning Huang",
            "Terence Lin",
            "Jhih-Ciang Wu",
            "Hong-Han Shuai",
            "Wen-Huang Cheng"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Diffusion models revolutionize image generation by leveraging natural\nlanguage to guide the creation of multimedia content. Despite significant\nadvancements in such generative models, challenges persist in depicting\ndetailed human-object interactions, especially regarding pose and object\nplacement accuracy. We introduce a training-free method named Reasoning and\nCorrecting Diffusion (ReCorD) to address these challenges. Our model couples\nLatent Diffusion Models with Visual Language Models to refine the generation\nprocess, ensuring precise depictions of HOIs. We propose an interaction-aware\nreasoning module to improve the interpretation of the interaction, along with\nan interaction correcting module to refine the output image for more precise\nHOI generation delicately. Through a meticulous process of pose selection and\nobject positioning, ReCorD achieves superior fidelity in generated images while\nefficiently reducing computational requirements. We conduct comprehensive\nexperiments on three benchmarks to demonstrate the significant progress in\nsolving text-to-image generation tasks, showcasing ReCorD's ability to render\ncomplex interactions accurately by outperforming existing methods in HOI\nclassification score, as well as FID and Verb CLIP-Score. Project website is\navailable at https://alberthkyhky.github.io/ReCorD/ .\n",
        "pdf_link": "http://arxiv.org/pdf/2407.17911v1"
    },
    {
        "title": "ASR-enhanced Multimodal Representation Learning for Cross-Domain Product\n  Retrieval",
        "authors": [
            "Ruixiang Zhao",
            "Jian Jia",
            "Yan Li",
            "Xuehan Bai",
            "Quan Chen",
            "Han Li",
            "Peng Jiang",
            "Xirong Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.02978v1"
    },
    {
        "title": "MultiHateClip: A Multilingual Benchmark Dataset for Hateful Video\n  Detection on YouTube and Bilibili",
        "authors": [
            "Han Wang",
            "Tan Rui Yang",
            "Usman Naseem",
            "Roy Ka-Wei Lee"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Hate speech is a pressing issue in modern society, with significant effects\nboth online and offline. Recent research in hate speech detection has primarily\ncentered on text-based media, largely overlooking multimodal content such as\nvideos. Existing studies on hateful video datasets have predominantly focused\non English content within a Western context and have been limited to binary\nlabels (hateful or non-hateful), lacking detailed contextual information. This\nstudy presents MultiHateClip1 , an novel multilingual dataset created through\nhate lexicons and human annotation. It aims to enhance the detection of hateful\nvideos on platforms such as YouTube and Bilibili, including content in both\nEnglish and Chinese languages. Comprising 2,000 videos annotated for\nhatefulness, offensiveness, and normalcy, this dataset provides a\ncross-cultural perspective on gender-based hate speech. Through a detailed\nexamination of human annotation results, we discuss the differences between\nChinese and English hateful videos and underscore the importance of different\nmodalities in hateful and offensive video analysis. Evaluations of\nstate-of-the-art video classification models, such as VLM, GPT-4V and Qwen-VL,\non MultiHateClip highlight the existing challenges in accurately distinguishing\nbetween hateful and offensive content and the urgent need for models that are\nboth multimodally and culturally nuanced. MultiHateClip represents a\nfoundational advance in enhancing hateful video detection by underscoring the\nnecessity of a multimodal and culturally sensitive approach in combating online\nhate speech.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.03468v2"
    },
    {
        "title": "Benchmarking Conventional and Learned Video Codecs with a Low-Delay\n  Configuration",
        "authors": [
            "Siyue Teng",
            "Yuxuan Jiang",
            "Ge Gao",
            "Fan Zhang",
            "Thomas Davis",
            "Zoe Liu",
            "David Bull"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Recent advances in video compression have seen significant coding performance\nimprovements with the development of new standards and learning-based video\ncodecs. However, most of these works focus on application scenarios that allow\na certain amount of system delay (e.g., Random Access mode in MPEG codecs),\nwhich is not always acceptable for live delivery. This paper conducts a\ncomparative study of state-of-the-art conventional and learned video coding\nmethods based on a low delay configuration. Specifically, this study includes\ntwo MPEG standard codecs (H.266/VVC VTM and JVET ECM), two AOM codecs (AV1\nlibaom and AVM), and two recent neural video coding models (DCVC-DC and\nDCVC-FM). To allow a fair and meaningful comparison, the evaluation was\nperformed on test sequences defined in the AOM and MPEG common test conditions\nin the YCbCr 4:2:0 color space. The evaluation results show that the JVET ECM\ncodecs offer the best overall coding performance among all codecs tested, with\na 16.1% (based on PSNR) average BD-rate saving over AOM AVM, and 11.0% over\nDCVC-FM. We also observed inconsistent performance with the learned video\ncodecs, DCVC-DC and DCVC-FM, for test content with large background motions.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.05042v1"
    },
    {
        "title": "Palantir: Towards Efficient Super Resolution for Ultra-high-definition\n  Live Streaming",
        "authors": [
            "Xinqi Jin",
            "Zhui Zhu",
            "Xikai Sun",
            "Fan Dang",
            "Jiangchuan Liu",
            "Jingao Xu",
            "Kebin Liu",
            "Xinlei Chen",
            "Yunhao Liu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Neural enhancement through super-resolution (SR) deep neural networks (DNNs)\nopens up new possibilities for ultra-high-definition (UHD) live streaming over\nexisting encoding and networking infrastructure. Yet, the heavy SR DNN\ninference overhead leads to severe deployment challenges. To reduce the\noverhead, existing systems propose to apply DNN-based SR only on carefully\nselected anchor frames while upscaling non-anchor frames via the lightweight\nreusing-based SR approach. However, frame-level scheduling is coarse-grained\nand fails to deliver optimal efficiency. In this work, we propose Palantir, the\nfirst neural-enhanced UHD live streaming system with fine-grained patch-level\nscheduling. Two novel techniques are incorporated into Palantir to select the\nmost beneficial anchor patches and support latency-sensitive UHD live streaming\napplications. Firstly, under the guidance of our pioneering and theoretical\nanalysis, Palantir constructs a directed acyclic graph (DAG) for lightweight\nyet accurate SR quality estimation under any possible anchor patch set.\nSecondly, to further optimize the scheduling latency, Palantir improves\nparallelizability by refactoring the computation subprocedure of the estimation\nprocess into a sparse matrix-matrix multiplication operation.\n  The evaluation results suggest that Palantir incurs a negligible scheduling\nlatency accounting for less than 5.7% of the end-to-end latency requirement.\nWhen compared to the naive method of applying DNN-based SR on all the frames,\nPalantir can reduce the SR DNN inference overhead by 20 times (or 60 times)\nwhile preserving 54.0-82.6% (or 32.8-64.0%) of the quality gain. When compared\nto the state-of-the-art real-time frame-level scheduling strategy, Palantir can\nreduce the SR DNN inference overhead by 80.1% at most (and 38.4% on average)\nwithout sacrificing the video quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.06152v2"
    },
    {
        "title": "An Efficient and Explanatory Image and Text Clustering System with\n  Multimodal Autoencoder Architecture",
        "authors": [
            "Tiancheng Shi",
            "Yuanchen Wei",
            "John R. Kender"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  We demonstrate the efficiencies and explanatory abilities of extensions to\nthe common tools of Autoencoders and LLM interpreters, in the novel context of\ncomparing different cultural approaches to the same international news event.\nWe develop a new Convolutional-Recurrent Variational Autoencoder (CRVAE) model\nthat extends the modalities of previous CVAE models, by using fully-connected\nlatent layers to embed in parallel the CNN encodings of video frames, together\nwith the LSTM encodings of their related text derived from audio. We\nincorporate the model within a larger system that includes frame-caption\nalignment, latent space vector clustering, and a novel LLM-based cluster\ninterpreter. We measure, tune, and apply this system to the task of summarizing\na video into three to five thematic clusters, with each theme described by ten\nLLM-produced phrases. We apply this system to two news topics, COVID-19 and the\nWinter Olympics, and five other topics are in progress.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.07791v1"
    },
    {
        "title": "Regularized Contrastive Partial Multi-view Outlier Detection",
        "authors": [
            "Yijia Wang",
            "Qianqian Xu",
            "Yangbangyan Jiang",
            "Siran Dai",
            "Qingming Huang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In recent years, multi-view outlier detection (MVOD) methods have advanced\nsignificantly, aiming to identify outliers within multi-view datasets. A key\npoint is to better detect class outliers and class-attribute outliers, which\nonly exist in multi-view data. However, existing methods either is not able to\nreduce the impact of outliers when learning view-consistent information, or\nstruggle in cases with varying neighborhood structures. Moreover, most of them\ndo not apply to partial multi-view data in real-world scenarios. To overcome\nthese drawbacks, we propose a novel method named Regularized Contrastive\nPartial Multi-view Outlier Detection (RCPMOD). In this framework, we utilize\ncontrastive learning to learn view-consistent information and distinguish\noutliers by the degree of consistency. Specifically, we propose (1) An\noutlier-aware contrastive loss with a potential outlier memory bank to\neliminate their bias motivated by a theoretical analysis. (2) A neighbor\nalignment contrastive loss to capture the view-shared local structural\ncorrelation. (3) A spreading regularization loss to prevent the model from\noverfitting over outliers. With the Cross-view Relation Transfer technique, we\ncould easily impute the missing view samples based on the features of\nneighbors. Experimental results on four benchmark datasets demonstrate that our\nproposed approach could outperform state-of-the-art competitors under different\nsettings.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.07819v1"
    },
    {
        "title": "Enhancing Modal Fusion by Alignment and Label Matching for Multimodal\n  Emotion Recognition",
        "authors": [
            "Qifei Li",
            "Yingming Gao",
            "Yuhua Wen",
            "Cong Wang",
            "Ya Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  To address the limitation in multimodal emotion recognition (MER) performance\narising from inter-modal information fusion, we propose a novel MER framework\nbased on multitask learning where fusion occurs after alignment, called\nFoal-Net. The framework is designed to enhance the effectiveness of modality\nfusion and includes two auxiliary tasks: audio-video emotion alignment (AVEL)\nand cross-modal emotion label matching (MEM). First, AVEL achieves alignment of\nemotional information in audio-video representations through contrastive\nlearning. Then, a modal fusion network integrates the aligned features.\nMeanwhile, MEM assesses whether the emotions of the current sample pair are the\nsame, providing assistance for modal information fusion and guiding the model\nto focus more on emotional information. The experimental results conducted on\nIEMOCAP corpus show that Foal-Net outperforms the state-of-the-art methods and\nemotion alignment is necessary before modal fusion.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09438v1"
    },
    {
        "title": "SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for\n  Multimodal Emotion Recognition",
        "authors": [
            "Zebang Cheng",
            "Shuyuan Tu",
            "Dawei Huang",
            "Minghan Li",
            "Xiaojiang Peng",
            "Zhi-Qi Cheng",
            "Alexander G. Hauptmann"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This paper presents our winning approach for the MER-NOISE and MER-OV tracks\nof the MER2024 Challenge on multimodal emotion recognition. Our system\nleverages the advanced emotional understanding capabilities of Emotion-LLaMA to\ngenerate high-quality annotations for unlabeled samples, addressing the\nchallenge of limited labeled data. To enhance multimodal fusion while\nmitigating modality-specific noise, we introduce Conv-Attention, a lightweight\nand efficient hybrid framework. Extensive experimentation vali-dates the\neffectiveness of our approach. In the MER-NOISE track, our system achieves a\nstate-of-the-art weighted average F-score of 85.30%, surpassing the second and\nthird-place teams by 1.47% and 1.65%, respectively. For the MER-OV track, our\nutilization of Emotion-LLaMA for open-vocabulary annotation yields an 8.52%\nimprovement in average accuracy and recall compared to GPT-4V, securing the\nhighest score among all participating large multimodal models. The code and\nmodel for Emotion-LLaMA are available at\nhttps://github.com/ZebangCheng/Emotion-LLaMA.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.10500v2"
    },
    {
        "title": "MCDubber: Multimodal Context-Aware Expressive Video Dubbing",
        "authors": [
            "Yuan Zhao",
            "Zhenqi Jia",
            "Rui Liu",
            "De Hu",
            "Feilong Bao",
            "Guanglai Gao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Automatic Video Dubbing (AVD) aims to take the given script and generate\nspeech that aligns with lip motion and prosody expressiveness. Current AVD\nmodels mainly utilize visual information of the current sentence to enhance the\nprosody of synthesized speech. However, it is crucial to consider whether the\nprosody of the generated dubbing aligns with the multimodal context, as the\ndubbing will be combined with the original context in the final video. This\naspect has been overlooked in previous studies. To address this issue, we\npropose a Multimodal Context-aware video Dubbing model, termed\n\\textbf{MCDubber}, to convert the modeling object from a single sentence to a\nlonger sequence with context information to ensure the consistency of the\nglobal context prosody. MCDubber comprises three main components: (1) A context\nduration aligner aims to learn the context-aware alignment between the text and\nlip frames; (2) A context prosody predictor seeks to read the global context\nvisual sequence and predict the context-aware global energy and pitch; (3) A\ncontext acoustic decoder ultimately predicts the global context mel-spectrogram\nwith the assistance of adjacent ground-truth mel-spectrograms of the target\nsentence. Through this process, MCDubber fully considers the influence of\nmultimodal context on the prosody expressiveness of the current sentence when\ndubbing. The extracted mel-spectrogram belonging to the target sentence from\nthe output context mel-spectrograms is the final required dubbing audio.\nExtensive experiments on the Chem benchmark dataset demonstrate that our\nMCDubber significantly improves dubbing expressiveness compared to all advanced\nbaselines. The code and demos are available at\nhttps://github.com/XiaoYuanJun-zy/MCDubber.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.11593v3"
    },
    {
        "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
        "authors": [
            "Xianzhi Zhang",
            "Yipeng Zhou",
            "Di Wu",
            "Quan Z. Sheng",
            "Miao Hu",
            "Linchang Xiao"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.14735v1"
    },
    {
        "title": "Human-Inspired Audio-Visual Speech Recognition: Spike Activity, Cueing\n  Interaction and Causal Processing",
        "authors": [
            "Qianhui Liu",
            "Jiadong Wang",
            "Yang Wang",
            "Xin Yang",
            "Gang Pan",
            "Haizhou Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Humans naturally perform audiovisual speech recognition (AVSR), enhancing the\naccuracy and robustness by integrating auditory and visual information. Spiking\nneural networks (SNNs), which mimic the brain's information-processing\nmechanisms, are well-suited for emulating the human capability of AVSR. Despite\ntheir potential, research on SNNs for AVSR is scarce, with most existing\naudio-visual multimodal methods focused on object or digit recognition. These\nmodels simply integrate features from both modalities, neglecting their unique\ncharacteristics and interactions. Additionally, they often rely on future\ninformation for current processing, which increases recognition latency and\nlimits real-time applicability. Inspired by human speech perception, this paper\nproposes a novel human-inspired SNN named HI-AVSNN for AVSR, incorporating\nthree key characteristics: cueing interaction, causal processing and spike\nactivity. For cueing interaction, we propose a visual-cued auditory attention\nmodule (VCA2M) that leverages visual cues to guide attention to auditory\nfeatures. We achieve causal processing by aligning the SNN's temporal dimension\nwith that of visual and auditory features and applying temporal masking to\nutilize only past and current information. To implement spike activity, in\naddition to using SNNs, we leverage the event camera to capture lip movement as\nspikes, mimicking the human retina and providing efficient visual data. We\nevaluate HI-AVSNN on an audiovisual speech recognition dataset combining the\nDVS-Lip dataset with its corresponding audio samples. Experimental results\ndemonstrate the superiority of our proposed fusion method, outperforming\nexisting audio-visual SNN fusion methods and achieving a 2.27% improvement in\naccuracy over the only existing SNN-based AVSR method.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.16564v1"
    },
    {
        "title": "Detecting Misinformation in Multimedia Content through Cross-Modal\n  Entity Consistency: A Dual Learning Approach",
        "authors": [
            "Zhe Fu",
            "Kanlun Wang",
            "Wangjiaxuan Xin",
            "Lina Zhou",
            "Shi Chen",
            "Yaorong Ge",
            "Daniel Janies",
            "Dongsong Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The landscape of social media content has evolved significantly, extending\nfrom text to multimodal formats. This evolution presents a significant\nchallenge in combating misinformation. Previous research has primarily focused\non single modalities or text-image combinations, leaving a gap in detecting\nmultimodal misinformation. While the concept of entity consistency holds\npromise in detecting multimodal misinformation, simplifying the representation\nto a scalar value overlooks the inherent complexities of high-dimensional\nrepresentations across different modalities. To address these limitations, we\npropose a Multimedia Misinformation Detection (MultiMD) framework for detecting\nmisinformation from video content by leveraging cross-modal entity consistency.\nThe proposed dual learning approach allows for not only enhancing\nmisinformation detection performance but also improving representation learning\nof entity consistency across different modalities. Our results demonstrate that\nMultiMD outperforms state-of-the-art baseline models and underscore the\nimportance of each modality in misinformation detection. Our research provides\nnovel methodological and technical insights into multimodal misinformation\ndetection.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.00022v1"
    },
    {
        "title": "Unveiling Visual Biases in Audio-Visual Localization Benchmarks",
        "authors": [
            "Liangyu Chen",
            "Zihao Yue",
            "Boshen Xu",
            "Qin Jin"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Audio-Visual Source Localization (AVSL) aims to localize the source of sound\nwithin a video. In this paper, we identify a significant issue in existing\nbenchmarks: the sounding objects are often easily recognized based solely on\nvisual cues, which we refer to as visual bias. Such biases hinder these\nbenchmarks from effectively evaluating AVSL models. To further validate our\nhypothesis regarding visual biases, we examine two representative AVSL\nbenchmarks, VGG-SS and EpicSounding-Object, where the vision-only models\noutperform all audiovisual baselines. Our findings suggest that existing AVSL\nbenchmarks need further refinement to facilitate audio-visual learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.06709v1"
    },
    {
        "title": "VMAS: Video-to-Music Generation via Semantic Alignment in Web Music\n  Videos",
        "authors": [
            "Yan-Bo Lin",
            "Yu Tian",
            "Linjie Yang",
            "Gedas Bertasius",
            "Heng Wang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  We present a framework for learning to generate background music from video\ninputs. Unlike existing works that rely on symbolic musical annotations, which\nare limited in quantity and diversity, our method leverages large-scale web\nvideos accompanied by background music. This enables our model to learn to\ngenerate realistic and diverse music. To accomplish this goal, we develop a\ngenerative video-music Transformer with a novel semantic video-music alignment\nscheme. Our model uses a joint autoregressive and contrastive learning\nobjective, which encourages the generation of music aligned with high-level\nvideo content. We also introduce a novel video-beat alignment scheme to match\nthe generated music beats with the low-level motions in the video. Lastly, to\ncapture fine-grained visual cues in a video needed for realistic background\nmusic generation, we introduce a new temporal video encoder architecture,\nallowing us to efficiently process videos consisting of many densely sampled\nframes. We train our framework on our newly curated DISCO-MV dataset,\nconsisting of 2.2M video-music samples, which is orders of magnitude larger\nthan any prior datasets used for video music generation. Our method outperforms\nexisting approaches on the DISCO-MV and MusicCaps datasets according to various\nmusic generation evaluation metrics, including human evaluation. Results are\navailable at https://genjib.github.io/project_page/VMAs/index.html\n",
        "pdf_link": "http://arxiv.org/pdf/2409.07450v1"
    },
    {
        "title": "Resource-Efficient Reference-Free Evaluation of Audio Captions",
        "authors": [
            "Rehana Mahfuz",
            "Yinyi Guo",
            "Erik Visser"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  To establish the trustworthiness of systems that automatically generate text\ncaptions for audio, images and video, existing reference-free metrics rely on\nlarge pretrained models which are impractical to accommodate in\nresource-constrained settings. To address this, we propose some metrics to\nelicit the model's confidence in its own generation. To assess how well these\nmetrics replace correctness measures that leverage reference captions, we test\ntheir calibration with correctness measures. We discuss why some of these\nconfidence metrics align better with certain correctness measures. Further, we\nprovide insight into why temperature scaling of confidence metrics is\neffective. Our main contribution is a suite of well-calibrated lightweight\nconfidence metrics for reference-free evaluation of captions in\nresource-constrained settings.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08489v2"
    },
    {
        "title": "The Practice of Averaging Rate-Distortion Curves over Testsets to\n  Compare Learned Video Codecs Can Cause Misleading Conclusions",
        "authors": [
            "M. Akin Yilmaz",
            "Onur Keleş",
            "A. Murat Tekalp"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This paper aims to demonstrate how the prevalent practice in the learned\nvideo compression community of averaging rate-distortion (RD) curves across a\ntest video set can lead to misleading conclusions in evaluating codec\nperformance. Through analytical analysis of a simple case and experimental\nresults with two recent learned video codecs, we show how averaged RD curves\ncan mislead comparative evaluation of different codecs, particularly when\nvideos in a dataset have varying characteristics and operating ranges. We\nillustrate how a single video with distinct RD characteristics from the rest of\nthe test set can disproportionately influence the average RD curve, potentially\novershadowing a codec's superior performance across most individual sequences.\nUsing two recent learned video codecs on the UVG dataset as a case study, we\ndemonstrate computing performance metrics, such as the BD rate, from the\naverage RD curve suggests conclusions that contradict those reached from\ncalculating the average of per-sequence metrics. Hence, we argue that the\nlearned video compression community should also report per-sequence RD curves\nand performance metrics for a test set should be computed from the average of\nper-sequence metrics, similar to the established practice in traditional video\ncoding, to ensure fair and accurate codec comparisons.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08772v2"
    },
    {
        "title": "DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical\n  Diffusion for Audio-driven Talking Head Synthesis",
        "authors": [
            "Fa-Ting Hong",
            "Yunfei Liu",
            "Yu Li",
            "Changyin Zhou",
            "Fei Yu",
            "Dan Xu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Audio-driven talking head synthesis strives to generate lifelike video\nportraits from provided audio. The diffusion model, recognized for its superior\nquality and robust generalization, has been explored for this task. However,\nestablishing a robust correspondence between temporal audio cues and\ncorresponding spatial facial expressions with diffusion models remains a\nsignificant challenge in talking head generation. To bridge this gap, we\npresent DreamHead, a hierarchical diffusion framework that learns\nspatial-temporal correspondences in talking head synthesis without compromising\nthe model's intrinsic quality and adaptability.~DreamHead learns to predict\ndense facial landmarks from audios as intermediate signals to model the spatial\nand temporal correspondences.~Specifically, a first hierarchy of\naudio-to-landmark diffusion is first designed to predict temporally smooth and\naccurate landmark sequences given audio sequence signals. Then, a second\nhierarchy of landmark-to-image diffusion is further proposed to produce\nspatially consistent facial portrait videos, by modeling spatial\ncorrespondences between the dense facial landmark and appearance. Extensive\nexperiments show that proposed DreamHead can effectively learn spatial-temporal\nconsistency with the designed hierarchical diffusion and produce high-fidelity\naudio-driven talking head videos for multiple identities.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.10281v1"
    },
    {
        "title": "\"Is This It?\": Towards Ecologically Valid Benchmarks for Situated\n  Collaboration",
        "authors": [
            "Dan Bohus",
            "Sean Andrist",
            "Yuwei Bao",
            "Eric Horvitz",
            "Ann Paradiso"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  We report initial work towards constructing ecologically valid benchmarks to\nassess the capabilities of large multimodal models for engaging in situated\ncollaboration. In contrast to existing benchmarks, in which question-answer\npairs are generated post hoc over preexisting or synthetic datasets via\ntemplates, human annotators, or large language models (LLMs), we propose and\ninvestigate an interactive system-driven approach, where the questions are\ngenerated by users in context, during their interactions with an end-to-end\nsituated AI system. We illustrate how the questions that arise are different in\nform and content from questions typically found in existing embodied question\nanswering (EQA) benchmarks and discuss new real-world challenge problems\nbrought to the fore.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.10525v1"
    },
    {
        "title": "Towards Effective User Attribution for Latent Diffusion Models via\n  Watermark-Informed Blending",
        "authors": [
            "Yongyang Pan",
            "Xiaohong Liu",
            "Siqi Luo",
            "Yi Xin",
            "Xiao Guo",
            "Xiaoming Liu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Rapid advancements in multimodal large language models have enabled the\ncreation of hyper-realistic images from textual descriptions. However, these\nadvancements also raise significant concerns about unauthorized use, which\nhinders their broader distribution. Traditional watermarking methods often\nrequire complex integration or degrade image quality. To address these\nchallenges, we introduce a novel framework Towards Effective user Attribution\nfor latent diffusion models via Watermark-Informed Blending (TEAWIB). TEAWIB\nincorporates a unique ready-to-use configuration approach that allows seamless\nintegration of user-specific watermarks into generative models. This approach\nensures that each user can directly apply a pre-configured set of parameters to\nthe model without altering the original model parameters or compromising image\nquality. Additionally, noise and augmentation operations are embedded at the\npixel level to further secure and stabilize watermarked images. Extensive\nexperiments validate the effectiveness of TEAWIB, showcasing the\nstate-of-the-art performance in perceptual quality and attribution accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.10958v2"
    },
    {
        "title": "DETECLAP: Enhancing Audio-Visual Representation Learning with Object\n  Information",
        "authors": [
            "Shota Nakada",
            "Taichi Nishimura",
            "Hokuto Munakata",
            "Masayoshi Kondo",
            "Tatsuya Komatsu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Current audio-visual representation learning can capture rough object\ncategories (e.g., ``animals'' and ``instruments''), but it lacks the ability to\nrecognize fine-grained details, such as specific categories like ``dogs'' and\n``flutes'' within animals and instruments. To address this issue, we introduce\nDETECLAP, a method to enhance audio-visual representation learning with object\ninformation. Our key idea is to introduce an audio-visual label prediction loss\nto the existing Contrastive Audio-Visual Masked AutoEncoder to enhance its\nobject awareness. To avoid costly manual annotations, we prepare object labels\nfrom both audio and visual inputs using state-of-the-art language-audio models\nand object detectors. We evaluate the method of audio-visual retrieval and\nclassification using the VGGSound and AudioSet20K datasets. Our method achieves\nimprovements in recall@10 of +1.5% and +1.2% for audio-to-visual and\nvisual-to-audio retrieval, respectively, and an improvement in accuracy of\n+0.6% for audio-visual classification.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.11729v1"
    },
    {
        "title": "RoWSFormer: A Robust Watermarking Framework with Swin Transformer for\n  Enhanced Geometric Attack Resilience",
        "authors": [
            "Weitong Chen",
            "Yuheng Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In recent years, digital watermarking techniques based on deep learning have\nbeen widely studied. To achieve both imperceptibility and robustness of image\nwatermarks, most current methods employ convolutional neural networks to build\nrobust watermarking frameworks. However, despite the success of CNN-based\nwatermarking models, they struggle to achieve robustness against geometric\nattacks due to the limitations of convolutional neural networks in capturing\nglobal and long-range relationships. To address this limitation, we propose a\nrobust watermarking framework based on the Swin Transformer, named RoWSFormer.\nSpecifically, we design the Locally-Channel Enhanced Swin Transformer Block as\nthe core of both the encoder and decoder. This block utilizes the\nself-attention mechanism to capture global and long-range information, thereby\nsignificantly improving adaptation to geometric distortions. Additionally, we\nconstruct the Frequency-Enhanced Transformer Block to extract frequency domain\ninformation, which further strengthens the robustness of the watermarking\nframework. Experimental results demonstrate that our RoWSFormer surpasses\nexisting state-of-the-art watermarking methods. For most non-geometric attacks,\nRoWSFormer improves the PSNR by 3 dB while maintaining the same extraction\naccuracy. In the case of geometric attacks (such as rotation, scaling, and\naffine transformations), RoWSFormer achieves over a 6 dB improvement in PSNR,\nwith extraction accuracy exceeding 97\\%.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.14829v1"
    },
    {
        "title": "FastTalker: Jointly Generating Speech and Conversational Gestures from\n  Text",
        "authors": [
            "Zixin Guo",
            "Jian Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Generating 3D human gestures and speech from a text script is critical for\ncreating realistic talking avatars. One solution is to leverage separate\npipelines for text-to-speech (TTS) and speech-to-gesture (STG), but this\napproach suffers from poor alignment of speech and gestures and slow inference\ntimes. In this paper, we introduce FastTalker, an efficient and effective\nframework that simultaneously generates high-quality speech audio and 3D human\ngestures at high inference speeds. Our key insight is reusing the intermediate\nfeatures from speech synthesis for gesture generation, as these features\ncontain more precise rhythmic information than features re-extracted from\ngenerated speech. Specifically, 1) we propose an end-to-end framework that\nconcurrently generates speech waveforms and full-body gestures, using\nintermediate speech features such as pitch, onset, energy, and duration\ndirectly for gesture decoding; 2) we redesign the causal network architecture\nto eliminate dependencies on future inputs for real applications; 3) we employ\nReinforcement Learning-based Neural Architecture Search (NAS) to enhance both\nperformance and inference speed by optimizing our network architecture.\nExperimental results on the BEAT2 dataset demonstrate that FastTalker achieves\nstate-of-the-art performance in both speech synthesis and gesture generation,\nprocessing speech and gestures in 0.17 seconds per second on an NVIDIA 3090.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.16404v1"
    },
    {
        "title": "Subjective and Objective Quality-of-Experience Evaluation Study for Live\n  Video Streaming",
        "authors": [
            "Zehao Zhu",
            "Wei Sun",
            "Jun Jia",
            "Wei Wu",
            "Sibin Deng",
            "Kai Li",
            "Ying Chen",
            "Xiongkuo Min",
            "Jia Wang",
            "Guangtao Zhai"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In recent years, live video streaming has gained widespread popularity across\nvarious social media platforms. Quality of experience (QoE), which reflects\nend-users' satisfaction and overall experience, plays a critical role for media\nservice providers to optimize large-scale live compression and transmission\nstrategies to achieve perceptually optimal rate-distortion trade-off. Although\nmany QoE metrics for video-on-demand (VoD) have been proposed, there remain\nsignificant challenges in developing QoE metrics for live video streaming. To\nbridge this gap, we conduct a comprehensive study of subjective and objective\nQoE evaluations for live video streaming. For the subjective QoE study, we\nintroduce the first live video streaming QoE dataset, TaoLive QoE, which\nconsists of $42$ source videos collected from real live broadcasts and $1,155$\ncorresponding distorted ones degraded due to a variety of streaming\ndistortions, including conventional streaming distortions such as compression,\nstalling, as well as live streaming-specific distortions like frame skipping,\nvariable frame rate, etc. Subsequently, a human study was conducted to derive\nsubjective QoE scores of videos in the TaoLive QoE dataset. For the objective\nQoE study, we benchmark existing QoE models on the TaoLive QoE dataset as well\nas publicly available QoE datasets for VoD scenarios, highlighting that current\nmodels struggle to accurately assess video QoE, particularly for live content.\nHence, we propose an end-to-end QoE evaluation model, Tao-QoE, which integrates\nmulti-scale semantic features and optical flow-based motion features to\npredicting a retrospective QoE score, eliminating reliance on statistical\nquality of service (QoS) features.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.17596v1"
    },
    {
        "title": "Early Joint Learning of Emotion Information Makes MultiModal Model\n  Understand You Better",
        "authors": [
            "Mengying Ge",
            "Mingyang Li",
            "Dongkai Tang",
            "Pengbo Li",
            "Kuo Liu",
            "Shuhao Deng",
            "Songbai Pu",
            "Long Liu",
            "Yang Song",
            "Tao Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we present our solutions for emotion recognition in the\nsub-challenges of Multimodal Emotion Recognition Challenge (MER2024). To\nmitigate the modal competition issue between audio and text, we adopt an early\nfusion strategy based on a large language model, where joint training of audio\nand text is conducted initially. And the joint Audio-Text modal feature will be\nlate-fused with other unimodal features. In order to solve the problems of data\ninsufficiency and class imbalance, We use multiple turns of multi-model voting\nfor data mining. Moreover, to enhance the quality of audio features, we employ\nspeech source separation to preprocess audios. Our model ranks \\textbf{2nd} in\nboth MER2024-SEMI and MER2024-NOISE, validating our method's effectiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.18971v1"
    },
    {
        "title": "IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding",
        "authors": [
            "Pengcheng Li",
            "Xulong Zhang",
            "Jing Xiao",
            "Jianzong Wang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The audio watermarking technique embeds messages into audio and accurately\nextracts messages from the watermarked audio. Traditional methods develop\nalgorithms based on expert experience to embed watermarks into the time-domain\nor transform-domain of signals. With the development of deep neural networks,\ndeep learning-based neural audio watermarking has emerged. Compared to\ntraditional algorithms, neural audio watermarking achieves better robustness by\nconsidering various attacks during training. However, current neural\nwatermarking methods suffer from low capacity and unsatisfactory\nimperceptibility. Additionally, the issue of watermark locating, which is\nextremely important and even more pronounced in neural audio watermarking, has\nnot been adequately studied. In this paper, we design a dual-embedding\nwatermarking model for efficient locating. We also consider the impact of the\nattack layer on the invertible neural network in robustness training, improving\nthe model to enhance both its reasonableness and stability. Experiments show\nthat the proposed model, IDEAW, can withstand various attacks with higher\ncapacity and more efficient locating ability compared to existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.19627v1"
    },
    {
        "title": "Art2Mus: Bridging Visual Arts and Music through Cross-Modal Generation",
        "authors": [
            "Ivan Rinaldi",
            "Nicola Fanelli",
            "Giovanna Castellano",
            "Gennaro Vessio"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Artificial Intelligence and generative models have revolutionized music\ncreation, with many models leveraging textual or visual prompts for guidance.\nHowever, existing image-to-music models are limited to simple images, lacking\nthe capability to generate music from complex digitized artworks. To address\nthis gap, we introduce $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$, a novel\nmodel designed to create music from digitized artworks or text inputs.\n$\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ extends the AudioLDM~2\narchitecture, a text-to-audio model, and employs our newly curated datasets,\ncreated via ImageBind, which pair digitized artworks with music. Experimental\nresults demonstrate that $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ can\ngenerate music that resonates with the input stimuli. These findings suggest\npromising applications in multimedia art, interactive installations, and\nAI-driven creative tools.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.04906v1"
    },
    {
        "title": "Shorter Is Different: Characterizing the Dynamics of Short-Form Video\n  Platforms",
        "authors": [
            "Zhilong Chen",
            "Peijie Liu",
            "Jinghua Piao",
            "Fengli Xu",
            "Yong Li"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The emerging short-form video platforms have been growing tremendously and\nbecome one of the leading social media recently. Although the expanded\npopularity of these platforms has attracted increasing research attention,\nthere has been a lack of understanding of whether and how they deviate from\ntraditional long-form video-sharing platforms such as YouTube and Bilibili. To\naddress this, we conduct a large-scale data-driven analysis of Kuaishou, one of\nthe largest short-form video platforms in China. Based on 248 million videos\nuploaded to the platform across all categories, we identify their notable\ndifferences from long-form video platforms through a comparison study with\nBilibili, a leading long-form video platform in China. We find that videos are\nshortened by multiples on Kuaishou, with distinctive categorical distributions\nover-represented by life-related rather than interest-based videos. Users\ninteract with videos less per view, but top videos can even more effectively\nacquire users' collective attention. More importantly, ordinary content\ncreators have higher probabilities of producing hit videos. Our results shed\nlight on the uniqueness of short-form video platforms and pave the way for\nfuture research and design for better short-form video ecology.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16058v1"
    },
    {
        "title": "Evaluation of strategies for efficient rate-distortion NeRF streaming",
        "authors": [
            "Pedro Martin",
            "António Rodrigues",
            "João Ascenso",
            "Maria Paula Queluz"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Neural Radiance Fields (NeRF) have revolutionized the field of 3D visual\nrepresentation by enabling highly realistic and detailed scene reconstructions\nfrom a sparse set of images. NeRF uses a volumetric functional representation\nthat maps 3D points to their corresponding colors and opacities, allowing for\nphotorealistic view synthesis from arbitrary viewpoints. Despite its\nadvancements, the efficient streaming of NeRF content remains a significant\nchallenge due to the large amount of data involved. This paper investigates the\nrate-distortion performance of two NeRF streaming strategies: pixel-based and\nneural network (NN) parameter-based streaming. While in the former, images are\ncoded and then transmitted throughout the network, in the latter, the\nrespective NeRF model parameters are coded and transmitted instead. This work\nalso highlights the trade-offs in complexity and performance, demonstrating\nthat the NN parameter-based strategy generally offers superior efficiency,\nmaking it suitable for one-to-many streaming scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.19459v1"
    },
    {
        "title": "Document Parsing Unveiled: Techniques, Challenges, and Prospects for\n  Structured Information Extraction",
        "authors": [
            "Qintong Zhang",
            "Victor Shea-Jay Huang",
            "Bin Wang",
            "Junyuan Zhang",
            "Zhengren Wang",
            "Hao Liang",
            "Shawn Wang",
            "Matthieu Lin",
            "Conghui He",
            "Wentao Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Document parsing is essential for converting unstructured and semi-structured\ndocuments-such as contracts, academic papers, and invoices-into structured,\nmachine-readable data. Document parsing extract reliable structured data from\nunstructured inputs, providing huge convenience for numerous applications.\nEspecially with recent achievements in Large Language Models, document parsing\nplays an indispensable role in both knowledge base construction and training\ndata generation. This survey presents a comprehensive review of the current\nstate of document parsing, covering key methodologies, from modular pipeline\nsystems to end-to-end models driven by large vision-language models. Core\ncomponents such as layout detection, content extraction (including text,\ntables, and mathematical expressions), and multi-modal data integration are\nexamined in detail. Additionally, this paper discusses the challenges faced by\nmodular document parsing systems and vision-language models in handling complex\nlayouts, integrating multiple modules, and recognizing high-density text. It\nemphasizes the importance of developing larger and more diverse datasets and\noutlines future research directions.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.21169v3"
    },
    {
        "title": "Quality-Aware End-to-End Audio-Visual Neural Speaker Diarization",
        "authors": [
            "Mao-Kui He",
            "Jun Du",
            "Shu-Tong Niu",
            "Qing-Feng Liu",
            "Chin-Hui Lee"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we propose a quality-aware end-to-end audio-visual neural\nspeaker diarization framework, which comprises three key techniques. First, our\naudio-visual model takes both audio and visual features as inputs, utilizing a\nseries of binary classification output layers to simultaneously identify the\nactivities of all speakers. This end-to-end framework is meticulously designed\nto effectively handle situations of overlapping speech, providing accurate\ndiscrimination between speech and non-speech segments through the utilization\nof multi-modal information. Next, we employ a quality-aware audio-visual fusion\nstructure to address signal quality issues for both audio degradations, such as\nnoise, reverberation and other distortions, and video degradations, such as\nocclusions, off-screen speakers, or unreliable detection. Finally, a cross\nattention mechanism applied to multi-speaker embedding empowers the network to\nhandle scenarios with varying numbers of speakers. Our experimental results,\nobtained from various data sets, demonstrate the robustness of our proposed\ntechniques in diverse acoustic environments. Even in scenarios with severely\ndegraded video quality, our system attains performance levels comparable to the\nbest available audio-visual systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.22350v1"
    },
    {
        "title": "Beyond Correlation: Evaluating Multimedia Quality Models with the\n  Constrained Concordance Index",
        "authors": [
            "Alessandro Ragano",
            "Helard Becerra Martinez",
            "Andrew Hines"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  This study investigates the evaluation of multimedia quality models, focusing\non the inherent uncertainties in subjective Mean Opinion Score (MOS) ratings\ndue to factors like rater inconsistency and bias. Traditional statistical\nmeasures such as Pearson's Correlation Coefficient (PCC), Spearman's Rank\nCorrelation Coefficient (SRCC), and Kendall's Tau (KTAU) often fail to account\nfor these uncertainties, leading to inaccuracies in model performance\nassessment. We introduce the Constrained Concordance Index (CCI), a novel\nmetric designed to overcome the limitations of existing metrics by considering\nthe statistical significance of MOS differences and excluding comparisons where\nMOS confidence intervals overlap. Through comprehensive experiments across\nvarious domains including speech and image quality assessment, we demonstrate\nthat CCI provides a more robust and accurate evaluation of instrumental quality\nmodels, especially in scenarios of low sample sizes, rater group variability,\nand restriction of range. Our findings suggest that incorporating rater\nsubjectivity and focusing on statistically significant pairs can significantly\nenhance the evaluation framework for multimedia quality prediction models. This\nwork not only sheds light on the overlooked aspects of subjective rating\nuncertainties but also proposes a methodological advancement for more reliable\nand accurate quality model evaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05794v1"
    },
    {
        "title": "Harmful YouTube Video Detection: A Taxonomy of Online Harm and MLLMs as\n  Alternative Annotators",
        "authors": [
            "Claire Wonjeong Jo",
            "Miki Wesołowska",
            "Magdalena Wojcieszak"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Short video platforms, such as YouTube, Instagram, or TikTok, are used by\nbillions of users globally. These platforms expose users to harmful content,\nranging from clickbait or physical harms to misinformation or online hate. Yet,\ndetecting harmful videos remains challenging due to an inconsistent\nunderstanding of what constitutes harm and limited resources and mental tolls\ninvolved in human annotation. As such, this study advances measures and methods\nto detect harm in video content. First, we develop a comprehensive taxonomy for\nonline harm on video platforms, categorizing it into six categories:\nInformation, Hate and harassment, Addictive, Clickbait, Sexual, and Physical\nharms. Next, we establish multimodal large language models as reliable\nannotators of harmful videos. We analyze 19,422 YouTube videos using 14 image\nframes, 1 thumbnail, and text metadata, comparing the accuracy of crowdworkers\n(Mturk) and GPT-4-Turbo with domain expert annotations serving as the gold\nstandard. Our results demonstrate that GPT-4-Turbo outperforms crowdworkers in\nboth binary classification (harmful vs. harmless) and multi-label harm\ncategorization tasks. Methodologically, this study extends the application of\nLLMs to multi-label and multi-modal contexts beyond text annotation and binary\nclassification. Practically, our study contributes to online harm mitigation by\nguiding the definitions and identification of harmful content on video\nplatforms.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05854v1"
    },
    {
        "title": "A Novel Multimodal System to Predict Agitation in People with Dementia\n  Within Clinical Settings: A Proof of Concept",
        "authors": [
            "Abeer Badawi",
            "Somayya Elmoghazy",
            "Samira Choudhury",
            "Sara Elgazzar",
            "Khalid Elgazzar",
            "Amer Burhan"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Dementia is a neurodegenerative condition that combines several diseases and\nimpacts millions around the world and those around them. Although cognitive\nimpairment is profoundly disabling, it is the noncognitive features of\ndementia, referred to as Neuropsychiatric Symptoms (NPS), that are most closely\nassociated with a diminished quality of life. Agitation and aggression (AA) in\npeople living with dementia (PwD) contribute to distress and increased\nhealthcare demands. Current assessment methods rely on caregiver intervention\nand reporting of incidents, introducing subjectivity and bias. Artificial\nIntelligence (AI) and predictive algorithms offer a potential solution for\ndetecting AA episodes in PwD when utilized in real-time. We present a 5-year\nstudy system that integrates a multimodal approach, utilizing the EmbracePlus\nwristband and a video detection system to predict AA in severe dementia\npatients. We conducted a pilot study with three participants at the Ontario\nShores Mental Health Institute to validate the functionality of the system. The\nsystem collects and processes raw and digital biomarkers from the EmbracePlus\nwristband to accurately predict AA. The system also detected pre-agitation\npatterns at least six minutes before the AA event, which was not previously\ndiscovered from the EmbracePlus wristband. Furthermore, the privacy-preserving\nvideo system uses a masking tool to hide the features of the people in frames\nand employs a deep learning model for AA detection. The video system also helps\nidentify the actual start and end time of the agitation events for labeling.\nThe promising results of the preliminary data analysis underscore the ability\nof the system to predict AA events. The ability of the proposed system to run\nautonomously in real-time and identify AA and pre-agitation symptoms without\nexternal assistance represents a significant milestone in this research field.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.08882v1"
    },
    {
        "title": "Narrative Information Theory",
        "authors": [
            "Lion Schulz",
            "Miguel Patrício",
            "Daan Odijk"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  We propose an information-theoretic framework to measure narratives,\nproviding a formalism to understand pivotal moments, cliffhangers, and plot\ntwists. This approach offers creatives and AI researchers tools to analyse and\nbenchmark human- and AI-created stories. We illustrate our method in TV shows,\nshowing its ability to quantify narrative complexity and emotional dynamics\nacross genres. We discuss applications in media and in human-in-the-loop\ngenerative AI storytelling.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.12907v1"
    },
    {
        "title": "Gotta Hear Them All: Sound Source Aware Vision to Audio Generation",
        "authors": [
            "Wei Guo",
            "Heng Wang",
            "Jianbo Ma",
            "Weidong Cai"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Vision-to-audio (V2A) synthesis has broad applications in multimedia. Recent\nadvancements of V2A methods have made it possible to generate relevant audios\nfrom inputs of videos or still images. However, the immersiveness and\nexpressiveness of the generation are limited. One possible problem is that\nexisting methods solely rely on the global scene and overlook details of local\nsounding objects (i.e., sound sources). To address this issue, we propose a\nSound Source-Aware V2A (SSV2A) generator. SSV2A is able to locally perceive\nmultimodal sound sources from a scene with visual detection and cross-modality\ntranslation. It then contrastively learns a Cross-Modal Sound Source (CMSS)\nManifold to semantically disambiguate each source. Finally, we attentively mix\ntheir CMSS semantics into a rich audio representation, from which a pretrained\naudio generator outputs the sound. To model the CMSS manifold, we curate a\nnovel single-sound-source visual-audio dataset VGGS3 from VGGSound. We also\ndesign a Sound Source Matching Score to measure localized audio relevance. This\nis to our knowledge the first work to address V2A generation at the\nsound-source level. Extensive experiments show that SSV2A surpasses\nstate-of-the-art methods in both generation fidelity and relevance. We further\ndemonstrate SSV2A's ability to achieve intuitive V2A control by compositing\nvision, text, and audio conditions. Our SSV2A generation can be tried and heard\nat https://ssv2a.github.io/SSV2A-demo .\n",
        "pdf_link": "http://arxiv.org/pdf/2411.15447v2"
    },
    {
        "title": "A review on Machine Learning based User-Centric Multimedia Streaming\n  Techniques",
        "authors": [
            "Monalisa Ghosh",
            "Chetna Singhal"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The multimedia content and streaming are a major means of information\nexchange in the modern era and there is an increasing demand for such services.\nThis coupled with the advancement of future wireless networks B5G/6G and the\nproliferation of intelligent handheld mobile devices, has facilitated the\navailability of multimedia content to heterogeneous mobile users. Apart from\nthe conventional video, the 360$^o$ videos have gained popularity with the\nemerging virtual reality applications. All formats of videos (conventional and\n360$^o$) undergo processing, compression, and transmission across dynamic\nwireless channels with restricted bandwidth to facilitate the streaming\nservices. This causes video impairments, leading to quality degradation and\nposes challenges in delivering good Quality-of-Experience (QoE) to the viewers.\nThe QoE is a prominent subjective quality measure to assess multimedia\nservices. This requires end-to-end QoE evaluation. Efficient multimedia\nstreaming techniques can improve the service quality while dealing with dynamic\nnetwork and end-user challenges. A paradigm shift in user-centric multimedia\nservices is envisioned with a focus on Machine Learning (ML) based QoE modeling\nand streaming strategies. This survey paper presents a comprehensive overview\nof the overall and continuous, time varying QoE modeling for the purpose of QoE\nmanagement in multimedia services. It also examines the recent research on\nintelligent and adaptive multimedia streaming strategies, with a special\nemphasis on ML based techniques for video (conventional and 360$^o$) streaming.\nThis paper discusses the overall and continuous QoE modeling to optimize the\nend-user viewing experience, efficient video streaming with a focus on\nuser-centric strategies, associated datasets for modeling and streaming, along\nwith existing shortcoming and open challenges.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.15801v1"
    },
    {
        "title": "Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis",
        "authors": [
            "Akshita Gupta",
            "Tatiana Likhomanenko",
            "Karren Dai Yang",
            "Richard He Bai",
            "Zakaria Aldeneh",
            "Navdeep Jaitly"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In this paper, we propose a new task -- generating speech from videos of\npeople and their transcripts (VTTS) -- to motivate new techniques for\nmultimodal speech generation. This task generalizes the task of generating\nspeech from cropped lip videos, and is also more complicated than the task of\ngenerating generic audio clips (e.g., dog barking) from videos and text.\nMultilingual versions of the task could lead to new techniques for\ncross-lingual dubbing. We also present a decoder-only multimodal model for this\ntask, which we call Visatronic. This model embeds vision, text and speech\ndirectly into the common subspace of a transformer model and uses an\nautoregressive loss to learn a generative model of discretized mel-spectrograms\nconditioned on speaker videos and transcripts of their speech. By embedding all\nmodalities into a common subspace, Visatronic can achieve improved results over\nmodels that use only text or video as input. Further, it presents a much\nsimpler approach for multimodal speech generation compared to prevailing\napproaches which rely on lip-detectors and complicated architectures to fuse\nmodalities while producing better results. Since the model is flexible enough\nto accommodate different ways of ordering inputs as a sequence, we carefully\nexplore different strategies to better understand the best way to propagate\ninformation to the generative steps. To facilitate further research on VTTS, we\nwill release (i) our code, (ii) clean transcriptions for the large-scale\nVoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTS\nincorporating both objective and subjective metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.17690v1"
    },
    {
        "title": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows",
        "authors": [
            "Shufan Li",
            "Konstantinos Kallidromitis",
            "Akash Gokul",
            "Zichun Liao",
            "Yusuke Kato",
            "Kazuki Kozuka",
            "Aditya Grover"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  We introduce OmniFlow, a novel generative model designed for any-to-any\ngeneration tasks such as text-to-image, text-to-audio, and audio-to-image\nsynthesis. OmniFlow advances the rectified flow (RF) framework used in\ntext-to-image models to handle the joint distribution of multiple modalities.\nIt outperforms previous any-to-any models on a wide range of tasks, such as\ntext-to-image and text-to-audio synthesis. Our work offers three key\ncontributions: First, we extend RF to a multi-modal setting and introduce a\nnovel guidance mechanism, enabling users to flexibly control the alignment\nbetween different modalities in the generated outputs. Second, we propose a\nnovel architecture that extends the text-to-image MMDiT architecture of Stable\nDiffusion 3 and enables audio and text generation. The extended modules can be\nefficiently pretrained individually and merged with the vanilla text-to-image\nMMDiT for fine-tuning. Lastly, we conduct a comprehensive study on the design\nchoices of rectified flow transformers for large-scale audio and text\ngeneration, providing valuable insights into optimizing performance across\ndiverse modalities. The Code will be available at\nhttps://github.com/jacklishufan/OmniFlows.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.01169v1"
    },
    {
        "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
        "authors": [
            "Shanti Stewart",
            "Gouthaman KV",
            "Lie Lu",
            "Andrea Fanelli"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.05831v2"
    },
    {
        "title": "Enhancing Modality Representation and Alignment for Multimodal\n  Cold-start Active Learning",
        "authors": [
            "Meng Shen",
            "Yake Wei",
            "Jianxiong Yin",
            "Deepu Rajan",
            "Di Hu",
            "Simon See"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Training multimodal models requires a large amount of labeled data. Active\nlearning (AL) aim to reduce labeling costs. Most AL methods employ warm-start\napproaches, which rely on sufficient labeled data to train a well-calibrated\nmodel that can assess the uncertainty and diversity of unlabeled data. However,\nwhen assembling a dataset, labeled data are often scarce initially, leading to\na cold-start problem. Additionally, most AL methods seldom address multimodal\ndata, highlighting a research gap in this field. Our research addresses these\nissues by developing a two-stage method for Multi-Modal Cold-Start Active\nLearning (MMCSAL).\n  Firstly, we observe the modality gap, a significant distance between the\ncentroids of representations from different modalities, when only using\ncross-modal pairing information as self-supervision signals. This modality gap\naffects data selection process, as we calculate both uni-modal and cross-modal\ndistances. To address this, we introduce uni-modal prototypes to bridge the\nmodality gap. Secondly, conventional AL methods often falter in multimodal\nscenarios where alignment between modalities is overlooked. Therefore, we\npropose enhancing cross-modal alignment through regularization, thereby\nimproving the quality of selected multimodal data pairs in AL. Finally, our\nexperiments demonstrate MMCSAL's efficacy in selecting multimodal data pairs\nacross three multimodal datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.09126v1"
    },
    {
        "title": "Video Seal: Open and Efficient Video Watermarking",
        "authors": [
            "Pierre Fernandez",
            "Hady Elsahar",
            "I. Zeki Yalniz",
            "Alexandre Mourachko"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The proliferation of AI-generated content and sophisticated video editing\ntools has made it both important and challenging to moderate digital platforms.\nVideo watermarking addresses these challenges by embedding imperceptible\nsignals into videos, allowing for identification. However, the rare open tools\nand methods often fall short on efficiency, robustness, and flexibility. To\nreduce these gaps, this paper introduces Video Seal, a comprehensive framework\nfor neural video watermarking and a competitive open-sourced model. Our\napproach jointly trains an embedder and an extractor, while ensuring the\nwatermark robustness by applying transformations in-between, e.g., video\ncodecs. This training is multistage and includes image pre-training, hybrid\npost-training and extractor fine-tuning. We also introduce temporal watermark\npropagation, a technique to convert any image watermarking model to an\nefficient video watermarking model without the need to watermark every\nhigh-resolution frame. We present experimental results demonstrating the\neffectiveness of the approach in terms of speed, imperceptibility, and\nrobustness. Video Seal achieves higher robustness compared to strong baselines\nespecially under challenging distortions combining geometric transformations\nand video compression. Additionally, we provide new insights such as the impact\nof video compression during training, and how to compare methods operating on\ndifferent payloads. Contributions in this work - including the codebase,\nmodels, and a public demo - are open-sourced under permissive licenses to\nfoster further research and development in the field.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.09492v1"
    },
    {
        "title": "Provably Secure Robust Image Steganography via Cross-Modal Error\n  Correction",
        "authors": [
            "Yuang Qi",
            "Kejiang Chen",
            "Na Zhao",
            "Zijin Yang",
            "Weiming Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The rapid development of image generation models has facilitated the\nwidespread dissemination of generated images on social networks, creating\nfavorable conditions for provably secure image steganography. However, existing\nmethods face issues such as low quality of generated images and lack of\nsemantic control in the generation process. To leverage provably secure\nsteganography with more effective and high-performance image generation models,\nand to ensure that stego images can accurately extract secret messages even\nafter being uploaded to social networks and subjected to lossy processing such\nas JPEG compression, we propose a high-quality, provably secure, and robust\nimage steganography method based on state-of-the-art autoregressive (AR) image\ngeneration models using Vector-Quantized (VQ) tokenizers. Additionally, we\nemploy a cross-modal error-correction framework that generates stego text from\nstego images to aid in restoring lossy images, ultimately enabling the\nextraction of secret messages embedded within the images. Extensive experiments\nhave demonstrated that the proposed method provides advantages in stego\nquality, embedding capacity, and robustness, while ensuring provable\nundetectability.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.12206v1"
    },
    {
        "title": "SyncFlow: Toward Temporally Aligned Joint Audio-Video Generation from\n  Text",
        "authors": [
            "Haohe Liu",
            "Gael Le Lan",
            "Xinhao Mei",
            "Zhaoheng Ni",
            "Anurag Kumar",
            "Varun Nagaraja",
            "Wenwu Wang",
            "Mark D. Plumbley",
            "Yangyang Shi",
            "Vikas Chandra"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Video and audio are closely correlated modalities that humans naturally\nperceive together. While recent advancements have enabled the generation of\naudio or video from text, producing both modalities simultaneously still\ntypically relies on either a cascaded process or multi-modal contrastive\nencoders. These approaches, however, often lead to suboptimal results due to\ninherent information losses during inference and conditioning. In this paper,\nwe introduce SyncFlow, a system that is capable of simultaneously generating\ntemporally synchronized audio and video from text. The core of SyncFlow is the\nproposed dual-diffusion-transformer (d-DiT) architecture, which enables joint\nvideo and audio modelling with proper information fusion. To efficiently manage\nthe computational cost of joint audio and video modelling, SyncFlow utilizes a\nmulti-stage training strategy that separates video and audio learning before\njoint fine-tuning. Our empirical evaluations demonstrate that SyncFlow produces\naudio and video outputs that are more correlated than baseline methods with\nsignificantly enhanced audio quality and audio-visual correspondence. Moreover,\nwe demonstrate strong zero-shot capabilities of SyncFlow, including zero-shot\nvideo-to-audio generation and adaptation to novel video resolutions without\nfurther training.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.15220v1"
    },
    {
        "title": "Towards Expressive Video Dubbing with Multiscale Multimodal Context\n  Interaction",
        "authors": [
            "Yuan Zhao",
            "Rui Liu",
            "Gaoxiang Cong"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Automatic Video Dubbing (AVD) generates speech aligned with lip motion and\nfacial emotion from scripts. Recent research focuses on modeling multimodal\ncontext to enhance prosody expressiveness but overlooks two key issues: 1)\nMultiscale prosody expression attributes in the context influence the current\nsentence's prosody. 2) Prosody cues in context interact with the current\nsentence, impacting the final prosody expressiveness. To tackle these\nchallenges, we propose M2CI-Dubber, a Multiscale Multimodal Context Interaction\nscheme for AVD. This scheme includes two shared M2CI encoders to model the\nmultiscale multimodal context and facilitate its deep interaction with the\ncurrent sentence. By extracting global and local features for each modality in\nthe context, utilizing attention-based mechanisms for aggregation and\ninteraction, and employing an interaction-based graph attention network for\nfusion, the proposed approach enhances the prosody expressiveness of\nsynthesized speech for the current sentence. Experiments on the Chem dataset\nshow our model outperforms baselines in dubbing expressiveness. The code and\ndemos are available at\n\\textcolor[rgb]{0.93,0.0,0.47}{https://github.com/AI-S2-Lab/M2CI-Dubber}.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.18748v2"
    },
    {
        "title": "Adaptive Rate Control for Deep Video Compression with Rate-Distortion\n  Prediction",
        "authors": [
            "Bowen Gu",
            "Hao Chen",
            "Ming Lu",
            "Jie Yao",
            "Zhan Ma"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Deep video compression has made significant progress in recent years,\nachieving rate-distortion performance that surpasses that of traditional video\ncompression methods. However, rate control schemes tailored for deep video\ncompression have not been well studied. In this paper, we propose a neural\nnetwork-based $\\lambda$-domain rate control scheme for deep video compression,\nwhich determines the coding parameter $\\lambda$ for each to-be-coded frame\nbased on the rate-distortion-$\\lambda$ (R-D-$\\lambda$) relationships directly\nlearned from uncompressed frames, achieving high rate control accuracy\nefficiently without the need for pre-encoding. Moreover, this content-aware\nscheme is able to mitigate inter-frame quality fluctuations and adapt to abrupt\nchanges in video content. Specifically, we introduce two neural network-based\npredictors to estimate the relationship between bitrate and $\\lambda$, as well\nas the relationship between distortion and $\\lambda$ for each frame. Then we\ndetermine the coding parameter $\\lambda$ for each frame to achieve the target\nbitrate. Experimental results demonstrate that our approach achieves high rate\ncontrol accuracy at the mini-GOP level with low time overhead and mitigates\ninter-frame quality fluctuations across video content of varying resolutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.18834v1"
    },
    {
        "title": "Listening and Seeing Again: Generative Error Correction for Audio-Visual\n  Speech Recognition",
        "authors": [
            "Rui Liu",
            "Hongyu Yuan",
            "Haizhou Li"
        ],
        "category": "cs.MM",
        "published_year": "2025",
        "summary": "  Unlike traditional Automatic Speech Recognition (ASR), Audio-Visual Speech\nRecognition (AVSR) takes audio and visual signals simultaneously to infer the\ntranscription. Recent studies have shown that Large Language Models (LLMs) can\nbe effectively used for Generative Error Correction (GER) in ASR by predicting\nthe best transcription from ASR-generated N-best hypotheses. However, these\nLLMs lack the ability to simultaneously understand audio and visual, making the\nGER approach challenging to apply in AVSR. In this work, we propose a novel GER\nparadigm for AVSR, termed AVGER, that follows the concept of ``listening and\nseeing again''. Specifically, we first use the powerful AVSR system to read the\naudio and visual signals to get the N-Best hypotheses, and then use the\nQ-former-based Multimodal Synchronous Encoder to read the audio and visual\ninformation again and convert them into an audio and video compression\nrepresentation respectively that can be understood by LLM. Afterward, the\naudio-visual compression representation and the N-Best hypothesis together\nconstitute a Cross-modal Prompt to guide the LLM in producing the best\ntranscription. In addition, we also proposed a Multi-Level Consistency\nConstraint training criterion, including logits-level, utterance-level and\nrepresentations-level, to improve the correction accuracy while enhancing the\ninterpretability of audio and visual compression representations. The\nexperimental results on the LRS3 dataset show that our method outperforms\ncurrent mainstream AVSR systems. The proposed AVGER can reduce the Word Error\nRate (WER) by 24% compared to them. Code and models can be found at:\nhttps://github.com/CircleRedRain/AVGER.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.04038v1"
    },
    {
        "title": "LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport",
        "authors": [
            "Kyeongha Rho",
            "Hyeongkeun Lee",
            "Valentio Iverson",
            "Joon Son Chung"
        ],
        "category": "cs.MM",
        "published_year": "2025",
        "summary": "  Automated audio captioning is a task that generates textual descriptions for\naudio content, and recent studies have explored using visual information to\nenhance captioning quality. However, current methods often fail to effectively\nfuse audio and visual data, missing important semantic cues from each modality.\nTo address this, we introduce LAVCap, a large language model (LLM)-based\naudio-visual captioning framework that effectively integrates visual\ninformation with audio to improve audio captioning performance. LAVCap employs\nan optimal transport-based alignment loss to bridge the modality gap between\naudio and visual features, enabling more effective semantic extraction.\nAdditionally, we propose an optimal transport attention module that enhances\naudio-visual fusion using an optimal transport assignment map. Combined with\nthe optimal training strategy, experimental results demonstrate that each\ncomponent of our framework is effective. LAVCap outperforms existing\nstate-of-the-art methods on the AudioCaps dataset, without relying on large\ndatasets or post-processing. Code is available at\nhttps://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.09291v1"
    },
    {
        "title": "Low-complexity Image and Video Coding Based on an Approximate Discrete\n  Tchebichef Transform",
        "authors": [
            "P. A. M. Oliveira",
            "R. J. Cintra",
            "F. M. Bayer",
            "S. Kulasekera",
            "A. Madanayake",
            "V. A. Coutinho"
        ],
        "category": "cs.MM",
        "published_year": "2016",
        "summary": "  The usage of linear transformations has great relevance for data\ndecorrelation applications, like image and video compression. In that sense,\nthe discrete Tchebichef transform (DTT) possesses useful coding and\ndecorrelation properties. The DTT transform kernel does not depend on the input\ndata and fast algorithms can be developed to real time applications. However,\nthe DTT fast algorithm presented in literature possess high computational\ncomplexity. In this work, we introduce a new low-complexity approximation for\nthe DTT. The fast algorithm of the proposed transform is multiplication-free\nand requires a reduced number of additions and bit-shifting operations. Image\nand video compression simulations in popular standards shows good performance\nof the proposed transform. Regarding hardware resource consumption for FPGA\nshows 43.1% reduction of configurable logic blocks and ASIC place and route\nrealization shows 57.7% reduction in the area-time figure when compared with\nthe 2-D version of the exact DTT.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.07630v4"
    },
    {
        "title": "Grounding Natural Language Commands to StarCraft II Game States for\n  Narration-Guided Reinforcement Learning",
        "authors": [
            "Nicholas Waytowich",
            "Sean L. Barton",
            "Vernon Lawhern",
            "Ethan Stump",
            "Garrett Warnell"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  While deep reinforcement learning techniques have led to agents that are\nsuccessfully able to learn to perform a number of tasks that had been\npreviously unlearnable, these techniques are still susceptible to the\nlongstanding problem of {\\em reward sparsity}. This is especially true for\ntasks such as training an agent to play StarCraft II, a real-time strategy game\nwhere reward is only given at the end of a game which is usually very long.\nWhile this problem can be addressed through reward shaping, such approaches\ntypically require a human expert with specialized knowledge. Inspired by the\nvision of enabling reward shaping through the more-accessible paradigm of\nnatural-language narration, we investigate to what extent we can contextualize\nthese narrations by grounding them to the goal-specific states. We present a\nmutual-embedding model using a multi-input deep-neural network that projects a\nsequence of natural language commands into the same high-dimensional\nrepresentation space as corresponding goal states. We show that using this\nmodel we can learn an embedding space with separable and distinct clusters that\naccurately maps natural-language commands to corresponding game states . We\nalso discuss how this model can allow for the use of narrations as a robust\nform of reward shaping to improve RL performance and efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.02671v1"
    },
    {
        "title": "Temporally Guided Music-to-Body-Movement Generation",
        "authors": [
            "Hsuan-Kai Kao",
            "Li Su"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  This paper presents a neural network model to generate virtual violinist's\n3-D skeleton movements from music audio. Improved from the conventional\nrecurrent neural network models for generating 2-D skeleton data in previous\nworks, the proposed model incorporates an encoder-decoder architecture, as well\nas the self-attention mechanism to model the complicated dynamics in body\nmovement sequences. To facilitate the optimization of self-attention model,\nbeat tracking is applied to determine effective sizes and boundaries of the\ntraining examples. The decoder is accompanied with a refining network and a\nbowing attack inference mechanism to emphasize the right-hand behavior and\nbowing attack timing. Both objective and subjective evaluations reveal that the\nproposed model outperforms the state-of-the-art methods. To the best of our\nknowledge, this work represents the first attempt to generate 3-D violinists'\nbody movements considering key features in musical body movement.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.08015v1"
    },
    {
        "title": "Deep Vocoder: Low Bit Rate Compression of Speech with Deep Autoencoder",
        "authors": [
            "Gang Min",
            "Changqing Zhang",
            "Xiongwei Zhang",
            "Wei Tan"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Inspired by the success of deep neural networks (DNNs) in speech processing,\nthis paper presents Deep Vocoder, a direct end-to-end low bit rate speech\ncompression method with deep autoencoder (DAE). In Deep Vocoder, DAE is used\nfor extracting the latent representing features (LRFs) of speech, which are\nthen efficiently quantized by an analysis-by-synthesis vector quantization (AbS\nVQ) method. AbS VQ aims to minimize the perceptual spectral reconstruction\ndistortion rather than the distortion of LRFs vector itself. Also, a suboptimal\ncodebook searching technique is proposed to further reduce the computational\ncomplexity. Experimental results demonstrate that Deep Vocoder yields\nsubstantial improvements in terms of frequency-weighted segmental SNR, STOI and\nPESQ score when compared to the output of the conventional SQ- or VQ-based\ncodec. The yielded PESQ score over the TIMIT corpus is 3.34 and 3.08 for speech\ncoding at 2400 bit/s and 1200 bit/s, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.04709v2"
    },
    {
        "title": "Very fast watermarking by reversible contrast mapping",
        "authors": [
            "Dinu Coltuc",
            "Jean-Marc Chassery"
        ],
        "category": "cs.MM",
        "published_year": "2007",
        "summary": "  Reversible contrast mapping (RCM) is a simple integer transform that applies\nto pairs of pixels. For some pairs of pixels, RCM is invertible, even if the\nleast significant bits (LSBs) of the transformed pixels are lost. The data\nspace occupied by the LSBs is suitable for data hiding. The embedded\ninformation bit-rates of the proposed spatial domain reversible watermarking\nscheme are close to the highest bit-rates reported so far. The scheme does not\nneed additional data compression, and, in terms of mathematical complexity, it\nappears to be the lowest complexity one proposed up to now. A very fast lookup\ntable implementation is proposed. Robustness against cropping can be ensured as\nwell.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.0802v1"
    },
    {
        "title": "Deep Multimodal Image-Repurposing Detection",
        "authors": [
            "Ekraam Sabir",
            "Wael AbdAlmageed",
            "Yue Wu",
            "Prem Natarajan"
        ],
        "category": "cs.MM",
        "published_year": "2018",
        "summary": "  Nefarious actors on social media and other platforms often spread rumors and\nfalsehoods through images whose metadata (e.g., captions) have been modified to\nprovide visual substantiation of the rumor/falsehood. This type of modification\nis referred to as image repurposing, in which often an unmanipulated image is\npublished along with incorrect or manipulated metadata to serve the actor's\nulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)\ndataset, a substantially challenging dataset over that which has been\npreviously available to support research into image repurposing detection. The\nnew dataset includes location, person, and organization manipulations on\nreal-world data sourced from Flickr. We also present a novel, end-to-end, deep\nmultimodal learning model for assessing the integrity of an image by combining\ninformation extracted from the image with related information from a knowledge\nbase. The proposed method is compared against state-of-the-art techniques on\nexisting datasets as well as MEIR, where it outperforms existing methods across\nthe board, with AUC improvement up to 0.23.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.06686v1"
    },
    {
        "title": "Viewport-Aware Deep Reinforcement Learning Approach for 360$^o$ Video\n  Caching",
        "authors": [
            "Pantelis Maniotis",
            "Nikolaos Thomos"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  360$^o$ video is an essential component of VR/AR/MR systems that provides\nimmersive experience to the users. However, 360$^o$ video is associated with\nhigh bandwidth requirements. The required bandwidth can be reduced by\nexploiting the fact that users are interested in viewing only a part of the\nvideo scene and that users request viewports that overlap with each other.\nMotivated by the findings of recent works where the benefits of caching video\ntiles at edge servers instead of caching entire 360$^o$ videos were shown, in\nthis paper, we introduce the concept of virtual viewports that have the same\nnumber of tiles with the original viewports. The tiles forming these viewports\nare the most popular ones for each video and are determined by the users'\nrequests. Then, we propose a proactive caching scheme that assumes unknown\nvideos' and viewports' popularity. Our scheme determines which videos to cache\nas well as which is the optimal virtual viewport per video. Virtual viewports\npermit to lower the dimensionality of the cache optimization problem. To solve\nthe problem, we first formulate the content placement of 360$^o$ videos in edge\ncache networks as a Markov Decision Process (MDP), and then we determine the\noptimal caching placement using the Deep Q-Network (DQN) algorithm. The\nproposed solution aims at maximizing the overall quality of the 360$^o$ videos\ndelivered to the end-users by caching the most popular 360$^o$ videos at base\nquality along with a virtual viewport in high quality. We extensively evaluate\nthe performance of the proposed system and compare it with that of known\nsystems such as LFU, LRU, FIFO, over both synthetic and real 360$^o$ video\ntraces. The results reveal the large benefits coming from proactive caching of\nvirtual viewports instead of the original ones in terms of the overall quality\nof the rendered viewports, the cache hit ratio, and the servicing cost.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.08473v2"
    },
    {
        "title": "Multi-Modal Video Forensic Platform for Investigating Post-Terrorist\n  Attack Scenarios",
        "authors": [
            "Alexander Schindler",
            "Andrew Lindley",
            "Anahid Jalali",
            "Martin Boyer",
            "Sergiu Gordea",
            "Ross King"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  The forensic investigation of a terrorist attack poses a significant\nchallenge to the investigative authorities, as often several thousand hours of\nvideo footage must be viewed. Large scale Video Analytic Platforms (VAP) assist\nlaw enforcement agencies (LEA) in identifying suspects and securing evidence.\nCurrent platforms focus primarily on the integration of different computer\nvision methods and thus are restricted to a single modality. We present a video\nanalytic platform that integrates visual and audio analytic modules and fuses\ninformation from surveillance cameras and video uploads from eyewitnesses.\nVideos are analyzed according their acoustic and visual content. Specifically,\nAudio Event Detection is applied to index the content according to\nattack-specific acoustic concepts. Audio similarity search is utilized to\nidentify similar video sequences recorded from different perspectives. Visual\nobject detection and tracking are used to index the content according to\nrelevant concepts. Innovative user-interface concepts are introduced to harness\nthe full potential of the heterogeneous results of the analytical modules,\nallowing investigators to more quickly follow-up on leads and eyewitness\nreports.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01023v1"
    },
    {
        "title": "MuSe 2020 -- The First International Multimodal Sentiment Analysis in\n  Real-life Media Challenge and Workshop",
        "authors": [
            "Lukas Stappen",
            "Alice Baird",
            "Georgios Rizos",
            "Panagiotis Tzirakis",
            "Xinchen Du",
            "Felix Hafner",
            "Lea Schumann",
            "Adria Mallol-Ragolta",
            "Björn W. Schuller",
            "Iulia Lefter",
            "Erik Cambria",
            "Ioannis Kompatsiaris"
        ],
        "category": "cs.MM",
        "published_year": "2020",
        "summary": "  Multimodal Sentiment Analysis in Real-life Media (MuSe) 2020 is a\nChallenge-based Workshop focusing on the tasks of sentiment recognition, as\nwell as emotion-target engagement and trustworthiness detection by means of\nmore comprehensively integrating the audio-visual and language modalities. The\npurpose of MuSe 2020 is to bring together communities from different\ndisciplines; mainly, the audio-visual emotion recognition community\n(signal-based), and the sentiment analysis community (symbol-based). We present\nthree distinct sub-challenges: MuSe-Wild, which focuses on continuous emotion\n(arousal and valence) prediction; MuSe-Topic, in which participants recognise\ndomain-specific topics as the target of 3-class (low, medium, high) emotions;\nand MuSe-Trust, in which the novel aspect of trustworthiness is to be\npredicted. In this paper, we provide detailed information on MuSe-CaR, the\nfirst of its kind in-the-wild database, which is utilised for the challenge, as\nwell as the state-of-the-art features and modelling approaches applied. For\neach sub-challenge, a competitive baseline for participants is set; namely, on\ntest we report for MuSe-Wild a combined (valence and arousal) CCC of .2568, for\nMuSe-Topic a score (computed as 0.34$\\cdot$ UAR + 0.66$\\cdot$F1) of 76.78 % on\nthe 10-class topic and 40.64 % on the 3-class emotion prediction, and for\nMuSe-Trust a CCC of .4359.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.14858v3"
    },
    {
        "title": "Analyzing Images for Music Recommendation",
        "authors": [
            "Anant Baijal",
            "Vivek Agarwal",
            "Danny Hyun"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Experiencing images with suitable music can greatly enrich the overall user\nexperience. The proposed image analysis method treats an artwork image\ndifferently from a photograph image. Automatic image classification is\nperformed using deep-learning based models. An illustrative analysis showcasing\nthe ability of our deep-models to inherently learn and utilize perceptually\nrelevant features when classifying artworks is also presented. The Mean Opinion\nScore (MOS) obtained from subjective assessments of the respective image and\nrecommended music pairs supports the effectiveness of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.07135v1"
    },
    {
        "title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for\n  Audio-Visual Scene Classification",
        "authors": [
            "Javier Naranjo-Alcazar",
            "Sergi Perez-Castanos",
            "Aaron Lopez-Garcia",
            "Pedro Zuccarello",
            "Maximo Cobos",
            "Francesc J. Ferri"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.13180v1"
    },
    {
        "title": "Audio-Visual Object Classification for Human-Robot Collaboration",
        "authors": [
            "A. Xompero",
            "Y. L. Pang",
            "T. Patten",
            "A. Prabhakar",
            "B. Calli",
            "A. Cavallaro"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Human-robot collaboration requires the contactless estimation of the physical\nproperties of containers manipulated by a person, for example while pouring\ncontent in a cup or moving a food box. Acoustic and visual signals can be used\nto estimate the physical properties of such objects, which may vary\nsubstantially in shape, material and size, and also be occluded by the hands of\nthe person. To facilitate comparisons and stimulate progress in solving this\nproblem, we present the CORSMAL challenge and a dataset to assess the\nperformance of the algorithms through a set of well-defined performance scores.\nThe tasks of the challenge are the estimation of the mass, capacity, and\ndimensions of the object (container), and the classification of the type and\namount of its content. A novel feature of the challenge is our\nreal-to-simulation framework for visualising and assessing the impact of\nestimation errors in human-to-robot handovers.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.01977v1"
    },
    {
        "title": "Dual-Path Cross-Modal Attention for better Audio-Visual Speech\n  Extraction",
        "authors": [
            "Zhongweiyang Xu",
            "Xulin Fan",
            "Mark Hasegawa-Johnson"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Audio-visual target speech extraction, which aims to extract a certain\nspeaker's speech from the noisy mixture by looking at lip movements, has made\nsignificant progress combining time-domain speech separation models and visual\nfeature extractors (CNN). One problem of fusing audio and video information is\nthat they have different time resolutions. Most current research upsamples the\nvisual features along the time dimension so that audio and video features are\nable to align in time. However, we believe that lip movement should mostly\ncontain long-term, or phone-level information. Based on this assumption, we\npropose a new way to fuse audio-visual features. We observe that for DPRNN\n\\cite{dprnn}, the interchunk dimension's time resolution could be very close to\nthe time resolution of video frames. Like \\cite{sepformer}, the LSTM in DPRNN\nis replaced by intra-chunk and inter-chunk self-attention, but in the proposed\nalgorithm, inter-chunk attention incorporates the visual features as an\nadditional feature stream. This prevents the upsampling of visual cues,\nresulting in more efficient audio-visual fusion. The result shows we achieve\nsuperior results compared with other time-domain based audio-visual fusion\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.04213v2"
    },
    {
        "title": "The Contribution of Lyrics and Acoustics to Collaborative Understanding\n  of Mood",
        "authors": [
            "Shahrzad Naseri",
            "Sravana Reddy",
            "Joana Correia",
            "Jussi Karlgren",
            "Rosie Jones"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this work, we study the association between song lyrics and mood through a\ndata-driven analysis. Our data set consists of nearly one million songs, with\nsong-mood associations derived from user playlists on the Spotify streaming\nplatform. We take advantage of state-of-the-art natural language processing\nmodels based on transformers to learn the association between the lyrics and\nmoods. We find that a pretrained transformer-based language model in a\nzero-shot setting -- i.e., out of the box with no further training on our data\n-- is powerful for capturing song-mood associations. Moreover, we illustrate\nthat training on song-mood associations results in a highly accurate model that\npredicts these associations for unseen songs. Furthermore, by comparing the\nprediction of a model using lyrics with one using acoustic features, we observe\nthat the relative importance of lyrics for mood prediction in comparison with\nacoustics depends on the specific mood. Finally, we verify if the models are\ncapturing the same information about lyrics and acoustics as humans through an\nannotation task where we obtain human judgments of mood-song relevance based on\nlyrics and acoustics.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.05680v1"
    },
    {
        "title": "Make Acoustic and Visual Cues Matter: CH-SIMS v2.0 Dataset and AV-Mixup\n  Consistent Module",
        "authors": [
            "Yihe Liu",
            "Ziqi Yuan",
            "Huisheng Mao",
            "Zhiyun Liang",
            "Wanqiuyue Yang",
            "Yuanzhe Qiu",
            "Tie Cheng",
            "Xiaoteng Li",
            "Hua Xu",
            "Kai Gao"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  Multimodal sentiment analysis (MSA), which supposes to improve text-based\nsentiment analysis with associated acoustic and visual modalities, is an\nemerging research area due to its potential applications in Human-Computer\nInteraction (HCI). However, the existing researches observe that the acoustic\nand visual modalities contribute much less than the textual modality, termed as\ntext-predominant. Under such circumstances, in this work, we emphasize making\nnon-verbal cues matter for the MSA task. Firstly, from the resource\nperspective, we present the CH-SIMS v2.0 dataset, an extension and enhancement\nof the CH-SIMS. Compared with the original dataset, the CH-SIMS v2.0 doubles\nits size with another 2121 refined video segments with both unimodal and\nmultimodal annotations and collects 10161 unlabelled raw video segments with\nrich acoustic and visual emotion-bearing context to highlight non-verbal cues\nfor sentiment prediction. Secondly, from the model perspective, benefiting from\nthe unimodal annotations and the unsupervised data in the CH-SIMS v2.0, the\nAcoustic Visual Mixup Consistent (AV-MC) framework is proposed. The designed\nmodality mixup module can be regarded as an augmentation, which mixes the\nacoustic and visual modalities from different videos. Through drawing\nunobserved multimodal context along with the text, the model can learn to be\naware of different non-verbal contexts for sentiment prediction. Our\nevaluations demonstrate that both CH-SIMS v2.0 and AV-MC framework enables\nfurther research for discovering emotion-bearing acoustic and visual cues and\npaves the path to interpretable end-to-end HCI applications for real-world\nscenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.02604v1"
    },
    {
        "title": "DM$^2$S$^2$: Deep Multi-Modal Sequence Sets with Hierarchical Modality\n  Attention",
        "authors": [
            "Shunsuke Kitada",
            "Yuki Iwazaki",
            "Riku Togashi",
            "Hitoshi Iyatomi"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  There is increasing interest in the use of multimodal data in various web\napplications, such as digital advertising and e-commerce. Typical methods for\nextracting important information from multimodal data rely on a mid-fusion\narchitecture that combines the feature representations from multiple encoders.\nHowever, as the number of modalities increases, several potential problems with\nthe mid-fusion model structure arise, such as an increase in the dimensionality\nof the concatenated multimodal features and missing modalities. To address\nthese problems, we propose a new concept that considers multimodal inputs as a\nset of sequences, namely, deep multimodal sequence sets (DM$^2$S$^2$). Our\nset-aware concept consists of three components that capture the relationships\namong multiple modalities: (a) a BERT-based encoder to handle the inter- and\nintra-order of elements in the sequences, (b) intra-modality residual attention\n(IntraMRA) to capture the importance of the elements in a modality, and (c)\ninter-modality residual attention (InterMRA) to enhance the importance of\nelements with modality-level granularity further. Our concept exhibits\nperformance that is comparable to or better than the previous set-aware models.\nFurthermore, we demonstrate that the visualization of the learned InterMRA and\nIntraMRA weights can provide an interpretation of the prediction results.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.03126v2"
    },
    {
        "title": "MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for Speech\n  Recognition",
        "authors": [
            "Xiaohuan Zhou",
            "Jiaming Wang",
            "Zeyu Cui",
            "Shiliang Zhang",
            "Zhijie Yan",
            "Jingren Zhou",
            "Chang Zhou"
        ],
        "category": "cs.MM",
        "published_year": "2022",
        "summary": "  In this paper, we propose a novel multi-modal multi-task encoder-decoder\npre-training framework (MMSpeech) for Mandarin automatic speech recognition\n(ASR), which employs both unlabeled speech and text data. The main difficulty\nin speech-text joint pre-training comes from the significant difference between\nspeech and text modalities, especially for Mandarin speech and text. Unlike\nEnglish and other languages with an alphabetic writing system, Mandarin uses an\nideographic writing system where character and sound are not tightly mapped to\none another. Therefore, we propose to introduce the phoneme modality into\npre-training, which can help capture modality-invariant information between\nMandarin speech and text. Specifically, we employ a multi-task learning\nframework including five self-supervised and supervised tasks with speech and\ntext data. For end-to-end pre-training, we introduce self-supervised\nspeech-to-pseudo-codes (S2C) and phoneme-to-text (P2T) tasks utilizing\nunlabeled speech and text data, where speech-pseudo-codes pairs and\nphoneme-text pairs are a supplement to the supervised speech-text pairs. To\ntrain the encoder to learn better speech representation, we introduce\nself-supervised masked speech prediction (MSP) and supervised phoneme\nprediction (PP) tasks to learn to map speech into phonemes. Besides, we\ndirectly add the downstream supervised speech-to-text (S2T) task into the\npre-training process, which can further improve the pre-training performance\nand achieve better recognition results even without fine-tuning. Experiments on\nAISHELL-1 show that our proposed method achieves state-of-the-art performance,\nwith a more than 40% relative improvement compared with other pre-training\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.00500v1"
    },
    {
        "title": "Watch or Listen: Robust Audio-Visual Speech Recognition with Visual\n  Corruption Modeling and Reliability Scoring",
        "authors": [
            "Joanna Hong",
            "Minsu Kim",
            "Jeongsoo Choi",
            "Yong Man Ro"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal\ninput corruption situations where audio inputs and visual inputs are both\ncorrupted, which is not well addressed in previous research directions.\nPrevious studies have focused on how to complement the corrupted audio inputs\nwith the clean visual inputs with the assumption of the availability of clean\nvisual inputs. However, in real life, clean visual inputs are not always\naccessible and can even be corrupted by occluded lip regions or noises. Thus,\nwe firstly analyze that the previous AVSR models are not indeed robust to the\ncorruption of multimodal input streams, the audio and the visual inputs,\ncompared to uni-modal models. Then, we design multimodal input corruption\nmodeling to develop robust AVSR models. Lastly, we propose a novel AVSR\nframework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that\nis robust to the corrupted multimodal inputs. The AV-RelScore can determine\nwhich input modal stream is reliable or not for the prediction and also can\nexploit the more reliable streams in prediction. The effectiveness of the\nproposed method is evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that the reliability scores\nobtained by AV-RelScore well reflect the degree of corruption and make the\nproposed model focus on the reliable multimodal representations.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.08536v2"
    },
    {
        "title": "Zero-shot personalized lip-to-speech synthesis with face image based\n  voice control",
        "authors": [
            "Zheng-Yan Sheng",
            "Yang Ai",
            "Zhen-Hua Ling"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Lip-to-Speech (Lip2Speech) synthesis, which predicts corresponding speech\nfrom talking face images, has witnessed significant progress with various\nmodels and training strategies in a series of independent studies. However,\nexisting studies can not achieve voice control under zero-shot condition,\nbecause extra speaker embeddings need to be extracted from natural reference\nspeech and are unavailable when only the silent video of an unseen speaker is\ngiven. In this paper, we propose a zero-shot personalized Lip2Speech synthesis\nmethod, in which face images control speaker identities. A variational\nautoencoder is adopted to disentangle the speaker identity and linguistic\ncontent representations, which enables speaker embeddings to control the voice\ncharacteristics of synthetic speech for unseen speakers. Furthermore, we\npropose associated cross-modal representation learning to promote the ability\nof face-based speaker embeddings (FSE) on voice control. Extensive experiments\nverify the effectiveness of the proposed method whose synthetic utterances are\nmore natural and matching with the personality of input video than the compared\nmethods. To our best knowledge, this paper makes the first attempt on zero-shot\npersonalized Lip2Speech synthesis with a face image rather than reference audio\nto control voice characteristics.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.14359v1"
    },
    {
        "title": "MSVD-Indonesian: A Benchmark for Multimodal Video-Text Tasks in\n  Indonesian",
        "authors": [
            "Willy Fitra Hendria"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Multimodal learning on video and text data has been receiving growing\nattention from many researchers in various research tasks, including\ntext-to-video retrieval, video-to-text retrieval, and video captioning.\nAlthough many algorithms have been proposed for those challenging tasks, most\nof them are developed on English language datasets. Despite Indonesian being\none of the most spoken languages in the world, the research progress on the\nmultimodal video-text with Indonesian sentences is still under-explored, likely\ndue to the absence of the public benchmark dataset. To address this issue, we\nconstruct the first public Indonesian video-text dataset by translating English\nsentences from the MSVD dataset to Indonesian sentences. Using our dataset, we\nthen train neural network models which were developed for the English\nvideo-text dataset on three tasks, i.e., text-to-video retrieval, video-to-text\nretrieval, and video captioning. The recent neural network-based approaches to\nvideo-text tasks often utilized a feature extractor that is primarily\npretrained on an English vision-language dataset. Since the availability of the\npretraining resources with Indonesian sentences is relatively limited, the\napplicability of those approaches to our dataset is still questionable. To\novercome the lack of pretraining resources, we apply cross-lingual transfer\nlearning by utilizing the feature extractors pretrained on the English dataset,\nand we then fine-tune the models on our Indonesian dataset. Our experimental\nresults show that this approach can help to improve the performance for the\nthree tasks on all metrics. Finally, we discuss potential future works using\nour dataset, inspiring further research in the Indonesian multimodal video-text\ntasks. We believe that our dataset and our experimental results could provide\nvaluable contributions to the community. Our dataset is available on GitHub.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.11341v1"
    },
    {
        "title": "DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement\n  Estimation in Conversation",
        "authors": [
            "Vu Ngoc Tu",
            "Van Thong Huynh",
            "Hyung-Jeong Yang",
            "M. Zaigham Zaheer",
            "Shah Nawaz",
            "Karthik Nandakumar",
            "Soo-Hyung Kim"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Conversational engagement estimation is posed as a regression problem,\nentailing the identification of the favorable attention and involvement of the\nparticipants in the conversation. This task arises as a crucial pursuit to gain\ninsights into human's interaction dynamics and behavior patterns within a\nconversation. In this research, we introduce a dilated convolutional\nTransformer for modeling and estimating human engagement in the MULTIMEDIATE\n2023 competition. Our proposed system surpasses the baseline models, exhibiting\na noteworthy $7$\\% improvement on test set and $4$\\% on validation set.\nMoreover, we employ different modality fusion mechanism and show that for this\ntype of data, a simple concatenated method with self-attention fusion gains the\nbest performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01966v1"
    },
    {
        "title": "TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body\n  Gestures Generation",
        "authors": [
            "Mireille Fares",
            "Catherine Pelachaud",
            "Nicolas Obin"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  This paper addresses the challenge of transferring the behavior expressivity\nstyle of a virtual agent to another one while preserving behaviors shape as\nthey carry communicative meaning. Behavior expressivity style is viewed here as\nthe qualitative properties of behaviors. We propose TranSTYLer, a multimodal\ntransformer based model that synthesizes the multimodal behaviors of a source\nspeaker with the style of a target speaker. We assume that behavior\nexpressivity style is encoded across various modalities of communication,\nincluding text, speech, body gestures, and facial expressions. The model\nemploys a style and content disentanglement schema to ensure that the\ntransferred style does not interfere with the meaning conveyed by the source\nbehaviors. Our approach eliminates the need for style labels and allows the\ngeneralization to styles that have not been seen during the training phase. We\ntrain our model on the PATS corpus, which we extended to include dialog acts\nand 2D facial landmarks. Objective and subjective evaluations show that our\nmodel outperforms state of the art models in style transfer for both seen and\nunseen styles during training. To tackle the issues of style and content\nleakage that may arise, we propose a methodology to assess the degree to which\nbehavior and gestures associated with the target style are successfully\ntransferred, while ensuring the preservation of the ones related to the source\ncontent.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.10843v1"
    },
    {
        "title": "On the Audio Hallucinations in Large Audio-Video Language Models",
        "authors": [
            "Taichi Nishimura",
            "Shota Nakada",
            "Masayoshi Kondo"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Large audio-video language models can generate descriptions for both video\nand audio. However, they sometimes ignore audio content, producing audio\ndescriptions solely reliant on visual information. This paper refers to this as\naudio hallucinations and analyzes them in large audio-video language models. We\ngather 1,000 sentences by inquiring about audio information and annotate them\nwhether they contain hallucinations. If a sentence is hallucinated, we also\ncategorize the type of hallucination. The results reveal that 332 sentences are\nhallucinated with distinct trends observed in nouns and verbs for each\nhallucination type. Based on this, we tackle a task of audio hallucination\nclassification using pre-trained audio-text models in the zero-shot and\nfine-tuning settings. Our experimental results reveal that the zero-shot models\nachieve higher performance (52.2% in F1) than the random (40.3%) and the\nfine-tuning models achieve 87.9%, outperforming the zero-shot models.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.09774v1"
    },
    {
        "title": "SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context\n  Misinformation Detection",
        "authors": [
            "Peng Qi",
            "Zehong Yan",
            "Wynne Hsu",
            "Mong Li Lee"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Misinformation is a prevalent societal issue due to its potential high risks.\nOut-of-context (OOC) misinformation, where authentic images are repurposed with\nfalse text, is one of the easiest and most effective ways to mislead audiences.\nCurrent methods focus on assessing image-text consistency but lack convincing\nexplanations for their judgments, which is essential for debunking\nmisinformation. While Multimodal Large Language Models (MLLMs) have rich\nknowledge and innate capability for visual reasoning and explanation\ngeneration, they still lack sophistication in understanding and discovering the\nsubtle crossmodal differences. In this paper, we introduce SNIFFER, a novel\nmultimodal large language model specifically engineered for OOC misinformation\ndetection and explanation. SNIFFER employs two-stage instruction tuning on\nInstructBLIP. The first stage refines the model's concept alignment of generic\nobjects with news-domain entities and the second stage leverages language-only\nGPT-4 generated OOC-specific instruction data to fine-tune the model's\ndiscriminatory powers. Enhanced by external tools and retrieval, SNIFFER not\nonly detects inconsistencies between text and image but also utilizes external\nknowledge for contextual verification. Our experiments show that SNIFFER\nsurpasses the original MLLM by over 40% and outperforms state-of-the-art\nmethods in detection accuracy. SNIFFER also provides accurate and persuasive\nexplanations as validated by quantitative and human evaluations.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.03170v1"
    },
    {
        "title": "A Parametric Rate-Distortion Model for Video Transcoding",
        "authors": [
            "Maedeh Jamali",
            "Nader Karimi",
            "Shadrokh Samavi",
            "Shahram Shirani"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Over the past two decades, the surge in video streaming applications has been\nfueled by the increasing accessibility of the internet and the growing demand\nfor network video. As users with varying internet speeds and devices seek\nhigh-quality video, transcoding becomes essential for service providers. In\nthis paper, we introduce a parametric rate-distortion (R-D) transcoding model.\nOur model excels at predicting transcoding distortion at various rates without\nthe need for encoding the video. This model serves as a versatile tool that can\nbe used to achieve visual quality improvement (in terms of PSNR) via\ntrans-sizing. Moreover, we use our model to identify visually lossless and\nnear-zero-slope bitrate ranges for an ingest video. Having this information\nallows us to adjust the transcoding target bitrate while introducing visually\nnegligible quality degradations. By utilizing our model in this manner, quality\nimprovements up to 2 dB and bitrate savings of up to 46% of the original target\nbitrate are possible. Experimental results demonstrate the efficacy of our\nmodel in video transcoding rate distortion prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.09029v1"
    },
    {
        "title": "A Rate-Distortion-Classification Approach for Lossy Image Compression",
        "authors": [
            "Yuefeng Zhang"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  In lossy image compression, the objective is to achieve minimal signal\ndistortion while compressing images to a specified bit rate. The increasing\ndemand for visual analysis applications, particularly in classification tasks,\nhas emphasized the significance of considering semantic distortion in\ncompressed images. To bridge the gap between image compression and visual\nanalysis, we propose a Rate-Distortion-Classification (RDC) model for lossy\nimage compression, offering a unified framework to optimize the trade-off\nbetween rate, distortion, and classification accuracy. The RDC model is\nextensively analyzed both statistically on a multi-distribution source and\nexperimentally on the widely used MNIST dataset. The findings reveal that the\nRDC model exhibits desirable properties, including monotonic non-increasing and\nconvex functions, under certain conditions. This work provides insights into\nthe development of human-machine friendly compression methods and Video Coding\nfor Machine (VCM) approaches, paving the way for end-to-end image compression\ntechniques in real-world applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.03500v1"
    },
    {
        "title": "Improving Multimodal Learning with Multi-Loss Gradient Modulation",
        "authors": [
            "Konstantinos Kontras",
            "Christos Chatzichristos",
            "Matthew Blaschko",
            "Maarten De Vos"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Learning from multiple modalities, such as audio and video, offers\nopportunities for leveraging complementary information, enhancing robustness,\nand improving contextual understanding and performance. However, combining such\nmodalities presents challenges, especially when modalities differ in data\nstructure, predictive contribution, and the complexity of their learning\nprocesses. It has been observed that one modality can potentially dominate the\nlearning process, hindering the effective utilization of information from other\nmodalities and leading to sub-optimal model performance. To address this issue\nthe vast majority of previous works suggest to assess the unimodal\ncontributions and dynamically adjust the training to equalize them. We improve\nupon previous work by introducing a multi-loss objective and further refining\nthe balancing process, allowing it to dynamically adjust the learning pace of\neach modality in both directions, acceleration and deceleration, with the\nability to phase out balancing effects upon convergence. We achieve superior\nresults across three audio-video datasets: on CREMA-D, models with ResNet\nbackbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer\nbackbone models deliver improvements ranging from 2.8% to 14.1% across\ndifferent fusion methods. On AVE, improvements range from 2.7% to 7.7%, while\non UCF101, gains reach up to 6.1%.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07930v2"
    },
    {
        "title": "Robust Multi-Modal Speech In-Painting: A Sequence-to-Sequence Approach",
        "authors": [
            "Mahsa Kadkhodaei Elyaderani",
            "Shahram Shirani"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  The process of reconstructing missing parts of speech audio from context is\ncalled speech in-painting. Human perception of speech is inherently\nmulti-modal, involving both audio and visual (AV) cues. In this paper, we\nintroduce and study a sequence-to-sequence (seq2seq) speech in-painting model\nthat incorporates AV features. Our approach extends AV speech in-painting\ntechniques to scenarios where both audio and visual data may be jointly\ncorrupted. To achieve this, we employ a multi-modal training paradigm that\nboosts the robustness of our model across various conditions involving acoustic\nand visual distortions. This makes our distortion-aware model a plausible\nsolution for real-world challenging environments. We compare our method with\nexisting transformer-based and recurrent neural network-based models, which\nattempt to reconstruct missing speech gaps ranging from a few milliseconds to\nover a second. Our experimental results demonstrate that our novel seq2seq\narchitecture outperforms the state-of-the-art transformer solution by 38.8% in\nterms of enhancing speech quality and 7.14% in terms of improving speech\nintelligibility. We exploit a multi-task learning framework that simultaneously\nperforms lip-reading (transcribing video components to text) while\nreconstructing missing parts of the associated speech.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.00901v1"
    },
    {
        "title": "Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot\n  Learning: A General Framework",
        "authors": [
            "Liuyuan Wen"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Generalized Zero-Shot Learning (GZSL) is a challenging task requiring\naccurate classification of both seen and unseen classes. Within this domain,\nAudio-visual GZSL emerges as an extremely exciting yet difficult task, given\nthe inclusion of both visual and acoustic features as multi-modal inputs.\nExisting efforts in this field mostly utilize either embedding-based or\ngenerative-based methods. However, generative training is difficult and\nunstable, while embedding-based methods often encounter domain shift problem.\nThus, we find it promising to integrate both methods into a unified framework\nto leverage their advantages while mitigating their respective disadvantages.\nOur study introduces a general framework employing out-of-distribution (OOD)\ndetection, aiming to harness the strengths of both approaches. We first employ\ngenerative adversarial networks to synthesize unseen features, enabling the\ntraining of an OOD detector alongside classifiers for seen and unseen classes.\nThis detector determines whether a test feature belongs to seen or unseen\nclasses, followed by classification utilizing separate classifiers for each\nfeature type. We test our framework on three popular audio-visual datasets and\nobserve a significant improvement comparing to existing state-of-the-art works.\nCodes can be found in https://github.com/liuyuan-wen/AV-OOD-GZSL.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.01284v1"
    },
    {
        "title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual\n  Representation and Generation",
        "authors": [
            "Kun Su",
            "Xiulong Liu",
            "Eli Shlizerman"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Video encompasses both visual and auditory data, creating a perceptually rich\nexperience where these two modalities complement each other. As such, videos\nare a valuable type of media for the investigation of the interplay between\naudio and visual elements. Previous studies of audio-visual modalities\nprimarily focused on either audio-visual representation learning or generative\nmodeling of a modality conditioned on the other, creating a disconnect between\nthese two branches. A unified framework that learns representation and\ngenerates modalities has not been developed yet. In this work, we introduce a\nnovel framework called Vision to Audio and Beyond (VAB) to bridge the gap\nbetween audio-visual representation learning and vision-to-audio generation.\nThe key approach of VAB is that rather than working with raw video frames and\naudio data, VAB performs representation learning and generative modeling within\nlatent spaces. In particular, VAB uses a pre-trained audio tokenizer and an\nimage encoder to obtain audio tokens and visual features, respectively. It then\nperforms the pre-training task of visual-conditioned masked audio token\nprediction. This training strategy enables the model to engage in contextual\nlearning and simultaneous video-to-audio generation. After the pre-training\nphase, VAB employs the iterative-decoding approach to rapidly generate audio\ntokens conditioned on visual features. Since VAB is a unified model, its\nbackbone can be fine-tuned for various audio-visual downstream tasks. Our\nexperiments showcase the efficiency of VAB in producing high-quality audio from\nvideo, and its capability to acquire semantic audio-visual features, leading to\ncompetitive results in audio-visual retrieval and classification.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.19132v1"
    },
    {
        "title": "Enhancing Lie Detection Accuracy: A Comparative Study of Classic ML,\n  CNN, and GCN Models using Audio-Visual Features",
        "authors": [
            "Abdelrahman Abdelwahab",
            "Akshaj Vishnubhatla",
            "Ayaan Vaswani",
            "Advait Bharathulwar",
            "Arnav Kommaraju"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Inaccuracies in polygraph tests often lead to wrongful convictions, false\ninformation, and bias, all of which have significant consequences for both\nlegal and political systems. Recently, analyzing facial micro-expressions has\nemerged as a method for detecting deception; however, current models have not\nreached high accuracy and generalizability. The purpose of this study is to aid\nin remedying these problems. The unique multimodal transformer architecture\nused in this study improves upon previous approaches by using auditory inputs,\nvisual facial micro-expressions, and manually transcribed gesture annotations,\nmoving closer to a reliable non-invasive lie detection model. Visual and\nauditory features were extracted using the Vision Transformer and OpenSmile\nmodels respectively, which were then concatenated with the transcriptions of\nparticipants micro-expressions and gestures. Various models were trained for\nthe classification of lies and truths using these processed and concatenated\nfeatures. The CNN Conv1D multimodal model achieved an average accuracy of\n95.4%. However, further research is still required to create higher-quality\ndatasets and even more generalized models for more diverse applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.08885v1"
    },
    {
        "title": "A Survey of Recent Advances and Challenges in Deep Audio-Visual\n  Correlation Learning",
        "authors": [
            "Luis Vilaca",
            "Yi Yu",
            "Paula Vinan"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Audio-visual correlation learning aims to capture and understand natural\nphenomena between audio and visual data. The rapid growth of Deep Learning\npropelled the development of proposals that process audio-visual data and can\nbe observed in the number of proposals in the past years. Thus encouraging the\ndevelopment of a comprehensive survey. Besides analyzing the models used in\nthis context, we also discuss some tasks of definition and paradigm applied in\nAI multimedia. In addition, we investigate objective functions frequently used\nand discuss how audio-visual data is exploited in the optimization process,\ni.e., the different methodologies for representing knowledge in the\naudio-visual domain. In fact, we focus on how human-understandable mechanisms,\ni.e., structured knowledge that reflects comprehensible knowledge, can guide\nthe learning process. Most importantly, we provide a summarization of the\nrecent progress of Audio-Visual Correlation Learning (AVCL) and discuss the\nfuture research directions.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.00049v1"
    },
    {
        "title": "A Digital Hardware Fast Algorithm and FPGA-based Prototype for a Novel\n  16-point Approximate DCT for Image Compression Applications",
        "authors": [
            "F. M. Bayer",
            "R. J. Cintra",
            "A. Edirisuriya",
            "A. Madanayake"
        ],
        "category": "cs.MM",
        "published_year": "2017",
        "summary": "  The discrete cosine transform (DCT) is the key step in many image and video\ncoding standards. The 8-point DCT is an important special case, possessing\nseveral low-complexity approximations widely investigated. However, 16-point\nDCT transform has energy compaction advantages. In this sense, this paper\npresents a new 16-point DCT approximation with null multiplicative complexity.\nThe proposed transform matrix is orthogonal and contains only zeros and ones.\nThe proposed transform outperforms the well-know Walsh-Hadamard transform and\nthe current state-of-the-art 16-point approximation. A fast algorithm for the\nproposed transform is also introduced. This fast algorithm is experimentally\nvalidated using hardware implementations that are physically realized and\nverified on a 40 nm CMOS Xilinx Virtex-6 XC6VLX240T FPGA chip for a maximum\nclock rate of 342 MHz. Rapid prototypes on FPGA for 8-bit input word size shows\nsignificant improvement in compressed image quality by up to 1-2 dB at the cost\nof only eight adders compared to the state-of-art 16-point DCT approximation\nalgorithm in the literature [S. Bouguezel, M. O. Ahmad, and M. N. S. Swamy. A\nnovel transform for image compression. In {\\em Proceedings of the 53rd IEEE\nInternational Midwest Symposium on Circuits and Systems (MWSCAS)}, 2010].\n",
        "pdf_link": "http://arxiv.org/pdf/1702.01805v1"
    },
    {
        "title": "Destruction of Image Steganography using Generative Adversarial Networks",
        "authors": [
            "Isaac Corley",
            "Jonathan Lwowski",
            "Justin Hoffman"
        ],
        "category": "cs.MM",
        "published_year": "2019",
        "summary": "  Digital image steganalysis, or the detection of image steganography, has been\nstudied in depth for years and is driven by Advanced Persistent Threat (APT)\ngroups', such as APT37 Reaper, utilization of steganographic techniques to\ntransmit additional malware to perform further post-exploitation activity on a\ncompromised host. However, many steganalysis algorithms are constrained to work\nwith only a subset of all possible images in the wild or are known to produce a\nhigh false positive rate. This results in blocking any suspected image being an\nunreasonable policy. A more feasible policy is to filter suspicious images\nprior to reception by the host machine. However, how does one optimally filter\nspecifically to obfuscate or remove image steganography while avoiding\ndegradation of visual image quality in the case that detection of the image was\na false positive? We propose the Deep Digital Steganography Purifier (DDSP), a\nGenerative Adversarial Network (GAN) which is optimized to destroy\nsteganographic content without compromising the perceptual quality of the\noriginal image. As verified by experimental results, our model is capable of\nproviding a high rate of destruction of steganographic image content while\nmaintaining a high visual quality in comparison to other state-of-the-art\nfiltering methods. Additionally, we test the transfer learning capability of\ngeneralizing to to obfuscate real malware payloads embedded into different\nimage file formats and types using an unseen steganographic algorithm and prove\nthat our model can in fact be deployed to provide adequate results.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10070v1"
    },
    {
        "title": "A Survey on Multimodal Disinformation Detection",
        "authors": [
            "Firoj Alam",
            "Stefano Cresci",
            "Tanmoy Chakraborty",
            "Fabrizio Silvestri",
            "Dimiter Dimitrov",
            "Giovanni Da San Martino",
            "Shaden Shaar",
            "Hamed Firooz",
            "Preslav Nakov"
        ],
        "category": "cs.MM",
        "published_year": "2021",
        "summary": "  Recent years have witnessed the proliferation of offensive content online\nsuch as fake news, propaganda, misinformation, and disinformation. While\ninitially this was mostly about textual content, over time images and videos\ngained popularity, as they are much easier to consume, attract more attention,\nand spread further than text. As a result, researchers started leveraging\ndifferent modalities and combinations thereof to tackle online multimodal\noffensive content. In this study, we offer a survey on the state-of-the-art on\nmultimodal disinformation detection covering various combinations of\nmodalities: text, images, speech, video, social media network structure, and\ntemporal information. Moreover, while some studies focused on factuality,\nothers investigated how harmful the content is. While these two components in\nthe definition of disinformation (i) factuality, and (ii) harmfulness, are\nequally important, they are typically studied in isolation. Thus, we argue for\nthe need to tackle disinformation detection by taking into account multiple\nmodalities as well as both factuality and harmfulness, in the same framework.\nFinally, we discuss current challenges and future research directions\n",
        "pdf_link": "http://arxiv.org/pdf/2103.12541v2"
    },
    {
        "title": "OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset",
        "authors": [
            "Jeongkyun Park",
            "Jung-Wook Hwang",
            "Kwanghee Choi",
            "Seung-Hyun Lee",
            "Jun Hwan Ahn",
            "Rae-Hong Park",
            "Hyung-Min Park"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  Inspired by humans comprehending speech in a multi-modal manner, various\naudio-visual datasets have been constructed. However, most existing datasets\nfocus on English, induce dependencies with various prediction models during\ndataset preparation, and have only a small number of multi-view videos. To\nmitigate the limitations, we recently developed the Open Large-scale Korean\nAudio-Visual Speech (OLKAVS) dataset, which is the largest among publicly\navailable audio-visual speech datasets. The dataset contains 1,150 hours of\ntranscribed audio from 1,107 Korean speakers in a studio setup with nine\ndifferent viewpoints and various noise situations. We also provide the\npre-trained baseline models for two tasks, audio-visual speech recognition and\nlip reading. We conducted experiments based on the models to verify the\neffectiveness of multi-modal and multi-view training over uni-modal and\nfrontal-view-only training. We expect the OLKAVS dataset to facilitate\nmulti-modal research in broader areas such as Korean speech recognition,\nspeaker recognition, pronunciation level classification, and mouth motion\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.06375v1"
    },
    {
        "title": "ImageBind-LLM: Multi-modality Instruction Tuning",
        "authors": [
            "Jiaming Han",
            "Renrui Zhang",
            "Wenqi Shao",
            "Peng Gao",
            "Peng Xu",
            "Han Xiao",
            "Kaipeng Zhang",
            "Chris Liu",
            "Song Wen",
            "Ziyu Guo",
            "Xudong Lu",
            "Shuai Ren",
            "Yafei Wen",
            "Xiaoxin Chen",
            "Xiangyu Yue",
            "Hongsheng Li",
            "Yu Qiao"
        ],
        "category": "cs.MM",
        "published_year": "2023",
        "summary": "  We present ImageBind-LLM, a multi-modality instruction tuning method of large\nlanguage models (LLMs) via ImageBind. Existing works mainly focus on language\nand image instruction tuning, different from which, our ImageBind-LLM can\nrespond to multi-modality conditions, including audio, 3D point clouds, video,\nand their embedding-space arithmetic by only image-text alignment training.\nDuring training, we adopt a learnable bind network to align the embedding space\nbetween LLaMA and ImageBind's image encoder. Then, the image features\ntransformed by the bind network are added to word tokens of all layers in\nLLaMA, which progressively injects visual instructions via an attention-free\nand zero-initialized gating mechanism. Aided by the joint embedding of\nImageBind, the simple image-text training enables our model to exhibit superior\nmulti-modality instruction-following capabilities. During inference, the\nmulti-modality inputs are fed into the corresponding ImageBind encoders, and\nprocessed by a proposed visual cache model for further cross-modal embedding\nenhancement. The training-free cache model retrieves from three million image\nfeatures extracted by ImageBind, which effectively mitigates the\ntraining-inference modality discrepancy. Notably, with our approach,\nImageBind-LLM can respond to instructions of diverse modalities and demonstrate\nsignificant language generation quality. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.03905v2"
    },
    {
        "title": "Multimodal Emotion Recognition using Audio-Video Transformer Fusion with\n  Cross Attention",
        "authors": [
            "Joe Dhanith P R",
            "Shravan Venkatraman",
            "Modigari Narendra",
            "Vigya Sharma",
            "Santhosh Malarvannan",
            "Amir H. Gandomi"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Understanding emotions is a fundamental aspect of human communication.\nIntegrating audio and video signals offers a more comprehensive understanding\nof emotional states compared to traditional methods that rely on a single data\nsource, such as speech or facial expressions. Despite its potential, multimodal\nemotion recognition faces significant challenges, particularly in\nsynchronization, feature extraction, and fusion of diverse data sources. To\naddress these issues, this paper introduces a novel transformer-based model\nnamed Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA\nmodel employs a transformer fusion approach to effectively capture and\nsynchronize interlinked features from both audio and video inputs, thereby\nresolving synchronization problems. Additionally, the Cross Attention mechanism\nwithin AVT-CA selectively extracts and emphasizes critical features while\ndiscarding irrelevant ones from both modalities, addressing feature extraction\nand fusion challenges. Extensive experimental analysis conducted on the\nCMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the\nproposed model. The results underscore the importance of AVT-CA in developing\nprecise and reliable multimodal emotion recognition systems for practical\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.18552v2"
    },
    {
        "title": "Revelio: A Real-World Screen-Camera Communication System with Visually\n  Imperceptible Data Embedding",
        "authors": [
            "Abbaas Alif Mohamed Nishar",
            "Shrinivas Kudekar",
            "Bernard Kintzing",
            "Ashwin Ashok"
        ],
        "category": "cs.MM",
        "published_year": "2025",
        "summary": "  We present `Revelio', a real-world screen-camera communication system\nleveraging temporal flicker fusion in the OKLAB color space. Using\nspatially-adaptive flickering and encoding information in pixel region shapes,\nRevelio achieves visually imperceptible data embedding while remaining robust\nagainst noise, asynchronicity, and distortions in screen-camera channels,\nensuring reliable decoding by standard smartphone cameras. The decoder, driven\nby a two-stage neural network, uses a weighted differential accumulator for\nprecise frame detection and symbol recognition. Initial experiments demonstrate\nRevelio's effectiveness in interactive television, offering an unobtrusive\nmethod for meta-information transmission.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.02349v1"
    },
    {
        "title": "Personality Analysis from Online Short Video Platforms with Multi-domain\n  Adaptation",
        "authors": [
            "Sixu An",
            "Xiangguo Sun",
            "Yicong Li",
            "Yu Yang",
            "Guandong Xu"
        ],
        "category": "cs.MM",
        "published_year": "2024",
        "summary": "  Personality analysis from online short videos has gained prominence due to\nits applications in personalized recommendation systems, sentiment analysis,\nand human-computer interaction. Traditional assessment methods, such as\nquestionnaires based on the Big Five Personality Framework, are limited by\nself-report biases and are impractical for large-scale or real-time analysis.\nLeveraging the rich, multi-modal data present in short videos offers a\npromising alternative for more accurate personality inference. However,\nintegrating these diverse and asynchronous modalities poses significant\nchallenges, particularly in aligning time-varying data and ensuring models\ngeneralize well to new domains with limited labeled data. In this paper, we\npropose a novel multi-modal personality analysis framework that addresses these\nchallenges by synchronizing and integrating features from multiple modalities\nand enhancing model generalization through domain adaptation. We introduce a\ntimestamp-based modality alignment mechanism that synchronizes data based on\nspoken word timestamps, ensuring accurate correspondence across modalities and\nfacilitating effective feature integration. To capture temporal dependencies\nand inter-modal interactions, we employ Bidirectional Long Short-Term Memory\nnetworks and self-attention mechanisms, allowing the model to focus on the most\ninformative features for personality prediction. Furthermore, we develop a\ngradient-based domain adaptation method that transfers knowledge from multiple\nsource domains to improve performance in target domains with scarce labeled\ndata. Extensive experiments on real-world datasets demonstrate that our\nframework significantly outperforms existing methods in personality prediction\ntasks, highlighting its effectiveness in capturing complex behavioral cues and\nrobustness in adapting to new domains.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.00813v1"
    }
]