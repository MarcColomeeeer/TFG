[
    {
        "title": "Alternative Local Discriminant Bases Using Empirical Expectation and\n  Variance Estimation",
        "authors": [
            "Eirik Fossgaard"
        ],
        "category": "cs.NA",
        "published_year": "1999",
        "summary": "  We propose alternative discriminant measures for selecting the best basis\namong a large collection of orthonormal bases for classification purposes. A\ngeneralization of the Local Discriminant Basis Algorithm of Saito and Coifman\nis constructed. The success of these new methods is evaluated and compared to\nearlier methods in experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/9907005v1"
    },
    {
        "title": "On the Significance of Digits in Interval Notation",
        "authors": [
            "M. H. van Emden"
        ],
        "category": "cs.NA",
        "published_year": "2002",
        "summary": "  To analyse the significance of the digits used for interval bounds, we\nclarify the philosophical presuppositions of various interval notations. We use\ninformation theory to determine the information content of the last digit of\nthe numeral used to denote the interval's bounds. This leads to the notion of\nefficiency of a decimal digit: the actual value as percentage of the maximal\nvalue of its information content. By taking this efficiency into account, many\npresentations of intervals can be made more readable at the expense of\nnegligible loss of information.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0201015v1"
    },
    {
        "title": "A sufficient condition for global invertibility of Lipschitz mapping",
        "authors": [
            "S. Tarasov"
        ],
        "category": "cs.NA",
        "published_year": "2002",
        "summary": "  We show that S.Vavasis' sufficient condition for global invertibility of a\npolynomial mapping can be easily generalized to the case of a general Lipschitz\nmapping. Keywords: Invertibility conditions, generalized Jacobian, nonsmooth\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0206031v1"
    },
    {
        "title": "New Developments in Interval Arithmetic and Their Implications for\n  Floating-Point Standardization",
        "authors": [
            "M. H. van Emden"
        ],
        "category": "cs.NA",
        "published_year": "2002",
        "summary": "  We consider the prospect of a processor that can perform interval arithmetic\nat the same speed as conventional floating-point arithmetic. This makes it\npossible for all arithmetic to be performed with the superior security of\ninterval methods without any penalty in speed. In such a situation the IEEE\nfloating-point standard needs to be compared with a version of floating-point\narithmetic that is ideal for the purpose of interval arithmetic. Such a\ncomparison requires a succinct and complete exposition of interval arithmetic\naccording to its recent developments. We present such an exposition in this\npaper. We conclude that the directed roundings toward the infinities and the\ndefinition of division by the signed zeros are valuable features of the\nstandard. Because the operations of interval arithmetic are always defined,\nexceptions do not arise. As a result neither Nans nor exceptions are needed. Of\nthe status flags, only the inexact flag may be useful. Denormalized numbers\nseem to have no use for interval arithmetic; in the use of interval\nconstraints, they are a handicap.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0210015v2"
    },
    {
        "title": "Computing sharp and scalable bounds on errors in approximate zeros of\n  univariate polynomials",
        "authors": [
            "P. H. D. Ramakrishna",
            "Sudebkumar Prasant Pal",
            "Samir Bhalla",
            "Hironmay Basu",
            "Sudhir Kumar Singh"
        ],
        "category": "cs.NA",
        "published_year": "2003",
        "summary": "  There are several numerical methods for computing approximate zeros of a\ngiven univariate polynomial. In this paper, we develop a simple and novel\nmethod for determining sharp upper bounds on errors in approximate zeros of a\ngiven polynomial using Rouche's theorem from complex analysis. We compute the\nerror bounds using non-linear optimization. Our bounds are scalable in the\nsense that we compute sharper error bounds for better approximations of zeros.\nWe use high precision computations using the LEDA/real floating-point filter\nfor computing our bounds robustly.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0306015v1"
    },
    {
        "title": "Propagation by Selective Initialization and Its Application to Numerical\n  Constraint Satisfaction Problems",
        "authors": [
            "M. H. van Emden",
            "B. Moa"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  Numerical analysis has no satisfactory method for the more realistic\noptimization models. However, with constraint programming one can compute a\ncover for the solution set to arbitrarily close approximation. Because the use\nof constraint propagation for composite arithmetic expressions is\ncomputationally expensive, consistency is computed with interval arithmetic. In\nthis paper we present theorems that support, selective initialization, a simple\nmodification of constraint propagation that allows composite arithmetic\nexpressions to be handled efficiently.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0404034v1"
    },
    {
        "title": "Solving Elliptic Finite Element Systems in Near-Linear Time with Support\n  Preconditioners",
        "authors": [
            "Erik Boman",
            "Bruce Hendrickson",
            "Stephen Vavasis"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  We consider linear systems arising from the use of the finite element method\nfor solving scalar linear elliptic problems. Our main result is that these\nlinear systems, which are symmetric and positive semidefinite, are well\napproximated by symmetric diagonally dominant matrices. Our framework for\ndefining matrix approximation is support theory. Significant graph theoretic\nwork has already been developed in the support framework for preconditioners in\nthe diagonally dominant case, and in particular it is known that such systems\ncan be solved with iterative methods in nearly linear time. Thus, our\napproximation result implies that these graph theoretic techniques can also\nsolve a class of finite element problems in nearly linear time. We show that\nthe support number bounds, which control the number of iterations in the\npreconditioned iterative solver, depend on mesh quality measures but not on the\nproblem size or shape of the domain.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0407022v4"
    },
    {
        "title": "Analysis of and workarounds for element reversal for a finite\n  element-based algorithm for warping triangular and tetrahedral meshes",
        "authors": [
            "Suzanne M. Shontz",
            "Stephen A. Vavasis"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  We consider an algorithm called FEMWARP for warping triangular and\ntetrahedral finite element meshes that computes the warping using the finite\nelement method itself. The algorithm takes as input a two- or three-dimensional\ndomain defined by a boundary mesh (segments in one dimension or triangles in\ntwo dimensions) that has a volume mesh (triangles in two dimensions or\ntetrahedra in three dimensions) in its interior. It also takes as input a\nprescribed movement of the boundary mesh. It computes as output updated\npositions of the vertices of the volume mesh. The first step of the algorithm\nis to determine from the initial mesh a set of local weights for each interior\nvertex that describes each interior vertex in terms of the positions of its\nneighbors. These weights are computed using a finite element stiffness matrix.\nAfter a boundary transformation is applied, a linear system of equations based\nupon the weights is solved to determine the final positions of the interior\nvertices. The FEMWARP algorithm has been considered in the previous literature\n(e.g., in a 2001 paper by Baker). FEMWARP has been succesful in computing\ndeformed meshes for certain applications. However, sometimes FEMWARP reverses\nelements; this is our main concern in this paper. We analyze the causes for\nthis undesirable behavior and propose several techniques to make the method\nmore robust against reversals. The most successful of the proposed methods\nincludes combining FEMWARP with an optimization-based untangler.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0410045v4"
    },
    {
        "title": "A Fully Sparse Implementation of a Primal-Dual Interior-Point Potential\n  Reduction Method for Semidefinite Programming",
        "authors": [
            "Gun Srijuntongsiri",
            "Stephen A. Vavasis"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  In this paper, we show a way to exploit sparsity in the problem data in a\nprimal-dual potential reduction method for solving a class of semidefinite\nprograms. When the problem data is sparse, the dual variable is also sparse,\nbut the primal one is not. To avoid working with the dense primal variable, we\napply Fukuda et al.'s theory of partial matrix completion and work with partial\nmatrices instead. The other place in the algorithm where sparsity should be\nexploited is in the computation of the search direction, where the gradient and\nthe Hessian-matrix product of the primal and dual barrier functions must be\ncomputed in every iteration. By using an idea from automatic differentiation in\nbackward mode, both the gradient and the Hessian-matrix product can be computed\nin time proportional to the time needed to compute the barrier functions of\nsparse variables itself. Moreover, the high space complexity that is normally\nassociated with the use of automatic differentiation in backward mode can be\navoided in this case. In addition, we suggest a technique to efficiently\ncompute the determinant of the positive definite matrix completion that is\nrequired to compute primal search directions. The method of obtaining one of\nthe primal search directions that minimizes the number of the evaluations of\nthe determinant of the positive definite completion is also proposed. We then\nimplement the algorithm and test it on the problem of finding the maximum cut\nof a graph.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0412009v1"
    },
    {
        "title": "Two Iterative Algorithms for Solving Systems of Simultaneous Linear\n  Algebraic Equations with Real Matrices of Coefficients",
        "authors": [
            "A. S. Kondratiev",
            "N. P. Polishchuk"
        ],
        "category": "cs.NA",
        "published_year": "2005",
        "summary": "  The paper describes two iterative algorithms for solving general systems of M\nsimultaneous linear algebraic equations (SLAE) with real matrices of\ncoefficients. The system can be determined, underdetermined, and\noverdetermined. Linearly dependent equations are also allowed. Both algorithms\nuse the method of Lagrange multipliers to transform the original SLAE into a\npositively determined function F of real original variables X(i) (i=1,...,N)\nand Lagrange multipliers Lambda(i) (i=1,...,M). Function F is differentiated\nwith respect to variables X(i) and the obtained relationships are used to\nexpress F in terms of Lagrange multipliers Lambda(i). The obtained function is\nminimized with respect to variables Lambda(i) with the help of one of two the\nfollowing minimization techniques: (1) relaxation method or (2) method of\nconjugate gradients by Fletcher and Reeves. Numerical examples are given.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0501041v1"
    },
    {
        "title": "New approach for Finite Difference Method for Thermal Analysis of\n  Passive Solar Systems",
        "authors": [
            "Stanko Shtrakov",
            "Anton Stoilov"
        ],
        "category": "cs.NA",
        "published_year": "2005",
        "summary": "  Mathematical treatment of massive wall systems is a useful tool for\ninvestigation of these solar applications. The objectives of this work are to\ndevelop (and validate) a numerical solution model for predication the thermal\nbehaviour of passive solar systems with massive wall, to improve knowledge of\nusing indirect passive solar systems and assess its energy efficiency according\nto climatic conditions in Bulgaria. The problem of passive solar systems with\nmassive walls is modelled by thermal and mass transfer equations. As a boundary\nconditions for the mathematical problem are used equations, which describe\ninfluence of weather data and constructive parameters of building on the\nthermal performance of the passive system. The mathematical model is solved by\nmeans of finite differences method and improved solution procedure. In article\nare presented results of theoretical and experimental study for developing and\nvalidating a numerical solution model for predication the thermal behaviour of\npassive solar systems with massive wall.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0502059v1"
    },
    {
        "title": "Divergence-free Wavelets for Navier-Stokes",
        "authors": [
            "Erwan Deriaz",
            "Valérie Perrier"
        ],
        "category": "cs.NA",
        "published_year": "2005",
        "summary": "  In this paper, we investigate the use of compactly supported divergence-free\nwavelets for the representation of the Navier-Stokes solution. After reminding\nthe theoretical construction of divergence-free wavelet vectors, we present in\ndetail the bases and corresponding fast algorithms for 2D and 3D incompressible\nflows. In order to compute the nonlinear term, we propose a new method which\nprovides in practice with the Hodge decomposition of any flow: this\ndecomposition enables us to separate the incompressible part of the flow from\nits orthogonal complement, which corresponds to the gradient component of the\nflow. Finally we show numerical tests to validate our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0502092v1"
    },
    {
        "title": "Segmentation of the Homogeneity of a Signal Using a Piecewise Linear\n  Recognition Tool",
        "authors": [
            "Joseph Morlier"
        ],
        "category": "cs.NA",
        "published_year": "2005",
        "summary": "  In this paper a new method of detection of homogeneous zones and singularity\nparts of a 1D signal is proposed. The entropy function is used to transform\nsignal in piecewise linear one. The multiple regression permits to detect lines\nand project them in the Hough parameters space in order to easily recognise\nhomogeneous zone and abrupt changes of the signal. Two application examples are\nanalysed, the first is a classical fractal signal and the other is issued from\na dynamic mechanical study.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0503086v2"
    },
    {
        "title": "A VFRoe scheme for 1D shallow water flows : wetting and drying\n  simulation",
        "authors": [
            "Abdou Wahidi Bello"
        ],
        "category": "cs.NA",
        "published_year": "2006",
        "summary": "  A finite-volume method for the one-dimensional shallow-water equations\nincluding topographic source terms is presented. Exploiting an original idea by\nLeroux, the system of partial-differential equations is completed by a trivial\nequation for the bathymetry. By applying a change of variable, the system is\ngiven a celerity-speed formulation, and linearized. As a result, an approximate\nRiemann solver preserving the positivity of the celerity can be constructed,\npermitting wetting and drying flow simulations to be performed. Finally, the\nsimulation of numerical test cases is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0609114v1"
    },
    {
        "title": "An informational approach to the global optimization of\n  expensive-to-evaluate functions",
        "authors": [
            "Julien Villemonteix",
            "Emmanuel Vazquez",
            "Eric Walter"
        ],
        "category": "cs.NA",
        "published_year": "2006",
        "summary": "  In many global optimization problems motivated by engineering applications,\nthe number of function evaluations is severely limited by time or cost. To\nensure that each evaluation contributes to the localization of good candidates\nfor the role of global minimizer, a sequential choice of evaluation points is\nusually carried out. In particular, when Kriging is used to interpolate past\nevaluations, the uncertainty associated with the lack of information on the\nfunction can be expressed and used to compute a number of criteria accounting\nfor the interest of an additional evaluation at any given point. This paper\nintroduces minimizer entropy as a new Kriging-based criterion for the\nsequential choice of points at which the function should be evaluated. Based on\n\\emph{stepwise uncertainty reduction}, it accounts for the informational gain\non the minimizer expected from a new evaluation. The criterion is approximated\nusing conditional simulations of the Gaussian process model behind Kriging, and\nthen inserted into an algorithm similar in spirit to the \\emph{Efficient Global\nOptimization} (EGO) algorithm. An empirical comparison is carried out between\nour criterion and \\emph{expected improvement}, one of the reference criteria in\nthe literature. Experimental results indicate major evaluation savings over\nEGO. Finally, the method, which we call IAGO (for Informational Approach to\nGlobal Optimization) is extended to robust optimization problems, where both\nthe factors to be tuned and the function evaluations are corrupted by noise.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0611143v2"
    },
    {
        "title": "The Fundamental Theorems of Interval Analysis",
        "authors": [
            "M. H. van Emden",
            "B. Moa"
        ],
        "category": "cs.NA",
        "published_year": "2007",
        "summary": "  Expressions are not functions. Confusing the two concepts or failing to\ndefine the function that is computed by an expression weakens the rigour of\ninterval arithmetic. We give such a definition and continue with the required\nre-statements and proofs of the fundamental theorems of interval arithmetic and\ninterval analysis.\n  Revision Feb. 10, 2009: added reference to and acknowledgement of P. Taylor.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0701141v2"
    },
    {
        "title": "Functions to Support Input and Output of Intervals",
        "authors": [
            "M. H. van Emden",
            "B. Moa",
            "S. C. Somosan"
        ],
        "category": "cs.NA",
        "published_year": "2007",
        "summary": "  Interval arithmetic is hardly feasible without directed rounding as provided,\nfor example, by the IEEE floating-point standard. Equally essential for\ninterval methods is directed rounding for conversion between the external\ndecimal and internal binary numerals. This is not provided by the standard I/O\nlibraries. Conversion algorithms exist that guarantee identity upon conversion\nfollowed by its inverse. Although it may be possible to adapt these algorithms\nfor use in decimal interval I/O, we argue that outward rounding in radix\nconversion is computationally a simpler problem than guaranteeing identity.\nHence it is preferable to develop decimal interval I/O ab initio, which is what\nwe do in this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0703003v1"
    },
    {
        "title": "Remarks on the O(N) Implementation of the Fast Marching Method",
        "authors": [
            "Christian Rasch",
            "Thomas Satzger"
        ],
        "category": "cs.NA",
        "published_year": "2007",
        "summary": "  The fast marching algorithm computes an approximate solution to the eikonal\nequation in O(N log N) time, where the factor log N is due to the\nadministration of a priority queue. Recently, Yatziv, Bartesaghi and Sapiro\nhave suggested to use an untidy priority queue, reducing the overall complexity\nto O(N) at the price of a small error in the computed solution. In this paper,\nwe give an explicit estimate of the error introduced, which is based on a\ndiscrete comparison principle. This estimates implies in particular that the\nchoice of an accuracy level that is independent of the speed function F results\nin the complexity bound O(Fmax /Fmin N). A numerical experiment illustrates\nthis robustness problem for large ratios Fmax /Fmin .\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0703082v1"
    },
    {
        "title": "Support-Graph Preconditioners for 2-Dimensional Trusses",
        "authors": [
            "Samuel I. Daitch",
            "Daniel A. Spielman"
        ],
        "category": "cs.NA",
        "published_year": "2007",
        "summary": "  We use support theory, in particular the fretsaw extensions of Shklarski and\nToledo, to design preconditioners for the stiffness matrices of 2-dimensional\ntruss structures that are stiffly connected. Provided that all the lengths of\nthe trusses are within constant factors of each other, that the angles at the\ncorners of the triangles are bounded away from 0 and $\\pi$, and that the\nelastic moduli and cross-sectional areas of all the truss elements are within\nconstant factors of each other, our preconditioners allow us to solve linear\nequations in the stiffness matrices to accuracy $\\epsilon$ in time $O (n^{5/4}\n(\\log^{2}n \\log \\log n)^{3/4} \\log (1/\\epsilon))$.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0703119v2"
    },
    {
        "title": "Families of traveling impulses and fronts in some models with\n  cross-diffusion",
        "authors": [
            "Faina Berezovskaya",
            "Artem Novozhilov",
            "Georgy Karev"
        ],
        "category": "cs.NA",
        "published_year": "2007",
        "summary": "  An analysis of traveling wave solutions of partial differential equation\n(PDE) systems with cross-diffusion is presented. The systems under study fall\nin a general class of the classical Keller-Segel models to describe chemotaxis.\nThe analysis is conducted using the theory of the phase plane analysis of the\ncorresponding wave systems without a priory restrictions on the boundary\nconditions of the initial PDE. Special attention is paid to families of\ntraveling wave solutions. Conditions for existence of front-impulse,\nimpulse-front, and front-front traveling wave solutions are formulated. In\nparticular, the simplest mathematical model is presented that has an\nimpulse-impulse solution; we also show that a non-isolated singular point in\nthe ordinary differential equation (ODE) wave system implies existence of\nfree-boundary fronts. The results can be used for construction and analysis of\ndifferent mathematical models describing systems with chemotaxis.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.0903v1"
    },
    {
        "title": "Fast Intrinsic Mode Decomposition of Time Series Data with Sawtooth\n  Transform",
        "authors": [
            "Louis Yu Lu"
        ],
        "category": "cs.NA",
        "published_year": "2007",
        "summary": "  An efficient method is introduced in this paper to find the intrinsic mode\nfunction (IMF) components of time series data. This method is faster and more\npredictable than the Empirical Mode Decomposition (EMD) method devised by the\nauthor of Hilbert Huang Transform (HHT). The approach is to transforms the\noriginal data function into a piecewise linear sawtooth function (or triangle\nwave function), then directly constructs the upper envelope by connecting the\nmaxima and construct lower envelope by connecting minima with straight line\nsegments in the sawtooth space, the IMF is calculated as the difference between\nthe sawtooth function and the mean of the upper and lower envelopes. The\nresults found in the sawtooth space are reversely transformed into the original\ndata space as the required IMF and envelopes mean. This decomposition method\nprocess the data in one pass to obtain a unique IMF component without the time\nconsuming repetitive sifting process of EMD method. An alternative\ndecomposition method with sawtooth function expansion is also presented.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.3170v2"
    },
    {
        "title": "Computing a Finite Size Representation of the Set of Approximate\n  Solutions of an MOP",
        "authors": [
            "Oliver Schuetze",
            "Carlos A. Coello Coello",
            "Emilia Tantar",
            "El-Ghazali Talbi"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  Recently, a framework for the approximation of the entire set of\n$\\epsilon$-efficient solutions (denote by $E_\\epsilon$) of a multi-objective\noptimization problem with stochastic search algorithms has been proposed. It\nwas proven that such an algorithm produces -- under mild assumptions on the\nprocess to generate new candidate solutions --a sequence of archives which\nconverges to $E_{\\epsilon}$ in the limit and in the probabilistic sense. The\nresult, though satisfactory for most discrete MOPs, is at least from the\npractical viewpoint not sufficient for continuous models: in this case, the set\nof approximate solutions typically forms an $n$-dimensional object, where $n$\ndenotes the dimension of the parameter space, and thus, it may come to\nperfomance problems since in practise one has to cope with a finite archive.\nHere we focus on obtaining finite and tight approximations of $E_\\epsilon$, the\nlatter measured by the Hausdorff distance. We propose and investigate a novel\narchiving strategy theoretically and empirically. For this, we analyze the\nconvergence behavior of the algorithm, yielding bounds on the obtained\napproximation quality as well as on the cardinality of the resulting\napproximation, and present some numerical results.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.0581v2"
    },
    {
        "title": "Communication-optimal parallel and sequential QR and LU factorizations:\n  theory and practice",
        "authors": [
            "James Demmel",
            "Laura Grigori",
            "Mark Hoemmen",
            "Julien Langou"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  We present parallel and sequential dense QR factorization algorithms that are\nboth optimal (up to polylogarithmic factors) in the amount of communication\nthey perform, and just as stable as Householder QR. Our first algorithm, Tall\nSkinny QR (TSQR), factors m-by-n matrices in a one-dimensional (1-D) block\ncyclic row layout, and is optimized for m >> n. Our second algorithm, CAQR\n(Communication-Avoiding QR), factors general rectangular matrices distributed\nin a two-dimensional block cyclic layout. It invokes TSQR for each block column\nfactorization.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.2159v3"
    },
    {
        "title": "A Data-Parallel Algorithm to Reliably Solve Systems of Nonlinear\n  Equations",
        "authors": [
            "Frédéric Goualard",
            "Alexandre Goldsztejn"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  Numerical methods based on interval arithmetic are efficient means to\nreliably solve nonlinear systems of equations. Algorithm bc3revise is an\ninterval method that tightens variables' domains by enforcing a property called\nbox consistency. It has been successfully used on difficult problems whose\nsolving eluded traditional numerical methods. We present a new algorithm to\nenforce box consistency that is simpler than bc3revise, faster, and easily data\nparallelizable. A parallel implementation with Intel SSE2 SIMD instructions\nshows that an increase in performance of up to an order of magnitude and more\nis achievable.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.2548v1"
    },
    {
        "title": "On the stability of bubble functions and a stabilized mixed finite\n  element formulation for the Stokes problem",
        "authors": [
            "D. Z. Turner",
            "K. B. Nakshatrala",
            "K. D. Hjelmstad"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  In this paper we investigate the relationship between stabilized and enriched\nfinite element formulations for the Stokes problem. We also present a new\nstabilized mixed formulation for which the stability parameter is derived\npurely by the method of weighted residuals. This new formulation allows equal\norder interpolation for the velocity and pressure fields. Finally, we show by\ncounterexample that a direct equivalence between subgrid-based stabilized\nfinite element methods and Galerkin methods enriched by bubble functions cannot\nbe constructed for quadrilateral and hexahedral elements using standard bubble\nfunctions.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.3099v2"
    },
    {
        "title": "Consistent Newton-Raphson vs. fixed-point for variational multiscale\n  formulations for incompressible Navier-Stokes",
        "authors": [
            "D. Z. Turner",
            "K. B. Nakshatrala",
            "K. D. Hjelmstad"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  The following paper compares a consistent Newton-Raphson and fixed-point\niteration based solution strategy for a variational multiscale finite element\nformulation for incompressible Navier-Stokes. The main contributions of this\nwork include a consistent linearization of the Navier-Stokes equations, which\nprovides an avenue for advanced algorithms that require origins in a consistent\nmethod. We also present a comparison between formulations that differ only in\ntheir linearization, but maintain all other equivalences. Using the variational\nmultiscale concept, we construct a stabilized formulation (that may be\nconsidered an extension of the MINI element to nonlinear Navier-Stokes). We\nthen linearize the problem using fixed-point iteration and by deriving a\nconsistent tangent matrix for the update equation to obtain the solution via\nNewton-Raphson iterations. We show that the consistent formulation converges in\nfewer iterations, as expected, for several test problems. We also show that the\nconsistent formulation converges for problems for which fixed-point iteration\ndiverges. We present the results of both methods for problems of Reynold's\nnumber up to 5000.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.3514v1"
    },
    {
        "title": "A stabilized finite element formulation for advection-diffusion using\n  the generalized finite element framework",
        "authors": [
            "D. Z. Turner",
            "K. B. Nakshatrala",
            "K. D. Hjelmstad"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  The following work presents a generalized (extended) finite element\nformulation for the advection-diffusion equation. Using enrichment functions\nthat represent the exponential nature of the exact solution, smooth numerical\nsolutions are obtained for problems with steep gradients and high Peclet\nnumbers (up to Pe = 25) in one and two-dimensions. As opposed to traditional\nstabilized methods that require the construction of stability parameters and\nstabilization terms, the present work avoids numerical instabilities by\nimproving the classical Galerkin solution with an enrichment function. To\ncontextualize this method among other stabilized methods, we show by\ndecomposition of the solution (in a multiscale manner) an equivalence to both\nGalerkin/least-squares type methods and those that use bubble functions. This\nwork also presents a strategy for constructing the enrichment function for\nproblems with complex geometries by employing a global-local approach.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.3963v1"
    },
    {
        "title": "Implementation for blow up of tornado-type solutions for complex version\n  of 3D Navier-Stokes system",
        "authors": [
            "M. D. Arnold",
            "A. V. Khokhlov"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  We consider Cauchy problem for Fourier transformation of 3-dimensional\nNavier-Stokes system with zero external force. Using initial data purposed by\nDong Li and Ya.G.Sinai we implement self-similar regime producing fast growing\nbehavior of the energy of solution while time tends to critical value.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.4286v1"
    },
    {
        "title": "Fast Intrinsic Mode Decomposition and Filtering of Time Series Data",
        "authors": [
            "Louis Yu Lu"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  The intrinsic mode function (IMF) provides adaptive function bases for\nnonlinear and non-stationary time series data. A fast convergent iterative\nmethod is introduced in this paper to find the IMF components of the data, the\nmethod is faster and more predictable than the Empirical Mode Decomposition\nmethod devised by the author of Hilbert Huang Transform. The approach is to\niteratively adjust the control points on the data function corresponding to the\nextrema of the refining IMF, the control points of the residue function are\ncalculated as the median of the straight line segments passing through the data\ncontrol points, the residue function is then constructed as the cubic spline\nfunction of the median points. The initial residue function is simply\nconstructed as the straight line segments passing through the extrema of the\nfirst derivative of the data function. The refining IMF is the difference\nbetween the data function and the improved residue function. The IMF found\nreveals all the riding waves in the whole data set. A new data filtering method\non frequency and amplitude of IMF is also presented with the similar approach\nof finding the residue on the part to be filtered out. The program to\ndemonstrate the method is distributed under BSD open source license.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.2827v3"
    },
    {
        "title": "The Stochastic Logarithmic Norm for Stability Analysis of Stochastic\n  Differential Equations",
        "authors": [
            "Sk. Safique Ahmad",
            "Nagalinga Rajan",
            "Soumyendu Raha"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  To analyze the stability of It\\^o stochastic differential equations with\nmultiplicative noise, we introduce the stochastic logarithmic norm. The\nlogarithmic norm was originally introduced by G. Dahlquist in 1958 as a tool to\nstudy the growth of solutions to ordinary differential equations and for\nestimating the error growth in discretization methods for their approximate\nsolutions. We extend the concept to the stability analysis of It\\^o stochastic\ndifferential equations with multiplicative noise. Stability estimates for\nlinear It\\^o SDEs using the one, two and $\\infty$-norms in the $l$-th mean,\nwhere $1 \\leq l < \\infty $, are derived and the application of the stochastic\nlogarithmic norm is illustrated with examples.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.0062v1"
    },
    {
        "title": "An algebraic approach to the set of intervals (a new approach of\n  arithmetic of intervals)",
        "authors": [
            "Nicolas Goze",
            "Elisabeth Remm"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  In this paper we present the set of intervals as a normed vector space. We\ndefine also a four-dimensional associative algebra whose product gives the\nproduct of intervals in any cases. This approach allows to give a notion of\ndivisibility and in some cases an euclidian division. We introduce differential\ncalculus and give some applications.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.5173v2"
    },
    {
        "title": "Non-negative mixed finite element formulations for a tensorial diffusion\n  equation",
        "authors": [
            "K. B. Nakshatrala",
            "A. J. Valocchi"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  We consider the tensorial diffusion equation, and address the discrete\nmaximum-minimum principle of mixed finite element formulations. In particular,\nwe address non-negative solutions (which is a special case of the\nmaximum-minimum principle) of mixed finite element formulations. The discrete\nmaximum-minimum principle is the discrete version of the maximum-minimum\nprinciple.\n  In this paper we present two non-negative mixed finite element formulations\nfor tensorial diffusion equations based on constrained optimization techniques\n(in particular, quadratic programming). These proposed mixed formulations\nproduce non-negative numerical solutions on arbitrary meshes for low-order\n(i.e., linear, bilinear and trilinear) finite elements. The first formulation\nis based on the Raviart-Thomas spaces, and is obtained by adding a non-negative\nconstraint to the variational statement of the Raviart-Thomas formulation. The\nsecond non-negative formulation based on the variational multiscale\nformulation.\n  For the former formulation we comment on the affect of adding the\nnon-negative constraint on the local mass balance property of the\nRaviart-Thomas formulation. We also study the performance of the active set\nstrategy for solving the resulting constrained optimization problems. The\noverall performance of the proposed formulation is illustrated on three\ncanonical test problems.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.0322v4"
    },
    {
        "title": "Condition Numbers of Gaussian Random Matrices",
        "authors": [
            "Zizhong Chen",
            "Jack Dongarra"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  Let $G_{m \\times n}$ be an $m \\times n$ real random matrix whose elements are\nindependent and identically distributed standard normal random variables, and\nlet $\\kappa_2(G_{m \\times n})$ be the 2-norm condition number of $G_{m \\times\nn}$. We prove that, for any $m \\geq 2$, $n \\geq 2$ and $x \\geq |n-m|+1$,\n$\\kappa_2(G_{m \\times n})$ satisfies $\n  \\frac{1}{\\sqrt{2\\pi}} ({c}/{x})^{|n-m|+1} < P(\\frac{\\kappa_2(G_{m \\times n})}\n{{n}/{(|n-m|+1)}}> x) <\n  \\frac{1}{\\sqrt{2\\pi}} ({C}/{x})^{|n-m|+1}, $ where $0.245 \\leq c \\leq 2.000$\nand $ 5.013 \\leq C \\leq 6.414$ are universal positive constants independent of\n$m$, $n$ and $x$. Moreover, for any $m \\geq 2$ and $n \\geq 2$, $\nE(\\log\\kappa_2(G_{m \\times n})) < \\log \\frac{n}{|n-m|+1} + 2.258. $ A similar\npair of results for complex Gaussian random matrices is also established.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.0800v1"
    },
    {
        "title": "Sensitivity Analysis Using a Fixed Point Interval Iteration",
        "authors": [
            "Alexandre Goldsztejn"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  Proving the existence of a solution to a system of real equations is a\ncentral issue in numerical analysis. In many situations, the system of\nequations depend on parameters which are not exactly known. It is then natural\nto aim proving the existence of a solution for all values of these parameters\nin some given domains. This is the aim of the parametrization of existence\ntests. A new parametric existence test based on the Hansen-Sengupta operator is\npresented and compared to a similar one based on the Krawczyk operator. It is\nused as a basis of a fixed point iteration dedicated to rigorous sensibility\nanalysis of parametric systems of equations.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.2984v1"
    },
    {
        "title": "How to improve the accuracy of the discrete gradient method in the\n  one-dimensional case",
        "authors": [
            "Jan L. Cieslinski",
            "Boguslaw Ratkiewicz"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  We present a new numerical scheme for one dimensional dynamical systems. This\nis a modification of the discrete gradient method and keeps its advantages,\nincluding the stability and the conservation of the energy integral. However,\nits accuracy is higher by several orders of magnitude.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.1906v1"
    },
    {
        "title": "On one method of boundary value problem regularization by passage to the\n  limit",
        "authors": [
            "Vladimir Gotsulenko",
            "Lyudmila Gaponova"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  For one class of boundary value problem depending on small parameter for\nwhich numerical methods for their solution are actually inapplicable, procedure\nof limiting problem acquisition which is much easier and which solution as much\nas close to the initial solution is described.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.3638v1"
    },
    {
        "title": "Two hierarchies of spline interpolations. Practical algorithms for\n  multivariate higher order splines",
        "authors": [
            "Cristian Constantin Lalescu"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  A systematic construction of higher order splines using two hierarchies of\npolynomials is presented. Explicit instructions on how to implement one of\nthese hierarchies are given. The results are limited to interpolations on\nregular, rectangular grids, but an approach to other types of grids is also\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3564v1"
    },
    {
        "title": "A fast algorithm for computing minimal-norm solutions to underdetermined\n  systems of linear equations",
        "authors": [
            "Mark Tygert"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  We introduce a randomized algorithm for computing the minimal-norm solution\nto an underdetermined system of linear equations. Given an arbitrary full-rank\nm x n matrix A with m<n, any m x 1 vector b, and any positive real number\nepsilon less than 1, the procedure computes an n x 1 vector x approximating to\nrelative precision epsilon or better the n x 1 vector p of minimal Euclidean\nnorm satisfying Ap=b. The algorithm typically requires O(mn\nlog(sqrt(n)/epsilon) + m**3) floating-point operations, generally less than the\nO(m**2 n) required by the classical schemes based on QR-decompositions or\nbidiagonalization. We present several numerical examples illustrating the\nperformance of the algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.4745v2"
    },
    {
        "title": "On the Convex Feasibility Problem",
        "authors": [
            "Laura Maruster",
            "Stefan Maruster"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  The convergence of the projection algorithm for solving the convex\nfeasibility problem for a family of closed convex sets, is in connection with\nthe regularity properties of the family. In the paper [18] are pointed out four\ncases of such a family depending of the two characteristics: the emptiness and\nboudedness of the intersection of the family. The case four (the interior of\nthe intersection is empty and the intersection itself is bounded) is unsolved.\nIn this paper we give a (partial) answer for the case four: in the case of two\nclosed convex sets in R3 the regularity property holds.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.4909v1"
    },
    {
        "title": "A stabilized finite element formulation of non-smooth contact",
        "authors": [
            "G. Haikal"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  The computational modeling of many engineering problems using the Finite\nElement method involves the modeling of two or more bodies that meet through an\ninterface. The interface can be physical, as in multi-physics and contact\nproblems, or purely numerical, as in the coupling of non-conforming meshes. The\nmost critical part of the modeling process is to ensure geometric compatibility\nand a complete transfer of surface tractions between the different components\nat the connecting interfaces. Popular contact modeling techniques rely on\ngeometric projections to detect and resolve overlapping or mass\ninterpenetration between two or more contacting bodies. Such approaches have\nbeen shown to have two major drawbacks: they are not suitable for contact at\nhighly nonlinear surfaces and sharp corners where smooth normal projections are\nnot feasible, and they fail to guarantee a complete and accurate transfer of\npressure across the interface. This dissertation presents a novel formulation\nfor the modeling of contact problems that possesses the ability to resolve\ncomplicated contact scenarios effectively, while being simpler to implement and\nmore widely applicable than currently available methods. We show that the\nformulation boils down to a node-to-surface gap function that works effectively\nfor non-smooth contact. The numerical implementation using the midpoint rule\nshows the need to guarantee the conservation of the total energy during impact,\nfor which a Lagrange multiplier method is used. We propose a local enrichment\nof the interface and a simple stabilization procedure based on the\ndiscontinuous Galerkin method to guarantee an accurate transfer of the pressure\nfield. The result is a robust interface formulation for contact problems and\nthe coupling of non-conforming meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.0504v3"
    },
    {
        "title": "Simultaneously Sparse Solutions to Linear Inverse Problems with Multiple\n  System Matrices and a Single Observation Vector",
        "authors": [
            "Adam C. Zelinski",
            "Vivek K Goyal",
            "Elfar Adalsteinsson"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  A linear inverse problem is proposed that requires the determination of\nmultiple unknown signal vectors. Each unknown vector passes through a different\nsystem matrix and the results are added to yield a single observation vector.\nGiven the matrices and lone observation, the objective is to find a\nsimultaneously sparse set of unknown vectors that solves the system. We will\nrefer to this as the multiple-system single-output (MSSO) simultaneous sparsity\nproblem. This manuscript contrasts the MSSO problem with other simultaneous\nsparsity problems and conducts a thorough initial exploration of algorithms\nwith which to solve it. Seven algorithms are formulated that approximately\nsolve this NP-Hard problem. Three greedy techniques are developed (matching\npursuit, orthogonal matching pursuit, and least squares matching pursuit) along\nwith four methods based on a convex relaxation (iteratively reweighted least\nsquares, two forms of iterative shrinkage, and formulation as a second-order\ncone program). The algorithms are evaluated across three experiments: the first\nand second involve sparsity profile recovery in noiseless and noisy scenarios,\nrespectively, while the third deals with magnetic resonance imaging\nradio-frequency excitation pulse design.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.2083v1"
    },
    {
        "title": "A numerical study of fluids with pressure dependent viscosity flowing\n  through a rigid porous medium",
        "authors": [
            "K. B. Nakshatrala",
            "K. R. Rajagopal"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  In this paper we consider modifications to Darcy's equation wherein the drag\ncoefficient is a function of pressure, which is a realistic model for\ntechnological applications like enhanced oil recovery and geological carbon\nsequestration. We first outline the approximations behind Darcy's equation and\nthe modifications that we propose to Darcy's equation, and derive the governing\nequations through a systematic approach using mixture theory. We then propose a\nstabilized mixed finite element formulation for the modified Darcy's equation.\nTo solve the resulting nonlinear equations we present a solution procedure\nbased on the consistent Newton-Raphson method. We solve representative test\nproblems to illustrate the performance of the proposed stabilized formulation.\nOne of the objectives of this paper is also to show that the dependence of\nviscosity on the pressure can have a significant effect both on the qualitative\nand quantitative nature of the solution.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.5234v2"
    },
    {
        "title": "Enhanced sampling schemes for MCMC based blind Bernoulli-Gaussian\n  deconvolution",
        "authors": [
            "D. Ge",
            "J. Idier",
            "E. Le Carpentier"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  This paper proposes and compares two new sampling schemes for sparse\ndeconvolution using a Bernoulli-Gaussian model. To tackle such a deconvolution\nproblem in a blind and unsupervised context, the Markov Chain Monte Carlo\n(MCMC) framework is usually adopted, and the chosen sampling scheme is most\noften the Gibbs sampler. However, such a sampling scheme fails to explore the\nstate space efficiently. Our first alternative, the $K$-tuple Gibbs sampler, is\nsimply a grouped Gibbs sampler. The second one, called partially marginalized\nsampler, is obtained by integrating the Gaussian amplitudes out of the target\ndistribution. While the mathematical validity of the first scheme is obvious as\na particular instance of the Gibbs sampler, a more detailed analysis is\nprovided to prove the validity of the second scheme.\n  For both methods, optimized implementations are proposed in terms of\ncomputation and storage cost. Finally, simulation results validate both schemes\nas more efficient in terms of convergence time compared with the plain Gibbs\nsampler. Benchmark sequence simulations show that the partially marginalized\nsampler takes fewer iterations to converge than the $K$-tuple Gibbs sampler.\nHowever, its computation load per iteration grows almost quadratically with\nrespect to the data length, while it only grows linearly for the $K$-tuple\nGibbs sampler.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.2793v2"
    },
    {
        "title": "A Numerical Algorithm for Zero Counting. II: Distance to Ill-posedness\n  and Smoothed Analysis",
        "authors": [
            "Felipe Cucker",
            "Teresa Krick",
            "Gregorio Malajovich",
            "Mario Wschebor"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  We show a Condition Number Theorem for the condition number of zero counting\nfor real polynomial systems. That is, we show that this condition number equals\nthe inverse of the normalized distance to the set of ill-posed systems (i.e.,\nthose having multiple real zeros). As a consequence, a smoothed analysis of\nthis condition number follows.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.4101v1"
    },
    {
        "title": "A Geometric Approach to Solve Fuzzy Linear Systems of Differential\n  Equations",
        "authors": [
            "N. Gasilov",
            "Sh. G. Amrahov",
            "A. Golayoglu Fatullayev"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  In this paper, systems of linear differential equations with crisp real\ncoefficients and with initial condition described by a vector of fuzzy numbers\nare studied. A new method based on the geometric representations of linear\ntransformations is proposed to find a solution. The most important difference\nbetween this method and methods offered in previous papers is that the solution\nis considered to be a fuzzy set of real vector-functions rather than a fuzzy\nvector-function. Each member of the set satisfies the given system with a\ncertain possibility. It is shown that at any time the solution constitutes a\nfuzzy region in the coordinate space, alfa-cuts of which are nested\nparallelepipeds. Proposed method is illustrated on examples.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.4307v1"
    },
    {
        "title": "An Example of Symmetry Exploitation for Energy-related Eigencomputations",
        "authors": [
            "Matthias Petschow",
            "Edoardo Di Napoli",
            "Paolo Bientinesi"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  One of the most used approaches in simulating materials is the tight-binding\napproximation. When using this method in a material simulation, it is necessary\nto compute the eigenvalues and eigenvectors of the Hamiltonian describing the\nsystem. In general, the system possesses few explicit symmetries. Due to them,\nthe problem has many degenerate eigenvalues. The ambiguity in choosing a\northonormal basis of the invariant subspaces, associated with degenerate\neigenvalues, will result in eigenvectors which are not invariant under the\naction of the symmetry operators in matrix form. A meaningful computation of\nthe eigenvectors needs to take those symmetries into account. A natural choice\nis a set of eigenvectors, which simultaneously diagonalizes the Hamiltonian and\nthe symmetry matrices. This is possible because all the matrices commute with\neach other. The simultaneous eigenvectors and the corresponding eigenvalues\nwill be in a parametrized form in terms of the lattice momentum components.\nThis functional dependence of the eigenvalues is the dispersion relation and\ndescribes the band structure of a material. Therefore it is important to find\nthis functional dependence in any numerical computation related to material\nproperties.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.5434v1"
    },
    {
        "title": "Fast algorithms for spherical harmonic expansions, III",
        "authors": [
            "Mark Tygert"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  We accelerate the computation of spherical harmonic transforms, using what is\nknown as the butterfly scheme. This provides a convenient alternative to the\napproach taken in the second paper from this series on \"Fast algorithms for\nspherical harmonic expansions.\" The requisite precomputations become manageable\nwhen organized as a \"depth-first traversal\" of the program's control-flow\ngraph, rather than as the perhaps more natural \"breadth-first traversal\" that\nprocesses one-by-one each level of the multilevel procedure. We illustrate the\nresults via several numerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.5435v6"
    },
    {
        "title": "On Element SDD Approximability",
        "authors": [
            "Haim Avron",
            "Gil Shklarski",
            "Sivan Toledo"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  This short communication shows that in some cases scalar elliptic finite\nelement matrices cannot be approximated well by an SDD matrix. We also give a\ntheoretical analysis of a simple heuristic method for approximating an element\nby an SDD matrix.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.0547v1"
    },
    {
        "title": "Solution of Non-Square Fuzzy Linear Systems",
        "authors": [
            "Nizami Gasilov",
            "Afet Golayoğlu Fatullayev",
            "Şahin Emrah Amrahov"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  In this paper, a linear system of equations with crisp coefficients and fuzzy\nright-hand sides is investigated. All possible cases pertaining to the number\nof variables, n, and the number of equations, m, are dealt with. A solution is\nsought not as a fuzzy vector, as usual, but as a fuzzy set of vectors. Each\nvector in the solution set solves the given fuzzy linear system with a certain\npossibility. Assuming that the coefficient matrix is a full rank matrix, three\ncases are considered: For m = n (square system), the solution set is shown to\nbe a parallelepiped in coordinate space and is expressed by an explicit\nformula. For m > n (overdetermined system), the solution set is proved to be a\nconvex polyhedron and a novel geometric method is proposed to compute it. For m\n< n (underdetermined system), by determining the contribution of free\nvariables, general solution is computed. From the results of three cases\nmentioned above, a method is proposed to handle the general case, in which the\ncoefficient matrix is not necessarily a full rank matrix. Comprehensive\nexamples are provided and investigated in depth to illustrate each case and\nsuggested method.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.0790v2"
    },
    {
        "title": "An Iteratively Reweighted Algorithm for Sparse Reconstruction of\n  Subsurface Flow Properties from Nonlinear Dynamic Data",
        "authors": [
            "Lianlin Li",
            "B. Jafarpour"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  In this paper, we present a practical algorithm based on sparsity\nregularization to effectively solve nonlinear dynamic inverse problems that are\nencountered in subsurface model calibration. We use an iteratively reweighted\nalgorithm that is widely used to solve linear inverse problems with sparsity\nconstraint known as compressed sensing to estimate permeability fields from\nnonlinear dynamic flow data.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.2270v2"
    },
    {
        "title": "Computation- and Space-Efficient Implementation of SSA",
        "authors": [
            "Anton Korobeynikov"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  The computational complexity of different steps of the basic SSA is\ndiscussed. It is shown that the use of the general-purpose \"blackbox\" routines\n(e.g. found in packages like LAPACK) leads to huge waste of time resources\nsince the special Hankel structure of the trajectory matrix is not taken into\naccount. We outline several state-of-the-art algorithms (for example,\nLanczos-based truncated SVD) which can be modified to exploit the structure of\nthe trajectory matrix. The key components here are hankel matrix-vector\nmultiplication and hankelization operator. We show that both can be computed\nefficiently by the means of Fast Fourier Transform. The use of these methods\nyields the reduction of the worst-case computational complexity from O(N^3) to\nO(k N log(N)), where N is series length and k is the number of eigentriples\ndesired.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.4498v2"
    },
    {
        "title": "On the indefinite Helmholtz equation: complex stretched absorbing\n  boundary layers, iterative analysis, and preconditioning",
        "authors": [
            "Bram Reps",
            "Wim Vanroose",
            "Hisham bin Zubair"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  This paper studies and analyzes a preconditioned Krylov solver for Helmholtz\nproblems that are formulated with absorbing boundary layers based on complex\ncoordinate stretching. The preconditioner problem is a Helmholtz problem where\nnot only the coordinates in the absorbing layer have an imaginary part, but\nalso the coordinates in the interior region. This results into a preconditioner\nproblem that is invertible with a multigrid cycle. We give a numerical analysis\nbased on the eigenvalues and evaluate the performance with several numerical\nexperiments. The method is an alternative to the complex shifted Laplacian and\nit gives a comparable performance for the studied model problems.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.0180v2"
    },
    {
        "title": "A fast randomized algorithm for orthogonal projection",
        "authors": [
            "Vladimir Rokhlin",
            "Mark Tygert"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  We describe an algorithm that, given any full-rank matrix A having fewer rows\nthan columns, can rapidly compute the orthogonal projection of any vector onto\nthe null space of A, as well as the orthogonal projection onto the row space of\nA, provided that both A and its adjoint can be applied rapidly to arbitrary\nvectors. As an intermediate step, the algorithm solves the overdetermined\nlinear least-squares regression involving the adjoint of A (and so can be used\nfor this, too). The basis of the algorithm is an obvious but numerically\nunstable scheme; suitable use of a preconditioner yields numerical stability.\nWe generate the preconditioner rapidly via a randomized procedure that succeeds\nwith extremely high probability. In many circumstances, the method can\naccelerate interior-point methods for convex optimization, such as linear\nprogramming (Ming Gu, personal communication).\n",
        "pdf_link": "http://arxiv.org/pdf/0912.1135v2"
    },
    {
        "title": "Least-Squares on the Real Symplectic Group",
        "authors": [
            "Simone Fiori"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  The present paper discusses the problem of least-squares over the real\nsymplectic group of matrices Sp(2n,R)$. The least-squares problem may be\nextended from flat spaces to curved spaces by the notion of geodesic distance.\nThe resulting non-linear minimization problem on manifold may be tackled by\nmeans of a gradient-descent algorithm tailored to the geometry of the space at\nhand. In turn, gradient steepest descent on manifold may be implemented through\na geodesic-based stepping method. As the space Sp(2n,R) is a non-compact Lie\ngroup, it is convenient to endow it with a pseudo-Riemannian geometry. Indeed,\na pseudo-Riemannian metric allows the computation of geodesic arcs and geodesic\ndistances in closed form on Sp(2n,R).\n",
        "pdf_link": "http://arxiv.org/pdf/1001.0829v2"
    },
    {
        "title": "Some Architectures for Chebyshev Interpolation",
        "authors": [
            "Theja Tulabandhula"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Digital architectures for Chebyshev interpolation are explored and a\nvariation which is word-serial in nature is proposed. These architectures are\ncontrasted with equispaced system structures. Further, Chebyshev interpolation\nscheme is compared to the conventional equispaced interpolation vis-a-vis\nreconstruction error and relative number of samples. It is also shown that the\nuse of a hybrid (or dual) Analog to Digital converter unit can reduce system\npower consumption by as much as 1/3rd of the original.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1185v1"
    },
    {
        "title": "Solving Tensor Structured Problems with Computational Tensor Algebra",
        "authors": [
            "Oleksii Morozov",
            "Patrick Hunziker"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Since its introduction by Gauss, Matrix Algebra has facilitated understanding\nof scientific problems, hiding distracting details and finding more elegant and\nefficient ways of computational solving. Today's largest problems, which often\noriginate from multidimensional data, might profit from even higher levels of\nabstraction. We developed a framework for solving tensor structured problems\nwith tensor algebra that unifies concepts from tensor analysis, multilinear\nalgebra and multidimensional signal processing. In contrast to the conventional\nmatrix approach, it allows the formulation of multidimensional problems, in a\nmultidimensional way, preserving structure and data coherence; and the\nimplementation of automated optimizations of solving algorithms, based on the\ncommutativity of all tensor operations. Its ability to handle large scientific\ntasks is showcased by a real-world, 4D medical imaging problem, with more than\n30 million unknown parameters solved on a current, inexpensive hardware. This\nsignificantly surpassed the best published matrix-based approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.5460v1"
    },
    {
        "title": "Geometric Programming Problem with Co-Efficients and Exponents\n  Associated with Binary Numbers",
        "authors": [
            "A. K. Ojha",
            "A. K. Das"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Geometric programming (GP) provides a power tool for solving a variety of\noptimization problems. In the real world, many applications of geometric\nprogramming (GP) are engineering design problems in which some of the problem\nparameters are estimating of actual values. This paper develops a solution\nprocedure to solve nonlinear programming problems using GP technique by\nsplitting the cost coefficients, constraint coefficients and exponents with the\nhelp of binary numbers. The equivalent mathematical programming problems are\nformulated to find their corresponding value of the objective function based on\nthe duality theorem. The ability of calculating the cost coefficients,\nconstraint coefficients and exponents developed in this paper might help lead\nto more realistic modeling efforts in engineering design areas. Standard\nnonlinear programming software has been used to solve the proposed optimization\nproblem. Two numerical examples are presented to illustrate the method.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.1167v1"
    },
    {
        "title": "Comments on \"Routh Stability Criterion\"",
        "authors": [
            "T. D. Roopamala",
            "S. K. Katti"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  In this note, we have shown special case on Routh stability criterion, which\nis not discussed, in previous literature. This idea can be useful in computer\nscience applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.1473v1"
    },
    {
        "title": "Multi-objective Geometric Programming Problem With Weighted Mean Method",
        "authors": [
            "A. K. Ojha",
            "K. K. Biswal"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Geometric programming is an important class of optimization problems that\nenable practitioners to model a large variety of real-world applications,\nmostly in the field of engineering design. In many real life optimization\nproblem multi-objective programming plays a vital role in socio-economical and\nindustrial optimizing problems. In this paper we have discussed the basic\nconcepts and principle of multiple objective optimization problems and\ndeveloped geometric programming (GP) technique to solve this optimization\nproblem using weighted method to obtain the non-inferior solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.1477v1"
    },
    {
        "title": "New Approach to Identify Common Eigenvalues of real matrices using\n  Gerschgorin Theorem and Bisection method",
        "authors": [
            "D. Roopamala",
            "S. K. Katti"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  In this paper, a new approach is presented to determine common eigenvalues of\ntwo matrices. It is based on Gerschgorin theorem and Bisection method. The\nproposed approach is simple and can be useful in image processing and noise\nestimation.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.1794v1"
    },
    {
        "title": "Enforcing the non-negativity constraint and maximum principles for\n  diffusion with decay on general computational grids",
        "authors": [
            "H. Nagarajan",
            "K. B. Nakshatrala"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  In this paper, we consider anisotropic diffusion with decay, and the\ndiffusivity coefficient to be a second-order symmetric and positive definite\ntensor. It is well-known that this particular equation is a second-order\nelliptic equation, and satisfies a maximum principle under certain regularity\nassumptions. However, the finite element implementation of the classical\nGalerkin formulation for both anisotropic and isotropic diffusion with decay\ndoes not respect the maximum principle.\n  We first show that the numerical accuracy of the classical Galerkin\nformulation deteriorates dramatically with increase in the decay coefficient\nfor isotropic medium and violates the discrete maximum principle. However, in\nthe case of isotropic medium, the extent of violation decreases with mesh\nrefinement. We then show that, in the case of anisotropic medium, the classical\nGalerkin formulation for anisotropic diffusion with decay violates the discrete\nmaximum principle even at lower values of decay coefficient and does not vanish\nwith mesh refinement. We then present a methodology for enforcing maximum\nprinciples under the classical Galerkin formulation for anisotropic diffusion\nwith decay on general computational grids using optimization techniques.\nRepresentative numerical results (which take into account anisotropy and\nheterogeneity) are presented to illustrate the performance of the proposed\nformulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.5257v3"
    },
    {
        "title": "VAGO method for the solution of elliptic second-order boundary value\n  problems",
        "authors": [
            "Nikolay P. Vabishchevich",
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Mathematical physics problems are often formulated using differential\noprators of vector analysis - invariant operators of first order, namely,\ndivergence, gradient and rotor operators. In approximate solution of such\nproblems it is natural to employ similar operator formulations for grid\nproblems, too. The VAGO (Vector Analysis Grid Operators) method is based on\nsuch a methodology. In this paper the vector analysis difference operators are\nconstructed using the Delaunay triangulation and the Voronoi diagrams. Further\nthe VAGO method is used to solve approximately boundary value problems for the\ngeneral elliptic equation of second order. In the convection-diffusion-reaction\nequation the diffusion coefficient is a symmetric tensor of second order.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4769v1"
    },
    {
        "title": "Effective Resistances, Statistical Leverage, and Applications to Linear\n  Equation Solving",
        "authors": [
            "Petros Drineas",
            "Michael W. Mahoney"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Recent work in theoretical computer science and scientific computing has\nfocused on nearly-linear-time algorithms for solving systems of linear\nequations. While introducing several novel theoretical perspectives, this work\nhas yet to lead to practical algorithms. In an effort to bridge this gap, we\ndescribe in this paper two related results. Our first and main result is a\nsimple algorithm to approximate the solution to a set of linear equations\ndefined by a Laplacian (for a graph $G$ with $n$ nodes and $m \\le n^2$ edges)\nconstraint matrix. The algorithm is a non-recursive algorithm; even though it\nruns in $O(n^2 \\cdot \\polylog(n))$ time rather than $O(m \\cdot polylog(n))$\ntime (given an oracle for the so-called statistical leverage scores), it is\nextremely simple; and it can be used to compute an approximate solution with a\ndirect solver. In light of this result, our second result is a straightforward\nconnection between the concept of graph resistance (which has proven useful in\nrecent algorithms for linear equation solvers) and the concept of statistical\nleverage (which has proven useful in numerically-implementable randomized\nalgorithms for large matrix problems and which has a natural data-analytic\ninterpretation).\n",
        "pdf_link": "http://arxiv.org/pdf/1005.3097v1"
    },
    {
        "title": "Experimental Comparisons of Derivative Free Optimization Algorithms",
        "authors": [
            "Anne Auger",
            "Nikolaus Hansen",
            "Jorge M. Perez Zerpa",
            "Raymond Ros",
            "Marc Schoenauer"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  In this paper, the performances of the quasi-Newton BFGS algorithm, the\nNEWUOA derivative free optimizer, the Covariance Matrix Adaptation Evolution\nStrategy (CMA-ES), the Differential Evolution (DE) algorithm and Particle Swarm\nOptimizers (PSO) are compared experimentally on benchmark functions reflecting\nimportant challenges encountered in real-world optimization problems.\nDependence of the performances in the conditioning of the problem and\nrotational invariance of the algorithms are in particular investigated.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.5631v1"
    },
    {
        "title": "SM stability for time-dependent problems",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Various classes of stable finite difference schemes can be constructed to\nobtain a numerical solution. It is important to select among all stable schemes\nsuch a scheme that is optimal in terms of certain additional criteria. In this\nstudy, we use a simple boundary value problem for a one-dimensional parabolic\nequation to discuss the selection of an approximation with respect to time. We\nconsider the pure diffusion equation, the pure convective transport equation\nand combined convection-diffusion phenomena. Requirements for the\nunconditionally stable finite difference schemes are formulated that are\nrelated to retaining the main features of the differential problem. The concept\nof SM stable finite difference scheme is introduced. The starting point are\ndifference schemes constructed on the basis of the various Pad$\\acute{e}$\napproximations.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.5712v1"
    },
    {
        "title": "Piecewise Convex-Concave Approximation in the $\\ell_{\\infty}$ Norm",
        "authors": [
            "M. P. Cullinan"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Suppose that $\\ff \\in \\reals^{n}$ is a vector of $n$ error-contaminated\nmeasurements of $n$ smooth values measured at distinct and strictly ascending\nabscissae. The following projective technique is proposed for obtaining a\nvector of smooth approximations to these values. Find \\yy\\ minimizing $\\| \\yy -\n\\ff \\|_{\\infty}$ subject to the constraints that the second order consecutive\ndivided differences of the components of \\yy\\ change sign at most $q$ times.\nThis optimization problem (which is also of general geometrical interest) does\nnot suffer from the disadvantage of the existence of purely local minima and\nallows a solution to be constructed in $O(nq)$ operations. A new algorithm for\ndoing this is developed and its effectiveness is proved. Some of the results of\napplying it to undulating and peaky data are presented, showing that it is\neconomical and can give very good results, particularly for large\ndensely-packed data, even when the errors are quite large.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.4518v1"
    },
    {
        "title": "Bootstrap Markov chain Monte Carlo and optimal solutions for the Law of\n  Categorical Judgment (Corrected)",
        "authors": [
            "Greg Kochanski",
            "Burton S. Rosner"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  A novel procedure is described for accelerating the convergence of Markov\nchain Monte Carlo computations. The algorithm uses an adaptive bootstrap\ntechnique to generate candidate steps in the Markov Chain. It is efficient for\nsymmetric, convex probability distributions, similar to multivariate Gaussians,\nand it can be used for Bayesian estimation or for obtaining maximum likelihood\nsolutions with confidence limits. As a test case, the Law of Categorical\nJudgment (Corrected) was fitted with the algorithm to data sets from simulated\nrating scale experiments. The correct parameters were recovered from\npractical-sized data sets simulated for Full Signal Detection Theory and its\nspecial cases of standard Signal Detection Theory and Complementary Signal\nDetection Theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1596v1"
    },
    {
        "title": "Splitting schemes for hyperbolic heat conduction equation",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Rapid processes of heat transfer are not described by the standard heat\nconduction equation. To take into account a finite velocity of heat transfer,\nwe use the hyperbolic model of heat conduction, which is connected with the\nrelaxation of heat fluxes. In this case, the mathematical model is based on a\nhyperbolic equation of second order or a system of equations for the\ntemperature and heat fluxes. In this paper we construct for the hyperbolic heat\nconduction equation the additive schemes of splitting with respect to\ndirections. Unconditional stability of locally one-dimensional splitting\nschemes is established. New splitting schemes are proposed and studied for a\nsystem of equations written in terms of the temperature and heat fluxes.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.2412v1"
    },
    {
        "title": "A Stable Explicit Scheme for Solving Non-Homogeneous Constant\n  Coefficients Equation using Green's Function",
        "authors": [
            "Hiroshi Abe"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  A numerical explicit method to evaluates transient solutions of linear\npartial differential non-homogeneous equation with constant coefficients is\nproposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.1599v2"
    },
    {
        "title": "A Stable Explicit Scheme for Solving Inhomogeneous Constant Coefficients\n  Differential Equation using Green's Function",
        "authors": [
            "Hiroshi Abe"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  A numerical explicit method to evaluates transient solutions of linear\npartial differential inhomogeneous equation with constant coefficients is\nproposed. A general form of the scheme for a specific linear inhomogeneous\nequation is shown. The method is applied to the wave equation and the diffuse\nequation and is investigated by simulating simple models. The numerical\nsolutions of the proposed method show good agreement to the exact solutions.\nComparing with explicit FDM, FDM shows the instability by the violation of CFL\ncondition whereas the proposed method is always stable irrespective of any time\nstep width.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.2531v1"
    },
    {
        "title": "A New Algorithm for General Cyclic Heptadiagonal Linear Systems Using\n  Sherman-Morrison-Woodbury formula",
        "authors": [
            "A. A. Karawia"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  In this paper, a new efficient computational algorithm is presented for\nsolving cyclic heptadiagonal linear systems based on using of heptadiagonal\nlinear solver and Sherman-Morrison-Woodbury formula. The implementation of the\nalgorithm using computer algebra systems (CAS) such as MAPLE and MATLAB is\nstraightforward. Numerical example is presented for the sake of illustration.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.4580v1"
    },
    {
        "title": "A preconditioned iterative solver for the scattering solutions of the\n  Schrödinger equation",
        "authors": [
            "Hisham bin Zubair",
            "Bram Reps",
            "Wim Vanroose"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  The Schr\\\"odinger equation defines the dynamics of quantum particles which\nhas been an area of unabated interest in physics. We demonstrate how simple\ntransformations of the Schr\\\"odinger equation leads to a coupled linear system,\nwhereby each diagonal block is a high frequency Helmholtz problem. Based on\nthis model, we derive indefinite Helmholtz model problems with strongly varying\nwavenumbers. We employ the iterative approach for their solution. In\nparticular, we develop a preconditioner that has its spectrum restricted to a\nquadrant (of the complex plane) thereby making it easily invertible by\nmultigrid methods with standard components. This multigrid preconditioner is\nused in conjuction with suitable Krylov-subspace methods for solving the\nindefinite Helmholtz model problems. The aim of this study is to report the\nfeasbility of this preconditioner for the model problems. We compare this idea\nwith the other prevalent preconditioning ideas, and discuss its merits. Results\nof numerical experiments are presented, which complement the proposed ideas,\nand show that this preconditioner may be used in an automatic setting.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.4307v2"
    },
    {
        "title": "GMRES-based multigrid for the complex scaled preconditoner for the\n  indefinite Helmholtz equation",
        "authors": [
            "Bram Reps",
            "Wim Vanroose",
            "Hisham bin Zubair"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Multigrid preconditioners and solvers for the indefinite Helmholtz equation\nsuffer from non-stability of the stationary smoothers due to the indefinite\nspectrum of the operator. In this paper we explore GMRES as a replacement for\nthe stationary smoothers of the standard multigrid method. This results in a\nrobust and efficient solver for a complex shifted or stretched Helmholtz\nproblem that can be used as a preconditioner. Very few GMRES iterations are\nrequired on each level to build a good multigrid method. The convergence\nbehavior is compared to a theoretically derived stable polynomial smoother. We\ntest this method on some benchmark problems and report on the observed\nconvergence behavior.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.5379v2"
    },
    {
        "title": "A contribution to the conditioning of the total least squares problem",
        "authors": [
            "Marc Baboulin",
            "Serge Gratton"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  We derive closed formulas for the condition number of a linear function of\nthe total least squares solution. Given an over determined linear system Ax=b,\nwe show that this condition number can be computed using the singular values\nand the right singular vectors of [A,b] and A. We also provide an upper bound\nthat requires the computation of the largest and the smallest singular value of\n[A,b] and the smallest singular value of A. In numerical examples, we compare\nthese values and the resulting forward error bounds with existing error\nestimates.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.5484v1"
    },
    {
        "title": "The power and Arnoldi methods in an algebra of circulants",
        "authors": [
            "David F. Gleich",
            "Chen Greif",
            "James M. Varah"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Circulant matrices play a central role in a recently proposed formulation of\nthree-way data computations. In this setting, a three-way table corresponds to\na matrix where each \"scalar\" is a vector of parameters defining a circulant.\nThis interpretation provides many generalizations of results from matrix or\nvector-space algebra. We derive the power and Arnoldi methods in this algebra.\nIn the course of our derivation, we define inner products, norms, and other\nnotions. These extensions are straightforward in an algebraic sense, but the\nimplications are dramatically different from the standard matrix case. For\nexample, a matrix of circulants has a polynomial number of eigenvalues in its\ndimension; although, these can all be represented by a carefully chosen\ncanonical set of eigenvalues and vectors. These results and algorithms are\nclosely related to standard decoupling techniques on block-circulant matrices\nusing the fast Fourier transform.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.2173v1"
    },
    {
        "title": "Domain decomposition schemes for evolutionary equations of first order\n  with not self-adjoint operators",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Domain decomposition methods are essential in solving applied problems on\nparallel computer systems. For boundary value problems for evolutionary\nequations the implicit schemes are in common use to solve problems at a new\ntime level employing iterative methods of domain decomposition. An alternative\napproach is based on constructing iteration-free methods based on special\nschemes of splitting into subdomains. Such regionally-additive schemes are\nconstructed using the general theory of additive operator-difference schemes.\nThere are employed the analogues of classical schemes of alternating direction\nmethod, locally one-dimensional schemes, factorization methods, vector and\nregularized additive schemes. The main results were obtained here for\ntime-dependent problems with self-adjoint elliptic operators of second order.\n  The paper discusses the Cauchy problem for the first order evolutionary\nequations with a nonnegative not self-adjoint operator in a finite-dimensional\nHilbert space. Based on the partition of unit, we have constructed the\noperators of decomposition which preserve nonnegativity for the individual\noperator terms of splitting. Unconditionally stable additive schemes of domain\ndecomposition were constructed using the regularization principle for\noperator-difference schemes. Vector additive schemes were considered, too. The\nresults of our work are illustrated by a model problem for the two-dimensional\nparabolic equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.2395v1"
    },
    {
        "title": "Domain decomposition schemes for the Stokes equation",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Numerical algorithms for solving problems of mathematical physics on modern\nparallel computers employ various domain decomposition techniques. Domain\ndecomposition schemes are developed here to solve numerically initial/boundary\nvalue problems for the Stokes system of equations in the primitive variables\npressure-velocity. Unconditionally stable schemes of domain decomposition are\nbased on the partition of unit for a computational domain and the corresponding\nHilbert spaces of grid functions.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.0642v1"
    },
    {
        "title": "Rank Aggregation via Nuclear Norm Minimization",
        "authors": [
            "David F. Gleich",
            "Lek-Heng Lim"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  The process of rank aggregation is intimately intertwined with the structure\nof skew-symmetric matrices. We apply recent advances in the theory and\nalgorithms of matrix completion to skew-symmetric matrices. This combination of\nideas produces a new method for ranking a set of items. The essence of our idea\nis that a rank aggregation describes a partially filled skew-symmetric matrix.\nWe extend an algorithm for matrix completion to handle skew-symmetric data and\nuse that to extract ranks for each item. Our algorithm applies to both pairwise\ncomparison and rating data. Because it is based on matrix completion, it is\nrobust to both noise and incomplete data. We show a formal recovery result for\nthe noiseless case and present a detailed study of the algorithm on synthetic\ndata and Netflix ratings.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.4821v1"
    },
    {
        "title": "Generalized Filtering Decomposition",
        "authors": [
            "Laura Grigori",
            "Frédéric Nataf"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  This paper introduces a new preconditioning technique that is suitable for\nmatrices arising from the discretization of a system of PDEs on unstructured\ngrids. The preconditioner satisfies a so-called filtering property, which\nensures that the input matrix is identical with the preconditioner on a given\nfiltering vector. This vector is chosen to alleviate the effect of low\nfrequency modes on convergence and so decrease or eliminate the plateau which\nis often observed in the convergence of iterative methods. In particular, the\npaper presents a general approach that allows to ensure that the filtering\ncondition is satisfied in a matrix decomposition. The input matrix can have an\narbitrary sparse structure. Hence, it can be reordered using nested dissection,\nto allow a parallel computation of the preconditioner and of the iterative\nprocess.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.3026v1"
    },
    {
        "title": "Substructuring domain decomposition scheme for unsteady problems",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Domain decomposition methods are used for approximate solving boundary\nproblems for partial differential equations on parallel computing systems.\nSpecific features of unsteady problems are taken into account in the most\ncomplete way in iteration-free schemes of domain decomposition.\nRegionally-additive schemes are based on different classes of splitting\nschemes. In this paper we highlight a class of domain decomposition schemes\nwhich is based on the partition of the initial domain into subdomains with\ncommon boundary nodes. Using the partition of unit we have constructed and\nstudied unconditionally stable schemes of domain decomposition based on\ntwo-component splitting: the problem within subdomain and the problem at their\nboundaries. As an example there is considered the Cauchy problem for\nevolutionary equations of first and second order with non-negative self-adjoint\noperator in a finite Hilbert space. The theoretical consideration is\nsupplemented with numerical solving a model problem for the two-dimensional\nparabolic equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.3448v1"
    },
    {
        "title": "Equivalent Effect Function and Fast Intrinsic Mode Decomposition",
        "authors": [
            "Louis Yu Lu"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  The Equivalent Effect Function (EEF) is defined as having the identical\nintegral values on the control points of the original time series data; the EEF\ncan be obtained from the derivative of the spline function passing through the\nintegral values on the control points. By choosing control points with\ndifferent criteria, the EEF can be used to find the intrinsic mode\nfunction(IMF, fluctuation) and the residue (trend); to fit the curve of the\noriginal data function; and to take samples on original data with equivalent\neffect. As examples of application, results of trend and fluctuation on real\nstock historical data are calculated on different time scales. A new approach\nto extend the EEF to 2D intrinsic mode decomposition is introduced to resolve\nthe inter slice non continuity problem, some photo image decomposition examples\nare presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.4337v1"
    },
    {
        "title": "Development and Modelling of High-Efficiency Computing Structure for\n  Digital Signal Processing",
        "authors": [
            "Annapurna Sharma",
            "Hakimjon Zaynidinov",
            "Hoon Jae Lee"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  The paper is devoted to problem of spline approximation. A new method of\nnodes location for curves and surfaces computer construction by means of\nB-splines and results of simulink-modeling is presented. The advantages of this\npaper is that we comprise the basic spline with classical polynomials both on\naccuracy, as well as degree of paralleling calculations are also shown.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4189v1"
    },
    {
        "title": "Numerical Stability of Explicit Runge-Kutta Finite-Difference Schemes\n  for the Nonlinear Schrödinger Equation",
        "authors": [
            "Ronald M. Caplan",
            "Ricardo Carretero-González"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Linearized numerical stability bounds for solving the nonlinear\ntime-dependent Schr\\\"odinger equation (NLSE) using explicit finite-differencing\nare shown. The bounds are computed for the fourth-order Runge-Kutta scheme in\ntime and both second-order and fourth-order central differencing in space.\nResults are given for Dirichlet, modulus-squared Dirichlet, Laplacian-zero, and\nperiodic boundary conditions for one, two, and three dimensions. Our approach\nis to use standard Runge-Kutta linear stability theory, treating the\nnonlinearity of the NLSE as a constant. The required bounds on the eigenvalues\nof the scheme matrices are found analytically when possible, and otherwise\nestimated using the Gershgorin circle theorem.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4810v4"
    },
    {
        "title": "Image representation by blob and its application in CT reconstruction\n  from few projections",
        "authors": [
            "Han Wang",
            "Laurent Desbat",
            "Samuel Legoupil"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  The localized radial symmetric function, or blob, is an ideal alternative to\nthe pixel basis for X-ray computed tomography (CT) image reconstruction. In\nthis paper we develop image representation models using blob, and propose\nreconstruction methods for few projections data. The image is represented in a\nshift invariant space generated by a Gaussian blob or a multiscale blob system\nof different frequency selectivity, and the reconstruction is done through\nminimizing the Total Variation or the 1 norm of blob coefficients. Some 2D\nnumerical results are presented, where we use GPU platform for accelerating the\nX-ray projection and back-projection, the interpolation and the gradient\ncomputations.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.5087v2"
    },
    {
        "title": "Iterative methods for solving the pressure problem at multiphase\n  filtration",
        "authors": [
            "P. Vabishchevich",
            "M. Vasil'eva"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Applied problems of oil and gas recovery are studied numerically using the\nmathematical models of multiphase fluid flows in porous media. The basic model\nincludes the continuity equations and the Darcy laws for each phase, as well as\nthe algebraic expression for the sum of saturations. Primary computational\nalgorithms are implemented for such problems using the pressure equation. In\nthis paper, we highlight the basic properties of the pressure problem and\ndiscuss the necessity of their fulfillment at the discrete level. The resulting\nelliptic problem for the pressure equation is characterized by a\nnon-selfadjoint operator. Possibilities of approximate solving the elliptic\nproblem are considered using the iterative methods. Special attention is given\nto the numerical algorithms for calculating the pressure on parallel computers.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.5479v1"
    },
    {
        "title": "Image Deblurring Using Derivative Compressed Sensing for Optical Imaging\n  Application",
        "authors": [
            "Mohammad Rostami",
            "Oleg Michailovich",
            "Zhou Wang"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Reconstruction of multidimensional signals from the samples of their partial\nderivatives is known to be a standard problem in inverse theory. Such and\nsimilar problems routinely arise in numerous areas of applied sciences,\nincluding optical imaging, laser interferometry, computer vision, remote\nsensing and control. Though being ill-posed in nature, the above problem can be\nsolved in a unique and stable manner, provided proper regularization and\nrelevant boundary conditions. In this paper, however, a more challenging setup\nis addressed, in which one has to recover an image of interest from its noisy\nand blurry version, while the only information available about the imaging\nsystem at hand is the amplitude of the generalized pupil function (GPF) along\nwith partial observations of the gradient of GPF's phase. In this case, the\nphase-related information is collected using a simplified version of the\nShack-Hartmann interferometer, followed by recovering the entire phase by means\nof derivative compressed sensing. Subsequently, the estimated phase can be\ncombined with the amplitude of the GPF to produce an estimate of the point\nspread function (PSF), whose knowledge is essential for subsequent image\ndeconvolution. In summary, the principal contribution of this work is twofold.\nFirst, we demonstrate how to simplify the construction of the Shack-Hartmann\ninterferometer so as to make it less expensive and hence more accessible.\nSecond, it is shown by means of numerical experiments that the above\nsimplification and its associated solution scheme produce image reconstructions\nof the quality comparable to those obtained using dense sampling of the GPF\nphase.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.5790v1"
    },
    {
        "title": "Banded Householder representation of linear subspaces",
        "authors": [
            "Geoffrey Irving"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  We show how to compactly represent any $n$-dimensional subspace of $R^m$ as a\nbanded product of Householder reflections using $n(m - n)$ floating point\nnumbers. This is optimal since these subspaces form a Grassmannian space\n$Gr_n(m)$ of dimension $n(m - n)$. The representation is stable and easy to\ncompute: any matrix can be factored into the product of a banded Householder\nmatrix and a square matrix using two to three QR decompositions.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5822v3"
    },
    {
        "title": "A Heuristic Description of Fast Fourier Transform",
        "authors": [
            "Zhengjun Cao",
            "Xiao Fan"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Fast Fourier Transform (FFT) is an efficient algorithm to compute the\nDiscrete Fourier Transform (DFT) and its inverse. In this paper, we pay special\nattention to the description of complex-data FFT. We analyze two common\ndescriptions of FFT and propose a new presentation. Our heuristic description\nis helpful for students and programmers to grasp the algorithm entirely and\ndeeply.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5989v1"
    },
    {
        "title": "The Numerical Generalized Least-Squares Estimator of an Unknown Constant\n  Mean of Random Field",
        "authors": [
            "Tomasz Suslo"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  We constraint on computer the best linear unbiased generalized statistics of\nrandom field for the best linear unbiased generalized statistics of an unknown\nconstant mean of random field and derive the numerical generalized\nleast-squares estimator of an unknown constant mean of random field. We derive\nthe third constraint of spatial statistics and show that the classic\ngeneralized least-squares estimator of an unknown constant mean of the field is\nonly an asymptotic disjunction of the numerical one.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.3971v1"
    },
    {
        "title": "Additive schemes (splitting schemes) for some systems of evolutionary\n  equations",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  On the basis of additive schemes (splitting schemes) we construct efficient\nnumerical algorithms to solve approximately the initial-boundary value problems\nfor systems of time-dependent partial differential equations (PDEs). In many\napplied problems the individual components of the vector of unknowns are\ncoupled together and then splitting schemes are applied in order to get a\nsimple problem for evaluating components at a new time level. Typically, the\nadditive operator-difference schemes for systems of evolutionary equations are\nconstructed for operators coupled in space. In this paper we investigate more\ngeneral problems where coupling of derivatives in time for components of the\nsolution vector takes place. Splitting schemes are developed using an additive\nrepresentation for both the primary operator of the problem and the operator at\nthe time derivative. Splitting schemes are based on a triangular two-component\nrepresentation of the operators.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.1294v1"
    },
    {
        "title": "Data-Driven Execution of Fast Multipole Methods",
        "authors": [
            "Hatem Ltaief",
            "Rio Yokota"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Fast multipole methods have O(N) complexity, are compute bound, and require\nvery little synchronization, which makes them a favorable algorithm on\nnext-generation supercomputers. Their most common application is to accelerate\nN-body problems, but they can also be used to solve boundary integral\nequations. When the particle distribution is irregular and the tree structure\nis adaptive, load-balancing becomes a non-trivial question. A common strategy\nfor load-balancing FMMs is to use the work load from the previous step as\nweights to statically repartition the next step. The authors discuss in the\npaper another approach based on data-driven execution to efficiently tackle\nthis challenging load-balancing problem. The core idea consists of breaking the\nmost time-consuming stages of the FMMs into smaller tasks. The algorithm can\nthen be represented as a Directed Acyclic Graph (DAG) where nodes represent\ntasks, and edges represent dependencies among them. The execution of the\nalgorithm is performed by asynchronously scheduling the tasks using the QUARK\nruntime environment, in a way such that data dependencies are not violated for\nnumerical correctness purposes. This asynchronous scheduling results in an\nout-of-order execution. The performance results of the data-driven FMM\nexecution outperform the previous strategy and show linear speedup on a\nquad-socket quad-core Intel Xeon system.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.0889v1"
    },
    {
        "title": "Analyzing and enhancing OSKI for sparse matrix-vector multiplication",
        "authors": [
            "Kadir Akbudak",
            "Enver Kayaaslan",
            "Cevdet Aykanat"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Sparse matrix-vector multiplication (SpMxV) is a kernel operation widely used\nin iterative linear solvers. The same sparse matrix is multiplied by a dense\nvector repeatedly in these solvers. Matrices with irregular sparsity patterns\nmake it difficult to utilize cache locality effectively in SpMxV computations.\nIn this work, we investigate single- and multiple-SpMxV frameworks for\nexploiting cache locality in SpMxV computations. For the single-SpMxV\nframework, we propose two cache-size-aware top-down row/column-reordering\nmethods based on 1D and 2D sparse matrix partitioning by utilizing the\ncolumn-net and enhancing the row-column-net hypergraph models of sparse\nmatrices. The multiple-SpMxV framework depends on splitting a given matrix into\na sum of multiple nonzero-disjoint matrices so that the SpMxV operation is\nperformed as a sequence of multiple input- and output-dependent SpMxV\noperations. For an effective matrix splitting required in this framework, we\npropose a cache-size-aware top-down approach based on 2D sparse matrix\npartitioning by utilizing the row-column-net hypergraph model. The primary\nobjective in all of the three methods is to maximize the exploitation of\ntemporal locality. We evaluate the validity of our models and methods on a wide\nrange of sparse matrices by performing actual runs through using OSKI.\nExperimental results show that proposed methods and models outperform\nstate-of-the-art schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.2739v1"
    },
    {
        "title": "Matrix Recipes for Hard Thresholding Methods",
        "authors": [
            "Anastasios Kyrillidis",
            "Volkan Cevher"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this paper, we present and analyze a new set of low-rank recovery\nalgorithms for linear inverse problems within the class of hard thresholding\nmethods. We provide strategies on how to set up these algorithms via basic\ningredients for different configurations to achieve complexity vs. accuracy\ntradeoffs. Moreover, we study acceleration schemes via memory-based techniques\nand randomized, $\\epsilon$-approximate matrix projections to decrease the\ncomputational costs in the recovery process. For most of the configurations, we\npresent theoretical analysis that guarantees convergence under mild problem\nconditions. Simulation results demonstrate notable performance improvements as\ncompared to state-of-the-art algorithms both in terms of reconstruction\naccuracy and computational complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.4481v2"
    },
    {
        "title": "The Correct Classic Generalized Least-Squares Estimator of an Unknown\n  Constant Mean of Randon Field",
        "authors": [
            "Tomasz Suslo"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The aim of the paper is to derive for the negative correlation function with\na time parameter an asymptotic disjunction of the numerical generalized\nleast-squares estimator of an unknown constant mean of random field in fact the\ncorrect classic generalized least-squares estimator of an unknown constant mean\nof the field.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.3597v1"
    },
    {
        "title": "A numerical methodology for enforcing maximum principles and the\n  non-negative constraint for transient diffusion equations",
        "authors": [
            "K. B. Nakshatrala",
            "H. Nagarajan",
            "M. Shabouei"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Transient diffusion equations arise in many branches of engineering and\napplied sciences (e.g., heat transfer and mass transfer), and are parabolic\npartial differential equations. It is well-known that, under certain\nassumptions on the input data, these equations satisfy important mathematical\nproperties like maximum principles and the non-negative constraint, which have\nimplications in mathematical modeling. However, existing numerical formulations\nfor these types of equations do not, in general, satisfy maximum principles and\nthe non-negative constraint. In this paper, we present a methodology for\nenforcing maximum principles and the non-negative constraint for transient\nanisotropic diffusion equation. The method of horizontal lines (also known as\nthe Rothe method) is applied in which the time is discretized first. This\nresults in solving steady anisotropic diffusion equation with decay equation at\nevery discrete time level. The proposed methodology for transient anisotropic\ndiffusion equation will satisfy maximum principles and the non-negative\nconstraint on general computational grids, and with no additional restrictions\non the time step. We illustrate the performance and accuracy of the proposed\nformulation using representative numerical examples. We also perform numerical\nconvergence of the proposed methodology. For comparison, we also present the\nresults from the standard single-field semi-discrete formulation and the\nresults from a popular software package, which all will violate maximum\nprinciples and the non-negative constraint.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.0701v3"
    },
    {
        "title": "A hybrid Hermitian general eigenvalue solver",
        "authors": [
            "Raffaele Solcà",
            "Thomas C. Schulthess",
            "Azzam Haidar",
            "Stanimire Tomov",
            "Ichitaro Yamazaki",
            "Jack Dongarra"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The adoption of hybrid GPU-CPU nodes in traditional supercomputing platforms\nopens acceleration opportunities for electronic structure calculations in\nmaterials science and chemistry applications, where medium sized Hermitian\ngeneralized eigenvalue problems must be solved many times. The small size of\nthe problems limits the scalability on a distributed memory system, hence they\ncan benefit from the massive computational performance concentrated on a single\nnode, hybrid GPU-CPU system. However, new algorithms that efficiently exploit\nheterogeneity and massive parallelism of not just GPUs, but of multi/many-core\nCPUs as well are required. Addressing these demands, we implemented a novel\nHermitian general eigensolver algorithm. This algorithm is based on a standard\neigenvalue solver, and existing algorithms can be used. The resulting\neigensolvers are state-of-the-art in HPC, significantly outperforming existing\nlibraries. We analyze their performance impact on applications of interest,\nwhen different fractions of eigenvectors are needed by the host electronic\nstructure code.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.1773v1"
    },
    {
        "title": "L1 Splines for Robust, Simple, and Fast Smoothing of Grid Data",
        "authors": [
            "Mariano Tepper",
            "Guillermo Sapiro"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Splines are a popular and attractive way of smoothing noisy data. Computing\nsplines involves minimizing a functional which is a linear combination of a\nfitting term and a regularization term. The former is classically computed\nusing a (weighted) L2 norm while the latter ensures smoothness. Thus, when\ndealing with grid data, the optimization can be solved very efficiently using\nthe DCT. In this work we propose to replace the L2 norm in the fitting term\nwith an L1 norm, leading to automatic robustness to outliers. To solve the\nresulting minimization problem we propose an extremely simple and efficient\nnumerical scheme based on split-Bregman iteration combined with DCT.\nExperimental validation shows the high-quality results obtained in short\nprocessing times.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.2292v2"
    },
    {
        "title": "LU factorization with panel rank revealing pivoting and its\n  communication avoiding version",
        "authors": [
            "Amal Khabou",
            "James W. Demmel",
            "Laura Grigori",
            "Ming Gu"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  We present the LU decomposition with panel rank revealing pivoting (LU_PRRP),\nan LU factorization algorithm based on strong rank revealing QR panel\nfactorization. LU_PRRP is more stable than Gaussian elimination with partial\npivoting (GEPP). Our extensive numerical experiments show that the new\nfactorization scheme is as numerically stable as GEPP in practice, but it is\nmore resistant to pathological cases and easily solves the Wilkinson matrix and\nthe Foster matrix. We also present CALU_PRRP, a communication avoiding version\nof LU_PRRP that minimizes communication. CALU_PRRP is based on tournament\npivoting, with the selection of the pivots at each step of the tournament being\nperformed via strong rank revealing QR factorization. CALU_PRRP is more stable\nthan CALU, the communication avoiding version of GEPP. CALU_PRRP is also more\nstable in practice and is resistant to pathological cases on which GEPP and\nCALU fail.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.2451v1"
    },
    {
        "title": "Real root refinements for univariate polynomial equations",
        "authors": [
            "Ye Liang"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Real root finding of polynomial equations is a basic problem in computer\nalgebra. This task is usually divided into two parts: isolation and refinement.\nIn this paper, we propose two algorithms LZ1 and LZ2 to refine real roots of\nunivariate polynomial equations. Our algorithms combine Newton's method and the\nsecant method to bound the unique solution in an interval of a monotonic convex\nisolation (MCI) of a polynomial, and have quadratic and cubic convergence\nrates, respectively. To avoid the swell of coefficients and speed up the\ncomputation, we implement the two algorithms by using the floating-point\ninterval method in Maple15 with the package intpakX. Experiments show that our\nmethods are effective and much faster than the function RefineBox in the\nsoftware Maple15 on benchmark polynomials.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.4332v1"
    },
    {
        "title": "Calculation of orthant probabilities by the holonomic gradient method",
        "authors": [
            "Tamio Koyama",
            "Akimichi Takemura"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  We apply the holonomic gradient method (HGM) introduced by [9] to the\ncalculation of orthant probabilities of multivariate normal distribution. The\nholonomic gradient method applied to orthant probabilities is found to be a\nvariant of Plackett's recurrence relation ([14]). However an implementation of\nthe method yields recurrence relations more suitable for numerical computation\nthan Plackett's recurrence relation. We derive some theoretical results on the\nholonomic system for the orthant probabilities. These results show that\nmultivariate normal orthant probabilities possess some remarkable properties\nfrom the viewpoint of holonomic systems. Finally we show that numerical\nperformance of our method is comparable or superior compared to existing\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.6822v2"
    },
    {
        "title": "General Lower Bounds based on Computer Generated Higher Order Expansions",
        "authors": [
            "Martijn Leisink",
            "Hilbert Kappen"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this article we show the rough outline of a computer algorithm to generate\nlower bounds on the exponential function of (in principle) arbitrary precision.\nWe implemented this to generate all necessary analytic terms for the Boltzmann\nmachine partition function thus leading to lower bounds of any order. It turns\nout that the extra variational parameters can be optimized analytically. We\nshow that bounds upto nineth order are still reasonably calculable in practical\nsituations. The generated terms can also be used as extra correction terms\n(beyond TAP) in mean field expansions.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.0581v1"
    },
    {
        "title": "Zur iterativen Loesung von linearen Gleichungssystemen",
        "authors": [
            "Hubert Karl",
            "Sebstian Karl"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  It is well known that a fixed point iteration for solving a linear equation\nsystem converges if and only if the spectral radius of the iteration matrix is\nless than one. A method is presented which guarantees the Fixed Point, even if\nthis condition is not (\"spectral radius <1\") fulfilled and demonstrated through\ncalculation examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2481v2"
    },
    {
        "title": "A2ILU: Auto-accelerated ILU Preconditioner for Sparse Linear Systems",
        "authors": [
            "Yuichiro Miki",
            "Teruyoshi Washizawa"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The ILU-based preconditioning methods in previous work have their own\nparameters to improve their performances. Although the parameters may degrade\nthe performance, their determination is left to users. Thus, these previous\nmethods are not reliable in practical computer-aided engineering use. This\npaper proposes a novel ILU-based preconditioner called the auto-accelerated\nILU, or A2ILU. In order to improve the convergence, A2ILU introduces\nacceleration parameters which modify the ILU factorized preconditioning matrix.\nA$^2$ILU needs no more operations than the original ILU because the\nacceleration parameters are optimized automatically by A2ILU itself. Numerical\ntests reveal the performance of A2ILU is superior to previous ILU-based methods\nwith manually optimized parameters. The numerical tests also demonstrate the\nability to apply auto-acceleration to ILU-based methods to improve their\nperformances and robustness of parameter sensitivities.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.5412v2"
    },
    {
        "title": "On the $\\mathbb{F}_2$-linear relations of Mersenne Twister pseudorandom\n  number generators",
        "authors": [
            "Shin Harase"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Sequence generators obtained by linear recursions over the two-element field\n$\\mathbb{F}_2$, i.e., $\\mathbb{F}_2$-linear generators, are widely used as\npseudorandom number generators. For example, the Mersenne Twister MT19937 is\none of the most successful applications. An advantage of such generators is\nthat we can assess them quickly by using theoretical criteria, such as the\ndimension of equidistribution with $v$-bit accuracy. To compute these\ndimensions, several polynomial-time lattice reduction algorithms have been\nproposed in the case of $\\mathbb{F}_2$-linear generators.\n  In this paper, in order to assess non-random bit patterns in dimensions that\nare higher than the dimension of equidistribution with $v$-bit accuracy,we\nfocus on the relationship between points in the Couture--L'Ecuyer dual lattices\nand $\\mathbb{F}_2$-linear relations on the most significant $v$ bits of output\nsequences, and consider a new figure of merit $N_v$ based on the minimum weight\nof $\\mathbb{F}_2$-linear relations whose degrees are minimal for $v$. Next, we\nnumerically show that MT19937 has low-weight $\\mathbb{F}_2$-linear relations in\ndimensions higher than 623, and show that some output vectors with specific\nlags are rejected or have small $p$-values in the birthday spacings tests. We\nalso report that some variants of Mersenne Twister, such as WELL generators,\nare significantly improved from the perspective of $N_v$.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.5435v3"
    },
    {
        "title": "An Efficient Implementation of the Ensemble Kalman Filter Based on an\n  Iterative Sherman-Morrison Formula",
        "authors": [
            "Elias D. Nino-Ruiz",
            "Adrian Sandu",
            "Jeffrey Anderson"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We present a practical implementation of the ensemble Kalman (EnKF) filter\nbased on an iterative Sherman-Morrison formula. The new direct method exploits\nthe special structure of the ensemble-estimated error covariance matrices in\norder to efficiently solve the linear systems involved in the analysis step of\nthe EnKF. The computational complexity of the proposed implementation is\nequivalent to that of the best EnKF implementations available in the literature\nwhen the number of observations is much larger than the number of ensemble\nmembers. Even when this conditions is not fulfilled, the proposed method is\nexpected to perform well since it does not employ matrix decompositions.\nComputational experiments using the Lorenz 96 and the oceanic quasi-geostrophic\nmodels are performed in order to compare the proposed algorithm with EnKF\nimplementations that use matrix decompositions. In terms of accuracy, the\nresults of all implementations are similar. The proposed method is considerably\nfaster than other EnKF variants, even when the number of observations is large\nrelative to the number of ensemble members.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.3876v2"
    },
    {
        "title": "Relative error due to a single bit-flip in floating-point arithmetic",
        "authors": [
            "Bradley R. Lowery"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We consider the error due to a single bit-flip in a floating point number. We\nassume IEEE 754 double precision arithmetic, which encodes binary floating\npoint numbers in a 64-bit word. We assume that the bit-flip happens randomly so\nit has equi-probability (1/64) to hit any of the 64 bits. Since we want to\nmitigate the assumption on our initial floating-point number, we assume that it\nis uniformly picked among all normalized number. With this framework, we can\nsummarize our findings as follows. The probability for a single bit flip to\ncause a relative error less than 10^-11 in a normalized floating-point number\nis above 25%; The probability for a single bit flip to cause a relative error\nless than 10^-6 in a normalized floating-point number is above 50%; Etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.4292v1"
    },
    {
        "title": "Computing Enclosures of Overdetermined Interval Linear Systems",
        "authors": [
            "Jaroslav Horáček",
            "Milan Hladík"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  This work considers special types of interval linear systems - overdetermined\nsystems. Simply said these systems have more equations than variables. The\nsolution set of an interval linear system is a collection of all solutions of\nall instances of an interval system. By the instance we mean a point real\nsystem that emerges when we independently choose a real number from each\ninterval coefficient of the interval system. Enclosing the solution set of\nthese systems is in some ways more difficult than for square systems. The main\ngoal of this work is to present various methods for solving overdetermined\ninterval linear systems. We would like to present them in an understandable way\neven for nonspecialists in a field of linear systems. The second goal is a\nnumerical comparison of all the methods on random interval linear systems\nregarding widths of enclosures, computation times and other special properties\nof methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.4738v1"
    },
    {
        "title": "Dominant Frequency Extraction",
        "authors": [
            "Rastislav Telgarsky"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Time series are collected and studied extensively for the knowledge about the\ndata source characteristics such as the trend or the spectral landscape. Some\npeaks in the spectral landscape correspond to dominant frequencies. The\napproach here is empirical: all time series are discrete and finite. Contents:\nIntroduction. 1 Examples of periodic phenomena. 2 Algorithms and libraries. 3\nTime series analysis. 4 Dominant frequency in ladar data. Conclusion.\nReferences.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.0103v1"
    },
    {
        "title": "Modification to Darcy model for high pressure and high velocity\n  applications and associated mixed finite element formulations",
        "authors": [
            "J. Chang",
            "K. B. Nakshatrala"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The Darcy model is based on a plethora of assumptions. One of the most\nimportant assumptions is that the Darcy model assumes the drag coefficient to\nbe constant. However, there is irrefutable experimental evidence that\nviscosities of organic liquids and carbon-dioxide depend on the pressure.\nExperiments have also shown that the drag varies nonlinearly with respect to\nthe velocity at high flow rates. In important technological applications like\nenhanced oil recovery and geological carbon-dioxide sequestration, one\nencounters both high pressures and high flow rates. It should be emphasized\nthat flow characteristics and pressure variation under varying drag are both\nquantitatively and qualitatively different from that of constant drag.\nMotivated by experimental evidence, we consider the drag coefficient to depend\non both the pressure and velocity. We consider two major modifications to the\nDarcy model based on the Barus formula and Forchheimer approximation. The\nproposed modifications to the Darcy model result in nonlinear partial\ndifferential equations, which are not amenable to analytical solutions. To this\nend, we present mixed finite element formulations based on least-squares\nformalism and variational multiscale formalism for the resulting governing\nequations. The proposed modifications to the Darcy model and its associated\nfinite element formulations are used to solve realistic problems with relevance\nto enhanced oil recovery. We also study the competition between the nonlinear\ndependence of drag on the velocity and the dependence of viscosity on the\npressure. To the best of the authors' knowledge such a systematic study has not\nbeen performed.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.5216v1"
    },
    {
        "title": "Efficient estimation of eigenvalue counts in an interval",
        "authors": [
            "Edoardo Di Napoli",
            "Eric Polizzi",
            "Yousef Saad"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Estimating the number of eigenvalues located in a given interval of a large\nsparse Hermitian matrix is an important problem in certain applications and it\nis a prerequisite of eigensolvers based on a divide-and-conquer paradigm. Often\nan exact count is not necessary and methods based on stochastic estimates can\nbe utilized to yield rough approximations. This paper examines a number of\ntechniques tailored to this specific task. It reviews standard approaches and\nexplores new ones based on polynomial and rational approximation filtering\ncombined with a stochastic procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.4275v2"
    },
    {
        "title": "Shape identification and classification in echolocation",
        "authors": [
            "Habib Ammari",
            "Minh Phuong Tran",
            "Han Wang"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The paper aims at proposing the first shape identification and classification\nalgorithm in echolocation. The approach is based on first extracting geometric\nfeatures from the reflected waves and then matching them with precomputed ones\nassociated with a dictionary of targets. The construction of such\nfrequency-dependent shape descriptors is based on some important properties of\nthe scattering coefficients and new invariants. The stability and resolution of\nthe proposed identification algorithm with respect to measurement noise and the\nlimited-view aspect are analytically and numerically quantified.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.5625v1"
    },
    {
        "title": "Generalized Perron--Frobenius Theorem for Nonsquare Matrices",
        "authors": [
            "Chen Avin",
            "Michael Borokhovich",
            "Yoram Haddad",
            "Erez Kantor",
            "Zvi Lotker",
            "Merav Parter",
            "David Peleg"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The celebrated Perron--Frobenius (PF) theorem is stated for irreducible\nnonnegative square matrices, and provides a simple characterization of their\neigenvectors and eigenvalues. The importance of this theorem stems from the\nfact that eigenvalue problems on such matrices arise in many fields of science\nand engineering, including dynamical systems theory, economics, statistics and\noptimization. However, many real-life scenarios give rise to nonsquare\nmatrices. A natural question is whether the PF Theorem (along with its\napplications) can be generalized to a nonsquare setting. Our paper provides a\ngeneralization of the PF Theorem to nonsquare matrices. The extension can be\ninterpreted as representing client-server systems with additional degrees of\nfreedom, where each client may choose between multiple servers that can\ncooperate in serving it (while potentially interfering with other clients).\nThis formulation is motivated by applications to power control in wireless\nnetworks, economics and others, all of which extend known examples for the use\nof the original PF Theorem.\n  We show that the option of cooperation between servers does not improve the\nsituation, in the sense that in the optimal solution no cooperation is needed,\nand only one server needs to serve each client. Hence, the additional power of\nhaving several potential servers per client translates into \\emph{choosing} the\nbest single server and not into \\emph{sharing} the load between the servers in\nsome way, as one might have expected.\n  The two main contributions of the paper are (i) a generalized PF Theorem that\ncharacterizes the optimal solution for a non-convex nonsquare problem, and (ii)\nan algorithm for finding the optimal solution in polynomial time.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.5915v1"
    },
    {
        "title": "A New Method for the Analysis of Signals: The Square Wave Transform\n  (SWT)",
        "authors": [
            "Osvaldo Skliar",
            "Ricardo E. Monge",
            "Guillermo Oviedo",
            "Sherry Gapper"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The results obtained by analyzing signals with the Square Wave Method (SWM)\nintroduced previously can be presented in the frequency domain clearly and\nprecisely by using the Square Wave Transform (SWT) described here. As an\nexample, the SWT is used to analyze a sequence of samples (that is, of measured\nvalues) taken from an electroencephalographic recording.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.3719v6"
    },
    {
        "title": "Exponential Integrators on Graphic Processing Units",
        "authors": [
            "Lukas Einkemmer",
            "Alexander Ostermann"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this paper we revisit stencil methods on GPUs in the context of\nexponential integrators. We further discuss boundary conditions, in the same\ncontext, and show that simple boundary conditions (for example, homogeneous\nDirichlet or homogeneous Neumann boundary conditions) do not affect the\nperformance if implemented directly into the CUDA kernel. In addition, we show\nthat stencil methods with position-dependent coefficients can be implemented\nefficiently as well.\n  As an application, we discuss the implementation of exponential integrators\nfor different classes of problems in a single and multi GPU setup (up to 4\nGPUs). We further show that for stencil based methods such parallelization can\nbe done very efficiently, while for some unstructured matrices the\nparallelization to multiple GPUs is severely limited by the throughput of the\nPCIe bus.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.4616v1"
    },
    {
        "title": "Pursuit Fractal Analysis of Time-Series Data",
        "authors": [
            "Satoshi Hasegawa",
            "Hajime Anada",
            "Shuya Kanagawa"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this study, we present a method to measure changes over time of fractal\ndimension. We confirmed that our method can calculate the fractal dimension\nwith the same precision as conventional methods, and tracking performance of\nour method is higher than that of the conventional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.3564v1"
    },
    {
        "title": "Explicit schemes for parabolic and hyperbolic equations",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Standard explicit schemes for parabolic equations are not very convenient for\ncomputing practice due to the fact that they have strong restrictions on a time\nstep. More promising explicit schemes are associated with explicit-implicit\nsplitting of the problem operator (Saul'yev asymmetric schemes, explicit\nalternating direction (ADE) schemes, group explicit method). These schemes\nbelong to the class of unconditionally stable schemes, but they demonstrate bad\napproximation properties. These explicit schemes are treated as schemes of the\nalternating triangle method and can be considered as factorized schemes where\nthe problem operator is splitted into the sum of two operators that are adjoint\nto each other. Here we propose a multilevel modification of the alternating\ntriangle method, which demonstrates better properties in terms of accuracy. We\nalso consider explicit schemes of the alternating triangle method for the\nnumerical solution of boundary value problems for hyperbolic equations of\nsecond order. The study is based on the general theory of stability\n(well-posedness) for operator-difference schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.4046v1"
    },
    {
        "title": "Linear Convergence of Comparison-based Step-size Adaptive Randomized\n  Search via Stability of Markov Chains",
        "authors": [
            "Anne Auger",
            "Nikolaus Hansen"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this paper, we consider comparison-based adaptive stochastic algorithms\nfor solving numerical optimisation problems. We consider a specific subclass of\nalgorithms that we call comparison-based step-size adaptive randomized search\n(CB-SARS), where the state variables at a given iteration are a vector of the\nsearch space and a positive parameter, the step-size, typically controlling the\noverall standard deviation of the underlying search distribution.We investigate\nthe linear convergence of CB-SARS on\\emph{scaling-invariant} objective\nfunctions. Scaling-invariantfunctions preserve the ordering of points with\nrespect to their functionvalue when the points are scaled with the same\npositive parameter (thescaling is done w.r.t. a fixed reference point). This\nclass offunctions includes norms composed with strictly increasing functions\naswell as many non quasi-convex and non-continuousfunctions. On\nscaling-invariant functions, we show the existence of ahomogeneous Markov\nchain, as a consequence of natural invarianceproperties of CB-SARS (essentially\nscale-invariance and invariance tostrictly increasing transformation of the\nobjective function). We thenderive sufficient conditions for \\emph{global\nlinear convergence} ofCB-SARS, expressed in terms of different stability\nconditions of thenormalised homogeneous Markov chain (irreducibility,\npositivity, Harrisrecurrence, geometric ergodicity) and thus define a general\nmethodologyfor proving global linear convergence of CB-SARS algorithms\nonscaling-invariant functions. As a by-product we provide aconnexion between\ncomparison-based adaptive stochasticalgorithms and Markov chain Monte Carlo\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.7697v6"
    },
    {
        "title": "Easy Accurate Reading and Writing of Floating-Point Numbers",
        "authors": [
            "Aubrey Jaffer"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Presented here are algorithms for converting between (decimal)\nscientific-notation and (binary) IEEE-754 double-precision floating-point\nnumbers. By employing a rounding integer quotient operation these algorithms\nare much simpler than those previously published. The values are stable under\nrepeated conversions between the formats. Unlike Java-1.8, the scientific\nrepresentations generated use only the minimum number of mantissa digits needed\nto convert back to the original binary values.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.8121v7"
    },
    {
        "title": "Linear Convergence on Positively Homogeneous Functions of a Comparison\n  Based Step-Size Adaptive Randomized Search: the (1+1) ES with Generalized\n  One-fifth Success Rule",
        "authors": [
            "Anne Auger",
            "Nikolaus Hansen"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In the context of unconstraint numerical optimization, this paper\ninvestigates the global linear convergence of a simple probabilistic\nderivative-free optimization algorithm (DFO). The algorithm samples a candidate\nsolution from a standard multivariate normal distribution scaled by a step-size\nand centered in the current solution. This solution is accepted if it has a\nbetter objective function value than the current one. Crucial to the algorithm\nis the adaptation of the step-size that is done in order to maintain a certain\nprobability of success. The algorithm, already proposed in the 60's, is a\ngeneralization of the well-known Rechenberg's $(1+1)$ Evolution Strategy (ES)\nwith one-fifth success rule which was also proposed by Devroye under the name\ncompound random search or by Schumer and Steiglitz under the name step-size\nadaptive random search. In addition to be derivative-free, the algorithm is\nfunction-value-free: it exploits the objective function only through\ncomparisons. It belongs to the class of comparison-based step-size adaptive\nrandomized search (CB-SARS). For the convergence analysis, we follow the\nmethodology developed in a companion paper for investigating linear convergence\nof CB-SARS: by exploiting invariance properties of the algorithm, we turn the\nstudy of global linear convergence on scaling-invariant functions into the\nstudy of the stability of an underlying normalized Markov chain (MC). We hence\nprove global linear convergence by studying the stability (irreducibility,\nrecurrence, positivity, geometric ergodicity) of the normalized MC associated\nto the $(1+1)$-ES. More precisely, we prove that starting from any initial\nsolution and any step-size, linear convergence with probability one and in\nexpectation occurs. Our proof holds on unimodal functions that are the\ncomposite of strictly increasing functions by positively homogeneous functions\nwith degree $\\alpha$ (assumed also to be continuously differentiable). This\nfunction class includes composite of norm functions but also non-quasi convex\nfunctions. Because of the composition by a strictly increasing function, it\nincludes non continuous functions. We find that a sufficient condition for\nglobal linear convergence is the step-size increase on linear functions, a\ncondition typically satisfied for standard parameter choices. While introduced\nmore than 40 years ago, we provide here the first proof of global linear\nconvergence for the $(1+1)$-ES with generalized one-fifth success rule and the\nfirst proof of linear convergence for a CB-SARS on such a class of functions\nthat includes non-quasi convex and non-continuous functions. Our proof also\nholds on functions where linear convergence of some CB-SARS was previously\nproven, namely convex-quadratic functions (including the well-know sphere\nfunction).\n",
        "pdf_link": "http://arxiv.org/pdf/1310.8397v1"
    },
    {
        "title": "An optimally concentrated Gabor transform for localized time-frequency\n  components",
        "authors": [
            "Benjamin Ricaud",
            "Guillaume Stempfel",
            "Bruno Torrésani",
            "Christoph Wiesmeyr",
            "Hélène Lachambre",
            "Darian Onchis"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Gabor analysis is one of the most common instances of time-frequency signal\nanalysis. Choosing a suitable window for the Gabor transform of a signal is\noften a challenge for practical applications, in particular in audio signal\nprocessing. Many time-frequency (TF) patterns of different shapes may be\npresent in a signal and they can not all be sparsely represented in the same\nspectrogram. We propose several algorithms, which provide optimal windows for a\nuser-selected TF pattern with respect to different concentration criteria. We\nbase our optimization algorithm on $l^p$-norms as measure of TF spreading. For\na given number of sampling points in the TF plane we also propose optimal\nlattices to be used with the obtained windows. We illustrate the potentiality\nof the method on selected numerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.8573v2"
    },
    {
        "title": "$L_{1/2}$ Regularization: Convergence of Iterative Half Thresholding\n  Algorithm",
        "authors": [
            "Jinshan Zeng",
            "Shaobo Lin",
            "Yao Wang",
            "Zongben Xu"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In recent studies on sparse modeling, the nonconvex regularization approaches\n(particularly, $L_{q}$ regularization with $q\\in(0,1)$) have been demonstrated\nto possess capability of gaining much benefit in sparsity-inducing and\nefficiency. As compared with the convex regularization approaches (say, $L_{1}$\nregularization), however, the convergence issue of the corresponding algorithms\nare more difficult to tackle. In this paper, we deal with this difficult issue\nfor a specific but typical nonconvex regularization scheme, the $L_{1/2}$\nregularization, which has been successfully used to many applications. More\nspecifically, we study the convergence of the iterative \\textit{half}\nthresholding algorithm (the \\textit{half} algorithm for short), one of the most\nefficient and important algorithms for solution to the $L_{1/2}$\nregularization. As the main result, we show that under certain conditions, the\n\\textit{half} algorithm converges to a local minimizer of the $L_{1/2}$\nregularization, with an eventually linear convergence rate. The established\nresult provides a theoretical guarantee for a wide range of applications of the\n\\textit{half} algorithm. We provide also a set of simulations to support the\ncorrectness of theoretical assertions and compare the time efficiency of the\n\\textit{half} algorithm with other known typical algorithms for $L_{1/2}$\nregularization like the iteratively reweighted least squares (IRLS) algorithm\nand the iteratively reweighted $l_{1}$ minimization (IRL1) algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.0156v3"
    },
    {
        "title": "An Approximate, Efficient Solver for LP Rounding",
        "authors": [
            "Srikrishna Sridhar",
            "Victor Bittorf",
            "Ji Liu",
            "Ce Zhang",
            "Christopher Ré",
            "Stephen J. Wright"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Many problems in machine learning can be solved by rounding the solution of\nan appropriate linear program (LP). This paper shows that we can recover\nsolutions of comparable quality by rounding an approximate LP solution instead\nof the ex- act one. These approximate LP solutions can be computed efficiently\nby applying a parallel stochastic-coordinate-descent method to a\nquadratic-penalty formulation of the LP. We derive worst-case runtime and\nsolution quality guarantees of this scheme using novel perturbation and\nconvergence analysis. Our experiments demonstrate that on such combinatorial\nproblems as vertex cover, independent set and multiway-cut, our approximate\nrounding scheme is up to an order of magnitude faster than Cplex (a commercial\nLP solver) while producing solutions of similar quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.2661v2"
    },
    {
        "title": "A fast directional BEM for large-scale acoustic problems based on the\n  Burton-Miller formulation",
        "authors": [
            "Yanchuang Cao",
            "Lihua Wen",
            "Jinyou Xiao",
            "Yijun Liu"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this paper, a highly efficient fast boundary element method (BEM) for\nsolving large-scale engineering acoustic problems in a broad frequency range is\ndeveloped and implemented. The acoustic problems are modeled by the\nBurton-Miller boundary integral equation (BIE), thus the fictitious frequency\nissue is completely avoided. The BIE is discretized by using the Nystr\\\"om\nmethod based on the curved quadratic elements, leading to simple numerical\nimplementation (no edge or corner problems) and high accuracy in the BEM\nanalysis. The linear systems are solved iteratively and accelerated by using a\nnewly developed kernel-independent wideband fast directional algorithm (FDA)\nfor fast summation of oscillatory kernels. In addition, the computational\nefficiency of the FDA is further promoted by exploiting the low-rank features\nof the translation matrices, resulting in two- to three-fold reduction in the\ncomputational time of the multipole-to-local translations. The high accuracy\nand nearly linear computational complexity of the present method are clearly\ndemonstrated by typical examples. An acoustic scattering problem with\ndimensionless wave number $kD$ (where $k$ is the wave number and $D$ is the\ntypical length of the obstacle) up to 1000 and the degrees of freedom up to 4\nmillion is successfully solved within 10 hours on a computer with one core and\nthe memory usage is 24 GB.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.5202v2"
    },
    {
        "title": "Randomness of D Sequences via Diehard Testing",
        "authors": [
            "James Bellamy"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  This paper presents a comparison of the quality of randomness of D sequences\nbased on diehard tests. Since D sequences can model any random sequence, this\ncomparison is of value beyond this specific class.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.3618v1"
    },
    {
        "title": "Insight into Primal Augmented Lagrangian Multilplier Method",
        "authors": [
            "B. Premjith",
            "S. Sachin Kumar",
            "Akhil Manikkoth",
            "T V Bijeesh",
            "K P Soman"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We provide a simplified form of Primal Augmented Lagrange Multiplier\nalgorithm. We intend to fill the gap in the steps involved in the mathematical\nderivations of the algorithm so that an insight into the algorithm is made. The\nexperiment is focused to show the reconstruction done using this algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.7637v1"
    },
    {
        "title": "Some isomorphic classes for noncanonical hypercomplex number systems of\n  dimension 2",
        "authors": [
            "Yakiv O. Kalinovsky",
            "Dmitry V. Lande",
            "Yuliya E. Boyarinova",
            "Iana V. Khitsko"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Building of some isomorphic classes for noncanonical hypercomplex number\nsystems o dimension 2 is described. In general case, such systems with specific\nconstraints to structural constants can be isomorphic to complex, dual or\ndouble number system. Isomorphic transition between noncanonical hypercomplex\nnumber systems of the general form and diagonal form is built.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.2273v1"
    },
    {
        "title": "Tensor Decompositions for Signal Processing Applications From Two-way to\n  Multiway Component Analysis",
        "authors": [
            "A. Cichocki",
            "D. Mandic",
            "A-H. Phan",
            "C. Caiafa",
            "G. Zhou",
            "Q. Zhao",
            "L. De Lathauwer"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  The widespread use of multi-sensor technology and the emergence of big\ndatasets has highlighted the limitations of standard flat-view matrix models\nand the necessity to move towards more versatile data analysis tools. We show\nthat higher-order tensors (i.e., multiway arrays) enable such a fundamental\nparadigm shift towards models that are essentially polynomial and whose\nuniqueness, unlike the matrix methods, is guaranteed under verymild and natural\nconditions. Benefiting fromthe power ofmultilinear algebra as theirmathematical\nbackbone, data analysis techniques using tensor decompositions are shown to\nhave great flexibility in the choice of constraints that match data properties,\nand to find more general latent components in the data than matrix-based\nmethods. A comprehensive introduction to tensor decompositions is provided from\na signal processing perspective, starting from the algebraic foundations, via\nbasic Canonical Polyadic and Tucker models, through to advanced cause-effect\nand multi-view data analysis schemes. We show that tensor decompositions enable\nnatural generalizations of some commonly used signal processing paradigms, such\nas canonical correlation and subspace techniques, signal separation, linear\nregression, feature extraction and classification. We also cover computational\naspects, and point out how ideas from compressed sensing and scientific\ncomputing may be used for addressing the otherwise unmanageable storage and\nmanipulation problems associated with big datasets. The concepts are supported\nby illustrative real world case studies illuminating the benefits of the tensor\nframework, as efficient and promising tools for modern signal processing, data\nanalysis and machine learning applications; these benefits also extend to\nvector/matrix data through tensorization. Keywords: ICA, NMF, CPD, Tucker\ndecomposition, HOSVD, tensor networks, Tensor Train.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.4462v1"
    },
    {
        "title": "A fast directional boundary element method for high frequency acoustic\n  problems in three dimensions",
        "authors": [
            "Yanchuang Cao",
            "Lihua Wen",
            "Jinyou Xiao"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  A highly efficient fast boundary element method (BEM) for solving large-scale\nengineering acoustic problems in a broad frequency range is developed and\nimplemented. The acoustic problems are modeled by the Burton-Miller boundary\nintegral equation (BIE), thus the fictitious frequency issue is completely\navoided. The BIE is discretized by using the collocation method with piecewise\nconstant elements. The linear systems are solved iteratively and accelerated by\nusing a newly developed kernel-independent wideband fast directional algorithm\n(FDA) for fast summation of oscillatory kernels. In addition, the computational\nefficiency of the FDA is further promoted by exploiting the low-rank features\nof the translation matrices. The high accuracy and nearly linear computational\ncomplexity of the present method are clearly demonstrated by typical examples.\nAn acoustic scattering problem with dimensionless wave number $kD$ (where $k$\nis the wave number and $D$ is the typical length of the obstacle) up to 1000\nand the degrees of freedom up to 4 million is successfully solved within 4\nhours on a computer with one core and the memory usage is 24.7 GB.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.4747v1"
    },
    {
        "title": "Solvers for $\\mathcal{O} (N)$ Electronic Structure in the Strong Scaling\n  Limit",
        "authors": [
            "Nicolas Bock",
            "Matt Challacombe",
            "Laxmikant V. Kalé"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  We present a hybrid OpenMP/Charm++ framework for solving the $\\mathcal{O}\n(N)$ Self-Consistent-Field eigenvalue problem with parallelism in the strong\nscaling regime, $P\\gg{N}$, where $P$ is the number of cores, and $N$ a measure\nof system size, i.e. the number of matrix rows/columns, basis functions, atoms,\nmolecules, etc. This result is achieved with a nested approach to Spectral\nProjection and the Sparse Approximate Matrix Multiply [Bock and Challacombe,\nSIAM J.~Sci.~Comput. 35 C72, 2013], and involves a recursive, task-parallel\nalgorithm, often employed by generalized $N$-Body solvers, to occlusion and\nculling of negligible products in the case of matrices with decay. Employing\nclassic technologies associated with generalized $N$-Body solvers, including\nover-decomposition, recursive task parallelism, orderings that preserve\nlocality, and persistence-based load balancing, we obtain scaling beyond\nhundreds of cores per molecule for small water clusters ([H${}_2$O]${}_N$, $N\n\\in \\{ 30, 90, 150 \\}$, $P/N \\approx \\{ 819, 273, 164 \\}$) and find support for\nan increasingly strong scalability with increasing system size $N$.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.7458v7"
    },
    {
        "title": "A Derivative-Free Trust Region Framework for Variational Data\n  Assimilation",
        "authors": [
            "Elias D. Nino",
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This study develops a hybrid ensemble-variational approach for solving data\nassimilation problems. The method, called TR-4D-EnKF, is based on a trust\nregion framework and consists of three computational steps. First an ensemble\nof model runs is propagated forward in time and snapshots of the state are\nstored. Next, a sequence of basis vectors is built and a low-dimensional\nrepresentation of the data assimilation system is obtained by projecting the\nmodel state onto the space spanned by the ensemble deviations from the mean.\nFinally, the low-dimensional optimization problem is solved in the\nreduced-space using a trust region approach; the size of the trust region is\nupdated according to the relative decrease of the reduced order surrogate cost\nfunction. The analysis state is projected back onto the full space, and the\nprocess is repeated with the current analysis serving as a new background. A\nheuristic approach based on the trust region size is proposed in order to\nadjust the background error statistics from one iteration to the next.\nExperimental simulations are carried out using the Lorenz and the\nquasi-geostrophic models. The results show that TR-4D-EnKF is an efficient\ncomputational approach, and is more accurate than the current state of the art\n4D-EnKF implementations such as the POD-4D-EnKF and the Iterative Subspace\nMinimization methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.7692v2"
    },
    {
        "title": "Recursive computation of spherical harmonic rotation coefficients of\n  large degree",
        "authors": [
            "Nail A. Gumerov",
            "Ramani Duraiswami"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Computation of the spherical harmonic rotation coefficients or elements of\nWigner's d-matrix is important in a number of quantum mechanics and\nmathematical physics applications. Particularly, this is important for the Fast\nMultipole Methods in three dimensions for the Helmholtz, Laplace and related\nequations, if rotation-based decomposition of translation operators are used.\nIn these and related problems related to representation of functions on a\nsphere via spherical harmonic expansions computation of the rotation\ncoefficients of large degree $n$ (of the order of thousands and more) may be\nnecessary. Existing algorithms for their computation, based on recursions, are\nusually unstable, and do not extend to $n$. We develop a new recursion and\nstudy its behavior for large degrees, via computational and asymptotic\nanalyses. Stability of this recursion was studied based on a novel application\nof the Courant-Friedrichs-Lewy condition and the von Neumann method for\nstability of finite-difference schemes for solution of PDEs. A recursive\nalgorithm of minimal complexity $O\\left(n^{2}\\right)$ for degree $n$ and\nFFT-based algorithms of complexity $O\\left(n^{2}\\log n\\right) $ suitable for\ncomputation of rotation coefficients of large degrees are proposed, studied\nnumerically, and cross-validated. It is shown that the latter algorithm can be\nused for $n\\lesssim 10^{3}$ in double precision, while the former algorithm was\ntested for large $n$ (up to $10^{4}$ in our experiments) and demonstrated\nbetter performance and accuracy compared to the FFT-based algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.7698v1"
    },
    {
        "title": "On the equivalence between low rank matrix completion and tensor rank",
        "authors": [
            "Harm Derksen"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  The Rank Minimization Problem asks to find a matrix of lowest rank inside a\nlinear variety of the space of n x n matrices. The Low Rank Matrix Completion\nproblem asks to complete a partially filled matrix such that the resulting\nmatrix has smallest possible rank.\n  The Tensor Rank Problem asks to determine the rank of a tensor. We show that\nthese three problems are equivalent: each one of the problems can be reduced to\nthe other two.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.0080v2"
    },
    {
        "title": "Consensus-based In-Network Computation of the PARAFAC Decomposition",
        "authors": [
            "Alain Y. Kibangou",
            "André L. F. de Almeida"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this work, we present a new approach for the distributed computation of\nthe PARAFAC decomposition of a third-order tensor across a network of\ncollaborating nodes. We are interested in the case where the overall data\ngathered across the network can be modeled as a data tensor admitting an\nessentially unique PARAFAC decomposition, while each node only observes a\nsub-tensor with not necessarily enough diversity so that identifiability\nconditions are not locally fulfilled at each node. In this situation,\nconventional (centralized) tensor based methods cannot be applied individually\nat each node. By allowing collaboration between neighboring nodes of the\nnetwork, we propose distributed versions of the alternating least squares (ALS)\nand Levenberg-Marquardt (LM) algorithms for the in-network estimation of the\nfactor matrices of a third-order tensor. We assume that one of the factor\nmatrices contains parameters that are local to each node, while the two\nremaining factor matrices contain global parameters that are common to the\nwhole network. The proposed algorithms combine the estimation of the local\nfactors with an in-network computation of the global factors of the PARAFAC\ndecomposition using average consensus over graphs. They emulate their\ncentralized counterparts in the case of ideal data exchange and ideal consensus\ncomputations. The performance of the proposed algorithms are evaluated in both\nideal and imperfect cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.1572v1"
    },
    {
        "title": "Clifford Type Algebra Characteristics Investigation",
        "authors": [
            "Yakiv O. Kalinovsky",
            "Dmitry V. Lande",
            "Dr. Sc.",
            "Yuliya E. Boyarinova",
            "Alina S. Turenko"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  The main properties of hypercomplex generalization of quaternion system as\nantiquaternion are presented in this article. Definitions and studied of\nantiquaternions conjugation are introduced, their norm and zero divisor, and\nhow to perform operations on them.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.3461v1"
    },
    {
        "title": "A benchmark generator for boolean quadratic programming",
        "authors": [
            "Xiaojun Zhou"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  For boolean quadratic programming (BQP), we will show that there is no\nduality gap between the primal and dual problems under some conditions by using\nthe classical Lagrangian duality. A benchmark generator is given to create\nrandom BQP problems which can be solved in polynomial time. Several numerical\nexamples are generated to demonstrate the effectiveness of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.4812v3"
    },
    {
        "title": "Preconditioned Krylov solvers for kernel regression",
        "authors": [
            "Balaji Vasan Srinivasan",
            "Qi Hu",
            "Nail A. Gumerov",
            "Raghu Murtugudde",
            "Ramani Duraiswami"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  A primary computational problem in kernel regression is solution of a dense\nlinear system with the $N\\times N$ kernel matrix. Because a direct solution has\nan O($N^3$) cost, iterative Krylov methods are often used with fast\nmatrix-vector products. For poorly conditioned problems, convergence of the\niteration is slow and preconditioning becomes necessary. We investigate\npreconditioning from the viewpoint of scalability and efficiency. The problems\nthat conventional preconditioners face when applied to kernel methods are\ndemonstrated. A \\emph{novel flexible preconditioner }that not only improves\nconvergence but also allows utilization of fast kernel matrix-vector products\nis introduced. The performance of this preconditioner is first illustrated on\nsynthetic data, and subsequently on a suite of test problems in kernel\nregression and geostatistical kriging.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.1237v1"
    },
    {
        "title": "A Field Guide to Forward-Backward Splitting with a FASTA Implementation",
        "authors": [
            "Tom Goldstein",
            "Christoph Studer",
            "Richard Baraniuk"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Non-differentiable and constrained optimization play a key role in machine\nlearning, signal and image processing, communications, and beyond. For\nhigh-dimensional minimization problems involving large datasets or many\nunknowns, the forward-backward splitting method provides a simple, practical\nsolver. Despite its apparently simplicity, the performance of the\nforward-backward splitting is highly sensitive to implementation details.\n  This article is an introductory review of forward-backward splitting with a\nspecial emphasis on practical implementation concerns. Issues like stepsize\nselection, acceleration, stopping conditions, and initialization are\nconsidered. Numerical experiments are used to compare the effectiveness of\ndifferent approaches.\n  Many variations of forward-backward splitting are implemented in the solver\nFASTA (short for Fast Adaptive Shrinkage/Thresholding Algorithm). FASTA\nprovides a simple interface for applying forward-backward splitting to a broad\nrange of problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.3406v6"
    },
    {
        "title": "Enhanced joint sparsity via Iterative Support Detection",
        "authors": [
            "Yaru Fan",
            "Yilun Wang",
            "Tingzhu Huang"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Joint sparsity has attracted considerable attention in recent years in many\nfields including sparse signal recovery in compressed sensing (CS), statistics,\nand machine learning. Traditional convex models suffer from the suboptimal\nperformance though enjoying tractable computation. In this paper, we propose a\nnew non-convex joint sparsity model, and develop a corresponding multi-stage\nadaptive convex relaxation algorithm. This method extends the idea of iterative\nsupport detection (ISD) from the single vector estimation to the multi-vector\nestimation by considering the joint sparsity prior. We provide some preliminary\ntheoretical analysis including convergence analysis and a sufficient recovery\ncondition. Numerical experiments from both compressive sensing and feature\nlearning show the better performance of the proposed method in comparison with\nseveral state-of-the-art alternatives. Moreover, we demonstrate that the\nextension of ISD from the single vector to multi-vector estimation is not\ntrivial. In particular, while ISD does not work well for reconstructing the\nsignal channel sparse Bernoulli signal, it does achieve significantly improved\nperformance when recovering the multi-channel sparse Bernoulli signal thanks to\nits ability of natural incorporation of the joint sparsity structure.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.2675v4"
    },
    {
        "title": "Improved Monte Carlo Variance Reduction for Space and Energy\n  Self-Shielding",
        "authors": [
            "S. C. Wilson",
            "R. N. Slaybaugh"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Continued demand for accurate and computationally efficient transport methods\nto solve optically thick, fixed-source transport problems has inspired research\non variance-reduction (VR) techniques for Monte Carlo (MC). Methods that use\ndeterministic results to create VR maps for MC constitute a dominant branch of\nthis research, with Forward Weighted-Consistent Adjoint Driven Importance\nSampling (FW-CADIS) being a particularly successful example. However, locations\nin which energy and spatial self-shielding are combined, such as thin plates\nembedded in concrete, challenge FW-CADIS. In these cases the deterministic flux\ncannot appropriately capture transport behavior, and the associated VR\nparameters result in high variance in and following the plate.\n  This work presents a new method that improves performance in transport\ncalculations that contain regions of combined space and energy self-shielding\nwithout significant impact on the solution quality in other parts of the\nproblem. This method is based on FW-CADIS and applies a Resonance Factor\ncorrection to the adjoint source. The impact of the Resonance Factor method is\ninvestigated in this work through an example problem. It is clear that this new\nmethod dramatically improves performance in terms of lowering the maximum 95%\nconfidence interval relative error and reducing the compute time. Based on this\nwork, we recommend that the Resonance Factor method be used when the accuracy\nof the solution in the presence of combined space and energy self-shielding is\nimportant.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.04749v1"
    },
    {
        "title": "Fast Spectral Low Rank Matrix Approximation",
        "authors": [
            "Haishan Ye",
            "Zhihua Zhang"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  First, we extend the results of approximate matrix multiplication from the\nFrobenius norm to the spectral norm. Second, We develop a class of fast\napproximate generalized linear regression algorithms with respect to the\nspectral norm. Finally, We give a fast approximate SVD.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.00523v4"
    },
    {
        "title": "Noise Robustness of a Combined Phase Retrieval and Reconstruction Method\n  for Phase-Contrast Tomography",
        "authors": [
            "Rasmus Dalgas Kongskov",
            "Jakob Sauer Jørgensen",
            "Henning Friis Poulsen",
            "Per Christian Hansen"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Classical reconstruction methods for phase-contrast tomography consist of two\nstages: phase retrieval and tomographic reconstruction. A novel algebraic\nmethod combining the two was suggested by Kostenko et al. (Opt. Express, 21,\n12185, 2013) and preliminary results demonstrating improved reconstruction\ncompared to a two-stage method given. Using simulated free-space propagation\nexperiments with a single sample-detector distance, we thoroughly compare the\nnovel method with the two-stage method to address limitations of the\npreliminary results. We demonstrate that the novel method is substantially more\nrobust towards noise; our simulations point to a possible reduction in counting\ntimes by an order of magnitude.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.02740v3"
    },
    {
        "title": "Automatic Detection of the Common and Non-common Frequencies in\n  Congruent Discrete Spectra. A Theoretical Approach",
        "authors": [
            "Cezar Doca",
            "Constantin Paunoiu"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Both sampling a time-varying signal, and its spectral analysis are activities\nsubjected to theoretically compelling, such as Shannon's theorem and the\nobjectively limiting of the frequency's resolution. Usually, the signals'\nspectra are processed and interpreted by a scientist who, presumably, has\nsufficient prior information about the monitored signals to conclude on the\nsignificant frequencies, for example. On the other hand, processing and\ninterpretation of signals' spectra can be routine tasks that must be automated\nusing suitable software, i.e. PC application. In the above context, the paper\npresents the theoretic bases of an intuitive and practical approach of the\n(automatic) detection of the common and non-common frequencies in two or more\ncongruent spectra.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.06151v1"
    },
    {
        "title": "Improving the numerical stability of fast matrix multiplication",
        "authors": [
            "Grey Ballard",
            "Austin R. Benson",
            "Alex Druinsky",
            "Benjamin Lipshitz",
            "Oded Schwartz"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Fast algorithms for matrix multiplication, namely those that perform\nasymptotically fewer scalar operations than the classical algorithm, have been\nconsidered primarily of theoretical interest. Apart from Strassen's original\nalgorithm, few fast algorithms have been efficiently implemented or used in\npractical applications. However, there exist many practical alternatives to\nStrassen's algorithm with varying performance and numerical properties. Fast\nalgorithms are known to be numerically stable, but because their error bounds\nare slightly weaker than the classical algorithm, they are not used even in\ncases where they provide a performance benefit.\n  We argue in this paper that the numerical sacrifice of fast algorithms,\nparticularly for the typical use cases of practical algorithms, is not\nprohibitive, and we explore ways to improve the accuracy both theoretically and\nempirically. The numerical accuracy of fast matrix multiplication depends on\nproperties of the algorithm and of the input matrices, and we consider both\ncontributions independently. We generalize and tighten previous error analyses\nof fast algorithms and compare their properties. We discuss algorithmic\ntechniques for improving the error guarantees from two perspectives:\nmanipulating the algorithms, and reducing input anomalies by various forms of\ndiagonal scaling. Finally, we benchmark performance and demonstrate our\nimproved numerical accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.00687v2"
    },
    {
        "title": "A Gauss-Seidel Iterative Thresholding Algorithm for lq Regularized Least\n  Squares Regression",
        "authors": [
            "Jinshan Zeng",
            "Zhimin Peng",
            "Shaobo Lin"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In recent studies on sparse modeling, $l_q$ ($0<q<1$) regularized least\nsquares regression ($l_q$LS) has received considerable attention due to its\nsuperiorities on sparsity-inducing and bias-reduction over the convex\ncounterparts. In this paper, we propose a Gauss-Seidel iterative thresholding\nalgorithm (called GAITA) for solution to this problem. Different from the\nclassical iterative thresholding algorithms using the Jacobi updating rule,\nGAITA takes advantage of the Gauss-Seidel rule to update the coordinate\ncoefficients. Under a mild condition, we can justify that the support set and\nsign of an arbitrary sequence generated by GAITA will converge within finite\niterations. This convergence property together with the Kurdyka-{\\L}ojasiewicz\nproperty of ($l_q$LS) naturally yields the strong convergence of GAITA under\nthe same condition as above, which is generally weaker than the condition for\nthe convergence of the classical iterative thresholding algorithms.\nFurthermore, we demonstrate that GAITA converges to a local minimizer under\ncertain additional conditions. A set of numerical experiments are provided to\nshow the effectiveness, particularly, much faster convergence of GAITA as\ncompared with the classical iterative thresholding algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.03173v1"
    },
    {
        "title": "Certified Roundoff Error Bounds Using Semidefinite Programming",
        "authors": [
            "Victor Magron",
            "George Constantinides",
            "Alastair Donaldson"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Roundoff errors cannot be avoided when implementing numerical programs with\nfinite precision. The ability to reason about rounding is especially important\nif one wants to explore a range of potential representations, for instance for\nFPGAs or custom hardware implementations. This problem becomes challenging when\nthe program does not employ solely linear operations, and non-linearities are\ninherent to many interesting computational problems in real-world applications.\n  Existing solutions to reasoning possibly lead to either inaccurate bounds or\nhigh analysis time in the presence of nonlinear correlations between variables.\nFurthermore, while it is easy to implement a straightforward method such as\ninterval arithmetic, sophisticated techniques are less straightforward to\nimplement in a formal setting. Thus there is a need for methods which output\ncertificates that can be formally validated inside a proof assistant.\n  We present a framework to provide upper bounds on absolute roundoff errors of\nfloating-point nonlinear programs. This framework is based on optimization\ntechniques employing semidefinite programming and sums of squares certificates,\nwhich can be checked inside the Coq theorem prover to provide formal roundoff\nerror bounds for polynomial programs. Our tool covers a wide range of nonlinear\nprograms, including polynomials and transcendental operations as well as\nconditional statements. We illustrate the efficiency and precision of this tool\non non-trivial programs coming from biology, optimization and space control.\nOur tool produces more accurate error bounds for 23% of all programs and yields\nbetter performance in 66% of all programs.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.03331v7"
    },
    {
        "title": "Rank-1 Tensor Approximation Methods and Application to Deflation",
        "authors": [
            "Alex Pereira da Silva",
            "Pierre Comon",
            "Andre Lima Ferrer de Almeida"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Because of the attractiveness of the canonical polyadic (CP) tensor\ndecomposition in various applications, several algorithms have been designed to\ncompute it, but efficient ones are still lacking. Iterative deflation\nalgorithms based on successive rank-1 approximations can be used to perform\nthis task, since the latter are rather easy to compute. We first present an\nalgebraic rank-1 approximation method that performs better than the standard\nhigher-order singular value decomposition (HOSVD) for three-way tensors.\nSecond, we propose a new iterative rank-1 approximation algorithm that improves\nany other rank-1 approximation method. Third, we describe a probabilistic\nframework allowing to study the convergence of deflation CP decomposition\n(DCPD) algorithms based on successive rank-1 approximations. A set of computer\nexperiments then validates theoretical results and demonstrates the efficiency\nof DCPD algorithms compared to other ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05273v1"
    },
    {
        "title": "Detecting Potential Instabilities of Numerical Algorithms",
        "authors": [
            "Yao Yang"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  It has been the standard teaching of today that backward stability analysis\nis taught as absolute, just as in Newtonian physics time is taught absolute\ntime. We will prove it is not true in general. It depends on algorithms. We\nwill prove that forward and mixed stability anlaysis are absolutely invalid\nstability analysis in the sense that they have absolutely wrong reference\npoints for detecting huge element growth of any algoritms(if any), even an\n\"ideal\" or \"desirable\" backward stability analysis is not so \"ideal\" or\n\"desirable\" in general. Any of forward stable, backward stable and mixed stable\nalgorihms as in Demmel, Kahan , Parlett and other's papers and text books, see\nDemmel(6) and Higham(8)may not be really stable at all because they may fail to\ndetect and expose any potential instabilities of the algorithm in corresponding\nstability analysis. Therefore, it is impossible to prove an algorithm is stable\naccording to the standard teachin of today, just as it is impossible to prove a\nmathematical equuation(Maxwell's) is a law of physics according to the standard\nteaching in Newtonian physics.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.02157v1"
    },
    {
        "title": "Trading Accuracy for Numerical Stability: Orthogonalization,\n  Biorthogonalization and Regularization",
        "authors": [
            "Tarek A. Lahlou",
            "Alan V. Oppenheim"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This paper presents two novel regularization methods motivated in part by the\ngeometric significance of biorthogonal bases in signal processing applications.\nThese methods, in particular, draw upon the structural relevance of\northogonality and biorthogonality principles and are presented from the\nperspectives of signal processing, convex programming, continuation methods and\nnonlinear projection operators. Each method is specifically endowed with either\na homotopy or tuning parameter to facilitate tradeoff analysis between accuracy\nand numerical stability. An example involving a basis comprised of real\nexponential signals illustrates the utility of the proposed methods on an\nill-conditioned inverse problem and the results are compared to standard\nregularization techniques from the signal processing literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.05895v2"
    },
    {
        "title": "Illustration of iterative linear solver behavior on simple 1D and 2D\n  problems",
        "authors": [
            "Nicolas Ray",
            "Sokolov Dmitry"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In geometry processing, numerical optimization methods often involve solving\nsparse linear systems of equations. These linear systems have a structure that\nstrongly resembles to adjacency graphs of the underlying mesh. We observe how\nclassic linear solvers behave on this specific type of problems. For the sake\nof simplicity, we minimise either the squared gradient or the squared\nLaplacian, evaluated by finite differences on a regular 1D or 2D grid. We\nobserved the evolution of the solution for both energies, in 1D and 2D, and\nwith different solvers: Jacobi, Gauss-Seidel, SSOR (Symmetric successive\nover-relaxation) and CG (conjugate gradient [She94]). Plotting results at\ndifferent iterations allows to have an intuition of the behavior of these\nclassic solvers.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.01118v1"
    },
    {
        "title": "Reduced Precision Checking to Detect Errors in Floating Point Arithmetic",
        "authors": [
            "Yaqi Zhang",
            "Ralph Nathan",
            "Daniel J. Sorin"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this paper, we use reduced precision checking (RPC) to detect errors in\nfloating point arithmetic. Prior work explored RPC for addition and\nmultiplication. In this work, we extend RPC to a complete floating point unit\n(FPU), including division and square root, and we present precise analyses of\nthe errors undetectable with RPC that show bounds that are smaller than prior\nwork. We implement RPC for a complete FPU in RTL and experimentally evaluate\nits error coverage and cost.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.01145v1"
    },
    {
        "title": "New fast divide-and-conquer algorithms for the symmetric tridiagonal\n  eigenvalue problem",
        "authors": [
            "Shengguo Li",
            "Xiangke Liao",
            "Jie Liu",
            "Hao Jiang"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this paper, two accelerated divide-and-conquer algorithms are proposed for\nthe symmetric tridiagonal eigenvalue problem, which cost $O(N^2r)$ {flops} in\nthe worst case, where $N$ is the dimension of the matrix and $r$ is a modest\nnumber depending on the distribution of eigenvalues. Both of these algorithms\nuse hierarchically semiseparable (HSS) matrices to approximate some\nintermediate eigenvector matrices which are Cauchy-like matrices and are\noff-diagonally low-rank. The difference of these two versions lies in using\ndifferent HSS construction algorithms, one (denoted by {ADC1}) uses a\nstructured low-rank approximation method and the other ({ADC2}) uses a\nrandomized HSS construction algorithm. For the ADC2 algorithm, a method is\nproposed to estimate the off-diagonal rank. Numerous experiments have been done\nto show their stability and efficiency. These algorithms are implemented in\nparallel in a shared memory environment, and some parallel implementation\ndetails are included. Comparing the ADCs with highly optimized multithreaded\nlibraries such as Intel MKL, we find that ADCs could be more than 6x times\nfaster for some large matrices with few deflations.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.04591v1"
    },
    {
        "title": "Monte Carlo Dynamically Weighted Importance Sampling For Finite Element\n  Model Updating",
        "authors": [
            "Daniel J Joubert",
            "Tshilidzi Marwala"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  The Finite Element Method (FEM) is generally unable to accurately predict\nnatural frequencies and mode shapes of structures (eigenvalues and\neigenvectors). Engineers develop numerical methods and a variety of techniques\nto compensate for this misalignment of modal properties, between experimentally\nmeasured data and the computed result from the FEM of structures. In this paper\nwe compare two indirect methods of updating namely, the Adaptive Metropolis\nHastings and a newly applied algorithm called Monte Carlo Dynamically Weighted\nImportance Sampling (MCDWIS). The approximation of a posterior predictive\ndistribution is based on Bayesian inference of continuous multivariate Gaussian\nprobability density functions, defining the variability of physical properties\naffected by forced vibration. The motivation behind applying MCDWIS is in the\ncomplexity of computing normalizing constants in higher dimensional or\nmultimodal systems. The MCDWIS accounts for this intractability by analytically\ncomputing importance sampling estimates at each time step of the algorithm. In\naddition, a dynamic weighting step with an Adaptive Pruned Enriched Population\nControl Scheme (APEPCS) allows for further control over weighted samples and\npopulation size. The performance of the MCDWIS simulation is graphically\nillustrated for all algorithm dependent parameters and show unbiased, stable\nsample estimates.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.04632v1"
    },
    {
        "title": "Spectral Partitioning with Blends of Eigenvectors",
        "authors": [
            "James P. Fairbanks",
            "Geoffrey D. Sanders",
            "David A. Bader"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Many common methods for data analysis rely on linear algebra. We provide new\nresults connecting data analysis error to numerical accuracy, which leads to\nthe first meaningful stopping criterion for two way spectral partitioning. More\ngenerally, we provide pointwise convergence guarantees so that blends (linear\ncombinations) of eigenvectors can be employed to solve data analysis problems\nwith confidence in their accuracy. We demonstrate this theory on an accessible\nmodel problem, the Ring of Cliques, by deriving the relevant eigenpairs and\ncomparing the predicted results to numerical solutions. These results bridge\nthe gap between linear algebra based data analysis methods and the convergence\ntheory of iterative approximation methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.04658v2"
    },
    {
        "title": "Efficient Approaches for Enclosing the United Solution Set of the\n  Interval Generalized Sylvester Matrix Equations",
        "authors": [
            "Marzieh Dehghani-Madiseh",
            "Milan Hladík"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this work, we investigate the interval generalized Sylvester matrix\nequation ${\\bf{A}}X{\\bf{B}}+{\\bf{C}}X{\\bf{D}}={\\bf{F}}$ and develop some\ntechniques for obtaining outer estimations for the so-called united solution\nset of this interval system. First, we propose a modified variant of the\nKrawczyk operator which causes reducing computational complexity to cubic,\ncompared to Kronecker product form. We then propose an iterative technique for\nenclosing the solution set. These approaches are based on spectral\ndecompositions of the midpoints of ${\\bf{A}}$, ${\\bf{B}}$, ${\\bf{C}}$ and\n${\\bf{D}}$ and in both of them we suppose that the midpoints of ${\\bf{A}}$ and\n${\\bf{C}}$ are simultaneously diagonalizable as well as for the midpoints of\nthe matrices ${\\bf{B}}$ and ${\\bf{D}}$. Some numerical experiments are given to\nillustrate the performance of the proposed methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.04853v3"
    },
    {
        "title": "Signal Processing Structures for Solving Conservative Constraint\n  Satisfaction Problems",
        "authors": [
            "Tarek A. Lahlou",
            "Thomas A. Baran"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This primary purpose of this paper is to succinctly state a number of\nverifiable and tractable sufficient conditions under which a particular class\nof conservative signal processing structures may be readily used to solve a\ncompanion class of constraint satisfaction problems using both synchronous and\nasynchronous implementation protocols. In particular, the mentioned class of\nstructures is shown to have desirable convergence and robustness properties\nwith respect to various uncertainties involving communication and processing\ndelays. Essential ingredients to the arguments herein involve blending together\nfunctional composition methods, conservation principles, asynchronous signal\nprocessing implementation protocols, and methods of homotopy. Numerical\nexperiments complement the theoretical presentation and connections to\noptimization theory are made.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.05231v1"
    },
    {
        "title": "Fast Multipole Method as a Matrix-Free Hierarchical Low-Rank\n  Approximation",
        "authors": [
            "Rio Yokota",
            "Huda Ibeid",
            "David Keyes"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  There has been a large increase in the amount of work on hierarchical\nlow-rank approximation methods, where the interest is shared by multiple\ncommunities that previously did not intersect. This objective of this article\nis two-fold; to provide a thorough review of the recent advancements in this\nfield from both analytical and algebraic perspectives, and to present a\ncomparative benchmark of two highly optimized implementations of contrasting\nmethods for some simple yet representative test cases. We categorize the recent\nadvances in this field from the perspective of compute-memory tradeoff, which\nhas not been considered in much detail in this area. Benchmark tests reveal\nthat there is a large difference in the memory consumption and performance\nbetween the different methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.02244v1"
    },
    {
        "title": "Toom-Cook Multiplication: Some Theoretical and Practical Aspects",
        "authors": [
            "M. J. Kronenburg"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Toom-Cook multiprecision multiplication is a well-known multiprecision\nmultiplication method, which can make use of multiprocessor systems. In this\npaper the Toom-Cook complexity is derived, some explicit proofs of the\nToom-Cook interpolation method are given, the even-odd method for interpolation\nis explained, and certain aspects of a 32-bit C++ and assembler implementation,\nwhich is in development, are discussed. A performance graph of this\nimplementation is provided. The Toom-Cook method can also be used to\nmultithread other types of multiplication, which is demonstrated for 32-bit GMP\nFFT multiplication.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.02740v1"
    },
    {
        "title": "Online Low-Rank Tensor Subspace Tracking from Incomplete Data by CP\n  Decomposition using Recursive Least Squares",
        "authors": [
            "Hiroyuki Kasai"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We propose an online tensor subspace tracking algorithm based on the CP\ndecomposition exploiting the recursive least squares (RLS), dubbed OnLine\nLow-rank Subspace tracking by TEnsor CP Decomposition (OLSTEC). Numerical\nevaluations show that the proposed OLSTEC algorithm gives faster convergence\nper iteration comparing with the state-of-the-art online algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.07067v2"
    },
    {
        "title": "The swept rule for breaking the latency barrier in time advancing\n  two-dimensional PDEs",
        "authors": [
            "Maitham Makki Alhubail",
            "Qiqi Wang",
            "John Williams"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  This article describes a method to accelerate parallel, explicit time\nintegration of two-dimensional unsteady PDEs. The method is motivated by our\nobservation that latency, not bandwidth, often limits how fast PDEs can be\nsolved in parallel. The method is called the swept rule of space-time domain\ndecomposition. Compared to conventional, space-only domain decomposition, it\ncommunicates similar amount of data, but in fewer messages. The swept rule\nachieves this by decomposing space and time among computing nodes in ways that\nexploit the domains of influence and the domain of dependency, making it\npossible to communicate once per many time steps with no redundant computation.\nBy communicating less often, the swept rule effectively breaks the latency\nbarrier, advancing on average more than one time step per ping-pong latency of\nthe network. The article presents simple theoretical analysis to the\nperformance of the swept rule in two spatial dimensions, and supports the\nanalysis with numerical experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.07558v2"
    },
    {
        "title": "Ultrafast a Distributed Arithmetic in multi-row codes",
        "authors": [
            "V. I. Shcherbakov"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this paper we consider the matrix structure of arithmetic processors based\non distributed arithmetic in multi-row codes. Scope - development of\nsupercomputers.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.08391v1"
    },
    {
        "title": "A Direct Elliptic Solver Based on Hierarchically Low-rank Schur\n  Complements",
        "authors": [
            "Gustavo Chávez",
            "George Turkiyyah",
            "David Keyes"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  A parallel fast direct solver for rank-compressible block tridiagonal linear\nsystems is presented. Algorithmic synergies between Cyclic Reduction and\nHierarchical matrix arithmetic operations result in a solver with $O(N \\log^2\nN)$ arithmetic complexity and $O(N \\log N)$ memory footprint. We provide a\nbaseline for performance and applicability by comparing with well known\nimplementations of the $\\mathcal{H}$-LU factorization and algebraic multigrid\nwith a parallel implementation that leverages the concurrency features of the\nmethod. Numerical experiments reveal that this method is comparable with other\nfast direct solvers based on Hierarchical Matrices such as $\\mathcal{H}$-LU and\nthat it can tackle problems where algebraic multigrid fails to converge.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.00617v1"
    },
    {
        "title": "Unit Consistency, Generalized Inverses, and Effective System Design\n  Methods",
        "authors": [
            "Jeffrey Uhlmann"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  This paper examines the potential role of unit consistency as a system design\nprinciple. Unit-consistent generalized matrix inverses and unit-invariant\nmatrix decompositions are derived in support of this principle. Applications of\nthe methods described are illustrated with examples relating to nonlinear\nsystem identification and robustness to multiplicative noise for image database\nretrieval.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.08476v2"
    },
    {
        "title": "Locality-Aware Laplacian Mesh Smoothing",
        "authors": [
            "Guillaume Aupy",
            "JeongHyung Park",
            "Padma Raghavan"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In this paper, we propose a novel reordering scheme to improve the\nperformance of a Laplacian Mesh Smoothing (LMS). While the Laplacian smoothing\nalgorithm is well optimized and studied, we show how a simple reordering of the\nvertices of the mesh can greatly improve the execution time of the smoothing\nalgorithm. The idea of our reordering is based on (i) the postulate that cache\nmisses are a very time consuming part of the execution of LMS, and (ii) the\nstudy of the reuse distance patterns of various executions of the LMS\nalgorithm.\n  Our reordering algorithm is very simple but allows for huge performance\nimprovement. We ran it on a Westmere-EX platform and obtained a speedup of 75\non 32 cores compared to the single core execution without reordering, and a\ngain in execution of 32% on 32 cores compared to state of the art reordering.\nFinally, we show that we leave little room for a better ordering by reducing\nthe L2 and L3 cache misses to a bare minimum.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00803v1"
    },
    {
        "title": "A Parallel Implementation of the Ensemble Kalman Filter Based on\n  Modified Cholesky Decomposition",
        "authors": [
            "Elias D. Nino",
            "Adrian Sandu",
            "Xinwei Deng"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  This paper discusses an efficient parallel implementation of the ensemble\nKalman filter based on the modified Cholesky decomposition. The proposed\nimplementation starts with decomposing the domain into sub-domains. In each\nsub-domain a sparse estimation of the inverse background error covariance\nmatrix is computed via a modified Cholesky decomposition; the estimates are\ncomputed concurrently on separate processors. The sparsity of this estimator is\ndictated by the conditional independence of model components for some radius of\ninfluence. Then, the assimilation step is carried out in parallel without the\nneed of inter-processor communication. Once the local analysis states are\ncomputed, the analysis sub-domains are mapped back onto the global domain to\nobtain the analysis ensemble. Computational experiments are performed using the\nAtmospheric General Circulation Model (SPEEDY) with the T-63 resolution on the\nBlueridge cluster at Virginia Tech. The number of processors used in the\nexperiments ranges from 96 to 2,048. The proposed implementation outperforms in\nterms of accuracy the well-known local ensemble transform Kalman filter (LETKF)\nfor all the model variables. The computational time of the proposed\nimplementation is similar to that of the parallel LETKF method (where no\ncovariance estimation is performed). Finally, for the largest number of\nprocessors, the proposed parallel implementation is 400 times faster than the\nserial version of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00807v1"
    },
    {
        "title": "Mathematical Modeling of General Inaccurate Adders",
        "authors": [
            "Zvi M. Kedem",
            "Kirthi Krishna Muntimadugu"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Inaccurate circuits make possible the conservation of limited resources, such\nas energy. But effective design of such circuits requires an understanding of\nresulting tradeoffs between accuracy and design parameters, such as voltages\nand speed of execution. Although studies of tradeoffs have been done on\nspecific circuits, the applicability of those studies is narrow. This paper\npresents a comprehensive and mathematically rigorous method for analyzing a\nlarge class of inaccurate circuits for addition. Furthermore, it presents new,\nfast algorithms for the computation of key statistical measures of inaccuracy\nin such adders, thus helping hardware architects explore the design space with\ngreater confidence.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.01753v1"
    },
    {
        "title": "A strategy to implement Dirichlet boundary conditions in the context of\n  ADER finite volume schemes. One-dimensional conservation laws",
        "authors": [
            "Gino I. Montecinos"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  ADER schemes are numerical methods, which can reach an arbitrary order of\naccuracy in both space and time. They are based on a reconstruction procedure\nand the solution of generalized Riemann problems. However, for general boundary\nconditions, in particular of Dirichlet type, a lack of accuracy might occur if\na suitable treatment of boundaries conditions is not properly carried out. In\nthis work the treatment of Dirichlet boundary conditions for conservation laws\nin the context of ADER schemes, is concerned. The solution of generalized\nRiemann problems at the extremes of the computational domain, provides the\ncorrect influence of boundaries. The reconstruction procedure, for data near to\nthe boundaries, demands for information outside the computational domain, which\nis carried out in terms of ghost cells, which are provided by using the\nnumerical solution of auxiliary problems. These auxiliary problems are\nhyperbolic and they are constructed from the conservation laws and the\ninformation at boundaries, which may be partially or totally known in terms of\nprescribed functions. The evolution of these problems, unlike to the usual\nmanner, is done in space rather than in time due to that these problems are\nnamed here, {\\it reverse problems}. The methodology can be considered as a\nnumerical counterpart of the inverse Lax-Wendroff procedure for filling ghost\ncells. However, the use of Taylor series expansions, as well as, Lax-Wendroff\nprocedure, are avoided. For the scalar case is shown that the present procedure\npreserve the accuracy of the scheme which is reinforced with some numerical\nresults. Expected orders of accuracy for solving conservation laws by using the\nproposed strategy at boundaries, are obtained up to fifth-order in both space\nand time.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.03032v1"
    },
    {
        "title": "Monotonicity-preserving finite element schemes based on differentiable\n  nonlinear stabilization",
        "authors": [
            "Santiago Badia",
            "Jesús Bonilla"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In this work, we propose a nonlinear stabilization technique for scalar\nconservation laws with implicit time stepping. The method relies on an\nartificial diffusion method, based on a graph-Laplacian operator. It is\nnonlinear, since it depends on a shock detector. The same shock detector is\nused to gradually lump the mass matrix. The resulting method is LED, positivity\npreserving, linearity preserving, and also satisfies a global DMP. Lipschitz\ncontinuity has also been proved. However, the resulting scheme is highly\nnonlinear, leading to very poor nonlinear convergence rates. We propose a\nsmooth version of the scheme, which leads to twice differentiable nonlinear\nstabilization schemes. It allows one to straightforwardly use Newton's method\nand obtain quadratic convergence. In the numerical experiments, steady and\ntransient linear transport, and transient Burgers' equation have been\nconsidered in 2D. Using the Newton method with a smooth version of the scheme\nwe can reduce 10 to 20 times the number of iterations of Anderson acceleration\nwith the original non-smooth scheme. In any case, these properties are only\ntrue for the converged solution, but not for iterates. In this sense, we have\nalso proposed the concept of projected nonlinear solvers, where a projection\nstep is performed at the end of every nonlinear iteration onto a FE space of\nadmissible solutions. The space of admissible solutions is the one that\nsatisfies the desired monotonic properties (maximum principle or positivity).\n",
        "pdf_link": "http://arxiv.org/pdf/1606.08743v2"
    },
    {
        "title": "A Unified Alternating Direction Method of Multipliers by Majorization\n  Minimization",
        "authors": [
            "Canyi Lu",
            "Jiashi Feng",
            "Shuicheng Yan",
            "Zhouchen Lin"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Accompanied with the rising popularity of compressed sensing, the Alternating\nDirection Method of Multipliers (ADMM) has become the most widely used solver\nfor linearly constrained convex problems with separable objectives. In this\nwork, we observe that many previous variants of ADMM update the primal variable\nby minimizing different majorant functions with their convergence proofs given\ncase by case. Inspired by the principle of majorization minimization, we\nrespectively present the unified frameworks and convergence analysis for the\nGauss-Seidel ADMMs and Jacobian ADMMs, which use different historical\ninformation for the current updating. Our frameworks further generalize\nprevious ADMMs to the ones capable of solving the problems with non-separable\nobjectives by minimizing their separable majorant surrogates. We also show that\nthe bound which measures the convergence speed of ADMMs depends on the\ntightness of the used majorant function. Then several techniques are introduced\nto improve the efficiency of ADMMs by tightening the majorant functions. In\nparticular, we propose the Mixed Gauss-Seidel and Jacobian ADMM (M-ADMM) which\nalleviates the slow convergence issue of Jacobian ADMMs by absorbing merits of\nthe Gauss-Seidel ADMMs. M-ADMM can be further improved by using backtracking,\nwise variable partition and fully exploiting the structure of the constraint.\nBeyond the guarantee in theory, numerical experiments on both synthesized and\nreal-world data further demonstrate the superiority of our new ADMMs in\npractice. Finally, we release a toolbox at https://github.com/canyilu/LibADMM\nthat implements efficient ADMMs for many problems in compressed sensing.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02584v1"
    },
    {
        "title": "Selecting Algorithms for Black Box Matrices: Checking for Matrix\n  Properties That Can Simplify Computations",
        "authors": [
            "Wayne Eberly"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Processes to automate the selection of appropriate algorithms for various\nmatrix computations are described. In particular, processes to check for, and\ncertify, various matrix properties of black box matrices are presented. These\ninclude sparsity patterns and structural properties that allow \"superfast\"\nalgorithms to be used in place of black-box algorithms. Matrix properties that\nhold generically, and allow the use of matrix preconditioning to be reduced or\neliminated, can also be checked for and certified - notably including in the\nsmall-field case, where this presently has the greatest impact on the\nefficiency of the computation.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.04499v2"
    },
    {
        "title": "Black Box Linear Algebra: Extending Wiedemann's Analysis of a Sparse\n  Matrix Preconditioner for Computations over Small Fields",
        "authors": [
            "Wayne Eberly"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Wiedemann's paper, introducing his algorithm for sparse and structured matrix\ncomputations over arbitrary fields, also presented a pair of matrix\npreconditioners for computations over small fields. The analysis of the second\nof these is extended in order to provide more explicit statements of the\nexpected number of nonzero entries in the matrices obtained as well as bounds\non the probability that such matrices have maximal rank.\n  This is part of ongoing work to establish that this matrix preconditioner can\nalso be used to bound the number of nontrivial nilpotent blocks in the Jordan\nnormal form of a preconditioned matrix, in such a way that one can also sample\nuniformly from the null space of the originally given matrix. If successful\nthis will result in a black box algorithm for the type of matrix computation\nrequired when using the number field sieve for integer factorization that is\nprovably reliable and - by a small factor - asymptotically more efficient than\nalternative techniques that make use of other matrix preconditioners or require\ncomputations over field extensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.04514v1"
    },
    {
        "title": "Accelerated Kaczmarz Algorithms using History Information",
        "authors": [
            "Tengfei Ma"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  The Kaczmarz algorithm is a well known iterative method for solving\noverdetermined linear systems. Its randomized version yields provably\nexponential convergence in expectation. In this paper, we propose two new\nmethods to speed up the randomized Kaczmarz algorithm by utilizing the past\nestimates in the iterations. The first one utilize the past estimates to get a\npreconditioner. The second one combines the stochastic average gradient (SAG)\nmethod with the randomized Kaczmarz algorithm. It takes advantage of past\ngradients to improve the convergence speed. Numerical experiments indicate that\nthe new algorithms can dramatically outperform the standard randomized Kaczmarz\nalgorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.00351v1"
    },
    {
        "title": "Analysis of time series and signals using the Square Wave Method",
        "authors": [
            "Osvaldo Skliar",
            "Ricardo E. Monge",
            "Sherry Gapper"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  The Square Wave Method (SWM), previously introduced for the analysis of\nsignals and images, is presented here as a mathematical tool suitable for the\nanalysis of time series and signals. To show the potential that the SWM has to\nanalyze many different types of time series, the results of the analysis of a\ntime series composed of a sequence of 10,000 numerical values are presented\nhere. These values were generated by using the Mathematical Random Number\nGenerator (MRNG).\n",
        "pdf_link": "http://arxiv.org/pdf/1608.02166v4"
    },
    {
        "title": "Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale\n  Optimization Problems: Perspectives and Challenges PART 1",
        "authors": [
            "A. Cichocki",
            "N. Lee",
            "I. V. Oseledets",
            "A. -H. Phan",
            "Q. Zhao",
            "D. Mandic"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Machine learning and data mining algorithms are becoming increasingly\nimportant in analyzing large volume, multi-relational and multi--modal\ndatasets, which are often conveniently represented as multiway arrays or\ntensors. It is therefore timely and valuable for the multidisciplinary research\ncommunity to review tensor decompositions and tensor networks as emerging tools\nfor large-scale data analysis and data mining. We provide the mathematical and\ngraphical representations and interpretation of tensor networks, with the main\nfocus on the Tucker and Tensor Train (TT) decompositions and their extensions\nor generalizations.\n  Keywords: Tensor networks, Function-related tensors, CP decomposition, Tucker\nmodels, tensor train (TT) decompositions, matrix product states (MPS), matrix\nproduct operators (MPO), basic tensor operations, multiway component analysis,\nmultilinear blind source separation, tensor completion, linear/multilinear\ndimensionality reduction, large-scale optimization problems, symmetric\neigenvalue decomposition (EVD), PCA/SVD, huge systems of linear equations,\npseudo-inverse of very large matrices, Lasso and Canonical Correlation Analysis\n(CCA) (This is Part 1)\n",
        "pdf_link": "http://arxiv.org/pdf/1609.00893v3"
    },
    {
        "title": "Tighter bound of Sketched Generalized Matrix Approximation",
        "authors": [
            "Haishan Ye",
            "Qiaoming Ye",
            "Zhihua Zhang"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Generalized matrix approximation plays a fundamental role in many machine\nlearning problems, such as CUR decomposition, kernel approximation, and matrix\nlow rank approximation. Especially with today's applications involved in larger\nand larger dataset, more and more efficient generalized matrix approximation\nalgorithems become a crucially important research issue. In this paper, we find\nnew sketching techniques to reduce the size of the original data matrix to\ndevelop new matrix approximation algorithms. Our results derive a much tighter\nbound for the approximation than previous works: we obtain a $(1+\\epsilon)$\napproximation ratio with small sketched dimensions which implies a more\nefficient generalized matrix approximation.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.02258v1"
    },
    {
        "title": "Soft Recovery Through $\\ell_{1,2}$ Minimization with Applications in\n  Recovery of Simultaneously Sparse and Low-Rank Matrice",
        "authors": [
            "Axel Flinth"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  This article provides a new type of analysis of a compressed-sensing based\ntechnique for recovering column-sparse matrices, namely minimization of the\n$\\ell_{1,2}$-norm. Rather than providing conditions on the measurement matrix\nwhich guarantees the solution of the program to be exactly equal to the ground\ntruth signal (which already has been thoroughly investigated), it presents a\ncondition which guarantees that the solution is approximately equal to the\nground truth. Soft recovery statements of this kind are to the best knowledge\nof the author a novelty in Compressed Sensing. Apart from the theoretical\nanalysis, we present two heuristic proposes how this property of the\n$\\ell_{1,2}$-program can be utilized to design algorithms for recovery of\nmatrices which are sparse and have low rank at the same time.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.02302v1"
    },
    {
        "title": "On Memory Footprints of Partitioned Sparse Matrices",
        "authors": [
            "Daniel Langr"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Runtime characteristics of sparse matrix computations and related processes\nmay be often improved by reducing memory footprints of involved matrices. Such\na reduction can be usually achieved when matrices are processed in a block-wise\nmanner. The presented study analysed memory footprints of 563 representative\nbenchmark sparse matrices with respect to their partitioning into\nuniformly-sized blocks. Different block sizes and different ways of storing\nblocks in memory were considered and statistically evaluated. Memory footprints\nof partitioned matrices were additionally compared with lower bounds and with\nthe CSR storage format. The average measured memory savings against CSR in case\nof single and double precision were 42.3 and 28.7 percents, the corresponding\nworst-case savings 25.5 and 17.1 percents. Moreover, memory footprints of\npartitioned matrices were in average 5 times closer to their lower bounds than\nCSR. Based on the obtained results, generic suggestions for efficient\npartitioning and storage of sparse matrices in a computer memory are provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.04585v2"
    },
    {
        "title": "Technical Report: Improved Fourier Reconstruction using Jump Information\n  with Applications to MRI",
        "authors": [
            "Jade Larriva-Latt",
            "Angela Morrison",
            "Alison Radgowski",
            "Joseph Tobin",
            "Aditya Viswanathan",
            "Mark Iwen"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Certain applications such as Magnetic Resonance Imaging (MRI) require the\nreconstruction of functions from Fourier spectral data. When the underlying\nfunctions are piecewise-smooth, standard Fourier approximation methods suffer\nfrom the Gibbs phenomenon - with associated oscillatory artifacts in the\nvicinity of edges and an overall reduced order of convergence in the\napproximation. This paper proposes an edge-augmented Fourier reconstruction\nprocedure which uses only the first few Fourier coefficients of an underlying\npiecewise-smooth function to accurately estimate jump information and then\nincorporate it into a Fourier partial sum approximation. We provide both\ntheoretical and empirical results showing the improved accuracy of the proposed\nmethod, as well as comparisons demonstrating superior performance over existing\nstate-of-the-art sparse optimization-based methods. Extensions of the proposed\ntechniques to functions of several variables are also addressed preliminarily.\nAll code used to generate the results in this report are made publicly\navailable.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.03764v1"
    },
    {
        "title": "Proximal Algorithms and Temporal Differences for Large Linear Systems:\n  Extrapolation, Approximation, and Simulation",
        "authors": [
            "Dimitri P. Bertsekas"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We consider large linear and nonlinear fixed point problems, and solution\nwith proximal algorithms. We show that there is a close connection between two\nseemingly different types of methods from distinct fields: 1) Proximal\niterations for linear systems of equations, which are prominent in numerical\nanalysis and convex optimization, and 2) Temporal difference (TD) type methods,\nsuch as TD(lambda), LSTD(lambda), and LSPE(lambda), which are central in\nsimulation-based approximate dynamic programming/reinforcement learning\n(DP/RL), and its recent prominent successes in large-scale game contexts, among\nothers.\n  One benefit of this connection is a new and simple way to accelerate the\nstandard proximal algorithm by extrapolation towards the TD iteration, which\ngenerically has a faster convergence rate. Another benefit is the potential\nintegration into the proximal algorithmic context of several new ideas that\nhave emerged in the DP/RL context. We discuss some of the possibilities, and in\nparticular, algorithms that project each proximal iterate onto the subspace\nspanned by a small number of basis functions, using low-dimensional\ncalculations and simulation. A third benefit is that insights and analysis from\nproximal algorithms can be brought to bear on the enhancement of TD methods.\n  The linear fixed point methodology can be extended to nonlinear fixed point\nproblems involving a contraction, thus providing guaranteed and potentially\nsubstantial acceleration of the proximal and forward backward splitting\nalgorithms at no extra cost. Moreover, the connection of proximal and TD\nmethods can be extended to nonlinear (nondifferentiable) fixed point problems\nthrough new proximal-like algorithms that involve successive linearization,\nsimilar to policy iteration in DP.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.05427v4"
    },
    {
        "title": "Certified Roundoff Error Bounds using Bernstein Expansions and Sparse\n  Krivine-Stengle Representations",
        "authors": [
            "Alexandre Rocca",
            "Victor Magron",
            "Thao Dang"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Floating point error is an inevitable drawback of embedded systems\nimplementation. Computing rigorous upper bounds of roundoff errors is\nabsolutely necessary to the validation of critical software. This problem is\neven more challenging when addressing non-linear programs. In this paper, we\npropose and compare two new methods based on Bernstein expansions and sparse\nKrivine-Stengle representations, adapted from the field of the global\noptimization to compute upper bounds of roundoff errors for programs\nimplementing polynomial functions. We release two related software package\nFPBern and FPKiSten, and compare them with state of the art tools. We show that\nthese two methods achieve competitive performance, while computing accurate\nupper bounds by comparison with other tools.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.07038v2"
    },
    {
        "title": "A Revisit of Block Power Methods for Finite State Markov Chain\n  Applications",
        "authors": [
            "Hao Ji",
            "Seth H. Weinberg",
            "Yaohang Li"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In this paper, we revisit the generalized block power methods for\napproximating the eigenvector associated with $\\lambda_1 = 1$ of a Markov chain\ntransition matrix. Our analysis of the block power method shows that when $s$\nlinearly independent probability vectors are used as the initial block, the\nconvergence of the block power method to the stationary distribution depends on\nthe magnitude of the $(s+1)$th dominant eigenvalue $\\lambda_{s+1}$ of $P$\ninstead of that of $\\lambda_2$ in the power method. Therefore, the block power\nmethod with block size $s$ is particularly effective for transition matrices\nwhere $|\\lambda_{s+1}|$ is well separated from $\\lambda_1 = 1$ but\n$|\\lambda_2|$ is not. This approach is particularly useful when visiting the\nelements of a large transition matrix is the main computational bottleneck over\nmatrix--vector multiplications, where the block power method can effectively\nreduce the total number of times to pass over the matrix. To further reduce the\noverall computational cost, we combine the block power method with a sliding\nwindow scheme, taking advantage of the subsequent vectors of the latest $s$\niterations to assemble the block matrix. The sliding window scheme correlates\nvectors in the sliding window to quickly remove the influences from the\neigenvalues whose magnitudes are smaller than $|\\lambda_{s}|$ to reduce the\noverall number of matrix--vector multiplications to reach convergence. Finally,\nwe compare the effectiveness of these methods in a Markov chain model\nrepresenting a stochastic luminal calcium release site.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.08881v1"
    },
    {
        "title": "Data-driven time parallelism via forecasting",
        "authors": [
            "Kevin Carlberg",
            "Lukas Brencher",
            "Bernard Haasdonk",
            "Andrea Barth"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  This work proposes a data-driven method for enabling the efficient, stable\ntime-parallel numerical solution of systems of ordinary differential equations\n(ODEs). The method assumes that low-dimensional bases that accurately capture\nthe time evolution of the state are available. The method adopts the parareal\nframework for time parallelism, which is defined by an initialization method, a\ncoarse propagator, and a fine propagator. Rather than employing usual\napproaches for initialization and coarse propagation, we propose novel\ndata-driven techniques that leverage the available time-evolution bases. The\ncoarse propagator is defined by a forecast (proposed in Ref. [12]) applied\nlocally within each coarse time interval, which comprises the following steps:\n(1) apply the fine propagator for a small number of time steps, (2) approximate\nthe state over the entire coarse time interval using gappy POD with the local\ntime-evolution bases, and (3) select the approximation at the end of the time\ninterval as the propagated state. We also propose both local-forecast and\nglobal-forecast initialization. The method is particularly well suited for\nPOD-based reduced-order models (ROMs). In this case, spatial parallelism\nquickly saturates, as the ROM dynamical system is low dimensional; thus, time\nparallelism is needed to enable lower wall times. Further, the time-evolution\nbases can be extracted from the (readily available) right singular vectors\narising during POD computation. In addition to performing analyses related to\nthe method's accuracy, speedup, stability, and convergence, we also numerically\ndemonstrate the method's performance. Here, numerical experiments on ROMs for a\nnonlinear convection-reaction problem demonstrate the method's ability to\nrealize near-ideal speedups; global-forecast initialization with a\nlocal-forecast coarse propagator leads to the best performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.09049v2"
    },
    {
        "title": "Stability analysis of the numerical Method of characteristics applied to\n  a class of energy-preserving systems. Part I: Periodic boundary conditions",
        "authors": [
            "Taras I. Lakoba",
            "Zihao Deng"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We study numerical (in)stability of the Method of characteristics (MoC)\napplied to a system of non-dissipative hyperbolic partial differential\nequations (PDEs) with periodic boundary conditions. We consider three different\nsolvers along the characteristics: simple Euler (SE), modified Euler (ME), and\nLeap-frog (LF). The two former solvers are well known to exhibit a mild, but\nunconditional, numerical instability for non-dissipative ordinary differential\nequations (ODEs). They are found to have a similar (or stronger, for the\nMoC-ME) instability when applied to non-dissipative PDEs. On the other hand,\nthe LF solver is known to be stable when applied to non-dissipative ODEs.\nHowever, when applied to non-dissipative PDEs within the MoC framework, it was\nfound to have by far the strongest instability among all three solvers. We also\ncomment on the use of the fourth-order Runge--Kutta solver within the MoC\nframework.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.09079v2"
    },
    {
        "title": "Stability analysis of the numerical Method of characteristics applied to\n  a class of energy-preserving systems. Part II: Nonreflecting boundary\n  conditions",
        "authors": [
            "Taras I. Lakoba",
            "Zihao Deng"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We show that imposition of non-periodic, in place of periodic, boundary\nconditions (BC) can alter stability of modes in the Method of characteristics\n(MoC) employing certain ordinary-differential equation (ODE) numerical solvers.\nThus, using non-periodic BC may render some of the MoC schemes stable for most\npractical computations, even though they are unstable for periodic BC. This\nfact contradicts a statement, found in some literature, that an instability\ndetected by the von Neumann analysis for a given numerical scheme implies an\ninstability of that scheme with arbitrary (i.e., non-periodic) BC. We explain\nthe mechanism behind this contradiction. We also show that, and explain why,\nfor the MoC employing some other ODE solvers, stability of the modes may be\nunaffected by the BC.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.09080v2"
    },
    {
        "title": "Systematic Generation of Algorithms for Iterative Methods",
        "authors": [
            "Henrik Barthels"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The FLAME methodology makes it possible to derive provably correct algorithms\nfrom a formal description of a linear algebra problem. So far, the methodology\nhas been successfully used to automate the derivation of direct algorithms such\nas the Cholesky decomposition and the solution of Sylvester equations. In this\nthesis, we present an extension of the FLAME methodology to tackle iterative\nmethods such as Conjugate Gradient. As a starting point, we use a formal\ndescription of the iterative method in matrix form. The result is a family of\nprovably correct pseudocode algorithms. We argue that all the intermediate\nsteps are sufficiently systematic to be fully automated.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00279v2"
    },
    {
        "title": "Using Approximate Computing for the Calculation of Inverse Matrix p-th\n  Roots",
        "authors": [
            "Michael Lass",
            "Thomas D. Kühne",
            "Christian Plessl"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Approximate computing has shown to provide new ways to improve performance\nand power consumption of error-resilient applications. While many of these\napplications can be found in image processing, data classification or machine\nlearning, we demonstrate its suitability to a problem from scientific\ncomputing. Utilizing the self-correcting behavior of iterative algorithms, we\nshow that approximate computing can be applied to the calculation of inverse\nmatrix p-th roots which are required in many applications in scientific\ncomputing. Results show great opportunities to reduce the computational effort\nand bandwidth required for the execution of the discussed algorithm, especially\nwhen targeting special accelerator hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.02283v1"
    },
    {
        "title": "Space-time least-squares Petrov-Galerkin projection for nonlinear model\n  reduction",
        "authors": [
            "Youngsoo Choi",
            "Kevin Carlberg"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This work proposes a space-time least-squares Petrov-Galerkin (ST-LSPG)\nprojection method for model reduction of nonlinear dynamical systems. In\ncontrast to typical nonlinear model-reduction methods that first apply\n(Petrov-)Galerkin projection in the spatial dimension and subsequently apply\ntime integration to numerically resolve the resulting low-dimensional dynamical\nsystem, the proposed method applies projection in space and time\nsimultaneously. To accomplish this, the method first introduces a\nlow-dimensional space-time trial subspace, which can be obtained by computing\ntensor decompositions of state-snapshot data. The method then computes\ndiscrete-optimal approximations in this space-time trial subspace by minimizing\nthe residual arising after time discretization over all space and time in a\nweighted $\\ell^2$-norm. This norm can be defined to enable complexity reduction\n(i.e., hyper-reduction) in time, which leads to space-time collocation and\nspace-time GNAT variants of the ST-LSPG method. Advantages of the approach\nrelative to typical spatial-projection-based nonlinear model reduction methods\nsuch as Galerkin projection and least-squares Petrov-Galerkin projection\ninclude: (1) a reduction of both the spatial and temporal dimensions of the\ndynamical system, (2) the removal of spurious temporal modes (e.g., unstable\ngrowth) from the state space, and (3) error bounds that exhibit slower growth\nin time. Numerical examples performed on model problems in fluid dynamics\ndemonstrate the ability of the method to generate orders-of-magnitude\ncomputational savings relative to spatial-projection-based reduced-order models\nwithout sacrificing accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.04560v5"
    },
    {
        "title": "Accelerated and Inexact Soft-Impute for Large-Scale Matrix and Tensor\n  Completion",
        "authors": [
            "Quanming Yao",
            "James T. Kwok"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Matrix and tensor completion aim to recover a low-rank matrix / tensor from\nlimited observations and have been commonly used in applications such as\nrecommender systems and multi-relational data mining. A state-of-the-art matrix\ncompletion algorithm is Soft-Impute, which exploits the special \"sparse plus\nlow-rank\" structure of the matrix iterates to allow efficient SVD in each\niteration. Though Soft-Impute is a proximal algorithm, it is generally believed\nthat acceleration destroys the special structure and is thus not useful. In\nthis paper, we show that Soft-Impute can indeed be accelerated without\ncomprising this structure. To further reduce the iteration time complexity, we\npropose an approximate singular value thresholding scheme based on the power\nmethod. Theoretical analysis shows that the proposed algorithm still enjoys the\nfast $O(1/T^2)$ convergence rate of accelerated proximal algorithms. We further\nextend the proposed algorithm to tensor completion with the scaled latent\nnuclear norm regularizer. We show that a similar \"sparse plus low-rank\"\nstructure also exists, leading to low iteration complexity and fast $O(1/T^2)$\nconvergence rate. Extensive experiments demonstrate that the proposed algorithm\nis much faster than Soft-Impute and other state-of-the-art matrix and tensor\ncompletion algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.05487v2"
    },
    {
        "title": "Report on Two-Step Knowledge-Aided Iterative ESPRIT Algorithm",
        "authors": [
            "R. C. de Lamare"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this work, we propose a subspace-based algorithm for direction-of-arrival\n(DOA) estimation, referred to as two-step knowledge-aided iterative estimation\nof signal parameters via rotational invariance techniques (ESPRIT) method\n(Two-Step KAI-ESPRIT), which achieves more accurate estimates than those of\nprior art. The proposed Two-Step KAI-ESPRIT improves the estimation of the\ncovariance matrix of the input data by incorporating prior knowledge of signals\nand by exploiting knowledge of the structure of the covariance matrix and its\nperturbation terms. Simulation results illustrate the improvement achieved by\nthe proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.10523v1"
    },
    {
        "title": "A stable and optimally convergent LaTIn-Cut Finite Element Method for\n  multiple unilateral contact problems",
        "authors": [
            "Susanne Claus",
            "Pierre Kerfriden"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper, we propose a novel unfitted finite element method for the\nsimulation of multiple body contact. The computational mesh is generated\nindependently of the geometry of the interacting solids, which can be\narbitrarily complex. The key novelty of the approach is the combination of\nelements of the CutFEM technology, namely the enrichment of the solution field\nvia the definition of overlapping fictitious domains with a dedicated\npenalty-type regularisation of discrete operators, and the LaTIn hybrid-mixed\nformulation of complex interface conditions. Furthermore, the novel P1-P1\ndiscretisation scheme that we propose for the unfitted LaTIn solver is shown to\nbe stable, robust and optimally convergent with mesh refinement. Finally, the\npaper introduces a high-performance 3D level-set/CutFEM framework for the\nversatile and robust solution of contact problems involving multiple bodies of\ncomplex geometries, with more than two bodies interacting at a single point.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01977v1"
    },
    {
        "title": "Eigenvalues of symmetric tridiagonal interval matrices revisited",
        "authors": [
            "Milan Hladík"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this short note, we present a novel method for computing exact lower and\nupper bounds of eigenvalues of a symmetric tridiagonal interval matrix.\nCompared to the known methods, our approach is fast, simple to present and to\nimplement, and avoids any assumptions. Our construction explicitly yields those\nmatrices for which particular lower and upper bounds are attained.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.03670v2"
    },
    {
        "title": "A Fast Implementation of Singular Value Thresholding Algorithm using\n  Recycling Rank Revealing Randomized Singular Value Decomposition",
        "authors": [
            "Yaohang Li",
            "Wenjian Yu"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper, we present a fast implementation of the Singular Value\nThresholding (SVT) algorithm for matrix completion. A rank-revealing randomized\nsingular value decomposition (R3SVD) algorithm is used to adaptively carry out\npartial singular value decomposition (SVD) to fast approximate the SVT operator\ngiven a desired, fixed precision. We extend the R3SVD algorithm to a recycling\nrank revealing randomized singular value decomposition (R4SVD) algorithm by\nreusing the left singular vectors obtained from the previous iteration as the\napproximate basis in the current iteration, where the computational cost for\npartial SVD at each SVT iteration is significantly reduced. A simulated\nannealing style cooling mechanism is employed to adaptively adjust the low-rank\napproximation precision threshold as SVT progresses. Our fast SVT\nimplementation is effective in both large and small matrices, which is\ndemonstrated in matrix completion applications including image recovery and\nmovie recommendation system.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.05528v1"
    },
    {
        "title": "Positive Semidefiniteness and Positive Definiteness of a Linear\n  Parametric Interval Matrix",
        "authors": [
            "Milan Hladík"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We consider a symmetric matrix, the entries of which depend linearly on some\nparameters. The domains of the parameters are compact real intervals. We\ninvestigate the problem of checking whether for each (or some) setting of the\nparameters, the matrix is positive definite (or positive semidefinite). We\nstate a characterization in the form of equivalent conditions, and also propose\nsome computationally cheap sufficient\\,/\\,necessary conditions. Our results\nextend the classical results on positive (semi-)definiteness of interval\nmatrices. They may be useful for checking convexity or non-convexity in global\noptimization methods based on branch and bound framework and using interval\ntechniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.05782v2"
    },
    {
        "title": "Multiscale Analysis for Higher-order Tensors",
        "authors": [
            "Alp Ozdemir",
            "Ali Zare",
            "Mark A. Iwen",
            "Selin Aviyente"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The widespread use of multisensor technology and the emergence of big\ndatasets have created the need to develop tools to reduce, approximate, and\nclassify large and multimodal data such as higher-order tensors. While early\napproaches focused on matrix and vector based methods to represent these\nhigher-order data, more recently it has been shown that tensor decomposition\nmethods are better equipped to capture couplings across their different modes.\nFor these reasons, tensor decomposition methods have found applications in many\ndifferent signal processing problems including dimensionality reduction, signal\nseparation, linear regression, feature extraction, and classification. However,\nmost of the existing tensor decomposition methods are based on the principle of\nfinding a low-rank approximation in a linear subspace structure, where the\ndefinition of the rank may change depending on the particular decomposition.\nSince many datasets are not necessarily low-rank in a linear subspace, this\noften results in high approximation errors or low compression rates. In this\npaper, we introduce a new adaptive, multi-scale tensor decomposition method for\nhigher order data inspired by hybrid linear modeling and subspace clustering\ntechniques. In particular, we develop a multi-scale higher-order singular value\ndecomposition (MS-HoSVD) approach where a given tensor is first permuted and\nthen partitioned into several sub-tensors each of which can be represented as a\nlow-rank tensor with increased representational efficiency. The proposed\napproach is evaluated for dimensionality reduction and classification for\nseveral different real-life tensor signals with promising results.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.08578v3"
    },
    {
        "title": "Two-component domain decomposition scheme with overlapping subdomains\n  for parabolic equations",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  An iteration-free method of domain decomposition is considered for\napproximate solving a boundary value problem for a second-order parabolic\nequation. A standard approach to constructing domain decomposition schemes is\nbased on a partition of unity for the domain under the consideration. Here a\nnew general approach is proposed for constructing domain decomposition schemes\nwith overlapping subdomains based on indicator functions of subdomains. The\nbasic peculiarity of this method is connected with a representation of the\nproblem operator as the sum of two operators, which are constructed for two\nseparate subdomains with the subtraction of the operator that is associated\nwith the intersection of the subdomains. There is developed a two-component\nfactorized scheme, which can be treated as a generalization of the standard\nAlternating Direction Implicit (ADI) schemes to the case of a special\nthree-component splitting. There are obtained conditions for the unconditional\nstability of regionally additive schemes constructed using indicator functions\nof subdomains. Numerical results are presented for a model two-dimensional\nproblem.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.03434v1"
    },
    {
        "title": "Cache-oblivious Matrix Multiplication for Exact Factorisation",
        "authors": [
            "Fatima K. Abu Salem",
            "Mira Al Arab"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We present a cache-oblivious adaptation of matrix multiplication to be\nincorporated in the parallel TU decomposition for rectangular matrices over\nfinite fields, based on the Morton-hybrid space-filling curve representation.\nTo realise this, we introduce the concepts of alignment and containment of\nsub-matrices under the Morton-hybrid layout. We redesign the decompositions\nwithin the recursive matrix multiplication to force the base case to avoid all\njumps in address space, at the expense of extra recursive matrix multiplication\n(MM) calls. We show that the resulting cache oblivious adaptation has low span,\nand our experiments demonstrate that its sequential evaluation order\ndemonstrates orders of magnitude improvement in run-time, despite the recursion\noverhead.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.04807v1"
    },
    {
        "title": "Fundamental mode exact schemes for unsteady problems",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The problem of increasing the accuracy of an approximate solution is\nconsidered for boundary value problems for parabolic equations. For ordinary\ndifferential equations (ODEs), nonstandard finite difference schemes are in\ncommon use for this problem. They are based on a modification of standard\ndiscretizations of time derivatives and, in some cases, allow to obtain the\nexact solution of problems. For multidimensional problems, we can consider the\nproblem of increasing the accuracy only for the most important components of\nthe approximate solution. In the present work, new unconditionally stable\nschemes for parabolic problems are constructed, which are exact for the\nfundamental mode. Such two-level schemes are designed via a modification of\nstandard schemes with weights using Pad\\'{e} approximations. Numerical results\nobtained for a model problem demonstrate advantages of the proposed fundamental\nmode exact schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.07010v1"
    },
    {
        "title": "Learning Efficient Tensor Representations with Ring Structure Networks",
        "authors": [
            "Qibin Zhao",
            "Masashi Sugiyama",
            "Andrzej Cichocki"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Tensor train (TT) decomposition is a powerful representation for high-order\ntensors, which has been successfully applied to various machine learning tasks\nin recent years. However, since the tensor product is not commutative,\npermutation of data dimensions makes solutions and TT-ranks of TT decomposition\ninconsistent. To alleviate this problem, we propose a permutation symmetric\nnetwork structure by employing circular multilinear products over a sequence of\nlow-order core tensors. This network structure can be graphically interpreted\nas a cyclic interconnection of tensors, and thus we call it tensor ring (TR)\nrepresentation. We develop several efficient algorithms to learn TR\nrepresentation with adaptive TR-ranks by employing low-rank approximations.\nFurthermore, mathematical properties are investigated, which enables us to\nperform basic operations in a computationally efficiently way by using TR\nrepresentations. Experimental results on synthetic signals and real-world\ndatasets demonstrate that the proposed TR network is more expressive and\nconsistently informative than existing TT networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.08286v3"
    },
    {
        "title": "Fully reliable error control for evolutionary problems",
        "authors": [
            "Bärbel Holm",
            "Svetlana Matculevich"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This work is focused on the application of functional-type a posteriori error\nestimates and corresponding indicators to a class of time-dependent problems.\nWe consider the algorithmic part of their derivation and implementation and\nalso discuss the numerical properties of these bounds that comply with obtained\nnumerical results. This paper examines two different methods of approximate\nsolution reconstruction for evolutionary models, i.e., a time-marching\ntechnique and a space-time approach. The first part of the study presents an\nalgorithm for global minimization of the majorant on each of discretization\ntime-cylinders (time-slabs), the effectiveness of this algorithm is confirmed\nby extensive numerical tests. In the second part of the publication, the\napplication of functional error estimates is discussed with respect to a\nspace-time approach. It is followed by a set of extensive numerical tests that\ndemonstrate the efficiency of proposed error control method.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.08614v1"
    },
    {
        "title": "Successive Rank-One Approximations for Nearly Orthogonally Decomposable\n  Symmetric Tensors",
        "authors": [
            "Cun Mu",
            "Daniel Hsu",
            "Donald Goldfarb"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Many idealized problems in signal processing, machine learning and statistics\ncan be reduced to the problem of finding the symmetric canonical decomposition\nof an underlying symmetric and orthogonally decomposable (SOD) tensor. Drawing\ninspiration from the matrix case, the successive rank-one approximations (SROA)\nscheme has been proposed and shown to yield this tensor decomposition exactly,\nand a plethora of numerical methods have thus been developed for the tensor\nrank-one approximation problem. In practice, however, the inevitable errors\n(say) from estimation, computation, and modeling necessitate that the input\ntensor can only be assumed to be a nearly SOD tensor---i.e., a symmetric tensor\nslightly perturbed from the underlying SOD tensor. This article shows that even\nin the presence of perturbation, SROA can still robustly recover the symmetric\ncanonical decomposition of the underlying tensor. It is shown that when the\nperturbation error is small enough, the approximation errors do not accumulate\nwith the iteration number. Numerical results are presented to support the\ntheoretical findings.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.10404v1"
    },
    {
        "title": "Digit Serial Methods with Applications to Division and Square Root (with\n  mechanically checked correctness proofs)",
        "authors": [
            "Warren E. Ferguson Jr",
            "Jesse Bingham",
            "Levent Erkök",
            "John R. Harrison",
            "Joe Leslie-Hurd"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We present a generic digit serial method (DSM) to compute the digits of a\nreal number $V$ . Bounds on these digits, and on the errors in the associated\nestimates of $V$ formed from these digits, are derived. To illustrate our\nresults, we derive such bounds for a parameterized family of high-radix\nalgorithms for division and square root. These bounds enable a DSM designer to\ndetermine, for example, whether a given choice of parameters allows rapid\nformation and rounding of its approximation to $V$. All our claims are\nmechanically verified using the HOL-Light theorem prover, and are included in\nthe appendix with commentary.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.00140v1"
    },
    {
        "title": "RAPToR: A Resampling Algorithm for Pseudo-Polar based Tomographic\n  Reconstruction",
        "authors": [
            "Shahar Tsiper",
            "Yonina C. Eldar"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We propose a stable and fast reconstruction technique for parallel-beam (PB)\ntomographic X-ray imaging, relying on the discrete pseudo-polar (PP) Radon\ntransform. Our main contribution is a resampling method, based on modern\nsampling theory, that transforms the acquired PB measurements to a PP grid. The\nresampling process is both fast and accurate, and in addition, simultaneously\ndenoises the measurements, by exploiting geometrical properties of the\ntomographic scan. The transformed measurements are then reconstructed using an\niterative solver with total variation (TV) regularization. We show that\nreconstructing from measurements on the PP grid, leads to improved recovery,\ndue to the inherent stability and accuracy of the PP Radon transform, compared\nwith the PB Radon transform. We also demonstrate recovery from a reduced number\nof PB acquisition angles, and high noise levels. Our approach is shown to\nachieve superior results over other state-of-the-art solutions, that operate\ndirectly on the given PB measurements. The proposed method can benefit fan-beam\nand/or cone-beam projections by coupling it with a rebinning process.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.05163v2"
    },
    {
        "title": "On the repeated inversion of a covariance matrix",
        "authors": [
            "M. de Jong"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In many cases, the values of some model parameters are determined by\nmaximising the likelihood of a set of data points given the parameter values.\nThe presence of outliers in the data and correlations between data points\ncomplicate this procedure. An efficient procedure for the elimination of\noutliers is presented which takes the correlations between data points into\naccount.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07622v1"
    },
    {
        "title": "Fast, Accurate, and Scalable Method for Sparse Coupled Matrix-Tensor\n  Factorization",
        "authors": [
            "Dongjin Choi",
            "Jun-Gi Jang",
            "U Kang"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  How can we capture the hidden properties from a tensor and a matrix data\nsimultaneously in a fast, accurate, and scalable way? Coupled matrix-tensor\nfactorization (CMTF) is a major tool to extract latent factors from a tensor\nand matrices at once. Designing an accurate and efficient CMTF method has\nbecome more crucial as the size and dimension of real-world data are growing\nexplosively. However, existing methods for CMTF suffer from lack of accuracy,\nslow running time, and limited scalability. In this paper, we propose S3CMTF, a\nfast, accurate, and scalable CMTF method. S3CMTF achieves high speed by\nexploiting the sparsity of real-world tensors, and high accuracy by capturing\ninter-relations between factors. Also, S3CMTF accomplishes additional speed-up\nby lock-free parallel SGD update for multi-core shared memory systems. We\npresent two methods, S3CMTF-naive and S3CMTF-opt. S3CMTF-naive is a basic\nversion of S3CMTF, and S3CMTF-opt improves its speed by exploiting intermediate\ndata. We theoretically and empirically show that S3CMTF is the fastest,\noutperforming existing methods. Experimental results show that S3CMTF is 11~43\ntimes faster, and 2.1~4.1 times more accurate than existing methods. S3CMTF\nshows linear scalability on the number of data entries and the number of cores.\nIn addition, we apply S3CMTF to Yelp recommendation tensor data coupled with 3\nadditional matrices to discover interesting properties.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.08640v6"
    },
    {
        "title": "Phase retrieval from noisy data based on sparse approximation of object\n  phase and amplitude",
        "authors": [
            "Vladimir Katkovnik"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  A variational approach to reconstruction of phase and amplitude of a\ncomplex-valued object from Poissonian intensity observations is developed. The\nobservation model corresponds to the typical optical setups with a phase\nmodulation of wavefronts. The transform domain sparsity is applied for the\namplitude and phase modeling. It is demonstrated that this modeling results in\nthe essential advantage of the developed algorithm for heavily noisy\nobservations corresponding to a short exposure time in optical experiments. We\nconsider also two simplified versions of this algorithm where the sparsity\nmodeling of phase and amplitude is omitted. In the simulation study we compare\nthe developed algorithms versus the Gerchberg-Saxton and truncation Wirtinger\nflow algorithms. The latter algorithm being the maximum likelihood based is the\nstate-of-the-art for the phase retrieval from Poissonian observations. For\nnoisy and very noisy observations the proposed algorithm demonstrates a\nvaluable advantage.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.01071v1"
    },
    {
        "title": "Complete Pascal Interpolation Scheme For Approximating The Geometry Of A\n  Quadrilateral Element",
        "authors": [
            "Sulaiman Y. Abo Diab"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This paper applies a complete parametric set for approximating the geometry\nof a quadrilateral element. The approximation basis used is a complete Pascal\npolynomial of second order with six free parameters. The interpolation\nprocedure is a natural interpolation scheme. The six free parameters are\ndetermined using the natural coordinates of the four nodal points (vertices) of\nthe quadrilateral element and the two intersections points of the lines\ncrossing every two opposite edges (poles). The presented scheme recovers the\nwell known Lagrangian interpolation scheme, when every two opposite edges are\nparallel. A third order Pascal interpolation scheme is also presented. The four\nmidpoints of the four edges in addition to the six nodal point from the second\norder case are used as significant nodal points. It is expected to reflect the\ngeometry properties better since the shape functions are complete\n",
        "pdf_link": "http://arxiv.org/pdf/1709.04765v1"
    },
    {
        "title": "Convergence characteristics of the generalized residual cutting method",
        "authors": [
            "Toshihiko Abe",
            "Anthony Theodore Chronopoulos"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The residual cutting (RC) method has been proposed for efficiently solving\nlinear equations obtained from elliptic partial differential equations. Based\non the RC, we have introduced the generalized residual cutting (GRC) method,\nwhich can be applied to general sparse matrix problems. In this paper, we study\nthe mathematics of the GRC algorithm and and prove it is a Krylov subspace\nmethod. Moreover, we show that it is deeply related to the conjugate residual\n(CR) method and that GRC becomes equivalent to CR for symmetric matrices. Also,\nin numerical experiments, GRC shows more robust convergence and needs less\nmemory compared to GMRES, for significantly larger matrix sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.07184v3"
    },
    {
        "title": "Best Rank-One Tensor Approximation and Parallel Update Algorithm for CPD",
        "authors": [
            "Anh-Huy Phan",
            "Petr Tichavský",
            "Andrzej Cichocki"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  A novel algorithm is proposed for CANDECOMP/PARAFAC tensor decomposition to\nexploit best rank-1 tensor approximation. Different from the existing\nalgorithms, our algorithm updates rank-1 tensors simultaneously in parallel. In\norder to achieve this, we develop new all-at-once algorithms for best rank-1\ntensor approximation based on the Levenberg-Marquardt method and the rotational\nupdate. We show that the LM algorithm has the same complexity of first-order\noptimisation algorithms, while the rotational method leads to solving the best\nrank-1 approximation of tensors of size $2 \\times 2 \\times \\cdots \\times 2$. We\nderive a closed-form expression of the best rank-1 tensor of $2\\times 2 \\times\n2$ tensors and present an ALS algorithm which updates 3 component at a time for\nhigher order tensors. The proposed algorithm is illustrated in decomposition of\ndifficult tensors which are associated with multiplication of two matrices.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08336v1"
    },
    {
        "title": "Error Preserving Correction for CPD and Bounded-Norm CPD",
        "authors": [
            "Anh-Huy Phan",
            "Petr Tichavský",
            "Andrzej Cichocki"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In CANDECOMP/PARAFAC tensor decomposition, degeneracy often occurs in some\ndifficult scenarios, e.g., when the rank exceeds the tensor dimension, or when\nthe loading components are highly collinear in several or all modes, or when\nCPD does not have an optimal solution. In such the cases, norms of some rank-1\nterms become significantly large and cancel each other. This makes algorithms\ngetting stuck in local minima while running a huge number of iterations does\nnot improve the decomposition. In this paper, we propose an error preservation\ncorrection method to deal with such problem. Our aim is to seek a new tensor\nwhose norms of rank-1 tensor components are minimised in an optimization\nproblem, while it preserves the approximation error. An alternating correction\nalgorithm and an all-atone algorithm have been developed for the problem. In\naddition, we propose a novel CPD with a bound constraint on a norm of the\nrank-one tensors. The method can be useful for decomposing tensors that cannot\nbe analyzed by traditional algorithms, such as tensors corresponding to the\nmatrix multiplication.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08349v1"
    },
    {
        "title": "V-cycle multigrid algorithms for discontinuous Galerkin methods on\n  non-nested polytopic meshes",
        "authors": [
            "P. F. Antonietti",
            "G. Pennesi"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper we analyse the convergence properties of V-cycle multigrid\nalgorithms for the numerical solution of the linear system of equations arising\nfrom discontinuous Galerkin discretization of second-order elliptic partial\ndifferential equations on polytopal meshes. Here, the sequence of spaces that\nstands at the basis of the multigrid scheme is possibly non nested and is\nobtained based on employing agglomeration with possible edge/face coarsening.\nWe prove that the method converges uniformly with respect to the granularity of\nthe grid and the polynomial approximation degree p, provided that the number of\nsmoothing steps, which depends on p, is chosen sufficiently large.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.09147v2"
    },
    {
        "title": "Preconditioners for Saddle Point Problems on Truncated Domains in Phase\n  Separation Modelling",
        "authors": [
            "Pawan Kumar"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The discretization of Cahn-Hilliard equation with obstacle potential leads to\na block 2 by 2 non-linear system, where the p1, 1q block has a non-linear and\nnon-smooth term. Recently a globally convergent Newton Schur method was\nproposed for the non-linear Schur complement corresponding to this non-linear\nsystem. The solver may be seen as an inexact Uzawa method which has the falvour\nof an active set method in the sense that the active sets are first identified\nby solving a quadratic obstacle problem corresponding to the p1, 1q block of\nthe block 2 by 2 nonlinear system, and a new decent direction is obtained after\ndiscarding the active set region. The problem becomes linear on nonactive set,\nand corresponds to solving a linear saddle point problem on truncated domains.\nFor solving the quadratic obstacle problem, various optimal multigrid like\nmethods have been proposed. In this paper solvers for the truncated saddle\npoint problem is considered. Three preconditioners are considered, two of them\nhave block diagonal structure, and the third one has block tridiagonal\nstructure. One of the block diagonal preconditioners is obtained by adding\ncertain scaling of stiffness and mass matrices, whereas, the remaining two\ninvolves Schur complement. Eigenvalue bound and condition number estimates are\nderived for the preconditioned untruncated problem. It is shown that the\nextreme eigenvalues of the preconditioned truncated system remain bounded by\nthe extreme eigenvalues of the preconditioned untruncated system. Numerical\nexperiments confirm the optimality of the solvers.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10339v3"
    },
    {
        "title": "A Comparison Between Laguerre, Hermite, and Sinc Orthogonal Functions",
        "authors": [
            "Fattaneh Bayatbabolghani",
            "Kourosh Parand"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  A series of problems in different fields such as physics and chemistry are\nmodeled by differential equations. Differential equations are divided into\npartial differential equations and ordinary differential equations which can be\nlinear or nonlinear. One approach to solve those kinds of equations is using\northogonal functions into spectral methods. In this paper, we firstly describe\nLaguerre, Hermite, and Sinc orthogonal functions. Secondly, we select three\ninteresting problems which are modeled as differential equations over the\ninterval $[0, +\\infty)$. Then, we use the collocation method as a spectral\nmethod for solving those selected problems and compare the performance of\nLaguerre, Hermite, and Sinc orthogonal functions in solving those types of\nequations.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10352v1"
    },
    {
        "title": "Analyzing the Approximation Error of the Fast Graph Fourier Transform",
        "authors": [
            "Luc LeMagoarou",
            "Nicolas Tremblay",
            "Rémi Gribonval"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The graph Fourier transform (GFT) is in general dense and requires O(n^2)\ntime to compute and O(n^2) memory space to store. In this paper, we pursue our\nprevious work on the approximate fast graph Fourier transform (FGFT). The FGFT\nis computed via a truncated Jacobi algorithm, and is defined as the product of\nJ Givens rotations (very sparse orthogonal matrices). The truncation parameter,\nJ, represents a trade-off between precision of the transform and time of\ncomputation (and storage space). We explore further this trade-off and study,\non different types of graphs, how is the approximation error distributed along\nthe spectrum.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.00386v2"
    },
    {
        "title": "Numerical time integration of lumped parameter systems governed by\n  implicit constitutive relations",
        "authors": [
            "Saeid Karimi"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Time-integration for lumped parameter systems obeying implicit Bingham-Kelvin\nconstitutive models is studied. The governing system of equations describing\nthe lumped parameter system is a non-linear differential-algebraic equation and\nneeds to be solved numerically. The response of this system is non-smooth and\nthe kinematic variables can not be written in terms of the dynamic variables,\nexplicitly. To gain insight into numerical time-integration of this system, a\nnew time-integration scheme based on the trapezoidal method is derived. This\nmethod relies on two independent parameters to adjust for damping and is\nstable. Numerical examples showcase the performance of the proposed\ntime-integration method and compare it to a benchmark algorithm. Under this\nscheme, implicit-explicit integration of the governing equations is possible.\nUsing this new method, the limitations of the trapezoidal time-integration\nmethods when applied to a non-smooth differential-algebraic equation are\nhighlighted.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.06352v1"
    },
    {
        "title": "Two-level schemes for the advection equation",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The advection equation is the basis for mathematical models of continuum\nmechanics. In the approximate solution of nonstationary problems it is\nnecessary to inherit main properties of the conservatism and monotonicity of\nthe solution. In this paper, the advection equation is written in the symmetric\nform, where the advection operator is the half-sum of advection operators in\nconservative (divergent) and non-conservative (characteristic) forms. The\nadvection operator is skew-symmetric. Standard finite element approximations in\nspace are used. The standart explicit two-level scheme for the advection\nequation is absolutly unstable. New conditionally stable regularized schemes\nare constructed, on the basis of the general theory of stability\n(well-posedness) of operator-difference schemes, the stability conditions of\nthe explicit Lax-Wendroff scheme are established. Unconditionally stable and\nconservative schemes are implicit schemes of the second (Crank-Nicolson scheme)\nand fourth order. The conditionally stable implicit Lax-Wendroff scheme is\nconstructed. The accuracy of the investigated explicit and implicit two-level\nschemes for an approximate solution of the advection equation is illustrated by\nthe numerical results of a model two-dimensional problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.07031v1"
    },
    {
        "title": "Fast and Stable Pascal Matrix Algorithms",
        "authors": [
            "Samuel F. Potter",
            "Ramani Duraiswami"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper, we derive a family of fast and stable algorithms for\nmultiplying and inverting $n \\times n$ Pascal matrices that run in $O(n log^2\nn)$ time and are closely related to De Casteljau's algorithm for B\\'ezier curve\nevaluation. These algorithms use a recursive factorization of the triangular\nPascal matrices and improve upon the cripplingly unstable $O(n log n)$ fast\nFourier transform-based algorithms which involve a Toeplitz matrix\nfactorization. We conduct numerical experiments which establish the speed and\nstability of our algorithm, as well as the poor performance of the Toeplitz\nfactorization algorithm. As an example, we show how our formulation relates to\nB\\'ezier curve evaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08453v1"
    },
    {
        "title": "An Overview of Polynomially Computable Characteristics of Special\n  Interval Matrices",
        "authors": [
            "Milan Hladík"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  It is well known that many problems in interval computation are intractable,\nwhich restricts our attempts to solve large problems in reasonable time. This\ndoes not mean, however, that all problems are computationally hard. Identifying\npolynomially solvable classes thus belongs to important current trends. The\npurpose of this paper is to review some of such classes. In particular, we\nfocus on several special interval matrices and investigate their convenient\nproperties. We consider tridiagonal matrices, {M,H,P,B}-matrices, inverse\nM-matrices, inverse nonnegative matrices, nonnegative matrices, totally\npositive matrices and some others. We focus in particular on computing the\nrange of the determinant, eigenvalues, singular values, and selected norms.\nWhenever possible, we state also formulae for determining the inverse matrix\nand the hull of the solution set of an interval system of linear equations. We\nsurvey not only the known facts, but we present some new views as well.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08732v1"
    },
    {
        "title": "An Overflow Free Fixed-point Eigenvalue Decomposition Algorithm: Case\n  Study of Dimensionality Reduction in Hyperspectral Images",
        "authors": [
            "Bibek Kabi",
            "Anand S Sahadevan",
            "Tapan Pradhan"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We consider the problem of enabling robust range estimation of eigenvalue\ndecomposition (EVD) algorithm for a reliable fixed-point design. The simplicity\nof fixed-point circuitry has always been so tempting to implement EVD algo-\nrithms in fixed-point arithmetic. Working towards an effective fixed-point\ndesign, integer bit-width allocation is a significant step which has a crucial\nimpact on accuracy and hardware efficiency. This paper investigates the\nshortcomings of the existing range estimation methods while deriving bounds for\nthe variables of the EVD algorithm. In light of the circumstances, we introduce\na range estimation approach based on vector and matrix norm properties together\nwith a scaling procedure that maintains all the assets of an analytical method.\nThe method could derive robust and tight bounds for the variables of EVD\nalgorithm. The bounds derived using the proposed approach remain same for any\ninput matrix and are also independent of the number of iterations or size of\nthe problem. Some benchmark hyperspectral data sets have been used to evaluate\nthe efficiency of the proposed technique. It was found that by the proposed\nrange estimation approach, all the variables generated during the computation\nof Jacobi EVD is bounded within $\\pm1$.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.10600v1"
    },
    {
        "title": "Conservative model reduction for finite-volume models",
        "authors": [
            "Kevin Carlberg",
            "Youngsoo Choi",
            "Syuzanna Sargsyan"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This work proposes a method for model reduction of finite-volume models that\nguarantees the resulting reduced-order model is conservative, thereby\npreserving the structure intrinsic to finite-volume discretizations. The\nproposed reduced-order models associate with optimization problems\ncharacterized by a minimum-residual objective function and nonlinear equality\nconstraints that explicitly enforce conservation over subdomains. Conservative\nGalerkin projection arises from formulating this optimization problem at the\ntime-continuous level, while conservative least-squares Petrov--Galerkin (LSPG)\nprojection associates with a time-discrete formulation. We equip these\napproaches with hyper-reduction techniques in the case of nonlinear flux and\nsource terms, and also provide approaches for handling infeasibility. In\naddition, we perform analyses that include deriving conditions under which\nconservative Galerkin and conservative LSPG are equivalent, as well as deriving\na posteriori error bounds. Numerical experiments performed on a parameterized\nquasi-1D Euler equation demonstrate the ability of the proposed method to\nensure not only global conservation, but also significantly lower state-space\nerrors than nonconservative reduced-order models such as standard Galerkin and\nLSPG projection.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.11550v3"
    },
    {
        "title": "Sobol Tensor Trains for Global Sensitivity Analysis",
        "authors": [
            "Rafael Ballester-Ripoll",
            "Enrique G. Paredes",
            "Renato Pajarola"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Sobol indices are a widespread quantitative measure for variance-based global\nsensitivity analysis, but computing and utilizing them remains challenging for\nhigh-dimensional systems. We propose the tensor train decomposition (TT) as a\nunified framework for surrogate modeling and global sensitivity analysis via\nSobol indices. We first overview several strategies to build a TT surrogate of\nthe unknown true model using either an adaptive sampling strategy or a\npredefined set of samples. We then introduce and derive the Sobol tensor train,\nwhich compactly represents the Sobol indices for all possible joint variable\ninteractions which are infeasible to compute and store explicitly. Our\nformulation allows efficient aggregation and subselection operations: we are\nable to obtain related indices (closed, total, and superset indices) at\nnegligible cost. Furthermore, we exploit an existing global optimization\nprocedure within the TT framework for variable selection and model analysis\ntasks. We demonstrate our algorithms with two analytical engineering models and\na parallel computing simulation data set.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.00233v1"
    },
    {
        "title": "Tensor Approximation of Advanced Metrics for Sensitivity Analysis",
        "authors": [
            "Rafael Ballester-Ripoll",
            "Enrique G. Paredes",
            "Renato Pajarola"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Following up on the success of the analysis of variance (ANOVA) decomposition\nand the Sobol indices (SI) for global sensitivity analysis, various related\nquantities of interest have been defined in the literature including the\neffective and mean dimensions, the dimension distribution, and the Shapley\nvalues. Such metrics combine up to exponential numbers of SI in different ways\nand can be of great aid in uncertainty quantification and model interpretation\ntasks, but are computationally challenging. We focus on surrogate based\nsensitivity analysis for independently distributed variables, namely via the\ntensor train (TT) decomposition. This format permits flexible and scalable\nsurrogate modeling and can efficiently extract all SI at once in a compressed\nTT representation of their own. Based on this, we contribute a range of novel\nalgorithms that compute more advanced sensitivity metrics by selecting and\naggregating certain subsets of SI in the tensor compressed domain. Drawing on\nan interpretation of the TT model in terms of deterministic finite automata, we\nare able to construct explicit auxiliary TT tensors that encode exactly all\nnecessary index selection masks. Having both the SI and the masks in the TT\nformat allows efficient computation of all aforementioned metrics, as we\ndemonstrate in a number of example models.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.01633v1"
    },
    {
        "title": "Monotone Difference Schemes for Convection-Dominated Diffusion-Reaction\n  Equations Based on Quadratic Spline",
        "authors": [
            "O. Stelia",
            "L. Potapenko",
            "I. Sirenko"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  A three-point monotone difference scheme is proposed for solving a\none-dimensional non-stationary convection-diffusion-reaction equation with\nvariable coefficients. The scheme is based on a parabolic spline and allows to\nlinearly reproduce the numerical solution of the boundary value problem over\nthe integral segment in the form of the function which continuous with its\nfirst derivative. The constructed difference scheme give a highly effective\ntool for solving problems with a small parameter at the older derivative in a\nwide range of output data of the problem. In the test case, numerical and exact\nsolutions of the problem are compared with the significant dominance of the\nconvective term of the equation over the diffusion. Numerous calculations\nshowed the high efficiency of the new monotonous scheme developed.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.08560v1"
    },
    {
        "title": "Approximate solutions to large nonsymmetric differential Riccati\n  problems with applications to transport theory",
        "authors": [
            "V. Angelova",
            "M. Hached",
            "K. Jbilou"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In the present paper, we consider large scale nonsymmetric differential\nmatrix Riccati equations with low rank right hand sides. These matrix equations\nappear in many applications such as control theory, transport theory, applied\nprobability and others. We show how to apply Krylov-type methods such as the\nextended block Arnoldi algorithm to get low rank approximate solutions. The\ninitial problem is projected onto small subspaces to get low dimensional\nnonsymmetric differential equations that are solved using the exponential\napproximation or via other integration schemes such as Backward Differentiation\nFormula (BDF) or Rosenbrok method. We also show how these technique could be\neasily used to solve some problems from the well known transport equation. Some\nnumerical experiments are given to illustrate the application of the proposed\nmethods to large-scale problems\n",
        "pdf_link": "http://arxiv.org/pdf/1801.01291v2"
    },
    {
        "title": "A generalized matrix Krylov subspace method for TV regularization",
        "authors": [
            "A. Bentbib",
            "M. El Guide",
            "K. Jbilou"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  This paper presents an efficient algorithm to solve total variation (TV)\nregularizations of images contaminated by a both blur and noise. The\nunconstrained structure of the problem suggests that one can solve a\nconstrained optimization problem by transforming the original unconstrained\nminimization problem to an equivalent constrained minimization one. An\naugmented Lagrangian method is developed to handle the constraints when the\nmodel is given with matrix variables, and an alternating direction method (ADM)\nis used to iteratively find solutions. The solutions of some sub-problems are\nbelonging to subspaces generated by application of successive orthogonal\nprojections onto a class of generalized matrix Krylov subspaces of increasing\ndimension.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.03527v1"
    },
    {
        "title": "Improving the accuracy of the fast inverse square root algorithm",
        "authors": [
            "Cezary J. Walczyk",
            "Leonid V. Moroz",
            "Jan L. Cieśliński"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We present improved algorithms for fast calculation of the inverse square\nroot for single-precision floating-point numbers. The algorithms are much more\naccurate than the famous fast inverse square root algorithm and have the same\nor similar computational cost. The main idea of our work consists in modifying\nthe Newton-Raphson method and demanding that the maximal error is as small as\npossible. Such modification is possible when the distribution of Newton-Raphson\ncorrections is not symmetric (e.g., if they are non-positive functions).\n",
        "pdf_link": "http://arxiv.org/pdf/1802.06302v1"
    },
    {
        "title": "Arbitrarily Substantial Number Representation for Complex Number",
        "authors": [
            "Satrya Fajri Pratama",
            "Azah Kamilah Muda",
            "Yun-Huoy Choo"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Researchers are often perplexed when their machine learning algorithms are\nrequired to deal with complex number. Various strategies are commonly employed\nto project complex number into real number, although it is frequently\nsacrificing the information contained in the complex number. This paper\nproposes a new method and four techniques to represent complex number as real\nnumber, without having to sacrifice the information contained. The proposed\ntechniques are also capable of retrieving the original complex number from the\nrepresenting real number, with little to none of information loss. The\npromising applicability of the proposed techniques has been demonstrated and\nworth to receive further exploration in representing the complex number.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.08245v3"
    },
    {
        "title": "A Decomposition Algorithm for the Sparse Generalized Eigenvalue Problem",
        "authors": [
            "Ganzhao Yuan",
            "Li Shen",
            "Wei-Shi Zheng"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The sparse generalized eigenvalue problem arises in a number of standard and\nmodern statistical learning models, including sparse principal component\nanalysis, sparse Fisher discriminant analysis, and sparse canonical correlation\nanalysis. However, this problem is difficult to solve since it is NP-hard. In\nthis paper, we consider a new decomposition method to tackle this problem.\nSpecifically, we use random or/and swapping strategies to find a working set\nand perform global combinatorial search over the small subset of variables. We\nconsider a bisection search method and a coordinate descent method for solving\nthe quadratic fractional programming subproblem. In addition, we provide some\ntheoretical analysis for the proposed method. Our experiments have shown that\nthe proposed method significantly and consistently outperforms existing\nsolutions in term of accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.09303v2"
    },
    {
        "title": "Symmetric indefinite triangular factorization revealing the rank profile\n  matrix",
        "authors": [
            "Jean-Guillaume Dumas",
            "Clement Pernet"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We present a novel recursive algorithm for reducing a symmetric matrix to a\ntriangular factorization which reveals the rank profile matrix. That is, the\nalgorithm computes a factorization $\\mathbf{P}^T\\mathbf{A}\\mathbf{P} =\n\\mathbf{L}\\mathbf{D}\\mathbf{L}^T$ where $\\mathbf{P}$ is a permutation matrix,\n$\\mathbf{L}$ is lower triangular with a unit diagonal and $\\mathbf{D}$ is\nsymmetric block diagonal with $1{\\times}1$ and $2{\\times}2$ antidiagonal\nblocks. The novel algorithm requires $O(n^2r^{\\omega-2})$ arithmetic\noperations. Furthermore, experimental results demonstrate that our algorithm\ncan even be slightly more than twice as fast as the state of the art\nunsymmetric Gaussian elimination in most cases, that is it achieves\napproximately the same computational speed. By adapting the pivoting strategy\ndeveloped in the unsymmetric case, we show how to recover the rank profile\nmatrix from the permutation matrix and the support of the block-diagonal\nmatrix. There is an obstruction in characteristic $2$ for revealing the rank\nprofile matrix which requires to relax the shape of the block diagonal by\nallowing the 2-dimensional blocks to have a non-zero bottom-right coefficient.\nThis relaxed decomposition can then be transformed into a standard\n$\\mathbf{P}\\mathbf{L}\\mathbf{D}\\mathbf{L}^T\\mathbf{P}^T$ decomposition at a\nnegligible cost.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.10453v1"
    },
    {
        "title": "Memoryless scalar quantization for random frames",
        "authors": [
            "Kateryna Melnykova",
            "Ozgur Yilmaz"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Memoryless scalar quantization (MSQ) is a common technique to quantize frame\ncoefficients of signals (which are used as a model for generalized linear\nsamples), making them compatible with our digital technology. The process of\nquantization is generally not invertible, and thus one can only recover an\napproximation to the original signal from its quantized coefficients. The\nnon-linear nature of quantization makes the analysis of the corresponding\napproximation error challenging, often resulting in the use of a simplifying\nassumption, called the \"white noise hypothesis\" (WNH) that simplifies this\nanalysis. However, the WNH is known to be not rigorous and, at least in certain\ncases, not valid.\n  Given a fixed, deterministic signal, we assume that we use a random frame,\nwhose analysis matrix has independent isotropic sub-Gaussian rows, to collect\nthe measurements, which are consecutively quantized via MSQ. For this setting,\nthe numerically observed decay rate seems to agree with the prediction by the\nWNH. We rigorously establish sharp non-asymptotic error bounds without using\nthe WNH that explains the observed decay rate. Furthermore, we show that the\nreconstruction error does not necessarily diminish as redundancy increases. We\nalso extend this approach to the compressed sensing setting, obtaining rigorous\nerror bounds that agree with empirical observations, again, without resorting\nto the WNH.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.02839v2"
    },
    {
        "title": "Computational identification of the lowest space-wise dependent\n  coefficient of a parabolic equation",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In the present work, we consider a nonlinear inverse problem of identifying\nthe lowest coefficient of a parabolic equation. The desired coefficient depends\non spatial variables only. Additional information about the solution is given\nat the final time moment, i.e., we consider the final redefinition. An\niterative process is used to evaluate the lowest coefficient, where at each\niteration we solve the standard initial-boundary value problem for the\nparabolic equation. On the basis of the maximum principle for the solution of\nthe differential problem, the monotonicity of the iterative process is\nestablished along with the fact that the coefficient approaches from above. The\npossibilities of the proposed computational algorithm are illustrated by\nnumerical examples for a model two-dimensional problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03171v1"
    },
    {
        "title": "A Rank-Preserving Generalized Matrix Inverse for Consistency with\n  Respect to Similarity",
        "authors": [
            "Jeffrey Uhlmann"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  There has recently been renewed recognition of the need to understand the\nconsistency properties that must be preserved when a generalized matrix inverse\nis required. The most widely known generalized inverse, the Moore-Penrose\npseudoinverse, provides consistency with respect to orthonormal transformations\n(e.g., rotations of a coordinate frame), and a recently derived inverse\nprovides consistency with respect to diagonal transformations (e.g., a change\nof units on state variables). Another well-known and theoretically important\ngeneralized inverse is the Drazin inverse, which preserves consistency with\nrespect to similarity transformations. In this paper we note a limitation of\nthe Drazin inverse is that it does not generally preserve the rank of the\nlinear system of interest. We then introduce an alternative generalized inverse\nthat both preserves rank and provides consistency with respect to similarity\ntransformations. Lastly we provide an example and discuss experiments which\nsuggest the need for algorithms with improved numerical stability.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.07334v3"
    },
    {
        "title": "Design of High-Order Decoupled Multirate GARK Schemes",
        "authors": [
            "Arash Sarshar",
            "Steven Roberts",
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Multirate time integration methods apply different step sizes to resolve\ndifferent components of the system based on the local activity levels. This\nlocal selection of step sizes allows increased computational efficiency while\nachieving the desired solution accuracy. While the multirate idea is elegant\nand has been around for decades, multirate methods are not yet widely used in\napplications. This is due, in part, to the difficulties raised by the\nconstruction of high order multirate schemes.\n  Seeking to overcome these challenges, this work focuses on the design of\npractical high-order multirate methods using the theoretical framework of\ngeneralized additive Runge-Kutta (MrGARK) methods, which provides the generic\norder conditions and the linear and nonlinear stability analyses.\n  A set of design criteria for practical multirate methods is defined herein:\nmethod coefficients should be generic in the step size ratio, but should not\ndepend strongly on this ratio; unnecessary coupling between the fast and the\nslow components should be avoided; and the step size controllers should adjust\nboth the micro- and the macro-steps.\n  Using these criteria, we develop MrGARK schemes of up to order four that are\nexplicit-explicit (both the fast and slow component are treated explicitly),\nimplicit-explicit (implicit in the fast component and explicit in the slow\none), and explicit-implicit (explicit in the fast component and implicit in the\nslow one). Numerical experiments illustrate the performance of these new\nschemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.07716v4"
    },
    {
        "title": "Parallel Approximation of the Maximum Likelihood Estimation for the\n  Prediction of Large-Scale Geostatistics Simulations",
        "authors": [
            "Sameh Abdulah",
            "Hatem Ltaief",
            "Ying Sun",
            "Marc G. Genton",
            "David E. Keyes"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Maximum likelihood estimation is an important statistical technique for\nestimating missing data, for example in climate and environmental applications,\nwhich are usually large and feature data points that are irregularly spaced. In\nparticular, the Gaussian log-likelihood function is the \\emph{de facto} model,\nwhich operates on the resulting sizable dense covariance matrix. The advent of\nhigh performance systems with advanced computing power and memory capacity have\nenabled full simulations only for rather small dimensional climate problems,\nsolved at the machine precision accuracy. The challenge for high dimensional\nproblems lies in the computation requirements of the log-likelihood function,\nwhich necessitates ${\\mathcal O}(n^2)$ storage and ${\\mathcal O}(n^3)$\noperations, where $n$ represents the number of given spatial locations. This\nprohibitive computational cost may be reduced by using approximation techniques\nthat not only enable large-scale simulations otherwise intractable but also\nmaintain the accuracy and the fidelity of the spatial statistics model. In this\npaper, we extend the Exascale GeoStatistics software framework (i.e.,\nExaGeoStat) to support the Tile Low-Rank (TLR) approximation technique, which\nexploits the data sparsity of the dense covariance matrix by compressing the\noff-diagonal tiles up to a user-defined accuracy threshold. The underlying\nlinear algebra operations may then be carried out on this data compression\nformat, which may ultimately reduce the arithmetic complexity of the maximum\nlikelihood estimation and the corresponding memory footprint. Performance\nresults of TLR-based computations on shared and distributed-memory systems\nattain up to 13X and 5X speedups, respectively, compared to full accuracy\nsimulations using synthetic and real datasets (up to 2M), while ensuring\nadequate prediction accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.09137v2"
    },
    {
        "title": "Adaptive Mesh Refinement in Analog Mesh Computers",
        "authors": [
            "Jeff Anderson",
            "Engin Kayraklioglu",
            "Volker Sorger",
            "Tarek El-Ghazawi"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The call for efficient computer architectures has introduced a variety of\napplication-specific compute engines to the heterogeneous computing landscape.\nOne particular engine, the analog mesh computer, has been well received due to\nits ability to efficiently solve partial differential equations by eliminating\nthe iterative stages common to numerical solvers. This article introduces an\nimplementation of refinement for analog mesh computers.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10110v2"
    },
    {
        "title": "Embedding with a Rigid Substructure",
        "authors": [
            "Igor Najfeld",
            "Timothy F. Havel"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  This paper presents a new distance geometry algorithm for calculating atomic\ncoordinates from estimates of the interatomic distances, which maintains the\npositions of the atoms in a known rigid substructure. Given an $M \\times 3$\nmatrix of coordinates for the rigid substructure $\\mathbf X$, this problem\nconsists of finding the $N \\times 3$ matrix $\\mathbf Y$ that yields of global\nminimum of the so-called STRAIN, i.e. \\[ \\min_{\\mathbf Y} \\left\\|\n\\begin{bmatrix} \\mathbf{XX}^\\top & \\mathbf{XY}^\\top \\\\ \\mathbf{YX}^\\top &\n\\mathbf{YY}^\\top \\end{bmatrix} \\,-\\, \\begin{bmatrix} \\mathbf A & \\mathbf B \\\\\n\\mathbf B^\\top & \\mathbf C \\end{bmatrix} \\right\\|_{\\mathsf F}^2 ~, \\] where\n$\\mathbf A = \\mathbf{XX}^\\top$ , and $\\mathbf B, \\mathbf C$ are matrices of\ninner products calculated from the estimated distances.\n  The vanishing of the gradient of the STRAIN is shown to be equivalent to a\nsystem of only six nonlinear equations in six unknowns for the inertial tensor\nassociated with the solution Y . The entire solution space is characterized in\nterms of the geometry of the intersection curves between the unit sphere and\ncertain variable ellipsoids. Upon deriving tight bilateral bounds on the\nmoments of inertia of any possible solution, we construct a search procedure\nthat reliably locates the global minimum. The effectiveness of this method is\ndemonstrated on realistic simulated and chemical test problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10953v1"
    },
    {
        "title": "Zoom-SVD: Fast and Memory Efficient Method for Extracting Key Patterns\n  in an Arbitrary Time Range",
        "authors": [
            "Jun-Gi Jang",
            "Dongjin Choi",
            "Jinhong Jung",
            "U Kang"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Given multiple time series data, how can we efficiently find latent patterns\nin an arbitrary time range? Singular value decomposition (SVD) is a crucial\ntool to discover hidden factors in multiple time series data, and has been used\nin many data mining applications including dimensionality reduction, principal\ncomponent analysis, recommender systems, etc. Along with its static version,\nincremental SVD has been used to deal with multiple semi infinite time series\ndata and to identify patterns of the data. However, existing SVD methods for\nthe multiple time series data analysis do not provide functionality for\ndetecting patterns of data in an arbitrary time range: standard SVD requires\ndata for all intervals corresponding to a time range query, and incremental SVD\ndoes not consider an arbitrary time range. In this paper, we propose Zoom-SVD,\na fast and memory efficient method for finding latent factors of time series\ndata in an arbitrary time range. Zoom-SVD incrementally compresses multiple\ntime series data block by block to reduce the space cost in storage phase, and\nefficiently computes singular value decomposition (SVD) for a given time range\nquery in query phase by carefully stitching stored SVD results. Through\nextensive experiments, we demonstrate that Zoom-SVD is up to 15x faster, and\nrequires 15x less space than existing methods. Our case study shows that\nZoom-SVD is useful for capturing past time ranges whose patterns are similar to\na query time range.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.00754v2"
    },
    {
        "title": "Efficient Explicit Time Stepping of High Order Discontinuous Galerkin\n  Schemes for Waves",
        "authors": [
            "Svenja Schoeder",
            "Katharina Kormann",
            "Wolfgang Wall",
            "Martin Kronbichler"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  This work presents algorithms for the efficient implementation of\ndiscontinuous Galerkin methods with explicit time stepping for acoustic wave\npropagation on unstructured meshes of quadrilaterals or hexahedra. A crucial\nstep towards efficiency is to evaluate operators in a matrix-free way with\nsum-factorization kernels. The method allows for general curved geometries and\nvariable coefficients. Temporal discretization is carried out by low-storage\nexplicit Runge-Kutta schemes and the arbitrary derivative (ADER) method. For\nADER, we propose a flexible basis change approach that combines cheap face\nintegrals with cell evaluation using collocated nodes and quadrature points.\nAdditionally, a degree reduction for the optimized cell evaluation is presented\nto decrease the computational cost when evaluating higher order spatial\nderivatives as required in ADER time stepping. We analyze and compare the\nperformance of state-of-the-art Runge-Kutta schemes and ADER time stepping with\nthe proposed optimizations. ADER involves fewer operations and additionally\nreaches higher throughput by higher arithmetic intensities and hence decreases\nthe required computational time significantly. Comparison of Runge-Kutta and\nADER at their respective CFL stability limit renders ADER especially beneficial\nfor higher orders when the Butcher barrier implies an overproportional amount\nof stages. Moreover, vector updates in explicit Runge--Kutta schemes are shown\nto take a substantial amount of the computational time due to their memory\nintensity.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03981v1"
    },
    {
        "title": "Spherical harmonics entropy for optimal 3D modeling",
        "authors": [
            "Malika Jallouli",
            "Wafa Bel Hadj Khalifa",
            "Anouar Ben Mabrouk",
            "Mohamed Ali Mahjoub"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  3D image processing constitutes nowadays a challenging topic in many\nscientific fields such as medicine, computational physics and informatics.\nTherefore, development of suitable tools that guaranty a best treatment is a\nnecessity. Spherical shapes are a big class of 3D images whom processing\nnecessitates adoptable tools. This encourages researchers to develop spherical\nwavelets and spherical harmonics as special mathematical bases able for 3D\nspherical shapes. The present work lies in the whole topic of 3D image\nprocessing with the special spherical harmonics bases. A spherical harmonics\nbased approach is proposed for the reconstruction of images provided with\nspherical harmonics Shannon-type entropy to evaluate the order/disorder of the\nreconstructed image. Efficiency and accuracy of the approach is demonstrated by\na simulation study on several spherical models.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.08084v1"
    },
    {
        "title": "Decoupling multivariate functions using second-order information and\n  tensors",
        "authors": [
            "Philippe Dreesen",
            "Jeroen De Geeter",
            "Mariya Ishteva"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The power of multivariate functions is their ability to model a wide variety\nof phenomena, but have the disadvantages that they lack an intuitive or\ninterpretable representation, and often require a (very) large number of\nparameters. We study decoupled representations of multivariate vector\nfunctions, which are linear combinations of univariate functions in linear\ncombinations of the input variables. This model structure provides a\ndescription with fewer parameters, and reveals the internal workings in a\nsimpler way, as the nonlinearities are one-to-one functions. In earlier work, a\ntensor-based method was developed for performing this decomposition by using\nfirst-order derivative information. In this article, we generalize this method\nand study how the use of second-order derivative information can be\nincorporated. By doing this, we are able to push the method towards more\ninvolved configurations, while preserving uniqueness of the underlying tensor\ndecompositions. Furthermore, even for some non-identifiable structures, the\nmethod seems to return a valid decoupled representation. These results are a\nstep towards more general data-driven and noise-robust tensor-based framework\nfor computing decoupled function representations.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.08479v1"
    },
    {
        "title": "Numerical methods for differential linear matrix equations via Krylov\n  subspace methods",
        "authors": [
            "M. Hached",
            "K. Jbilou"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In the present paper, we present some numerical methods for computing\napproximate solutions to some large differential linear matrix equations. In\nthe first part of this work, we deal with differential generalized Sylvester\nmatrix equations with full rank right-hand sides using a global Galerkin and a\nnorm-minimization approaches. In the second part, we consider large\ndifferential Lyapunov matrix equations with low rank right-hand sides and use\nthe extended global Arnoldi process to produce low rank approximate solutions.\nWe give some theoretical results and present some numerical experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.10192v1"
    },
    {
        "title": "Polynomial Root-Finding and Algebraic Eigenvalue Problem",
        "authors": [
            "Victor Y. Pan"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Univariate polynomial root-finding has been studied for four millennia and\nvery intensively in the last decades. Our new near-optimal root-finders\napproximate all zeros of a polynomial p almost as fast as one accesses its\ncoefficients with the precision required for the solution within a prescribed\nerror bound. Furthermore, our root-finders can be applied to a black box\npolynomial, defined by an oracle (black box subroutine) for its evaluation\nrather than by its coefficients. Due to this feature our root-finders support\napproximation of the eigenvalues of a matrix in a record Las Vegas expected bit\noperation time and are particularly fast for a polynomial that can be evaluated\nfast such as the sum of a few shifted monomials or a Mandelbrot-like polynomial\ndefined by a recurrence. Our divide and conquer algorithm of ACM STOC 1995 is\nthe only other known near-optimal polynomial root-finder, but it extensively\nuses the coefficients, is quite involved, and has never been implemented, while\naccording to extensive numerical experiments with standard test polynomials,\nalready a slower initial implementation of our new root-finders competes with\nuser's choice package of root-finding subroutines MPSolve and supersedes it\nmore and more significantly as the degree of a polynomial grows large. We\nelaborate upon the design and analysis of our algorithms, comment on their\npotential heuristic acceleration, and briefly cover polynomial root-finding by\nmeans of functional iterations. Our techniques can be of independent interest.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.12042v37"
    },
    {
        "title": "IRA assisted MMC-based topology optimization method",
        "authors": [
            "Kangjia Mo",
            "Hu Wang",
            "Zhenxing Cheng",
            "Yu Li"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  An Iterative Reanalysis Approximation (IRA) is integrated with the Moving\nMorphable Components (MMCs) based topology optimization (IRA-MMC) in this\nstudy. Compared with other classical topology optimization methods, the Finite\nElement (FE) based solver is replaced with the suggested IRA method. In this\nway, the expensive computational cost can be significantly saved by several\nnested iterations. The optimization of linearly elastic planar structures is\nconstructed by the MMC, the specifically geometric parameters of which are\ntaken as design variables to acquire explicitly geometric boundary. In the\nsuggested algorithm, a hybrid optimizer based on the Method of Moving\nAsymptotes (MMA) approach and the Globally Convergent version of the Method of\nMoving Asymptotes (GCMMA) is suggested to improve convergence ratio and avoid\nlocal optimum. The proposed approach is evaluated by some classical benchmark\nproblems in topology optimization, where the results show significant time\nsaving without compromising accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.07021v1"
    },
    {
        "title": "Finite Volume Simulation Framework for Die Casting with Uncertainty\n  Quantification",
        "authors": [
            "Shantanu Shahane",
            "Narayana Aluru",
            "Placid Ferreira",
            "Shiv G Kapoor",
            "Surya Pratap Vanka"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The present paper describes the development of a novel and comprehensive\ncomputational framework to simulate solidification problems in materials\nprocessing, specifically casting processes. Heat transfer, solidification and\nfluid flow due to natural convection are modeled. Empirical relations are used\nto estimate the microstructure parameters and mechanical properties. The\nfractional step algorithm is modified to deal with the numerical aspects of\nsolidification by suitably altering the coefficients in the discretized\nequation to simulate selectively only in the liquid and mushy zones. This\nbrings significant computational speed up as the simulation proceeds. Complex\ndomains are represented by unstructured hexahedral elements. The algebraic\nmultigrid method, blended with a Krylov subspace solver is used to accelerate\nconvergence. State of the art uncertainty quantification technique is included\nin the framework to incorporate the effects of stochastic variations in the\ninput parameters. Rigorous validation is presented using published experimental\nresults of a solidification problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.08572v1"
    },
    {
        "title": "Accurate and efficient explicit approximations of the Colebrook flow\n  friction equation based on the Wright-Omega function",
        "authors": [
            "Dejan Brkić",
            "Pavel Praks"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The Colebrook equation is a popular model for estimating friction loss\ncoefficients in water and gas pipes. The model is implicit in the unknown flow\nfriction factor f. To date, the captured flow friction factor f can be\nextracted from the logarithmic form analytically only in the term of the\nLambert W-function. The purpose of this study is to find an accurate and\ncomputationally efficient solution based on the shifted Lambert W-function also\nknown as the Wright Omega-function. The Wright Omega-function is more suitable\nbecause it overcomes the problem with the overflow error by switching the fast\ngrowing term y=W(e^x) of the Lambert W-function to the series expansions that\nfurther can be easily evaluated in computers without causing overflow run-time\nerrors. Although the Colebrook equation transformed through the Lambert\nW-function is identical to the original expression in term of accuracy, a\nfurther evaluation of the Lambert W-function can be only approximate. Very\naccurate explicit approximations of the Colebrook equation that contains only\none or two logarithms are shown. The final result is an accurate explicit\napproximation of the Colebrook equation with the relative error of no more than\n0.0096%. The presented approximations are in the form suitable for everyday\nengineering use, they are both accurate and computationally efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.10273v2"
    },
    {
        "title": "A Comparison of Automatic Differentiation and Continuous Sensitivity\n  Analysis for Derivatives of Differential Equation Solutions",
        "authors": [
            "Yingbo Ma",
            "Vaibhav Dixit",
            "Mike Innes",
            "Xingjian Guo",
            "Christopher Rackauckas"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Derivatives of differential equation solutions are commonly for parameter\nestimation, fitting neural differential equations, and as model diagnostics.\nHowever, with a litany of choices and a Cartesian product of potential methods,\nit can be difficult for practitioners to understand which method is likely to\nbe the most effective on their particular application. In this manuscript we\ninvestigate the performance characteristics of Discrete Local Sensitivity\nAnalysis implemented via Automatic Differentiation (DSAAD) against continuous\nadjoint sensitivity analysis. Non-stiff and stiff biological and pharmacometric\nmodels, including a PDE discretization, are used to quantify the performance of\nsensitivity analysis methods. Our benchmarks show that on small systems of ODEs\n(approximately $<100$ parameters+ODEs), forward-mode DSAAD is more efficient\nthan both reverse-mode and continuous forward/adjoint sensitivity analysis. The\nscalability of continuous adjoint methods is shown to be more efficient than\ndiscrete adjoints and forward methods after crossing this size range. These\ncomparative studies demonstrate a trade-off between memory usage and\nperformance in the continuous adjoint methods that should be considered when\nchoosing the technique, while numerically unstable backsolve techniques from\nthe machine learning literature are demonstrated as unsuitable for most\nscientific models. The performance of adjoint methods is shown to be heavily\ntied to the reverse-mode AD method, with tape-based AD methods shown to be 2\norders of magnitude slower on nonlinear partial differential equations than\nstatic AD techniques. These results also demonstrate the applicability of DSAAD\nto differential-algebraic equations, delay differential equations, and hybrid\ndifferential equation systems, showcasing an ease of implementation advantage\nfor DSAAD approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.01892v2"
    },
    {
        "title": "Maximum-principle preserving space-time isogeometric analysis",
        "authors": [
            "Jesús Bonilla",
            "Santiago Badia"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In this work we propose a nonlinear stabilization technique for\nconvection-diffusion-reaction and pure transport problems discretized with\nspace-time isogeometric analysis. The stabilization is based on a\ngraph-theoretic artificial diffusion operator and a novel shock detector for\nisogeometric analysis. Stabilization in time and space directions are performed\nsimilarly, which allow us to use high-order discretizations in time without any\nCFL-like condition. The method is proven to yield solutions that satisfy the\ndiscrete maximum principle (DMP) unconditionally for arbitrary order. In\naddition, the stabilization is linearity preserving in a space-time sense.\nMoreover, the scheme is proven to be Lipschitz continuous ensuring that the\nnonlinear problem is well-posed. Solving large problems using a space-time\ndiscretization can become highly costly. Therefore, we also propose a\npartitioned space-time scheme that allows us to select the length of every time\nslab, and solve sequentially for every subdomain. As a result, the\ncomputational cost is reduced while the stability and convergence properties of\nthe scheme remain unaltered. In addition, we propose a twice differentiable\nversion of the stabilization scheme, which enjoys the same stability properties\nwhile the nonlinear convergence is significantly improved. Finally, the\nproposed schemes are assessed with numerical experiments. In particular, we\nconsidered steady and transient pure convection and convection-diffusion\nproblems in one and two dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.05442v2"
    },
    {
        "title": "Model reduction of dynamical systems on nonlinear manifolds using deep\n  convolutional autoencoders",
        "authors": [
            "Kookjin Lee",
            "Kevin Carlberg"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Nearly all model-reduction techniques project the governing equations onto a\nlinear subspace of the original state space. Such subspaces are typically\ncomputed using methods such as balanced truncation, rational interpolation, the\nreduced-basis method, and (balanced) POD. Unfortunately, restricting the state\nto evolve in a linear subspace imposes a fundamental limitation to the accuracy\nof the resulting reduced-order model (ROM). In particular, linear-subspace ROMs\ncan be expected to produce low-dimensional models with high accuracy only if\nthe problem admits a fast decaying Kolmogorov $n$-width (e.g.,\ndiffusion-dominated problems). Unfortunately, many problems of interest exhibit\na slowly decaying Kolmogorov $n$-width (e.g., advection-dominated problems). To\naddress this, we propose a novel framework for projecting dynamical systems\nonto nonlinear manifolds using minimum-residual formulations at the\ntime-continuous and time-discrete levels; the former leads to manifold Galerkin\nprojection, while the latter leads to manifold least-squares Petrov--Galerkin\n(LSPG) projection. We perform analyses that provide insight into the\nrelationship between these proposed approaches and classical linear-subspace\nreduced-order models; we also derive a posteriori discrete-time error bounds\nfor the proposed approaches. In addition, we propose a computationally\npractical approach for computing the nonlinear manifold, which is based on\nconvolutional autoencoders from deep learning. Finally, we demonstrate the\nability of the method to significantly outperform even the optimal\nlinear-subspace ROM on benchmark advection-dominated problems, thereby\ndemonstrating the method's ability to overcome the intrinsic $n$-width\nlimitations of linear subspaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.08373v3"
    },
    {
        "title": "Vilin: Unconstrained Numerical Optimization Application",
        "authors": [
            "Marko Miladinović",
            "Predrag Živadinović"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We introduce an application for executing and testing different unconstrained\noptimization algorithms. The application contains a library of various test\nfunctions with pre-defined starting points. A several known classes of methods\nas well as different classes of line search procedures are covered. Each method\ncan be tested on various test function with a chosen number of parameters.\nSolvers come with optimal pre-defined parameter values which simplifies the\nusage. Additionally, user friendly interface gives an opportunity for advanced\nusers to use their expertise and also easily fine-tune a large number of hyper\nparameters for obtaining even more optimal solution.\n  This application can be used as a tool for developing new optimization\nalgorithms (by using simple API), as well as for testing and comparing existing\nones, by using given standard library of test functions. Special care has been\ngiven in order to achieve good numerical stability of all vital parts of the\napplication. The application is implemented in programming language Matlab with\nvery helpful gui support.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.10986v1"
    },
    {
        "title": "Preconditioning Kaczmarz method by sketching",
        "authors": [
            "Alexandr Katrutsa",
            "Ivan Oseledets"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  We propose a new method for preconditioning Kaczmarz method by sketching.\nKaczmarz method is a stochastic method for solving overdetermined linear\nsystems based on a sampling of matrix rows. The standard approach to speed up\nconvergence of iterative methods is using preconditioner. As we show the best\npossible preconditioner for this method can be constructed from QR\ndecomposition of the system matrix, but the complexity of this procedure is too\nhigh. Therefore, to reduce this complexity, we use random sketching and compare\nit with the Kaczmarz method without preconditioning. The developed method is\napplicable for different modifications of classical Kaczmarz method that were\nproposed recently. We provide numerical experiments to show the performance of\nthe developed methods on solving both random and real overdetermined linear\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01806v1"
    },
    {
        "title": "Geometry Mapping, Complete Pascal Scheme versus Standard Bilinear\n  Approach",
        "authors": [
            "Sulaiman Y. Abo Diab"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  This paper presents a complete Pascal interpolation scheme for use in the\nplane geometry mapping applied in association with numerical methods. The\ngeometry of a domain element is approximated by a complete Pascal polynomial.\nThe interpolation procedure is formulated in a natural coordinate system. It\nalso presents the methodology of constructing shape functions of Pascal type\nand establishing a transformation relation between natural and Cartesian\nvariables. The performance of the presented approach is investigated firstly by\ncalculating the geometrical properties of an arbitrary quadrilateral\ncross-section like area and moments of inertia and comparing the results with\nthe exact values and with those provided by the standard linear approach and a\nserendipity family approach. Secondly, the assessment of the scheme follows\nusing a straight-sided, compatible quadrilateral finite element for plate\nbending of which geometry is approximated by a complete set of second order\nwith six free parameters. Triangular and quadrilateral shaped plates with\ndifferent boundary conditions are computed and compared with well-known results\nin the literature. The presented procedure is of general applicability for\nelements with curved edges and not limited to straight-sided edges in the\nframework of numerical methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.03453v1"
    },
    {
        "title": "Varying-order NURBS discretization: An accurate and efficient method for\n  isogeometric analysis of large deformation contact problems",
        "authors": [
            "Vishal Agrawal",
            "Sachin S. Gautam"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  In this paper, a novel varying order NURBS discretization method is proposed\nto enhance the performance of isogeometric analysis within the framework of\ncomputational contact mechanics. The method makes use of higher-order NURBS for\ncontact integral evaluations. Lower-orders NURBS capable of modelling complex\ngeometries exactly are utilized for the bulk discretization. This unexplored\nidea provides the possibility to refine the geometry through controllable order\nelevation strategy for isogeometric analysis. To achieve this, a higher-order\nNURBS layer is used as the contact boundary layer of the bodies. The NURBS\nlayer is constructed using the surface refinement strategies such that it is\naccompanied by a large number of additional degrees of freedom and matches with\nthe bulk parametrization.\n  The validity of the presented isogeometric mortar contact formulation with\nvarying-order NURBS discretization is first examined through the contact patch\ntest. The capabilities and benefits of the proposed methodology are then\ndemonstrated in detail using two-dimensional frictionless and frictional\ncontact problems, considering both small and large deformations. It is shown\nthat using the proposed method, accurate solutions can be achieved even with a\ncoarse mesh. It is also shown that the current method requires a considerably\nlower computational cost compared to standard NURBS discretization while\nretaining robustness and stability. The simplicity of the method lends itself\nto be conveniently embedded in an existing isogeometric contact code after only\na few minor modifications.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.05859v3"
    },
    {
        "title": "A block tangential Lanczos method for model reduction of large-scale\n  first and second order dynamical systems",
        "authors": [
            "Yassine Kaouane",
            "Khalide Jbilou"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  In this paper, we present a new approach for model reduction of large scale\nfirst and second order dynamical systems with multiple inputs and multiple\noutputs (MIMO). This approach is based on the projection of the initial problem\nonto tangential Krylov subspaces to produce a simpler reduced-order model that\napproximates well the behavior of the original model. We present an algorithm\nnamed: Adaptive Block Tangential Lanczos-type (ABTL) algorithm. We give some\nalgebraic properties and present some numerical experiences to show the\neffectiveness of the proposed algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.06876v1"
    },
    {
        "title": "A compact high order Alternating Direction Implicit method for\n  three-dimensional acoustic wave equation with variable coefficient",
        "authors": [
            "Keran Li",
            "Wenyuan Liao",
            "Yaoting Lin"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  Efficient and accurate numerical simulation of seismic wave propagation is\nimportant in various Geophysical applications such as seismic full waveform\ninversion (FWI) problem. However, due to the large size of the physical domain\nand requirement on low numerical dispersion, many existing numerical methods\nare inefficient for numerical modelling of seismic wave propagation in a\nheterogeneous media. Despite the great efforts that have been devoted during\nthe past decades, it still remains a challenging task in the development of\nefficient and accurate finite difference method for the multi-dimensional\nacoustic wave equation with variable velocity. In this paper, we proposed a\nPad\\'{e} approximation based finite difference scheme for solving the acoustic\nwave equation in three-dimensional heterogeneous media. The new method is\nobtained by combining the Pad\\'{e} approximation and a novel algebraic\nmanipulation. The efficiency of the new algorithm is further improved through\nthe Alternative Directional Implicit (ADI) method. The stability of the new\nalgorithm has been theoretically proved by the energy method. The new method is\nconditionally stable with a better Courant - Friedrichs - Lewy condition (CFL)\ncondition, which has been verified numerically. Extensive numerical examples\nhave been solved, which demonstrated that the new method is accurate, efficient\nand stable.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.08108v2"
    },
    {
        "title": "A Large-Scale Comparison of Tetrahedral and Hexahedral Elements for\n  Solving Elliptic PDEs with the Finite Element Method",
        "authors": [
            "Teseo Schneider",
            "Yixin Hu",
            "Xifeng Gao",
            "Jeremie Dumas",
            "Denis Zorin",
            "Daniele Panozzo"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  The Finite Element Method (FEM) is widely used to solve discrete Partial\nDifferential Equations (PDEs) in engineering and graphics applications. The\npopularity of FEM led to the development of a large family of variants, most of\nwhich require a tetrahedral or hexahedral mesh to construct the basis. While\nthe theoretical properties of FEM basis (such as convergence rate, stability,\netc.) are well understood under specific assumptions on the mesh quality, their\npractical performance, influenced both by the choice of the basis construction\nand quality of mesh generation, have not been systematically documented for\nlarge collections of automatically meshed 3D geometries.\n  We introduce a set of benchmark problems involving most commonly solved\nelliptic PDEs, starting from simple cases with an analytical solution, moving\nto commonly used test problem setups, and using manufactured solutions for\nthousands of real-world, automatically meshed geometries. For all these cases,\nwe use state-of-the-art meshing tools to create both tetrahedral and hexahedral\nmeshes, and compare the performance of different element types for common\nelliptic PDEs.\n  The goal of his benchmark is to enable comparison of complete FEM pipelines,\nfrom mesh generation to algebraic solver, and exploration of relative impact of\ndifferent factors on the overall system performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.09332v3"
    },
    {
        "title": "Non-recursive equivalent of the conjugate gradient method without the\n  need to restart",
        "authors": [
            "Josip Dvornik",
            "Damir Lazarevic",
            "Antonia Jaguljnjak Lazarevic",
            "Marija Demsic"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  A simple alternative to the conjugate gradient(CG) method is presented; this\nmethod is developed as a special case of the more general iterated Ritz method\n(IRM) for solving a system of linear equations. This novel algorithm is not\nbased on conjugacy, i.e. it is not necessary to maintain overall\northogonalities between various vectors from distant steps. This method is more\nstable than CG, and restarting techniques are not required. As in CG, only one\nmatrix-vector multiplication is required per step with appropriate\ntransformations. The algorithm is easily explained by energy considerations\nwithout appealing to the A-orthogonality in n-dimensional space. Finally,\nrelaxation factor and preconditioning-like techniques can be adopted easily.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.11379v1"
    },
    {
        "title": "Algorithms and Polynomiography for Solving Quaternion Quadratic\n  Equations",
        "authors": [
            "Fedor Andreev",
            "Bahman Kalantari"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Solving a quadratic equation $P(x)=ax^2+bx+c=0$ with real coefficients is\nknown to middle school students. Solving the equation over the quaternions is\nnot straightforward. Huang and So \\cite{Huang} give a complete set of formulas,\nbreaking it into several cases depending on the coefficients. From a result of\nthe second author in \\cite{kalQ}, zeros of $P(x)$ can be expressed in terms of\nthe zeros of a real quartic equation. This drastically simplifies solving a\nquadratic equation. Here we also consider solving $P(x)=0$ iteratively via\nNewton and Halley methods developed in \\cite{kalQ}. We prove a property of the\nJacobian of Newton and Halley methods and describe several 2D polynomiography\nbased on these methods. The images not only encode the outcome of the iterative\nprocess, but by measuring the time taken to render them we find the relative\nspeed of convergence for the methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.2030v1"
    },
    {
        "title": "AE regularity of interval matrices",
        "authors": [
            "Milan Hladík"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Consider a linear system of equations with interval coefficients, and each\ninterval coefficient is associated with either a universal or an existential\nquantifier. The AE solution set and AE solvability of the system is defined by\n$\\forall\\exists$-quantification.\n  Herein, we deal with the problem what properties must the coefficient matrix\nhave in order that there is guaranteed an existence of an AE solution. Based on\nthis motivation, we introduce a concept of AE regularity, which implies that\nthe AE solution set is nonempty and the system is AE solvable for every\nright-hand side. We discuss characterization of AE regularity, and we also\nfocus on various classes of matrices that are implicitly AE regular. Some of\nthese classes are polynomially decidable, and therefore give an efficient way\nfor checking AE regularity. We also state open problems related to\ncomputational complexity and characterization.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02102v1"
    },
    {
        "title": "A Technical Note: Two-Step PECE Methods for Approximating Solutions To\n  First- and Second-Order ODEs",
        "authors": [
            "Alan D. Freed"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Two-step predictor/corrector methods are provided to solve three classes of\nproblems that present themselves as systems of ordinary differential equations\n(ODEs). In the first class, velocities are given from which displacements are\nto be solved. In the second class, velocities and accelerations are given from\nwhich displacements are to be solved. And in the third class, accelerations are\ngiven from which velocities and displacements are to be solved. Two-step\nmethods are not self starting, so compatible one-step methods are provided to\ntake that first step with. An algorithm is presented for controlling the step\nsize so that the local truncation error does not exceed a specified tolerance.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02125v1"
    },
    {
        "title": "The Convergence of Least-Squares Progressive Iterative Approximation\n  with Singular Iterative Matrix",
        "authors": [
            "Hongwei Lin",
            "Qi Cao",
            "Xiaoting Zhang"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Developed in [Deng and Lin, 2014], Least-Squares Progressive Iterative\nApproximation (LSPIA) is an efficient iterative method for solving B-spline\ncurve and surface least-squares fitting systems. In [Deng and Lin 2014], it was\nshown that LSPIA is convergent when the iterative matrix is nonsingular. In\nthis paper, we will show that LSPIA is still convergent even the iterative\nmatrix is singular.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09109v1"
    },
    {
        "title": "An adjustable-width window with good dynamic range",
        "authors": [
            "I M Stewart"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  A new variable-width window is presented and compared with several other\nwindows, both of variable and fixed widths. The comparison focuses on\nsensitivity and dynamic range. The equivalent noise bandwidth or ENBW (or\nrather, its reciprocal) is used as a proxy for the first; maximum sidelobe\nlevel and high-frequency roll-off in the Fourier transform, for the second. The\nnew window can access any value of ENBW by appropriate choice of the width\nparameter. At any given value of ENBW below about 3, a setting can be found at\nwhich the sidelobes of the window are lower than those of any other in the\nmoderate frequency regime below about 100 cycles.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.6979v1"
    },
    {
        "title": "Reversible Digital Filters Total Parametric Sensitivity Optimization\n  using Non-canonical Hypercomplex Number Systems",
        "authors": [
            "Yakiv O. Kalinovsky",
            "Yuliya E. Boyarinova",
            "Iana V. Khitsko"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Digital filter construction method, which is optimal by parametric\nsensitivity, based on using of non-canonical hypercomplex number systems is\nproposed and investigated. It is shown that the use of non-canonical\nhypercomplex number system with greater number of non-zero structure constants\nin multiplication table can significantly improve the sensitivity of the\ndigital filter.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.01701v1"
    },
    {
        "title": "Spectral Compressed Sensing via CANDECOMP/PARAFAC Decomposition of\n  Incomplete Tensors",
        "authors": [
            "Jun Fang",
            "Linxiao Yang",
            "Hongbin Li"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We consider the line spectral estimation problem which aims to recover a\nmixture of complex sinusoids from a small number of randomly observed time\ndomain samples. Compressed sensing methods formulates line spectral estimation\nas a sparse signal recovery problem by discretizing the continuous frequency\nparameter space into a finite set of grid points. Discretization, however,\ninevitably incurs errors and leads to deteriorated estimation performance. In\nthis paper, we propose a new method which leverages recent advances in tensor\ndecomposition. Specifically, we organize the observed data into a structured\ntensor and cast line spectral estimation as a CANDECOMP/PARAFAC (CP)\ndecomposition problem with missing entries. The uniqueness of the CP\ndecomposition allows the frequency components to be super-resolved with\ninfinite precision. Simulation results show that the proposed method provides a\ncompetitive estimate accuracy compared with existing state-of-the-art\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.03224v1"
    },
    {
        "title": "Spectral Statistics of Lattice Graph Structured, Non-uniform\n  Percolations",
        "authors": [
            "Stephen Kruzick",
            "José M. F. Moura"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Design of filters for graph signal processing benefits from knowledge of the\nspectral decomposition of matrices that encode graphs, such as the adjacency\nmatrix and the Laplacian matrix, used to define the shift operator. For shift\nmatrices with real eigenvalues, which arise for symmetric graphs, the empirical\nspectral distribution captures the eigenvalue locations. Under realistic\ncircumstances, stochastic influences often affect the network structure and,\nconsequently, the shift matrix empirical spectral distribution. Nevertheless,\ndeterministic functions may often be found to approximate the asymptotic\nbehavior of empirical spectral distributions of random matrices. This paper\nuses stochastic canonical equation methods developed by Girko to derive such\ndeterministic equivalent distributions for the empirical spectral distributions\nof random graphs formed by structured, non-uniform percolation of a\nD-dimensional lattice supergraph. Included simulations demonstrate the results\nfor sample parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.01780v1"
    },
    {
        "title": "Multivariate predictions of local reduced-order-model errors and\n  dimensions",
        "authors": [
            "Azam Moosavi",
            "Razvan Stefanescu",
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This paper introduces multivariate input-output models to predict the errors\nand bases dimensions of local parametric Proper Orthogonal Decomposition\nreduced-order models. We refer to these multivariate mappings as the MP-LROM\nmodels. We employ Gaussian Processes and Artificial Neural Networks to\nconstruct approximations of these multivariate mappings. Numerical results with\na viscous Burgers model illustrate the performance and potential of the machine\nlearning based regression MP-LROM models to approximate the characteristics of\nparametric local reduced-order models. The predicted reduced-order models\nerrors are compared against the multi-fidelity correction and reduced order\nmodel error surrogates methods predictions, whereas the predicted reduced-order\ndimensions are tested against the standard method based on the spectrum of\nsnapshots matrix. Since the MP-LROM models incorporate more features and\nelements to construct the probabilistic mappings they achieve more accurate\nresults. However, for high-dimensional parametric spaces, the MP-LROM models\nmight suffer from the curse of dimensionality. Scalability challenges of\nMP-LROM models and the feasible ways of addressing them are also discussed in\nthis study.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.03720v1"
    },
    {
        "title": "EPIRK-W and EPIRK-K time discretization methods",
        "authors": [
            "Mahesh Narayanamurthi",
            "Paul Tranquilli",
            "Adrian Sandu",
            "Mayya Tokman"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Exponential integrators are special time discretization methods where the\ntraditional linear system solves used by implicit schemes are replaced with\ncomputing the action of matrix exponential-like functions on a vector. A very\ngeneral formulation of exponential integrators is offered by the Exponential\nPropagation Iterative methods of Runge-Kutta type (EPIRK) family of schemes.\nThe use of Jacobian approximations is an important strategy to drastically\nreduce the overall computational costs of implicit schemes while maintaining\nthe quality of their solutions. This paper extends the EPIRK class to allow the\nuse of inexact Jacobians as arguments of the matrix exponential-like functions.\nSpecifically, we develop two new families of methods: EPIRK-W integrators that\ncan accommodate any approximation of the Jacobian, and EPIRK-K integrators that\nrely on a specific Krylov-subspace projection of the exact Jacobian. Classical\norder conditions theories are constructed for these families. A practical\nEPIRK-W method of order three and an EPIRK-K method of order four are\ndeveloped. Numerical experiments indicate that the methods proposed herein are\ncomputationally favorable when compared to existing exponential integrators.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06528v2"
    },
    {
        "title": "A Practical Randomized CP Tensor Decomposition",
        "authors": [
            "Casey Battaglino",
            "Grey Ballard",
            "Tamara G. Kolda"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The CANDECOMP/PARAFAC (CP) decomposition is a leading method for the analysis\nof multiway data. The standard alternating least squares algorithm for the CP\ndecomposition (CP-ALS) involves a series of highly overdetermined linear least\nsquares problems. We extend randomized least squares methods to tensors and\nshow the workload of CP-ALS can be drastically reduced without a sacrifice in\nquality. We introduce techniques for efficiently preprocessing, sampling, and\ncomputing randomized least squares on a dense tensor of arbitrary order, as\nwell as an efficient sampling-based technique for checking the stopping\ncondition. We also show more generally that the Khatri-Rao product (used within\nthe CP-ALS iteration) produces conditions favorable for direct sampling. In\nnumerical results, we see improvements in speed, reductions in memory\nrequirements, and robustness with respect to initialization.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06600v2"
    },
    {
        "title": "Approximate Newton Methods",
        "authors": [
            "Haishan Ye",
            "Luo Luo",
            "Zhihua Zhang"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Many machine learning models involve solving optimization problems. Thus, it\nis important to deal with a large-scale optimization problem in big data\napplications. Recently, subsampled Newton methods have emerged to attract much\nattention due to their efficiency at each iteration, rectified a weakness in\nthe ordinary Newton method of suffering a high cost in each iteration while\ncommanding a high convergence rate. Other efficient stochastic second order\nmethods are also proposed. However, the convergence properties of these methods\nare still not well understood. There are also several important gaps between\nthe current convergence theory and the performance in real applications. In\nthis paper, we aim to fill these gaps. We propose a unifying framework to\nanalyze both local and global convergence properties of second order methods.\nBased on this framework, we present our theoretical results which match the\nperformance in real applications well.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.08124v2"
    },
    {
        "title": "A Hierarchical Singular Value Decomposition Algorithm for Low Rank\n  Matrices",
        "authors": [
            "Vinita Vasudevan",
            "M. Ramakrishna"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Singular value decomposition (SVD) is a widely used technique for\ndimensionality reduction and computation of basis vectors. In many\napplications, especially in fluid mechanics and image processing the matrices\nare dense, but low-rank matrices. In these cases, a truncated SVD corresponding\nto the most significant singular values is sufficient. In this paper, we\npropose a tree based merge-and-truncate algorithm to obtain an approximate\ntruncated SVD of the matrix. Unlike previous methods, our technique is not\nlimited to \"tall and skinny\" or \"short and fat\" matrices and it can be used for\nmatrices of arbitrary size. The matrix is partitioned into blocks and the\ntruncated SVDs of blocks are merged to obtain the final SVD. If the matrices\nare low rank, this algorithm gives significant speedup over finding the\ntruncated SVD, even when run on a single core. The error is typically less than\n3\\%.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.02812v2"
    },
    {
        "title": "Reduction of Look Up Tables for Computation of Reciprocal of Square\n  Roots",
        "authors": [
            "Shadrokh Samavi",
            "Mohammad Reza Jahangir"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Among many existing algorithms, convergence methods are the most popular\nmeans of computing square root and the reciprocal of square root of numbers. An\ninitial approximation is required in these methods. Look up tables (LUT) are\nemployed to produce the initial approximation. In this paper a number of\nmethods are suggested to reduce the size of the look up tables. The precision\nof the initial approximation plays an important role in the quality of the\nfinal result. There are constraints for the use of a LUT in terms of its size\nand its access time. Therefore, the optimization of the LUTs must be done in a\nway to minimize hardware while offering acceptable convergence speed and\nexactitude.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.04688v1"
    },
    {
        "title": "A Novel Partitioning Method for Accelerating the Block Cimmino Algorithm",
        "authors": [
            "F. Sukru Torun",
            "Murat Manguoglu",
            "Cevdet Aykanat"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We propose a novel block-row partitioning method in order to improve the\nconvergence rate of the block Cimmino algorithm for solving general sparse\nlinear systems of equations. The convergence rate of the block Cimmino\nalgorithm depends on the orthogonality among the block rows obtained by the\npartitioning method. The proposed method takes numerical orthogonality among\nblock rows into account by proposing a row inner-product graph model of the\ncoefficient matrix. In the graph partitioning formulation defined on this graph\nmodel, the partitioning objective of minimizing the cutsize directly\ncorresponds to minimizing the sum of inter-block inner products between block\nrows thus leading to an improvement in the eigenvalue spectrum of the iteration\nmatrix. This in turn leads to a significant reduction in the number of\niterations required for convergence. Extensive experiments conducted on a large\nset of matrices confirm the validity of the proposed method against a\nstate-of-the-art method.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.07769v2"
    },
    {
        "title": "Tensor Matched Subspace Detection",
        "authors": [
            "Cuiping Li",
            "Xiao-Yang Liu",
            "Yue Sun"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The problem of testing whether a signal lies within a given subspace, also\nnamed matched subspace detection, has been well studied when the signal is\nrepresented as a vector. However, the matched subspace detection methods based\non vectors can not be applied to the situations that signals are naturally\nrepresented as multi-dimensional data arrays or tensors. Considering that\ntensor subspaces and orthogonal projections onto these subspaces are well\ndefined in the recently proposed transform-based tensor model, which motivates\nus to investigate the problem of matched subspace detection in high dimensional\ncase. In this paper, we propose an approach for tensor matched subspace\ndetection based on the transform-based tensor model with tubal-sampling and\nelementwise-sampling, respectively. First, we construct estimators based on\ntubal-sampling and elementwise-sampling to estimate the energy of a signal\noutside a given subspace of a third-order tensor and then give the probability\nbounds of our estimators, which show that our estimators work effectively when\nthe sample size is greater than a constant. Secondly, the detectors both for\nnoiseless data and noisy data are given, and the corresponding detection\nperformance analyses are also provided. Finally, based on discrete Fourier\ntransform (DFT) and discrete cosine transform (DCT), the performance of our\nestimators and detectors are evaluated by several simulations, and simulation\nresults verify the effectiveness of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.08308v4"
    },
    {
        "title": "Matrix Compression using the Nystroöm Method",
        "authors": [
            "Arik Nemtsov",
            "Amir Averbuch",
            "Alon Schclar"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The Nystr\\\"{o}m method is routinely used for out-of-sample extension of\nkernel matrices. We describe how this method can be applied to find the\nsingular value decomposition (SVD) of general matrices and the eigenvalue\ndecomposition (EVD) of square matrices. We take as an input a matrix $M\\in\n\\mathbb{R}^{m\\times n}$, a user defined integer $s\\leq min(m,n)$ and $A_M \\in\n\\mathbb{R}^{s\\times s}$, a matrix sampled from the columns and rows of $M$.\nThese are used to construct an approximate rank-$s$ SVD of $M$ in\n$O\\left(s^2\\left(m+n\\right)\\right)$ operations. If $M$ is square, the rank-$s$\nEVD can be similarly constructed in $O\\left(s^2 n\\right)$ operations. Thus, the\nmatrix $A_M$ is a compressed version of $M$. We discuss the choice of $A_M$ and\npropose an algorithm that selects a good initial sample for a pivoted version\nof $M$. The proposed algorithm performs well for general matrices and kernel\nmatrices whose spectra exhibit fast decay.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.0203v1"
    },
    {
        "title": "Subsquares Approach - Simple Scheme for Solving Overdetermined Interval\n  Linear Systems",
        "authors": [
            "Jaroslav Horáček",
            "Milan Hladík"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this work we present a new simple but efficient scheme - Subsquares\napproach - for development of algorithms for enclosing the solution set of\noverdetermined interval linear systems. We are going to show two algorithms\nbased on this scheme and discuss their features. We start with a simple\nalgorithm as a motivation, then we continue with a sequential algorithm. Both\nalgorithms can be easily parallelized. The features of both algorithms will be\ndiscussed and numerically tested.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.1059v1"
    },
    {
        "title": "Almost-commuting matrices are almost jointly diagonalizable",
        "authors": [
            "Klaus Glashoff",
            "Michael M. Bronstein"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We study the relation between approximate joint diagonalization of\nself-adjoint matrices and the norm of their commutator, and show that almost\ncommuting self-adjoint matrices are almost jointly diagonalizable by a unitary\nmatrix.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.2135v2"
    },
    {
        "title": "Splitting schemes for unsteady problems involving the grad-div operator",
        "authors": [
            "Peter Minev",
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In this paper we consider various splitting schemes for unsteady problems\ncontaining the grad-div operator. The fully implicit discretization of such\nproblems would yield at each time step a linear problem that couples all\ncomponents of the solution vector. In this paper we discuss various\npossibilities to decouple the equations for the different components that\nresult in unconditionally stable schemes. If the spatial discretization uses\nCartesian grids, the resulting schemes are Locally One Dimensional (LOD). The\nstability analysis of these schemes is based on the general stability theory of\nadditive operator-difference schemes developed by Samarskii and his\ncollaborators. The results of the theoretical analysis are illustrated on a 2D\nnumerical example with a smooth manufactured solution.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.05800v1"
    },
    {
        "title": "Fast Multipole Method based filtering of non-uniformly sampled data",
        "authors": [
            "Nail A. Gumerov",
            "Ramani Duraiswami"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Non-uniform fast Fourier Transform (NUFFT) and inverse NUFFT (INUFFT)\nalgorithms, based on the Fast Multipole Method (FMM) are developed and tested.\nOur algorithms are based on a novel factorization of the FFT kernel, and are\nimplemented with attention to data structures and error analysis.\n  Note: This unpublished manuscript was available on our web pages and has been\nreferred to by others in the literature. To provide a proper archival reference\nwe are placing it on arXiv.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.09379v1"
    },
    {
        "title": "Revisiting Hammel et al. (1987): Does the shadowing property hold for\n  modern computers?",
        "authors": [
            "B. C. Silva",
            "F. L. Milani",
            "E. G. Nepomuceno",
            "S. A. M. Martins",
            "G. F. V. Amaral"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Computational techniques are extensively applied in nonlinear science.\nHowever, while the use of computers for research has been expressive, the\nevaluation of numerical results does not grow in the same pace. Hammel et al.\n(Journal of Complexity, 1987, 3(2), 136--145) were pioneers in the numerical\nreliability field and have proved a theorem that a pseudo-orbit of a logistic\nmap is shadowed by a true orbit within a distance of $10^{-8}$ for $10^{7}$\niterates. But the simulation of the logistic map with less than 100 iterates\npresents an error greater than $10^{-8}$ in a modern computer, performing a\ntest based on the concept of multiple pseudo-orbits and symbolic computing.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.02153v1"
    },
    {
        "title": "Approximate fast graph Fourier transforms via multi-layer sparse\n  approximations",
        "authors": [
            "Luc Le Magoarou",
            "Rémi Gribonval",
            "Nicolas Tremblay"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  The Fast Fourier Transform (FFT) is an algorithm of paramount importance in\nsignal processing as it allows to apply the Fourier transform in O(n log n)\ninstead of O(n 2) arithmetic operations. Graph Signal Processing (GSP) is a\nrecent research domain that generalizes classical signal processing tools, such\nas the Fourier transform, to situations where the signal domain is given by any\narbitrary graph instead of a regular grid. Today, there is no method to rapidly\napply graph Fourier transforms. We propose in this paper a method to obtain\napproximate graph Fourier transforms that can be applied rapidly and stored\nefficiently. It is based on a greedy approximate diagonalization of the graph\nLaplacian matrix, carried out using a modified version of the famous Jacobi\neigenvalues algorithm. The method is described and analyzed in detail, and then\napplied to both synthetic and real graphs, showing its potential.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.04542v3"
    },
    {
        "title": "RG Smoothing Algorithm Which Makes Data Compression",
        "authors": [
            "Anna Sinelnikova"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  I describe a new method for smoothing a one-dimensional curve in Euclidian\nspace with an arbitrary number of dimensions. The basic idea is borrowed from\nrenormalization group theory which previously was applied to biological\nmacromolecules. There are two crucial differences from other smoothing methods\nwhich make the algorithm unique: data compression and recursive implementation.\nOne of the simplest forms of the method that is described in this article has\nonly one free parameter - the number of iterative steps. This means that\nhardware implementation should be relatively easy because each loop is simple\nand strictly defined. The method could be beneficially applied to pattern\nrecognition and data compression in future studies.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01663v1"
    },
    {
        "title": "A Generalized Matrix Splitting Algorithm",
        "authors": [
            "Ganzhao Yuan",
            "Wei-Shi Zheng",
            "Li Shen",
            "Bernard Ghanem"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Composite function minimization captures a wide spectrum of applications in\nboth computer vision and machine learning. It includes bound constrained\noptimization, $\\ell_1$ norm regularized optimization, and $\\ell_0$ norm\nregularized optimization as special cases. This paper proposes and analyzes a\nnew Generalized Matrix Splitting Algorithm (GMSA) for minimizing composite\nfunctions. It can be viewed as a generalization of the classical Gauss-Seidel\nmethod and the Successive Over-Relaxation method for solving linear systems in\nthe literature. Our algorithm is derived from a novel triangle operator\nmapping, which can be computed exactly using a new generalized Gaussian\nelimination procedure. We establish the global convergence, convergence rate,\nand iteration complexity of GMSA for convex problems. In addition, we also\ndiscuss several important extensions of GMSA. Finally, we validate the\nperformance of our proposed method on three particular applications:\nnonnegative matrix factorization, $\\ell_0$ norm regularized sparse coding, and\n$\\ell_1$ norm regularized Dantzig selector problem. Extensive experiments show\nthat our method achieves state-of-the-art performance in term of both\nefficiency and efficacy.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.03165v1"
    },
    {
        "title": "Two-level preconditioning for Ridge Regression",
        "authors": [
            "Joris Tavernier",
            "Jaak Simm",
            "Karl Meerbergen",
            "Yves Moreau"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Solving linear systems is often the computational bottleneck in real-life\nproblems. Iterative solvers are the only option due to the complexity of direct\nalgorithms or because the system matrix is not explicitly known. Here, we\ndevelop a two-level preconditioner for regularized least squares linear systems\ninvolving a feature or data matrix. Variants of this linear system may appear\nin machine learning applications, such as ridge regression, logistic\nregression, support vector machines and Bayesian regression. We use clustering\nalgorithms to create a coarser level that preserves the principal components of\nthe covariance or Gram matrix. This coarser level approximates the dominant\neigenvectors and is used to build a subspace preconditioner accelerating the\nConjugate Gradient method. We observed speed-ups for artificial and real-life\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.05826v2"
    },
    {
        "title": "Global Complex Roots and Poles Finding Algorithm Based on Phase Analysis\n  for Propagation and Radiation Problems",
        "authors": [
            "Piotr Kowalczyk"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  A flexible and effective algorithm for complex roots and poles finding is\npresented. A wide class of analytic functions can be analyzed, and any\narbitrarily shaped search region can be considered. The method is very simple\nand intuitive. It is based on sampling a function at the nodes of a regular\nmesh, and on the analysis of the function phase. As a result, a set of\ncandidate regions is created and then the roots/poles are verified using a\ndiscretized Cauchy's argument principle. The accuracy of the results can be\nimproved by the application of a self-adaptive mesh. The effectiveness of the\npresented technique is supported by numerical tests involving different types\nof structures, where electromagnetic waves are guided and radiated. The results\nare verified, and the computational efficiency of the method is examined.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.06522v2"
    },
    {
        "title": "Gradient Descent-based D-optimal Design for the Least-Squares Polynomial\n  Approximation",
        "authors": [
            "V. P. Zankin",
            "G. V. Ryzhakov",
            "I. V. Oseledets"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In this work, we propose a novel sampling method for Design of Experiments.\nThis method allows to sample such input values of the parameters of a\ncomputational model for which the constructed surrogate model will have the\nleast possible approximation error. High efficiency of the proposed method is\ndemonstrated by its comparison with other sampling techniques (LHS, Sobol'\nsequence sampling, and Maxvol sampling) on the problem of least-squares\npolynomial approximation. Also, numerical experiments for the Lebesgue constant\ngrowth for the points sampled by the proposed method are carried out.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.06631v2"
    },
    {
        "title": "An Improved Formula for Jacobi Rotations",
        "authors": [
            "Carlos F. Borges"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We present an improved form of the algorithm for constructing Jacobi\nrotations. This is simultaneously a more accurate code for finding the\neigenvalues and eigenvectors of a real symmetric 2x2 matrix.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07876v1"
    },
    {
        "title": "Truncation Error Estimation in the p-Anisotropic Discontinuous Galerkin\n  Spectral Element Method",
        "authors": [
            "Andrés M. Rueda-Ramírez",
            "Gonzalo Rubio",
            "Esteban Ferrer",
            "Eusebio Valero"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In the context of Discontinuous Galerkin Spectral Element Methods (DGSEM),\n$\\tau$-estimation has been successfully used for p-adaptation algorithms. This\nmethod estimates the truncation error of representations with different\npolynomial orders using the solution on a reference mesh of relatively high\norder.\n  In this paper, we present a novel anisotropic truncation error estimator\nderived from the $\\tau$-estimation procedure for DGSEM. We exploit the tensor\nproduct basis properties of the numerical solution to design a method where the\ntotal truncation error is calculated as a sum of its directional components. We\nshow that the new error estimator is cheaper to evaluate than previous\nimplementations of the $\\tau$-estimation procedure and that it obtains more\naccurate extrapolations of the truncation error for representations of a higher\norder than the reference mesh. The robustness of the method allows performing\nthe p-adaptation strategy with coarser reference solutions, thus further\nreducing the computational cost. The proposed estimator is validated using the\nmethod of manufactured solutions in a test case for the compressible\nNavier-Stokes equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.08439v1"
    },
    {
        "title": "Numerically Stable Recurrence Relations for the Communication Hiding\n  Pipelined Conjugate Gradient Method",
        "authors": [
            "Siegfried Cools",
            "Jeffrey Cornelis",
            "Wim Vanroose"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  Pipelined Krylov subspace methods (also referred to as communication-hiding\nmethods) have been proposed in the literature as a scalable alternative to\nclassic Krylov subspace algorithms for iteratively computing the solution to a\nlarge linear system in parallel. For symmetric and positive definite system\nmatrices the pipelined Conjugate Gradient method outperforms its classic\nConjugate Gradient counterpart on large scale distributed memory hardware by\noverlapping global communication with essential computations like the\nmatrix-vector product, thus hiding global communication. A well-known drawback\nof the pipelining technique is the (possibly significant) loss of numerical\nstability. In this work a numerically stable variant of the pipelined Conjugate\nGradient algorithm is presented that avoids the propagation of local rounding\nerrors in the finite precision recurrence relations that construct the Krylov\nsubspace basis. The multi-term recurrence relation for the basis vector is\nreplaced by two-term recurrences, improving stability without increasing the\noverall computational cost of the algorithm. The proposed modification ensures\nthat the pipelined Conjugate Gradient method is able to attain a highly\naccurate solution independently of the pipeline length. Numerical experiments\ndemonstrate a combination of excellent parallel performance and improved\nmaximal attainable accuracy for the new pipelined Conjugate Gradient algorithm.\nThis work thus resolves one of the major practical restrictions for the\nuseability of pipelined Krylov subspace methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.03100v2"
    },
    {
        "title": "Interpolation of scattered data in $\\mathbb{R}^3$ using minimum\n  $L_p$-norm networks, $1<p<\\infty$",
        "authors": [
            "Krassimira Vlachkova"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  We consider the extremal problem of interpolation of scattered data in\n$\\mathbb{R}^3$ by smooth curve networks with minimal $L_p$-norm of the second\nderivative for $1<p<\\infty$. The problem for $p=2$ was set and solved by\nNielson (1983). Andersson et al. (1995) gave a new proof of Nielson's result by\nusing a different approach. Partial results for the problem for $1<p<\\infty$\nwere announced without proof in (Vlachkova (1992)). Here we present a complete\ncharacterization of the solution for $1<p<\\infty$. Numerical experiments are\nvisualized and presented to illustrate and support our results.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.07264v1"
    },
    {
        "title": "Online adaptive basis refinement and compression for reduced-order\n  models via vector-space sieving",
        "authors": [
            "Philip A. Etter",
            "Kevin T. Carlberg"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  In many applications, projection-based reduced-order models (ROMs) have\ndemonstrated the ability to provide rapid approximate solutions to\nhigh-fidelity full-order models (FOMs). However, there is no a priori assurance\nthat these approximate solutions are accurate; their accuracy depends on the\nability of the low-dimensional trial basis to represent the FOM solution. As a\nresult, ROMs can generate inaccurate approximate solutions, e.g., when the FOM\nsolution at the online prediction point is not well represented by training\ndata used to construct the trial basis. To address this fundamental deficiency\nof standard model-reduction approaches, this work proposes a novel\nonline-adaptive mechanism for efficiently enriching the trial basis in a manner\nthat ensures convergence of the ROM to the FOM, yet does not incur any FOM\nsolves. The mechanism is based on the previously proposed adaptive\n$h$-refinement method for ROMs [12], but improves upon this work in two crucial\nways. First, the proposed method enables basis refinement with respect to any\northogonal basis (not just the Kronecker basis), thereby generalizing the\nrefinement mechanism and enabling it to be tailored to the physics\ncharacterizing the problem at hand. Second, the proposed method provides a fast\nonline algorithm for periodically compressing the enriched basis via an\nefficient proper orthogonal decomposition (POD) method, which does not incur\nany operations that scale with the FOM dimension. These two features allow the\nproposed method to serve as (1) a failsafe mechanism for ROMs, as the method\nenables the ROM to satisfy any prescribed error tolerance online (even in the\ncase of inadequate training), and (2) an efficient online basis-adaptation\nmechanism, as the combination of basis enrichment and compression enables the\nbasis to adapt online while controlling its dimension.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10659v2"
    },
    {
        "title": "A Note on Adjoint Linear Algebra",
        "authors": [
            "Uwe Naumann"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  A new proof for adjoint systems of linear equations is presented. The\nargument is built on the principles of Algorithmic Differentiation. Application\nto scalar multiplication sets the base line. Generalization yields adjoint\ninner vector, matrix-vector, and matrix-matrix products leading to an\nalternative proof for first- as well as higher-order adjoint linear systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.00578v1"
    },
    {
        "title": "Fast and Accurate Proper Orthogonal Decomposition using Efficient\n  Sampling and Iterative Techniques for Singular Value Decomposition",
        "authors": [
            "V. Charumathi",
            "M. Ramakrishna",
            "Vinita Vasudevan"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  In this paper, we propose a computationally efficient iterative algorithm for\nproper orthogonal decomposition (POD) using random sampling based techniques.\nIn this algorithm, additional rows and columns are sampled and a merging\ntechnique is used to update the dominant POD modes in each iteration. We derive\nbounds for the spectral norm of the error introduced by a series of merging\noperations. We use an existing theorem to get an approximate measure of the\nquality of subspaces obtained on convergence of the iteration. Results on\nvarious datasets indicate that the POD modes and/or the subspaces are\napproximated with excellent accuracy with a significant runtime improvement\nover computing the truncated SVD. We also propose a method to compute the POD\nmodes of large matrices that do not fit in the RAM using this iterative\nsampling and merging algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.05107v3"
    },
    {
        "title": "On the Convergence Rate of Variants of the Conjugate Gradient Algorithm\n  in Finite Precision Arithmetic",
        "authors": [
            "Anne Greenbaum",
            "Hexuan Liu",
            "Tyler Chen"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  We consider three mathematically equivalent variants of the conjugate\ngradient (CG) algorithm and how they perform in finite precision arithmetic. It\nwas shown in [{\\em Behavior of slightly perturbed Lanczos and\nconjugate-gradient recurrences}, Lin.~Alg.~Appl., 113 (1989), pp.~7-63] that\nunder certain conditions the convergence of a slightly perturbed CG computation\nis like that of exact CG for a matrix with many eigenvalues distributed\nthroughout tiny intervals about the eigenvalues of the given matrix, the size\nof the intervals being determined by how closely these conditions are\nsatisfied. We determine to what extent each of these variants satisfies the\ndesired conditions, using a set of test problems and show that there is\nsignificant correlation between how well these conditions are satisfied and how\nwell the finite precision computation converges before reaching its ultimately\nattainable accuracy. We show that for problems where the width of the intervals\ncontaining the eigenvalues of the associated exact CG matrix makes a\nsignificant difference in the behavior of exact CG, the different CG variants\nbehave differently in finite precision arithmetic. For problems where the\ninterval width makes little difference or where the convergence of exact CG is\nessentially governed by the upper bound based on the square root of the\ncondition number of the matrix, the different CG variants converge similarly in\nfinite precision arithmetic until the ultimate level of accuracy is achieved,\nalthough this ultimate level of accuracy may be different for the different\nvariants. This points to the need for testing new CG variants on problems that\nare especially sensitive to rounding errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.05874v2"
    },
    {
        "title": "Approximation of a fractional power of an elliptic operator",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  Some mathematical models of applied problems lead to the need of solving\nboundary value problems with a fractional power of an elliptic operator. In a\nnumber of works, approximations of such a nonlocal operator are constructed on\nthe basis of an integral representation with a singular integrand. In the\npresent paper, new and more convenient integral representations are proposed\nfor operators with fractional powers. Approximations are based on the classical\nquadrature formulas. The results of numerical experiments on the accuracy of\nquadrature formulas are presented. The proposed approximations are used for\nnumerical solving a model two-dimensional boundary value problem for fractional\ndiffusion.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.10838v1"
    },
    {
        "title": "A Two-Step High-Order Compact Scheme for the Laplacian Operator and its\n  Implementation in an Explicit Method for Integrating the Nonlinear\n  Schrödinger Equation",
        "authors": [
            "R. M. Caplan",
            "R. Carretero"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  We describe and test an easy-to-implement two-step high-order compact (2SHOC)\nscheme for the Laplacian operator and its implementation into an explicit\nfinite-difference scheme for simulating the nonlinear Schr\\\"odinger equation\n(NLSE). Our method relies on a compact `double-differencing' which is shown to\nbe computationally equivalent to standard fourth-order non-compact schemes.\nThrough numerical simulations of the NLSE using fourth-order Runge-Kutta, we\nconfirm that our scheme shows the desired fourth-order accuracy. A computation\nand storage requirement comparison is made between the 2SHOC scheme and the\nnon-compact equivalent scheme for both the Laplacian operator alone, as well as\nwhen implemented in the NLSE simulations. Stability bounds are also shown in\norder to get maximum efficiency out of the method. We conclude that the modest\nincrease in storage and computation of the 2SHOC schemes are well worth the\nadvantages of having the schemes compact, and their ease of implementation\nmakes their use very useful for practical implementations.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.1027v2"
    },
    {
        "title": "On the control of the load increments for a proper description of\n  multiple delamination in a domain decomposition framework",
        "authors": [
            "Olivier Allix",
            "Pierre Kerfriden",
            "Pierre Gosselet"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  In quasi-static nonlinear time-dependent analysis, the choice of the time\ndiscretization is a complex issue. The most basic strategy consists in\ndetermining a value of the load increment that ensures the convergence of the\nsolution with respect to time on the base of preliminary simulations. In more\nadvanced applications, the load increments can be controlled for instance by\nprescribing the number of iterations of the nonlinear resolution procedure, or\nby using an arc-length algorithm. These techniques usually introduce a\nparameter whose correct value is not easy to obtain. In this paper, an\nalternative procedure is proposed. It is based on the continuous control of the\nresidual of the reference problem over time, whose measure is easy to\ninterpret. This idea is applied in the framework of a multiscale domain\ndecomposition strategy in order to perform 3D delamination analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.5913v1"
    },
    {
        "title": "An FMM Based on Dual Tree Traversal for Many-core Architectures",
        "authors": [
            "Rio Yokota"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The present work attempts to integrate the independent efforts in the fast\nN-body community to create the fastest N-body library for many-core and\nheterogenous architectures. Focus is placed on low accuracy optimizations, in\nresponse to the recent interest to use FMM as a preconditioner for sparse\nlinear solvers. A direct comparison with other state-of-the-art fast N-body\ncodes demonstrates that orders of magnitude increase in performance can be\nachieved by careful selection of the optimal algorithm and low-level\noptimization of the code. The current N-body solver uses a fast multipole\nmethod with an efficient strategy for finding the list of cell-cell\ninteractions by a dual tree traversal. A task-based threading model is used to\nmaximize thread-level parallelism and intra-node load-balancing. In order to\nextract the full potential of the SIMD units on the latest CPUs, the inner\nkernels are optimized using AVX instructions. Our code -- exaFMM -- is an order\nof magnitude faster than the current state-of-the-art FMM codes, which are\nthemselves an order of magnitude faster than the average FMM code.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.3516v3"
    },
    {
        "title": "About Notations in Multiway Array Processing",
        "authors": [
            "Jeremy E. Cohen"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This paper gives an overview of notations used in multiway array processing.\nWe redefine the vectorization and matricization operators to comply with some\nproperties of the Kronecker product. The tensor product and Kronecker product\nare also represented with two different symbols, and it is shown how these\nnotations lead to clearer expressions for multiway array operations. Finally,\nthe paper recalls the useful yet widely unknown properties of the array normal\nlaw with suggested notations.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.01306v2"
    },
    {
        "title": "A Simple Approach to Optimal CUR Decomposition",
        "authors": [
            "Haishan Ye",
            "Yujun Li",
            "Zhihua Zhang"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Prior optimal CUR decomposition and near optimal column reconstruction\nmethods have been established by combining BSS sampling and adaptive sampling.\nIn this paper, we propose a new approach to the optimal CUR decomposition and\nnear optimal column reconstruction by just using leverage score sampling. In\nour approach, both the BSS sampling and adaptive sampling are not needed.\nMoreover, our approach is the first $O(\\mathrm{nnz}(\\A))$ optimal CUR algorithm\nwhere $\\A$ is a data matrix in question. We also extend our approach to the\nNystr{\\\"o}m method, obtaining a fast algorithm which runs $\\tilde{O}(n^{2})$ or\n$O(\\mathrm{\\nnz}(\\A))$\n",
        "pdf_link": "http://arxiv.org/pdf/1511.01598v4"
    },
    {
        "title": "Accelerating Random Kaczmarz Algorithm Based on Clustering Information",
        "authors": [
            "Yujun Li",
            "Kaichun Mo",
            "Haishan Ye"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Kaczmarz algorithm is an efficient iterative algorithm to solve\noverdetermined consistent system of linear equations. During each updating\nstep, Kaczmarz chooses a hyperplane based on an individual equation and\nprojects the current estimate for the exact solution onto that space to get a\nnew estimate. Many vairants of Kaczmarz algorithms are proposed on how to\nchoose better hyperplanes. Using the property of randomly sampled data in\nhigh-dimensional space, we propose an accelerated algorithm based on clustering\ninformation to improve block Kaczmarz and Kaczmarz via Johnson-Lindenstrauss\nlemma. Additionally, we theoretically demonstrate convergence improvement on\nblock Kaczmarz algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.05362v2"
    },
    {
        "title": "Empirical Research and Automatic Processing Method of Precision-specific\n  Operation",
        "authors": [
            "Ran Wang",
            "Xinrui He"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Significant inaccuracy often occurs during the process of mathematical\ncalculation due to the digit limitation of floating point, which may lead to\ncatastrophic loss. Normally, people believe that adjustment of floating-point\nprecision is an effective way to solve this problem, since high-precision\nfloating-point has more digits to store information. Thus, it is a prevalent\nmethod to reduce the inaccuracy in much floating-point related research, that\nperforming all the operations with higher precision. However, we discover that\nsome operations may lead to larger error in higher precision. In this paper, we\ndefine this kind of operation that generates large error due to precision\nadjustment a precision-specific operation. Furthermore, we propose a\nlight-weight searching algorithm for detecting precision-specific operations\nand figure out an automatic processing method to fixing them. In addition, we\nconducted an experiment on the scientific mathematical library of GLIBC. The\nresult shows that there are many precision-specific operations, and our fixing\napproach can significantly reduce the inaccuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.06227v2"
    },
    {
        "title": "Wilkinson's Inertia-Revealing Factorization and Its Application to\n  Sparse Matrices",
        "authors": [
            "Alex Druinsky",
            "Eyal Carlebach",
            "Sivan Toledo"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We propose a new inertia-revealing factorization for sparse symmetric\nmatrices. The factorization scheme and the method for extracting the inertia\nfrom it were proposed in the 1960s for dense, banded, or tridiagonal matrices,\nbut they have been abandoned in favor of faster methods. We show that this\nscheme can be applied to any sparse symmetric matrix and that the fill in the\nfactorization is bounded by the fill in the sparse QR factorization of the same\nmatrix (but is usually much smaller). We describe our serial proof-of-concept\nimplementation, and present experimental results, studying the method's\nnumerical stability and performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.08547v2"
    },
    {
        "title": "Recover Fine-Grained Spatial Data from Coarse Aggregation",
        "authors": [
            "Bang Liu",
            "Borislav Mavrin",
            "Linglong Kong",
            "Di Niu"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In this paper, we study a new type of spatial sparse recovery problem, that\nis to infer the fine-grained spatial distribution of certain density data in a\nregion only based on the aggregate observations recorded for each of its\nsubregions. One typical example of this spatial sparse recovery problem is to\ninfer spatial distribution of cellphone activities based on aggregate mobile\ntraffic volumes observed at sparsely scattered base stations. We propose a\nnovel Constrained Spatial Smoothing (CSS) approach, which exploits the local\ncontinuity that exists in many types of spatial data to perform sparse recovery\nvia finite-element methods, while enforcing the aggregated observation\nconstraints through an innovative use of the ADMM algorithm. We also improve\nthe approach to further utilize additional geographical attributes. Extensive\nevaluations based on a large dataset of phone call records and a demographical\ndataset from the city of Milan show that our approach significantly outperforms\nvarious state-of-the-art approaches, including Spatial Spline Regression (SSR).\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00192v1"
    },
    {
        "title": "Stability Analysis of Inexact Solves in Moment Matching based Model\n  Reduction",
        "authors": [
            "Navneet Pratap Singh",
            "Kapil Ahuja"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Recently a new algorithm for model reduction of second order linear dynamical\nsystems with proportional damping, the Adaptive Iterative Rational Global\nArnoldi (AIRGA) algorithm (Bonin et. al., 2016), has been proposed. The main\ncomputational cost of the AIRGA algorithm is solving a sequence of linear\nsystems. Usually, direct methods (e.g., LU) are used for solving these systems.\nAs model sizes grow, direct methods become prohibitively expensive. Iterative\nmethods (e.g., Krylov) scale well with size, and hence, are a good choice with\nan appropriate preconditioner.\n  Preconditioned iterative methods introduce errors in linear solves because\nthey are not exact. They solve linear systems up to a certain tolerance. We\nprove that, under mild conditions, the AIRGA algorithm is backward stable with\nrespect to the errors introduced by these inexact linear solves. Our first\nassumption is use of a Ritz-Galerkin based solver that satisfies few extra\northogonality conditions. Since Conjugate Gradient (CG) is the most popular\nmethod based upon the Ritz-Galerkin theory, we use it. We show how to modify CG\nto achieve these extra orthogonalities.\n  Modifying CG with the suggested changes is non-trivial. Hence, we demonstrate\nthat using Recycling CG (RCG) helps us achieve these orthogonalities with no\ncode changes. The extra cost of orthogonalizations is often offset by savings\nbecause of recycling. Our second and third assumptions involve existence,\ninvertibility and boundedness of two matrices, which are easy to satisfy.\n  While satisfying the backward stability assumptions, by numerical experiments\nwe show that as we iteratively solve the linear systems arising in the AIRGA\nalgorithm more accurately, we obtain a more accurate reduced system. We also\nshow that recycling Krylov subspaces helps satisfy the backward stability\nassumptions (extra-orthogonalities) at almost no extra cost.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09283v1"
    },
    {
        "title": "Influence of the Forward Difference Scheme for the Time Derivative on\n  the Stability of Wave Equation Numerical Solution",
        "authors": [
            "Aslam Abdullah"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Research on numerical stability of difference equations has been quite\nintensive in the past century. The choice of difference schemes for the\nderivative terms in these equations contributes to a wide range of the\nstability analysis issues - one of which is how a chosen scheme may directly or\nindirectly contribute to such stability. In the present paper, how far the\nforward difference scheme for the time derivative in the wave equation\ninfluences the stability of the equation numerical solution, is particularly\ninvestigated. The stability analysis of the corresponding difference equation\ninvolving four schemes, namely Lax's, central, forward, and rearward\ndifferences, were carried out, and the resulting stability criteria were\ncompared. The results indicate that the instability of the solution of wave\nequation is not always due to the forward difference scheme for the time\nderivative. Rather, it is shown in this paper that the stability criterion is\nstill possible when the spatial derivative is represented by an appropriate\ndifference scheme. This sheds light on the degree of applicability of a\ndifference scheme for a hyperbolic equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00440v1"
    },
    {
        "title": "Higher-dimension Tensor Completion via Low-rank Tensor Ring\n  Decomposition",
        "authors": [
            "Longhao Yuan",
            "Jianting Cao",
            "Qiang Wu",
            "Qibin Zhao"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The problem of incomplete data is common in signal processing and machine\nlearning. Tensor completion algorithms aim to recover the incomplete data from\nits partially observed entries. In this paper, taking advantages of high\ncompressibility and flexibility of recently proposed tensor ring (TR)\ndecomposition, we propose a new tensor completion approach named tensor ring\nweighted optimization (TR-WOPT). It finds the latent factors of the incomplete\ntensor by gradient descent algorithm, then the latent factors are employed to\npredict the missing entries of the tensor. We conduct various tensor completion\nexperiments on synthetic data and real-world data. The simulation results show\nthat TR-WOPT performs well in various high-dimension tensors. Furthermore,\nimage completion results show that our proposed algorithm outperforms the\nstate-of-the-art algorithms in many situations. Especially when the missing\nrate of the test images is high (e.g., over 0.9), the performance of our\nTR-WOPT is significantly better than the compared algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.01589v2"
    },
    {
        "title": "Ideal Preconditioners for Saddle Point Systems with a Rank-Deficient\n  Leading Block",
        "authors": [
            "Susanne Bradley"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We consider the iterative solution of symmetric saddle point systems with a\nrank-deficient leading block. We develop two preconditioners that, under\ncertain assumptions on the rank structure of the system, yield a preconditioned\nmatrix with a constant number of eigenvalues. We then derive some properties of\nthe inverse of a particular class of saddle point system and exploit these to\ndevelop a third preconditioner, which remains ideal even when the earlier\nassumptions on rank structure are relaxed.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.08590v1"
    },
    {
        "title": "VeST: Very Sparse Tucker Factorization of Large-Scale Tensors",
        "authors": [
            "Moonjeong Park",
            "Jun-Gi Jang",
            "Lee Sael"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  Given a large tensor, how can we decompose it to sparse core tensor and\nfactor matrices such that it is easier to interpret the results? How can we do\nthis without reducing the accuracy? Existing approaches either output dense\nresults or give low accuracy. In this paper, we propose VeST, a tensor\nfactorization method for partially observable data to output a very sparse core\ntensor and factor matrices. VeST performs initial decomposition, determines\nunimportant entries in the decomposition results, removes the unimportant\nentries, and carefully updates the remaining entries. To determine unimportant\nentries, we define and use entry-wise 'responsibility' for the decomposed\nresults. The entries are updated iteratively in a coordinate descent manner in\nparallel for scalable computation. Extensive experiments show that our method\nVeST is at least 2.2 times more sparse and at least 2.8 times more accurate\ncompared to competitors. Moreover, VeST is scalable in terms of input order,\ndimension, and the number of observable entries. Thanks to VeST, we\nsuccessfully interpret the result of real-world tensor data based on the\nsparsity pattern of the resulting factor matrices.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.02603v3"
    },
    {
        "title": "Tensor B-Spline Numerical Methods for PDEs: a High-Performance\n  Alternative to FEM",
        "authors": [
            "Dmytro Shulga",
            "Oleksii Morozov",
            "Volker Roth",
            "Felix Friedrich",
            "Patrick Hunziker"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  Tensor B-spline methods are a high-performance alternative to solve partial\ndifferential equations (PDEs). This paper gives an overview on the principles\nof Tensor B-spline methodology, shows their use and analyzes their performance\nin application examples, and discusses its merits. Tensors preserve the\ndimensional structure of a discretized PDE, which makes it possible to develop\nhighly efficient computational solvers. B-splines provide high-quality\napproximations, lead to a sparse structure of the system operator represented\nby shift-invariant separable kernels in the domain, and are mesh-free by\nconstruction. Further, high-order bases can easily be constructed from\nB-splines. In order to demonstrate the advantageous numerical performance of\ntensor B-spline methods, we studied the solution of a large-scale heat-equation\nproblem (consisting of roughly 0.8 billion nodes!) on a heterogeneous\nworkstation consisting of multi-core CPU and GPUs. Our experimental results\nnicely confirm the excellent numerical approximation properties of tensor\nB-splines, and their unique combination of high computational efficiency and\nlow memory consumption, thereby showing huge improvements over standard\nfinite-element methods (FEM).\n",
        "pdf_link": "http://arxiv.org/pdf/1904.03057v1"
    },
    {
        "title": "Sparse Identification of Truncation Errors",
        "authors": [
            "Stephan Thaler",
            "Ludger Paehler",
            "Nikolaus A. Adams"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  This work presents a data-driven approach to the identification of spatial\nand temporal truncation errors for linear and nonlinear discretization schemes\nof Partial Differential Equations (PDEs). Motivated by the central role of\ntruncation errors, for example in the creation of implicit Large Eddy schemes,\nwe introduce the Sparse Identification of Truncation Errors (SITE) framework to\nautomatically identify the terms of the modified differential equation from\nsimulation data. We build on recent advances in the field of data-driven\ndiscovery and control of complex systems and combine it with classical work on\nmodified differential equation analysis of Warming, Hyett, Lerat and Peyret. We\naugment a sparse regression-rooted approach with appropriate preconditioning\nroutines to aid in the identification of the individual modified differential\nequation terms. The construction of such a custom algorithm pipeline allows\nattenuating of multicollinearity effects as well as automatic tuning of the\nsparse regression hyperparameters using the Bayesian information criterion\n(BIC). As proof of concept, we constrain the analysis to finite difference\nschemes and leave other numerical schemes open for future inquiry. Test cases\ninclude the linear advection equation with a forward-time, backward-space\ndiscretization, the Burgers' equation with a MacCormack predictor-corrector\nscheme and the Korteweg-de Vries equation with a Zabusky and Kruska\ndiscretization scheme. Based on variation studies, we derive guidelines for the\nselection of discretization parameters, preconditioning approaches and sparse\nregression algorithms. The results showcase highly accurate predictions\nunderlining the promise of SITE for the analysis and optimization of\ndiscretization schemes, where analytic derivation of modified differential\nequations is infeasible.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.03669v2"
    },
    {
        "title": "Data-driven discovery of partial differential equation models with\n  latent variables",
        "authors": [
            "Patrick A. K. Reinbold",
            "Roman O. Grigoriev"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  In spatially extended systems, it is common to find latent variables that are\nhard, or even impossible, to measure with acceptable precision, but are\ncrucially important for the proper description of the dynamics. This\nsubstantially complicates construction of an accurate model for such systems\nusing data-driven approaches. The present paper illustrates how physical\nconstraints can be employed to overcome this limitation using the example of a\nweakly turbulent quasi-two-dimensional Kolmogorov flow driven by a steady\nLorenz force with an unknown spatial profile. Specifically, the terms involving\nlatent variables in the partial differential equations governing the dynamics\ncan be eliminated at the expense of raising the order of that equation. We show\nthat local polynomial interpolation combined with symbolic regression can\nhandle sparse data on grids that are representative of typical experimental\nmeasurement techniques such as particle image velocimetry. However, we also\nfind that the reconstructed model is sensitive to measurement noise and trace\nthis sensitivity to the presence of high order spatial and/or temporal\nderivatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.04314v1"
    },
    {
        "title": "Construction of conformal maps based on the locations of singularities\n  for improving the double exponential formula",
        "authors": [
            "Shunki Kyoya",
            "Ken'ichiro Tanaka"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  The double exponential formula, or the DE formula, is a high-precision\nintegration formula using a change of variables called a DE transformation;\nwhereas there is a disadvantage that it is sensitive to singularities of an\nintegrand near the real axis. To overcome this disadvantage, Slevinsky and\nOlver (SIAM J. Sci. Comput., 2015) attempted to improve it by constructing\nconformal maps based on the locations of singularities. Based on their ideas,\nwe construct a new transformation formula. Our method employs special types of\nthe Schwarz-Christoffel transformations for which we can derive their explicit\nform. Then, the new transformation formula can be regarded as a generalization\nof the DE transformations. We confirm its effectiveness by numerical\nexperiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.05989v1"
    },
    {
        "title": "Approximation and Uncertainty Quantification of Systems with Arbitrary\n  Parameter Distributions using Weighted Leja Interpolation",
        "authors": [
            "Dimitrios Loukrezis",
            "Herbert De Gersem"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  Approximation and uncertainty quantification methods based on Lagrange\ninterpolation are typically abandoned in cases where the probability\ndistributions of one or more {system} parameters are not normal, uniform, or\nclosely related {distributions}, due to the computational issues that arise\nwhen one wishes to define interpolation nodes for general distributions. This\npaper examines the use of the recently introduced weighted Leja nodes for that\npurpose. Weighted Leja interpolation rules are presented, along with a\ndimension-adaptive sparse interpolation algorithm, to be employed in the case\nof high-dimensional input uncertainty. The performance and reliability of the\nsuggested approach is verified by four numerical experiments, where the\nrespective models feature extreme value and truncated normal parameter\ndistributions. Furthermore, the suggested approach is compared with a\nwell-established polynomial chaos method and found to be either comparable or\nsuperior in terms of approximation and statistics estimation accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.07709v2"
    },
    {
        "title": "Threshold shift method for reliability-based design optimization",
        "authors": [
            "Somdatta Goswami",
            "Souvik Chakraborty",
            "Rajib Chowdhury",
            "Timon Rabczuk"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  We present a novel approach, referred to as the 'threshold shift method'\n(TSM), for reliability based design optimization (RBDO). The proposed approach\nis similar in spirit with the sequential optimization and reliability analysis\n(SORA) method where the RBDO problem is decoupled into an optimization and a\nreliability analysis problem. However, unlike SORA that utilizes shift-vector\nto shift the design variables within a constraint (independently), in TSM we\npropose to shift the threshold of the constraints. We argue that modifying a\nconstraint, either by shifting the design variables (SORA) or by shifting the\nthreshold of the constraints (TSM), influences the other constraints of the\nsystem. Therefore, we propose to determine the thresholds for all the\nconstraints by solving a single optimization problem. Additionally, the\nproposed TSM is equipped with an active-constraint determination scheme. To\nmake the method scalable, a practical algorithm for TSM that utilizes two\nsurrogate models is proposed. Unlike the conventional RBDO methods, the\nproposed approach has the ability to handle highly non-linear probabilistic\nconstraints. The performance of the proposed approach is examined on six\nbenchmark problems selected from the literature. The proposed approach yields\nexcellent results outperforming other popular methods in literature. As for the\ncomputational efficiency, the proposed approach is found to be highly\nefficient, indicating it's future application to other real-life problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11424v1"
    },
    {
        "title": "Floating-Point Arithmetic on Round-to-Nearest Representations",
        "authors": [
            "Peter Kornerup",
            "Jean-Michel Muller",
            "Adrien Panhaleux"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Recently we introduced a class of number representations denoted\nRN-representations, allowing an un-biased rounding-to-nearest to take place by\na simple truncation. In this paper we briefly review the binary fixed-point\nrepresentation in an encoding which is essentially an ordinary 2's complement\nrepresentation with an appended round-bit. Not only is this rounding a constant\ntime operation, so is also sign inversion, both of which are at best log-time\noperations on ordinary 2's complement representations. Addition, multiplication\nand division is defined in such a way that rounding information can be carried\nalong in a meaningful way, at minimal cost. Based on the fixed-point encoding\nwe here define a floating point representation, and describe to some detail a\npossible implementation of a floating point arithmetic unit employing this\nrepresentation, including also the directed roundings.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3914v1"
    },
    {
        "title": "Floating-Point Numbers with Error Estimates (revised)",
        "authors": [
            "Glauco Masotti"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The study addresses the problem of precision in floating-point (FP)\ncomputations. A method for estimating the errors which affect intermediate and\nfinal results is proposed and a summary of many software simulations is\ndiscussed. The basic idea consists of representing FP numbers by means of a\ndata structure collecting value and estimated error information. Under certain\nconstraints, the estimate of the absolute error is accurate and has a compact\nstatistical distribution. By monitoring the estimated relative error during a\ncomputation (an ad-hoc definition of relative error has been used), the\nvalidity of results can be ensured. The error estimate enables the\nimplementation of robust algorithms, and the detection of ill-conditioned\nproblems. A dynamic extension of number precision, under the control of error\nestimates, is advocated, in order to compute results within given error bounds.\nA reduced time penalty could be achieved by a specialized FP processor. The\nrealization of a hardwired processor incorporating the method, with current\ntechnology, should not be anymore a problem and would make the practical\nadoption of the method feasible for most applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.5975v1"
    },
    {
        "title": "The DFLU flux for systems of conservation laws",
        "authors": [
            "Adi Adimurthi",
            "G. D. Veerappa Gowda",
            "Jérôme Jaffré"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The DFLU numerical flux was introduced in order to solve hyperbolic scalar\nconservation laws with a flux function discontinuous in space. We show how this\nflux can be used to solve certain class of systems of conservation laws such as\nsystems modeling polymer flooding in oil reservoir engineering. Furthermore,\nthese results are extended to the case where the flux function is discontinuous\nin the space variable. Such a situation arises for example while dealing with\noil reservoirs which are heterogeneous. Numerical experiments are presented to\nillustrate the efficiency of this new scheme compared to other standard schemes\nlike upstream mobility, Lax-Friedrichs and Force schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.0190v1"
    },
    {
        "title": "Mathematical analysis of a discrete fracture model coupling Darcy flow\n  in the matrix with Darcy-Forchheimer flow in the fracture",
        "authors": [
            "Peter Knabner",
            "Jean Roberts"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We consider a model for flow in a porous medium with a fracture in which the\nflow in the fracture is governed by the Darcy-Forchheimer law while that in the\nsurrounding matrix is governed by Darcy's law. We give an appropriate mixed,\nvariational formulation and show existence and uniqueness of the solution. To\nshow existence we give an analogous formulation for the model in which the\nDarcy-Forchheimer law is the governing equation throughout the domain. We show\nexistence and uniqueness of the solution and show that the solution for the\nmodel with Darcy's law in the matrix is the weak limit of solutions of the\nmodel with the Darcy-Forchheimer law in the entire domain when the Forchheimer\ncoefficient in the matrix tends toward zero.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.0193v1"
    },
    {
        "title": "Twofold fast summation",
        "authors": [
            "Evgeny Latkin"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Debugging accumulation of floating-point errors is hard; ideally, computer\nshould track it automatically. Here we consider twofold approximation of an\nexact real with value + error pair of floating-point numbers. Normally, value +\nerror sum is more accurate than value alone, so error can estimate deviation\nbetween value and its exact target. Fast summation algorithm, that provides\ntwofold sum of x[1]+...+x[N] or dot product x[1]*y[1]+...+x[N]*y[N], can be\nsame fast as direct summation sometimes if leveraging processor underused\npotential. This way, we can hit three goals: improve precision, track\ninaccuracy, and do this with little if any loss in performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.0248v2"
    },
    {
        "title": "Domain decomposition methods with overlapping subdomains for\n  time-dependent problems",
        "authors": [
            "Petr Vabishchevich",
            "Petr Zakharov"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Domain decomposition (DD) methods for solving time-dependent problems can be\nclassified by (i) the method of domain decomposition used, (ii) the choice of\ndecomposition operators (exchange of boundary conditions), and (iii) the\nsplitting scheme employed. To construct homogeneous numerical algorithms,\noverlapping subdomain methods are preferable. Domain decomposition is\nassociated with the corresponding additive representation of the problem\noperator. To solve time-dependent problems with the DD splitting, different\noperator-splitting schemes are used. Various variants of decomposition\noperators differ by distinct types of data exchanges on interfaces. They ensure\nthe convergence of the approximate solution in various spaces of grid\nfunctions.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.0798v2"
    },
    {
        "title": "Twofold fast arithmetic",
        "authors": [
            "Evgeny Latkin"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Can we assure math computations by automatic verifying floating-point\naccuracy? We define fast arithmetic (based on Dekker [1971]) over twofold\napproximations $z\\approx z_0+z_1$, such that $z_0$ is standard result and $z_1$\nassesses inaccuracy $\\Delta z_0=z-z_0$. We propose on-fly tracking $z_1$,\ndetecting if $\\Delta z_0$ appears too high. We believe permanent tracking is\nworth its cost. C++ test code for Intel AVX available via web.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.6235v6"
    },
    {
        "title": "Towards a Resolution of P = NP Conjecture",
        "authors": [
            "Garimella Rama Murthy"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this research paper, the problem of optimization of a quadratic form over\nthe convex hull generated by the corners of hypercube is attempted and solved.\nIt is reasoned that under some conditions, the optimum occurs at the corners of\nhypercube. Results related to the computation of global optimum stable state\n(an NP hard problem) are discussed. An algorithm is proposed. It is hoped that\nthe results shed light on resolving the P not equal to NP problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.7129v1"
    },
    {
        "title": "Overview of Constrained PARAFAC Models",
        "authors": [
            "Gérard Favier",
            "André L. F. de Almeida"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this paper, we present an overview of constrained PARAFAC models where the\nconstraints model linear dependencies among columns of the factor matrices of\nthe tensor decomposition, or alternatively, the pattern of interactions between\ndifferent modes of the tensor which are captured by the equivalent core tensor.\nSome tensor prerequisites with a particular emphasis on mode combination using\nKronecker products of canonical vectors that makes easier matricization\noperations, are first introduced. This Kronecker product based approach is also\nformulated in terms of the index notation, which provides an original and\nconcise formalism for both matricizing tensors and writing tensor models. Then,\nafter a brief reminder of PARAFAC and Tucker models, two families of\nconstrained tensor models, the co-called PARALIND/CONFAC and PARATUCK models,\nare described in a unified framework, for $N^{th}$ order tensors. New tensor\nmodels, called nested Tucker models and block PARALIND/CONFAC models, are also\nintroduced. A link between PARATUCK models and constrained PARAFAC models is\nthen established. Finally, new uniqueness properties of PARATUCK models are\ndeduced from sufficient conditions for essential uniqueness of their associated\nconstrained PARAFAC models.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.7442v2"
    },
    {
        "title": "An improved car-following model considering variable safety headway\n  distance",
        "authors": [
            "Yuhan Jia",
            "Jianping Wu",
            "Yiman Du"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Considering high speed following on expressway or highway, an improved\ncar-following model is developed in this paper by introducing variable safety\nheadway distance. Stability analysis of the new model is carried out using the\ncontrol theory method. Finally, numerical simulations are implemented and the\nresults show good consistency with theoretical study.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.3374v1"
    },
    {
        "title": "Approximate Exponential Algorithms to Solve the Chemical Master Equation",
        "authors": [
            "Azam S. Zavar Moosavi",
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This paper discusses new simulation algorithms for stochastic chemical\nkinetics that exploit the linearity of the chemical master equation and its\nmatrix exponential exact solution. These algorithms make use of various\napproximations of the matrix exponential to evolve probability densities in\ntime. A sampling of the approximate solutions of the chemical master equation\nis used to derive accelerated stochastic simulation algorithms. Numerical\nexperiments compare the new methods with the established stochastic simulation\nalgorithm and the tau-leaping method.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.1934v1"
    },
    {
        "title": "Newton-Ellipsoid Method and its Polynomiography",
        "authors": [
            "Bahman Kalantari",
            "Eric Lee"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  We introduce a new iterative root-finding method for complex polynomials,\ndubbed {\\it Newton-Ellipsoid} method. It is inspired by the Ellipsoid method, a\nclassical method in optimization, and a property of Newton's Method derived in\n\\cite{kalFTA}, according to which at each complex number a half-space can be\nfound containing a root. Newton-Ellipsoid method combines this property, bounds\non zeros, together with the plane-cutting properties of the Ellipsoid Method.\nWe present computational results for several examples, as well as corresponding\npolynomiography. Polynomiography refers to algorithmic visualization of\nroot-finding. Newton's method is the first member of the infinite family of\niterations, the {\\it basic family}. We also consider general versions of this\nellipsoid approach where Newton's method is replaced by a higher-order member\nof the family such as Halley's method.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.2202v1"
    },
    {
        "title": "Solving stochastic chemical kinetics by Metropolis Hastings sampling",
        "authors": [
            "Azam S. Zavar Moosavi",
            "Paul Tranquilli",
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This study considers using Metropolis-Hastings algorithm for stochastic\nsimulation of chemical reactions. The proposed method uses SSA (Stochastic\nSimulation Algorithm) distribution which is a standard method for solving\nwell-stirred chemically reacting systems as a desired distribution. A new\nnumerical solvers based on exponential form of exact and approximate solutions\nof CME (Chemical Master Equation) is employed for obtaining target and proposal\ndistributions in Metropolis-Hastings algorithm to accelerate the accuracy of\nthe tau-leap method. Samples generated by this technique have the same\ndistribution as SSA and the histogram of samples show it's convergence to SSA.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.8155v1"
    },
    {
        "title": "Simultaneous diagonalization: the asymmetric, low-rank, and noisy\n  settings",
        "authors": [
            "Volodymyr Kuleshov",
            "Arun Tesjavi Chaganty",
            "Percy Liang"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Simultaneous matrix diagonalization is used as a subroutine in many machine\nlearning problems, including blind source separation and paramater estimation\nin latent variable models. Here, we extend algorithms for performing joint\ndiagonalization to low-rank and asymmetric matrices, and we also provide\nextensions to the perturbation analysis of these methods. Our results allow\njoint diagonalization to be applied in several new settings.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.06318v2"
    },
    {
        "title": "Stability and bifurcation properties of the algorithms for keeping of\n  differential equations solutions on the required level",
        "authors": [
            "Yu. V. Troshchiev"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Algorithms of control of differential equations solutions are under\ninvestigation in the article. Idealized and real modifications of the\nalgorithms are distinguished. An equation, which can be the base equation for\ninvestigation of the idealized algorithms properties, is constructed. The\ndifference appearing for real systems and real algorithms is for separate\ninvestigation. This difference tends to zero under tending to zero of the time\nstep of control. If the systems of equations satisfy or almost satisfy some\nproperties for which the algorithms are intended, then the results are similar\nnumerically as well. One of the algorithms demonstrates high reliability.\nAnother one is of more complex properties. Bifurcations, periodic solutions and\nstrange attractors are possible in both algorithms in addition to stable steady\nstates.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.00112v1"
    },
    {
        "title": "Efficient tensor completion: Low-rank tensor train",
        "authors": [
            "Ho N. Phien",
            "Hoang D. Tuan",
            "Johann A. Bengua",
            "Minh N. Do"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  This paper proposes a novel formulation of the tensor completion problem to\nimpute missing entries of data represented by tensors. The formulation is\nintroduced in terms of tensor train (TT) rank which can effectively capture\nglobal information of tensors thanks to its construction by a well-balanced\nmatricization scheme. Two algorithms are proposed to solve the corresponding\ntensor completion problem. The first one called simple low-rank tensor\ncompletion via tensor train (SiLRTC-TT) is intimately related to minimizing the\nTT nuclear norm. The second one is based on a multilinear matrix factorization\nmodel to approximate the TT rank of the tensor and called tensor completion by\nparallel matrix factorization via tensor train (TMac-TT). These algorithms are\napplied to complete both synthetic and real world data tensors. Simulation\nresults of synthetic data show that the proposed algorithms are efficient in\nestimating missing entries for tensors with either low Tucker rank or TT rank\nwhile Tucker-based algorithms are only comparable in the case of low Tucker\nrank tensors. When applied to recover color images represented by ninth-order\ntensors augmented from third-order ones, the proposed algorithms outperforms\nthe Tucker-based algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.01083v1"
    },
    {
        "title": "An Optimal Block Diagonal Preconditioner for Heterogeneous Saddle Point\n  Problems in Phase Separation",
        "authors": [
            "Pawan Kumar"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  The phase separation processes are typically modeled by Cahn-Hilliard\nequations. This equation was originally introduced to model phase separation in\nbinary alloys, where phase stands for concentration of different components in\nalloy. When the binary alloy under preparation is subjected to a rapid\nreduction in temperature below a critical temperature, it has been\nexperimentally observed that the concentration changes from a mixed state to a\nvisibly distinct spatially separated two phase for binary alloy. This rapid\nreduction in the temperature, the so-called \"deep quench limit\", is modeled\neffectively by obstacle potential. The discretization of Cahn-Hilliard equation\nwith obstacle potential leads to a block $2 \\times 2$ {\\em non-linear} system,\nwhere the $(1,1)$ block has a non-linear and non-smooth term. Recently a\nglobally convergent Newton Schur method was proposed for the non-linear Schur\ncomplement corresponding to this non-linear system. The proposed method is\nsimilar to an inexact active set method in the sense that the active sets are\nfirst approximately identified by solving a quadratic obstacle problem\ncorresponding to the $(1,1)$ block of the block $2 \\times 2$ system, and later\nsolving a reduced linear system by annihilating the rows and columns\ncorresponding to identified active sets. For solving the quadratic obstacle\nproblem, various optimal multigrid like methods have been proposed. In this\npaper, we study a non-standard norm that is equivalent to applying a block\ndiagonal preconditioner to the reduced linear systems. Numerical experiments\nconfirm the optimality of the solver and convergence independent of problem\nparameters on sufficiently fine mesh.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.03230v1"
    },
    {
        "title": "Fluid Dynamics Modeling : The Numerical Solution Of 2D Navier Hyperbolic\n  Equations",
        "authors": [
            "Erik Arakelyan",
            "Aram Serobyan",
            "Narek Jilavyan"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In the following paper we will consider Navier-Stokes problem and it's\ninterpretation by hyperbolic waves, focusing on wave propagation. We will begin\nwith solution for linear waves, then present problem for non-linear waves.\nLater we will derive for numerical solution using PDE's. Also we will design a\nMatlab program to solve and simulate wave propagation.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.05695v1"
    },
    {
        "title": "A New Pivot Selection Algorithm for Symmetric Indefinite Factorization\n  Arising in Quadratic Programming with Block Constraint Matrices",
        "authors": [
            "Duangpen Jetpipattanapong",
            "Gun Srijuntongsiri"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Quadratic programmingis a class of constrained optimization problem with\nquadratic objective functions and linear constraints. It has applications in\nmany areas and is also used to solve nonlinear optimization problems. This\narticle focuses on the equality constrained quadratic programs whose constraint\nmatrices are block diagonal. Using the direct solution method, we propose a new\npivot selection algorithm for the factorization of the Karush-Kuhn-Tucker(KKT)\nmatrix for this problem that maintains the sparsity and stability of the\nproblem. Our experiments show that our pivot selection algorithm appears to\nproduce no fill-ins in the factorizationof such matrices. In addition, we\ncompare our method with MA57 and find that the factors produced by our\nalgorithm are sparser.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.05758v2"
    },
    {
        "title": "New Pivot Selection for Sparse Symmetric Indefinite Factorization",
        "authors": [
            "Duangpen Jetpipattanapong",
            "Gun Srijuntongsiri"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We propose a new pivot selection technique for symmetric indefinite\nfactorization of sparse matrices. Such factorization should maintain both\nsparsity and numerical stability of the factors, both of which depend solely on\nthe choices of the pivots. Our method is based on the minimum degree algorithm\nand also considers the stability of the factors at the same time. Our\nexperiments show that our method produces factors that are sparser than the\nfactors computed by MA57 and are stable.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.06812v1"
    },
    {
        "title": "Pixel matrices: An elementary technique for solving nonlinear systems",
        "authors": [
            "David I. Spivak"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  A new technique for approximating the entire solution set for a nonlinear\nsystem of relations (nonlinear equations, inequalities, etc. involving\nalgebraic, smooth, or even continuous functions) is presented. The technique is\nto first plot each function as a pixel matrix, and to then perform a sequence\nof basic matrix operations, as dictated by how variables are shared by the\nrelations in the system. The result is a pixel matrix graphing the approximated\nsimultaneous solution set for the system.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.00190v3"
    },
    {
        "title": "A Necessary and Sufficient Condition for Local Maxima of Polynomial\n  Modulus Over Unit Disc",
        "authors": [
            "Bahman Kalantari"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  An important quantity associated with a complex polynomial $p(z)$ is $\\Vert p\n\\Vert_\\infty$, the maximum of its modulus over the unit disc $D$. We prove,\n$z_* \\in D$ is a local maximum of $|p(z)|$ if and only if $a_*$ satisfies,\n$z_*=p(z_*)|p'(z_*)|/p'(z_*)|p(z_*)|$, i.e. it is proportional to its\ncorresponding Newton direction. This explicit formula gives rise to novel\niterative algorithms for computing $\\Vert p \\Vert_\\infty$. We describe two such\nalgorithms, including a Newton-like method and present some visualization of\ntheir performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.00621v1"
    },
    {
        "title": "Abnormal Subspace Sparse PCA for Anomaly Detection and Interpretation",
        "authors": [
            "Xingyan Bin",
            "Ying Zhao",
            "Bilong Shen"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  The main shortage of principle component analysis (PCA) based anomaly\ndetection models is their interpretability. In this paper, our goal is to\npropose an interpretable PCA-based model for anomaly detection and\ninterpretation. The propose ASPCA model constructs principal components with\nsparse and orthogonal loading vectors to represent the abnormal subspace, and\nuses them to interpret detected anomalies. Our experiments on a synthetic\ndataset and two real world datasets showed that the proposed ASPCA models\nachieved comparable detection accuracies as the PCA model, and can provide\ninterpretations for individual anomalies.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.04644v1"
    },
    {
        "title": "Comments on \"A Square-Root-Free Matrix Decomposition Method for\n  Energy-Efficient Least Square Computation on Embedded Systems\"",
        "authors": [
            "Mohammad M. Mansour"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  A square-root-free matrix QR decomposition (QRD) scheme was rederived in [1]\nbased on [2] to simplify computations when solving least-squares (LS) problems\non embedded systems. The scheme of [1] aims at eliminating both the square-root\nand division operations in the QRD normalization and backward substitution\nsteps in the LS computations. It is claimed in [1] that the LS solution only\nrequires finding the directions of the orthogonal basis of the matrix in\nquestion, regardless of the normalization of their Euclidean norms. MIMO\ndetection problems have been named as potential applications that benefit from\nthis. While this is true for unconstrained LS problems, we conversely show here\nthat constrained LS problems such as MIMO detection still require computing the\nnorms of the orthogonal basis to produce the correct result.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05023v1"
    },
    {
        "title": "A Short Proof for Gap Independence of Simultaneous Iteration",
        "authors": [
            "Edo Liberty"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  This note provides a very short proof of a spectral gap independent property\nof the simultaneous iterations algorithm for finding the top singular space of\na matrix. See Rokhlin-Szlam-Tygert-2009, Halko-Martinsson-Tropp-2011 and\nMusco-Musco-2015. The proof is terse but completely self contained and should\nbe accessible to the linear algebra savvy reader.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05610v2"
    },
    {
        "title": "Space-Efficient Karatsuba Multiplication for Multi-Precision Integers",
        "authors": [
            "Yiping Cheng"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  The traditional Karatsuba algorithm for the multiplication of polynomials and\nmulti-precision integers has a time complexity of $O(n^{1.59})$ and a space\ncomplexity of $O(n)$. Roche proposed an improved algorithm with the same\n$O(n^{1.59})$ time complexity but with a much reduced $O(\\log n)$ space\ncomplexity. In Roche's paper details were provided for multiplication of\npolynomials, but not for multi-precision integers. Multi-precision integers\ndiffer from polynomials by the presence of carries, which poses difficulties in\nimplementing Roche's scheme in multi-precision integers. This paper provides a\ndetailed solution to these difficulties. Finally, numerical comparisons between\nthe schoolbook, traditional Karatsuba, and space-efficient Karatsuba algorithms\nare provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.06760v1"
    },
    {
        "title": "A Rank Revealing Randomized Singular Value Decomposition (R3SVD)\n  Algorithm for Low-rank Matrix Approximations",
        "authors": [
            "Hao Ji",
            "Wenjian Yu",
            "Yaohang Li"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In this paper, we present a Rank Revealing Randomized Singular Value\nDecomposition (R3SVD) algorithm to incrementally construct a low-rank\napproximation of a potentially large matrix while adaptively estimating the\nappropriate rank that can capture most of the actions of the matrix. Starting\nfrom a low-rank approximation with an initial guessed rank, R3SVD adopts an\northogonal Gaussian sampling approach to obtain the dominant subspace within\nthe leftover space, which is used to add up to the existing low-rank\napproximation. Orthogonal Gaussian sampling is repeated until an appropriate\nlow-rank approximation with satisfactory accuracy, measured by the overall\nenergy percentage of the original matrix, is obtained. While being a fast\nalgorithm, R3SVD is also a memory-aware algorithm where the computational\nprocess can be decomposed into a series of sampling tasks that use constant\namount of memory. Numerical examples in image compression and matrix completion\nare used to demonstrate the effectiveness of R3SVD in low-rank approximation.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.08134v1"
    },
    {
        "title": "Higher-order meshing of implicit geometries - part II: Approximations on\n  manifolds",
        "authors": [
            "T. P. Fries",
            "D. Schöllhammer"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  A new concept for the higher-order accurate approximation of partial\ndifferential equations on manifolds is proposed where a surface mesh composed\nby higher-order elements is automatically generated based on level-set data.\nThereby, it enables a completely automatic workflow from the geometric\ndescription to the numerical analysis without any user-intervention. A master\nlevel-set function defines the shape of the manifold through its\nzero-isosurface which is then restricted to a finite domain by additional\nlevel-set functions. It is ensured that the surface elements are sufficiently\ncontinuous and shape regular which is achieved by manipulating the background\nmesh. The numerical results show that optimal convergence rates are obtained\nwith a moderate increase in the condition number compared to handcrafted\nsurface meshes.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00840v1"
    },
    {
        "title": "Higher-order meshing of implicit geometries - part III: Conformal\n  Decomposition FEM (CDFEM)",
        "authors": [
            "T. P. Fries"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  A higher-order accurate finite element method is proposed which uses\nautomatically generated meshes based on implicit level-set data for the\ndescription of boundaries and interfaces in two and three dimensions. The\nmethod is an alternative for fictitious domain and extended finite element\nmethods. The domain of interest is immersed in a background mesh composed by\nhigher-order elements. The zero-level sets are identified and meshed followed\nby a decomposition of the cut background elements into conforming sub-elements.\nAdaptivity is a crucial ingredient of the method to guarantee the success of\nthe mesh generation. It ensures the successful decomposition of cut elements\nand enables improved geometry descriptions and approximations. It is confirmed\nthat higher-order accurate results with optimal convergence rates are achieved\nwith the proposed conformal decomposition finite element method (CDFEM).\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00919v1"
    },
    {
        "title": "Greedy Approaches to Symmetric Orthogonal Tensor Decomposition",
        "authors": [
            "Cun Mu",
            "Daniel Hsu",
            "Donald Goldfarb"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Finding the symmetric and orthogonal decomposition (SOD) of a tensor is a\nrecurring problem in signal processing, machine learning and statistics. In\nthis paper, we review, establish and compare the perturbation bounds for two\nnatural types of incremental rank-one approximation approaches. Numerical\nexperiments and open questions are also presented and discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01169v1"
    },
    {
        "title": "Numerically Stable Variants of the Communication-hiding Pipelined\n  Conjugate Gradients Algorithm for the Parallel Solution of Large Scale\n  Symmetric Linear Systems",
        "authors": [
            "Siegfried Cools",
            "Wim Vanroose"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  By reducing the number of global synchronization bottlenecks per iteration\nand hiding communication behind useful computational work, pipelined Krylov\nsubspace methods achieve significantly improved parallel scalability on\npresent-day HPC hardware. However, this typically comes at the cost of a\nreduced maximal attainable accuracy. This paper presents and compares several\nstabilized versions of the communication-hiding pipelined Conjugate Gradients\nmethod. The main novel contribution of this work is the reformulation of the\nmulti-term recurrence pipelined CG algorithm by introducing shifts in the\nrecursions for specific auxiliary variables. These shifts reduce the\namplification of local rounding errors on the residual. The stability analysis\npresented in this work provides a rigorous method for selection of the optimal\nshift value in practice. It is shown that, given a proper choice for the shift\nparameter, the resulting shifted pipelined CG algorithm restores the attainable\naccuracy and displays nearly identical robustness to local rounding error\npropagation compared to classical CG. Numerical results on a variety of SPD\nbenchmark problems compare different stabilization techniques for the pipelined\nCG algorithm, showing that the shifted pipelined CG algorithm is able to attain\na high accuracy while displaying excellent parallel performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.05988v2"
    },
    {
        "title": "Machine-learning error models for approximate solutions to parameterized\n  systems of nonlinear equations",
        "authors": [
            "Brian A. Freno",
            "Kevin T. Carlberg"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  This work proposes a machine-learning framework for constructing statistical\nmodels of errors incurred by approximate solutions to parameterized systems of\nnonlinear equations. These approximate solutions may arise from early\ntermination of an iterative method, a lower-fidelity model, or a\nprojection-based reduced-order model, for example. The proposed statistical\nmodel comprises the sum of a deterministic regression-function model and a\nstochastic noise model. The method constructs the regression-function model by\napplying regression techniques from machine learning (e.g., support vector\nregression, artificial neural networks) to map features (i.e., error indicators\nsuch as sampled elements of the residual) to a prediction of the\napproximate-solution error. The method constructs the noise model as a\nmean-zero Gaussian random variable whose variance is computed as the sample\nvariance of the approximate-solution error on a test set; this variance can be\ninterpreted as the epistemic uncertainty introduced by the approximate\nsolution. This work considers a wide range of feature-engineering methods,\ndata-set-construction techniques, and regression techniques that aim to ensure\nthat (1) the features are cheaply computable, (2) the noise model exhibits low\nvariance (i.e., low epistemic uncertainty introduced), and (3) the regression\nmodel generalizes to independent test data. Numerical experiments performed on\nseveral computational-mechanics problems and types of approximate solutions\ndemonstrate the ability of the method to generate statistical models of the\nerror that satisfy these criteria and significantly outperform more commonly\nadopted approaches for error modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.02097v3"
    },
    {
        "title": "Parametric Topology Optimization with Multi-Resolution Finite Element\n  Models",
        "authors": [
            "Vahid Keshavarzzadeh",
            "Robert M. Kirby",
            "Akil Narayan"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We present a methodical procedure for topology optimization under uncertainty\nwith multi-resolution finite element models. We use our framework in a\nbi-fidelity setting where a coarse and a fine mesh corresponding to low- and\nhigh-resolution models are available. The inexpensive low-resolution model is\nused to explore the parameter space and approximate the parameterized\nhigh-resolution model and its sensitivity where parameters are considered in\nboth structural load and stiffness. We provide error bounds for bi-fidelity\nfinite element (FE) approximations and their sensitivities and conduct\nnumerical studies to verify these theoretical estimates. We demonstrate our\napproach on benchmark compliance minimization problems where we show\nsignificant reduction in computational cost for expensive problems such as\ntopology optimization under manufacturing variability while generating almost\nidentical designs to those obtained with single resolution mesh. We also\ncompute the parametric Von-Mises stress for the generated designs via our\nbi-fidelity FE approximation and compare them with standard Monte Carlo\nsimulations. The implementation of our algorithm which extends the well-known\n88-line topology optimization code in MATLAB is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.10367v1"
    },
    {
        "title": "Tensor Networks for Latent Variable Analysis: Higher Order Canonical\n  Polyadic Decomposition",
        "authors": [
            "Anh-Huy Phan",
            "Andrzej Cichocki",
            "Ivan Oseledets",
            "Salman Ahmadi Asl",
            "Giuseppe Calvi",
            "Danilo Mandic"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The Canonical Polyadic decomposition (CPD) is a convenient and intuitive tool\nfor tensor factorization; however, for higher-order tensors, it often exhibits\nhigh computational cost and permutation of tensor entries, these undesirable\neffects grow exponentially with the tensor order. Prior compression of tensor\nin-hand can reduce the computational cost of CPD, but this is only applicable\nwhen the rank $R$ of the decomposition does not exceed the tensor dimensions.\nTo resolve these issues, we present a novel method for CPD of higher-order\ntensors, which rests upon a simple tensor network of representative\ninter-connected core tensors of orders not higher than 3. For rigour, we\ndevelop an exact conversion scheme from the core tensors to the factor matrices\nin CPD, and an iterative algorithm with low complexity to estimate these factor\nmatrices for the inexact case. Comprehensive simulations over a variety of\nscenarios support the approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.00535v1"
    },
    {
        "title": "Determination of Stationary Points and Their Bindings in Dataset using\n  RBF Methods",
        "authors": [
            "Zuzana Majdisova",
            "Vaclav Skala",
            "Michal Smolik"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Stationary points of multivariable function which represents some surface\nhave an important role in many application such as computer vision, chemical\nphysics, etc. Nevertheless, the dataset describing the surface for which a\nsampling function is not known is often given. Therefore, it is necessary to\npropose an approach for finding the stationary points without knowledge of the\nsampling function.\n  In this paper, an algorithm for determining a set of stationary points of\ngiven sampled surface and detecting the bindings between these stationary\npoints (such as stationary points lie on line segment, circle, etc.) is\npresented. Our approach is based on the piecewise RBF interpolation of the\ngiven dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.01847v1"
    },
    {
        "title": "Analyzing and improving maximal attainable accuracy in the communication\n  hiding pipelined BiCGStab method",
        "authors": [
            "Siegfried Cools"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Pipelined Krylov subspace methods avoid communication latency by reducing the\nnumber of global synchronization bottlenecks and by hiding global communication\nbehind useful computational work. In exact arithmetic pipelined Krylov subspace\nalgorithms are equivalent to classic Krylov subspace methods and generate\nidentical series of iterates. However, as a consequence of the reformulation of\nthe algorithm to improve parallelism, pipelined methods may suffer from\nseverely reduced attainable accuracy in a practical finite precision setting.\nThis work presents a numerical stability analysis that describes and quantifies\nthe impact of local rounding error propagation on the maximal attainable\naccuracy of the multi-term recurrences in the preconditioned pipelined BiCGStab\nmethod. Theoretical expressions for the gaps between the true and computed\nresidual as well as other auxiliary variables used in the algorithm are\nderived, and the elementary dependencies between the gaps on the various\nrecursively computed vector variables are analyzed. The norms of the\ncorresponding propagation matrices and vectors provide insights in the possible\namplification of local rounding errors throughout the algorithm. Stability of\nthe pipelined BiCGStab method is compared numerically to that of pipelined CG\non a symmetric benchmark problem. Furthermore, numerical evidence supporting\nthe effectiveness of employing a residual replacement type strategy to improve\nthe maximal attainable accuracy for the pipelined BiCGStab method is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.01948v2"
    },
    {
        "title": "SNS: A Solution-based Nonlinear Subspace method for time-dependent model\n  order reduction",
        "authors": [
            "Youngsoo Choi",
            "Deshawn Coombs",
            "Robert Anderson"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Several reduced order models have been developed for nonlinear dynamical\nsystems. To achieve a considerable speed-up, a hyper-reduction step is needed\nto reduce the computational complexity due to nonlinear terms. Many\nhyper-reduction techniques require the construction of nonlinear term basis,\nwhich introduces a computationally expensive offline phase. A novel way of\nconstructing nonlinear term basis within the hyper-reduction process is\nintroduced. In contrast to the traditional hyper-reduction techniques where the\ncollection of nonlinear term snapshots is required, the SNS method avoids\ncollecting the nonlinear term snapshots. Instead, it uses the solution\nsnapshots that are used for building a solution basis, which enables avoiding\nan extra data compression of nonlinear term snapshots. As a result, the SNS\nmethod provides a more efficient offline strategy than the traditional model\norder reduction techniques, such as the DEIM, GNAT, and ST-GNAT methods. The\nSNS method is theoretically justified by the conforming subspace condition and\nthe subspace inclusion relation. It is useful for model order reduction of\nlarge-scale nonlinear dynamical problems to reduce the offline cost. It is\nespecially useful for ST-GNAT that has shown promising results, such as a good\naccuracy with a considerable online speed-up for hyperbolic problems in a\nrecent paper by Choi and Carlberg in SISC 2019, because ST-GNAT involves an\nexpensive offline cost related to collecting nonlinear term snapshots.\nNumerical results support that the accuracy of the solution from the SNS method\nis comparable to the traditional methods and a considerable speed-up (i.e., a\nfactor of two to a hundred) is achieved in the offline phase.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.04064v4"
    },
    {
        "title": "Low synchronization GMRES algorithms",
        "authors": [
            "Kasia Swirydowicz",
            "Julien Langou",
            "Shreyas Ananthan",
            "Ulrike Yang",
            "Stephen Thomas"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Communication-avoiding and pipelined variants of Krylov solvers are critical\nfor the scalability of linear system solvers on future exascale architectures.\nWe present low synchronization variants of iterated classical (CGS) and\nmodified Gram-Schmidt (MGS) algorithms that require one and two global\nreduction communication steps. Derivations of low synchronization iterated CGS\nalgorithms are based on previous work by Ruhe. Our main contribution is to\nintroduce a backward normalization lag into the compact $WY$ form of MGS\nresulting in a ${\\cal O}(\\eps)\\kappa(A)$ stable GMRES algorithm that requires\nonly one global synchronization per iteration. The reduction operations are\noverlapped with computations and pipelined to optimize performance. Further\nimprovements in performance are achieved by accelerating GMRES BLAS-2\noperations on GPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.05805v1"
    },
    {
        "title": "Tight Continuous-Time Reachtubes for Lagrangian Reachability",
        "authors": [
            "Jacek Cyranka",
            "Md. Ariful Islam",
            "Scott A. Smolka",
            "Sicun Gao",
            "Radu Grosu"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We introduce continuous Lagrangian reachability (CLRT), a new algorithm for\nthe computation of a tight and continuous-time reachtube for the solution flows\nof a nonlinear, time-variant dynamical system. CLRT employs finite strain\ntheory to determine the deformation of the solution set from time $t_i$ to time\n$t_{i+1}$. We have developed simple explicit analytic formulas for the optimal\nmetric for this deformation; this is superior to prior work, which used\nsemi-definite programming. CLRT also uses infinitesimal strain theory to derive\nan optimal time increment $h_i$ between $t_i$ and $t_{i+1}$, nonlinear\noptimization to minimally bloat (i.e., using a minimal radius) the state set at\ntime $t_i$ such that it includes all the states of the solution flow in the\ninterval $[t_i,t_{i+1}]$. We use $\\delta$-satisfiability to ensure the\ncorrectness of the bloating. Our results on a series of benchmarks show that\nCLRT performs favorably compared to state-of-the-art tools such as CAPD in\nterms of the continuous reachtube volumes they compute.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.07450v2"
    },
    {
        "title": "On constructing orthogonal generalized doubly stochastic matrices",
        "authors": [
            "Gianluca Oderda",
            "Alicja Smoktunowicz",
            "Ryszard Kozera"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  A real quadratic matrix is generalized doubly stochastic (g.d.s.) if all of\nits row sums and column sums equal one. We propose numerically stable methods\nfor generating such matrices having possibly orthogonality property or/and\nsatisfying Yang-Baxter equation (YBE). Additionally, an inverse eigenvalue\nproblem for finding orthogonal generalized doubly stochastic matrices with\nprescribed eigenvalues is solved here. The tests performed in \\textsl{MATLAB}\nillustrate our proposed algorithms and demonstrate their useful numerical\nproperties.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.07618v1"
    },
    {
        "title": "The trouble with tensor ring decompositions",
        "authors": [
            "Kim Batselier"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The tensor train decomposition decomposes a tensor into a \"train\" of 3-way\ntensors that are interconnected through the summation of auxiliary indices. The\ndecomposition is stable, has a well-defined notion of rank and enables the user\nto perform various linear algebra operations on vectors and matrices of\nexponential size in a computationally efficient manner. The tensor ring\ndecomposition replaces the train by a ring through the introduction of one\nadditional auxiliary variable. This article discusses a major issue with the\ntensor ring decomposition: its inability to compute an exact minimal-rank\ndecomposition from a decomposition with sub-optimal ranks. Both the contraction\noperation and Hadamard product are motivated from applications and it is shown\nthrough simple examples how the tensor ring-rounding procedure fails to\nretrieve minimal-rank decompositions with these operations. These observations,\ntogether with the already known issue of not being able to find a best low-rank\ntensor ring approximation to a given tensor indicate that the applicability of\ntensor rings is severely limited.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03813v1"
    },
    {
        "title": "Reciprocal and Positive Real Balanced Truncations for Model Order\n  Reduction of Descriptor Systems",
        "authors": [
            "Yuichi Tanji"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Model order reduction algorithms for large-scale descriptor systems are\nproposed using balanced truncation, in which symmetry or block skew symmetry\n(reciprocity) and the positive realness of the original transfer matrix are\npreserved. Two approaches based on standard and generalized algebraic Riccati\nequations are proposed. To accelerate the algorithms, a fast Riccati solver,\nRADI (alternating directions implicit [ADI]-type iteration for Riccati\nequations), is also introduced. As a result, the proposed methods are general\nand efficient as a model order reduction algorithm for descriptor systems\nassociated with electrical circuit networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04630v1"
    },
    {
        "title": "Several new domain-type and boundary-type numerical discretization\n  schemes with radial basis function",
        "authors": [
            "W. Chen"
        ],
        "category": "cs.NA",
        "published_year": "2001",
        "summary": "  This paper is concerned with a few novel RBF-based numerical schemes\ndiscretizing partial differential equations. For boundary-type methods, we\nderive the indirect and direct symmetric boundary knot methods (BKM). The\nresulting interpolation matrix of both is always symmetric irrespective of\nboundary geometry and conditions. In particular, the direct BKM applies the\npractical physical variables rather than expansion coefficients and becomes\nvery competitive to the boundary element method. On the other hand, based on\nthe multiple reciprocity principle, we invent the RBF-based boundary particle\nmethod (BPM) for general inhomogeneous problems without a need using inner\nnodes. The direct and symmetric BPM schemes are also developed.\n  For domain-type RBF discretization schemes, by using the Green integral we\ndevelop a new Hermite RBF scheme called as the modified Kansa method (MKM),\nwhich differs from the symmetric Hermite RBF scheme in that the MKM discretizes\nboth governing equation and boundary conditions on the same boundary nodes. The\nlocal spline version of the MKM is named as the finite knot method (FKM). Both\nMKM and FKM significantly reduce calculation errors at nodes adjacent to\nboundary. In addition, the nonsingular high-order fundamental or general\nsolution is strongly recommended as the RBF in the domain-type methods and dual\nreciprocity method approximation of particular solution relating to the BKM.\n  It is stressed that all the above discretization methods of boundary-type and\ndomain-type are symmetric, meshless, and integration-free. The spline-based\nschemes will produce desirable symmetric sparse banded interpolation matrix. In\nappendix, we present a Hermite scheme to eliminate edge effect on the RBF\ngeometric modeling and imaging.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0104018v1"
    },
    {
        "title": "Errata and supplements to: Orthonormal RBF Wavelet and Ridgelet-like\n  Series and Transforms for High-Dimensional Problems",
        "authors": [
            "W. Chen"
        ],
        "category": "cs.NA",
        "published_year": "2001",
        "summary": "  In recent years some attempts have been done to relate the RBF with wavelets\nin handling high dimensional multiscale problems. To the author's knowledge,\nhowever, the orthonormal and bi-orthogonal RBF wavelets are still missing in\nthe literature. By using the nonsingular general solution and singular\nfundamental solution of differential operator, recently the present author,\nrefer. 3, made some substantial headway to derive the orthonormal RBF wavelets\nseries and transforms. The methodology can be generalized to create the RBF\nwavelets by means of the orthogonal convolution kernel function of various\nintegral operators. In particular, it is stressed that the presented RBF\nwavelets does not apply the tensor product to handle multivariate problems at\nall.\n  This note is to correct some errata in reference 3 and also to supply a few\nlatest advances in the study of orthornormal RBF wavelet transforms.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0105014v1"
    },
    {
        "title": "Analytical solution of transient scalar wave and diffusion problems of\n  arbitrary dimensionality and geometry by RBF wavelet series",
        "authors": [
            "W. Chen"
        ],
        "category": "cs.NA",
        "published_year": "2001",
        "summary": "  This study applies the RBF wavelet series to the evaluation of analytical\nsolutions of linear time-dependent wave and diffusion problems of any\ndimensionality and geometry. To the best of the author's knowledge, such\nanalytical solutions have never been achieved before. The RBF wavelets can be\nunderstood an alternative for multidimensional problems to the standard Fourier\nseries via fundamental and general solutions of partial differential equation.\nThe present RBF wavelets are infinitely differential, compactly supported,\northogonal over different scales and very simple. The rigorous mathematical\nproof of completeness and convergence is still missing in this study. The\npresent work may open a new window to numerical solution and theoretical\nanalysis of many other high-dimensional time-dependent PDE problems under\narbitrary geometry.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0110055v1"
    },
    {
        "title": "New RBF collocation methods and kernel RBF with applications",
        "authors": [
            "W. Chen"
        ],
        "category": "cs.NA",
        "published_year": "2001",
        "summary": "  A few novel radial basis function (RBF) discretization schemes for partial\ndifferential equations are developed in this study. For boundary-type methods,\nwe derive the indirect and direct symmetric boundary knot methods. Based on the\nmultiple reciprocity principle, the boundary particle method is introduced for\ngeneral inhomogeneous problems without using inner nodes. For domain-type\nschemes, by using the Green integral we develop a novel Hermite RBF scheme\ncalled the modified Kansa method, which significantly reduces calculation\nerrors at close-to-boundary nodes. To avoid Gibbs phenomenon, we present the\nleast square RBF collocation scheme. Finally, five types of the kernel RBF are\nalso briefly presented.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0111063v1"
    },
    {
        "title": "Some addenda on distance function wavelets",
        "authors": [
            "W. Chen"
        ],
        "category": "cs.NA",
        "published_year": "2002",
        "summary": "  This report will add some supplements to the recently finished report series\non the distance function wavelets (DFW). First, we define the general distance\nin terms of the Riesz potential, and then, the distance function Abel wavelets\nare derived via the fractional integral and Laplacian. Second, the DFW Weyl\ntransform is found to be a shifted Laplace potential DFW. The DFW Radon\ntransform is also presented. Third, we present a conjecture on truncation error\nformula of the multiple reciprocity Laplace DFW series and discuss its error\ndistributions in terms of node density distributions. Forth, we point out that\nthe Hermite distance function interpolation can be used to replace overlapping\nin the domain decomposition in order to produce sparse matrix. Fifth, the shape\nparameter is explained as a virtual extra axis contribution in terms of the\nMQ-type Possion kernel. The report is concluded with some remarks on a range of\nother issues.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0207062v1"
    },
    {
        "title": "A new definition of the fractional Laplacian",
        "authors": [
            "W. Chen"
        ],
        "category": "cs.NA",
        "published_year": "2002",
        "summary": "  It is noted that the standard definition of the fractional Laplacian leads to\na hyper-singular convolution integral and is also obscure about how to\nimplement the boundary conditions. This purpose of this note is to introduce a\nnew definition of the fractional Laplacian to overcome these major drawbacks.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0209020v1"
    },
    {
        "title": "Excellence in Computer Simulation",
        "authors": [
            "Leo P. Kadanoff"
        ],
        "category": "cs.NA",
        "published_year": "2003",
        "summary": "  Excellent computer simulations are done for a purpose. The most valid\npurposes are to explore uncharted territory, to resolve a well-posed scientific\nor technical question, or to make a design choice. Stand-alone modeling can\nserve the first purpose. The other two goals need a full integration of the\nmodeling effort into a scientific or engineering program.\n  Some excellent work, much of it related to the Department of Energy\nLaboratories, is reviewed. Some less happy stories are recounted.\n  In the past, some of the most impressive work has involved complexity and\nchaos. Prediction in a complex world requires a first principles understanding\nbased upon the intersection of theory, experiment and simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0307033v1"
    },
    {
        "title": "A Bernstein-Bezier Sufficient Condition for Invertibility of Polynomial\n  Mapping Functions",
        "authors": [
            "Stephen Vavasis"
        ],
        "category": "cs.NA",
        "published_year": "2003",
        "summary": "  We propose a sufficient condition for invertibility of a polynomial mapping\nfunction defined on a cube or simplex. This condition is applicable to finite\nelement analysis using curved meshes. The sufficient condition is based on an\nanalysis of the Bernstein-B\\'ezier form of the columns of the derivative.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0308021v1"
    },
    {
        "title": "Computational Aspects of a Numerical Model for Combustion Flow",
        "authors": [
            "Gianluca Argentini"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  A computational method for numeric resolution of a PDEs system, based on a\nFinite Differences schema integrated by interpolations of partial results, and\nan estimate of the error of its solution respect to the normal FD solution.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0411004v1"
    },
    {
        "title": "A Condition Number Analysis of a Line-Surface Intersection Algorithm",
        "authors": [
            "Gun Srijuntongsiri",
            "Stephen A. Vavasis"
        ],
        "category": "cs.NA",
        "published_year": "2006",
        "summary": "  We propose an algorithm based on Newton's method and subdivision for finding\nall zeros of a polynomial system in a bounded region of the plane. This\nalgorithm can be used to find the intersections between a line and a surface,\nwhich has applications in graphics and computer-aided geometric design. The\nalgorithm can operate on polynomials represented in any basis that satisfies a\nfew conditions. The power basis, the Bernstein basis, and the first-kind\nChebyshev basis are among those compatible with the algorithm. The main novelty\nof our algorithm is an analysis showing that its running is bounded only in\nterms of the condition number of the polynomial's zeros and a constant\ndepending on the polynomial basis.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0608090v2"
    },
    {
        "title": "A Robust Solution Procedure for Hyperelastic Solids with Large Boundary\n  Deformation",
        "authors": [
            "Suzanne M. Shontz",
            "Stephen A. Vavasis"
        ],
        "category": "cs.NA",
        "published_year": "2006",
        "summary": "  Compressible Mooney-Rivlin theory has been used to model hyperelastic solids,\nsuch as rubber and porous polymers, and more recently for the modeling of soft\ntissues for biomedical tissues, undergoing large elastic deformations. We\npropose a solution procedure for Lagrangian finite element discretization of a\nstatic nonlinear compressible Mooney-Rivlin hyperelastic solid. We consider the\ncase in which the boundary condition is a large prescribed deformation, so that\nmesh tangling becomes an obstacle for straightforward algorithms. Our solution\nprocedure involves a largely geometric procedure to untangle the mesh: solution\nof a sequence of linear systems to obtain initial guesses for interior nodal\npositions for which no element is inverted. After the mesh is untangled, we\ntake Newton iterations to converge to a mechanical equilibrium. The Newton\niterations are safeguarded by a line search similar to one used in\noptimization. Our computational results indicate that the algorithm is up to 70\ntimes faster than a straightforward Newton continuation procedure and is also\nmore robust (i.e., able to tolerate much larger deformations). For a few\nextremely large deformations, the deformed mesh could only be computed through\nthe use of an expensive Newton continuation method while using a tight\nconvergence tolerance and taking very small steps.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0609001v2"
    },
    {
        "title": "Modern Statistics by Kriging",
        "authors": [
            "Tomasz Suslo"
        ],
        "category": "cs.NA",
        "published_year": "2006",
        "summary": "  We present statistics (S-statistics) based only on random variable (not\nrandom value) with a mean squared error of mean estimation as a concept of\nerror.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0609079v4"
    },
    {
        "title": "Doppler Spectrum Estimation by Ramanujan Fourier Transforms",
        "authors": [
            "Mohand Lagha",
            "Messaoud Bensebti"
        ],
        "category": "cs.NA",
        "published_year": "2006",
        "summary": "  The Doppler spectrum estimation of a weather radar signal in a classic way\ncan be made by two methods, temporal one based in the autocorrelation of the\nsuccessful signals, whereas the other one uses the estimation of the power\nspectral density PSD by using Fourier transforms. We introduces a new tool of\nsignal processing based on Ramanujan sums cq(n), adapted to the analysis of\narithmetical sequences with several resonances p/q. These sums are almost\nperiodic according to time n of resonances and aperiodic according to the order\nq of resonances. New results will be supplied by the use of Ramanujan Fourier\nTransform (RFT) for the estimation of the Doppler spectrum for the weather\nradar signal.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0610108v2"
    },
    {
        "title": "On generic frequency decomposition. Part 1: vectorial decomposition",
        "authors": [
            "Sossio Vergara"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  The famous Fourier theorem states that, under some restrictions, any periodic\nfunction (or real world signal) can be obtained as a sum of sinusoids, and\nhence, a technique exists for decomposing a signal into its sinusoidal\ncomponents. From this theory an entire branch of research has flourished: from\nthe Short-Time or Windowed Fourier Transform to the Wavelets, the Frames, and\nlately the Generic Frequency Analysis. The aim of this paper is to take the\nFrequency Analysis a step further. It will be shown that keeping the same\nreconstruction algorithm as the Fourier Theorem but changing to a new computing\nmethod for the analysis phase allows the generalization of the Fourier Theorem\nto a large class of nonorthogonal bases. New methods and algorithms can be\nemployed in function decomposition on such generic bases. It will be shown that\nthese algorithms are a generalization of the Fourier analysis, i.e. they are\nreduced to the familiar Fourier tools when using orthogonal bases. The\ndifferences between this tool and the wavelets and frames theories will be\ndiscussed. Examples of analysis and reconstruction of functions using the given\nalgorithms and nonorthogonal bases will be given. In this first part the focus\nwill be on vectorial decomposition, while the second part will be on phased\ndecomposition. The phased decomposition thanks to a single function basis has\nmany interesting consequences and applications.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.3650v1"
    },
    {
        "title": "On dual Schur domain decomposition method for linear first-order\n  transient problems",
        "authors": [
            "K. B. Nakshatrala",
            "A. Prakash",
            "K. D. Hjelmstad"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  This paper addresses some numerical and theoretical aspects of dual Schur\ndomain decomposition methods for linear first-order transient partial\ndifferential equations. In this work, we consider the trapezoidal family of\nschemes for integrating the ordinary differential equations (ODEs) for each\nsubdomain and present four different coupling methods, corresponding to\ndifferent algebraic constraints, for enforcing kinematic continuity on the\ninterface between the subdomains.\n  Method 1 (d-continuity) is based on the conventional approach using\ncontinuity of the primary variable and we show that this method is unstable for\na lot of commonly used time integrators including the mid-point rule. To\nalleviate this difficulty, we propose a new Method 2 (Modified d-continuity)\nand prove its stability for coupling all time integrators in the trapezoidal\nfamily (except the forward Euler). Method 3 (v-continuity) is based on\nenforcing the continuity of the time derivative of the primary variable.\nHowever, this constraint introduces a drift in the primary variable on the\ninterface. We present Method 4 (Baumgarte stabilized) which uses Baumgarte\nstabilization to limit this drift and we derive bounds for the stabilization\nparameter to ensure stability.\n  Our stability analysis is based on the ``energy'' method, and one of the main\ncontributions of this paper is the extension of the energy method (which was\npreviously introduced in the context of numerical methods for ODEs) to assess\nthe stability of numerical formulations for index-2 differential-algebraic\nequations (DAEs).\n",
        "pdf_link": "http://arxiv.org/pdf/0807.2108v2"
    },
    {
        "title": "Interval Semantics for Standard Floating-Point Arithmetic",
        "authors": [
            "W. W. Edmonson",
            "M. H. van Emden"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  If the non-zero finite floating-point numbers are interpreted as point\nintervals, then the effect of rounding can be interpreted as computing one of\nthe bounds of the result according to interval arithmetic. We give an interval\ninterpretation for the signed zeros and infinities, so that the undefined\noperations 0*inf, inf - inf, inf/inf, and 0/0 become defined.\n  In this way no operation remains that gives rise to an error condition.\nMathematically questionable features of the floating-point standard become\nwell-defined sets of reals. Interval semantics provides a basis for the\nverification of numerical algorithms. We derive the results of the newly\ndefined operations and consider the implications for hardware implementation.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.4196v1"
    },
    {
        "title": "Self-stabilizing Numerical Iterative Computation",
        "authors": [
            "Danny Bickson",
            "Ezra N. Hoch",
            "Harel Avissar",
            "Danny Dolev"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  Many challenging tasks in sensor networks, including sensor calibration,\nranking of nodes, monitoring, event region detection, collaborative filtering,\ncollaborative signal processing, {\\em etc.}, can be formulated as a problem of\nsolving a linear system of equations. Several recent works propose different\ndistributed algorithms for solving these problems, usually by using linear\niterative numerical methods.\n  The main problem with previous approaches is that once the problem inputs\nchange during the process of computation, the computation may output unexpected\nresults. In real life settings, sensor measurements are subject to varying\nenvironmental conditions and to measurement noise.\n  We present a simple iterative scheme called SS-Iterative for solving systems\nof linear equations, and examine its properties in the self-stabilizing\nperspective. We analyze the behavior of the proposed scheme under changing\ninput sequences using two different assumptions on the input: a box bound, and\na probabilistic distribution.\n  As a case study, we discuss the sensor calibration problem and provide\nsimulation results to support the applicability of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.2682v1"
    },
    {
        "title": "An Improved Algorithm based on Shannon-Happ Formula for Calculating\n  Transfer Function from Signal Flow Graph and Its Visualization",
        "authors": [
            "Hongyu Lu",
            "Chongguang Wu",
            "Shanglian Bao"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  A new method based on Shannon-Happ formula to calculate transfer function\nfrom Signal Flow Graph (SFG) is presented. The algorithm provides an explicit\napproach to get the transfer function in a format with both numerical and\nsymbolic expressions. The adoption of the symbolic variable in SFG, which could\nrepresent the nonlinear item or the independent sub-system, is achieved by\nvariable separation approach. An investigation is given for the solutions of\nseveral special conditions of SFG. To improve the efficiency of the algorithm,\na new technique combined with Johnson method for generating the combinations of\nthe non-touching loops is developed. It uses the previous combinations in lower\norder to get the ones in higher order. There is an introduction about the\nvisualization of SFG and the subroutines for system performance analysis in the\nsoftware, AVANT.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.5561v2"
    },
    {
        "title": "A Geometric Approach to Solve Fuzzy Linear Systems",
        "authors": [
            "N. Gasilov",
            "Şahin Emrah Amrahov",
            "A. Golayoglu Fatullayev",
            "H. I. Karakas",
            "O. Akin"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  In this paper, linear systems with a crisp real coefficient matrix and with a\nvector of fuzzy triangular numbers on the right-hand side are studied. A new\nmethod, which is based on the geometric representations of linear\ntransformations, is proposed to find solutions. The method uses the fact that a\nvector of fuzzy triangular numbers forms a rectangular prism in n-dimensional\nspace and that the image of a parallelepiped is also a parallelepiped under a\nlinear transformation. The suggested method clarifies why in general case\ndifferent approaches do not generate solutions as fuzzy numbers. It is\ngeometrically proved that if the coefficient matrix is a generalized\npermutation matrix, then the solution of a fuzzy linear system (FLS) is a\nvector of fuzzy numbers irrespective of the vector on the right-hand side. The\nmost important difference between this and previous papers on FLS is that the\nsolution is sought as a fuzzy set of vectors (with real components) rather than\na vector of fuzzy numbers. Each vector in the solution set solves the given FLS\nwith a certain possibility. The suggested method can also be applied in the\ncase when the right-hand side is a vector of fuzzy numbers in parametric form.\nHowever, in this case, -cuts of the solution can not be determined by geometric\nsimilarity and additional computations are needed.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.4049v2"
    },
    {
        "title": "On the vibrations of lumped parameter systems governed by\n  differential-algebraic equations",
        "authors": [
            "S. Darbha",
            "K. B. Nakshatrala",
            "K. R. Rajagopal"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  In this paper, we consider the vibratory motions of lumped parameter systems\nwherein the components of the system cannot be described by constitutive\nexpressions for the force in terms of appropriate kinematical quantities. Such\nphysical systems reduce to a system of differential-algebraic equations, which\ninvariably need to be solved numerically. To illustrate the issues with\nclarity, we consider a simple system in which the dashpot is assumed to contain\na \"Bingham\" fluid for which one cannot describe the force in the dashpot as a\nfunction of the velocity. On the other hand, one can express the velocity as a\nfunction of the force.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.0137v5"
    },
    {
        "title": "Algebraic Change-Point Detection",
        "authors": [
            "Michel Fliess",
            "Cédric Join",
            "Mamadou Mboup"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  Elementary techniques from operational calculus, differential algebra, and\nnoncommutative algebra lead to a new approach for change-point detection, which\nis an important field of investigation in various areas of applied sciences and\nengineering. Several successful numerical experiments are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.1178v1"
    },
    {
        "title": "On the precision attainable with various floating-point number systems",
        "authors": [
            "Richard P. Brent"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  For scientific computations on a digital computer the set of real number is\nusually approximated by a finite set F of \"floating-point\" numbers. We compare\nthe numerical accuracy possible with difference choices of F having\napproximately the same range and requiring the same word length. In particular,\nwe compare different choices of base (or radix) in the usual floating-point\nsystems. The emphasis is on the choice of F, not on the details of the number\nrepresentation or the arithmetic, but both rounded and truncated arithmetic are\nconsidered. Theoretical results are given, and some simulations of typical\nfloating-point computations (forming sums, solving systems of linear equations,\nfinding eigenvalues) are described. If the leading fraction bit of a normalized\nbase 2 number is not stored explicitly (saving a bit), and the criterion is to\nminimize the mean square roundoff error, then base 2 is best. If unnormalized\nnumbers are allowed, so the first bit must be stored explicitly, then base 4\n(or sometimes base 8) is the best of the usual systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.3374v1"
    },
    {
        "title": "Using vector divisions in solving linear complementarity problem",
        "authors": [
            "Youssef Elfoutayeni",
            "Mohamed Khaladi"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  The linear complementarity problem is to find vector $z$ in $\\mathrm{IR}^{n}$\nsatisfying $z^{T}(Mz+q)=0$, $Mz+q\\geqslant0,$ $z\\geqslant0$, where $M$ as a\nmatrix and $q$ as a vector, are given data; this problem becomes in present the\nsubject of much important research because it arises in many areas and it\nincludes important fields, we cite for example the linear and nonlinear\nprogramming, the convex quadratic programming and the variational inequalities\nproblems, ... It is known that the linear complementarity problem is completely\nequivalent to solving nonlinear equation $F(x)=0$ with $F$ is a function from\n$\\mathrm{IR}^{n}$ into itself defined by $F(x)=(M+I)x+(M-I)|x|+q$. In this\npaper we propose a globally convergent hybrid algorithm for solving this\nequation; this method is based on an algorithm given by Shi \\cite{Y. Shi}, he\nuses vector divisions with the secant method; but for using this method we must\nhave a function continuous with partial derivatives on an open set of\n$\\mathrm{IR}^{n}$; so we built a sequence of functions $\\tilde{F}(p,x)\\in\nC^{\\infty}$ which converges uniformly to the function $F(x)$; and we show that\nfinding the zero of the function $F$ is completely equivalent to finding the\nzero of the sequence of the functions $\\tilde{F}(p,x)$. We close our paper with\nsome numerical simulation examples to illustrate our theoretical results.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.1417v1"
    },
    {
        "title": "On a new class of additive (splitting) operator-difference schemes",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Many applied time-dependent problems are characterized by an additive\nrepresentation of the problem operator. Additive schemes are constructed using\nsuch a splitting and associated with the transition to a new time level on the\nbasis of the solution of more simple problems for the individual operators in\nthe additive decomposition. We consider a new class of additive schemes for\nproblems with additive representation of the operator at the time derivative.\nIn this paper we construct and study the vector operator-difference schemes,\nwhich are characterized by a transition from one initial the evolution equation\nto a system of such equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.2086v1"
    },
    {
        "title": "Increasing the Reliability of Adaptive Quadrature Using Explicit\n  Interpolants",
        "authors": [
            "Pedro Gonnet"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  We present two new adaptive quadrature routines. Both routines differ from\npreviously published algorithms in many aspects, most significantly in how they\nrepresent the integrand, how they treat non-numerical values of the integrand,\nhow they deal with improper divergent integrals and how they estimate the\nintegration error. The main focus of these improvements is to increase the\nreliability of the algorithms without significantly impacting their efficiency.\nBoth algorithms are implemented in Matlab and tested using both the \"families\"\nsuggested by Lyness and Kaganove and the battery test used by Gander and\nGautschi and Kahaner. They are shown to be more reliable, albeit in some cases\nless efficient, than other commonly-used adaptive integrators.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.3962v1"
    },
    {
        "title": "Novel Modifications of Parallel Jacobi Algorithms",
        "authors": [
            "Sanja Singer",
            "Sasa Singer",
            "Vedran Novakovic",
            "Aleksandar Uscumlic",
            "Vedran Dunjko"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  We describe two main classes of one-sided trigonometric and hyperbolic\nJacobi-type algorithms for computing eigenvalues and eigenvectors of Hermitian\nmatrices. These types of algorithms exhibit significant advantages over many\nother eigenvalue algorithms. If the matrices permit, both types of algorithms\ncompute the eigenvalues and eigenvectors with high relative accuracy.\n  We present novel parallelization techniques for both trigonometric and\nhyperbolic classes of algorithms, as well as some new ideas on how pivoting in\neach cycle of the algorithm can improve the speed of the parallel one-sided\nalgorithms. These parallelization approaches are applicable to both\ndistributed-memory and shared-memory machines.\n  The numerical testing performed indicates that the hyperbolic algorithms may\nbe superior to the trigonometric ones, although, in theory, the latter seem\nmore natural.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.0201v2"
    },
    {
        "title": "A GPU-based hyperbolic SVD algorithm",
        "authors": [
            "Vedran Novakovic",
            "Sanja Singer"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  A one-sided Jacobi hyperbolic singular value decomposition (HSVD) algorithm,\nusing a massively parallel graphics processing unit (GPU), is developed. The\nalgorithm also serves as the final stage of solving a symmetric indefinite\neigenvalue problem. Numerical testing demonstrates the gains in speed and\naccuracy over sequential and MPI-parallelized variants of similar Jacobi-type\nHSVD algorithms. Finally, possibilities of hybrid CPU--GPU parallelism are\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1371v2"
    },
    {
        "title": "A stabilized mixed formulation for unsteady Brinkman equation based on\n  the method of horizontal lines",
        "authors": [
            "S. Srinivasan",
            "K. B. Nakshatrala"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  In this paper, we present a stabilized mixed formulation for unsteady\nBrinkman equation. The formulation is systematically derived based on the\nvariational multiscale formalism and the method of horizontal lines. The\nderivation does not need the assumption that the fine-scale variables do not\ndepend on the time, which is the case with the conventional derivation of\nmultiscale stabilized formulations for transient mixed problems. An expression\nfor the stabilization parameter is obtained in terms of a bubble function, and\nappropriate bubble functions for various finite elements are also presented.\nUnder the proposed formulation, equal-order interpolation for the velocity and\npressure (which is computationally the most convenient) is stable.\nRepresentative numerical results are presented to illustrate the performance of\nthe proposed formulation. Spatial and temporal convergence studies are also\nperformed, and the proposed formulation performed well.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.3845v2"
    },
    {
        "title": "Three-Level Parallel J-Jacobi Algorithms for Hermitian Matrices",
        "authors": [
            "Sanja Singer",
            "Sasa Singer",
            "Vedran Novakovic",
            "Davor Davidovic",
            "Kresimir Bokulic",
            "Aleksandar Uscumlic"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  The paper describes several efficient parallel implementations of the\none-sided hyperbolic Jacobi-type algorithm for computing eigenvalues and\neigenvectors of Hermitian matrices. By appropriate blocking of the algorithms\nan almost ideal load balancing between all available processors/cores is\nobtained. A similar blocking technique can be used to exploit local cache\nmemory of each processor to further speed up the process. Due to diversity of\nmodern computer architectures, each of the algorithms described here may be the\nmethod of choice for a particular hardware and a given matrix size. All\nproposed block algorithms compute the eigenvalues with relative accuracy\nsimilar to the original non-blocked Jacobi algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.4166v1"
    },
    {
        "title": "On Euclidean Norm Approximations",
        "authors": [
            "M. Emre Celebi",
            "Fatih Celiker",
            "Hassan A. Kingravi"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Euclidean norm calculations arise frequently in scientific and engineering\napplications. Several approximations for this norm with differing complexity\nand accuracy have been proposed in the literature. Earlier approaches were\nbased on minimizing the maximum error. Recently, Seol and Cheun proposed an\napproximation based on minimizing the average error. In this paper, we first\nexamine these approximations in detail, show that they fit into a single\nmathematical formulation, and compare their average and maximum errors. We then\nshow that the maximum errors given by Seol and Cheun are significantly\noptimistic.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.4870v1"
    },
    {
        "title": "Uncertainty Updating in the Description of Coupled Heat and Moisture\n  Transport in Heterogeneous Materials",
        "authors": [
            "Anna Kucerova",
            "Jan Sykora"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  To assess the durability of structures, heat and moisture transport need to\nbe analyzed. To provide a reliable estimation of heat and moisture distribution\nin a certain structure, one needs to include all available information about\nthe loading conditions and material parameters. Moreover, the information\nshould be accompanied by a corresponding evaluation of its credibility. Here,\nthe Bayesian inference is applied to combine different sources of information,\nso as to provide a more accurate estimation of heat and moisture fields [1].\nThe procedure is demonstrated on the probabilistic description of heterogeneous\nmaterial where the uncertainties consist of a particular value of individual\nmaterial characteristic and spatial fluctuations. As for the heat and moisture\ntransfer, it is modelled in coupled setting [2].\n",
        "pdf_link": "http://arxiv.org/pdf/1102.5239v1"
    },
    {
        "title": "A mixed formulation for a modification to Darcy equation based on Picard\n  linearization and numerical solutions to large-scale realistic problems",
        "authors": [
            "K. B. Nakshatrala",
            "D. Z. Turner"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  In this paper we consider a modification to Darcy equation by taking into\naccount the dependence of viscosity on the pressure. We present a stabilized\nmixed formulation for the resulting governing equations. Equal-order\ninterpolation for the velocity and pressure is considered, and shown to be\nstable (which is not the case under the classical mixed formulation). The\nproposed mixed formulation is tested using a wide variety of numerical\nexamples. The proposed formulation is also implemented in a parallel setting,\nand the performance of the formulation for large-scale problems is illustrated\nusing a representative problem. Two practical and technologically important\nproblems, one each on enhanced oil recovery and geological carbon-dioxide\nsequestration, are solved using the proposed formulation. The numerical\nexamples show that the predictions based on Darcy model are qualitatively and\nquantitatively different from that of the predictions based on the modified\nDarcy model, which takes into account the dependence of the viscosity on the\npressure. In particular, the numerical example on the geological carbon-dioxide\nsequestration shows that Darcy model over-predicts the leakage into an\nabandoned well when compared to that of the modified Darcy model. On the other\nhand, the modified Darcy model predicts higher pressures and higher pressure\ngradients near the injection well. These predictions have dire consequences in\npredicting damage and fracture zones, and designing the seal, whose integrity\nis crucial to the safety of a geological carbon-dioxide sequestration\ngeosystem.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.0706v3"
    },
    {
        "title": "A time-parallel algorithm for almost integrable Hamiltonian systems",
        "authors": [
            "Hugo Jiménez-Pérez",
            "Jacques Laskar"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  We introduce a time-parallel algorithm for solving numerically almost\nintegrable Hamiltonian systems in action-angle coordinates. This algorithm is a\nrefinement of that introduced by Saha, Stadel and Tremaine in 1997 (SST97) for\nthe same type of problems. Our refined algorithm has a better convergence\nobtained from the use of derivatives of the perturbing term not considered in\nthe original SST97 algorithm. An advantage of this algorithm is its\nindependence of the step-size for the parallelized procedures which can be\nconsider as a particular case of the parareal scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.3694v1"
    },
    {
        "title": "A new algebraic and arithmetic framework for interval computations",
        "authors": [
            "Nicolas Goze",
            "Michel Goze",
            "Abdel Kenoufi",
            "Elisabeth Remm"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  In this paper we propose some very promissing results in interval arithmetics\nwhich permit to build well-defined arithmetics including distributivity of\nmultiplication and division according addition and substraction. Thus, it\nallows to build all algebraic operations and functions on intervals. This will\navoid completely the wrapping effects and data dependance. Some simple\napplications for matrix eigenvalues calculations, inversion of symmetric\nmatrices and finally optimization are exhibited in the object-oriented\nprogramming language python.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.3667v1"
    },
    {
        "title": "On the approximation in the smoothed finite element method (SFEM)",
        "authors": [
            "Stephane PA Bordas",
            "Sundararajan Natarajan"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  This letter aims at resolving the issues raised in the recent short\ncommunication [1] and answered by [2] by proposing a systematic approximation\nscheme based on non-mapped shape functions, which both allows to fully exploit\nthe unique advantages of the smoothed finite element method (SFEM) [3, 4, 5, 6,\n7, 8, 9] and resolve the existence, linearity and positivity deficiencies\npointed out in [1]. We show that Wachspress interpolants [10] computed in the\nphysical coordinate system are very well suited to the SFEM, especially when\nelements are heavily distorted (obtuse interior angles). The proposed\napproximation leads to results which are almost identical to those of the SFEM\ninitially proposed in [3]. These results that the proposed approximation scheme\nforms a strong and rigorous basis for construction of smoothed finite element\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.3729v1"
    },
    {
        "title": "On the performance of high-order finite elements with respect to maximum\n  principles and the non-negative constraint for diffusion-type equations",
        "authors": [
            "G. S. Payette",
            "K. B. Nakshatrala",
            "J. N. Reddy"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  The main aim of this paper is to document the performance of $p$-refinement\nwith respect to maximum principles and the non-negative constraint. The model\nproblem is (steady-state) anisotropic diffusion with decay (which is a\nsecond-order elliptic partial differential equation). We considered the\nstandard single-field formulation (which is based on the Galerkin formalism)\nand two least-squares-based mixed formulations. We have employed non-uniform\nLagrange polynomials for altering the polynomial order in each element, and we\nhave used $p = 1, ..., 10$.\n  It will be shown that the violation of the non-negative constraint will not\nvanish with $p$-refinement for anisotropic diffusion. We shall illustrate the\nperformance of $p$-refinement using several representative problems. The\nintended outcome of the paper is twofold. Firstly, this study will caution the\nusers of high-order approximations about its performance with respect to\nmaximum principles and the non-negative constraint. Secondly, this study will\nhelp researchers to develop new methodologies for enforcing maximum principles\nand the non-negative constraint under high-order approximations.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.0952v1"
    },
    {
        "title": "On strong homogeneity of two global optimization algorithms based on\n  statistical models of multimodal objective functions",
        "authors": [
            "Antanas Zilinskas"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  The implementation of global optimization algorithms, using the arithmetic of\ninfinity, is considered. A relatively simple version of implementation is\nproposed for the algorithms that possess the introduced property of strong\nhomogeneity. It is shown that the P-algorithm and the one-step Bayesian\nalgorithm are strongly homogeneous.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.1042v1"
    },
    {
        "title": "Solving Principal Component Pursuit in Linear Time via $l_1$ Filtering",
        "authors": [
            "Risheng Liu",
            "Zhouchen Lin",
            "Siming Wei",
            "Zhixun Su"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  In the past decades, exactly recovering the intrinsic data structure from\ncorrupted observations, which is known as robust principal component analysis\n(RPCA), has attracted tremendous interests and found many applications in\ncomputer vision. Recently, this problem has been formulated as recovering a\nlow-rank component and a sparse component from the observed data matrix. It is\nproved that under some suitable conditions, this problem can be exactly solved\nby principal component pursuit (PCP), i.e., minimizing a combination of nuclear\nnorm and $l_1$ norm. Most of the existing methods for solving PCP require\nsingular value decompositions (SVD) of the data matrix, resulting in a high\ncomputational complexity, hence preventing the applications of RPCA to very\nlarge scale computer vision problems. In this paper, we propose a novel\nalgorithm, called $l_1$ filtering, for \\emph{exactly} solving PCP with an\n$O(r^2(m+n))$ complexity, where $m\\times n$ is the size of data matrix and $r$\nis the rank of the matrix to recover, which is supposed to be much smaller than\n$m$ and $n$. Moreover, $l_1$ filtering is \\emph{highly parallelizable}. It is\nthe first algorithm that can \\emph{exactly} solve a nuclear norm minimization\nproblem in \\emph{linear time} (with respect to the data size). Experiments on\nboth synthetic data and real applications testify to the great advantage of\n$l_1$ filtering in speed over state-of-the-art algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5359v4"
    },
    {
        "title": "A Modulus-Squared Dirichlet Boundary Condition for Time-Dependent\n  Complex Partial Differential Equations and its Application to the Nonlinear\n  Schrödinger Equation",
        "authors": [
            "R. M. Caplan",
            "R. Carretero-González"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  An easy to implement modulus-squared Dirichlet (MSD) boundary condition is\nformulated for numerical simulations of time-dependent complex partial\ndifferential equations in multidimensional settings. The MSD boundary condition\napproximates a constant modulus-square value of the solution at the boundaries.\nApplication of the MSD boundary condition to the nonlinear Schr\\\"odinger\nequation is shown, and numerical simulations are performed to demonstrate its\nusefulness and advantages over other simple boundary conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.0569v1"
    },
    {
        "title": "High-order finite volume schemes for layered atmospheric models",
        "authors": [
            "Dante Kalise",
            "Ivar Lie",
            "Eleuterio F. Toro"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  We present a numerical scheme for the solution of a class of atmospheric\nmodels where high horizontal resolution is required while a coarser vertical\nstructure is allowed. The proposed scheme considers a layering procedure for\nthe original set of equations, and the use of high-order ADER finite volume\nschemes for the solution of the system of balance laws arising from the\ndimensional reduction procedure. We present several types of layering based\nupon Galerkin discretizations of the vertical structure, and we study the\neffect of incrementing the order of horizontal approximation. Numerical\nexperiments for the computational validation of the convergence of the scheme\ntogether with the study of physical phenomena are performed over 2D linear\nadvective models, including a set of equations for an isothermal atmosphere.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.6834v1"
    },
    {
        "title": "Vertex-centroid finite volume scheme on tetrahedral grids for\n  conservation laws",
        "authors": [
            "Praveen Chandrashekar",
            "Ashish Garg"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Vertex-centroid schemes are cell-centered finite volume schemes for\nconservation laws which make use of vertex values to construct high resolution\nschemes. The vertex values must be obtained through a consistent averaging\n(interpolation) procedure. A modified interpolation scheme is proposed which is\nbetter than existing schemes in giving positive weights in the interpolation\nformula. A simplified reconstruction scheme is also proposed which is also more\naccurate and efficient. For scalar conservation laws, we develop limited\nversions of the schemes which are stable in maximum norm by constructing\nsuitable limiters. The schemes are applied to compressible flows governed by\nthe Euler equations of inviscid gas dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.4238v1"
    },
    {
        "title": "Improving non-linear fits",
        "authors": [
            "Massimo Di Pierro"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this notes we describe an algorithm for non-linear fitting which\nincorporates some of the features of linear least squares into a general\nminimum $\\chi^2$ fit and provide a pure Python implementation of the algorithm.\nIt consists of the variable projection method (varpro), combined with a Newton\noptimizer and stabilized using the steepest descent with an adaptative step.\nThe algorithm includes a term to account for Bayesian priors. We performed\ntests of the algorithm using simulated data. This method is suitable, for\nexample, for fitting with sums of exponentials as often needed in Lattice\nQuantum Chromodynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.0988v2"
    },
    {
        "title": "Technical Report on Hypergraph-Partitioning-Based Models and Methods for\n  Exploiting Cache Locality in Sparse-Matrix Vector Multiplication",
        "authors": [
            "Kadir Akbudak",
            "Enver Kayaaslan",
            "Cevdet Aykanat"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The sparse matrix-vector multiplication (SpMxV) is a kernel operation widely\nused in iterative linear solvers. The same sparse matrix is multiplied by a\ndense vector repeatedly in these solvers. Matrices with irregular sparsity\npatterns make it difficult to utilize cache locality effectively in SpMxV\ncomputations. In this work, we investigate single- and multiple-SpMxV\nframeworks for exploiting cache locality in SpMxV computations. For the\nsingle-SpMxV framework, we propose two cache-size-aware top-down\nrow/column-reordering methods based on 1D and 2D sparse matrix partitioning by\nutilizing the column-net and enhancing the row-column-net hypergraph models of\nsparse matrices. The multiple-SpMxV framework depends on splitting a given\nmatrix into a sum of multiple nonzero-disjoint matrices so that the SpMxV\noperation is performed as a sequence of multiple input- and output- dependent\nSpMxV operations. For an effective matrix splitting required in this framework,\nwe propose a cache- size-aware top-down approach based on 2D sparse matrix\npartitioning by utilizing the row-column-net hypergraph model. For this\nframework, we also propose two methods for effective ordering of individual\nSpMxV operations. The primary objective in all of the three methods is to\nmaximize the exploitation of temporal locality. We evaluate the validity of our\nmodels and methods on a wide range of sparse matrices using both cache-miss\nsimulations and actual runs by using OSKI. Experimental results show that\nproposed methods and models outperform state-of-the-art schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.3856v3"
    },
    {
        "title": "On the complexity of solving initial value problems",
        "authors": [
            "Olivier Bournez",
            "Daniel S. Graça",
            "Amaury Pouly"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this paper we prove that computing the solution of an initial-value\nproblem $\\dot{y}=p(y)$ with initial condition $y(t_0)=y_0\\in\\R^d$ at time\n$t_0+T$ with precision $e^{-\\mu}$ where $p$ is a vector of polynomials can be\ndone in time polynomial in the value of $T$, $\\mu$ and $Y=\\sup_{t_0\\leqslant\nu\\leqslant T}\\infnorm{y(u)}$. Contrary to existing results, our algorithm works\nfor any vector of polynomials $p$ over any bounded or unbounded domain and has\na guaranteed complexity and precision. In particular we do not assume $p$ to be\nfixed, nor the solution to lie in a compact domain, nor we assume that $p$ has\na Lipschitz constant.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.4407v1"
    },
    {
        "title": "Divide-and-Conquer Method for L1 Norm Matrix Factorization in the\n  Presence of Outliers and Missing Data",
        "authors": [
            "Deyu Meng",
            "Zongben Xu"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The low-rank matrix factorization as a L1 norm minimization problem has\nrecently attracted much attention due to its intrinsic robustness to the\npresence of outliers and missing data. In this paper, we propose a new method,\ncalled the divide-and-conquer method, for solving this problem. The main idea\nis to break the original problem into a series of smallest possible\nsub-problems, each involving only unique scalar parameter. Each of these\nsubproblems is proved to be convex and has closed-form solution. By recursively\noptimizing these small problems in an analytical way, efficient algorithm,\nentirely avoiding the time-consuming numerical optimization as an inner loop,\nfor solving the original problem can naturally be constructed. The\ncomputational complexity of the proposed algorithm is approximately linear in\nboth data size and dimensionality, making it possible to handle large-scale L1\nnorm matrix factorization problems. The algorithm is also theoretically proved\nto be convergent. Based on a series of experiment results, it is substantiated\nthat our method always achieves better results than the current\nstate-of-the-art methods on $L1$ matrix factorization calculation in both\ncomputational time and accuracy, especially on large-scale applications such as\nface recognition and structure from motion.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.5844v3"
    },
    {
        "title": "Power Series Method applied to Inverse Analysis in Chemical Kinetics\n  Problem",
        "authors": [
            "E. Lopez-Sandoval",
            "A. Mello",
            "J. J. Godina-Nava",
            "A. R. Samana"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Power Series Solution Method has been traditionally used to solve Ordinary\nand Partial Linear Differential Equations. However, despite their usefulness\nthe application of this method has been limited to this particular kind of\nequations. In this work we use the method of power series to solve nonlinear\npartial differential equations. The method is applied to solve three versions\nof nonlinear time-dependent Burgers-Type differential equations in order to\ndemonstrate its scope and applicability.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.3800v7"
    },
    {
        "title": "An Arithmetic and Geometric Mean Invariant",
        "authors": [
            "John Lindgren",
            "Vibeke Libby"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  A positive real interval, [a, b], can be partitioned into sub-intervals such\nthat sub-interval widths divided by sub-interval \"average\" values remains\nconstant. That both Arithmetic Mean and Geometric Mean \"average\" values produce\nconstant ratios for the same log scale is the stated invariance proved in this\nshort note. The continuous analog is briefly considered and shown to have\nsimilar properties.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.4617v1"
    },
    {
        "title": "Revisiting the D-iteration method: from theoretical to practical\n  computation cost",
        "authors": [
            "Dohy Hong"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this paper, we revisit the D-iteration algorithm in order to better\nexplain its connection to the Gauss-Seidel method and different performance\nresults that were observed. In particular, we study here the practical\ncomputation cost based on the execution runtime compared to the theoretical\nnumber of iterations. We also propose an exact formula of the error for\nPageRank class of equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.6030v1"
    },
    {
        "title": "A parallel sweeping preconditioner for heterogeneous 3D Helmholtz\n  equations",
        "authors": [
            "Jack Poulson",
            "Björn Engquist",
            "Siwei Li",
            "Lexing Ying"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  A parallelization of a sweeping preconditioner for 3D Helmholtz equations\nwithout large cavities is introduced and benchmarked for several challenging\nvelocity models. The setup and application costs of the sequential\npreconditioner are shown to be O({\\gamma}^2 N^{4/3}) and O({\\gamma} N log N),\nwhere {\\gamma}({\\omega}) denotes the modestly frequency-dependent number of\ngrid points per Perfectly Matched Layer. Several computational and memory\nimprovements are introduced relative to using black-box sparse-direct solvers\nfor the auxiliary problems, and competitive runtimes and iteration counts are\nreported for high-frequency problems distributed over thousands of cores. Two\nopen-source packages are released along with this paper: \"Parallel Sweeping\nPreconditioner (PSP)\" and the underlying distributed multifrontal solver,\n\"Clique\".\n",
        "pdf_link": "http://arxiv.org/pdf/1204.0111v4"
    },
    {
        "title": "D-iteration: application to differential equations",
        "authors": [
            "Dohy Hong"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this paper, we study how the D-iteration algorithm can be applied to\nnumerically solve the differential equations such as heat equation in 2D or 3D.\nThe method can be applied on the class of problems that can be addressed by the\nGauss-Seidel iteration, based on the linear approximation of the differential\nequations.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.1423v1"
    },
    {
        "title": "On Fast Computation of Gradients for CANDECOMP/PARAFAC Algorithms",
        "authors": [
            "Anh Huy Phan",
            "Petr Tichavský",
            "Andrzej Cichocki"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Product between mode-$n$ unfolding $\\bY_{(n)}$ of an $N$-D tensor $\\tY$ and\nKhatri-Rao products of $(N-1)$ factor matrices $\\bA^{(m)}$, $m = 1,..., n-1,\nn+1, ..., N$ exists in algorithms for CANDECOMP/PARAFAC (CP). If $\\tY$ is an\nerror tensor of a tensor approximation, this product is the gradient of a cost\nfunction with respect to factors, and has the largest workload in most CP\nalgorithms. In this paper, a fast method to compute this product is proposed.\nExperimental verification shows that the fast CP gradient can accelerate the\nCP_ALS algorithm 2 times and 8 times faster for factorizations of 3-D and 4-D\ntensors, and the speed-up ratios can be 20-30 times for higher dimensional\ntensors.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.1586v1"
    },
    {
        "title": "Computational complexity and memory usage for multi-frontal direct\n  solvers in structured mesh finite elements",
        "authors": [
            "Nathan Collier",
            "David Pardo",
            "Maciej Paszynski",
            "Victor M. Calo"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The multi-frontal direct solver is the state-of-the-art algorithm for the\ndirect solution of sparse linear systems. This paper provides computational\ncomplexity and memory usage estimates for the application of the multi-frontal\ndirect solver algorithm on linear systems resulting from B-spline-based\nisogeometric finite elements, where the mesh is a structured grid. Specifically\nwe provide the estimates for systems resulting from $C^{p-1}$ polynomial\nB-spline spaces and compare them to those obtained using $C^0$ spaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.1718v1"
    },
    {
        "title": "Dissecting the FEAST algorithm for generalized eigenproblems",
        "authors": [
            "Lukas Krämer",
            "Edoardo Di Napoli",
            "Martin Galgon",
            "Bruno Lang",
            "Paolo Bientinesi"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  We analyze the FEAST method for computing selected eigenvalues and\neigenvectors of large sparse matrix pencils. After establishing the close\nconnection between FEAST and the well-known Rayleigh-Ritz method, we identify\nseveral critical issues that influence convergence and accuracy of the solver:\nthe choice of the starting vector space, the stopping criterion, how the inner\nlinear systems impact the quality of the solution, and the use of FEAST for\ncomputing eigenpairs from multiple intervals. We complement the study with\nnumerical examples, and hint at possible improvements to overcome the existing\nproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.1726v1"
    },
    {
        "title": "Alternative Tilings for the Fast Multipole Method on the Plane",
        "authors": [
            "Yuancheng Luo",
            "Ramani Duraiswami"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The fast multipole method (FMM) performs fast approximate kernel summation to\na specified tolerance $\\epsilon$ by using a hierarchical division of the\ndomain, which groups source and receiver points into regions that satisfy local\nseparation and the well-separated pair decomposition properties. While square\ntilings and quadtrees are commonly used in 2D, we investigate alternative\ntilings and associated spatial data structures: regular hexagons (septree) and\ntriangles (triangle-quadtree). We show that both structures satisfy separation\nproperties for the FMM and prove their theoretical error bounds and\ncomputational costs. Empirical runtime and error analysis of our\nimplementations are provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.3105v1"
    },
    {
        "title": "Computing without a computer: a new approach for solving nonlinear\n  differential equations",
        "authors": [
            "Vladimir Aristov",
            "Andrey Stroganov"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The well-known Turing machine is an example of a theoretical digital\ncomputer, and it was the logical basis of constructing real electronic\ncomputers. In the present paper we propose an alternative, namely, by\nformalising arithmetic operations in the ordinary computing device, we attempt\nto go to the analytical procedure (for calculations). The method creates\npossibilities for solving nonlinear differential equations and systems. Our\ntheoretical computer model requires retaining a finite number of terms to\nrepresent numbers, and utilizes digit carry procedure. The solution is\nrepresented in the form of a segment of a series in the powers of the step size\nof the independent variable in the finite-difference scheme. The algorithm\ngenerates a schematic representation that approximates the convergent\nfinite-difference scheme, which, in turn, approximates the equation under\nconsideration. The use of probabilistic methods allows us to average the\nrecurrent calculations and exclude intermediate levels of computation. All the\nstages of formalizing operations of the classical computer result in \"the\nmethod of the computer analogy\". The proposed method leads to an explicit\nanalytical representation of the solution. We present the general features of\nthe algorithm which are illustrated by an example of solutions for a system of\nnonlinear equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.3241v1"
    },
    {
        "title": "Understanding differential equations through diffusion point of view",
        "authors": [
            "Dohy Hong"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this paper, we propose a new adaptation of the D-iteration algorithm to\nnumerically solve the differential equations. This problem can be reinterpreted\nin 2D or 3D (or higher dimensions) as a limit of a diffusion process where the\nboundary or initial conditions are replaced by fluid catalysts. Pre-computing\nthe diffusion process for an elementary catalyst case as a fundamental block of\na class of differential equations, we show that the computation efficiency can\nbe greatly improved. The method can be applied on the class of problems that\ncan be addressed by the Gauss-Seidel iteration, based on the linear\napproximation of the differential equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.5429v1"
    },
    {
        "title": "Revisiting the D-iteration method: runtime comparison",
        "authors": [
            "Dohy Hong",
            "Gérard Burnside",
            "Philippe Raoult"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this paper, we revisit the D-iteration algorithm in order to better\nexplain different performance results that were observed for the numerical\ncomputation of the eigenvector associated to the PageRank score. We revisit\nhere the practical computation cost based on the execution runtime compared to\nthe theoretical number of iterations.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.6255v1"
    },
    {
        "title": "A Hough Transform Approach to Solving Linear Min-Max Problems",
        "authors": [
            "Carmi Grushko"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Several ways to accelerate the solution of 2D/3D linear min-max problems in\n$n$ constraints are discussed. We also present an algorithm for solving such\nproblems in the 2D case, which is superior to CGAL's linear programming solver,\nboth in performance and in stability.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.5911v1"
    },
    {
        "title": "Comments on \"On Approximating Euclidean Metrics by Weighted t-Cost\n  Distances in Arbitrary Dimension\"",
        "authors": [
            "M. Emre Celebi",
            "Hassan A. Kingravi",
            "Fatih Celiker"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Mukherjee (Pattern Recognition Letters, vol. 32, pp. 824-831, 2011) recently\nintroduced a class of distance functions called weighted t-cost distances that\ngeneralize m-neighbor, octagonal, and t-cost distances. He proved that weighted\nt-cost distances form a family of metrics and derived an approximation for the\nEuclidean norm in $\\mathbb{Z}^n$. In this note we compare this approximation to\ntwo previously proposed Euclidean norm approximations and demonstrate that the\nempirical average errors given by Mukherjee are significantly optimistic in\n$\\mathbb{R}^n$. We also propose a simple normalization scheme that improves the\naccuracy of his approximation substantially with respect to both average and\nmaximum relative errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2061v1"
    },
    {
        "title": "The cost of continuity: performance of iterative solvers on isogeometric\n  finite elements",
        "authors": [
            "Nathan Collier",
            "Lisandro Dalcin",
            "David Pardo",
            "V. M. Calo"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this paper we study how the use of a more continuous set of basis\nfunctions affects the cost of solving systems of linear equations resulting\nfrom a discretized Galerkin weak form. Specifically, we compare performance of\nlinear solvers when discretizing using $C^0$ B-splines, which span traditional\nfinite element spaces, and $C^{p-1}$ B-splines, which represent maximum\ncontinuity. We provide theoretical estimates for the increase in cost of the\nmatrix-vector product as well as for the construction and application of\nblack-box preconditioners. We accompany these estimates with numerical results\nand study their sensitivity to various grid parameters such as element size $h$\nand polynomial order of approximation $p$. Finally, we present timing results\nfor a range of preconditioning options for the Laplace problem. We conclude\nthat the matrix-vector product operation is at most $\\slfrac{33p^2}{8}$ times\nmore expensive for the more continuous space, although for moderately low $p$,\nthis number is significantly reduced. Moreover, if static condensation is not\nemployed, this number further reduces to at most a value of 8, even for high\n$p$. Preconditioning options can be up to $p^3$ times more expensive to setup,\nalthough this difference significantly decreases for some popular\npreconditioners such as Incomplete LU factorization.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2948v1"
    },
    {
        "title": "Optimizing the eigenvector computation algorithm with diffusion approach",
        "authors": [
            "Dohy Hong",
            "Philippe Jacquet"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this paper, we apply the ideas of the matrix column based diffusion\napproach to define a new eigenvector computation algorithm of a stationary\nprobability of a Markov chain.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.3177v1"
    },
    {
        "title": "Flux-splitting schemes for parabolic problems",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  To solve numerically boundary value problems for parabolic equations with\nmixed derivatives, the construction of difference schemes with prescribed\nquality faces essential difficulties. In parabolic problems, some possibilities\nare associated with the transition to a new formulation of the problem, where\nthe fluxes (derivatives with respect to a spatial direction) are treated as\nunknown quantities. In this case, the original problem is rewritten in the form\nof a boundary value problem for the system of equations in the fluxes. This\nwork deals with studying schemes with weights for parabolic equations written\nin the flux coordinates. Unconditionally stable flux locally one-dimensional\nschemes of the first and second order of approximation in time are constructed\nfor parabolic equations without mixed derivatives. A peculiarity of the system\nof equations written in flux variables for equations with mixed derivatives is\nthat there do exist coupled terms with time derivatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.3450v1"
    },
    {
        "title": "Numerical Computations For Operator Axioms",
        "authors": [
            "Pith Peishu Xie"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The Operator axioms have produced new real numbers with new operators. New\noperators naturally produce new equations and thus extend the traditional\nmathematical models which are selected to describe various scientific rules. So\nnew operators help to describe complex scientific rules which are difficult\ndescribed by traditional equations and have an enormous application potential.\nAs to the equations including new operators, engineering computation often need\nthe approximate solutions reflecting an intuitive order relation and\nequivalence relation. However, the order relation and equivalence relation of\nreal numbers are not as intuitive as those of base-b expansions. Thus, this\npaper introduces numerical computations to approximate all real numbers with\nbase-b expansions.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.0701v14"
    },
    {
        "title": "Numerical Methods for Solving Convection-Diffusion Problems",
        "authors": [
            "A. Churbanov",
            "P. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Convection-diffusion equations provide the basis for describing heat and mass\ntransfer phenomena as well as processes of continuum mechanics. To handle flows\nin porous media, the fundamental issue is to model correctly the convective\ntransport of individual phases. Moreover, for compressible media, the pressure\nequation itself is just a time-dependent convection-diffusion equation.\n  For different problems, a convection-diffusion equation may be be written in\nvarious forms. The most popular formulation of convective transport employs the\ndivergent (conservative) form. In some cases, the nondivergent (characteristic)\nform seems to be preferable. The so-called skew-symmetric form of convective\ntransport operators that is the half-sum of the operators in the divergent and\nnondivergent forms is of great interest in some applications.\n  Here we discuss the basic classes of discretization in space: finite\ndifference schemes on rectangular grids, approximations on general polyhedra\n(the finite volume method), and finite element procedures. The key properties\nof discrete operators are studied for convective and diffusive transport. We\nemphasize the problems of constructing approximations for convection and\ndiffusion operators that satisfy the maximum principle at the discrete level\n--- they are called monotone approximations.\n  Two- and three-level schemes are investigated for transient problems.\nUnconditionally stable explicit-implicit schemes are developed for\nconvection-diffusion problems. Stability conditions are obtained both in\nfinite-dimensional Hilbert spaces and in Banach spaces depending on the form in\nwhich the convection-diffusion equation is written.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.5649v1"
    },
    {
        "title": "Unconditionally stable schemes for non-stationary convection-diffusion\n  equations",
        "authors": [
            "N. Afanasyeva",
            "P. Vabishchevich",
            "M. Vasil'eva"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Convection-diffusion problem are the base for continuum mechanics. The main\nfeatures of these problems are associated with an indefinite operator the\nproblem. In this work we construct unconditionally stable scheme for\nnon-stationary convection-diffusion equations, which are based on use of new\nvariables. Also, we consider these equations in the form of\nconvection-diffusion-reaction and construct unconditionally stable schemes when\nexplicit-implicit approximations are used with splitting of the reaction\noperator.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.6140v1"
    },
    {
        "title": "A numerical framework for diffusion-controlled bimolecular-reactive\n  systems to enforce maximum principles and non-negative constraint",
        "authors": [
            "K. B. Nakshatrala",
            "M. K. Mudunuru",
            "A. J. Valocchi"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  We present a novel computational framework for diffusive-reactive systems\nthat satisfies the non-negative constraint and maximum principles on general\ncomputational grids. The governing equations for the concentration of reactants\nand product are written in terms of tensorial diffusion-reaction equations. %\nWe restrict our studies to fast irreversible bimolecular reactions. If one\nassumes that the reaction is diffusion-limited and all chemical species have\nthe same diffusion coefficient, one can employ a linear transformation to\nrewrite the governing equations in terms of invariants, which are unaffected by\nthe reaction. This results in two uncoupled tensorial diffusion equations in\nterms of these invariants, which are solved using a novel non-negative solver\nfor tensorial diffusion-type equations. The concentrations of the reactants and\nthe product are then calculated from invariants using algebraic manipulations.\nThe novel aspect of the proposed computational framework is that it will always\nproduce physically meaningful non-negative values for the concentrations of all\nchemical species. Several representative numerical examples are presented to\nillustrate the robustness, convergence, and the numerical performance of the\nproposed computational framework. We will also compare the proposed framework\nwith other popular formulations. In particular, we will show that the Galerkin\nformulation (which is the standard single-field formulation) does not produce\nreliable solutions, and the reason can be attributed to the fact that the\nsingle-field formulation does not guarantee non-negative solutions. We will\nalso show that the clipping procedure (which produces non-negative solutions\nbut is considered as a variational crime) does not give accurate results when\ncompared with the proposed computational framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.5290v4"
    },
    {
        "title": "Epigraphical splitting for solving constrained convex formulations of\n  inverse problems with proximal tools",
        "authors": [
            "Giovanni Chierchia",
            "Nelly Pustelnik",
            "Jean-Christophe Pesquet",
            "Béatrice Pesquet-Popescu"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  We propose a proximal approach to deal with a class of convex variational\nproblems involving nonlinear constraints. A large family of constraints, proven\nto be effective in the solution of inverse problems, can be expressed as the\nlower level set of a sum of convex functions evaluated over different, but\npossibly overlapping, blocks of the signal. For such constraints, the\nassociated projection operator generally does not have a simple form. We\ncircumvent this difficulty by splitting the lower level set into as many\nepigraphs as functions involved in the sum. A closed half-space constraint is\nalso enforced, in order to limit the sum of the introduced epigraphical\nvariables to the upper bound of the original lower level set. In this paper, we\nfocus on a family of constraints involving linear transforms of distance\nfunctions to a convex set or $\\ell_{1,p}$ norms with $p\\in \\{1,2,\\infty\\}$. In\nthese cases, the projection onto the epigraph of the involved function has a\nclosed form expression.\n  The proposed approach is validated in the context of image restoration with\nmissing samples, by making use of constraints based on Non-Local Total\nVariation. Experiments show that our method leads to significant improvements\nin term of convergence speed over existing algorithms for solving similar\nconstrained problems. A second application to a pulse shape design problem is\nprovided in order to illustrate the flexibility of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.5844v3"
    },
    {
        "title": "Strict localization of eigenvectors and eigenvalues",
        "authors": [
            "Łukasz Struski",
            "Jacek Tabor"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this article we show and implement a simple and effcient method to\nstrictly locate eigenvectors and eigenvalues of a given matrix, based on the\nmodified cone condition. As a consequence we can also effectively localize\nzeros of complex polynomials.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.8072v1"
    },
    {
        "title": "A SVD accelerated kernel-independent fast multipole method and its\n  application to BEM",
        "authors": [
            "Yanchuang Cao",
            "Lihua Wen",
            "Junjie Rong"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The kernel-independent fast multipole method (KIFMM) proposed in [1] is of\nalmost linear complexity. In the original KIFMM the time-consuming M2L\ntranslations are accelerated by FFT. However, when more equivalent points are\nused to achieve higher accuracy, the efficiency of the FFT approach tends to be\nlower because more auxiliary volume grid points have to be added. In this\npaper, all the translations of the KIFMM are accelerated by using the singular\nvalue decomposition (SVD) based on the low-rank property of the translating\nmatrices. The acceleration of M2L is realized by first transforming the\nassociated translating matrices into more compact form, and then using low-rank\napproximations. By using the transform matrices for M2L, the orders of the\ntranslating matrices in upward and downward passes are also reduced. The\nimproved KIFMM is then applied to accelerate BEM. The performance of the\nproposed algorithms are demonstrated by three examples. Numerical results show\nthat, compared with the original KIFMM, the present method can reduce about 40%\nof the iterating time and 25% of the memory requirement.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.2517v2"
    },
    {
        "title": "Report: Error estimation of recovered solution in FE analysis",
        "authors": [
            "Enrique Nadal Soriano",
            "Octavio Andrés González Estrada",
            "Juan José Ródenas García",
            "Francisco Javier Fuenmayor Fernández"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The recovery type error estimators introduced by Zienkiewicz and Zhu use a\nrecovered stress field evaluated from the Finite Element (FE) solution. Their\naccuracy depends on the quality of the recovered field. In this sense, accurate\nresults are obtained using recovery procedures based on the Superconvergent\nPatch recovery technique (SPR). These error estimators can be easily\nimplemented and provide accurate estimates. Another important feature is that\nthe recovered solution is of a better quality than the FE solution and can\ntherefore be used as an enhanced solution. We have developed an SPR-type\nrecovery technique that considers equilibrium and displacements constraints to\nobtain a very accurate recovered displacements field from which a recovered\nstress field can also be evaluated. We propose the use of these recovered\nfields as the standard output of the FE code instead of the raw FE solution.\nTechniques to quantify the error of the recovered solution are therefore\nneeded. In this report we present an error estimation technique that accurately\nevaluates the error of the recovered solution both at global and local levels\nin the FEM and XFEM frameworks. We have also developed an h-adaptive mesh\nrefinement strategy based on the error of the recovered solution. As the\nconverge rate of the error of the recovered solution is higher than that of the\nFE one, the computational cost required to obtain a solution with a prescribed\naccuracy is smaller than for traditional h-adaptive processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.3821v1"
    },
    {
        "title": "Numerical comparison of different algorithms for construction of wavelet\n  matrices",
        "authors": [
            "Nika Salia",
            "Alexander Gamkrelidze",
            "Lasha Ephremidze"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Factorization of compact wavelet matrices into primitive ones has been known\nfor more than 20 years. This method makes it possible to generate wavelet\nmatrix coefficients and also to specify them by their first row. Recently, a\nnew parametrization of compact wavelet matrices of the same order and degree\nhas been introduced by the last author. This method also enables us to fulfill\nthe above mentioned tasks of matrix constructions. In the present paper, we\nbriefly describe the corresponding algorithms based on two different methods,\nand numerically compare their performance\n",
        "pdf_link": "http://arxiv.org/pdf/1211.4516v1"
    },
    {
        "title": "A Mathematical Random Number Generator (MRNG)",
        "authors": [
            "Osvaldo Skliar",
            "Ricardo E. Monge",
            "Sherry Gapper",
            "Guillermo Oviedo"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  A novel Mathematical Random Number Generator (MRNG) is presented here. In\nthis case, \"mathematical\" refers to the fact that to construct that generator\nit is not necessary to resort to a physical phenomenon, such as the thermal\nnoise of an electronic device, but rather to a mathematical procedure. The MRNG\ngenerates binary strings - in principle, as long as desired - which may be\nconsidered genuinely random in the sense that they pass the statistical tests\ncurrently accepted to evaluate the randomness of those strings. From those\nstrings, the MRNG also generates random numbers expressed in base 10. An MRNG\nhas been installed as a facility on the following web page:\nhttp://www.appliedmathgroup.org. This generator may be used for applications in\ntasks in: a) computational simulation of probabilistic-type systems, and b) the\nrandom selection of samples of different populations. Users interested in\napplications in cryptography can build another MRNG, but they would have to\nwithhold information - specified in section 5 - from people who are not\nauthorized to decode messages encrypted using that resource.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.5052v1"
    },
    {
        "title": "An inverse iteration method for eigenvalue problems with eigenvector\n  nonlinearities",
        "authors": [
            "Elias Jarlebring",
            "Simen Kvaal",
            "Wim Michiels"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Consider a symmetric matrix $A(v)\\in\\RR^{n\\times n}$ depending on a vector\n$v\\in\\RR^n$ and satisfying the property $A(\\alpha v)=A(v)$ for any\n$\\alpha\\in\\RR\\backslash{0}$. We will here study the problem of finding\n$(\\lambda,v)\\in\\RR\\times \\RR^n\\backslash\\{0\\}$ such that $(\\lambda,v)$ is an\neigenpair of the matrix $A(v)$ and we propose a generalization of inverse\niteration for eigenvalue problems with this type of eigenvector nonlinearity.\nThe convergence of the proposed method is studied and several convergence\nproperties are shown to be analogous to inverse iteration for standard\neigenvalue problems, including local convergence properties. The algorithm is\nalso shown to be equivalent to a particular discretization of an associated\nordinary differential equation, if the shift is chosen in a particular way. The\nalgorithm is adapted to a variant of the Schr\\\"odinger equation known as the\nGross-Pitaevskii equation. We use numerical simulations toillustrate the\nconvergence properties, as well as the efficiency of the algorithm and the\nadaption.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.0417v1"
    },
    {
        "title": "A direct solver with reutilization of previously-computed LU\n  factorizations for h-adaptive finite element grids with point singularities",
        "authors": [
            "Maciej Paszynski",
            "Victor Calo",
            "David Pardo"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  This paper describes a direct solver algorithm for a sequence of finite\nelement meshes that are h-refined towards one or several point singularities.\nFor such a sequence of grids, the solver delivers linear computational cost\nO(N) in terms of CPU time and memory with respect to the number of unknowns N.\nThe linear computational cost is achieved by utilizing the recursive structure\nprovided by the sequence of h-adaptive grids with a special construction of the\nelimination tree that allows for reutilization of previously computed partial\nLU factorizations over the entire unrefined part of the computational mesh. The\nreutilization technique reduces the computational cost of the entire sequence\nof h-refined grids from O(N^2) down to O(N). Theoretical estimates are\nillustrated with numerical results on two- and three-dimensional model problems\nexhibiting one or several point singularities.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.1992v1"
    },
    {
        "title": "Nonsymmetric multigrid preconditioning for conjugate gradient methods",
        "authors": [
            "Henricus Bouwmeester",
            "Andrew Dougherty",
            "Andrew V. Knyazev"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  We numerically analyze the possibility of turning off post-smoothing\n(relaxation) in geometric multigrid when used as a preconditioner in conjugate\ngradient linear and eigenvalue solvers for the 3D Laplacian. The geometric\nSemicoarsening Multigrid (SMG) method is provided by the hypre parallel\nsoftware package. We solve linear systems using two variants (standard and\nflexible) of the preconditioned conjugate gradient (PCG) and preconditioned\nsteepest descent (PSD) methods. The eigenvalue problems are solved using the\nlocally optimal block preconditioned conjugate gradient (LOBPCG) method\navailable in hypre through BLOPEX software. We observe that turning off the\npost-smoothing in SMG dramatically slows down the standard PCG-SMG. For\nflexible PCG and LOBPCG, our numerical results show that post-smoothing can be\navoided, resulting in overall acceleration, due to the high costs of smoothing\nand relatively insignificant decrease in convergence speed. We numerically\ndemonstrate for linear systems that PSD-SMG and flexible PCG-SMG converge\nsimilarly if SMG post-smoothing is off. We experimentally show that the effect\nof acceleration is independent of memory interconnection. A theoretical\njustification is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.6680v3"
    },
    {
        "title": "Spectral Condition-Number Estimation of Large Sparse Matrices",
        "authors": [
            "Haim Avron",
            "Alex Druinsky",
            "Sivan Toledo"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We describe a randomized Krylov-subspace method for estimating the spectral\ncondition number of a real matrix A or indicating that it is numerically rank\ndeficient. The main difficulty in estimating the condition number is the\nestimation of the smallest singular value \\sigma_{\\min} of A. Our method\nestimates this value by solving a consistent linear least-squares problem with\na known solution using a specific Krylov-subspace method called LSQR. In this\nmethod, the forward error tends to concentrate in the direction of a right\nsingular vector corresponding to \\sigma_{\\min}. Extensive experiments show that\nthe method is able to estimate well the condition number of a wide array of\nmatrices. It can sometimes estimate the condition number when running a dense\nSVD would be impractical due to the computational cost or the memory\nrequirements. The method uses very little memory (it inherits this property\nfrom LSQR) and it works equally well on square and rectangular matrices.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1107v6"
    },
    {
        "title": "On the Behavior of the Residual in Conjugate Gradient Method",
        "authors": [
            "Teruyoshi Washizawa"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In conjugate gradient method, it is well known that the recursively computed\nresidual differs from true one as the iteration proceeds in finite arithmetic.\nSome work have been devoted to analyze this be-havior and to evaluate the lower\nand the upper bounds of the difference. This paper focuses on the behavior of\nthese two kinds of residuals, especially their lower bounds caused by the loss\nof trailing digit, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.4749v1"
    },
    {
        "title": "Introducing One Step Back Iterative Approach to Solve Linear and Non\n  Linear Fixed Point Problem",
        "authors": [
            "Dohy Hong"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this paper, we introduce a new iterative method which we call one step\nback approach: the main idea is to anticipate the consequence of the iterative\ncomputation per coordinate and to optimize on the choice of the sequence of the\ncoordinates on which the iterative update computations are done. The method\nrequires the increase of the size of the state vectors and one iteration step\nloss from the initial vector. We illustrate the approach in linear and non\nlinear iterative equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.4317v1"
    },
    {
        "title": "A decomposition method with minimum communication amount for\n  parallelization of multi-dimensional FFTs",
        "authors": [
            "Truong Vinh Truong Duy",
            "Taisuke Ozaki"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The fast Fourier transform (FFT) is undoubtedly an essential primitive that\nhas been applied in various fields of science and engineering. In this paper,\nwe present a decomposition method for parallelization of multi-dimensional FFTs\nwith smallest communication amount for all ranges of the number of processes\ncompared to previously proposed methods. This is achieved by two distinguishing\nfeatures: adaptive decomposition and transpose order awareness. In the proposed\nmethod, the FFT data are decomposed based on a row-wise basis that maps the\nmulti-dimensional data into one-dimensional data, and translates the\ncorresponding coordinates from multi-dimensions into one-dimension so that the\nresultant one-dimensional data can be divided and allocated equally to the\nprocesses. As a result, differently from previous works that have the\ndimensions of decomposition pre-defined, our method can adaptively decompose\nthe FFT data on the lowest possible dimensions depending on the number of\nprocesses. In addition, this row-wise decomposition provides plenty of\nalternatives in data transpose, and different transpose order results in\ndifferent amount of communication. We identify the best transpose orders with\nsmallest communication amounts for the 3-D, 4-D, and 5-D FFTs by analyzing all\npossible cases. Given both communication efficiency and scalability, our method\nis promising in development of highly efficient parallel packages for the FFT.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.6189v1"
    },
    {
        "title": "A Novel Algorithm for Linear Programming",
        "authors": [
            "K. Eswaran"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The problem of optimizing a linear objective function,given a number of\nlinear constraints has been a long standing problem ever since the times of\nKantorovich, Dantzig and von Neuman. These developments have been followed by a\ndifferent approach pioneered by Khachiyan and Karmarkar.\n  In this paper we present an entirely new method for solving an old\noptimization problem in a novel manner, a technique that reduces the dimension\nof the problem step by step and interestingly is recursive. A theorem which\nproves the correctness of the approach is given.\n  The method can be extended to other types of optimization problems in convex\nspace, e.g. for solving a linear optimization problem subject to nonlinear\nconstraints in a convex region.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.4942v1"
    },
    {
        "title": "Note: interpreting iterative methods convergence with diffusion point of\n  view",
        "authors": [
            "Dohy Hong"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this paper, we explain the convergence speed of different iteration\nschemes with the fluid diffusion view when solving a linear fixed point\nproblem. This interpretation allows one to better understand why power\niteration or Jacobi iteration may converge faster or slower than Gauss-Seidel\niteration.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.1760v1"
    },
    {
        "title": "Extrapolation-based implicit-explicit general linear methods",
        "authors": [
            "Angelamaria Cardone",
            "Zdzislaw Jackiewicz",
            "Hong Zhang",
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  For many systems of differential equations modeling problems in science and\nengineering, there are natural splittings of the right hand side into two\nparts, one non-stiff or mildly stiff, and the other one stiff. For such systems\nimplicit-explicit (IMEX) integration combines an explicit scheme for the\nnon-stiff part with an implicit scheme for the stiff part.\n  In a recent series of papers two of the authors (Sandu and Zhang) have\ndeveloped IMEX GLMs, a family of implicit-explicit schemes based on general\nlinear methods. It has been shown that, due to their high stage order, IMEX\nGLMs require no additional coupling order conditions, and are not marred by\norder reduction.\n  This work develops a new extrapolation-based approach to construct practical\nIMEX GLM pairs of high order. We look for methods with large absolute stability\nregion, assuming that the implicit part of the method is A- or L-stable. We\nprovide examples of IMEX GLMs with optimal stability properties. Their\napplication to a two dimensional test problem confirms the theoretical\nfindings.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.2276v1"
    },
    {
        "title": "Numerical solving the identification problem for the lower coefficient\n  of parabolic equation",
        "authors": [
            "P. N. Vabishchevich",
            "V. I. Vasil'ev"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In the theory and practice of inverse problems for partial differential\nequations (PDEs) much attention is paid to the problem of the identification of\ncoefficients from some additional information. This work deals with the problem\nof determining in a multidimensional parabolic equation the lower coefficient\nthat depends on time only. To solve numerically a nonlinear inverse problem,\nlinearized approximations in time are constructed using standard finite element\nprocedures in space. The computational algorithm is based on a special\ndecomposition, where the transition to a new time level is implemented via\nsolving two standard elliptic problems. The numerical results presented here\nfor a model 2D problem demonstrate capabilities of the proposed computational\nalgorithms for approximate solving inverse problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.5923v1"
    },
    {
        "title": "Local Convergence of an Algorithm for Subspace Identification from\n  Partial Data",
        "authors": [
            "Laura Balzano",
            "Stephen J. Wright"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an iterative\nalgorithm for identifying a linear subspace of R^n from data consisting of\npartial observations of random vectors from that subspace. This paper examines\nlocal convergence properties of GROUSE, under assumptions on the randomness of\nthe observed vectors, the randomness of the subset of elements observed at each\niteration, and incoherence of the subspace with the coordinate directions.\nConvergence at an expected linear rate is demonstrated under certain\nassumptions. The case in which the full random vector is revealed at each\niteration allows for much simpler analysis, and is also described. GROUSE is\nrelated to incremental SVD methods and to gradient projection algorithms in\noptimization.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3391v2"
    },
    {
        "title": "Improved bounds on sample size for implicit matrix trace estimators",
        "authors": [
            "Farbod Roosta-Khorasani",
            "Uri Ascher"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  This article is concerned with Monte-Carlo methods for the estimation of the\ntrace of an implicitly given matrix $A$ whose information is only available\nthrough matrix-vector products. Such a method approximates the trace by an\naverage of $N$ expressions of the form $\\ww^t (A\\ww)$, with random vectors\n$\\ww$ drawn from an appropriate distribution. We prove, discuss and experiment\nwith bounds on the number of realizations $N$ required in order to guarantee a\nprobabilistic bound on the relative error of the trace estimation upon\nemploying Rademacher (Hutchinson), Gaussian and uniform unit vector (with and\nwithout replacement) probability distributions.\n  In total, one necessary bound and six sufficient bounds are proved, improving\nupon and extending similar estimates obtained in the seminal work of Avron and\nToledo (2011) in several dimensions. We first improve their bound on $N$ for\nthe Hutchinson method, dropping a term that relates to $rank(A)$ and making the\nbound comparable with that for the Gaussian estimator.\n  We further prove new sufficient bounds for the Hutchinson, Gaussian and the\nunit vector estimators, as well as a necessary bound for the Gaussian\nestimator, which depend more specifically on properties of the matrix $A$. As\nsuch they may suggest for what type of matrices one distribution or another\nprovides a particularly effective or relatively ineffective stochastic\nestimation method.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.2475v2"
    },
    {
        "title": "Fast Multipole Preconditioners for Sparse Matrices Arising from Elliptic\n  Equations",
        "authors": [
            "Huda Ibeid",
            "Rio Yokota",
            "Jennifer Pestana",
            "David Keyes"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Among optimal hierarchical algorithms for the computational solution of\nelliptic problems, the Fast Multipole Method (FMM) stands out for its\nadaptability to emerging architectures, having high arithmetic intensity,\ntunable accuracy, and relaxable global synchronization requirements. We\ndemonstrate that, beyond its traditional use as a solver in problems for which\nexplicit free-space kernel representations are available, the FMM has\napplicability as a preconditioner in finite domain elliptic boundary value\nproblems, by equipping it with boundary integral capability for satisfying\nconditions at finite boundaries and by wrapping it in a Krylov method for\nextensibility to more general operators. Here, we do not discuss the well\ndeveloped applications of FMM to implement matrix-vector multiplications within\nKrylov solvers of boundary element methods. Instead, we propose using FMM for\nthe volume-to-volume contribution of inhomogeneous Poisson-like problems, where\nthe boundary integral is a small part of the overall computation. Our method\nmay be used to precondition sparse matrices arising from finite\ndifference/element discretizations, and can handle a broader range of\nscientific applications. Compared with multigrid methods, it is capable of\ncomparable algebraic convergence rates down to the truncation error of the\ndiscretized PDE, and it offers potentially superior multicore and distributed\nmemory scalability properties on commodity architecture supercomputers.\nCompared with other methods exploiting the low rank character of off-diagonal\nblocks of the dense resolvent operator, FMM-preconditioned Krylov iteration may\nreduce the amount of communication because it is matrix-free and exploits the\ntree structure of FMM. We describe our tests in reproducible detail with freely\navailable codes and outline directions for further extensibility.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.3339v4"
    },
    {
        "title": "Randomized algorithms for low-rank matrix factorizations: sharp\n  performance bounds",
        "authors": [
            "Rafi Witten",
            "Emmanuel Candes"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The development of randomized algorithms for numerical linear algebra, e.g.\nfor computing approximate QR and SVD factorizations, has recently become an\nintense area of research. This paper studies one of the most frequently\ndiscussed algorithms in the literature for dimensionality\nreduction---specifically for approximating an input matrix with a low-rank\nelement. We introduce a novel and rather intuitive analysis of the algorithm in\nMartinsson et al. (2008), which allows us to derive sharp estimates and give\nnew insights about its performance. This analysis yields theoretical guarantees\nabout the approximation error and at the same time, ultimate limits of\nperformance (lower bounds) showing that our upper bounds are tight. Numerical\nexperiments complement our study and show the tightness of our predictions\ncompared with empirical observations.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.5697v1"
    },
    {
        "title": "Numerical solutions of a class of second order boundary value problems\n  on using Bernoulli Polynomials",
        "authors": [
            "Md. Shafiqul Islam",
            "Afroza Shirin"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The aim of this paper is to find the numerical solutions of the second order\nlinear and nonlinear differential equations with Dirichlet, Neumann and Robin\nboundary conditions. We use the Bernoulli polynomials as linear combination to\nthe approximate solutions of 2nd order boundary value problems. Here the\nBernoulli polynomials over the interval [0, 1] are chosen as trial functions so\nthat care has been taken to satisfy the corresponding homogeneous form of the\nDirichlet boundary conditions in the Galerkin weighted residual method. In\naddition to that the given differential equation over arbitrary finite domain\n[a, b] and the boundary conditions are converted into its equivalent form over\nthe interval [0, 1]. All the formulas are verified by considering numerical\nexamples. The approximate solutions are compared with the exact solutions, and\nalso with the solutions of the existing methods. A reliable good accuracy is\nobtained in all cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.6064v2"
    },
    {
        "title": "Coefficient Matrices Computation of Structural Vector Autoregressive\n  Model",
        "authors": [
            "Aravindh Krishnamoorthy"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this paper we present the Large Inverse Cholesky (LIC) method, an\nefficient method for computing the coefficient matrices of a Structural Vector\nAutoregressive (SVAR) model.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.6290v3"
    },
    {
        "title": "Numerical Solutions of Fredholm Integral Equations Using Bernstein\n  Polynomials",
        "authors": [
            "Afroza Shirin",
            "Md. Shafiqul Islam"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this paper, Bernstein piecewise polynomials are used to solve the integral\nequations numerically. A matrix formulation is given for a non-singular linear\nFredholm Integral Equation by the technique of Galerkin method. In the Galerkin\nmethod, the Bernstein polynomials are used as the approximation of basis\nfunctions. Examples are considered to verify the effectiveness of the proposed\nderivations, and the numerical solutions guarantee the desired accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.6311v1"
    },
    {
        "title": "Kaczmarz Algorithm with Soft Constraints for User Interface Layout",
        "authors": [
            "Noreen Jamil",
            "Deanna Needell",
            "Johannes Muller",
            "Christof Lutteroth",
            "Gerald Weber"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The Kaczmarz method is an iterative method for solving large systems of\nequations that projects iterates orthogonally onto the solution space of each\nequation. In contrast to direct methods such as Gaussian elimination or\nQR-factorization, this algorithm is efficient for problems with sparse\nmatrices, as they appear in constraint-based user interface (UI) layout\nspecifications. However, the Kaczmarz method as described in the literature has\nits limitations: it considers only equality constraints and does not support\nsoft constraints, which makes it inapplicable to the UI layout problem.\n  In this paper we extend the Kaczmarz method for solving specifications\ncontaining soft constraints, using the prioritized IIS detection algorithm.\nFurthermore, the performance and convergence of the proposed algorithms are\nevaluated empirically using randomly generated UI layout specifications of\nvarious sizes. The results show that these methods offer improvements in\nperformance over standard methods like Matlab's LINPROG, a well-known efficient\nlinear programming solver.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.7001v2"
    },
    {
        "title": "General inner approximation of vector-valued functions",
        "authors": [
            "Mullier Olivier",
            "Éric Goubault",
            "Michel Kieffer",
            "Sylvie Putot"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  This paper addresses the problem of evaluating a subset of the range of a\nvector-valued function. It is based on a work by Gold- sztejn and Jaulin which\nprovides methods based on interval analysis to address this problem when the\ndimension of the domain and co-domain of the function are equal. This paper\nextends this result to vector-valued functions with domain and co-domain of\ndifferent dimensions. This ex- tension requires the knowledge of the rank of\nthe Jacobian function on the whole domain. This leads to the sub-problem of\nextracting an in- terval sub-matrix of maximum rank from a given interval\nmatrix. Three different techniques leading to approximate solutions of this\nextraction are proposed and compared.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.1709v1"
    },
    {
        "title": "Isogeometric finite element analysis of functionally graded plates using\n  a refined plate theory",
        "authors": [
            "H. Nguyen-Xuan",
            "Loc V. Tran",
            "Chien H. Thai",
            "S. Kulasegaram",
            "S. P. A. Bordas"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We propose in this paper a novel inverse tangent transverse shear deformation\nformulation for functionally graded material (FGM) plates. The isogeometric\nfinite element analysis (IGA) of static, free vibration and buckling problems\nof FGM plates is then addressed using a refined plate theory (RPT). The RPT\nenables us to describe the non-linear distribution of shear stresses through\nthe plate thickness without any requirement of shear correction factors (SCF).\nIGA utilizes basis functions, namely B-splines or non-uniform rational\nB-splines (NURBS), which achieve easily the smoothness of any arbitrary order.\nIt hence satisfies the C1 requirement of the RPT model. The present method\napproximates the displacement field of four degrees of freedom per each control\npoint and retains the computational efficiency while ensuring the high accuracy\nin solution.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.1847v1"
    },
    {
        "title": "The application of the exact operational matrices for solving the\n  Emden-Fowler equations, arising in astrophysics",
        "authors": [
            "K. Parand",
            "Sayyed A. Hossayni",
            "J. A. Rad"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The objective of this paper is to apply the well-known exact operational\nmatrices (EOMs) idea for solving the Emden-Fowler equations, illustrating the\nsuperiority of EOMs versus ordinary operational matrices (OOMs). Up to now, a\nfew studies have been conducted on EOMs and the differential equations solved\nby them do not have high-degree nonlinearity and the reported results are not\nregarded as appropriate criteria for the excellence of the new method. So, we\nchose Emden-Fowler type differential equations and solved them by this method.\nTo confirm the accuracy of the new method and to show the preeminence of EOMs\nversus OOMs, the norm1 of the residual and error function of both methods are\nevaluated for multiple $m$ values, where $m$ is the degree of the Bernstein\npolynomials. We reported the results in form of plots to illustrate the error\nconvergence of both methods to zero and also to show the primacy of the new\nmethod versus OOMs. The obtained results have demonstrated the increased\naccuracy of the new method.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.1906v2"
    },
    {
        "title": "GPU-Acceleration of Parallel Unconditionally Stable Group Explicit\n  Finite Difference Method",
        "authors": [
            "K. Parand",
            "Saeed Zafarvahedian",
            "Sayyed A. Hossayni"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Graphics Processing Units (GPUs) are high performance co-processors\noriginally intended to improve the use and quality of computer graphics\napplications. Once, researchers and practitioners noticed the potential of\nusing GPU for general purposes, GPUs applications have been extended from\ngraphics applications to other fields. The main objective of this paper is to\nevaluate the impact of using GPU in solution of the transient diffusion type\nequation by parallel and stable group explicit finite difference method and\nencourage the researchers in this field to immigrate from implementing their\nalgorithms in CPU to the GPU emerging world. For comparing them, we implemented\nthe method in both GPU and CPU (multi-core) programming context. Moreover, we\nproposed an optimal synchronization arrangement for the implementation\npseudo-code. Also, the interrelation of GPU parallel programming and\ninitializing the algorithm variables were discussed, taking advantage of\nnumerical experiences. The GPU-approach results are faster than those obtained\nfrom a much expensive parallel 8-thread CPU-based programming. The GPU used in\nthis paper, is an ordinary old laptop GPU (GT 335M, launched at 2010) and is\naccessible for everyone and the newer generations of GPU (as discussed in\npaper) have even more performance priority over the similar-price GPUs. Then,\nthe results are expected to encourage the entire research society to take\nadvantage of GPUs and improve the time efficiency of their studies.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.3422v3"
    },
    {
        "title": "A class of generalized additive Runge-Kutta methods",
        "authors": [
            "Adrian Sandu",
            "Michael Guenther"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  This work generalizes the additively partitioned Runge-Kutta methods by\nallowing for different stage values as arguments of different components of the\nright hand side. An order conditions theory is developed for the new family of\ngeneralized additive methods, and stability and monotonicity investigations are\ncarried out. The paper discusses the construction and properties of\nimplicit-explicit and implicit-implicit,methods in the new framework. The new\nfamily, named GARK, introduces additional flexibility when compared to\ntraditional partitioned Runge-Kutta methods, and therefore offers additional\nopportunities for the development of flexible solvers for systems with multiple\nscales, or driven by multiple physical processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.5573v1"
    },
    {
        "title": "Multirate generalized additive Runge Kutta methods",
        "authors": [
            "Michael Guenther",
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  This work constructs a new class of multirate schemes based on the recently\ndeveloped generalized additive Runge-Kutta (GARK) methods (Sandu and Guenther,\n2013). Multirate schemes use different step sizes for different components and\nfor different partitions of the right-hand side based on the local activity\nlevels. We show that the new multirate GARK family includes many well-known\nmultirate schemes as special cases. The order conditions theory follows\ndirectly from the GARK accuracy theory. Nonlinear stability and monotonicity\ninvestigations show that these properties are inherited from the base schemes\nprovided that additional coupling conditions hold.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.6055v1"
    },
    {
        "title": "A priori estimation of a time step for numerically solving parabolic\n  problems",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  This work deals with the problem of choosing a time step for the numerical\nsolution of boundary value problems for parabolic equations. The problem\nsolution is derived using the fully implicit scheme, whereas a time step is\nselected via explicit calculations. The selection strategy consists of the\nfollowing steps. First, using the explicit scheme, we calculate the solution at\na new time level. Next, we employ this solution in order to obtain the solution\nat the previous time level (the implicit scheme, explicit calculations). This\nsolution should be close to the solution of our problem at this time level with\na prescribed accuracy. Such an algorithm leads to explicit formulas for the\ncalculation of the time step and takes into account both the dynamics of the\nproblem solution and changes in coefficients of the equation and in its\nright-hand side. The same formulas for the evaluation of the time step we get\nusing a comparison of two approximate solutions, which are obtained using the\nexplicit scheme with the primary time step and the step that is reduced by\nhalf. Numerical results are presented for a model parabolic boundary value\nproblem, which demonstrate the robustness of the developed algorithm for the\ntime step selection.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.2780v1"
    },
    {
        "title": "Splitting schemes for poroelasticity and thermoelasticity problems",
        "authors": [
            "A. E. Kolesov",
            "P. N. Vabishchevich",
            "M. V. Vasilyeva"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this work, we consider the coupled systems of linear unsteady partial\ndifferential equations, which arise in the modeling of poroelasticity\nprocesses. Stability estimates of weighted difference schemes for the coupled\nsystem of equations are presented. Approximation in space is based on the\nfinite element method. We construct splitting schemes and give some numerical\ncomparisons for typical poroelasticity problems. The results of numerical\nsimulation of a 3D problem are presented. Special attention is given to using\nhight performance computing systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.3766v1"
    },
    {
        "title": "Data completion and stochastic algorithms for PDE inversion problems\n  with many measurements",
        "authors": [
            "Farbod Roosta-Khorasani",
            "Kees van den Doel",
            "Uri Ascher"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Inverse problems involving systems of partial differential equations (PDEs)\nwith many measurements or experiments can be very expensive to solve\nnumerically. In a recent paper we examined dimensionality reduction methods,\nboth stochastic and deterministic, to reduce this computational burden,\nassuming that all experiments share the same set of receivers. In the present\narticle we consider the more general and practically important case where\nreceivers are not shared across experiments. We propose a data completion\napproach to alleviate this problem. This is done by means of an approximation\nusing an appropriately restricted gradient or Laplacian regularization,\nextending existing data for each experiment to the union of all receiver\nlocations. Results using the method of simultaneous sources (SS) with the\ncompleted data are then compared to those obtained by a more general but slower\nrandom subset (RS) method which requires no modifications.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.0707v2"
    },
    {
        "title": "Exploiting Data Representation for Fault Tolerance",
        "authors": [
            "James Elliott",
            "Mark Hoemmen",
            "Frank Mueller"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We explore the link between data representation and soft errors in dot\nproducts. We present an analytic model for the absolute error introduced should\na soft error corrupt a bit in an IEEE-754 floating-point number. We show how\nthis finding relates to the fundamental linear algebra concepts of\nnormalization and matrix equilibration. We present a case study illustrating\nthat the probability of experiencing a large error in a dot product is\nminimized when both vectors are normalized. Furthermore, when data is\nnormalized we show that the absolute error is less than one or very large,\nwhich allows us to detect large errors. We demonstrate how this finding can be\nused by instrumenting the GMRES iterative solver. We count all possible errors\nthat can be introduced through faults in arithmetic in the computationally\nintensive orthogonalization phase, and show that when scaling is used the\nabsolute error can be bounded above by one.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.2333v1"
    },
    {
        "title": "Numerical Reproducibility and Parallel Computations: Issues for Interval\n  Algorithms",
        "authors": [
            "Nathalie Revol",
            "Philippe Théveny"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  What is called \"numerical reproducibility\" is the problem of getting the same\nresult when the scientific computation is run several times, either on the same\nmachine or on different machines, with different types and numbers of\nprocessing units, execution environments, computational loads etc. This problem\nis especially stringent for HPC numerical simulations. In what follows, the\nfocus is on parallel implementations of interval arithmetic using\nfloating-point arithmetic. For interval computations, numerical reproducibility\nis of course an issue for testing and debugging purposes. However, as long as\nthe computed result encloses the exact and unknown result, the inclusion\nproperty, which is the main property of interval arithmetic, is satisfied and\ngetting bit for bit identical results may not be crucial. Still, implementation\nissues may invalidate the inclusion property. Several ways to preserve the\ninclusion property are presented, on the example of the product of matrices\nwith interval coefficients.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.3300v1"
    },
    {
        "title": "Numerical solving the boundary value problem for fractional powers of\n  elliptic operators",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  A boundary value problem for a fractional power of the second-order elliptic\noperator is considered. It is solved numerically using a time-dependent problem\nfor a pseudo-parabolic equation. For the auxiliary Cauchy problem, the standard\ntwo-level schemes with weights are applied. Stability conditions are obtained\nfor the fully discrete schemes under the consideration. The numerical results\nare presented for a model two-dimensional boundary value problem wit a\nfractional power of an elliptic operator. The dependence of accuracy on grids\nin time and in space is studied.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.1636v1"
    },
    {
        "title": "Non-Orthogonal Tensor Diagonalization",
        "authors": [
            "Petr Tichavsky",
            "Anh Huy Phan",
            "Andrzej Cichocki"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Tensor diagonalization means transforming a given tensor to an exactly or\nnearly diagonal form through multiplying the tensor by non-orthogonal\ninvertible matrices along selected dimensions of the tensor. It is\ngeneralization of approximate joint diagonalization (AJD) of a set of matrices.\nIn particular, we derive (1) a new algorithm for symmetric AJD, which is called\ntwo-sided symmetric diagonalization of order-three tensor, (2) a similar\nalgorithm for non-symmetric AJD, also called general two-sided diagonalization\nof an order-3 tensor, and (3) an algorithm for three-sided diagonalization of\norder-3 or order-4 tensors. The latter two algorithms may serve for canonical\npolyadic (CP) tensor decomposition, and they can outperform other CP tensor\ndecomposition methods in terms of computational speed under the restriction\nthat the tensor rank does not exceed the tensor multilinear rank. Finally, we\npropose (4) similar algorithms for tensor block diagonalization, which is\nrelated to the tensor block-term decomposition.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.1673v3"
    },
    {
        "title": "Comparison of POD reduced order strategies for the nonlinear 2D Shallow\n  Water Equations",
        "authors": [
            "Răzvan Ştefănescu",
            "Adrian Sandu",
            "Ionel M. Navon"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This paper introduces tensorial calculus techniques in the framework of\nProper Orthogonal Decomposition (POD) to reduce the computational complexity of\nthe reduced nonlinear terms. The resulting method, named tensorial POD, can be\napplied to polynomial nonlinearities of any degree $p$. Such nonlinear terms\nhave an on-line complexity of $\\mathcal{O}(k^{p+1})$, where $k$ is the\ndimension of POD basis, and therefore is independent of full space dimension.\nHowever it is efficient only for quadratic nonlinear terms since for higher\nnonlinearities standard POD proves to be less time consuming once the POD basis\ndimension $k$ is increased. Numerical experiments are carried out with a two\ndimensional shallow water equation (SWE) test problem to compare the\nperformance of tensorial POD, standard POD, and POD/Discrete Empirical\nInterpolation Method (DEIM). Numerical results show that tensorial POD\ndecreases by $76\\times$ times the computational cost of the on-line stage of\nstandard POD for configurations using more than $300,000$ model variables. The\ntensorial POD SWE model was only $2-8\\times$ slower than the POD/DEIM SWE model\nbut the implementation effort is considerably increased. Tensorial calculus was\nagain employed to construct a new algorithm allowing POD/DEIM shallow water\nequation model to compute its off-line stage faster than the standard and\ntensorial POD approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.2018v1"
    },
    {
        "title": "On the maximum relative error when computing x^n in floating-point\n  arithmetic",
        "authors": [
            "Stef Graillat",
            "Vincent Lefèvre",
            "Jean-Michel Muller"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this paper, we improve the usual relative error bound for the computation\nof x^n through iterated multiplications by x in binary floating-point\narithmetic. The obtained error bound is only slightly better than the usual\none, but it is simpler. We also discuss the more general problem of computing\nthe product of n terms.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.2991v1"
    },
    {
        "title": "Non-linear mass-spring system for large soft tissue deformations\n  modeling",
        "authors": [
            "Sergei Nikolaev"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Implant placement under soft tissues operation is described. In this\noperation tissues can reach such deformations that nonlinear properties are\nappeared. A mass-spring model modification for modeling nonlinear tissue\noperation is developed. A method for creating elasticity module using splines\nis described. For Poisson ratio different stiffness for different types of\nsprings in cubic grid is used. For stiffness finding an equation system that\ndescribed material tension is solved. The model is verified with quadratic\nsample tension experiment. These tests show that sample tension under external\nforces is equal to defined nonlinear elasticity module. The accuracy of Poisson\nratio modeling is thirty five percent that is better the results of available\nratio modeling method.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.2294v1"
    },
    {
        "title": "Improvement of the monotonicity properties of the difference schemes by\n  building in them of the monotonizing operators",
        "authors": [
            "Y. V. Troshchiev"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  The method of monotonization of difference schemes is being considered in the\npaper. The method was earlier proposed by the author for stationary problems.\nIt is investigated in the paper more profoundly. The idea of the method is to\nbuild the monotonizing operators into the schemes so that the balance relations\nfrom point to point are not violated. Different monotonizing operators can be\nused to be installed in the schemes. Propositions concerning approximation and\nstability of the monotonized schemes are formulated and proved. Also a\nproposition significant for practical use of the schemes is formulated and\nproved. The idea is to use the monotonized schemes in the cases when the\nproposition conditions are fulfilled. The proposition is based on closeness of\nsolutions of the initial and auxiliary schemes. Constructions for solving of\ntime dependent problems are also written in the paper. One dimensional example\nand three-dimensional hydrodynamic example are considered. The method allows to\nconsiderably decrease value of calculations in many cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.3046v1"
    },
    {
        "title": "A Fast Block Low-Rank Dense Solver with Applications to Finite-Element\n  Matrices",
        "authors": [
            "Amirhossein Aminfar",
            "Sivaram Ambikasaran",
            "Eric Darve"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This article presents a fast solver for the dense \"frontal\" matrices that\narise from the multifrontal sparse elimination process of 3D elliptic PDEs. The\nsolver relies on the fact that these matrices can be efficiently represented as\na hierarchically off-diagonal low-rank (HODLR) matrix. To construct the\nlow-rank approximation of the off-diagonal blocks, we propose a new\npseudo-skeleton scheme, the boundary distance low-rank approximation, that\npicks rows and columns based on the location of their corresponding vertices in\nthe sparse matrix graph. We compare this new low-rank approximation method to\nthe adaptive cross approximation (ACA) algorithm and show that it achieves\nbetters speedup specially for unstructured meshes. Using the HODLR direct\nsolver as a preconditioner (with a low tolerance) to the GMRES iterative\nscheme, we can reach machine accuracy much faster than a conventional LU\nsolver. Numerical benchmarks are provided for frontal matrices arising from 3D\nfinite element problems corresponding to a wide range of applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.5337v3"
    },
    {
        "title": "Fast Isogeometric Boundary Element Method based on Independent Field\n  Approximation",
        "authors": [
            "Benjamin Marussig",
            "Jürgen Zechner",
            "Gernot Beer",
            "Thomas-Peter Fries"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  An isogeometric boundary element method for problems in elasticity is\npresented, which is based on an independent approximation for the geometry,\ntraction and displacement field. This enables a flexible choice of refinement\nstrategies, permits an efficient evaluation of geometry related information, a\nmixed collocation scheme which deals with discontinuous tractions along\nnon-smooth boundaries and a significant reduction of the right hand side of the\nsystem of equations for common boundary conditions. All these benefits are\nachieved without any loss of accuracy compared to conventional isogeometric\nformulations. The system matrices are approximated by means of hierarchical\nmatrices to reduce the computational complexity for large scale analysis. For\nthe required geometrical bisection of the domain, a strategy for the evaluation\nof bounding boxes containing the supports of NURBS basis functions is\npresented. The versatility and accuracy of the proposed methodology is\ndemonstrated by convergence studies showing optimal rates and real world\nexamples in two and three dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.0306v3"
    },
    {
        "title": "Isogeometric Boundary Element Method with Hierarchical Matrices",
        "authors": [
            "Jürgen Zechner",
            "Benjamin Marussig",
            "Gernot Beer",
            "Thomas-Peter Fries"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this work we address the complexity problem of the isogeometric Boundary\nElement Method by proposing a collocation scheme for practical problems in\nlinear elasticity and the application of hierarchical matrices. For mixed\nboundary value problems, a block system of matrices similar to Galerkin\nformulations is constructed allowing an effective application of that matrix\nformat. We introduce a strategy for the geometric bisection of surfaces based\non NURBS patches. The approximation of system matrices is carried out by means\nof kernel interpolation. Numerical results are shown that prove the success of\nthe formulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.2817v1"
    },
    {
        "title": "Boundary Element Analysis with trimmed NURBS and a generalized IGA\n  approach",
        "authors": [
            "Gernot Beer",
            "Benjamin Marussig",
            "Jürgen Zechner",
            "Christian Dünser",
            "Thomas-Peter Fries"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  A novel approach to the simulation with the boundary element method using\ntrimmed NURBS patches is presented. The advantage of this approach is its\nefficiency and easy implementation. The analysis with trimmed NURBS is achieved\nby double mapping. The variation of the unknowns on the boundary is specified\nin a local coordinate system and is completely independent of the description\nof the geometry. The method is tested on a branched tunnel and the results\ncompared with those obtained from a conventional analysis. The conclusion is\nthat the proposed approach is superior in terms of number of unknowns and\neffort required.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.3499v1"
    },
    {
        "title": "Structural index reduction algorithms for differential algebraic\n  equations via fixed-point iteration",
        "authors": [
            "Juan Tang",
            "Wenyuan Wu",
            "Xiaolin Qin",
            "Yong Feng"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Motivated by Pryce's structural index reduction method for differential\nalgebraic equations (DAEs), we show the complexity of the fixed-point iteration\nalgorithm and propose a fixed-point iteration method with parameters. It leads\nto a block fixed-point iteration method which can be applied to large-scale\nDAEs with block upper triangular structure. Moreover, its complexity analysis\nis also given in this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.4473v3"
    },
    {
        "title": "A new operational matrix based on Bernoulli polynomials",
        "authors": [
            "J. A. Rad",
            "S. Kazem",
            "M. Shaban",
            "K. Parand"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this research, the Bernoulli polynomials are introduced. The properties of\nthese polynomials are employed to construct the operational matrices of\nintegration together with the derivative and product. These properties are then\nutilized to transform the differential equation to a matrix equation which\ncorresponds to a system of algebraic equations with unknown Bernoulli\ncoefficients. This method can be used for many problems such as differential\nequations, integral equations and so on. Numerical examples show the method is\ncomputationally simple and also illustrate the efficiency and accuracy of the\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.2207v1"
    },
    {
        "title": "The meshless method for solving radiative transfer problems in a slab\n  medium based on radial basis functions",
        "authors": [
            "J. A. Rad",
            "S. Kazem",
            "K. Parand"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this paper a numerical meshless method for solving the radiative transfer\nequations in a slab medium with an isotropic scattering is considered. The\nmethod is based on radial basis functions to approximate the solution of an\nintegral-partial differential equation by using collocation method. For this\npurpose different applications of RBFs are used. To this end the numerical\nsolutions are obtained without any mesh generation into the domain of the\nproblems. The results of numerical experiments are compared with the existing\nresults in illustrative examples to confirm the accuracy and efficiency of the\npresented scheme. Also the norm of the residual functions are obtained to show\nthe convergence of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.2209v1"
    },
    {
        "title": "Algorithms that satisfy a stopping criterion, probably",
        "authors": [
            "Uri Ascher",
            "Farbod Roosta-Khorasani"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Iterative numerical algorithms are typically equipped with a stopping\ncriterion, where the iteration process is terminated when some error or misfit\nmeasure is deemed to be below a given tolerance. This is a useful setting for\ncomparing algorithm performance, among other purposes. However, in practical\napplications a precise value for such a tolerance is rarely known; rather, only\nsome possibly vague idea of the desired quality of the numerical approximation\nis at hand. We discuss four case studies from different areas of numerical\ncomputation, where uncertainty in the error tolerance value and in the stopping\ncriterion is revealed in different ways. This leads us to think of approaches\nto relax the notion of exactly satisfying a tolerance value. We then\nconcentrate on a {\\em probabilistic} relaxation of the given tolerance. This\nallows, for instance, derivation of proven bounds on the sample size of certain\nMonte Carlo methods. We describe an algorithm that becomes more efficient in a\ncontrolled way as the uncertainty in the tolerance increases, and demonstrate\nthis in the context of some particular applications of inverse problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.5946v2"
    },
    {
        "title": "A random algorithm for low-rank decomposition of large-scale matrices\n  with missing entries",
        "authors": [
            "Yiguang Liu"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  A Random SubMatrix method (RSM) is proposed to calculate the low-rank\ndecomposition of large-scale matrices with known entry percentage \\rho. RSM is\nvery fast as the floating-point operations (flops) required are compared\nfavorably with the state-of-the-art algorithms. Meanwhile RSM is very\nmemory-saving. With known entries homogeneously distributed in the given\nmatrix, sub-matrices formed by known entries are randomly selected. According\nto the just proved theorem that subspace related to smaller singular values is\nless perturbed by noise, the null vectors or the right singular vectors\nassociated with the minor singular values are calculated for each submatrix.\nThe vectors are the null vectors of the corresponding submatrix in the ground\ntruth of the given large-scale matrix. If enough sub-matrices are randomly\nchosen, the low-rank decomposition is estimated. The experimental results on\nrandom synthetical matrices with sizes such as 131072X1024 and on real data\nsets indicate that RSM is much faster and memory-saving, and, meanwhile, has\nconsiderable high precision achieving or approximating to the best.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.0814v1"
    },
    {
        "title": "Tensor Transpose and Its Properties",
        "authors": [
            "Ran Pan"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Tensor transpose is a higher order generalization of matrix transpose. In\nthis paper, we use permutations and symmetry group to define? the tensor\ntranspose. Then we discuss the classification and composition of tensor\ntransposes. Properties of tensor transpose are studied in relation to tensor\nmultiplication, tensor eigenvalues, tensor decompositions and tensor rank.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1503v1"
    },
    {
        "title": "Better Late Than Never: Filling a Void in the History of Fast Matrix\n  Multiplication and Tensor Decompositions",
        "authors": [
            "Victor Y. Pan"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Multilinear and tensor decompositions are a popular tool in linear and\nmultilinear algebra and have a wide range of important applications to modern\ncomputing. Our paper of 1972 presented the first nontrivial application of such\ndecompositions to fundamental matrix computations and was also a landmark in\nthe history of the acceleration of matrix multiplication. Published in 1972 in\nRussian, it has never been translated into English. It has been very rarely\ncited in the Western literature on matrix multiplication and never in the works\non multilinear and tensor decompositions. This motivates us to present its\ntranslation into English, together with our brief comments on its impact on the\ntwo fields.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1972v1"
    },
    {
        "title": "Approximating Matrices with Multiple Symmetries",
        "authors": [
            "Charles Van Loan",
            "Joseph Vokt"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  If a tensor with various symmetries is properly unfolded, then the resulting\nmatrix inherits those symmetries. As tensor computations become increasingly\nimportant it is imperative that we develop efficient structure preserving\nmethods for matrices with multiple symmetries. In this paper we consider how to\nexploit and preserve structure in the pivoted Cholesky factorization when\napproximating a matrix $A$ that is both symmetric ($A=A^T$) and what we call\n{\\em perfect shuffle symmetric}, or {\\em perf-symmetric}. The latter property\nmeans that $A = \\Pi A\\Pi$ where $\\Pi$ is a permutation with the property that\n$\\Pi v = v$ if $v$ is the vec of a symmetric matrix and $\\Pi v = -v$ if $v$ is\nthe vec of a skew-symmetric matrix. Matrices with this structure can arise when\nan order-4 tensor $\\cal A$ is unfolded and its elements satisfy ${\\cal\nA}(i_{1},i_{2},i_{3},i_{4}) = {\\cal A}(i_{2},i_{1},i_{3},i_{4}) ={\\cal\nA}(i_{1},i_{2},i_{4},i_{3}) ={\\cal A}(i_{3},i_{4},i_{1},i_{2}).$ This is the\ncase in certain quantum chemistry applications where the tensor entries are\nelectronic repulsion integrals. Our technique involves a closed-form block\ndiagonalization followed by one or two half-sized pivoted Cholesky\nfactorizations. This framework allows for a lazy evaluation feature that is\nimportant if the entries in $\\cal A$ are expensive to compute. In addition to\nbeing a structure preserving rank reduction technique, we find that this\napproach for obtaining the Cholesky factorization reduces the work by up to a\nfactor of 4.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.6296v2"
    },
    {
        "title": "A Two Stage CVT / Eikonal Convection Mesh Deformation Approach for Large\n  Nodal Deformations",
        "authors": [
            "Stephan Schmidt"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  A two step mesh deformation approach for large nodal deformations, typically\narising from non-parametric shape optimization, fluid-structure interaction or\ncomputer graphics, is considered. Two major difficulties, collapsed cells and\nan undesirable parameterization, are overcome by considering a special form of\nray tracing paired with a centroid Voronoi reparameterization. The ray\ndirection is computed by solving an Eikonal equation. With respect to the\nHadamard form of the shape derivative, both steps are within the kernel of the\nobjective and have no negative impact on the minimizer. The paper concludes\nwith applications in 2D and 3D fluid dynamics and automatic code generation and\nmanages to solve these problems without any remeshing. The methodology is\navailable as a FEniCS shape optimization add-on at\nhttp://www.mathematik.uni-wuerzburg.de/~schmidt/femorph.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.7663v1"
    },
    {
        "title": "Decomposition of Big Tensors With Low Multilinear Rank",
        "authors": [
            "Guoxu Zhou",
            "Andrzej Cichocki",
            "Shengli Xie"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Tensor decompositions are promising tools for big data analytics as they\nbring multiple modes and aspects of data to a unified framework, which allows\nus to discover complex internal structures and correlations of data.\nUnfortunately most existing approaches are not designed to meet the major\nchallenges posed by big data analytics. This paper attempts to improve the\nscalability of tensor decompositions and provides two contributions: A flexible\nand fast algorithm for the CP decomposition (FFCP) of tensors based on their\nTucker compression; A distributed randomized Tucker decomposition approach for\narbitrarily big tensors but with relatively low multilinear rank. These two\nalgorithms can deal with huge tensors, even if they are dense. Extensive\nsimulations provide empirical evidence of the validity and efficiency of the\nproposed algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.1885v2"
    },
    {
        "title": "Subspace based low rank and joint sparse matrix recovery",
        "authors": [
            "Sampurna Biswas",
            "Sunrita Poddar",
            "Soura Dasgupta",
            "Raghuraman Mudumbai",
            "Mathews Jacob"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  We consider the recovery of a low rank and jointly sparse matrix from under\nsampled measurements of its columns. This problem is highly relevant in the\nrecovery of dynamic MRI data with high spatio-temporal resolution, where each\ncolumn of the matrix corresponds to a frame in the image time series; the\nmatrix is highly low-rank since the frames are highly correlated. Similarly the\nnon-zero locations of the matrix in appropriate transform/frame domains (e.g.\nwavelet, gradient) are roughly the same in different frame. The superset of the\nsupport can be safely assumed to be jointly sparse. Unlike the classical\nmultiple measurement vector (MMV) setup that measures all the snapshots using\nthe same matrix, we consider each snapshot to be measured using a different\nmeasurement matrix. We show that this approach reduces the total number of\nmeasurements, especially when the rank of the matrix is much smaller than than\nits sparsity. Our experiments in the context of dynamic imaging shows that this\napproach is very useful in realizing free breathing cardiac MRI.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.2700v2"
    },
    {
        "title": "Towards a Broader View of Theory of Computing",
        "authors": [
            "Narendra Karmarkar"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Beginning with the projectively invariant method for linear programming,\ninterior point methods have led to powerful algorithms for many difficult\ncomputing problems, in combinatorial optimization, logic, number theory and\nnon-convex optimization. Algorithms for convex optimization benefitted from\nmany pre-established ideas from classical mathematics, but non-convex problems\nrequire new concepts. Lecture series I am presenting at the conference on\nFoundations of Computational Mathematics, 2014, outlines some of these\nconcepts{computational models based on the concept of the continuum, algorithms\ninvariant w.r.t. projective, bi-rational, and bi-holomorphic transformations on\nco-ordinate representation, extended proof systems for more efficient\ncertificates of optimality, extensions of Grassmanns extension theory,\nefficient evaluation methods for the effect of exponential number of\nconstraints, theory of connected sets based on graded connectivity, theory of\ncurved spaces adapted to the problem data, and concept of relatively algebraic\nsets in curved space. Since this conference does not have a proceedings, the\npurpose of this article is to provide the material being presented at the\nconference in more widely accessible form.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.3335v1"
    },
    {
        "title": "Numerical solution of nonstationary problems for a space-fractional\n  diffusion equation",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  An unsteady problem is considered for a space-fractional diffusion equation\nin a bounded domain. A first-order evolutionary equation containing a\nfractional power of an elliptic operator of second order is studied for general\nboundary conditions of Robin type. Finite element approximation in space is\nemployed. To construct approximation in time, regularized two-level schemes are\nused. The numerical implementation is based on solving the equation with the\nfractional power of the elliptic operator using an auxiliary Cauchy problem for\na pseudo-parabolic equation. The results of numerical experiments are presented\nfor a model two-dimensional problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.5706v1"
    },
    {
        "title": "Perona-Malik equation and its numerical properties",
        "authors": [
            "Maciek Wielgus"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This work concerns the Perona-Malik equation, which plays essential role in\nimage processing. The first part gives a survey of results on existance,\nuniqueness and stability of solutions, the second part introduces\ndiscretisations of equation and deals with an analysis of discrete problem. In\nthe last part I present some numerical results, in particular with algorithms\napplied to real images.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.6291v1"
    },
    {
        "title": "Semi-Stochastic Coordinate Descent",
        "authors": [
            "Jakub Konečný",
            "Zheng Qu",
            "Peter Richtárik"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  We propose a novel stochastic gradient method---semi-stochastic coordinate\ndescent (S2CD)---for the problem of minimizing a strongly convex function\nrepresented as the average of a large number of smooth convex functions:\n$f(x)=\\tfrac{1}{n}\\sum_i f_i(x)$. Our method first performs a deterministic\nstep (computation of the gradient of $f$ at the starting point), followed by a\nlarge number of stochastic steps. The process is repeated a few times, with the\nlast stochastic iterate becoming the new starting point where the deterministic\nstep is taken. The novelty of our method is in how the stochastic steps are\nperformed. In each such step, we pick a random function $f_i$ and a random\ncoordinate $j$---both using nonuniform distributions---and update a single\ncoordinate of the decision vector only, based on the computation of the\n$j^{th}$ partial derivative of $f_i$ at two different points. Each random step\nof the method constitutes an unbiased estimate of the gradient of $f$ and\nmoreover, the squared norm of the steps goes to zero in expectation, meaning\nthat the stochastic estimate of the gradient progressively improves. The\ncomplexity of the method is the sum of two terms: $O(n\\log(1/\\epsilon))$\nevaluations of gradients $\\nabla f_i$ and $O(\\hat{\\kappa}\\log(1/\\epsilon))$\nevaluations of partial derivatives $\\nabla_j f_i$, where $\\hat{\\kappa}$ is a\nnovel condition number.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.6293v1"
    },
    {
        "title": "Erasure coding for fault oblivious linear system solvers",
        "authors": [
            "David F. Gleich",
            "Ananth Grama",
            "Yao Zhu"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Dealing with hardware and software faults is an important problem as parallel\nand distributed systems scale to millions of processing cores and wide area\nnetworks. Traditional methods for dealing with faults include\ncheckpoint-restart, active replicas, and deterministic replay. Each of these\ntechniques has associated resource overheads and constraints. In this paper, we\npropose an alternate approach to dealing with faults, based on input\naugmentation. This approach, which is an algorithmic analog of erasure coded\nstorage, applies a minimally modified algorithm on the augmented input to\nproduce an augmented output. The execution of such an algorithm proceeds\ncompletely oblivious to faults in the system. In the event of one or more\nfaults, the real solution is recovered using a rapid reconstruction method from\nthe augmented output. We demonstrate this approach on the problem of solving\nsparse linear systems using a conjugate gradient solver. We present input\naugmentation and output recovery techniques. Through detailed experiments, we\nshow that our approach can be made oblivious to a large number of faults with\nlow computational overhead. Specifically, we demonstrate cases where a single\nfault can be corrected with less than 10% overhead in time, and even in extreme\ncases (fault rates of 20%), our approach is able to compute a solution with\nreasonable overhead. These results represent a significant improvement over the\nstate of the art.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.7364v1"
    },
    {
        "title": "Generalized quaternions and their relations with Grassmann-Clifford\n  procedure of doubling",
        "authors": [
            "Yakiv O. Kalinovsky",
            "Yuliya E. Boyarinova",
            "Alina S. Turenko",
            "Yana V. Khitsko"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  The class of non-commutative hypercomplex number systems (HNS) of\n4-dimension, constructed by using of non-commutative Grassmann-Clifford\nprocedure of doubling of 2-dimensional systems is investigated in the article\nand established here are their relationships with the generalized quaternions.\nAlgorithms of performance of operations and methods of algebraic\ncharacteristics calculation in them, such as conjugation, normalization, a type\nof zero divisors are investigated. The considered arithmetic and algebraic\noperations and procedures in this class HNS allow to use these HNS in\nmathematical modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.8185v1"
    },
    {
        "title": "Close Approximations for Daublets and their Spectra",
        "authors": [
            "V. V. Vermehren",
            "J. E. Wesen",
            "H. M. de Oliveira"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This paper offers a new regard on compactly supported wavelets derived from\nFIR filters. Although being continuous wavelets, analytical formulation are\nlacking for such wavelets. Close approximations for daublets (Daubechies\nwavelets) and their spectra are introduced here. The frequency detection\nproperties of daublets are investigated through scalograms derived from these\nnew analytical expressions. These near-daublets have been implemented on the\nMatlab wavelet toolbox and a few scalograms presented. This approach can be\nvaluable for wavelet synthesis from hardware or for application involving\ncontinuous wavelet-based systems, such as wavelet OFDM.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.01424v1"
    },
    {
        "title": "A comparison of the Extrapolated Successive Overrelaxation and the\n  Preconditioned Simultaneous Displacement methods for augmented linear systems",
        "authors": [
            "M. A. Louka",
            "N. M. Missirlis"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this paper we study the impact of two types of preconditioning on the\nnumerical solution of large sparse augmented linear systems. The first\npreconditioning matrix is the lower triangular part whereas the second is the\nproduct of the lower triangular part with the upper triangular part of the\naugmented system's coefficient matrix. For the first preconditioning matrix we\nform the Generalized Modified Extrapolated Successive Overrelaxation (GMESOR)\nmethod, whereas the second preconditioning matrix yields the Generalized\nModified Preconditioned Simultaneous Displacement (GMPSD) method, which is an\nextrapolated form of the Symmetric Successive Overrelaxation method. We find\nsufficient conditions for each aforementioned iterative method to converge. In\naddition, we develop a geometric approach, for determining the optimum values\nof their parameters and corresponding spectral radii. It is shown that both\niterative methods studied (GMESOR and GMPSD) attain the same rate of\nconvergence. Numerical results confirm our theoretical expectations.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.02280v1"
    },
    {
        "title": "Primal Dual Affine Scaling on GPUs",
        "authors": [
            "Nithish Divakar"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Here we present an implementation of Primal-Dual Affine scaling method to\nsolve linear optimization problem on GPU based systems. Strategies to convert\nthe system generated by complementary slackness theorem into a symmetric system\nare given. A new CUDA friendly technique to solve the resulting symmetric\npositive definite subsystem is also developed. Various strategies to reduce the\nmemory transfer and storage requirements were also explored.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.03543v1"
    },
    {
        "title": "On mesh restrictions to satisfy comparison principles, maximum\n  principles, and the non-negative constraint: Recent developments and new\n  results",
        "authors": [
            "M. K. Mudunuru",
            "K. B. Nakshatrala"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This paper concerns with mesh restrictions that are needed to satisfy several\nimportant mathematical properties -- maximum principles, comparison principles,\nand the non-negative constraint -- for a general linear second-order elliptic\npartial differential equation. We critically review some recent developments in\nthe field of discrete maximum principles, derive new results, and discuss some\npossible future research directions in this area. In particular, we derive\nrestrictions for a three-node triangular (T3) element and a four-node\nquadrilateral (Q4) element to satisfy comparison principles, maximum\nprinciples, and the non-negative constraint under the standard single-field\nGalerkin formulation. Analysis is restricted to uniformly elliptic linear\ndifferential operators in divergence form with Dirichlet boundary conditions\nspecified on the entire boundary of the domain. Various versions of maximum\nprinciples and comparison principles are discussed in both continuous and\ndiscrete settings. In the literature, it is well-known that an acute-angled\ntriangle is sufficient to satisfy the discrete weak maximum principle for pure\nisotropic diffusion. An iterative algorithm is developed to construct\nsimplicial meshes that preserves discrete maximum principles using existing\nopen source mesh generators. Various numerical examples based on different\ntypes of triangulations are presented to show the pros and cons of placing\nrestrictions on a computational mesh. We also quantify local and global mass\nconservation errors using representative numerical examples, and illustrate the\nperformance of metric-based meshes with respect to mass conservation.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.06164v1"
    },
    {
        "title": "A Comparative Analysis of Tensor Decomposition Models Using Hyper\n  Spectral Image",
        "authors": [
            "Ankit Gupta",
            "Ashish Oberoi"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Hyper spectral imaging is a remote sensing technology, providing variety of\napplications such as material identification, space object identification,\nplanetary exploitation etc. It deals with capturing continuum of images of the\nearth surface from different angles. Due to the multidimensional nature of the\nimage, multi-way arrays are one of the possible solutions for analyzing hyper\nspectral data. This multi-way array is called tensor. Our approach deals with\nimplementing three decomposition models LMLRA, BTD and CPD to the sample data\nfor choosing the best decomposition of the data set. The results have proved\nthat Block Term Decomposition (BTD) is the best tensor model for decomposing\nthe hyper spectral image in to resultant factor matrices.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.06561v1"
    },
    {
        "title": "A Finite Element Based P3M Method for N-body Problems",
        "authors": [
            "Natalie N. Beams",
            "Luke N. Olson",
            "Jonathan B. Freund"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We introduce a fast mesh-based method for computing N-body interactions that\nis both scalable and accurate. The method is founded on a\nparticle-particle--particle-mesh P3M approach, which decomposes a potential\ninto rapidly decaying short-range interactions and smooth, mesh-resolvable\nlong-range interactions. However, in contrast to the traditional approach of\nusing Gaussian screen functions to accomplish this decomposition, our method\nemploys specially designed polynomial bases to construct the screened\npotentials. Because of this form of the screen, the long-range component of the\npotential is then solved exactly with a finite element method, leading\nultimately to a sparse matrix problem that is solved efficiently with standard\nmultigrid methods. Moreover, since this system represents an exact\ndiscretization, the optimal resolution properties of the FFT are unnecessary,\nthough the short-range calculation is now more involved than P3M/PME methods.\nWe introduce the method, analyze its key properties, and demonstrate the\naccuracy of the algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08509v1"
    },
    {
        "title": "Finding a low-rank basis in a matrix subspace",
        "authors": [
            "Yuji Nakatsukasa",
            "Tasuku Soma",
            "André Uschmajew"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  For a given matrix subspace, how can we find a basis that consists of\nlow-rank matrices? This is a generalization of the sparse vector problem. It\nturns out that when the subspace is spanned by rank-1 matrices, the matrices\ncan be obtained by the tensor CP decomposition. For the higher rank case, the\nsituation is not as straightforward. In this work we present an algorithm based\non a greedy process applicable to higher rank problems. Our algorithm first\nestimates the minimum rank by applying soft singular value thresholding to a\nnuclear norm relaxation, and then computes a matrix with that rank using the\nmethod of alternating projections. We provide local convergence results, and\ncompare our algorithm with several alternative approaches. Applications include\ndata compression beyond the classical truncated SVD, computing accurate\neigenvectors of a near-multiple eigenvalue, image separation and graph\nLaplacian eigenproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08601v2"
    },
    {
        "title": "The Discretely-Discontinuous Galerkin Coarse Grid for Domain\n  Decomposition",
        "authors": [
            "Essex Edwards",
            "Robert Bridson"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We present an algebraic method for constructing a highly effective coarse\ngrid correction to accelerate domain decomposition. The coarse problem is\nconstructed from the original matrix and a small set of input vectors that span\na low-degree polynomial space, but no further knowledge of meshes or continuous\nfunctionals is used. We construct a coarse basis by partitioning the problem\ninto subdomains and using the restriction of each input vector to each\nsubdomain as its own basis function. This basis resembles a Discontinuous\nGalerkin basis on subdomain-sized elements. Constructing the coarse problem by\nGalerkin projection, we prove a high-order convergent error bound for the\ncoarse solutions. Used in a two-level symmetric multiplicative overlapping\nSchwarz preconditioner, the resulting conjugate gradient solver shows optimal\nscaling. Convergence requires a constant number of iterations, independent of\nfine problem size, on a range of scalar and vector-valued second-order and\nfourth-order PDEs.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.00907v1"
    },
    {
        "title": "Multi-Block ADMM for Big Data Optimization in Modern Communication\n  Networks",
        "authors": [
            "Lanchao Liu",
            "Zhu Han"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this paper, we review the parallel and distributed optimization algorithms\nbased on the alternating direction method of multipliers (ADMM) for solving\n\"big data\" optimization problems in modern communication networks. We first\nintroduce the canonical formulation of the large-scale optimization problem.\nNext, we describe the general form of ADMM and then focus on several direct\nextensions and sophisticated modifications of ADMM from $2$-block to $N$-block\nsettings to deal with the optimization problem. The iterative schemes and\nconvergence properties of each extension/modification are given, and the\nimplementation on large-scale computing facilities is also illustrated.\nFinally, we numerate several applications in communication networks, such as\nthe security constrained optimal power flow problem in smart grid networks and\nmobile data offloading problem in software defined networks (SDNs).\n",
        "pdf_link": "http://arxiv.org/pdf/1504.01809v1"
    },
    {
        "title": "Galerkin v. least-squares Petrov--Galerkin projection in nonlinear model\n  reduction",
        "authors": [
            "Kevin Carlberg",
            "Matthew Barone",
            "Harbir Antil"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Least-squares Petrov--Galerkin (LSPG) model-reduction techniques such as the\nGauss--Newton with Approximated Tensors (GNAT) method have shown promise, as\nthey have generated stable, accurate solutions for large-scale turbulent,\ncompressible flow problems where standard Galerkin techniques have failed.\nHowever, there has been limited comparative analysis of the two approaches.\nThis is due in part to difficulties arising from the fact that Galerkin\ntechniques perform optimal projection associated with residual minimization at\nthe time-continuous level, while LSPG techniques do so at the time-discrete\nlevel. This work provides a detailed theoretical and computational comparison\nof the two techniques for two common classes of time integrators: linear\nmultistep schemes and Runge--Kutta schemes. We present a number of new\nfindings, including conditions under which the LSPG ROM has a time-continuous\nrepresentation, conditions under which the two techniques are equivalent, and\ntime-discrete error bounds for the two approaches. Perhaps most surprisingly,\nwe demonstrate both theoretically and computationally that decreasing the time\nstep does not necessarily decrease the error for the LSPG ROM; instead, the\ntime step should be `matched' to the spectral content of the reduced basis. In\nnumerical experiments carried out on a turbulent compressible-flow problem with\nover one million unknowns, we show that increasing the time step to an\nintermediate value decreases both the error and the simulation time of the LSPG\nreduced-order model by an order of magnitude.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.03749v3"
    },
    {
        "title": "Factorized schemes of second-order accuracy for numerical solving\n  unsteady problems",
        "authors": [
            "P. N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Schemes with the second-order approximation in time are considered for\nnumerical solving the Cauchy problem for an evolutionary equation of first\norder with a self-adjoint operator. The implicit two-level scheme based on the\nPad\\'{e} polynomial approximation is unconditionally stable. It demonstrates\ngood asymptotic properties in time and provides an adequate evolution in time\nfor individual harmonics of the solution (has spectral mimetic stability). In\nfact, the only drawback of this scheme is the necessity to solve an equation\nwith an operator polynomial of second degree at each time level. We consider\nmodifications of these schemes, which are based on solving equations with\noperator polynomials of first degree. Such computational implementations occur,\nfor example, if we apply the fully implicit two-level scheme (the backward\nEuler scheme). A three-level modification of the SM-stable scheme is proposed.\nIts unconditional stability is established in the corresponding norms. The\nemphasis is on the scheme, where the numerical algorithm involves two stages,\nnamely, the backward Euler scheme of first order at the first (prediction)\nstage and the following correction of the approximate solution using a\nfactorized operator. The SM-stability is established for the proposed scheme.\nTo illustrate the theoretical results of the work, a model problem is solved\nnumerically.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.04179v1"
    },
    {
        "title": "Convergence of a finite difference scheme to weak solutions of the\n  system of partial differential equation arising in mean field games",
        "authors": [
            "Yves Achdou",
            "Alessio Porretta"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Mean field type models describing the limiting behavior of stochastic\ndifferential games as the number of players tends to +$\\infty$, have been\nrecently introduced by J-M. Lasry and P-L. Lions. Under suitable assumptions,\nthey lead to a system of two coupled partial differential equations, a forward\nBellman equation and a backward Fokker-Planck equations. Finite difference\nschemes for the approximation of such systems have been proposed in previous\nworks. Here, we prove the convergence of these schemes towards a weak solution\nof the system of partial differential equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.05705v1"
    },
    {
        "title": "On the Global Convergence of Majorization Minimization Algorithms for\n  Nonconvex Optimization Problems",
        "authors": [
            "Yangyang Kang",
            "Zhihua Zhang",
            "Wu-Jun Li"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this paper, we study the global convergence of majorization minimization\n(MM) algorithms for solving nonconvex regularized optimization problems. MM\nalgorithms have received great attention in machine learning. However, when\napplied to nonconvex optimization problems, the convergence of MM algorithms is\na challenging issue. We introduce theory of the Kurdyka- Lojasiewicz inequality\nto address this issue. In particular, we show that many nonconvex problems\nenjoy the Kurdyka- Lojasiewicz property and establish the global convergence\nresult of the corresponding MM procedure. We also extend our result to a well\nknown method that called CCCP (concave-convex procedure).\n",
        "pdf_link": "http://arxiv.org/pdf/1504.07791v2"
    },
    {
        "title": "Numerical investigation of a space-fractional model of turbulent fluid\n  flow in rectangular ducts",
        "authors": [
            "Alexander G. Churbanov",
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  The models that are based of fractional derivatives should be highlighted\namong promising new models to describe turbulent fluid flows. In the present\nwork, a steady-state flow in a duct is considered under the condition that the\nturbulent diffusion is governed by a fractional power of the Laplace operator.\nTo study numerically flows in rectangular channels, finite-difference\napproximations are employed. For approximate solving the corresponding boundary\nvalue problem, the iterative method of conjugate gradients is used. At each\niteration, the problem with a fractional power of the grid Laplace operator is\nsolved. Predictions of turbulent flows in ducts at different Reynolds numbers\nare presented via mean velocity fields.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.01519v1"
    },
    {
        "title": "A splitting scheme to solve an equation for fractional powers of\n  elliptic operators",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  An equation containing a fractional power of an elliptic operator of second\norder is studied for Dirichlet boundary conditions. Finite difference\napproximations in space are employed. The proposed numerical algorithm is based\non solving an auxiliary Cauchy problem for a pseudo-parabolic equation.\nUnconditionally stable vector additive schemes (splitting schemes) are\nconstructed. Numerical results for a model problem in a rectangle calculated\nusing the splitting with respect to spatial variables are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04103v1"
    },
    {
        "title": "Schur Complement based domain decomposition preconditioners with\n  Low-rank corrections",
        "authors": [
            "Ruipeng Li",
            "Yuanzhe Xi",
            "Yousef Saad"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This paper introduces a robust preconditioner for general sparse symmetric\nmatrices, that is based on low-rank approximations of the Schur complement in a\nDomain Decomposition (DD) framework. In this \"Schur Low Rank\" (SLR)\npreconditioning approach, the coefficient matrix is first decoupled by DD, and\nthen a low-rank correction is exploited to compute an approximate inverse of\nthe Schur complement associated with the interface points. The method avoids\nexplicit formation of the Schur complement matrix. We show the feasibility of\nthis strategy for a model problem, and conduct a detailed spectral analysis for\nthe relationship between the low-rank correction and the quality of the\npreconditioning. Numerical experiments on general matrices illustrate the\nrobustness and efficiency of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04340v1"
    },
    {
        "title": "Low-rank correction methods for algebraic domain decomposition\n  preconditioners",
        "authors": [
            "Ruipeng Li",
            "Yousef Saad"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This paper presents a parallel preconditioning method for distributed sparse\nlinear systems, based on an approximate inverse of the original matrix, that\nadopts a general framework of distributed sparse matrices and exploits the\ndomain decomposition method and low-rank corrections. The domain decomposition\napproach decouples the matrix and once inverted, a low-rank approximation is\napplied by exploiting the Sherman-Morrison-Woodbury formula, which yields two\nvariants of the preconditioning methods. The low-rank expansion is computed by\nthe Lanczos procedure with reorthogonalizations. Numerical experiments indicate\nthat, when combined with Krylov subspace accelerators, this preconditioner can\nbe efficient and robust for solving symmetric sparse linear systems.\nComparisons with other distributed-memory preconditioning methods are\npresented.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04341v2"
    },
    {
        "title": "A Time-parallel Approach to Strong-constraint Four-dimensional\n  Variational Data Assimilation",
        "authors": [
            "Vishwas Rao",
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  A parallel-in-time algorithm based on an augmented Lagrangian approach is\nproposed to solve four-dimensional variational (4D-Var) data assimilation\nproblems. The assimilation window is divided into multiple sub-intervals that\nallows to parallelize cost function and gradient computations. Solution\ncontinuity equations across interval boundaries are added as constraints. The\naugmented Lagrangian approach leads to a different formulation of the\nvariational data assimilation problem than weakly constrained 4D-Var. A\ncombination of serial and parallel 4D-Vars to increase performance is also\nexplored. The methodology is illustrated on data assimilation problems with\nLorenz-96 and the shallow water models.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04515v1"
    },
    {
        "title": "A Hybrid Monte-Carlo Sampling Smoother for Four Dimensional Data\n  Assimilation",
        "authors": [
            "Ahmed Attia",
            "Vishwas Rao",
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This paper constructs an ensemble-based sampling smoother for\nfour-dimensional data assimilation using a Hybrid/Hamiltonian Monte-Carlo\napproach. The smoother samples efficiently from the posterior probability\ndensity of the solution at the initial time. Unlike the well-known ensemble\nKalman smoother, which is optimal only in the linear Gaussian case, the\nproposed methodology naturally accommodates non-Gaussian errors and non-linear\nmodel dynamics and observation operators. Unlike the four-dimensional\nvariational met\\-hod, which only finds a mode of the posterior distribution,\nthe smoother provides an estimate of the posterior uncertainty. One can use the\nensemble mean as the minimum variance estimate of the state, or can use the\nensemble in conjunction with the variational approach to estimate the\nbackground errors for subsequent assimilation windows. Numerical results\ndemonstrate the advantages of the proposed method compared to the traditional\nvariational and ensemble-based smoothing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04724v1"
    },
    {
        "title": "Centralized and Distributed Newton Methods for Network Optimization and\n  Extensions",
        "authors": [
            "Dimitri P. Bertsekas"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We consider Newton methods for common types of single commodity and\nmulti-commodity network flow problems. Despite the potentially very large\ndimension of the problem, they can be implemented using the conjugate gradient\nmethod and low-dimensional network operations, as shown nearly thirty years\nago. We revisit these methods, compare them to more recent proposals, and\ndescribe how they can be implemented in a distributed computing system. We also\ndiscuss generalizations, including the treatment of arc gains, linear side\nconstraints, and related special structures.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.00702v1"
    },
    {
        "title": "A quadrilateral 'mini' finite element for the Stokes problem using a\n  single bubble function",
        "authors": [
            "Bishnu P. Lamichhane"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We consider a quadrilateral 'mini' finite element for approximating the\nsolution of Stokes equations using a quadrilateral mesh. We use the standard\nbilinear finite element space enriched with element-wise defined bubble\nfunctions for the velocity and the standard bilinear finite element space for\nthe pressure space. With a simple modification of the standard bubble function\nwe show that a single bubble function is sufficient to ensure the inf-sup\ncondition. We have thus improved an earlier result on the quadrilateral 'mini'\nelement, where more than one bubble function are used to get the stability.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04417v1"
    },
    {
        "title": "On Dual-Finite Volume Methods for Extended Porous Medium Equations",
        "authors": [
            "Hidekazu Yoshioka"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This article shows that the unconditional stability of the Dual-Finite Volume\nMethod, which is at least valid for linear problems, is not true for generic\nnonlinear differential equations including the PMEs unless the coefficient\nappearing in the numerical fluxes are appropriately evaluated. This article\nprovides a theoretically truly isotone numerical fluxes specialized for solving\nthe PMEs presented, which is still as simple as the conventional fully-upwind\ncounterpart.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.05281v1"
    },
    {
        "title": "An Efficient Solver for Sparse Linear Systems Based on Rank-Structured\n  Cholesky Factorization",
        "authors": [
            "Jeffrey N. Chadwick",
            "David S. Bindel"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Direct factorization methods for the solution of large, sparse linear systems\nthat arise from PDE discretizations are robust, but typically show poor time\nand memory scalability for large systems. In this paper, we describe an\nefficient sparse, rank-structured Cholesky algorithm for solution of the\npositive definite linear system $A x = b$ when $A$ comes from a discretized\npartial-differential equation. Our approach combines the efficient memory\naccess patterns of conventional supernodal Cholesky algorithms with the memory\nefficiency of rank-structured direct solvers. For several test problems arising\nfrom PDE discretizations, our method takes less memory than standard sparse\nCholesky solvers and less wall-clock time than standard preconditioned\niterations.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.05593v1"
    },
    {
        "title": "A $N$-Body Solver for Square Root Iteration",
        "authors": [
            "Matt Challacombe",
            "Terry Haut",
            "Nicolas Bock"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We develop the Sparse Approximate Matrix Multiply ($\\tt SpAMM$) $n$-body\nsolver for first order Newton Schulz iteration of the matrix square root and\ninverse square root. The solver performs recursive two-sided metric queries on\na modified Cauchy-Schwarz criterion, culling negligible sub-volumes of the\nproduct-tensor for problems with structured decay in the sub-space metric.\nThese sub-structures are shown to bound the relative error in the matrix-matrix\nproduct, and in favorable cases, to enjoy a reduced computational complexity\ngoverned by dimensionality reduction of the product volume. A main contribution\nis demonstration of a new, algebraic locality that develops under contractive\nidentity iteration, with collapse of the metric-subspace onto the identity's\nplane diagonal, resulting in a stronger $\\tt SpAMM$ bound. Also, we carry out a\nfirst order {Fr\\'{e}chet} analyses for single and dual channel instances of the\nsquare root iteration, and look at bifurcations due to ill-conditioning and a\ntoo aggressive $\\tt SpAMM$ approximation. Then, we show that extreme $\\tt\nSpAMM$ approximation and contractive identity iteration can be achieved for\nill-conditioned systems through regularization, and we demonstrate the\npotential for acceleration with a scoping, product representation of the\ninverse factor.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05856v2"
    },
    {
        "title": "Stochastic Behavior of the Nonnegative Least Mean Fourth Algorithm for\n  Stationary Gaussian Inputs and Slow Learning",
        "authors": [
            "Jingen Ni",
            "Jian Yang",
            "Jie Chen",
            "Cédric Richard",
            "José Carlos M. Bermudez"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Some system identification problems impose nonnegativity constraints on the\nparameters to estimate due to inherent physical characteristics of the unknown\nsystem. The nonnegative least-mean-square (NNLMS) algorithm and its variants\nallow to address this problem in an online manner. A nonnegative least mean\nfourth (NNLMF) algorithm has been recently proposed to improve the performance\nof these algorithms in cases where the measurement noise is not Gaussian. This\npaper provides a first theoretical analysis of the stochastic behavior of the\nNNLMF algorithm for stationary Gaussian inputs and slow learning. Simulation\nresults illustrate the accuracy of the proposed analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05873v1"
    },
    {
        "title": "Two-level space-time domain decomposition methods for unsteady inverse\n  problems",
        "authors": [
            "Xiaomao Deng",
            "Xiao-chuan Cai",
            "Jun Zou"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  As the number of processor cores on supercomputers becomes larger and larger,\nalgorithms with high degree of parallelism attract more attention. In this\nwork, we propose a novel space-time coupled algorithm for solving an inverse\nproblem associated with the time-dependent convection-diffusion equation in\nthree dimensions. We introduce a mixed finite element/finite difference method\nand a one-level and a two-level space-time parallel domain decomposition\npreconditioner for the Karush-Kuhn-Tucker (KKT) system induced from\nreformulating the inverse problem as an output least-squares optimization\nproblem in the space-time domain. The new full space approach eliminates the\nsequential steps of the optimization outer loop and the inner forward and\nbackward time marching processes, thus achieves high degree of parallelism.\nNumerical experiments validate that this approach is effective and robust for\nrecovering unsteady moving sources. We report strong scalability results\nobtained on a supercomputer with more than 1,000 processors.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.06103v1"
    },
    {
        "title": "Improved Analyses of the Randomized Power Method and Block Lanczos\n  Method",
        "authors": [
            "Shusen Wang",
            "Zhihua Zhang",
            "Tong Zhang"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  The power method and block Lanczos method are popular numerical algorithms\nfor computing the truncated singular value decomposition (SVD) and eigenvalue\ndecomposition problems. Especially in the literature of randomized numerical\nlinear algebra, the power method is widely applied to improve the quality of\nrandomized sketching, and relative-error bounds have been well established.\nRecently, Musco & Musco (2015) proposed a block Krylov subspace method that\nfully exploits the intermediate results of the power iteration to accelerate\nconvergence. They showed spectral gap-independent bounds which are stronger\nthan the power method by order-of-magnitude. This paper offers novel error\nanalysis techniques and significantly improves the bounds of both the\nrandomized power method and the block Lanczos method. This paper also\nestablishes the first gap-independent bound for the warm-start block Lanczos\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.06429v2"
    },
    {
        "title": "Rational Chebyshev of Second Kind Collocation Method for Solving a Class\n  of Astrophysics Problems",
        "authors": [
            "Kourosh Parand",
            "Sajjad Khaleqi"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  The Lane-Emden equation has been used to model several phenomenas in\ntheoretical physics, mathematical physics and astrophysics such as the theory\nof stellar structure. This study is an attempt to utilize the collocation\nmethod with the Rational Chebyshev of Second Kind function (RSC) to solve the\nLane-Emden equation over the semi-infinit interval [0; +infinity). According to\nwell-known results and comparing with previous methods, it can be said that\nthis method is efficient and applicable.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.07240v1"
    },
    {
        "title": "Fast Algorithms for the computation of Fourier Extensions of arbitrary\n  length",
        "authors": [
            "Roel Matthysen",
            "Daan Huybrechs"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Fourier series of smooth, non-periodic functions on $[-1,1]$ are known to\nexhibit the Gibbs phenomenon, and exhibit overall slow convergence. One way of\novercoming these problems is by using a Fourier series on a larger domain, say\n$[-T,T]$ with $T>1$, a technique called Fourier extension or Fourier\ncontinuation. When constructed as the discrete least squares minimizer in\nequidistant points, the Fourier extension has been shown shown to converge\ngeometrically in the truncation parameter $N$. A fast ${\\mathcal O}(N \\log^2\nN)$ algorithm has been described to compute Fourier extensions for the case\nwhere $T=2$, compared to ${\\mathcal O}(N^3)$ for solving the dense discrete\nleast squares problem. We present two ${\\mathcal O}(N\\log^2 N )$ algorithms for\nthe computation of these approximations for the case of general $T$, made\npossible by exploiting the connection between Fourier extensions and Prolate\nSpheroidal Wave theory. The first algorithm is based on the explicit\ncomputation of so-called periodic discrete prolate spheroidal sequences, while\nthe second algorithm is purely algebraic and only implicitly based on the\ntheory.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.00206v1"
    },
    {
        "title": "Exact Real Arithmetic with Perturbation Analysis and Proof of\n  Correctness",
        "authors": [
            "Sarmen Keshishzadeh",
            "Jan Friso Groote"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this article, we consider a simple representation for real numbers and\npropose top-down procedures to approximate various algebraic and transcendental\noperations with arbitrary precision. Detailed algorithms and proofs are\nprovided to guarantee the correctness of the approximations. Moreover, we\ndevelop and apply a perturbation analysis method to show that our approximation\nprocedures only recompute expressions when unavoidable.\n  In the last decade, various theories have been developed and implemented to\nrealize real computations with arbitrary precision. Proof of correctness for\nexisting approaches typically consider basic algebraic operations, whereas\ndetailed arguments about transcendental operations are not available. Another\nimportant observation is that in each approach some expressions might require\niterative computations to guarantee the desired precision. However, no formal\nreasoning is provided to prove that such iterative calculations are essential\nin the approximation procedures. In our approximations of real functions, we\nexplicitly relate the precision of the inputs to the guaranteed precision of\nthe output, provide full proofs and a precise analysis of the necessity of\niterations.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.06265v1"
    },
    {
        "title": "On the geometry of border rank algorithms for n x 2 by 2 x 2 matrix\n  multiplication",
        "authors": [
            "J. M. Landsberg",
            "Nicholas Ryder"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We make an in-depth study of the known border rank (i.e. approximate)\nalgorithms for the matrix multiplication tensor encoding the multiplication of\nan n x 2 matrix by a 2 x 2 matrix.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.08323v1"
    },
    {
        "title": "Parallel Tensor Compression for Large-Scale Scientific Data",
        "authors": [
            "Woody Austin",
            "Grey Ballard",
            "Tamara G. Kolda"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  As parallel computing trends towards the exascale, scientific data produced\nby high-fidelity simulations are growing increasingly massive. For instance, a\nsimulation on a three-dimensional spatial grid with 512 points per dimension\nthat tracks 64 variables per grid point for 128 time steps yields 8~TB of data,\nassuming double precision. By viewing the data as a dense five-way tensor, we\ncan compute a Tucker decomposition to find inherent low-dimensional multilinear\nstructure, achieving compression ratios of up to 5000 on real-world data sets\nwith negligible loss in accuracy. So that we can operate on such massive data,\nwe present the first-ever distributed-memory parallel implementation for the\nTucker decomposition, whose key computations correspond to parallel linear\nalgebra operations, albeit with nonstandard data layouts. Our approach\nspecifies a data distribution for tensors that avoids any tensor data\nredistribution, either locally or in parallel. We provide accompanying analysis\nof the computation and communication costs of the algorithms. To demonstrate\nthe compression and accuracy of the method, we apply our approach to real-world\ndata sets from combustion science simulations. We also provide detailed\nperformance results, including parallel performance in both weak and strong\nscaling experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.06689v2"
    },
    {
        "title": "An Efficient, Sparsity-Preserving, Online Algorithm for Low-Rank\n  Approximation",
        "authors": [
            "David G. Anderson",
            "Ming Gu"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Low-rank matrix approximation is a fundamental tool in data analysis for\nprocessing large datasets, reducing noise, and finding important signals. In\nthis work, we present a novel truncated LU factorization called\nSpectrum-Revealing LU (SRLU) for effective low-rank matrix approximation, and\ndevelop a fast algorithm to compute an SRLU factorization. We provide both\nmatrix and singular value approximation error bounds for the SRLU approximation\ncomputed by our algorithm. Our analysis suggests that SRLU is competitive with\nthe best low-rank matrix approximation methods, deterministic or randomized, in\nboth computational complexity and approximation quality. Numeric experiments\nillustrate that SRLU preserves sparsity, highlights important data features and\nvariables, can be efficiently updated, and calculates data approximations\nnearly as accurately as possible. To the best of our knowledge this is the\nfirst practical variant of the LU factorization for effective and efficient\nlow-rank matrix approximation.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05950v2"
    },
    {
        "title": "Lipschitz Continuity of Mahalanobis Distances and Bilinear Forms",
        "authors": [
            "Valentina Zantedeschi",
            "Rémi Emonet",
            "Marc Sebban"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Many theoretical results in the machine learning domain stand only for\nfunctions that are Lipschitz continuous. Lipschitz continuity is a strong form\nof continuity that linearly bounds the variations of a function. In this paper,\nwe derive tight Lipschitz constants for two families of metrics: Mahalanobis\ndistances and bounded-space bilinear forms. To our knowledge, this is the first\ntime the Mahalanobis distance is formally proved to be Lipschitz continuous and\nthat such tight Lipschitz constants are derived.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01376v1"
    },
    {
        "title": "A Singularly Perturbed Boundary Value Problems with Fractional Powers of\n  Elliptic Operators",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  A boundary value problem for a fractional power $0 < \\varepsilon < 1$ of the\nsecond-order elliptic operator is considered. The boundary value problem is\nsingularly perturbed when $\\varepsilon \\rightarrow 0$. It is solved numerically\nusing a time-dependent problem for a pseudo-parabolic equation. For the\nauxiliary Cauchy problem, the standard two-level schemes with weights are\napplied. The numerical results are presented for a model two-dimen\\-sional\nboundary value problem with a fractional power of an elliptic operator. Our\nwork focuses on the solution of the boundary value problem with $0 <\n\\varepsilon \\ll 1$.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.04427v1"
    },
    {
        "title": "Iterative computational identification of a spacewise dependent the\n  source in a parabolic equations",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Coefficient inverse problems related to identifying the right-hand side of an\nequation with use of additional information is of interest among inverse\nproblems for partial differential equations. When considering non-stationary\nproblems, tasks of recovering the dependence of the right-hand side on time and\nspatial variables can be treated as independent. These tasks relate to a class\nof linear inverse problems, which sufficiently simplifies their study. This\nwork is devoted to a finding the dependence of right-hand side of\nmultidimensional parabolic equation on spatial variables using additional\nobservations of the solution at the final point of time - the final\noverdetermination. More general problems are associated with some integral\nobservation of the solution on time - the integral overdetermination. The first\nmethod of numerical solution of inverse problems is based on iterative solution\nof boundary value problem for time derivative with non-local acceleration. The\nsecond method is based on the known approach with iterative refinement of\ndesired dependence of the right-hand side on spacial variables. Capabilities of\nproposed methods are illustrated by numerical examples for model\ntwo-dimensional problem of identifying the right-hand side of a parabolic\nequation. The standard finite-element approximation on space is used, the time\ndiscretization is based on fully implicit two-level schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.04443v1"
    },
    {
        "title": "Profile-Driven Automated Mixed Precision",
        "authors": [
            "Ralph Nathan",
            "Helia Naeimi",
            "Daniel J. Sorin",
            "Xiaobai Sun"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We present a scheme to automatically set the precision of floating point\nvariables in an application. We design a framework that profiles applications\nto measure undesirable numerical behavior at the floating point operation\nlevel. We use this framework to perform mixed precision analysis to\nheuristically set the precision of all variables in an application based on\ntheir numerical profiles. We experimentally evaluate the mixed precision\nanalysis to show that it can generate a range of results with different\naccuracy and performance characteristics.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00251v1"
    },
    {
        "title": "A critical analysis of some popular methods for the discretisation of\n  the gradient operator in finite volume methods",
        "authors": [
            "Alexandros Syrakos",
            "Stylianos Varchanis",
            "Yannis Dimakopoulos",
            "Apostolos Goulas",
            "John Tsamopoulos"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Finite volume methods (FVMs) constitute a popular class of methods for the\nnumerical simulation of fluid flows. Among the various components of these\nmethods, the discretisation of the gradient operator has received less\nattention despite its fundamental importance with regards to the accuracy of\nthe FVM. The most popular gradient schemes are the divergence theorem (DT) (or\nGreen-Gauss) scheme, and the least-squares (LS) scheme. Both are widely\nbelieved to be second-order accurate, but the present study shows that in fact\nthe common variant of the DT gradient is second-order accurate only on\nstructured meshes whereas it is zeroth-order accurate on general unstructured\nmeshes, and the LS gradient is second-order and first-order accurate,\nrespectively. This is explained through a theoretical analysis and is confirmed\nby numerical tests. The schemes are then used within a FVM to solve a simple\ndiffusion equation on unstructured grids generated by several methods; the\nresults reveal that the zeroth-order accuracy of the DT gradient is inherited\nby the FVM as a whole, and the discretisation error does not decrease with grid\nrefinement. On the other hand, use of the LS gradient leads to second-order\naccurate results, as does the use of alternative, consistent, DT gradient\nschemes, including a new iterative scheme that makes the common DT gradient\nconsistent at almost no extra cost. The numerical tests are performed using\nboth an in-house code and the popular public domain PDE solver OpenFOAM.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.05556v6"
    },
    {
        "title": "On the non-uniqueness of the instantaneous frequency",
        "authors": [
            "Peyman Tavallali",
            "Thomas Y. Hou"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In this article, we investigate the debated Instantaneous Frequency (IF)\ntopic. Here, we show that IF is non-unique inherently. We explain how this\nnon-uniqueness can be quantified and explained from a mathematical perspective.\nThe non-uniqueness of the IF can also be observed if different methods of\nadaptive signal processing are used. We will also show that even if we know the\nphysical origin of an oscillatory signal, e.g. linear second order ordinary\ndifferential equation, the non-uniqueness is still present. All in all, we will\nend up with the conclusion that, without any a priori assumption about the\nrelationship of the envelope and phase function of an oscillatory signal, there\nis not any preferred neither best representation of the IF of such oscillatory\nsignal.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02548v2"
    },
    {
        "title": "Inexact Block Coordinate Descent Methods For Symmetric Nonnegative\n  Matrix Factorization",
        "authors": [
            "Qingjiang Shi",
            "Haoran Sun",
            "Songtao Lu",
            "Mingyi Hong",
            "Meisam Razaviyayn"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Symmetric nonnegative matrix factorization (SNMF) is equivalent to computing\na symmetric nonnegative low rank approximation of a data similarity matrix. It\ninherits the good data interpretability of the well-known nonnegative matrix\nfactorization technique and have better ability of clustering nonlinearly\nseparable data. In this paper, we focus on the algorithmic aspect of the SNMF\nproblem and propose simple inexact block coordinate decent methods to address\nthe problem, leading to both serial and parallel algorithms. The proposed\nalgorithms have guaranteed stationary convergence and can efficiently handle\nlarge-scale and/or sparse SNMF problems. Extensive simulations verify the\neffectiveness of the proposed algorithms compared to recent state-of-the-art\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.03092v1"
    },
    {
        "title": "Higher-Order Block Term Decomposition for Spatially Folded fMRI Data",
        "authors": [
            "Christos Chatzichristos",
            "Eleftherios Kofidis",
            "Giannis Kopsinis",
            "Sergios Theodoridis"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  The growing use of neuroimaging technologies generates a massive amount of\nbiomedical data that exhibit high dimensionality. Tensor-based analysis of\nbrain imaging data has been proved quite effective in exploiting their multiway\nnature. The advantages of tensorial methods over matrix-based approaches have\nalso been demonstrated in the characterization of functional magnetic resonance\nimaging (fMRI) data, where the spatial (voxel) dimensions are commonly grouped\n(unfolded) as a single way/mode of the 3-rd order array, the other two ways\ncorresponding to time and subjects. However, such methods are known to be\nineffective in more demanding scenarios, such as the ones with strong noise\nand/or significant overlapping of activated regions. This paper aims at\ninvestigating the possible gains from a better exploitation of the spatial\ndimension, through a higher- (4 or 5) order tensor modeling of the fMRI signal.\nIn this context, and in order to increase the degrees of freedom of the\nmodeling process, a higher-order Block Term Decomposition (BTD) is applied, for\nthe first time in fMRI analysis. Its effectiveness is demonstrated via\nextensive simulation results.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.05073v1"
    },
    {
        "title": "Dimensionality reduction based on Distance Preservation to Local Mean\n  (DPLM) for SPD matrices and its application in BCI",
        "authors": [
            "Alireza Davoudi",
            "Saeed Shiry Ghidary",
            "Khadijeh Sadatnejad"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In this paper, we propose a nonlinear dimensionality reduction algorithm for\nthe manifold of Symmetric Positive Definite (SPD) matrices that considers the\ngeometry of SPD matrices and provides a low dimensional representation of the\nmanifold with high class discrimination. The proposed algorithm, tries to\npreserve the local structure of the data by preserving distance to local mean\n(DPLM) and also provides an implicit projection matrix. DPLM is linear in terms\nof the number of training samples and may use the label information when they\nare available in order to performance improvement in classification tasks. We\nperformed several experiments on the multi-class dataset IIa from BCI\ncompetition IV. The results show that our approach as dimensionality reduction\ntechnique - leads to superior results in comparison with other competitor in\nthe related literature because of its robustness against outliers. The\nexperiments confirm that the combination of DPLM with FGMDM as the classifier\nleads to the state of the art performance on this dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.00514v1"
    },
    {
        "title": "Multi-way Monte Carlo Method for Linear Systems",
        "authors": [
            "Tao Wu",
            "David F. Gleich"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We study the Monte Carlo method for solving a linear system of the form $x =\nH x + b$. A sufficient condition for the method to work is $\\| H \\| < 1$, which\ngreatly limits the usability of this method. We improve this condition by\nproposing a new multi-way Markov random walk, which is a generalization of the\nstandard Markov random walk. Under our new framework we prove that the\nnecessary and sufficient condition for our method to work is the spectral\nradius $\\rho(H^{+}) < 1$, which is a weaker requirement than $\\| H \\| < 1$. In\naddition to solving more problems, our new method can work faster than the\nstandard algorithm. In numerical experiments on both synthetic and real world\nmatrices, we demonstrate the effectiveness of our new method.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04361v1"
    },
    {
        "title": "Structural Convergence Results for Approximation of Dominant Subspaces\n  from Block Krylov Spaces",
        "authors": [
            "Petros Drineas",
            "Ilse Ipsen",
            "Eugenia-Maria Kontopoulou",
            "Malik Magdon-Ismail"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  This paper is concerned with approximating the dominant left singular vector\nspace of a real matrix $A$ of arbitrary dimension, from block Krylov spaces\ngenerated by the matrix $AA^T$ and the block vector $AX$. Two classes of\nresults are presented. First are bounds on the distance, in the two and\nFrobenius norms, between the Krylov space and the target space. The distance is\nexpressed in terms of principal angles. Second are quality of approximation\nbounds, relative to the best approximation in the Frobenius norm. For starting\nguesses $X$ of full column-rank, the bounds depend on the tangent of the\nprincipal angles between $X$ and the dominant right singular vector space of\n$A$. The results presented here form the structural foundation for the analysis\nof randomized Krylov space methods. The innovative feature is a combination of\ntraditional Lanczos convergence analysis with optimal approximations via least\nsquares problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.00671v2"
    },
    {
        "title": "Consistent Discretization and Minimization of the L1 Norm on Manifolds",
        "authors": [
            "Alex Bronstein",
            "Yoni Choukroun",
            "Ron Kimmel",
            "Matan Sela"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  The L1 norm has been tremendously popular in signal and image processing in\nthe past two decades due to its sparsity-promoting properties. More recently,\nits generalization to non-Euclidean domains has been found useful in shape\nanalysis applications. For example, in conjunction with the minimization of the\nDirichlet energy, it was shown to produce a compactly supported quasi-harmonic\northonormal basis, dubbed as compressed manifold modes. The continuous L1 norm\non the manifold is often replaced by the vector l1 norm applied to sampled\nfunctions. We show that such an approach is incorrect in the sense that it does\nnot consistently discretize the continuous norm and warn against its\nsensitivity to the specific sampling. We propose two alternative\ndiscretizations resulting in an iteratively-reweighed l2 norm. We demonstrate\nthe proposed strategy on the compressed modes problem, which reduces to a\nsequence of simple eigendecomposition problems not requiring non-convex\noptimization on Stiefel manifolds and producing more stable and accurate\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.05434v1"
    },
    {
        "title": "A Fast Algorithm for Convolutional Structured Low-Rank Matrix Recovery",
        "authors": [
            "Greg Ongie",
            "Mathews Jacob"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Fourier domain structured low-rank matrix priors are emerging as powerful\nalternatives to traditional image recovery methods such as total variation and\nwavelet regularization. These priors specify that a convolutional structured\nmatrix, i.e., Toeplitz, Hankel, or their multi-level generalizations, built\nfrom Fourier data of the image should be low-rank. The main challenge in\napplying these schemes to large-scale problems is the computational complexity\nand memory demand resulting from lifting the image data to a large scale\nmatrix. We introduce a fast and memory efficient approach called the Generic\nIterative Reweighted Annihilation Filter (GIRAF) algorithm that exploits the\nconvolutional structure of the lifted matrix to work in the original un-lifted\ndomain, thus considerably reducing the complexity. Our experiments on the\nrecovery of images from undersampled Fourier measurements show that the\nresulting algorithm is considerably faster than previously proposed algorithms,\nand can accommodate much larger problem sizes than previously studied.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.07429v3"
    },
    {
        "title": "Modeling Parallel Wiener-Hammerstein Systems Using Tensor Decomposition\n  of Volterra Kernels",
        "authors": [
            "Philippe Dreesen",
            "David Westwick",
            "Johan Schoukens",
            "Mariya Ishteva"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Providing flexibility and user-interpretability in nonlinear system\nidentification can be achieved by means of block-oriented methods. One of such\nblock-oriented system structures is the parallel Wiener-Hammerstein system,\nwhich is a sum of Wiener-Hammerstein branches, consisting of static\nnonlinearities sandwiched between linear dynamical blocks. Parallel\nWiener-Hammerstein models have more descriptive power than their single-branch\ncounterparts, but their identification is a non-trivial task that requires\ntailored system identification methods. In this work, we will tackle the\nidentification problem by performing a tensor decomposition of the Volterra\nkernels obtained from the nonlinear system. We illustrate how the parallel\nWiener-Hammerstein block-structure gives rise to a joint tensor decomposition\nof the Volterra kernels with block-circulant structured factors. The\ncombination of Volterra kernels and tensor methods is a fruitful way to tackle\nthe parallel Wiener-Hammerstein system identification task. In simulation\nexperiments, we were able to reconstruct very accurately the underlying blocks\nunder noisy conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.08063v1"
    },
    {
        "title": "Tensor Networks for Latent Variable Analysis. Part I: Algorithms for\n  Tensor Train Decomposition",
        "authors": [
            "Anh-Huy Phan",
            "Andrzej Cichocki",
            "Andre Uschmajew",
            "Petr Tichavsky",
            "George Luta",
            "Danilo Mandic"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Decompositions of tensors into factor matrices, which interact through a core\ntensor, have found numerous applications in signal processing and machine\nlearning. A more general tensor model which represents data as an ordered\nnetwork of sub-tensors of order-2 or order-3 has, so far, not been widely\nconsidered in these fields, although this so-called tensor network\ndecomposition has been long studied in quantum physics and scientific\ncomputing. In this study, we present novel algorithms and applications of\ntensor network decompositions, with a particular focus on the tensor train\ndecomposition and its variants. The novel algorithms developed for the tensor\ntrain decomposition update, in an alternating way, one or several core tensors\nat each iteration, and exhibit enhanced mathematical tractability and\nscalability to exceedingly large-scale data tensors. The proposed algorithms\nare tested in classic paradigms of blind source separation from a single\nmixture, denoising, and feature extraction, and achieve superior performance\nover the widely used truncated algorithms for tensor train decomposition.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.09230v1"
    },
    {
        "title": "Real-time Error Control for Surgical Simulation",
        "authors": [
            "Huu Phuoc Bui",
            "Satyendra Tomar",
            "Hadrien Courtecuisse",
            "Stéphane Cotin",
            "Stéphane Bordas"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Objective: To present the first real-time a posteriori error-driven adaptive\nfinite element approach for real-time simulation and to demonstrate the method\non a needle insertion problem. Methods: We use corotational elasticity and a\nfrictional needle/tissue interaction model. The problem is solved using finite\nelements within SOFA. The refinement strategy relies upon a hexahedron-based\nfinite element method, combined with a posteriori error estimation driven local\n$h$-refinement, for simulating soft tissue deformation. Results: We control the\nlocal and global error level in the mechanical fields (e.g. displacement or\nstresses) during the simulation. We show the convergence of the algorithm on\nacademic examples, and demonstrate its practical usability on a percutaneous\nprocedure involving needle insertion in a liver. For the latter case, we\ncompare the force displacement curves obtained from the proposed adaptive\nalgorithm with that obtained from a uniform refinement approach. Conclusions:\nError control guarantees that a tolerable error level is not exceeded during\nthe simulations. Local mesh refinement accelerates simulations. Significance:\nOur work provides a first step to discriminate between discretization error and\nmodeling error by providing a robust quantification of discretization error\nduring simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.02570v3"
    },
    {
        "title": "Fast and Accurate Surface Normal Integration on Non-Rectangular Domains",
        "authors": [
            "Martin Bähr",
            "Michael Breuß",
            "Yvain Quéau",
            "Ali Sharifi Boroujerdi",
            "Jean-Denis Durou"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  The integration of surface normals for the purpose of computing the shape of\na surface in 3D space is a classic problem in computer vision. However, even\nnowadays it is still a challenging task to devise a method that combines the\nflexibility to work on non-trivial computational domains with high accuracy,\nrobustness and computational efficiency. By uniting a classic approach for\nsurface normal integration with modern computational techniques we construct a\nsolver that fulfils these requirements. Building upon the Poisson integration\nmodel we propose to use an iterative Krylov subspace solver as a core step in\ntackling the task. While such a method can be very efficient, it may only show\nits full potential when combined with a suitable numerical preconditioning and\na problem-specific initialisation. We perform a thorough numerical study in\norder to identify an appropriate preconditioner for our purpose. To address the\nissue of a suitable initialisation we propose to compute this initial state via\na recently developed fast marching integrator. Detailed numerical experiments\nilluminate the benefits of this novel combination. In addition, we show on\nreal-world photometric stereo datasets that the developed numerical framework\nis flexible enough to tackle modern computer vision applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.06049v1"
    },
    {
        "title": "On the Approximation of Functionals of Very Large Hermitian Matrices\n  represented as Matrix Product Operators",
        "authors": [
            "Moritz August",
            "Mari Carmen Bañuls",
            "Thomas Huckle"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We present a method to approximate functionals $\\text{Tr} \\, f(A)$ of very\nhigh-dimensional hermitian matrices $A$ represented as Matrix Product Operators\n(MPOs). Our method is based on a reformulation of a block Lanczos algorithm in\ntensor network format. We state main properties of the method and show how to\nadapt the basic Lanczos algorithm to the tensor network formalism to allow for\nhigh-dimensional computations. Additionally, we give an analysis of the\ncomplexity of our method and provide numerical evidence that it yields good\napproximations of the entropy of density matrices represented by MPOs while\nbeing robust against truncations.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.06086v1"
    },
    {
        "title": "Error estimates with explicit constants for the Sinc approximation over\n  infinite intervals",
        "authors": [
            "Tomoaki Okayama"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  The Sinc approximation is a function approximation formula that attains\nexponential convergence for rapidly decaying functions defined on the whole\nreal axis. Even for other functions, the Sinc approximation works accurately\nwhen combined with a proper variable transformation. The convergence rate has\nbeen analyzed for typical cases including finite, semi-infinite, and infinite\nintervals. Recently, for verified numerical computations, a more explicit,\n\"computable\" error bound has been given in the case of a finite interval. In\nthis paper, such explicit error bounds are derived for other cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.06685v3"
    },
    {
        "title": "Recovery of Sparse and Low Rank Components of Matrices Using Iterative\n  Method with Adaptive Thresholding",
        "authors": [
            "Nematollah Zarmehi",
            "Farokh Marvasti"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this letter, we propose an algorithm for recovery of sparse and low rank\ncomponents of matrices using an iterative method with adaptive thresholding. In\neach iteration, the low rank and sparse components are obtained using a\nthresholding operator. This algorithm is fast and can be implemented easily. We\ncompare it with one of the most common fast methods in which the rank and\nsparsity are approximated by $\\ell_1$ norm. We also apply it to some real\napplications where the noise is not so sparse. The simulation results show that\nit has a suitable performance with low run-time.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.03722v2"
    },
    {
        "title": "Randomized CP Tensor Decomposition",
        "authors": [
            "N. Benjamin Erichson",
            "Krithika Manohar",
            "Steven L. Brunton",
            "J. Nathan Kutz"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The CANDECOMP/PARAFAC (CP) tensor decomposition is a popular\ndimensionality-reduction method for multiway data. Dimensionality reduction is\noften sought after since many high-dimensional tensors have low intrinsic rank\nrelative to the dimension of the ambient measurement space. However, the\nemergence of `big data' poses significant computational challenges for\ncomputing this fundamental tensor decomposition. By leveraging modern\nrandomized algorithms, we demonstrate that coherent structures can be learned\nfrom a smaller representation of the tensor in a fraction of the time. Thus,\nthis simple but powerful algorithm enables one to compute the approximate CP\ndecomposition even for massive tensors. The approximation error can thereby be\ncontrolled via oversampling and the computation of power iterations. In\naddition to theoretical results, several empirical results demonstrate the\nperformance of the proposed algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.09074v2"
    },
    {
        "title": "Vico-Greengard-Ferrando quadratures in the tensor solver for integral\n  equations",
        "authors": [
            "Valentin Khrulkov",
            "Maxim Rakhuba",
            "Ivan Oseledets"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Convolution with Green's function of a differential operator appears in a lot\nof applications e.g. Lippmann-Schwinger integral equation. Algorithms for\ncomputing such are usually non-trivial and require non-uniform mesh. However,\nrecently Vico, Greengard and Ferrando developed method for computing\nconvolution with smooth functions with compact support with spectral accuracy,\nrequiring nothing more than Fast Fourier Transform (FFT). Their approach is\nvery suitable for the low-rank tensor implementation which we develop using\nQuantized Tensor Train (QTT) decomposition.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01669v1"
    },
    {
        "title": "Numerical solution of time-dependent problems with fractional power\n  elliptic operator",
        "authors": [
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  An unsteady problem is considered for a space-fractional equation in a\nbounded domain. A first-order evolutionary equation involves a fractional power\nof an elliptic operator of second order. Finite element approximation in space\nis employed. To construct approximation in time, standard two-level schemes are\nused. The approximate solution at a new time-level is obtained as a solution of\na discrete problem with the fractional power of the elliptic operator. A\nPade-type approximation is constructed on the basis of special quadrature\nformulas for an integral representation of the fractional power elliptic\noperator using explicit schemes. A similar approach is applied in the numerical\nimplementation of implicit schemes. The results of numerical experiments are\npresented for a test two-dimensional problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.03851v1"
    },
    {
        "title": "On the Necessity of Superparametric Geometry Representation for\n  Discontinuous Galerkin Methods on Domains with Curved Boundaries",
        "authors": [
            "Philip Zwanenburg",
            "Siva Nadarajah"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We provide numerical evidence demonstrating the necessity of employing a\nsuperparametric geometry representation in order to obtain optimal convergence\norders on two-dimensional domains with curved boundaries when solving the Euler\nequations using Discontinuous Galerkin methods. However, concerning the\nobtention of optimal convergence orders for the Navier-Stokes equations, we\ndemonstrate numerically that the use of isoparametric geometry representation\nis sufficient for the case considered here.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01668v1"
    },
    {
        "title": "Fast Singular Value Shrinkage with Chebyshev Polynomial Approximation\n  Based on Signal Sparsity",
        "authors": [
            "Masaki Onuki",
            "Shunsuke Ono",
            "Keiichiro Shirai",
            "Yuichi Tanaka"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We propose an approximation method for thresholding of singular values using\nChebyshev polynomial approximation (CPA). Many signal processing problems\nrequire iterative application of singular value decomposition (SVD) for\nminimizing the rank of a given data matrix with other cost functions and/or\nconstraints, which is called matrix rank minimization. In matrix rank\nminimization, singular values of a matrix are shrunk by hard-thresholding,\nsoft-thresholding, or weighted soft-thresholding. However, the computational\ncost of SVD is generally too expensive to handle high dimensional signals such\nas images; hence, in this case, matrix rank minimization requires enormous\ncomputation time. In this paper, we leverage CPA to (approximately) manipulate\nsingular values without computing singular values and vectors. The thresholding\nof singular values is expressed by a multiplication of certain matrices, which\nis derived from a characteristic of CPA. The multiplication is also efficiently\ncomputed using the sparsity of signals. As a result, the computational cost is\nsignificantly reduced. Experimental results suggest the effectiveness of our\nmethod through several image processing applications based on matrix rank\nminimization with nuclear norm relaxation in terms of computation time and\napproximation precision.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.07112v1"
    },
    {
        "title": "Image reconstruction with imperfect forward models and applications in\n  deblurring",
        "authors": [
            "Yury Korolev",
            "Jan Lellmann"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We present and analyse an approach to image reconstruction problems with\nimperfect forward models based on partially ordered spaces - Banach lattices.\nIn this approach, errors in the data and in the forward models are described\nusing order intervals. The method can be characterised as the lattice analogue\nof the residual method, where the feasible set is defined by linear inequality\nconstraints. The study of this feasible set is the main contribution of this\npaper. Convexity of this feasible set is examined in several settings and\nmodifications for introducing additional information about the forward operator\nare considered. Numerical examples demonstrate the performance of the method in\ndeblurring with errors in the blurring kernel.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.01244v3"
    },
    {
        "title": "Parallelizing Over Artificial Neural Network Training Runs with\n  Multigrid",
        "authors": [
            "Jacob B. Schroder"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Artificial neural networks are a popular and effective machine learning\ntechnique. Great progress has been made parallelizing the expensive training\nphase of an individual network, leading to highly specialized pieces of\nhardware, many based on GPU-type architectures, and more concurrent algorithms\nsuch as synthetic gradients. However, the training phase continues to be a\nbottleneck, where the training data must be processed serially over thousands\nof individual training runs. This work considers a multigrid reduction in time\n(MGRIT) algorithm that is able to parallelize over the thousands of training\nruns and converge to the exact same solution as traditional training would\nprovide. MGRIT was originally developed to provide parallelism for time\nevolution problems that serially step through a finite number of time-steps.\nThis work recasts the training of a neural network similarly, treating neural\nnetwork training as an evolution equation that evolves the network weights from\none step to the next. Thus, this work concerns distributed computing approaches\nfor neural networks, but is distinct from other approaches which seek to\nparallelize only over individual training runs. The work concludes with\nsupporting numerical results for two model problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.02276v2"
    },
    {
        "title": "Preconditioning immersed isogeometric finite element methods with\n  application to flow problems",
        "authors": [
            "Frits de Prenter",
            "Clemens Verhoosel",
            "Harald van Brummelen"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Immersed finite element methods generally suffer from conditioning problems\nwhen cut elements intersect the physical domain only on a small fraction of\ntheir volume. De Prenter et al. [Computer Methods in Applied Mechanics and\nEngineering, 316 (2017) pp. 297-327] present an analysis for symmetric positive\ndefinite (SPD) immersed problems, and for this class of problems an algebraic\npreconditioner is developed. In this contribution the conditioning analysis is\nextended to immersed finite element methods for systems that are not SPD and\nthe preconditioning technique is generalized to a connectivity-based\npreconditioner inspired by Additive-Schwarz preconditioning. This\nConnectivity-based Additive-Schwarz (CbAS) preconditioner is applicable to\nproblems that are not SPD and to mixed problems, such as the Stokes and\nNavier-Stokes equations. A detailed numerical investigation of the effectivity\nof the CbAS preconditioner to a range of flow problems is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.03519v1"
    },
    {
        "title": "Eigenvalue Solvers for Modeling Nuclear Reactors on Leadership Class\n  Machines",
        "authors": [
            "R. N. Slaybaugh",
            "M. Ramirez-Zweiger",
            "Tara Pandya",
            "Steven Hamilton",
            "T. M. Evans"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Three complementary methods have been implemented in the code Denovo that\naccelerate neutral particle transport calculations with methods that use\nleadership-class computers fully and effectively: a multigroup block (MG)\nKrylov solver, a Rayleigh Quotient Iteration (RQI) eigenvalue solver, and a\nmultigrid in energy (MGE) preconditioner. The MG Krylov solver converges more\nquickly than Gauss Seidel and enables energy decomposition such that Denovo can\nscale to hundreds of thousands of cores. RQI should converge in fewer\niterations than power iteration (PI) for large and challenging problems. RQI\ncreates shifted systems that would not be tractable without the MG Krylov\nsolver. It also creates ill-conditioned matrices. The MGE preconditioner\nreduces iteration count significantly when used with RQI and takes advantage of\nthe new energy decomposition such that it can scale efficiently. Each\nindividual method has been described before, but this is the first time they\nhave been demonstrated to work together effectively.\n  The combination of solvers enables the RQI eigenvalue solver to work better\nthan the other available solvers for large reactors problems on leadership\nclass machines. Using these methods together, RQI converged in fewer iterations\nand in less time than PI for a full pressurized water reactor core. These\nsolvers also performed better than an Arnoldi eigenvalue solver for a reactor\nbenchmark problem when energy decomposition is needed. The MG Krylov, MGE\npreconditioner, and RQI solver combination also scales well in energy. This\nsolver set is a strong choice for very large and challenging problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04928v2"
    },
    {
        "title": "Recover the lost Phasor Measurement Unit Data Using Alternating\n  Direction Multipliers Method",
        "authors": [
            "Mang Liao",
            "Di Shi",
            "Zhe Yu",
            "Wendong Zhu",
            "Zhiwei Wang",
            "Yingmeng Xiang"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This paper presents a novel algorithm for recovering missing data of phasor\nmeasurement units (PMUs). Due to the low-rank property of PMU data, missing\nmeasurement recovery can be formulated as a low-rank matrix-completion problem.\nBased on maximum-margin matrix factorization, we propose an efficient algorithm\nbased on alternating direction method of multipliers (ADMM) for solving the\nmatrix completion problem. Comparing to existing approaches, the proposed ADMM\nbased algorithm does not need to estimate the rank of the target data matrix\nand provides better performance in computation complexity. In addition, we\nconsider the case of measurements missing from all PMU channels and provide a\nstrategy of reshaping the matrix which contains the received PMU data for\nrecovery. Numerical results using PMU measurements from IEEE 68-bus power\nsystem model illustrate the effectiveness and efficiency of the proposed\napproaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07733v2"
    },
    {
        "title": "Tensor Networks for Dimensionality Reduction and Large-Scale\n  Optimizations. Part 2 Applications and Future Perspectives",
        "authors": [
            "A. Cichocki",
            "A-H. Phan",
            "Q. Zhao",
            "N. Lee",
            "I. V. Oseledets",
            "M. Sugiyama",
            "D. Mandic"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Part 2 of this monograph builds on the introduction to tensor networks and\ntheir operations presented in Part 1. It focuses on tensor network models for\nsuper-compressed higher-order representation of data/parameters and related\ncost functions, while providing an outline of their applications in machine\nlearning and data analytics. A particular emphasis is on the tensor train (TT)\nand Hierarchical Tucker (HT) decompositions, and their physically meaningful\ninterpretations which reflect the scalability of the tensor network approach.\nThrough a graphical approach, we also elucidate how, by virtue of the\nunderlying low-rank tensor approximations and sophisticated contractions of\ncore tensors, tensor networks have the ability to perform distributed\ncomputations on otherwise prohibitively large volumes of data/parameters,\nthereby alleviating or even eliminating the curse of dimensionality. The\nusefulness of this concept is illustrated over a number of applied areas,\nincluding generalized regression and classification (support tensor machines,\ncanonical correlation analysis, higher order partial least squares),\ngeneralized eigenvalue decomposition, Riemannian optimization, and in the\noptimization of deep neural networks. Part 1 and Part 2 of this work can be\nused either as stand-alone separate texts, or indeed as a conjoint\ncomprehensive review of the exciting field of low-rank tensor networks and\ntensor decompositions.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.09165v1"
    },
    {
        "title": "An iterative approximate method of solving boundary value problems using\n  dual Bernstein polynomials",
        "authors": [
            "Przemysław Gospodarczyk",
            "Paweł Woźny"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper, we present a new iterative approximate method of solving\nboundary value problems. The idea is to compute approximate polynomial\nsolutions in the Bernstein form using least squares approximation combined with\nsome properties of dual Bernstein polynomials which guarantee high efficiency\nof our approach. The method can deal with both linear and nonlinear\ndifferential equations. Moreover, not only second order differential equations\ncan be solved but also higher order differential equations. Illustrative\nexamples confirm the versatility of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.02162v1"
    },
    {
        "title": "Completion of High Order Tensor Data with Missing Entries via\n  Tensor-train Decomposition",
        "authors": [
            "Longhao Yuan",
            "Qibin Zhao",
            "Jianting Cao"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper, we aim at the completion problem of high order tensor data\nwith missing entries. The existing tensor factorization and completion methods\nsuffer from the curse of dimensionality when the order of tensor N>>3. To\novercome this problem, we propose an efficient algorithm called TT-WOPT\n(Tensor-train Weighted OPTimization) to find the latent core tensors of tensor\ndata and recover the missing entries. Tensor-train decomposition, which has the\npowerful representation ability with linear scalability to tensor order, is\nemployed in our algorithm. The experimental results on synthetic data and\nnatural image completion demonstrate that our method significantly outperforms\nthe other related methods. Especially when the missing rate of data is very\nhigh, e.g., 85% to 99%, our algorithm can achieve much better performance than\nother state-of-the-art algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.02641v2"
    },
    {
        "title": "Bayesian Filtering for ODEs with Bounded Derivatives",
        "authors": [
            "Emilia Magnani",
            "Hans Kersting",
            "Michael Schober",
            "Philipp Hennig"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Recently there has been increasing interest in probabilistic solvers for\nordinary differential equations (ODEs) that return full probability measures,\ninstead of point estimates, over the solution and can incorporate uncertainty\nover the ODE at hand, e.g. if the vector field or the initial value is only\napproximately known or evaluable. The ODE filter proposed in recent work models\nthe solution of the ODE by a Gauss-Markov process which serves as a prior in\nthe sense of Bayesian statistics. While previous work employed a Wiener process\nprior on the (possibly multiple times) differentiated solution of the ODE and\nestablished equivalence of the corresponding solver with classical numerical\nmethods, this paper raises the question whether other priors also yield\npractically useful solvers. To this end, we discuss a range of possible priors\nwhich enable fast filtering and propose a new prior--the Integrated Ornstein\nUhlenbeck Process (IOUP)--that complements the existing Integrated Wiener\nprocess (IWP) filter by encoding the property that a derivative in time of the\nsolution is bounded in the sense that it tends to drift back to zero. We\nprovide experiments comparing IWP and IOUP filters which support the belief\nthat IWP approximates better divergent ODE's solutions whereas IOUP is a better\nprior for trajectories with bounded derivatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08471v1"
    },
    {
        "title": "Fast online low-rank tensor subspace tracking by CP decomposition using\n  recursive least squares from incomplete observations",
        "authors": [
            "Hiroyuki Kasai"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We consider the problem of online subspace tracking of a partially observed\nhigh-dimensional data stream corrupted by noise, where we assume that the data\nlie in a low-dimensional linear subspace. This problem is cast as an online\nlow-rank tensor completion problem. We propose a novel online tensor subspace\ntracking algorithm based on the CANDECOMP/PARAFAC (CP) decomposition, dubbed\nOnLine Low-rank Subspace tracking by TEnsor CP Decomposition (OLSTEC). The\nproposed algorithm especially addresses the case in which the subspace of\ninterest is dynamically time-varying. To this end, we build up our proposed\nalgorithm exploiting the recursive least squares (RLS), which is the\nsecond-order gradient algorithm. Numerical evaluations on synthetic datasets\nand real-world datasets such as communication network traffic, environmental\ndata, and surveillance videos, show that the proposed OLSTEC algorithm\noutperforms state-of-the-art online algorithms in terms of the convergence rate\nper iteration.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10276v1"
    },
    {
        "title": "High-order Tensor Completion for Data Recovery via Sparse Tensor-train\n  Optimization",
        "authors": [
            "Longhao Yuan",
            "Qibin Zhao",
            "Jianting Cao"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper, we aim at the problem of tensor data completion. Tensor-train\ndecomposition is adopted because of its powerful representation ability and\nlinear scalability to tensor order. We propose an algorithm named Sparse\nTensor-train Optimization (STTO) which considers incomplete data as sparse\ntensor and uses first-order optimization method to find the factors of\ntensor-train decomposition. Our algorithm is shown to perform well in\nsimulation experiments at both low-order cases and high-order cases. We also\nemploy a tensorization method to transform data to a higher-order form to\nenhance the performance of our algorithm. The results of image recovery\nexperiments in various cases manifest that our method outperforms other\ncompletion algorithms. Especially when the missing rate is very high, e.g.,\n90\\% to 99\\%, our method is significantly better than the state-of-the-art\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02271v2"
    },
    {
        "title": "On the Numerical Solution of Fourth-Order Linear Two-Point Boundary\n  Value Problems",
        "authors": [
            "William Leeb",
            "Vladimir Rokhlin"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This paper introduces a fast and numerically stable algorithm for the\nsolution of fourth-order linear boundary value problems on an interval. This\ntype of equation arises in a variety of settings in physics and signal\nprocessing. Our method reformulates the equation as a collection of second-kind\nintegral equations defined on local subdomains. Each such equation can be\nstably discretized and solved. The boundary values of these local solutions are\nmatched by solving a banded linear system. The method of deferred corrections\nis then used to increase the accuracy of the scheme. Deferred corrections\nrequires applying the integral operator to a function on the entire domain, for\nwhich we provide an algorithm with linear cost. We illustrate the performance\nof our method on several numerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.05354v3"
    },
    {
        "title": "A Coordinate-wise Optimization Algorithm for Sparse Inverse Covariance\n  Selection",
        "authors": [
            "Ganzhao Yuan",
            "Haoxian Tan",
            "Wei-Shi Zheng"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Sparse inverse covariance selection is a fundamental problem for analyzing\ndependencies in high dimensional data. However, such a problem is difficult to\nsolve since it is NP-hard. Existing solutions are primarily based on convex\napproximation and iterative hard thresholding, which only lead to sub-optimal\nsolutions. In this work, we propose a coordinate-wise optimization algorithm to\nsolve this problem which is guaranteed to converge to a coordinate-wise minimum\npoint. The algorithm iteratively and greedily selects one variable or swaps two\nvariables to identify the support set, and then solves a reduced convex\noptimization problem over the support set to achieve the greatest descent. As a\nside contribution of this paper, we propose a Newton-like algorithm to solve\nthe reduced convex sub-problem, which is proven to always converge to the\noptimal solution with global linear convergence rate and local quadratic\nconvergence rate. Finally, we demonstrate the efficacy of our method on\nsynthetic data and real-world data sets. As a result, the proposed method\nconsistently outperforms existing solutions in terms of accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.07038v2"
    },
    {
        "title": "Accelerated Optimization in the PDE Framework: Formulations for the\n  Active Contour Case",
        "authors": [
            "Anthony Yezzi",
            "Ganesh Sundaramoorthi"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Following the seminal work of Nesterov, accelerated optimization methods have\nbeen used to powerfully boost the performance of first-order, gradient-based\nparameter estimation in scenarios where second-order optimization strategies\nare either inapplicable or impractical. Not only does accelerated gradient\ndescent converge considerably faster than traditional gradient descent, but it\nalso performs a more robust local search of the parameter space by initially\novershooting and then oscillating back as it settles into a final\nconfiguration, thereby selecting only local minimizers with a basis of\nattraction large enough to contain the initial overshoot. This behavior has\nmade accelerated and stochastic gradient search methods particularly popular\nwithin the machine learning community. In their recent PNAS 2016 paper,\nWibisono, Wilson, and Jordan demonstrate how a broad class of accelerated\nschemes can be cast in a variational framework formulated around the Bregman\ndivergence, leading to continuum limit ODE's. We show how their formulation may\nbe further extended to infinite dimension manifolds (starting here with the\ngeometric space of curves and surfaces) by substituting the Bregman divergence\nwith inner products on the tangent space and explicitly introducing a\ndistributed mass model which evolves in conjunction with the object of interest\nduring the optimization process. The co-evolving mass model, which is\nintroduced purely for the sake of endowing the optimization with helpful\ndynamics, also links the resulting class of accelerated PDE based optimization\nschemes to fluid dynamical formulations of optimal mass transport.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.09867v1"
    },
    {
        "title": "A fast nonconvex Compressed Sensing algorithm for highly low-sampled MR\n  images reconstruction",
        "authors": [
            "Damiana Lazzaro",
            "Elena Loli Piccolomini",
            "Fabiana Zama"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper we present a fast and efficient method for the reconstruction\nof Magnetic Resonance Images (MRI) from severely under-sampled data. From the\nCompressed Sensing theory we have mathematically modeled the problem as a\nconstrained minimization problem with a family of non-convex regularizing\nobjective functions depending on a parameter and a least squares data fit\nconstraint. We propose a fast and efficient algorithm, named Fast NonConvex\nReweighting (FNCR) algorithm, based on an iterative scheme where the non-convex\nproblem is approximated by its convex linearization and the penalization\nparameter is automatically updated. The convex problem is solved by a\nForward-Backward procedure, where the Backward step is performed by a Split\nBregman strategy. Moreover, we propose a new efficient iterative solver for the\narising linear systems. We prove the convergence of the proposed FNCR method.\nThe results on synthetic phantoms and real images show that the algorithm is\nvery well performing and computationally efficient, even when compared to the\nbest performing methods proposed in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.11075v1"
    },
    {
        "title": "Multi-dimensional imaging data recovery via minimizing the partial sum\n  of tubal nuclear norm",
        "authors": [
            "Tai-Xiang Jiang",
            "Ting-Zhu Huang",
            "Xi-Le Zhao",
            "Liang-Jian Deng"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper, we investigate tensor recovery problems within the tensor\nsingular value decomposition (t-SVD) framework. We propose the partial sum of\nthe tubal nuclear norm (PSTNN) of a tensor. The PSTNN is a surrogate of the\ntensor tubal multi-rank. We build two PSTNN-based minimization models for two\ntypical tensor recovery problems, i.e., the tensor completion and the tensor\nprincipal component analysis. We give two algorithms based on the alternating\ndirection method of multipliers (ADMM) to solve proposed PSTNN-based tensor\nrecovery models. Experimental results on the synthetic data and real-world data\nreveal the superior of the proposed PSTNN.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.05870v3"
    },
    {
        "title": "Constructing an orthonormal set of eigenvectors for DFT matrix using\n  Gramians and determinants",
        "authors": [
            "Vadim Zaliva"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The problem of constructing an orthogonal set of eigenvectors for a DFT\nmatrix is well studied. An elegant solution is mentioned by Matveev in his\npaper \"Interwining relations between the Fourier transfom and discrete Fourier\ntransform, the related functional identities and beyond\". In this paper, we\npresent a distilled form of his solution including some steps unexplained in\nhis paper, along with correction of typos and errors using more consistent\nnotation. Then we compare the computational complexity of his method with the\nmore traditional method involving direct application of the Gram-Schmidt\nprocess. Finally, we present our implementation of Matveev's method as a\nMathematica module.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.06959v1"
    },
    {
        "title": "Numerical Analysis of Automodel Solutions for Superdiffusive Transport",
        "authors": [
            "Alexander B. Kukushkin",
            "Vladislav S. Neverov",
            "Petr A. Sdvizhenskii",
            "Vladimir V. Voloshinov"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The distributed computing analysis of the accuracy of automodel solutions for\nthe Green's function of a wide class of superdiffusive transport of\nperturbation on a uniform background is carried out. The approximate automodel\nsolutions have been suggested for the 1D transport equation with a model\nlong-tailed step-length probability distribution function (PDF) with various\npower-law exponents. These PDFs describe the transport dominated by the L\\'evy\nflights. Massive computing experiments were done to verify automodel solutions.\nThe Everest distributed computing platform and the cluster at NRC Kurchatov\nInstitute were used. The results verify the high accuracy of automodel\nsolutions in a wide range of space-time variables and suggest extending the\ndeveloped method of automodel solutions to a wider class of stochastic\nphenomena.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.03407v1"
    },
    {
        "title": "Numerical solution of boundary value problems for the eikonal equation\n  in an anisotropic medium",
        "authors": [
            "Alexander G. Churbanov",
            "Petr N. Vabishchevich"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  A Dirichlet problem is considered for the eikonal equation in an anisotropic\nmedium. The nonlinear boundary value problem (BVP) formulated in the present\nwork is the limit of the diffusion-reaction problem with a reaction parameter\ntending to infinity. To solve numerically the singularly perturbed\ndiffusion-reaction problem, monotone approximations are employed. Numerical\nexamples are presented for a two-dimensional BVP for the eikonal equation in an\nanisotropic medium. The standard piecewise-linear finite-element approximation\nin space is used in computations.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.06203v1"
    },
    {
        "title": "A Learning Based Approach for Uncertainty Analysis in Numerical Weather\n  Prediction Models",
        "authors": [
            "Azam Moosavi",
            "Vishwas Rao",
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Complex numerical weather prediction models incorporate a variety of physical\nprocesses, each described by multiple alternative physical schemes with\nspecific parameters. The selection of the physical schemes and the choice of\nthe corresponding physical parameters during model configuration can\nsignificantly impact the accuracy of model forecasts. There is no combination\nof physical schemes that works best for all times, at all locations, and under\nall conditions. It is therefore of considerable interest to understand the\ninterplay between the choice of physics and the accuracy of the resulting\nforecasts under different conditions. This paper demonstrates the use of\nmachine learning techniques to study the uncertainty in numerical weather\nprediction models due to the interaction of multiple physical processes. The\nfirst problem addressed herein is the estimation of systematic model errors in\noutput quantities of interest at future times, and the use of this information\nto improve the model forecasts. The second problem considered is the\nidentification of those specific physical processes that contribute most to the\nforecast uncertainty in the quantity of interest under specified meteorological\nconditions.\n  The discrepancies between model results and observations at past times are\nused to learn the relationships between the choice of physical processes and\nthe resulting forecast errors. Numerical experiments are carried out with the\nWeather Research and Forecasting (WRF) model. The output quantity of interest\nis the model precipitation, a variable that is both extremely important and\nvery challenging to forecast. The physical processes under consideration\ninclude various micro-physics schemes, cumulus parameterizations, short wave,\nand long wave radiation schemes. The experiments demonstrate the strong\npotential of machine learning approaches to aid the study of model errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.08055v1"
    },
    {
        "title": "Subspace-Orbit Randomized Decomposition for Low-rank Matrix\n  Approximation",
        "authors": [
            "Maboud F. Kaloorazi",
            "Rodrigo C. de Lamare"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  An efficient, accurate and reliable approximation of a matrix by one of lower\nrank is a fundamental task in numerical linear algebra and signal processing\napplications. In this paper, we introduce a new matrix decomposition approach\ntermed Subspace-Orbit Randomized singular value decomposition (SOR-SVD), which\nmakes use of random sampling techniques to give an approximation to a low-rank\nmatrix. Given a large and dense data matrix of size $m\\times n$ with numerical\nrank $k$, where $k \\ll \\text{min} \\{m,n\\}$, the algorithm requires a few passes\nthrough data, and can be computed in $O(mnk)$ floating-point operations.\nMoreover, the SOR-SVD algorithm can utilize advanced computer architectures,\nand, as a result, it can be optimized for maximum efficiency. The SOR-SVD\nalgorithm is simple, accurate, and provably correct, and outperforms previously\nreported techniques in terms of accuracy and efficiency. Our numerical\nexperiments support these claims.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.00462v1"
    },
    {
        "title": "Numerical Verification of Affine Systems with up to a Billion Dimensions",
        "authors": [
            "Stanley Bak",
            "Hoang-Dung Tran",
            "Taylor T. Johnson"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Affine systems reachability is the basis of many verification methods. With\nfurther computation, methods exist to reason about richer models with inputs,\nnonlinear differential equations, and hybrid dynamics. As such, the scalability\nof affine systems verification is a prerequisite to scalable analysis for more\ncomplex systems. In this paper, we improve the scalability of affine systems\nverification, in terms of the number of dimensions (variables) in the system.\n  The reachable states of affine systems can be written in terms of the matrix\nexponential, and safety checking can be performed at specific time steps with\nlinear programming. Unfortunately, for large systems with many state variables,\nthis direct approach requires an intractable amount of memory while using an\nintractable amount of computation time. We overcome these challenges by\ncombining several methods that leverage common problem structure. Memory is\nreduced by exploiting initial states that are not full-dimensional and safety\nproperties (outputs) over a few linear projections of the state variables.\nComputation time is saved by using numerical simulations to compute only\nprojections of the matrix exponential relevant for the verification problem.\nSince large systems often have sparse dynamics, we use Krylov-subspace\nsimulation approaches based on the Arnoldi or Lanczos iterations. Our method\nproduces accurate counter-examples when properties are violated and, in the\nextreme case with sufficient problem structure, can analyze a system with one\nbillion real-valued state variables.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01583v3"
    },
    {
        "title": "Numerical analysis of the maximal attainable accuracy in communication\n  hiding pipelined Conjugate Gradient methods",
        "authors": [
            "Siegfried Cools"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Krylov subspace methods are widely known as efficient algebraic methods for\nsolving large scale linear systems. However, on massively parallel hardware the\nperformance of these methods is typically limited by communication latency\nrather than floating point performance. With HPC hardware advancing towards the\nexascale regime the gap between computation and communication keeps steadily\nincreasing, imposing the need for scalable alternatives to traditional Krylov\nsubspace methods. One such approach are the so-called pipelined Krylov subspace\nmethods, which reduce the number of global synchronization points and overlap\nglobal communication latency with local arithmetic operations, thus hiding the\nglobal reduction phases behind useful computations. To obtain this overlap the\ntraditional Krylov subspace algorithm is reformulated by introducing a number\nof auxiliary vector quantities, which are computed using additional recurrence\nrelations. Although pipelined Krylov subspace methods are equivalent to\ntraditional Krylov subspace methods in exact arithmetic, local rounding errors\ninduced by the multi-term recurrence relations in finite precision may in\npractice affect convergence significantly. This numerical stability study aims\nto characterize the effect of local rounding errors on attainable accuracy in\nvarious pipelined versions of the popular Conjugate Gradient method.\nExpressions for the gaps between the true and recursively computed variables\nthat are used to update the search directions in the different CG variants are\nderived. Furthermore, it is shown how these results can be used to analyze and\ncorrect the effect of local rounding error propagation on the maximal\nattainable accuracy of pipelined CG methods. The analysis in this work is\nsupplemented by numerical experiments that demonstrate the numerical behavior\nof the pipelined CG methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.02962v2"
    },
    {
        "title": "Fast and Accurate Tensor Completion with Total Variation Regularized\n  Tensor Trains",
        "authors": [
            "Ching-Yun Ko",
            "Kim Batselier",
            "Wenjian Yu",
            "Ngai Wong"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We propose a new tensor completion method based on tensor trains. The\nto-be-completed tensor is modeled as a low-rank tensor train, where we use the\nknown tensor entries and their coordinates to update the tensor train. A novel\ntensor train initialization procedure is proposed specifically for image and\nvideo completion, which is demonstrated to ensure fast convergence of the\ncompletion algorithm. The tensor train framework is also shown to easily\naccommodate Total Variation and Tikhonov regularization due to their low-rank\ntensor train representations. Image and video inpainting experiments verify the\nsuperiority of the proposed scheme in terms of both speed and scalability,\nwhere a speedup of up to 155X is observed compared to state-of-the-art tensor\ncompletion methods at a similar accuracy. Moreover, we demonstrate the proposed\nscheme is especially advantageous over existing algorithms when only tiny\nportions (say, 1%) of the to-be-completed images/videos are known.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06128v3"
    },
    {
        "title": "Numerical Integration in Multiple Dimensions with Designed Quadrature",
        "authors": [
            "Vahid Keshavarzzadeh",
            "Robert M. Kirby",
            "Akil Narayan"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We present a systematic computational framework for generating positive\nquadrature rules in multiple dimensions on general geometries. A direct\nmoment-matching formulation that enforces exact integration on polynomial\nsubspaces yields nonlinear conditions and geometric constraints on nodes and\nweights. We use penalty methods to address the geometric constraints, and\nsubsequently solve a quadratic minimization problem via the Gauss-Newton\nmethod. Our analysis provides guidance on requisite sizes of quadrature rules\nfor a given polynomial subspace, and furnishes useful user-end stability bounds\non error in the quadrature rule in the case when the polynomial moment\nconditions are violated by a small amount due to, e.g., finite precision\nlimitations or stagnation of the optimization procedure. We present several\nnumerical examples investigating optimal low-degree quadrature rules, Lebesgue\nconstants, and 100-dimensional quadrature. Our capstone examples compare our\nquadrature approach to popular alternatives, such as sparse grids and\nquasi-Monte Carlo methods, for problems in linear elasticity and topology\noptimization.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06501v1"
    },
    {
        "title": "Polar Wavelets in Space",
        "authors": [
            "Christian Lessig"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Recent work introduced a unified framework for steerable and directional\nwavelets in two and three dimensions that ensures many desirable properties,\nsuch as a multi-scale structure, fast transforms, and a flexible angular\nlocalization. We show that, for an appropriate choice for the radial window\nfunction, these wavelets also have closed form expressions for, among other\nthings, the spatial representation, the filter taps for the fast transform, and\nthe frame representation of the Laplace operator. The numerical practicality\nand benefits of our work are demonstrated using signal estimation from\nnon-uniform, point-wise samples, as required for example in ray tracing, and\nfor reconstructing a signal over a lower-dimensional sub-manifold, with\napplications for instance in medical imaging.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.02061v1"
    },
    {
        "title": "Divergence Free Polar Wavelets for the Analysis and Representation of\n  Fluid Flows",
        "authors": [
            "Christian Lessig"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We present a Parseval tight wavelet frame for the representation and analysis\nof velocity vector fields of incompressible fluids. Our wavelets have closed\nform expressions in the frequency and spatial domains, are divergence free in\nthe ideal, analytic sense, have a multi-resolution structure and fast\ntransforms, and an intuitive correspondence to common flow phenomena. Our\nconstruction also allows for well defined directional selectivity, e.g. to\nmodel the behavior of divergence free vector fields in the vicinity of\nboundaries or to represent highly directional features like in a von K\\'arm\\'an\nvortex street. We demonstrate the practicality and efficiency of our\nconstruction by analyzing the representation of different divergence free\nvector fields in our wavelets.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.02062v3"
    },
    {
        "title": "Exponential Integrators with Parallel-in-Time Rational Approximations\n  for the Shallow-Water Equations on the Rotating Sphere",
        "authors": [
            "Martin Schreiber",
            "Nathanaël Schaeffer",
            "Richard Loft"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  High-performance computing trends towards many-core systems are expected to\ncontinue over the next decade. As a result, parallel-in-time methods,\nmathematical formulations which exploit additional degrees of parallelism in\nthe time dimension, have gained increasing interest in recent years. In this\nwork we study a massively parallel rational approximation of exponential\nintegrators (REXI). This method replaces a time integration of stiff linear\noscillatory and diffusive systems by the sum of the solutions of many decoupled\nsystems, which can be solved in parallel. Previous numerical studies showed\nthat this reformulation allows taking arbitrarily long time steps for the\nlinear oscillatory parts.\n  The present work studies the non-linear shallow-water equations on the\nrotating sphere, a simplified system of equations used to study properties of\nspace and time discretization methods in the context of atmospheric\nsimulations. After introducing time integrators, we first compare the time step\nsizes to the errors in the simulation, discussing pros and cons of different\nformulations of REXI. Here, REXI already shows superior properties compared to\nexplicit and implicit time stepping methods. Additionally, we present\nwallclock-time-to-error results revealing the sweet spots of REXI obtaining\neither an over 6x higher accuracy within the same time frame or an about 3x\nreduced time-to-solution for a similar error threshold. Our results motivate\nfurther explorations of REXI for operational weather/climate systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06557v3"
    },
    {
        "title": "Low-Cost Parameterizations of Deep Convolutional Neural Networks",
        "authors": [
            "Eran Treister",
            "Lars Ruthotto",
            "Michal Sharoni",
            "Sapir Zafrani",
            "Eldad Haber"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Convolutional Neural Networks (CNNs) filter the input data using a series of\nspatial convolution operators with compactly supported stencils and point-wise\nnonlinearities. Commonly, the convolution operators couple features from all\nchannels. For wide networks, this leads to immense computational cost in the\ntraining of and prediction with CNNs. In this paper, we present novel ways to\nparameterize the convolution more efficiently, aiming to decrease the number of\nparameters in CNNs and their computational complexity. We propose new\narchitectures that use a sparser coupling between the channels and thereby\nreduce both the number of trainable weights and the computational cost of the\nCNN. Our architectures arise as new types of residual neural network (ResNet)\nthat can be seen as discretizations of a Partial Differential Equations (PDEs)\nand thus have predictable theoretical properties. Our first architecture\ninvolves a convolution operator with a special sparsity structure, and is\napplicable to a large class of CNNs. Next, we present an architecture that can\nbe seen as a discretization of a diffusion reaction PDE, and use it with three\ndifferent convolution operators. We outline in our experiments that the\nproposed architectures, although considerably reducing the number of trainable\nweights, yield comparable accuracy to existing CNNs that are fully coupled in\nthe channel dimension.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.07821v3"
    },
    {
        "title": "Rank Minimization on Tensor Ring: A New Paradigm in Scalable Tensor\n  Decomposition and Completion",
        "authors": [
            "Longhao Yuan",
            "Chao Li",
            "Danilo Mandic",
            "Jianting Cao",
            "Qibin Zhao"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In low-rank tensor completion tasks, due to the underlying multiple\nlarge-scale singular value decomposition (SVD) operations and rank selection\nproblem of the traditional methods, they suffer from high computational cost\nand high sensitivity of model complexity. In this paper, taking advantages of\nhigh compressibility of the recently proposed tensor ring (TR) decomposition,\nwe propose a new model for tensor completion problem. This is achieved through\nintroducing convex surrogates of tensor low-rank assumption on latent tensor\nring factors, which makes it possible for the Schatten norm regularization\nbased models to be solved at much smaller scale. We propose two algorithms\nwhich apply different structured Schatten norms on tensor ring factors\nrespectively. By the alternating direction method of multipliers (ADMM) scheme,\nthe tensor ring factors and the predicted tensor can be optimized\nsimultaneously. The experiments on synthetic data and real-world data show the\nhigh performance and efficiency of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.08468v1"
    },
    {
        "title": "A Robust Iterative Scheme for Symmetric Indefinite Systems",
        "authors": [
            "Murat Manguoglu",
            "Volker Mehrmann"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We propose a two-level nested preconditioned iterative scheme for solving\nsparse linear systems of equations in which the coefficient matrix is symmetric\nand indefinite with relatively small number of negative eigenvalues. The\nproposed scheme consists of an outer Minimum Residual (MINRES) iteration,\npreconditioned by an inner Conjugate Gradient (CG) iteration in which CG can be\nfurther preconditioned. The robustness of the proposed scheme is illustrated by\nsolving indefinite linear systems that arise in the solution of quadratic\neigenvalue problems in the context of model reduction methods for finite\nelement models of disk brakes as well as on other problems that arise in a\nvariety of applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.12417v3"
    },
    {
        "title": "Uncertainty Quantification in Three Dimensional Natural Convection using\n  Polynomial Chaos Expansion and Deep Neural Networks",
        "authors": [
            "Shantanu Shahane",
            "Narayana R. Aluru",
            "Surya Pratap Vanka"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  This paper analyzes the effects of input uncertainties on the outputs of a\nthree dimensional natural convection problem in a differentially heated cubical\nenclosure. Two different cases are considered for parameter uncertainty\npropagation and global sensitivity analysis. In case A, stochastic variation is\nintroduced in the two non-dimensional parameters (Rayleigh and Prandtl numbers)\nwith an assumption that the boundary temperature is uniform. Being a two\ndimensional stochastic problem, the polynomial chaos expansion (PCE) method is\nused as a surrogate model. Case B deals with non-uniform stochasticity in the\nboundary temperature. Instead of the traditional Gaussian process model with\nthe Karhunen-Lo$\\grave{e}$ve expansion, a novel approach is successfully\nimplemented to model uncertainty in the boundary condition. The boundary is\ndivided into multiple domains and the temperature imposed on each domain is\nassumed to be an independent and identically distributed (i.i.d) random\nvariable. Deep neural networks are trained with the boundary temperatures as\ninputs and Nusselt number, internal temperature or velocities as outputs. The\nnumber of domains which is essentially the stochastic dimension is 4, 8, 16 or\n32. Rigorous training and testing process shows that the neural network is able\nto approximate the outputs to a reasonable accuracy. For a high stochastic\ndimension such as 32, it is computationally expensive to fit the PCE. This\npaper demonstrates a novel way of using the deep neural network as a surrogate\nmodeling method for uncertainty quantification with the number of simulations\nmuch fewer than that required for fitting the PCE, thus, saving the\ncomputational cost.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.11934v2"
    },
    {
        "title": "A global sensitivity analysis and reduced order models for\n  hydraulically-fractured horizontal wells",
        "authors": [
            "A. Rezaei",
            "K. B. Nakshatrala",
            "F. Siddiqui",
            "B. Dindoruk",
            "M. Soliman"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We present a systematic global sensitivity analysis using the Sobol method\nwhich can be utilized to rank the variables that affect two quantity of\ninterests -- pore pressure depletion and stress change -- around a\nhydraulically-fractured horizontal well based on their degree of importance.\nThese variables include rock properties and stimulation design variables. A\nfully-coupled poroelastic hydraulic fracture model is used to account for pore\npressure and stress changes due to production. To ease the computational cost\nof a simulator, we also provide reduced order models (ROMs), which can be used\nto replace the complex numerical model with a rather simple analytical model,\nfor calculating the pore pressure and stresses at different locations around\nhydraulic fractures. The main findings of this research are: (i) mobility,\nproduction pressure, and fracture half-length are the main contributors to the\nchanges in the quantities of interest. The percentage of the contribution of\neach parameter depends on the location with respect to pre-existing hydraulic\nfractures and the quantity of interest. (ii) As the time progresses, the effect\nof mobility decreases and the effect of production pressure increases. (iii)\nThese two variables are also dominant for horizontal stresses at large\ndistances from hydraulic fractures. (iv) At zones close to hydraulic fracture\ntips or inside the spacing area, other parameters such as fracture spacing and\nhalf-length are the dominant factors that affect the minimum horizontal stress.\nThe results of this study will provide useful guidelines for the stimulation\ndesign of legacy wells and secondary operations such as refracturing and infill\ndrilling.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.03424v1"
    },
    {
        "title": "Coherence-Based Performance Guarantee of Regularized $\\ell_{1}$-Norm\n  Minimization and Beyond",
        "authors": [
            "Wendong Wang",
            "Feng Zhang",
            "Zhi Wang",
            "Jianjun Wang"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In this paper, we consider recovering the signal $\\bm{x}\\in\\mathbb{R}^{n}$\nfrom its few noisy measurements $\\bm{b}=A\\bm{x}+\\bm{z}$, where\n$A\\in\\mathbb{R}^{m\\times n}$ with $m\\ll n$ is the measurement matrix, and\n$\\bm{z}\\in\\mathbb{R}^{m}$ is the measurement noise/error. We first establish a\ncoherence-based performance guarantee for a regularized $\\ell_{1}$-norm\nminimization model to recover such signals $\\bm{x}$ in the presence of the\n$\\ell_{2}$-norm bounded noise, i.e., $\\|\\bm{z}\\|_{2}\\leq\\epsilon$, and then\nextend these theoretical results to guarantee the robust recovery of the\nsignals corrupted with the Dantzig Selector (DS) type noise, i.e.,\n$\\|A^{T}\\bm{z}\\|_{\\infty}\\leq\\epsilon$, and the structured block-sparse signal\nrecovery in the presence of the bounded noise. To the best of our knowledge, we\nfirst extend nontrivially the sharp uniform recovery condition derived by Cai,\nWang and Xu (2010) for the constrained $\\ell_{1}$-norm minimization model,\nwhich takes the form of \\begin{align*} \\mu<\\frac{1}{2k-1}, \\end{align*} where\n$\\mu$ is defined as the (mutual) coherence of $A$, to two unconstrained\nregularized $\\ell_{1}$-norm minimization models to guarantee the robust\nrecovery of any signals (not necessary to be $k$-sparse) under the\n$\\ell_{2}$-norm bounded noise and the DS type noise settings, respectively.\nBesides, a uniform recovery condition and its two resulting error estimates are\nalso established for the first time to our knowledge, for the robust\nblock-sparse signal recovery using a regularized mixed $\\ell_{2}/\\ell_{1}$-norm\nminimization model, and these results well complement the existing theoretical\ninvestigation on this model which focuses on the non-uniform recovery\nconditions and/or the robust signal recovery in presence of the random noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.03739v1"
    },
    {
        "title": "Application of the Fast Multipole Fully Coupled Poroelastic Displacement\n  Discontinuity Method to Hydraulic Fracturing Problems",
        "authors": [
            "Ali Rezaei",
            "Fahd Siddiqui",
            "Giorgio Bornia",
            "Mohamed Y. Soliman"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In this study, a fast multipole method (FMM) is used to decrease the\ncomputational time of a fully-coupled poroelastic hydraulic fracture model with\na controllable effect on its accuracy. The hydraulic fracture model is based on\nthe poroelastic formulation of the displacement discontinuity method (DDM)\nwhich is a special formulation of the boundary element method (BEM). DDM is a\npowerful and efficient method for problems involving fractures. However, this\nmethod becomes slow as the number of temporal, or spatial elements increases,\nor necessary details such as poroelasticity, that makes the solution\nhistory-dependent, are added to the model. FMM is a technique to expedite\nmatrix-vector multiplications within a controllable error without forming the\nmatrix explicitly. Fully-coupled poroelastic formulation of DDM involves the\nmultiplication of a dense matrix with a vector in several places. A crucial\nmodification to DDM is suggested in two places in the algorithm to leverage the\nspeed efficiency of FMM for carrying out these multiplications. The first\nmodification is in the time-marching scheme, which accounts for the solution of\nprevious time steps to compute the current time step. The second modification\nis in the generalized minimal residual method (GMRES) to iteratively solve for\nthe problem unknowns. Several examples are provided to show the efficiency of\nthe proposed approach in problems with large degrees of freedom (in time and\nspace). Examples include hydraulic fracturing of a horizontal well and randomly\ndistributed pressurized fractures at different orientations with respect to\nhorizontal stresses. The results are compared to the conventional DDM in terms\nof computational processing time and accuracy. Accordingly, the proposed\nalgorithm may be used for fracture propagation studies while substantially\nreducing the processing time with a controllable error.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.05087v2"
    },
    {
        "title": "Reissner-Mindlin shell theory based on tangential differential calculus",
        "authors": [
            "D. Schöllhammer",
            "T. P. Fries"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The linear Reissner-Mindlin shell theory is reformulated in the frame of the\ntangential differential calculus (TDC) using a global Cartesian coordinate\nsystem. The rotation of the normal vector is modelled with a difference vector\napproach. The resulting equations are applicable to both explicitly and\nimplicitly defined shells, because the employed surface operators do not\nnecessarily rely on a parametrization. Hence, shell analysis on surfaces\nimplied by level-set functions is enabled, but also the classical case of\nparametrized surfaces is captured. As a consequence, the proposed TDC-based\nformulation is more general and may also be used in recent finite element\napproaches such as the TraceFEM and CutFEM where a parametrization of the\nmiddle surface is not required. Herein, the numerical results are obtained by\nisogeometric analysis using NURBS as trial and test functions for classical and\nnew benchmark tests. In the residual errors, optimal higher-order convergence\nrates are confirmed when the involved physical fields are sufficiently smooth.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.05596v2"
    },
    {
        "title": "El mètode de les línies per a la resolució numèrica d'equacions\n  en derivades parcials. The method of lines for numerical solutions of partial\n  differential equations",
        "authors": [
            "C. Dalfó",
            "M. A. Fiol"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  In this paper, we describe a semi-discrete method for a numerical resolution\nof a type of partial differential equations, called the method of lines (MOL).\nThis method is based on the discretization of all but one of the variables of\nthe problem. We illustrate this method by solving the Laplace equation in\nCartesian coordinates. We compare the concepts used by the MOL with respect to\nthe analytical method of variable separation. We show that the results obtained\nwith the MOL are very good approximations of the analytical solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.03431v1"
    },
    {
        "title": "Algorithms and Comparisons of Non-negative Matrix Factorization with\n  Volume Regularization for Hyperspectral Unmixing",
        "authors": [
            "M. S. Ang",
            "Nicolas Gillis"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  In this work, we consider nonnegative matrix factorization (NMF) with a\nregularization that promotes small volume of the convex hull spanned by the\nbasis matrix. We present highly efficient algorithms for three different volume\nregularizers, and compare them on endmember recovery in hyperspectral unmixing.\nThe NMF algorithms developed in this work are shown to outperform the\nstate-of-the-art volume-regularized NMF methods, and produce meaningful\ndecompositions on real-world hyperspectral images in situations where\nendmembers are highly mixed (no pure pixels). Furthermore, our extensive\nnumerical experiments show that when the data is highly separable, meaning that\nthere are data points close to the true endmembers, and there are a few\nendmembers, the regularizer based on the determinant of the Gramian produces\nthe best results in most cases. For data that is less separable and/or contains\nmore endmembers, the regularizer based on the logarithm of the determinant of\nthe Gramian performs best in general.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.04362v2"
    },
    {
        "title": "An extended polygonal finite element method for large deformation\n  fracture analysis",
        "authors": [
            "Hai D. Huynh",
            "Phuong Tran",
            "Xiaoying Zhuang",
            "H. Nguyen-Xuan"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  The modeling of large deformation fracture mechanics has been a challenging\nproblem regarding the accuracy of numerical methods and their ability to deal\nwith considerable changes in deformations of meshes where having the presence\nof cracks. This paper further investigates the extended finite element method\n(XFEM) for the simulation of large strain fracture for hyper-elastic materials,\nin particular rubber ones. A crucial idea is to use a polygonal mesh to\nrepresent space of the present numerical technique in advance, and then a local\nrefinement of structured meshes at the vicinity of the discontinuities is\nadditionally established. Due to differences in the size and type of elements\nat the boundaries of those two regions, hanging nodes produced in the modified\nmesh are considered as normal nodes in an arbitrarily polygonal element.\nConforming these special elements becomes straightforward by the flexible use\nof basis functions over polygonal elements. Results of this study are shown\nthrough several numerical examples to prove its efficiency and accuracy through\ncomparison with former achievements.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.05160v2"
    },
    {
        "title": "Numerical Algorithmic Science and Engineering within Computer Science:\n  Rationale, Foundations and Organization",
        "authors": [
            "John Lawrence Nazareth"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  A re-calibration is proposed for \"numerical analysis\" as it arises\nspecifically within the broader, embracing field of modern computer science\n(CS). This would facilitate research into theoretical and practicable models of\nreal-number computation at the foundations of CS, and it would also advance the\ninstructional objectives of the CS field. Our approach is premised on the key\nobservation that the great \"watershed\" in numerical computation is much more\nbetween finite- and infinite-dimensional numerical problems than it is between\ndiscrete and continuous numerical problems. A revitalized discipline for\nnumerical computation within modern CS can more accurately be defined as\n\"numerical algorithmic science & engineering (NAS&E), or more compactly, as\n\"numerical algorithmics,\" its focus being the algorithmic solution of numerical\nproblems that are either discrete, or continuous over a space of finite\ndimension, or a combination of the two. It is the counterpart within modern CS\nof the numerical analysis discipline, whose primary focus is the algorithmic\nsolution of continuous, infinite-dimensional numerical problems and their\nfinite-dimensional approximates, and whose specialists today have largely been\nrepatriated to departments of mathematics. Our detailed overview of NAS&E from\nthe viewpoints of rationale, foundations, and organization is preceded by a\nrecounting of the role played by numerical analysts in the evolution of\nacademic departments of computer science, in order to provide background for\nNAS&E and place the newly-emerging discipline within its larger historical\ncontext.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.08647v1"
    },
    {
        "title": "Multilinear PageRank",
        "authors": [
            "David F. Gleich",
            "Lek-Heng Lim",
            "Yongyang Yu"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this paper, we first extend the celebrated PageRank modification to a\nhigher-order Markov chain. Although this system has attractive theoretical\nproperties, it is computationally intractable for many interesting problems. We\nnext study a computationally tractable approximation to the higher-order\nPageRank vector that involves a system of polynomial equations called\nmultilinear PageRank, which is a type of tensor PageRank vector. It is\nmotivated by a novel \"spacey random surfer\" model, where the surfer remembers\nbits and pieces of history and is influenced by this information. The\nunderlying stochastic process is an instance of a vertex-reinforced random\nwalk. We develop convergence theory for a simple fixed-point method, a shifted\nfixed-point method, and a Newton iteration in a particular parameter regime. In\nmarked contrast to the case of the PageRank vector of a Markov chain where the\nsolution is always unique and easy to compute, there are parameter regimes of\nmultilinear PageRank where solutions are not unique and simple algorithms do\nnot converge. We provide a repository of these non-convergent cases that we\nencountered through exhaustive enumeration and randomly sampling that we\nbelieve is useful for future study of the problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.1465v1"
    },
    {
        "title": "The Hildreth's Algorithm with Applications to Soft Constraints for User\n  Interface Layout",
        "authors": [
            "Noreen Jamil",
            "Xuemei Chen",
            "Alex Cloninger"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  The Hildreth's algorithm is a row action method for solving large systems of\ninequalities. This algorithm is efficient for problems with sparse matrices, as\nopposed to direct methods such as Gaussian elimination or QR-factorization. We\napply the Hildreth's algorithm, as well as a randomized version, along with\nprioritized selection of the inequalities, to efficiently detect the highest\npriority feasible subsystem of equations. We prove convergence results and\nfeasibility criteria for both cyclic and randomized Hildreth's algorithm, as\nwell as a mixed algorithm which uses Hildreth's algorithm for inequalities and\nKaczmarz algorithm for equalities. These prioritized, sparse systems of\ninequalities commonly appear in constraint-based user interface (UI) layout\nspecifications. The performance and convergence of these proposed algorithms\nare evaluated empirically using randomly generated UI layout specifications of\nvarious sizes. The results show that these methods offer improvements in\nperformance over standard methods like Matlab's LINPROG, a well-known efficient\nlinear programming solver, and the recent developed Kaczmarz algorithm with\nprioritized IIS detection.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.2902v1"
    },
    {
        "title": "An improved isogeometric analysis method for trimmed geometries",
        "authors": [
            "Jinlan Xu",
            "Ningning Sun",
            "Laixin Shu",
            "Timon Rabczuk",
            "Gang Xu"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Trimming techniques are efficient ways to generate complex geometries in\nComputer-Aided Design(CAD). In this paper, an improved isogeometric\nanalysis(IGA) method for trimmed geometries is proposed. We will show that the\nproposed method reduces the numerical error of physical solution by 50% for\nsimple trimmed geometries, and the condition number of stiffness matrix is also\ndecreased. Furthermore, the number of integration elements and integration\npoints involved in the solving process can be significantly reduced compared to\nprevious approaches, drastically improving the computational efficiency for IGA\nproblems on the trimmed geometry. Several examples are illustrated to show the\neffectiveness of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.00323v1"
    },
    {
        "title": "Functional approach to the error control in adaptive IgA schemes for\n  elliptic boundary value problems",
        "authors": [
            "Svetlana Matculevich"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This work presents a numerical study of functional type a posteriori error\nestimates for IgA approximation schemes in the context of elliptic\nboundary-value problems. Along with the detailed discussion of the most crucial\nproperties of such estimates, we present the algorithm of a reliable solution\napproximation together with the scheme of efficient a posteriori error bound\ngeneration-based on solving an auxiliary problem with respect to an introduced\nvector-valued variable. In this approach, we take advantage of B-(THB-)spline's\nhigh smoothness for the auxiliary vector function reconstruction, which, at the\nsame time, allows to use much coarser meshes and decrease the number of\nunknowns substantially. The most representative numerical results, obtained\nduring a systematic testing of error estimates, are presented in the second\npart of the paper. The efficiency of the obtained error bounds is analysed from\nboth the error estimation (indication) and the computational expenses points of\nview. Several examples illustrate that functional error estimates\n(alternatively referred to as the majorants and minorants of deviation from an\nexact solution) perform a much sharper error control than, for instance,\nresidual-based error estimates. Simultaneously, assembling and solving the\nroutines for an auxiliary variable reconstruction which generate the majorant\nof an error can be executed several times faster than the routines for a primal\nunknown.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.03201v3"
    },
    {
        "title": "The Normalized Singular Value Decomposition of Non-Symmetric Matrices\n  Using Givens fast Rotations",
        "authors": [
            "Ehsan Rohani",
            "Gwan Choi",
            "Mi Lu"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper we introduce the algorithm and the fixed point hardware to\ncalculate the normalized singular value decomposition of a non-symmetric\nmatrices using Givens fast (approximate) rotations. This algorithm only uses\nthe basic combinational logic modules such as adders, multiplexers, encoders,\nBarrel shifters (B-shifters), and comparators and does not use any lookup\ntable. This method in fact combines the iterative properties of singular value\ndecomposition method and CORDIC method in one single iteration. The introduced\narchitecture is a systolic architecture that uses two different types of\nprocessors, diagonal and non-diagonal processors. The diagonal processor\ncalculates, transmits and applies the horizontal and vertical rotations, while\nthe non-diagonal processor uses a fully combinational architecture to receive,\nand apply the rotations. The diagonal processor uses priority encoders, Barrel\nshifters, and comparators to calculate the rotation angles. Both processors use\na series of adders to apply the rotation angles. The design presented in this\nwork provides $2.83\\sim649$ times better energy per matrix performance compared\nto the state of the art designs. This performance achieved without the\nemployment of pipelining; a better performance advantage is expected to be\nachieved employing pipelining.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05189v1"
    },
    {
        "title": "On the Computation of Neumann Series",
        "authors": [
            "Vassil Dimitrov",
            "Diego Coelho"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This paper proposes new factorizations for computing the Neumann series. The\nfactorizations are based on fast algorithms for small prime sizes series and\nthe splitting of large sizes into several smaller ones. We propose a different\nbasis for factorizations other than the well-known binary and ternary basis. We\nshow that is possible to reduce the overall complexity for the usual binary\ndecomposition from 2log2(N)-2 multiplications to around 1.72log2(N)-2 using a\nbasis of size five. Merging different basis we can demonstrate that we can\nbuild fast algorithms for particular sizes. We also show the asymptotic case\nwhere one can reduce the number of multiplications to around 1.70log2(N)-2.\nSimulations are performed for applications in the context of wireless\ncommunications and image rendering, where is necessary perform large sized\nmatrices inversion.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05846v1"
    },
    {
        "title": "A novel method based on the Tikhonov functional for non-negative\n  solution of a system of linear equations with non-negative coefficients",
        "authors": [
            "Fiks Ilya"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We propose a novel method for a solution of a system of linear equations with\nthe non-negativity condition. The method is based on the Tikhonov functional\nand has better accuracy and stability than other well-known algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.7345v1"
    },
    {
        "title": "Adaptive $h$-refinement for reduced-order models",
        "authors": [
            "Kevin Carlberg"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This work presents a method to adaptively refine reduced-order models \\emph{a\nposteriori} without requiring additional full-order-model solves. The technique\nis analogous to mesh-adaptive $h$-refinement: it enriches the reduced-basis\nspace online by `splitting' a given basis vector into several vectors with\ndisjoint support. The splitting scheme is defined by a tree structure\nconstructed offline via recursive $k$-means clustering of the state variables\nusing snapshot data. The method identifies the vectors to split online using a\ndual-weighted-residual approach that aims to reduce error in an output quantity\nof interest. The resulting method generates a hierarchy of subspaces online\nwithout requiring large-scale operations or full-order-model solves. Further,\nit enables the reduced-order model to satisfy \\emph{any prescribed error\ntolerance} regardless of its original fidelity, as a completely refined\nreduced-order model is mathematically equivalent to the original full-order\nmodel. Experiments on a parameterized inviscid Burgers equation highlight the\nability of the method to capture phenomena (e.g., moving shocks) not contained\nin the span of the original reduced basis.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.0442v3"
    },
    {
        "title": "Projection Algorithms for Non-Convex Minimization with Application to\n  Sparse Principal Component Analysis",
        "authors": [
            "William W. Hager",
            "Dzung T. Phan",
            "Jia-Jie Zhu"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  We consider concave minimization problems over non-convex sets.Optimization\nproblems with this structure arise in sparse principal component analysis. We\nanalyze both a gradient projection algorithm and an approximate Newton\nalgorithm where the Hessian approximation is a multiple of the identity.\nConvergence results are established. In numerical experiments arising in sparse\nprincipal component analysis, it is seen that the performance of the gradient\nprojection algorithm is very similar to that of the truncated power method and\nthe generalized power method. In some cases, the approximate Newton algorithm\nwith a Barzilai-Borwein (BB) Hessian approximation can be substantially faster\nthan the other algorithms, and can converge to a better solution.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.4132v5"
    },
    {
        "title": "Global Newton Iteration over Archimedean and non-Archimedean Fields",
        "authors": [
            "Jonathan D. Hauenstein",
            "Victor Pan",
            "Agnes Szanto"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this paper, we study iterative methods on the coefficients of the rational\nunivariate representation (RUR) of a given algebraic set, called global Newton\niteration. We compare two natural approaches to define locally quadratically\nconvergent iterations: the first one involves Newton iteration applied to the\napproximate roots individually and then interpolation to find the RUR of these\napproximate roots; the second one considers the coefficients in the exact RUR\nas zeroes of a high dimensional map defined by polynomial reduction, and\napplies Newton iteration on this map. We prove that over fields with a p-adic\nvaluation these two approaches give the same iteration function, but over\nfields equipped with the usual Archimedean absolute value, they are not\nequivalent. In the latter case, we give explicitly the iteration function for\nboth approaches. Finally, we analyze the parallel complexity of the different\nversions of the global Newton iteration, compare them, and demonstrate that\nthey can be efficiently computed. The motivation for this study comes from the\ncertification of approximate roots of overdetermined and singular polynomial\nsystems via the recovery of an exact RUR from approximate numerical data.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.5525v1"
    },
    {
        "title": "Proximal Iteratively Reweighted Algorithm with Multiple Splitting for\n  Nonconvex Sparsity Optimization",
        "authors": [
            "Canyi Lu",
            "Yunchao Wei",
            "Zhouchen Lin",
            "Shuicheng Yan"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This paper proposes the Proximal Iteratively REweighted (PIRE) algorithm for\nsolving a general problem, which involves a large body of nonconvex sparse and\nstructured sparse related problems. Comparing with previous iterative solvers\nfor nonconvex sparse problem, PIRE is much more general and efficient. The\ncomputational cost of PIRE in each iteration is usually as low as the\nstate-of-the-art convex solvers. We further propose the PIRE algorithm with\nParallel Splitting (PIRE-PS) and PIRE algorithm with Alternative Updating\n(PIRE-AU) to handle the multi-variable problems. In theory, we prove that our\nproposed methods converge and any limit solution is a stationary point.\nExtensive experiments on both synthesis and real data sets demonstrate that our\nmethods achieve comparative learning performance, but are much more efficient,\nby comparing with previous nonconvex solvers.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.6871v1"
    },
    {
        "title": "Saddle-free Hessian-free Optimization",
        "authors": [
            "Martin Arjovsky"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Nonconvex optimization problems such as the ones in training deep neural\nnetworks suffer from a phenomenon called saddle point proliferation. This means\nthat there are a vast number of high error saddle points present in the loss\nfunction. Second order methods have been tremendously successful and widely\nadopted in the convex optimization community, while their usefulness in deep\nlearning remains limited. This is due to two problems: computational complexity\nand the methods being driven towards the high error saddle points. We introduce\na novel algorithm specially designed to solve these two issues, providing a\ncrucial first step to take the widely known advantages of Newton's method to\nthe nonconvex optimization community, especially in high dimensional settings.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.00059v3"
    },
    {
        "title": "Faster SGD Using Sketched Conditioning",
        "authors": [
            "Alon Gonen",
            "Shai Shalev-Shwartz"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We propose a novel method for speeding up stochastic optimization algorithms\nvia sketching methods, which recently became a powerful tool for accelerating\nalgorithms for numerical linear algebra. We revisit the method of conditioning\nfor accelerating first-order methods and suggest the use of sketching methods\nfor constructing a cheap conditioner that attains a significant speedup with\nrespect to the Stochastic Gradient Descent (SGD) algorithm. While our\ntheoretical guarantees assume convexity, we discuss the applicability of our\nmethod to deep neural networks, and experimentally demonstrate its merits.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.02649v1"
    },
    {
        "title": "Fast Methods for Eikonal Equations: an Experimental Survey",
        "authors": [
            "Javier V. Gomez",
            "David Alvarez",
            "Santiago Garrido",
            "Luis Moreno"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  The Fast Marching Method is a very popular algorithm to compute\ntimes-of-arrival maps (distances map measured in time units). Since their\nproposal in 1995, it has been applied to many different applications such as\nrobotics, medical computer vision, fluid simulation, etc. Many alternatives\nhave been proposed with two main objectives: to reduce its computational time\nand to improve its accuracy. In this paper, we collect the main approaches\nwhich improve the computational time of the standard Fast Marching Method,\nfocusing on single-threaded methods and isotropic environments. 9 different\nmethods are studied under a common mathematical framework and experimentally in\nrepresentative environments: Fast Marching Method with binary heap, Fast\nMarching Method with Fibonacci Heap, Simplified Fast Marching Method, Untidy\nFast Marching Method, Fast Iterative Method, Group Marching Method, Fast\nSweeping Method, Lock Sweeping Method and Double Dynamic Queue Method.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.03771v1"
    },
    {
        "title": "The Isogeometric Nyström Method",
        "authors": [
            "Jürgen Zechner",
            "Benjamin Marussig",
            "Gernot Beer",
            "Thomas-Peter Fries"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this paper the isogeometric Nystr\\\"om method is presented. It's\noutstanding features are: it allows the analysis of domains described by many\ndifferent geometrical mapping methods in computer aided geometric design and it\nrequires only pointwise function evaluations just like isogeometric collocation\nmethods. The analysis of the computational domain is carried out by means of\nboundary integral equations, therefor only the boundary representation is\nrequired. The method is thoroughly integrated into the isogeometric framework.\nFor example, the regularization of the arising singular integrals performed\nwith local correction as well as the interpolation of the pointwise existing\nresults are carried out by means of Bezier elements.\n  The presented isogeometric Nystr\\\"om method is applied to practical problems\nsolved by the Laplace and the Lame-Navier equation. Numerical tests show higher\norder convergence in two and three dimensions. It is concluded that the\npresented approach provides a simple and flexible alternative to currently used\nmethods for solving boundary integral equations, but has some limitations.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.03914v1"
    },
    {
        "title": "Efficient algorithm for computing large scale systems of differential\n  algebraic equations",
        "authors": [
            "Xiaolin Qin",
            "Juan Tang",
            "Yong Feng",
            "Bernhard Bachmann",
            "Peter Fritzson"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In many mathematical models of physical phenomenons and engineering fields,\nsuch as electrical circuits or mechanical multibody systems, which generate the\ndifferential algebraic equations (DAEs) systems naturally. In general, the\nfeature of DAEs is a sparse large scale system of fully nonlinear and high\nindex. To make use of its sparsity, this paper provides a simple and efficient\nalgorithm for computing the large scale DAEs system. We exploit the shortest\naugmenting path algorithm for finding maximum value transversal (MVT) as well\nas block triangular forms (BTF). We also present the extended signature matrix\nmethod with the block fixed point iteration and its complexity results.\nFurthermore, a range of nontrivial problems are demonstrated by our algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.03963v1"
    },
    {
        "title": "Tensor Deflation for CANDECOMP/PARAFAC. Part 3: Rank Splitting",
        "authors": [
            "Anh-Huy Phan",
            "Petr Tichavsky",
            "Andrzej Cichocki"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  CANDECOMP/PARAFAC (CPD) approximates multiway data by sum of rank-1 tensors.\nOur recent study has presented a method to rank-1 tensor deflation, i.e.\nsequential extraction of the rank-1 components. In this paper, we extend the\nmethod to block deflation problem. When at least two factor matrices have full\ncolumn rank, one can extract two rank-1 tensors simultaneously, and rank of the\ndata tensor is reduced by 2. For decomposition of order-3 tensors of size R x R\nx R and rank-R, the block deflation has a complexity of O(R^3) per iteration\nwhich is lower than the cost O(R^4) of the ALS algorithm for the overall CPD.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.04971v1"
    },
    {
        "title": "Nonuniformly weighted Schwarz smoothers for spectral element multigrid",
        "authors": [
            "Joerg Stiller"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  A hybrid Schwarz/multigrid method for spectral element solvers to the Poisson\nequation in $\\mathbb R^2$ is presented. It extends the additive Schwarz method\nstudied by J. Lottes and P. Fischer (J. Sci. Comput. 24:45--78, 2005) by\nintroducing nonuniform weight distributions based on the smoothed sign\nfunction. Using a V-cycle with only one pre-smoothing, the new method attains\nlogarithmic convergence rates in the range from 1.2 to 1.9, which corresponds\nto residual reductions of almost two orders of magnitude. Compared to the\noriginal method, it reduces the iteration count by a factor of 1.5 to 3,\nleading to runtime savings of about 50 percent. In numerical experiments the\nmethod proved robust with respect to the mesh size and polynomial orders up to\n32. Used as a preconditioner for the (inexact) CG method it is also suited for\nanisotropic meshes and easily extended to diffusion problems with variable\ncoefficients.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.02390v2"
    },
    {
        "title": "Histogram Arithmetic under Uncertainty of Probability Density Function",
        "authors": [
            "V. N. Petrushin",
            "E. V. Nikulchev",
            "D. A. Korolev"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this article we propose a method of performing arithmetic operations on\nvaria-bles with unknown distribution. The approach to the evaluation results of\narithme-tic operations can select probability intervals of the algebraic\nequations and their systems solutions, of differential equations and their\nsystems in case of histogram evaluation of the empirical density distributions\nof random parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.03251v1"
    },
    {
        "title": "Adaptive multi-stage integrators for optimal energy conservation in\n  molecular simulations",
        "authors": [
            "Mario Fernández-Pendás",
            "Elena Akhmatskaya",
            "J. M. Sanz-Serna"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We introduce a new Adaptive Integration Approach (AIA) to be used in a wide\nrange of molecular simulations. Given a simulation problem and a step size, the\nmethod automatically chooses the optimal scheme out of an available family of\nnumerical integrators. Although we focus on two-stage splitting integrators,\nthe idea may be used with more general families. In each instance, the\nsystem-specific integrating scheme identified by our approach is optimal in the\nsense that it provides the best conservation of energy for harmonic forces. The\nAIA method has been implemented in the BCAM-modified GROMACS software package.\nNumerical tests in molecular dynamics and hybrid Monte Carlo simulations of\nconstrained and unconstrained physical systems show that the method\nsuccessfully realises the fail-safe strategy. In all experiments, and for each\nof the criteria employed, the AIA is at least as good as, and often\nsignificantly outperforms the standard Verlet scheme, as well as fixed\nparameter, optimized two-stage integrators. In particular, the sampling\nefficiency found in simulations using the AIA is up to 5 times better than the\none achieved with other tested schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.03335v2"
    },
    {
        "title": "On Approximating Univariate NP-Hard Integrals",
        "authors": [
            "Ohad Asor",
            "Avishy Carmi"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Approximating a definite integral of product of cosines to within an accuracy\nof n binary digits where the integrand depends on input integers x[k] given in\nbinary radix, is equivalent to counting the number of equal-sum partitions of\nthe integers and is thus a #P problem. Similarly, integrating this function\nfrom zero to infinity and deciding whether the result is either zero or\ninfinity is an NP-Complete problem. Efficient numerical integration methods\nsuch as the double exponential formula and the sinc approximation have been\naround since the mid 70's. Noting the hardness of approximating the integral we\nargue that the proven rates of convergence of such methods cannot possibly be\ncorrect since they give rise to an anomalous result as P=#P.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.08716v3"
    },
    {
        "title": "On the Computation of Complex-valued Gradients with Application to\n  Statistically Optimum Beamforming",
        "authors": [
            "Christoph Boeddeker",
            "Patrick Hanebrink",
            "Lukas Drude",
            "Jahn Heymann",
            "Reinhold Haeb-Umbach"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This report describes the computation of gradients by algorithmic\ndifferentiation for statistically optimum beamforming operations. Especially\nthe derivation of complex-valued functions is a key component of this approach.\nTherefore the real-valued algorithmic differentiation is extended via the\ncomplex-valued chain rule. In addition to the basic mathematic operations the\nderivative of the eigenvalue problem with complex-valued eigenvectors is one of\nthe key results of this report. The potential of this approach is shown with\nexperimental results on the CHiME-3 challenge database. There, the beamforming\ntask is used as a front-end for an ASR system. With the developed derivatives a\njoint optimization of a speech enhancement and speech recognition system w.r.t.\nthe recognition optimization criterion is possible.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.00392v2"
    },
    {
        "title": "Unconstrained inverse quadratic programming problem",
        "authors": [
            "E. G. Abramov"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The paper covers a formulation of the inverse quadratic programming problem\nin terms of unconstrained optimization where it is required to find the unknown\nparameters (the matrix of the quadratic form and the vector of the quasi-linear\npart of the quadratic form) provided that approximate estimates of the optimal\nsolution of the direct problem and those of the target function to be minimized\nin the form of pairs of values lying in the corresponding neighborhoods are\nonly known. The formulation of the inverse problem and its solution are based\non the least squares method. In the explicit form the inverse problem solution\nhas been derived in the form a system of linear equations. The parameters\nobtained can be used for reconstruction of the direct quadratic programming\nproblem and determination of the optimal solution and the extreme value of the\ntarget function, which were not known formerly. It is possible this approach\nopens new ways in over applications, for example, in neurocomputing and quadric\nsurfaces fitting. Simple numerical examples have been demonstrated. A scenario\nin the Octave/MATLAB programming language has been proposed for practical\nimplementation of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.01477v1"
    },
    {
        "title": "Error modeling for surrogates of dynamical systems using machine\n  learning",
        "authors": [
            "Sumeet Trehan",
            "Kevin Carlberg",
            "Louis J. Durlofsky"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  A machine-learning-based framework for modeling the error introduced by\nsurrogate models of parameterized dynamical systems is proposed. The framework\nentails the use of high-dimensional regression techniques (e.g., random\nforests, LASSO) to map a large set of inexpensively computed `error indicators'\n(i.e., features) produced by the surrogate model at a given time instance to a\nprediction of the surrogate-model error in a quantity of interest (QoI). This\neliminates the need for the user to hand-select a small number of informative\nfeatures. The methodology requires a training set of parameter instances at\nwhich the time-dependent surrogate-model error is computed by simulating both\nthe high-fidelity and surrogate models. Using these training data, the method\nfirst determines regression-model locality (via classification or clustering),\nand subsequently constructs a `local' regression model to predict the\ntime-instantaneous error within each identified region of feature space. We\nconsider two uses for the resulting error model: (1) as a correction to the\nsurrogate-model QoI prediction at each time instance, and (2) as a way to\nstatistically model arbitrary functions of the time-dependent surrogate-model\nerror (e.g., time-integrated errors). We apply the proposed framework to model\nerrors in reduced-order models of nonlinear oil--water subsurface flow\nsimulations. The reduced-order models used in this work entail application of\ntrajectory piecewise linearization with proper orthogonal decomposition. When\nthe first use of the method is considered, numerical experiments demonstrate\nconsistent improvement in accuracy in the time-instantaneous QoI prediction\nrelative to the original surrogate model, across a large number of test cases.\nWhen the second use is considered, results show that the proposed method\nprovides accurate statistical predictions of the time- and well-averaged\nerrors.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.03240v2"
    },
    {
        "title": "Space-time balancing domain decomposition",
        "authors": [
            "Santiago Badia",
            "Marc Olm"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this work, we propose two-level space-time domain decomposition\npreconditioners for parabolic problems discretized using finite elements. They\nare motivated as an extension to space-time of balancing domain decomposition\nby constraints preconditioners. The key ingredients to be defined are the\nsub-assembled space and operator, the coarse degrees of freedom (DOFs) in which\nwe want to enforce continuity among subdomains at the preconditioner level, and\nthe transfer operator from the sub-assembled to the original finite element\nspace. With regard to the sub-assembled operator, a perturbation of the time\nderivative is needed to end up with a well-posed preconditioner. The set of\ncoarse DOFs includes the time average (at the space-time subdomain) of\nclassical space constraints plus new constraints between consecutive subdomains\nin time. Numerical experiments show that the proposed schemes are weakly\nscalable in time, i.e., we can efficiently exploit increasing computational\nresources to solve more time steps in the same {total elapsed} time. Further,\nthe scheme is also weakly space-time scalable, since it leads to asymptotically\nconstant iterations when solving larger problems both in space and time.\nExcellent {wall clock} time weak scalability is achieved for space-time\nparallel solvers on some thousands of cores.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.03477v1"
    },
    {
        "title": "The Adaptive $s$-step Conjugate Gradient Method",
        "authors": [
            "Erin Carson"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  On modern large-scale parallel computers, the performance of Krylov subspace\niterative methods is limited by global synchronization. This has inspired the\ndevelopment of $s$-step Krylov subspace method variants, in which iterations\nare computed in blocks of $s$, which can reduce the number of global\nsynchronizations per iteration by a factor of $O(s)$.\n  Although the $s$-step variants are mathematically equivalent to their\nclassical counterparts, they can behave quite differently in finite precision\ndepending on the parameter $s$. If $s$ is chosen too large, the $s$-step method\ncan suffer a convergence delay and a decrease in attainable accuracy relative\nto the classical method. This makes it difficult for a potential user of such\nmethods - the $s$ value that minimizes the time per iteration may not be the\nbest $s$ for minimizing the overall time-to-solution, and further may cause an\nunacceptable decrease in accuracy.\n  Towards improving the reliability and usability of $s$-step Krylov subspace\nmethods, in this work we derive the \\emph{adaptive $s$-step CG method}, a\nvariable $s$-step CG method where in block $k$, the parameter $s_k$ is\ndetermined automatically such that a user-specified accuracy is attainable. The\nmethod for determining $s_k$ is based on a bound on growth of the residual gap\nwithin block $k$, from which we derive a constraint on the condition numbers of\nthe computed $O(s_k)$-dimensional Krylov subspace bases. The computations\nrequired for determining the block size $s_k$ can be performed without\nincreasing the number of global synchronizations per block. Our numerical\nexperiments demonstrate that the adaptive $s$-step CG method is able to attain\nup to the same accuracy as classical CG while still significantly reducing the\ntotal number of global synchronizations.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.03989v1"
    },
    {
        "title": "An hp-adaptive strategy for elliptic problems",
        "authors": [
            "Hui Liu",
            "Tao Cui",
            "Wei Leng",
            "Linbo Zhang"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper a new hp-adaptive strategy for elliptic problems based on\nrefinement history is proposed, which chooses h-, p- or hp-refinement on\nindividual elements according to a posteriori error estimate, as well as\nsmoothness estimate of the solution obtained by comparing the actual and\nexpected error reduction rate. Numerical experiments show that exponential\nconvergence can be achieved with this strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06920v1"
    },
    {
        "title": "On the optimality and sharpness of Laguerre's lower bound on the\n  smallest eigenvalue of a symmetric positive definite matrix",
        "authors": [
            "Yusaku Yamamoto"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Lower bounds on the smallest eigenvalue of a symmetric positive definite\nmatrices $A\\in\\mathbb{R}^{m\\times m}$ play an important role in condition\nnumber estimation and in iterative methods for singular value computation. In\nparticular, the bounds based on ${\\rm Tr}(A^{-1})$ and ${\\rm Tr}(A^{-2})$\nattract attention recently because they can be computed in $O(m)$ work when $A$\nis tridiagonal. In this paper, we focus on these bounds and investigate their\nproperties in detail. First, we consider the problem of finding the optimal\nbound that can be computed solely from ${\\rm Tr}(A^{-1})$ and ${\\rm\nTr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one\nin terms of sharpness. Next, we study the gap between the Laguerre bound and\nthe smallest eigenvalue. We characterize the situation in which the gap becomes\nlargest in terms of the eigenvalue distribution of $A$ and show that the gap\nbecomes smallest when ${\\rm Tr}(A^{-2})/\\{{\\rm Tr}(A^{-1})\\}^2$ approaches 1 or\n$\\frac{1}{m}$. These results will be useful, for example, in designing\nefficient shift strategies for singular value computation algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.00108v1"
    },
    {
        "title": "Hybrid System Modelling and Simulation with Dirac Deltas",
        "authors": [
            "Cláudio Gomes",
            "Yentl Van Tendeloo",
            "Joachim Denil",
            "Paul De Meulenaere",
            "Hans Vangheluwe"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  For a wide variety of problems, creating detailed continuous models of\n(continuous) physical systems is, at the very least, impractical. Hybrid models\ncan abstract away short transient behaviour (thus introducing discontinuities)\nin order to simplify the study of such systems. For example, when modelling a\nbouncing ball, the bounce can be abstracted as a discontinuous change of the\nvelocity, instead of resorting to the physics of the ball (de-)compression to\nkeep the velocity signal continuous. Impulsive differential equations can be\nused to model and simulate hybrid systems such as the bouncing ball. In this\napproach, the force acted on the ball by the floor is abstracted as an\ninfinitely large function in an infinitely small interval of time, that is, an\nimpulse. Current simulators cannot handle such approximations well due to the\nlimitations of machine precision.\n  In this paper, we explore the simulation of impulsive differential equations,\nwhere impulses are first class citizens. We present two approaches for the\nsimulation of impulses: symbolic and numerical. Our contribution is a\ntheoretically founded description of the implementation of both approaches in a\nCausal Block Diagram modelling and simulation tool. Furthermore, we investigate\nthe conditions for which one approach is better than the other.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.04274v1"
    },
    {
        "title": "Efficient Preconditioning for Noisy Separable NMFs by Successive\n  Projection Based Low-Rank Approximations",
        "authors": [
            "Tomohiko Mizutani",
            "Mirai Tanaka"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The successive projection algorithm (SPA) can quickly solve a nonnegative\nmatrix factorization problem under a separability assumption. Even if noise is\nadded to the problem, SPA is robust as long as the perturbations caused by the\nnoise are small. In particular, robustness against noise should be high when\nhandling the problems arising from real applications. The preconditioner\nproposed by Gillis and Vavasis (2015) makes it possible to enhance the noise\nrobustness of SPA. Meanwhile, an additional computational cost is required. The\nconstruction of the preconditioner contains a step to compute the top-$k$\ntruncated singular value decomposition of an input matrix. It is known that the\ndecomposition provides the best rank-$k$ approximation to the input matrix; in\nother words, a matrix with the smallest approximation error among all matrices\nof rank less than $k$. This step is an obstacle to an efficient implementation\nof the preconditioned SPA.\n  To address the cost issue, we propose a modification of the algorithm for\nconstructing the preconditioner. Although the original algorithm uses the best\nrank-$k$ approximation, instead of it, our modification uses an alternative.\nIdeally, this alternative should have high approximation accuracy and low\ncomputational cost. To ensure this, our modification employs a rank-$k$\napproximation produced by an SPA based algorithm. We analyze the accuracy of\nthe approximation and evaluate the computational cost of the algorithm. We then\npresent an empirical study revealing the actual performance of the SPA based\nrank-$k$ approximation algorithm and the modified preconditioned SPA.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.00387v1"
    },
    {
        "title": "Solutions of Quadratic First-Order ODEs applied to Computer Vision\n  Problems",
        "authors": [
            "David Casillas-Perez",
            "Daniel Pizarro",
            "Manuel Mazo",
            "Adrien Bartoli"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This article is a study about the existence and the uniqueness of solutions\nof a specific quadratic first-order ODE that frequently appears in multiple\nreconstruction problems. It is called the \\emph{planar-perspective equation}\ndue to the duality with the geometric problem of reconstruction of\nplanar-perspective curves from their modulus. Solutions of the\n\\emph{planar-perspective equation} are related with planar curves parametrized\nwith perspective parametrization due to this geometric interpretation. The\narticle proves the existence of only two local solutions to the \\emph{initial\nvalue problem} with \\emph{regular initial conditions} and a maximum of two\nanalytic solutions with \\emph{critical initial conditions}. The article also\ngives theorems to extend the local definition domain where the existence of\nboth solutions are guaranteed. It introduces the \\emph{maximal depth function}\nas a function that upper-bound all possible solutions of the\n\\emph{planar-perspective equation} and contains all its possible \\emph{critical\npoints}. Finally, the article describes the \\emph{maximal-depth solution\nproblem} that consists of finding the solution of the referred equation that\nhas maximum the depth and proves its uniqueness. It is an important problem as\nit does not need initial conditions to obtain the unique solution and its the\nfrequent solution that practical algorithms of the state-of-the-art give.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.04265v3"
    },
    {
        "title": "A Short Note on Improved ROSETA",
        "authors": [
            "Hassan Mansour"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This note presents a more efficient formulation of the robust online subspace\nestimation and tracking algorithm (ROSETA) that is capable of identifying and\ntracking a time-varying low dimensional subspace from incomplete measurements\nand in the presence of sparse outliers. The algorithm minimizes a robust l1\nnorm cost function between the observed measurements and their projection onto\nthe estimated subspace. The projection coefficients and sparse outliers are\ncomputed using a LASSO solver and the subspace estimate is updated using a\nproximal point iteration with adaptive parameter selection.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.05961v1"
    },
    {
        "title": "Scalable Robust Matrix Factorization with Nonconvex Loss",
        "authors": [
            "Quanming Yao",
            "James T. Kwok"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Robust matrix factorization (RMF), which uses the $\\ell_1$-loss, often\noutperforms standard matrix factorization using the $\\ell_2$-loss, particularly\nwhen outliers are present. The state-of-the-art RMF solver is the RMF-MM\nalgorithm, which, however, cannot utilize data sparsity. Moreover, sometimes\neven the (convex) $\\ell_1$-loss is not robust enough. In this paper, we propose\nthe use of nonconvex loss to enhance robustness. To address the resultant\ndifficult optimization problem, we use majorization-minimization (MM)\noptimization and propose a new MM surrogate. To improve scalability, we exploit\ndata sparsity and optimize the surrogate via its dual with the accelerated\nproximal gradient algorithm. The resultant algorithm has low time and space\ncomplexities and is guaranteed to converge to a critical point. Extensive\nexperiments demonstrate its superiority over the state-of-the-art in terms of\nboth accuracy and scalability.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.07205v4"
    },
    {
        "title": "Constrained Optimisation of Rational Functions for Accelerating Subspace\n  Iteration",
        "authors": [
            "Konrad Kollnig"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Earlier this decade, the so-called FEAST algorithm was released for computing\nthe eigenvalues of a matrix in a given interval. Previously, rational filter\nfunctions have been examined as a parameter of FEAST. In this thesis, we expand\non existing work with the following contributions: (i) Obtaining\nwell-performing rational filter functions via standard minimisation algorithms,\n(ii) Obtaining constrained rational filter functions efficiently, and (iii)\nImproving existing rational filter functions algorithmically. Using our new\nrational filter functions, FEAST requires up to one quarter fewer iterations on\naverage compared to state-of-art rational filter functions.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.07771v1"
    },
    {
        "title": "Sparse Grid Discretizations based on a Discontinuous Galerkin Method",
        "authors": [
            "Alexander B. Atanasov",
            "Erik Schnetter"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We examine and extend Sparse Grids as a discretization method for partial\ndifferential equations (PDEs). Solving a PDE in $D$ dimensions has a cost that\ngrows as $O(N^D)$ with commonly used methods. Even for moderate $D$ (e.g.\n$D=3$), this quickly becomes prohibitively expensive for increasing problem\nsize $N$. This effect is known as the Curse of Dimensionality. Sparse Grids\noffer an alternative discretization method with a much smaller cost of $O(N\n\\log^{D-1}N)$. In this paper, we introduce the reader to Sparse Grids, and\nextend the method via a Discontinuous Galerkin approach. We then solve the\nscalar wave equation in up to $6+1$ dimensions, comparing cost and accuracy\nbetween full and sparse grids. Sparse Grids perform far superior, even in three\ndimensions. Our code is freely available as open source, and we encourage the\nreader to reproduce the results we show.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.09356v1"
    },
    {
        "title": "Asymptotic metrics on the space of matrices under the commutation\n  relation",
        "authors": [
            "Klaus Glashoff",
            "Michael M. Bronstein"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We show that the norm of the commutator defines \"almost a metric\" on the\nquotient space of commuting matrices, in the sense that it is a semi-metric\nsatisfying the triangle inequality asymptotically for large matrices drawn from\na \"good\" distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.2384v3"
    },
    {
        "title": "Computing Solution Operators of Boundary-value Problems for Some Linear\n  Hyperbolic Systems of PDEs",
        "authors": [
            "Svetlana Selivanova",
            "Victor Selivanov"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We discuss possibilities of application of Numerical Analysis methods to\nproving computability, in the sense of the TTE approach, of solution operators\nof boundary-value problems for systems of PDEs. We prove computability of the\nsolution operator for a symmetric hyperbolic system with computable real\ncoefficients and dissipative boundary conditions, and of the Cauchy problem for\nthe same system (we also prove computable dependence on the coefficients) in a\ncube $Q\\subseteq\\mathbb R^m$. Such systems describe a wide variety of physical\nprocesses (e.g. elasticity, acoustics, Maxwell equations). Moreover, many\nboundary-value problems for the wave equation also can be reduced to this case,\nthus we partially answer a question raised in Weihrauch and Zhong (2002).\nCompared with most of other existing methods of proving computability for PDEs,\nthis method does not require existence of explicit solution formulas and is\nthus applicable to a broader class of (systems of) equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.2494v3"
    },
    {
        "title": "Local error estimates for adaptive simulation of the Reaction-Diffusion\n  Master Equation via operator splitting",
        "authors": [
            "Andreas Hellander",
            "Michael Lawson",
            "Brian Drawert",
            "Linda Petzold"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The efficiency of exact simulation methods for the reaction-diffusion master\nequation (RDME) is severely limited by the large number of diffusion events if\nthe mesh is fine or if diffusion constants are large. Furthermore, inherent\nproperties of exact kinetic-Monte Carlo simulation methods limit the efficiency\nof parallel implementations. Several approximate and hybrid methods have\nappeared that enable more efficient simulation of the RDME. A common feature to\nmost of them is that they rely on splitting the system into its reaction and\ndiffusion parts and updating them sequentially over a discrete timestep. This\nuse of operator splitting enables more efficient simulation but it comes at the\nprice of a temporal discretization error that depends on the size of the\ntimestep. So far, existing methods have not attempted to estimate or control\nthis error in a systematic manner. This makes the solvers hard to use for\npractitioners since they must guess an appropriate timestep. It also makes the\nsolvers potentially less efficient than if the timesteps are adapted to control\nthe error. Here, we derive estimates of the local error and propose a strategy\nto adaptively select the timestep when the RDME is simulated via a first order\noperator splitting. While the strategy is general and applicable to a wide\nrange of approximate and hybrid methods, we exemplify it here by extending a\npreviously published approximate method, the Diffusive Finite-State Projection\n(DFSP) method, to incorporate temporal adaptivity.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.3639v1"
    },
    {
        "title": "On multi-time-step monolithic coupling algorithms for elastodynamics",
        "authors": [
            "S. Karimi",
            "K. B. Nakshatrala"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We present a way of constructing multi-time-step monolithic coupling methods\nfor elastodynamics. The governing equations for constrained multiple subdomains\nare written in dual Schur form and enforce the continuity of velocities at\nsystem time levels. The resulting equations will be in the form of\ndifferential-algebraic equations. To crystallize the ideas we shall employ\nNewmark family of time-stepping schemes. The proposed method can handle\nmultiple subdomains, and allows different time-steps as well as different time\nstepping schemes from the Newmark family in different subdomains. We shall use\nthe energy method to assess the numerical stability, and quantify the influence\nof perturbations under the proposed coupling method. We also discuss the\nconditions under which the proposed method will be energy preserving, and the\nconditions under which the method will be energy conserving. Several numerical\nexamples are presented to illustrate the accuracy and stability properties of\nthe proposed method. We shall also compare the proposed multi-time-step\ncoupling method with some other similar methods available in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.6355v2"
    },
    {
        "title": "An adaptive time integration strategy based on displacement history\n  curvature",
        "authors": [
            "E. N. Lages",
            "E. S. S. Silveira",
            "D. T. Cintra",
            "A. C. Frery"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  This work introduces a time-adaptive strategy that uses a refinement\nestimator based on the first Frenet curvature. In dynamics, a time-adaptive\nstrategy is a mechanism that interactively proposes changes to the time step\nused in iterative methods of solution. These changes aim to improve the\nrelation between quality of response and computational cost. The method here\nproposed is suitable for a variety of numerical time integration problems,\ne.g., in the study of bodies subjected to dynamical loads. The motion equation\nin its space-discrete form is used as reference to derive the formulation\npresented in this paper. Our method is contrasted with other ones based on\nlocal error estimator and apparent frequencies. We check the performance of our\nproposal when employed with the central difference, the explicit\ngeneralized-alpha and the Chung-Lee integration methods. The proposed\nrefinement estimator demands low computational resources, being easily applied\nto several direct integration methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.6857v1"
    },
    {
        "title": "Interval Enclosures of Upper Bounds of Roundoff Errors using\n  Semidefinite Programming",
        "authors": [
            "Victor Magron"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  A longstanding problem related to floating-point implementation of numerical\nprograms is to provide efficient yet precise analysis of output errors.\n  We present a framework to compute lower bounds on largest absolute roundoff\nerrors, for a particular rounding model. This method applies to numerical\nprograms implementing polynomial functions with box constrained input\nvariables. Our study is based on three different hierarchies, relying\nrespectively on generalized eigenvalue problems, elementary computations and\nsemidefinite programming (SDP) relaxations. This is complementary of\nover-approximation frameworks, consisting of obtaining upper bounds on the\nlargest absolute roundoff error. Combining the results of both frameworks\nallows to get enclosures for upper bounds on roundoff errors.\n  The under-approximation framework provided by the third hierarchy is based on\na new sequence of convergent robust SDP approximations for certain classes of\npolynomial optimization problems. Each problem in this hierarchy can be solved\nexactly via SDP. By using this hierarchy, one can provide a monotone\nnon-decreasing sequence of lower bounds converging to the absolute roundoff\nerror of a program implementing a polynomial function, applying for a\nparticular rounding model.\n  We investigate the efficiency and precision of our method on non-trivial\npolynomial programs coming from space control, optimization and computational\nbiology.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.01318v4"
    },
    {
        "title": "Accelerated Stochastic ADMM with Variance Reduction",
        "authors": [
            "Chao Zhang",
            "Zebang Shen",
            "Hui Qian",
            "Tengfei Zhou",
            "Jianya Zhou",
            "Jianying Zhou"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Alternating Direction Method of Multipliers (ADMM) is a popular method for\nsolving large-scale Machine Learning problems. Stochastic ADMM was proposed to\nreduce the per iteration computational complexity, which is more suitable for\nbig data problems. Recently, variance reduction techniques have been integrated\nwith stochastic ADMM in order to get a faster convergence rate, such as\nSAG-ADMM and SVRG-ADMM. However, their convergence rate is still suboptimal\nw.r.t the smoothness constant. In this paper, we propose an accelerated\nstochastic ADMM algorithm with variance reduction, which enjoys a faster\nconvergence than all the existing stochastic ADMM algorithms. We theoretically\nanalyse its convergence rate and show its dependence on the smoothness constant\nis optimal. We also empirically validate its effectiveness and show its\npriority over other stochastic ADMM algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.04074v4"
    },
    {
        "title": "How to overcome the Courant-Friedrichs-Lewy condition of explicit\n  discretizations?",
        "authors": [
            "Denys Dutykh"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  This manuscript contains some thoughts on the discretization of the classical\nheat equation. Namely, we discuss the advantages and disadvantages of explicit\nand implicit schemes. Then, we show how to overcome some disadvantages while\npreserving some advantages. However, since there is no free lunch, there is a\nprice to pay for any improvement in the numerical scheme. This price will be\nthoroughly discussed below. In particular, we like explicit discretizations for\nthe ease of their implementation even for nonlinear problems. Unfortunately,\nwhen these schemes are applied to parabolic equations, severe stability limits\nappear for the time step magnitude making the explicit simulations\nprohibitively expensive. Implicit schemes remove the stability limit, but each\ntime step requires now the solution of linear (at best) or even nonlinear\nsystems of equations. However, there exists a number of tricks to overcome (or\nat least to relax) severe stability limitations of explicit schemes without\ngoing into the trouble of fully implicit ones. The purpose of this manuscript\nis just to inform the readers about these alternative techniques to extend the\nstability limits. It was not written for classical scientific publication\npurposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.09646v2"
    },
    {
        "title": "Robust Multigrid for Cartesian Interior Penalty DG Formulations of the\n  Poisson Equation in 3D",
        "authors": [
            "Joerg Stiller"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We present a polynomial multigrid method for the nodal interior penalty\nformulation of the Poisson equation on three-dimensional Cartesian grids. Its\nkey ingredient is a weighted overlapping Schwarz smoother operating on\nelement-centered subdomains. The MG method reaches superior convergence rates\ncorresponding to residual reductions of about two orders of magnitude within a\nsingle V(1,1) cycle. It is robust with respect to the mesh size and the ansatz\norder, at least up to ${P=32}$. Rigorous exploitation of tensor-product\nfactorization yields a computational complexity of $O(PN)$ for $N$ unknowns,\nwhereas numerical experiments indicate even linear runtime scaling. Moreover,\nby allowing adjustable subdomain overlaps and adding Krylov acceleration, the\nmethod proved feasible for anisotropic grids with element aspect ratios up to\n48.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.04796v1"
    },
    {
        "title": "Construction and implementation of asymptotic expansions for\n  Laguerre-type orthogonal polynomials",
        "authors": [
            "Daan Huybrechs",
            "Peter Opsomer"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Laguerre and Laguerre-type polynomials are orthogonal polynomials on the\ninterval $[0,\\infty)$ with respect to a weight function of the form $w(x) =\nx^{\\alpha} e^{-Q(x)}, Q(x) = \\sum_{k=0}^m q_k x^k, \\alpha > -1, q_m > 0$. The\nclassical Laguerre polynomials correspond to $Q(x)=x$. The computation of\nhigher-order terms of the asymptotic expansions of these polynomials for large\ndegree becomes quite complicated, and a full description seems to be lacking in\nliterature. However, this information is implicitly available in the work of\nVanlessen, based on a non-linear steepest descent analysis of an associated\nso-called Riemann--Hilbert problem. We will extend this work and show how to\nefficiently compute an arbitrary number of higher-order terms in the asymptotic\nexpansions of Laguerre and Laguerre-type polynomials. This effort is similar to\nthe case of Jacobi and Jacobi-type polynomials in a previous paper. We supply\nan implementation with explicit expansions in four different regions of the\ncomplex plane. These expansions can also be extended to Hermite-type weights of\nthe form $\\exp(-\\sum_{k=0}^m q_k x^{2k})$ on $(-\\infty,\\infty)$, and to general\nnon-polynomial functions $Q(x)$ using contour integrals. The expansions may be\nused, e.g., to compute Gauss-Laguerre quadrature rules in a lower computational\ncomplexity than based on the recurrence relation, and with improved accuracy\nfor large degree. They are also of interest in random matrix theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07578v1"
    },
    {
        "title": "Differentiable monotonicity-preserving schemes for discontinuous\n  Galerkin methods on arbitrary meshes",
        "authors": [
            "Santiago Badia",
            "Jesús Bonilla",
            "Alba Hierro"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  This work is devoted to the design of interior penalty discontinuous Galerkin\n(dG) schemes that preserve maximum principles at the discrete level for the\nsteady transport and convection-diffusion problems and the respective transient\nproblems with implicit time integration. Monotonic schemes that combine\nexplicit time stepping with dG space discretization are very common, but the\ndesign of such schemes for implicit time stepping is rare, and it had only been\nattained so far for 1D problems. The proposed scheme is based on an artificial\ndiffusion that linearly depends on a shock detector that identifies the\ntroublesome areas. In order to define the new shock detector, we have\nintroduced the concept of discrete local extrema. The diffusion operator is a\ngraph-Laplacian, instead of the more common finite element discretization of\nthe Laplacian operator, which is essential to keep monotonicity on general\nmeshes and in multi-dimension. The resulting nonlinear stabilization is\nnon-smooth and nonlinear solvers can fail to converge. As a result, we propose\na smoothed (twice differentiable) version of the nonlinear stabilization, which\nallows us to use Newton with line search nonlinear solvers and dramatically\nimprove nonlinear convergence. A theoretical numerical analysis of the proposed\nschemes show that they satisfy the desired monotonicity properties. Further,\nthe resulting operator is Lipschitz continuous and there exists at least one\nsolution of the discrete problem, even in the non-smooth version. We provide a\nset of numerical results to support our findings.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08686v2"
    },
    {
        "title": "Numerical Integration as an Initial Value Problem",
        "authors": [
            "Daniel Gebremedhin",
            "Charles Weatherford"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Numerical integration (NI) packages commonly used in scientific research are\nlimited to returning the value of a definite integral at the upper integration\nlimit, also commonly referred to as numerical quadrature. These quadrature\nalgorithms are typically of a fixed accuracy and have only limited ability to\nadapt to the application. In this article, we will present a highly adaptive\nalgorithm that not only can efficiently compute definite integrals encountered\nin physical problems but also can be applied to other problems such as\nindefinite integrals, integral equations and linear and non-linear eigenvalue\nproblems. More specifically, a finite element based algorithm is presented that\nnumerically solves first order ordinary differential equations (ODE) by\npropagating the solution function from a given initial value (lower integration\nvalue). The algorithm incorporates powerful techniques including, adaptive step\nsize choice of elements, local error checking and enforces continuity of both\nthe integral and the integrand across consecutive elements.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01638v1"
    },
    {
        "title": "Radial Basis Function Approximations: Comparison and Applications",
        "authors": [
            "Zuzana Majdisova",
            "Vaclav Skala"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Approximation of scattered data is often a task in many engineering problems.\nThe Radial Basis Function (RBF) approximation is appropriate for large\nscattered (unordered) datasets in d-dimensional space. This approach is useful\nfor a higher dimension d>2, because the other methods require the conversion of\na scattered dataset to an ordered dataset (i.e. a semi-regular mesh is obtained\nby using some tessellation techniques), which is computationally expensive. The\nRBF approximation is non-separable, as it is based on the distance between two\npoints. This method leads to a solution of Linear System of Equations (LSE)\nAc=h.\n  In this paper several RBF approximation methods are briefly introduced and a\ncomparison of those is made with respect to the stability and accuracy of\ncomputation. The proposed RBF approximation offers lower memory requirements\nand better quality of approximation.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07705v1"
    },
    {
        "title": "A Local Fourier Slice Theorem",
        "authors": [
            "Christian Lessig"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We present a local Fourier slice equation that enables local and sparse\nprojection of a signal. Our result exploits that a slice in frequency space is\nan iso-parameter set in spherical coordinates. Therefore, the projection of\nsuitable wavelets defined separably in these coordinates can be computed\nanalytically, yielding a sequence of wavelets closed under projection. Our\nlocal Fourier slice equation then realizes projection as reconstruction with\n\"sliced\" wavelets with computational costs that scale linearly in the\ncomplexity of the projected signal. We numerically evaluate the performance of\nour local Fourier slice equation for synthetic test data and tomographic\nreconstruction, demonstrating that locality and sparsity can significantly\nreduce computation times and memory requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.09706v2"
    },
    {
        "title": "A CutFEM method for two-phase flow problems",
        "authors": [
            "Susanne Claus",
            "Pierre Kerfriden"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In this article, we present a cut finite element method for two-phase\nNavier-Stokes flows. The main feature of the method is the formulation of a\nunified continuous interior penalty stabilisation approach for, on the one\nhand, stabilising advection and the pressure-velocity coupling and, on the\nother hand, stabilising the cut region. The accuracy of the algorithm is\nenhanced by the development of extended fictitious domains to guarantee a well\ndefined velocity from previous time steps in the current geometry. Finally, the\nrobustness of the moving-interface algorithm is further improved by the\nintroduction of a curvature smoothing technique that reduces spurious\nvelocities. The algorithm is shown to perform remarkably well for low capillary\nnumber flows, and is a first step towards flexible and robust CutFEM algorithms\nfor the simulation of microfluidic devices.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.10156v1"
    },
    {
        "title": "Stability conditions for the explicit integration of projection based\n  nonlinear reduced-order and hyper reduced structural mechanics finite element\n  models",
        "authors": [
            "C. Bach",
            "L. Song",
            "T. Erhart",
            "F. Duddeck"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Projection-based nonlinear model order reduction methods can be used to\nreduce simulation times for the solution of many PDE-constrained problems. It\nhas been observed in literature that such nonlinear reduced-order models (ROMs)\nbased on Galerkin projection sometimes exhibit much larger stable time step\nsizes than their unreduced counterparts. This work provides a detailed\ntheoretical analysis of this phenomenon for structural mechanics. We first show\nthat many desirable system matrix properties are preserved by the Galerkin\nprojection. Next, we prove that the eigenvalues of the linearized Galerkin\nreduced-order system separate the eigenvalues of the linearized original\nsystem. Assuming non-negative Rayleigh damping and a time integration using the\npopular central difference method, we further prove that the theoretical linear\nstability time step of the ROM is in fact always larger than or equal to the\ncritical time step of its corresponding full-order model. We also give\nmathematical expressions for computing the stable time step size. Finally, we\nshow that under certain conditions this increase in the stability time step\neven extends to some hyper-reduction methods. The findings can be used to\ncompute numerical stability time step sizes for the integration of nonlinear\nROMs in structural mechanics, and to speed up simulations by permitting the use\nof larger time steps.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.11404v1"
    },
    {
        "title": "A Fast Volume Integral Equation Solver with Linear Basis Functions for\n  the Accurate Computation of Electromagnetic Fields in MRI",
        "authors": [
            "Ioannis P. Georgakis",
            "Ilias I. Giannakopoulos",
            "Mikhail S. Litsarev",
            "Athanasios G. Polimeridis"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  A stable volume integral equation (VIE) solver based on\npolarization/magnetization currents is presented, for the accurate and\nefficient computation of the electromagnetic scattering from highly\ninhomogeneous and high contrast objects.We employ the Galerkin Method of\nMoments to discretize the formulation with discontinuous piecewise linear basis\nfunctions on uniform voxelized grids, allowing for the acceleration of the\nassociated matrix-vector products in an iterative solver, with the help of FFT.\nNumerical results illustrate the superior accuracy and more stable convergence\nproperties of the proposed framework, when compared against standard low order\n(piecewise constant) discretization schemes and a more conventional VIE\nformulation based on electric flux densities. Finally, the developed solver is\napplied to analyze complex geometries, including realistic human body models,\ntypically used in modeling the interactions between electromagnetic waves and\nbiological tissue.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.02196v2"
    },
    {
        "title": "Streaming Low-Rank Matrix Approximation with an Application to\n  Scientific Simulation",
        "authors": [
            "Joel A. Tropp",
            "Alp Yurtsever",
            "Madeleine Udell",
            "Volkan Cevher"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  This paper argues that randomized linear sketching is a natural tool for\non-the-fly compression of data matrices that arise from large-scale scientific\nsimulations and data collection. The technical contribution consists in a new\nalgorithm for constructing an accurate low-rank approximation of a matrix from\nstreaming data. This method is accompanied by an a priori analysis that allows\nthe user to set algorithm parameters with confidence and an a posteriori error\nestimator that allows the user to validate the quality of the reconstructed\nmatrix. In comparison to previous techniques, the new method achieves smaller\nrelative approximation errors and is less sensitive to parameter choices. As\nconcrete applications, the paper outlines how the algorithm can be used to\ncompress a Navier--Stokes simulation and a sea surface temperature dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.08651v1"
    },
    {
        "title": "Predict-and-recompute conjugate gradient variants",
        "authors": [
            "Tyler Chen",
            "Erin C. Carson"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  The standard implementation of the conjugate gradient algorithm suffers from\ncommunication bottlenecks on parallel architectures, due primarily to the two\nglobal reductions required every iteration. In this paper, we study conjugate\ngradient variants which decrease the runtime per iteration by overlapping\nglobal synchronizations, and in the case of pipelined variants, matrix-vector\nproducts. Through the use of a predict-and-recompute scheme, whereby\nrecursively-updated quantities are first used as a predictor for their true\nvalues and then recomputed exactly at a later point in the iteration, these\nvariants are observed to have convergence behavior nearly as good as the\nstandard conjugate gradient implementation on a variety of test problems. We\nprovide a rounding error analysis which provides insight into this observation.\nIt is also verified experimentally that the variants studied do indeed reduce\nthe runtime per iteration in practice and that they scale similarly to\npreviously-studied communication-hiding variants. Finally, because these\nvariants achieve good convergence without the use of any additional input\nparameters, they have the potential to be used in place of the standard\nconjugate gradient implementation in a range of applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.01549v5"
    },
    {
        "title": "An Unconditionally Stable First-Order Constraint Solver for Multibody\n  Systems",
        "authors": [
            "Evan Drumwright"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  This article describes an absolutely stable, first-order constraint solverfor\nmulti-rigid body systems that calculates (predicts) constraint forces for\ntypical bilateral and unilateral constraints, contact constraints with\nfriction, and many other constraint types. Redundant constraints do not pose\nnumerical problems or require regularization. Coulomb friction for contact is\nmodeled using a true friction cone, rather than a linearized approximation. The\ncomputational expense of the solver is dependent upon the types of constraints\npresent in the input. The hardest (in a computational complexity sense) inputs\nare reducible to solving convex optimization problems, i.e., polynomial time\nsolvable. The simplest inputs require only solving a linear system. The solver\nis L-stable, which will imply that the forces due to constraints induce no\ncomputational stiffness into the multi-body dynamics differential equations.\nThis approach is targeted to multibodies simulated with coarse accuracy,\nsubject to computational stiffness arising from constraints, and where the\nnumber of constraint equations is not large compared to the number of multibody\nposition and velocity state variables. For such applications, the approach\nshould prove far faster than using other implicit integration approaches. I\nassess the approach on some fundamental multibody dynamics problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.10828v1"
    },
    {
        "title": "Properties of polynomial bases used in a line-surface intersection\n  algorithm",
        "authors": [
            "Gun Srijuntongsiri",
            "Stephen A. Vavasis"
        ],
        "category": "cs.NA",
        "published_year": "2007",
        "summary": "  In [5], Srijuntongsiri and Vavasis propose the \"Kantorovich-Test Subdivision\nalgorithm\", or KTS, which is an algorithm for finding all zeros of a polynomial\nsystem in a bounded region of the plane. This algorithm can be used to find the\nintersections between a line and a surface. The main features of KTS are that\nit can operate on polynomials represented in any basis that satisfies certain\nconditions and that its efficiency has an upper bound that depends only on the\nconditioning of the problem and the choice of the basis representing the\npolynomial system.\n  This article explores in detail the dependence of the efficiency of the KTS\nalgorithm on the choice of basis. Three bases are considered: the power, the\nBernstein, and the Chebyshev bases. These three bases satisfy the basis\nproperties required by KTS. Theoretically, Chebyshev case has the smallest\nupper bound on its running time. The computational results, however, do not\nshow that Chebyshev case performs better than the other two.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.1515v5"
    },
    {
        "title": "Trace Norm Regularized Tensor Classification and Its Online Learning\n  Approaches",
        "authors": [
            "Ziqiang Shi",
            "Tieran Zheng",
            "Jiqing Han"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  In this paper we propose an algorithm to classify tensor data. Our\nmethodology is built on recent studies about matrix classification with the\ntrace norm constrained weight matrix and the tensor trace norm. Similar to\nmatrix classification, the tensor classification is formulated as a convex\noptimization problem which can be solved by using the off-the-shelf accelerated\nproximal gradient (APG) method. However, there are no analytic solutions as the\nmatrix case for the updating of the weight tensors via the proximal gradient.\nTo tackle this problem, the Douglas-Rachford splitting technique and the\nalternating direction method of multipliers (ADM) used in tensor completion are\nadapted to update the weight tensors. Further more, due to the demand of real\napplications, we also propose its online learning approaches. Experiments\ndemonstrate the efficiency of the methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.1342v1"
    },
    {
        "title": "Demonstrating the Applicability of PAINT to Computationally Expensive\n  Real-life Multiobjective Optimization",
        "authors": [
            "Markus Hartikainen",
            "Vesa Ojalehto"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  We demonstrate the applicability of a new PAINT method to speed up iterations\nof interactive methods in multiobjective optimization. As our test case, we\nsolve a computationally expensive non-linear, five-objective problem of\ndesigning and operating a wastewater treatment plant. The PAINT method\ninterpolates between a given set of Pareto optimal outcomes and constructs a\ncomputationally inexpensive mixed integer linear surrogate problem for the\noriginal problem. We develop an IND-NIMBUS(R) PAINT module to combine the\ninteractive NIMBUS method and the PAINT method and to find a preferred solution\nto the original problem. With the PAINT method, the solution process with the\nNIMBUS method take a comparatively short time even though the original problem\nis computationally expensive.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.3411v1"
    },
    {
        "title": "Adaptive Smolyak Pseudospectral Approximations",
        "authors": [
            "Patrick R. Conrad",
            "Youssef M. Marzouk"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Polynomial approximations of computationally intensive models are central to\nuncertainty quantification. This paper describes an adaptive method for\nnon-intrusive pseudospectral approximation, based on Smolyak's algorithm with\ngeneralized sparse grids. We rigorously analyze and extend the non-adaptive\nmethod proposed in [6], and compare it to a common alternative approach for\nusing sparse grids to construct polynomial approximations, direct quadrature.\nAnalysis of direct quadrature shows that O(1) errors are an intrinsic property\nof some configurations of the method, as a consequence of internal aliasing. We\nprovide precise conditions, based on the chosen polynomial basis and quadrature\nrules, under which this aliasing error occurs. We then establish theoretical\nresults on the accuracy of Smolyak pseudospectral approximation, and show that\nthe Smolyak approximation avoids internal aliasing and makes far more effective\nuse of sparse function evaluations. These results are applicable to broad\nchoices of quadrature rule and generalized sparse grids. Exploiting this\nflexibility, we introduce a greedy heuristic for adaptive refinement of the\npseudospectral approximation. We numerically demonstrate convergence of the\nalgorithm on the Genz test functions, and illustrate the accuracy and\nefficiency of the adaptive approach on a realistic chemical kinetics problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.1406v2"
    },
    {
        "title": "On Implementation and Evaluation of Inverse Iteration Algorithm with\n  compact WY Orthogonalization",
        "authors": [
            "Hiroyuki Ishigami",
            "Kinji Kimura",
            "Yoshimasa Nakamura"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  A new inverse iteration algorithm that can be used to compute all the\neigenvectors of a real symmetric tri-diagonal matrix on parallel computers is\ndeveloped. The modified Gram-Schmidt orthogonalization is used in the classical\ninverse iteration. This algorithm is sequential and causes a bottleneck in\nparallel computing. In this paper, the use of the compact WY representation is\nproposed in the orthogonalization process of the inverse iteration with the\nHouseholder transformation. This change results in drastically reduced\nsynchronization cost in parallel computing. The new algorithm is evaluated on\nboth an 8-core and a 32-core parallel computer, and it is shown that the new\nalgorithm is greatly faster than the classical inverse iteration algorithm in\ncomputing all the eigenvectors of matrices with several thousand dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.1910v1"
    },
    {
        "title": "The scalability of the matrices in direct Trefftz method in 2D Laplace\n  problem",
        "authors": [
            "M. Borkowski"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This paper presents an interesting property of the matrices that may be\nobtained with the use of direct Trefftz method. It is proved analytically for\n2D Laplace problem that values of the elements of matrices describing the\ncapacitance of two scaled domains are inversely proportional to the scalability\nfactor. As an example of the application the capacitance extraction problem is\nchosen. Concise description of the algorithm in which the scalability property\ncan be utilized is given. Furthermore some numerical results of the algorithm\nare presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.03925v1"
    },
    {
        "title": "An Iterative Reweighted Method for Tucker Decomposition of Incomplete\n  Multiway Tensors",
        "authors": [
            "Linxiao Yang",
            "Jun Fang",
            "Hongbin Li",
            "Bing Zeng"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We consider the problem of low-rank decomposition of incomplete multiway\ntensors. Since many real-world data lie on an intrinsically low dimensional\nsubspace, tensor low-rank decomposition with missing entries has applications\nin many data analysis problems such as recommender systems and image\ninpainting. In this paper, we focus on Tucker decomposition which represents an\nNth-order tensor in terms of N factor matrices and a core tensor via\nmultilinear operations. To exploit the underlying multilinear low-rank\nstructure in high-dimensional datasets, we propose a group-based log-sum\npenalty functional to place structural sparsity over the core tensor, which\nleads to a compact representation with smallest core tensor. The method for\nTucker decomposition is developed by iteratively minimizing a surrogate\nfunction that majorizes the original objective function, which results in an\niterative reweighted process. In addition, to reduce the computational\ncomplexity, an over-relaxed monotone fast iterative shrinkage-thresholding\ntechnique is adapted and embedded in the iterative reweighted process. The\nproposed method is able to determine the model complexity (i.e. multilinear\nrank) in an automatic way. Simulation results show that the proposed algorithm\noffers competitive performance compared with other existing algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.04695v1"
    },
    {
        "title": "On the a posteriori error analysis for linear Fokker-Planck models in\n  convection-dominated diffusion problems",
        "authors": [
            "Svetlana Matculevich",
            "Monika Wolfmayr"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  This work is aimed at the derivation of reliable and efficient a posteriori\nerror estimates for convection-dominated diffusion problems motivated by a\nlinear Fokker-Planck problem appearing in computational neuroscience. We obtain\ncomputable error bounds of the functional type for the static and\ntime-dependent case and for different boundary conditions (mixed and pure\nNeumann boundary conditions). Finally, we present a set of various numerical\nexamples including discussions on mesh adaptivity and space-time\ndiscretisation. The numerical results confirm the reliability and efficiency of\nthe error estimates derived.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09232v2"
    },
    {
        "title": "PDE-constrained LDDMM via geodesic shooting and inexact\n  Gauss-Newton-Krylov optimization using the incremental adjoint Jacobi\n  equations",
        "authors": [
            "Monica Hernandez"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The class of non-rigid registration methods proposed in the framework of\nPDE-constrained Large Deformation Diffeomorphic Metric Mapping is a\nparticularly interesting family of physically meaningful diffeomorphic\nregistration methods. Inexact Newton-Krylov optimization has shown an excellent\nnumerical accuracy and an extraordinarily fast convergence rate in this\nframework. However, the Galerkin representation of the non-stationary velocity\nfields does not provide proper geodesic paths. In this work, we propose a\nmethod for PDE-constrained LDDMM parameterized in the space of initial velocity\nfields under the EPDiff equation. The derivation of the gradient and the\nHessian-vector products are performed on the final velocity field and\ntransported backward using the adjoint and the incremental adjoint Jacobi\nequations. This way, we avoid the complex dependence on the initial velocity\nfield in the derivations and the computation of the adjoint equation and its\nincremental counterpart. The proposed method provides geodesics in the\nframework of PDE-constrained LDDMM, and it shows performance competitive to\nbenchmark PDE-constrained LDDMM and EPDiff-LDDMM methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.04638v1"
    },
    {
        "title": "Data driven approximation of parametrized PDEs by Reduced Basis and\n  Neural Networks",
        "authors": [
            "Niccolò Dal Santo",
            "Simone Deparis",
            "Luca Pegolotti"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  We are interested in the approximation of partial differential equations with\na data-driven approach based on the reduced basis method and machine learning.\nWe suppose that the phenomenon of interest can be modeled by a parametrized\npartial differential equation, but that the value of the physical parameters is\nunknown or difficult to be directly measured. Our method allows to estimate\nfields of interest, for instance temperature of a sample of material or\nvelocity of a fluid, given data at a handful of points in the domain. We\npropose to accomplish this task with a neural network embedding a reduced basis\nsolver as exotic activation function in the last layer. The reduced basis\nsolver accounts for the underlying physical phenomenonon and it is constructed\nfrom snapshots obtained from randomly selected values of the physical\nparameters during an expensive offline phase. The same full order solutions are\nthen employed for the training of the neural network. As a matter of fact, the\nchosen architecture resembles an asymmetric autoencoder in which the decoder is\nthe reduced basis solver and as such it does not contain trainable parameters.\nThe resulting latent space of our autoencoder includes parameter-dependent\nquantities feeding the reduced basis solver, which -- depending on the\nconsidered partial differential equation -- are the values of the physical\nparameters themselves or the affine decomposition coefficients of the\ndifferential operators.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.01514v2"
    },
    {
        "title": "Dynamic evaluation of exponential polynomial curves and surfaces via\n  basis transformation",
        "authors": [
            "Xunnian Yang",
            "Jialin Hong"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  It is shown in \"SIAM J. Sci. Comput. 39 (2017):B424-B441\" that free-form\ncurves used in computer aided geometric design can usually be represented as\nthe solutions of linear differential systems and points and derivatives on the\ncurves can be evaluated dynamically by solving the differential systems\nnumerically. In this paper we present an even more robust and efficient\nalgorithm for dynamic evaluation of exponential polynomial curves and surfaces.\nBased on properties that spaces spanned by general exponential polynomials are\ntranslation invariant and polynomial spaces are invariant with respect to a\nlinear transform of the parameter, the transformation matrices between bases\nwith or without translated or linearly transformed parameters are explicitly\ncomputed. Points on curves or surfaces with equal or changing parameter steps\ncan then be evaluated dynamically from a start point using a pre-computed\nmatrix. Like former dynamic evaluation algorithms, the newly proposed approach\nneeds only arithmetic operations for evaluating exponential polynomial curves\nand surfaces. Unlike conventional numerical methods that solve a linear\ndifferential system, the new method can give robust and accurate evaluation\nresults for any chosen parameter steps. Basis transformation technique also\nenables dynamic evaluation of polynomial curves with changing parameter steps\nusing a constant matrix, which reduces time costs significantly than computing\neach point individually by classical algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.10205v2"
    },
    {
        "title": "A Simple Local Variational Iteration Method and Related Algorithm for\n  Nonlinear Science and Engineering",
        "authors": [
            "Xuechuan Wang",
            "Qiuyi Xu",
            "Satya N. Atluri"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  A very simple and efficient local variational iteration method for solving\nproblems of nonlinear science is proposed in this paper. The analytical\niteration formula of this method is derived first using a general form of first\norder nonlinear differential equations, followed by straightforward\ndiscretization using Chebyshev polynomials and collocation method. The\nresulting numerical algorithm is very concise and easy to use, only involving\nhighly sparse matrix operations of addition and multiplication, and no\ninversion of the Jacobian in nonlinear problems. Apart from the simple yet\nefficient iteration formula, another extraordinary feature of LVIM is that in\neach local domain, all the collocation nodes participate in the calculation\nsimultaneously, thus each local domain can be regarded as one node in\ncalculation through GPU acceleration and parallel processing. For illustration,\nthe proposed algorithm of LVIM is applied to various nonlinear problems\nincluding Blasius equations in fluid mechanics, buckled bar equations in solid\nmechanics, the Chandrasekhar equation in astrophysics, the low-Earth-orbit\nequation in orbital mechanics, etc. Using the built-in highly optimized ode45\nfunction of MATLAB as a comparison, it is found that the LVIM is not only very\naccurate, but also much faster by an order of magnitude than ode45 in all the\nnumerical examples, especially when the nonlinear terms are very complicated\nand difficult to evaluate.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11021v1"
    },
    {
        "title": "A compact subcell WENO limiting strategy using immediate neighbors for\n  Runge-Kutta Discontinuous Galerkin Methods",
        "authors": [
            "S R Siva Prasad Kochi",
            "M Ramakrishna"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  A compact subcell WENO (CSWENO) limiter is proposed for the solution of\nhyperbolic conservation laws with Discontinuous Galerkin Method which uses only\nthe immediate neighbors of a given cell. These neighbors are divided into the\nrequired stencil for WENO reconstruction and an existing WENO limiting strategy\nis used. Accuracy tests and results for one-dimensional and two-dimensional\nBurgers equation and one-dimensional and two-dimensional Euler equations for\nCartesian meshes are presented using this limiter. Comparisons with the parent\nWENO limiter are provided wherever appropriate and the performance of the\ncurrent limiter is found to be slightly better than the parent WENO limiter for\nhigher orders.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11147v2"
    },
    {
        "title": "Parameter Identification in a Probabilistic Setting",
        "authors": [
            "Bojana V. Rosić",
            "Anna Kučerová",
            "Jan Sýkora",
            "Oliver Pajonk",
            "Alexander Litvinenko",
            "Hermann G. Matthies"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Parameter identification problems are formulated in a probabilistic language,\nwhere the randomness reflects the uncertainty about the knowledge of the true\nvalues. This setting allows conceptually easily to incorporate new information,\ne.g. through a measurement, by connecting it to Bayes's theorem. The unknown\nquantity is modelled as a (may be high-dimensional) random variable. Such a\ndescription has two constituents, the measurable function and the measure. One\ngroup of methods is identified as updating the measure, the other group changes\nthe measurable function. We connect both groups with the relatively recent\nmethods of functional approximation of stochastic problems, and introduce\nespecially in combination with the second group of methods a new procedure\nwhich does not need any sampling, hence works completely deterministically. It\nalso seems to be the fastest and more reliable when compared with other\nmethods. We show by example that it also works for highly nonlinear non-smooth\nproblems with non-Gaussian measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4049v1"
    },
    {
        "title": "How Accurate is inv(A)*b?",
        "authors": [
            "Alex Druinsky",
            "Sivan Toledo"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Several widely-used textbooks lead the reader to believe that solving a\nlinear system of equations Ax = b by multiplying the vector b by a computed\ninverse inv(A) is inaccurate. Virtually all other textbooks on numerical\nanalysis and numerical linear algebra advise against using computed inverses\nwithout stating whether this is accurate or not. In fact, under reasonable\nassumptions on how the inverse is computed, x = inv(A)*b is as accurate as the\nsolution computed by the best backward-stable solvers. This fact is not new,\nbut obviously obscure. We review the literature on the accuracy of this\ncomputation and present a self-contained numerical analysis of it.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.6035v1"
    },
    {
        "title": "Speeding-Up Convergence via Sequential Subspace Optimization: Current\n  State and Future Directions",
        "authors": [
            "Michael Zibulevsky"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  This is an overview paper written in style of research proposal. In recent\nyears we introduced a general framework for large-scale unconstrained\noptimization -- Sequential Subspace Optimization (SESOP) and demonstrated its\nusefulness for sparsity-based signal/image denoising, deconvolution,\ncompressive sensing, computed tomography, diffraction imaging, support vector\nmachines. We explored its combination with Parallel Coordinate Descent and\nSeparable Surrogate Function methods, obtaining state of the art results in\nabove-mentioned areas. There are several methods, that are faster than plain\nSESOP under specific conditions: Trust region Newton method - for problems with\neasily invertible Hessian matrix; Truncated Newton method - when fast\nmultiplication by Hessian is available; Stochastic optimization methods - for\nproblems with large stochastic-type data; Multigrid methods - for problems with\nnested multilevel structure. Each of these methods can be further improved by\nmerge with SESOP. One can also accelerate Augmented Lagrangian method for\nconstrained optimization problems and Alternating Direction Method of\nMultipliers for problems with separable objective function and non-separable\nconstraints.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.0159v1"
    },
    {
        "title": "Exponential-Krylov methods for ordinary differential equations",
        "authors": [
            "Paul Tranquilli",
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This paper develops a new class of exponential-type integrators where all the\nmatrix exponentiations are performed in a single Krylov space of low dimension.\nThe new family, called Lightly Implicit Krylov-Exponential (LIKE), is well\nsuited for solving large scale systems of ODEs or semi-discrete PDEs. The time\ndiscretization and the Krylov space approximation are treated as a single\ncomputational process, and the Krylov space properties are an integral part of\nthe new LIKE order condition theory developed herein. Consequently, LIKE\nmethods require a small number of basis vectors determined solely by the\ntemporal order of accuracy. The subspace size is independent of the ODE under\nconsideration, and there is no need to monitor the errors in linear system\nsolutions at each stage. Numerical results illustrate the favorable properties\nof new family of methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.2125v2"
    },
    {
        "title": "Solving Cubic Equations By the Quadratic Formula",
        "authors": [
            "Bahman Kalantari"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Let $p(z)$ be a monic cubic complex polynomial with distinct roots and\ndistinct critical points. We say a critical point has the {\\it Voronoi\nproperty} if it lies in the Voronoi cell of a root $\\theta$, $V(\\theta)$, i.e.\nthe set of points that are closer to $\\theta$ than to the other roots. We prove\nat least one critical point has the Voronoi property and characterize the cases\nwhen both satisfy this property. It is known that for any $\\xi \\in V(\\theta)$,\nthe sequence $B_m(\\xi) =\\xi - p(\\xi) d_{m-2}/d_{m-1}$ converges to $\\theta$,\nwhere $d_m$ satisfies the recurrence $d_m =p'(\\xi)d_{m-1}-0.5\np(\\xi)p''(\\xi)d_{m-2} +p^2(\\xi)d_{m-3}$, $d_0 =1, d_{-1}=d_{-2}=0$. Thus by the\nVoronoi property, there is a solution $c$ of $p'(z)=0$ where $B_m(c)$ converges\nto a root of $p(z)$. The speed of convergence is dependent on the ratio of the\ndistances between $c$ and the closest and the second closest roots of $p(z)$.\nThis results in a different algorithm for solving a cubic equation than the\nclassical methods. We give polynomiography for an example.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.5148v1"
    },
    {
        "title": "Characterizing the Topography of Multi-dimensional Energy Landscapes",
        "authors": [
            "H. Lydia Deng",
            "John A. Scales"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  A basic issue in optimization, inverse theory,neural networks, computational\nchemistry and many other problems is the geometrical characterization of high\ndimensional functions. In inverse calculations one aims to characterize the set\nof models that fit the data (among other constraints). If the data misfit\nfunction is unimodal then one can find its peak by local optimization methods\nand characterize its width (related to the range of data-fitting models) by\nestimating derivatives at this peak. On the other hand, if there are local\nextrema, then a number of interesting and difficult problems arise. Are the\nlocal extrema important compared to the global or can they be eliminated (e.g.,\nby smoothing) without significant loss of information? Is there a sufficiently\nsmall number of local extrema that they can be enumerated via local\noptimization? What are the basins of attraction of these local extrema? Can two\nextrema be joined by a path that never goes uphill? Can the whole problem be\nreduced to one of enumerating the local extrema and their basins of attraction?\nFor locally ill-conditioned functions, premature convergence of local\noptimization can be confused with the presence of local extrema. Addressing any\nof these issues requires topographic information about the functions under\nstudy. But in many applications these functions may have hundreds or thousands\nof variables and can only be evaluated pointwise (by some numerical method for\ninstance). In this paper we describe systematic (but generic) methods of\nanalysing the topography of high dimensional functions using local optimization\nmethods applied to randomly chosen starting models. We provide a number of\nquantitative measures of function topography that have proven to be useful in\npractical problems along with error estimates.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.2948v1"
    },
    {
        "title": "A monolithic multi-time-step computational framework for first-order\n  transient systems with disparate scales",
        "authors": [
            "S. Karimi",
            "K. B. Nakshatrala"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Developing robust simulation tools for problems involving multiple\nmathematical scales has been a subject of great interest in computational\nmathematics and engineering. A desirable feature to have in a numerical\nformulation for multiscale transient problems is to be able to employ different\ntime-steps (multi-time-step coupling), and different time integrators and\ndifferent numerical formulations (mixed methods) in different regions of the\ncomputational domain. We present two new monolithic multi-time-step mixed\ncoupling methods for first-order transient systems. We shall employ unsteady\nadvection-diffusion-reaction equation with linear decay as the model problem,\nwhich offers several unique challenges in terms of non-self-adjoint spatial\noperator and rich features in the solutions. We shall employ the dual Schur\ndomain decomposition technique to handle the decomposition of domain into\nsubdomains. Two different methods of enforcing compatibility along the\nsubdomain interface will be used in the time discrete setting. A systematic\ntheoretical analysis (which includes numerical stability, influence of\nperturbations, bounds on drift along the subdomain interface) will be\nperformed. The first coupling method ensures that there is no drift along the\nsubdomain interface but does not facilitate explicit/implicit coupling. The\nsecond coupling method allows explicit/implicit coupling with controlled (but\nnon-zero) drift in the solution along the subdomain interface. Several\ncanonical problems will be solved to numerically verify the theoretical\npredictions, and to illustrate the overall performance of the proposed coupling\nmethods. Finally, we shall illustrate the robustness of the proposed coupling\nmethods using a multi-time-step transient simulation of a fast bimolecular\nadvective-diffusive-reactive system.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.3230v2"
    },
    {
        "title": "An Alternating KMF Algorithm to Solve the Cauchy Problem for Laplaces\n  Equation",
        "authors": [
            "Chakir Tajani",
            "Jaafar Abouchabaka"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This work concerns the use of the iterative algorithm (KMF algorithm)\nproposed by Kozlov, Mazya and Fomin to solve the Cauchy problem for Laplaces\nequation. This problem consists to recovering the lacking data on some part of\nthe boundary using the over specified conditions on the other part of the\nboundary. We describe an alternating formulation of the KMF algorithm and its\nrelationship with a classical formulation. The implementation of this algorithm\nfor a regular domain is performed by the finite element method using the\nsoftware Freefem. The numerical tests developed show the effectiveness of the\nproposed algorithm since it allows to have more accurate results as well as\nreducing the number of iterations needed for convergence.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.3235v1"
    },
    {
        "title": "An error estimate of Gaussian Recursive Filter in 3Dvar problem",
        "authors": [
            "S. Cuomo",
            "R. Farina",
            "A. Galletti",
            "L. Marcellino"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Computational kernel of the three-dimensional variational data assimilation\n(3D-Var) problem is a linear system, generally solved by means of an iterative\nmethod. The most costly part of each iterative step is a matrix-vector product\nwith a very large covariance matrix having Gaussian correlation structure. This\noperation may be interpreted as a Gaussian convolution, that is a very\nexpensive numerical kernel. Recursive Filters (RFs) are a well known way to\napproximate the Gaussian convolution and are intensively applied in the\nmeteorology, in the oceanography and in forecast models. In this paper, we deal\nwith an oceanographic 3D-Var data assimilation scheme, named OceanVar, where\nthe linear system is solved by using the Conjugate Gradient (GC) method by\nreplacing, at each step, the Gaussian convolution with RFs. Here we give\ntheoretical issues on the discrete convolution approximation with a first order\n(1st-RF) and a third order (3rd-RF) recursive filters. Numerical experiments\nconfirm given error bounds and show the benefits, in terms of accuracy and\nperformance, of the 3-rd RF.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.3468v1"
    },
    {
        "title": "Development of the method of computer analogy for studying and solving\n  complex nonlinear systems",
        "authors": [
            "Vladimir Aristov",
            "Andrey Stroganov"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  A method of representation of a solution as segments of the series in powers\nof the step of the independent variable is expanded for solving complex systems\nof ordinary differential equations (ODE): the Lorenz system and other systems.\nA new procedure of reduction of the representation of the solution to a sum of\ntwo parts (regular and random) is performed. A shifting procedure is applied in\neach level of the independent variable to the random part and it acts as the\nfilter that extracts the values to the regular part. In certain cases it is\npossible to omit the random part and construct the approximation which does not\nconverge but still provides the qualitative information about the full solution\n(a linear approximation provides a simple exact solution). Evaluation of the\nerror for this case is performed. Constructing the analytical representation of\nthe solutions for these systems by the developed method is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.6139v1"
    },
    {
        "title": "Enhancing adaptive sparse grid approximations and improving refinement\n  strategies using adjoint-based a posteriori error estimates",
        "authors": [
            "John D. Jakeman",
            "Timothy Wildey"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this paper we present an algorithm for adaptive sparse grid approximations\nof quantities of interest computed from discretized partial differential\nequations. We use adjoint-based a posteriori error estimates of the physical\ndiscretization error and the interpolation error in the sparse grid to enhance\nthe sparse grid approximation and to drive adaptivity of the sparse grid.\nUtilizing these error estimates provides significantly more accurate functional\nvalues for random samples of the sparse grid approximation. We also demonstrate\nthat alternative refinement strategies based upon a posteriori error estimates\ncan lead to further increases in accuracy in the approximation over traditional\nhierarchical surplus based strategies. Throughout this paper we also provide\nand test a framework for balancing the physical discretization error with the\nstochastic interpolation error of the enhanced sparse grid approximation.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.1061v1"
    },
    {
        "title": "Generalized Higher-Order Tensor Decomposition via Parallel ADMM",
        "authors": [
            "Fanhua Shang",
            "Yuanyuan Liu",
            "James Cheng"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Higher-order tensors are becoming prevalent in many scientific areas such as\ncomputer vision, social network analysis, data mining and neuroscience.\nTraditional tensor decomposition approaches face three major challenges: model\nselecting, gross corruptions and computational efficiency. To address these\nproblems, we first propose a parallel trace norm regularized tensor\ndecomposition method, and formulate it as a convex optimization problem. This\nmethod does not require the rank of each mode to be specified beforehand, and\ncan automatically determine the number of factors in each mode through our\noptimization scheme. By considering the low-rank structure of the observed\ntensor, we analyze the equivalent relationship of the trace norm between a\nlow-rank tensor and its core tensor. Then, we cast a non-convex tensor\ndecomposition model into a weighted combination of multiple much smaller-scale\nmatrix trace norm minimization. Finally, we develop two parallel alternating\ndirection methods of multipliers (ADMM) to solve our problems. Experimental\nresults verify that our regularized formulation is effective, and our methods\nare robust to noise or outliers.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.1399v1"
    },
    {
        "title": "High Order Implicit-Explicit General Linear Methods with Optimized\n  Stability Regions",
        "authors": [
            "Hong Zhang",
            "Adrian Sandu",
            "Sebastien Blaise"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In the numerical solution of partial differential equations using a\nmethod-of-lines approach, the availability of high order spatial discretization\nschemes motivates the development of sophisticated high order time integration\nmethods. For multiphysics problems with both stiff and non-stiff terms\nimplicit-explicit (IMEX) time stepping methods attempt to combine the lower\ncost advantage of explicit schemes with the favorable stability properties of\nimplicit schemes. Existing high order IMEX Runge Kutta or linear multistep\nmethods, however, suffer from accuracy or stability reduction.\n  This work shows that IMEX general linear methods (GLMs) are competitive\nalternatives to classic IMEX schemes for large problems arising in practice.\nHigh order IMEX-GLMs are constructed in the framework developed by the authors\n[34]. The stability regions of the new schemes are optimized numerically. The\nresulting IMEX-GLMs have similar stability properties as IMEX Runge-Kutta\nmethods, but they do not suffer from order reduction, and are superior in terms\nof accuracy and efficiency. Numerical experiments with two and three\ndimensional test problems illustrate the potential of the new schemes to speed\nup complex applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.2337v1"
    },
    {
        "title": "Tensor Networks for Big Data Analytics and Large-Scale Optimization\n  Problems",
        "authors": [
            "Andrzej Cichocki"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this paper we review basic and emerging models and associated algorithms\nfor large-scale tensor networks, especially Tensor Train (TT) decompositions\nusing novel mathematical and graphical representations. We discus the concept\nof tensorization (i.e., creating very high-order tensors from lower-order\noriginal data) and super compression of data achieved via quantized tensor\ntrain (QTT) networks. The purpose of a tensorization and quantization is to\nachieve, via low-rank tensor approximations \"super\" compression, and\nmeaningful, compact representation of structured data. The main objective of\nthis paper is to show how tensor networks can be used to solve a wide class of\nbig data optimization problems (that are far from tractable by classical\nnumerical methods) by applying tensorization and performing all operations\nusing relatively small size matrices and tensors and applying iteratively\noptimized and approximative tensor contractions.\n  Keywords: Tensor networks, tensor train (TT) decompositions, matrix product\nstates (MPS), matrix product operators (MPO), basic tensor operations,\ntensorization, distributed representation od data optimization problems for\nvery large-scale problems: generalized eigenvalue decomposition (GEVD),\nPCA/SVD, canonical correlation analysis (CCA).\n",
        "pdf_link": "http://arxiv.org/pdf/1407.3124v2"
    },
    {
        "title": "Deterministic Versus Randomized Kaczmarz Iterative Projection",
        "authors": [
            "Tim Wallace",
            "Ali Sekmen"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Kaczmarz's alternating projection method has been widely used for solving a\nconsistent (mostly over-determined) linear system of equations Ax=b. Because of\nits simple iterative nature with light computation, this method was\nsuccessfully applied in computerized tomography. Since tomography generates a\nmatrix A with highly coherent rows, randomized Kaczmarz algorithm is expected\nto provide faster convergence as it picks a row for each iteration at random,\nbased on a certain probability distribution. It was recently shown that picking\na row at random, proportional with its norm, makes the iteration converge\nexponentially in expectation with a decay constant that depends on the scaled\ncondition number of A and not the number of equations. Since Kaczmarz's method\nis a subspace projection method, the convergence rate for simple Kaczmarz\nalgorithm was developed in terms of subspace angles. This paper provides\nanalyses of simple and randomized Kaczmarz algorithms and explain the link\nbetween them. It also propose new versions of randomization that may speed up\nconvergence.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.5593v1"
    },
    {
        "title": "Enhancing $\\ell_1$-minimization estimates of polynomial chaos expansions\n  using basis selection",
        "authors": [
            "John D. Jakeman",
            "Michael S. Eldred",
            "Khachik Sargsyan"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this paper we present a basis selection method that can be used with\n$\\ell_1$-minimization to adaptively determine the large coefficients of\npolynomial chaos expansions (PCE). The adaptive construction produces\nanisotropic basis sets that have more terms in important dimensions and limits\nthe number of unimportant terms that increase mutual coherence and thus degrade\nthe performance of $\\ell_1$-minimization. The important features and the\naccuracy of basis selection are demonstrated with a number of numerical\nexamples. Specifically, we show that for a given computational budget, basis\nselection produces a more accurate PCE than would be obtained if the basis is\nfixed a priori. We also demonstrate that basis selection can be applied with\nnon-uniform random variables and can leverage gradient information.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.8093v1"
    },
    {
        "title": "A Note on Archetypal Analysis and the Approximation of Convex Hulls",
        "authors": [
            "Christian Bauckhage"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  We briefly review the basic ideas behind archetypal analysis for matrix\nfactorization and discuss its behavior in approximating the convex hull of a\ndata sample. We then ask how good such approximations can be and consider\ndifferent cases. Understanding archetypal analysis as the problem of computing\na convexity constrained low-rank approximation of the identity matrix provides\nestimates for archetypal analysis and the SiVM heuristic.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.0642v1"
    },
    {
        "title": "A Fast and Memory Efficient Sparse Solver with Applications to\n  Finite-Element Matrices",
        "authors": [
            "AmirHossein Aminfar",
            "Eric Darve"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this article, we introduce a fast and memory efficient solver for sparse\nmatrices arising from the finite element discretization of elliptic partial\ndifferential equations (PDEs). We use a fast direct (but approximate)\nmultifrontal solver as a preconditioner, and use an iterative solver to achieve\na desired accuracy. This approach combines the advantages of direct and\niterative schemes to arrive at a fast, robust and accurate solver. We will show\nthat this solver is faster ($\\sim$ 2x) and more memory efficient ($\\sim$ 2--3x)\nthan a conventional direct multifrontal solver. Furthermore, we will\ndemonstrate that the solver is both a faster and more effective preconditioner\nthan other preconditioners such as the incomplete LU preconditioner. Specific\nspeed-ups depend on the matrix size and improve as the size of the matrix\nincreases. The solver can be applied to both structured and unstructured meshes\nin a similar manner. We build on our previous work and utilize the fact that\ndense frontal and update matrices, in the multifrontal algorithm, can be\nrepresented as hierarchically off-diagonal low-rank (HODLR) matrices. Using\nthis idea, we replace all large dense matrix operations in the multifrontal\nelimination process with $O(N)$ HODLR operations to arrive at a faster and more\nmemory efficient solver.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.2697v2"
    },
    {
        "title": "Generalized Summation-by-Parts Operators for the Second Derivative with\n  Variable Coefficients",
        "authors": [
            "David C. Del Rey Fernández",
            "David W. Zingg"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  The comprehensive generalization of summation-by-parts of Del Rey Fern\\'andez\net al.\\ (J. Comput. Phys., 266, 2014) is extended to approximations of second\nderivatives with variable coefficients. This enables the construction of\nsecond-derivative operators with one or more of the following characteristics:\ni) non-repeating interior stencil, ii) nonuniform nodal distributions, and iii)\nexclusion of one or both boundary nodes. Definitions are proposed that give\nrise to generalized SBP operators that result in consistent, conservative, and\nstable discretizations of PDEs with or without mixed derivatives. It is proven\nthat such operators can be constructed using a correction to the application of\nthe first-derivative operator twice that is the same as used for the\nconstant-coefficient operator. Moreover, for operators with a repeating\ninterior stencil, a decomposition is proposed that makes the application of\nsuch operators particularly simple. A number of novel operators are\nconstructed, including operators on pseudo-spectral nodal distributions and\noperators that have a repeating interior stencil, but unequal nodal spacing\nnear boundaries. The various operators are compared to the application of the\nfirst-derivative operator twice in the context of the linear\nconvection-diffusion equation with constant and variable coefficients.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.5029v1"
    },
    {
        "title": "Optimization-based smoothing algorithm for triangle meshes over\n  arbitrarily shaped domains",
        "authors": [
            "Daniel Aubram"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This paper describes a node relocation algorithm based on nonlinear\noptimization which delivers excellent results for both unstructured and\nstructured plane triangle meshes over convex as well as non-convex domains with\nhigh curvature. The local optimization scheme is a damped Newton's method in\nwhich the gradient and Hessian of the objective function are evaluated exactly.\nThe algorithm has been developed in order to continuously rezone the mesh in\narbitrary Lagrangian-Eulerian (ALE) methods for large deformation penetration\nproblems, but it is also suitable for initial mesh improvement. Numerical\nexamples highlight the capabilities of the algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.5977v1"
    },
    {
        "title": "Optimization of the Multigrid-Convergence Rate on Semi-structured Meshes\n  by Local Fourier Analysis",
        "authors": [
            "B. Gmeiner",
            "T. Gradl",
            "F. Gaspar",
            "U. Rüde"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this paper a local Fourier analysis for multigrid methods on tetrahedral\ngrids is presented. Different smoothers for the discretization of the Laplace\noperator by linear finite elements on such grids are analyzed. A four-color\nsmoother is presented as an efficient choice for regular tetrahedral grids,\nwhereas line and plane relaxations are needed for poorly shaped tetrahedra. A\nnovel partitioning of the Fourier space is proposed to analyze the four-color\nsmoother. Numerical test calculations validate the theoretical predictions. A\nmultigrid method is constructed in a block-wise form, by using different\nsmoothers and different numbers of pre- and post-smoothing steps in each\ntetrahedron of the coarsest grid of the domain. Some numerical experiments are\npresented to illustrate the efficiency of this multigrid algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.7254v1"
    },
    {
        "title": "Solving Stress and Compliance Constrained Volume Minimization using\n  Anisotropic Mesh Adaptation, the Method of Moving Asymptotes and a Global\n  p-norm",
        "authors": [
            "Kristian Jensen"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  The p-norm often used in stress constrained topology optimisation supposedly\nmimics a delta function and it is thus characterised by a small length scale\nand ideally one would also prefer to have the solid-void transition occur over\na small length scale, since the material in this transition does not have a\nclear physical interpretation. We propose to resolve these small length scales\nusing anisotropic mesh adaptation. We use the method of moving asymptotes with\ninterpolation of sensitivities, asymptotes and design variables between\niterations. We demonstrate this combination for the portal and L-bracket\nproblems with p=10, and we are able to investigate mesh dependence. Finally, we\nsuggest relaxing the L-bracket problem statement by introducing a rounded\ncorner.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.8104v2"
    },
    {
        "title": "Mechanics-based solution verification for porous media models",
        "authors": [
            "M. Shabouei",
            "K. B. Nakshatrala"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This paper presents a new approach to verify accuracy of computational\nsimulations. We develop mathematical theorems which can serve as robust a\nposteriori error estimation techniques to identify numerical pollution, check\nthe performance of adaptive meshes, and verify numerical solutions. We\ndemonstrate performance of this methodology on problems from flow thorough\nporous media. However, one can extend it to other models. We construct\nmathematical properties such that the solutions to Darcy and Darcy-Brinkman\nequations satisfy them. The mathematical properties include the total minimum\nmechanical power, minimum dissipation theorem, reciprocal relation, and maximum\nprinciple for the vorticity. All the developed theorems have firm mechanical\nbases and are independent of numerical methods. So, these can be utilized for\nsolution verification of finite element, finite volume, finite difference,\nlattice Boltzmann methods and so forth. In particular, we show that, for a\ngiven set of boundary conditions, Darcy velocity has the minimum total\nmechanical power of all the kinematically admissible vector fields. We also\nshow that a similar result holds for Darcy-Brinkman velocity. We then show for\na conservative body force, the Darcy and Darcy-Brinkman velocities have the\nminimum total dissipation among their respective kinematically admissible\nvector fields. Using numerical examples, we show that the minimum dissipation\nand total mechanical power theorems can be utilized to identify pollution\nerrors in numerical solutions. The solutions to Darcy and Darcy-Brinkman\nequations are shown to satisfy a reciprocal relation, which has the potential\nto identify errors in the numerical implementation of boundary conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.02581v2"
    },
    {
        "title": "A simple approach to the numerical simulation with trimmed CAD surfaces",
        "authors": [
            "Gernot Beer",
            "Benjamin Marussig",
            "Jürgen Zechner"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this work a novel method for the analysis with trimmed CAD surfaces is\npresented. The method involves an additional mapping step and the attraction\nstems from its sim- plicity and ease of implementation into existing Finite\nElement (FEM) or Boundary Element (BEM) software. The method is first verified\nwith classical test examples in structural mechanics. Then two practical\napplications are presented one using the FEM, the other the BEM, that show the\napplicability of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.06741v1"
    },
    {
        "title": "Geometric Integration Over Irregular Domains with topologic Guarantees",
        "authors": [
            "Christian Engwer",
            "Andreas Nüßing"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Implicitly described domains are a well established tool in the simulation of\ntime dependent problems, e.g. using level-set methods. In order to solve\npartial differential equations on such domains, a range of numerical methods\nwas developed, e.g. the Immersed Boundary method, Unfitted Finite Element or\nUnfitted discontinuous Galerkin methods, eXtended or Generalised Finite Element\nmethods, just to name a few. Many of these methods involve integration over\ncut-cells or their boundaries, as they are described by sub-domains of the\noriginal level-set mesh. We present a new algorithm to geometrically evaluate\nthe integrals over domains described by a first-order, conforming level-set\nfunction. The integration is based on a polyhedral reconstruction of the\nimplicit geometry, following the concepts of the Marching Cubes algorithm. The\nalgorithm preserves various topological properties of the implicit geometry in\nits polyhedral reconstruction, making it suitable for Finite Element\ncomputations. Numerical experiments show second order accuracy of the\nintegration. An implementation of the algorithm is available as free software,\nwhich allows for an easy incorporation into other projects. The software is in\nproductive use within the DUNE framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.03597v1"
    },
    {
        "title": "Distributed Low Rank Approximation of Implicit Functions of a Matrix",
        "authors": [
            "David P. Woodruff",
            "Peilin Zhong"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We study distributed low rank approximation in which the matrix to be\napproximated is only implicitly represented across the different servers. For\nexample, each of $s$ servers may have an $n \\times d$ matrix $A^t$, and we may\nbe interested in computing a low rank approximation to $A = f(\\sum_{t=1}^s\nA^t)$, where $f$ is a function which is applied entrywise to the matrix\n$\\sum_{t=1}^s A^t$. We show for a wide class of functions $f$ it is possible to\nefficiently compute a $d \\times d$ rank-$k$ projection matrix $P$ for which\n$\\|A - AP\\|_F^2 \\leq \\|A - [A]_k\\|_F^2 + \\varepsilon \\|A\\|_F^2$, where $AP$\ndenotes the projection of $A$ onto the row span of $P$, and $[A]_k$ denotes the\nbest rank-$k$ approximation to $A$ given by the singular value decomposition.\nThe communication cost of our protocols is $d \\cdot (sk/\\varepsilon)^{O(1)}$,\nand they succeed with high probability. Our framework allows us to efficiently\ncompute a low rank approximation to an entry-wise softmax, to a Gaussian kernel\nexpansion, and to $M$-Estimators applied entrywise (i.e., forms of robust low\nrank approximation). We also show that our additive error approximation is best\npossible, in the sense that any protocol achieving relative error for these\nproblems requires significantly more communication. Finally, we experimentally\nvalidate our algorithms on real datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.07721v1"
    },
    {
        "title": "Factorizing the factorization - a spectral-element solver for elliptic\n  equations with linear operation count",
        "authors": [
            "Immo Huismann",
            "Jörg Stiller",
            "Jochen Fröhlich"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  High-order methods gain more and more attention in computational fluid\ndynamics. However, the potential advantage of these methods depends critically\non the availability of efficient elliptic solvers. With spectral-element\nmethods, static condensation is a common approach to reduce the number of\ndegree of freedoms and to improve the condition of the algebraic equations. The\nresulting system is block-structured and the face-based operator well suited\nfor matrix-matrix multiplications. However, a straight-forward implementation\nscales super-linearly with the number of unknowns and, therefore, prohibits the\napplication to high polynomial degrees. This paper proposes a novel\nfactorization technique, which yields a linear operation count of just 13N\nmultiplications, where N is the total number of unknowns. In comparison to\nprevious work it saves a factor larger than 3 and clearly outpaces unfactored\nvariants for all polynomial degrees. Using the new technique as a building\nblock for a preconditioned conjugate gradient method resulted in a runtime\nscaling linearly with N for polynomial degrees $2 \\leq p \\leq 32$ . Moreover\nthe solver proved remarkably robust for aspect ratios up to 128.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.08179v1"
    },
    {
        "title": "Wanted: Floating-Point Add Round-off Error instruction",
        "authors": [
            "Marat Dukhan",
            "Richard Vuduc",
            "Jason Riedy"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We propose a new instruction (FPADDRE) that computes the round-off error in\nfloating-point addition. We explain how this instruction benefits\nhigh-precision arithmetic operations in applications where double precision is\nnot sufficient. Performance estimates on Intel Haswell, Intel Skylake, and AMD\nSteamroller processors, as well as Intel Knights Corner co-processor,\ndemonstrate that such an instruction would improve the latency of double-double\naddition by up to 55% and increase double-double addition throughput by up to\n103%, with smaller, but non-negligible benefits for double-double\nmultiplication. The new instruction delivers up to 2x speedups on three\nbenchmarks that use high-precision floating-point arithmetic: double-double\nmatrix-matrix multiplication, compensated dot product, and polynomial\nevaluation via the compensated Horner scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.00491v1"
    },
    {
        "title": "A Data-Scalable Randomized Misfit Approach for Solving Large-Scale\n  PDE-Constrained Inverse Problems",
        "authors": [
            "Ellen B. Le",
            "Aaron Myers",
            "Tan Bui-Thanh",
            "Quoc P. Nguyen"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  A randomized misfit approach is presented for the efficient solution of\nlarge-scale PDE-constrained inverse problems with high-dimensional data. The\npurpose of this paper is to offer a theory-based framework for random\nprojections in this inverse problem setting. The stochastic approximation to\nthe misfit is analyzed using random projection theory. By expanding beyond mean\nestimator convergence, a practical characterization of randomized misfit\nconvergence can be achieved. The theoretical results developed hold with any\nvalid random projection in the literature. The class of feasible distributions\nis broad yet simple to characterize compared to previous stochastic misfit\nmethods. This class includes very sparse random projections which provide\nadditional computational benefit. A different proof for a variant of the\nJohnson-Lindenstrauss lemma is also provided. This leads to a different\nintuition for the $O(\\epsilon^{-2})$ factor in bounds for Johnson-Lindenstrauss\nresults. The main contribution of this paper is a theoretical result showing\nthe method guarantees a valid solution for small reduced misfit dimensions. The\ninterplay between Johnson-Lindenstrauss theory and Morozov's discrepancy\nprinciple is shown to be essential to the result. The computational cost\nsavings for large-scale PDE-constrained problems with high- dimensional data is\ndiscussed. Numerical verification of the developed theory is presented for\nmodel problems of estimating a distributed parameter in an elliptic partial\ndifferential equation. Results with different random projections are presented\nto demonstrate the viability and accuracy of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01562v3"
    },
    {
        "title": "A Penalty Function Promoting Sparsity Within and Across Groups",
        "authors": [
            "İlker Bayram",
            "Savaşkan Bulek"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We introduce a new weakly-convex penalty function for signals with a group\nbehavior. The penalty promotes signals with a few number of active groups,\nwhere within each group, only a few high magnitude coefficients are active. We\nderive the threshold function associated with the proposed penalty and study\nits properties. We discuss how the proposed penalty/threshold function can be\nuseful for signals with isolated non-zeros, such as audio with isolated\nharmonics along the frequency axis, or reflection functions in exploration\nseismology where the non-zeros occur on the boundaries of subsoil layers. We\ndemonstrate the use of the proposed penalty/threshold functions in a convex\ndenoising and a non-convex deconvolution formulation. We provide convergent\nalgorithms for both formulations and compare the performance with\nstate-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.03650v2"
    },
    {
        "title": "On the Petras algorithm for verified integration of piecewise analytic\n  functions",
        "authors": [
            "Małgorzata Moczurad",
            "Piotr Zgliczyński"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We consider the algorithm for verified integration of piecewise analytic\nfunctions given by Petras. The analysis of the algorithm contained in Patras'\npaper is limited to a narrow class of functions and gives upper bounds only. We\npresent an estimation of the complexity (measured by a number of evaluations of\nan integrand) of the algorithm, both upper and lower bounds, for a wider class\nof functions. We show examples with complexity $\\Theta(|\\ln\\eps|/\\eps^{p-1})$,\nfor any $p >1$, where $\\eps$ is the desired accuracy of the computed integral.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.03945v1"
    },
    {
        "title": "A note on the convergence of nonconvex line search",
        "authors": [
            "Tao Sun",
            "Lizhi Chenga",
            "Hao Jiang"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In this note, we consider the line search for a class of abstract nonconvex\nalgorithm which have been deeply studied in the Kurdyka-Lojasiewicz theory. We\nprovide a weak convergence result of the line search in general. When the\nobjective function satisfies the Kurdyka-Lojasiewicz property and some certain\nassumption, a global convergence result can be derived. An application is\npresented for the L0-regularized least square minimization in the end of the\npaper.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06912v1"
    },
    {
        "title": "Stable Isogeometric Analysis of Trimmed Geometries",
        "authors": [
            "Benjamin Marussig",
            "Jürgen Zechner",
            "Gernot Beer",
            "Thomas-Peter Fries"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We explore extended B-splines as a stable basis for isogeometric analysis\nwith trimmed parameter spaces. The stabilization is accomplished by an\nappropriate substitution of B-splines that may lead to ill-conditioned system\nmatrices. The construction for non-uniform knot vectors is presented. The\nproperties of extended B-splines are examined in the context of interpolation,\npotential, and linear elasticity problems and excellent results are attained.\nThe analysis is performed by an isogeometric boundary element formulation using\ncollocation. It is argued that extended B-splines provide a flexible and simple\nstabilization scheme which ideally suits the isogeometric paradigm.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.09660v2"
    },
    {
        "title": "Conformal higher-order remeshing schemes for implicitly defined\n  interface problems",
        "authors": [
            "Samir Omerović",
            "Thomas-Peter Fries"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  A new higher-order accurate method is proposed that combines the advantages\nof the classical $p$-version of the FEM on body-fitted meshes with embedded\ndomain methods. A background mesh composed by higher-order Lagrange elements is\nused. Boundaries and interfaces are described implicitly by the level set\nmethod and are within elements. In the elements cut by the boundaries or\ninterfaces, an automatic decomposition into higher-order accurate sub-elements\nis realized. Therefore, the zero level sets are detected and meshed in a first\nstep which is called reconstruction. Then, based on the topological situation\nin the cut element, higher-order sub-elements are mapped to the two sides of\nthe boundary or interface. The quality of the reconstruction and the mapping\nlargely determines the properties of the resulting, automatically generated\nconforming mesh. It is found that optimal convergence rates are possible\nalthough the resulting sub-elements are not always well-shaped.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.09678v2"
    },
    {
        "title": "How Many Real Attractive Fixed Points Can A Polynomial Have?",
        "authors": [
            "Terence Coelho",
            "Bahman Kalantari"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We prove a complex polynomial of degree $n$ has at most $\\lceil n/2 \\rceil$\nattractive fixed points lying on a line. We also consider the general case.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.07859v1"
    },
    {
        "title": "Higher-order meshing of implicit geometries - part I: Integration and\n  interpolation in cut elements",
        "authors": [
            "T. P. Fries",
            "S. Omerović",
            "D. Schöllhammer",
            "J. Steidl"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  An accurate implicit description of geometries is enabled by the level-set\nmethod. Level-set data is given at the nodes of a higher-order background mesh\nand the interpolated zero-level sets imply boundaries of the domain or\ninterfaces within. The higher-order accurate integration of elements cut by the\nzero-level sets is described. The proposed strategy relies on an automatic\nmeshing of the cut elements. Firstly, the zero-level sets are identified and\nmeshed by higher-order interface elements. Secondly, the cut elements are\ndecomposed into conforming sub-elements on the two sides of the zero-level\nsets. Any quadrature rule may then be employed within the sub-elements. The\napproach is described in two and three dimensions without any requirements on\nthe background meshes. Special attention is given to the consideration of\ncorners and edges of the implicit geometries.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00578v1"
    },
    {
        "title": "Fast Eigen Decomposition for Low-Rank Matrix Approximation",
        "authors": [
            "Youhei Akimoto"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  In this paper we present an efficient algorithm to compute the eigen\ndecomposition of a matrix that is a weighted sum of the self outer products of\nvectors such as a covariance matrix of data. A well known algorithm to compute\nthe eigen decomposition of such matrices is though the singular value\ndecomposition, which is available only if all the weights are nonnegative. Our\nproposed algorithm accepts both positive and negative weights.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.02069v1"
    },
    {
        "title": "Asymptotic convergence of spectral inverse iterations for stochastic\n  eigenvalue problems",
        "authors": [
            "Harri Hakula",
            "Mikael Laaksonen"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We consider and analyze applying a spectral inverse iteration algorithm and\nits subspace iteration variant for computing eigenpairs of an elliptic operator\nwith random coefficients. With these iterative algorithms the solution is\nsought from a finite dimensional space formed as the tensor product of the\napproximation space for the underlying stochastic function space, and the\napproximation space for the underlying spatial function space. Sparse\npolynomial approximation is employed to obtain the first one, while classical\nfinite elements are employed to obtain the latter. An error analysis is\npresented for the asymptotic convergence of the spectral inverse iteration to\nthe smallest eigenvalue and the associated eigenvector of the problem. A series\nof detailed numerical experiments supports the conclusions of this analysis.\nNumerical experiments are also presented for the spectral subspace iteration,\nand convergence of the algorithm is observed in an example case, where the\neigenvalues cross within the parameter space. The outputs of both algorithms\nare verified by comparing to solutions obtained by a sparse stochastic\ncollocation method.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.03558v1"
    },
    {
        "title": "On the numerical rank of radial basis function kernels in high dimension",
        "authors": [
            "Ruoxi Wang",
            "Yingzhou Li",
            "Eric Darve"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Low-rank approximations are popular methods to reduce the high computational\ncost of algorithms involving large-scale kernel matrices. The success of\nlow-rank methods hinges on the matrix rank of the kernel matrix, and in\npractice, these methods are effective even for high-dimensional datasets. Their\npractical success motivates our analysis of the function rank, an upper bound\nof the matrix rank. In this paper, we consider radial basis functions (RBF),\napproximate the RBF kernel with a low-rank representation that is a finite sum\nof separate products and provide explicit upper bounds on the function rank and\nthe $L_\\infty$ error for such approximations. Our three main results are as\nfollows. First, for a fixed precision, the function rank of RBFs, in the worst\ncase, grows polynomially with the data dimension. Second, precise error bounds\nfor the low-rank approximations in the $L_\\infty$ norm are derived in terms of\nthe function smoothness and the domain diameters. Finally, a group pattern in\nthe magnitude of singular values for RBF kernel matrices is observed and\nanalyzed, and is explained by a grouping of the expansion terms in the kernel's\nlow-rank representation. Empirical results verify the theoretical results.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.07883v3"
    },
    {
        "title": "A Class of Multirate Infinitesimal GARK Methods",
        "authors": [
            "Adrian Sandu"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Differential equations arising in many practical applications are\ncharacterized by multiple time scales. Multirate time integration seeks to\nsolve them efficiently by discretizing each scale with a different, appropriate\ntime step, while ensuring the overall accuracy and stability of the numerical\nsolution. In a seminal paper Knoth and Wolke (APNUM, 1998) proposed a hybrid\nsolution approach: discretize the slow component with an explicit Runge-Kutta\nmethod, and advance the fast component via a modified fast differential\nequation. The idea led to the development of multirate infinitesimal step (MIS)\nmethods by Wensch et al. (BIT, 2009.)G\\\"{u}nther and Sandu (BIT, 2016)\nexplained MIS schemes as a particular case of multirate General-structure\nAdditive Runge-Kutta (MR-GARK) methods. The hybrid approach offers extreme\nflexibility in the choice of the numerical solution process for the fast\ncomponent.\n  This work constructs a family of multirate infinitesimal GARK schemes\n(MRI-GARK) that extends the hybrid dynamics approachin multiple ways. Order\nconditions theory and stability analyses are developed, and practical explicit\nand implicit methods of up to order four are constructed. Numerical results\nconfirm the theoretical findings. We expect the new MRI-GARK family to be most\nuseful for systems of equations with widely disparate time scales, where the\nfast process is dispersive, and where the influence of the fast component on\nthe slow dynamics is weak.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.02759v3"
    },
    {
        "title": "Random Walk Laplacian and Network Centrality Measures",
        "authors": [
            "Daniel Boley",
            "Alejandro Buendia",
            "Golshan Golnari"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Random walks over directed graphs are used to model activities in many\ndomains, such as social networks, influence propagation, and Bayesian graphical\nmodels. They are often used to compute the importance or centrality of\nindividual nodes according to a variety of different criteria. Here we show how\nthe pseudoinverse of the \"random walk\" Laplacian can be used to quickly compute\nmeasures such as the average number of visits to a given node and various\ncentrality and betweenness measures for individual nodes, both for the network\nin general and in the case a subset of nodes is to be avoided. We show that\nwith a single matrix inversion it is possible to rapidly compute many such\nquantities.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.02912v1"
    },
    {
        "title": "Choosing the optimal multi-point iterative method for the Colebrook flow\n  friction equation -- Numerical validation",
        "authors": [
            "Pavel Praks",
            "Dejan Brkic"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The Colebrook equation $\\zeta$ is implicitly given in respect to the unknown\nflow friction factor $\\lambda$; $\\lambda=\\zeta(Re,\\epsilon^*,\\lambda)$ which\ncannot be expressed explicitly in exact way without simplifications and use of\napproximate calculus. Common approach to solve it is through the Newton-Raphson\niterative procedure or through the fixed-point iterative procedure. Both\nrequires in some case even eight iterations. On the other hand numerous more\npowerful iterative methods such as three-or two-point methods, etc. are\navailable. The purpose is to choose optimal iterative method in order to solve\nthe implicit Colebrook equation for flow friction accurately using the least\npossible number of iterations. The methods are thoroughly tested and those\nwhich require the least possible number of iterations to reach the accurate\nsolution are identified. The most powerful three-point methods require in worst\ncase only two iterations to reach final solution. The recommended\nrepresentatives are Sharma-Guha-Gupta, Sharma-Sharma, Sharma-Arora,\nD\\v{z}uni\\'c-Petkovi\\'c-Petkovi\\'c; Bi-Ren-Wu, Chun-Neta based on Kung-Traub,\nNeta, and Jain method based on Steffensen scheme. The recommended iterative\nmethods can reach the final accurate solution with the least possible number of\niterations. The approach is hybrid between iterative procedure and one-step\nexplicit approximations and can be used in engineering design for initial\nrough, but also for final fine calculations.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.03568v1"
    },
    {
        "title": "Stochastic Collocation with Non-Gaussian Correlated Parameters via a New\n  Quadrature Rule",
        "authors": [
            "Chunfeng Cui",
            "Zheng Zhang"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  This paper generalizes stochastic collocation methods to handle correlated\nnon-Gaussian random parameters. The key challenge is to perform a multivariate\nnumerical integration in a correlated parameter space when computing the\ncoefficient of each basis function via a projection step. We propose an\noptimization model and a block coordinate descent solver to compute the\nrequired quadrature samples. Our method is verified with a CMOS ring oscillator\nand an optical ring resonator, showing 3000x speedup over Monte Carlo.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.08381v1"
    },
    {
        "title": "Spectrum-Adapted Polynomial Approximation for Matrix Functions",
        "authors": [
            "Li Fan",
            "David I Shuman",
            "Shashanka Ubaru",
            "Yousef Saad"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We propose and investigate two new methods to approximate $f({\\bf A}){\\bf b}$\nfor large, sparse, Hermitian matrices ${\\bf A}$. The main idea behind both\nmethods is to first estimate the spectral density of ${\\bf A}$, and then find\npolynomials of a fixed order that better approximate the function $f$ on areas\nof the spectrum with a higher density of eigenvalues. Compared to\nstate-of-the-art methods such as the Lanczos method and truncated Chebyshev\nexpansion, the proposed methods tend to provide more accurate approximations of\n$f({\\bf A}){\\bf b}$ at lower polynomial orders, and for matrices ${\\bf A}$ with\na large number of distinct interior eigenvalues and a small spectral width.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.09506v1"
    },
    {
        "title": "A fast Fourier transform based direct solver for the Helmholtz problem",
        "authors": [
            "Jari Toivanen",
            "Monika Wolfmayr"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  This paper is devoted to the efficient numerical solution of the Helmholtz\nequation in a two- or three-dimensional rectangular domain with an absorbing\nboundary condition (ABC). The Helmholtz problem is discretized by standard\nbilinear and trilinear finite elements on an orthogonal mesh yielding a\nseparable system of linear equations. The main key to high performance is to\nemploy the Fast Fourier transform (FFT) within a fast direct solver to solve\nthe large separable systems. The computational complexity of the proposed FFT\nbased direct solver is O(N log N) operations. Numerical results for both two-\nand three-dimensional problems are presented confirming the efficiency of the\nmethod discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.03808v3"
    },
    {
        "title": "Memory footprint reduction for the FFT-based volume integral equation\n  method via tensor decompositions",
        "authors": [
            "Ilias I. Giannakopoulos",
            "Mikhail S. Litsarev",
            "Athanasios G. Polimeridis"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We present a method of memory footprint reduction for FFT-based,\nelectromagnetic (EM) volume integral equation (VIE) formulations. The arising\nGreen's function tensors have low multilinear rank, which allows Tucker\ndecomposition to be employed for their compression, thereby greatly reducing\nthe required memory storage for numerical simulations. Consequently, the\ncompressed components are able to fit inside a graphical processing unit (GPU)\non which highly parallelized computations can vastly accelerate the iterative\nsolution of the arising linear system. In addition, the element-wise products\nthroughout the iterative solver's process require additional flops, thus, we\nprovide a variety of novel and efficient methods that maintain the linear\ncomplexity of the classic element-wise product with an additional\nmultiplicative small constant. We demonstrate the utility of our approach via\nits application to VIE simulations for the Magnetic Resonance Imaging (MRI) of\na human head. For these simulations we report an order of magnitude\nacceleration over standard techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.00484v3"
    },
    {
        "title": "Rethinking floating point for deep learning",
        "authors": [
            "Jeff Johnson"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Reducing hardware overhead of neural networks for faster or lower power\ninference and training is an active area of research. Uniform quantization\nusing integer multiply-add has been thoroughly investigated, which requires\nlearning many quantization parameters, fine-tuning training or other\nprerequisites. Little effort is made to improve floating point relative to this\nbaseline; it remains energy inefficient, and word size reduction yields drastic\nloss in needed dynamic range. We improve floating point to be more energy\nefficient than equivalent bit width integer hardware on a 28 nm ASIC process\nwhile retaining accuracy in 8 bits with a novel hybrid log multiply/linear add,\nKulisch accumulation and tapered encodings from Gustafson's posit format. With\nno network retraining, and drop-in replacement of all math and float32\nparameters via round-to-nearest-even only, this open-sourced 8-bit log float is\nwithin 0.9% top-1 and 0.2% top-5 accuracy of the original float32 ResNet-50 CNN\nmodel on ImageNet. Unlike int8 quantization, it is still a general purpose\nfloating point arithmetic, interpretable out-of-the-box. Our 8/38-bit log float\nmultiply-add is synthesized and power profiled at 28 nm at 0.96x the power and\n1.12x the area of 8/32-bit integer multiply-add. In 16 bits, our log float\nmultiply-add is 0.59x the power and 0.68x the area of IEEE 754 float16 fused\nmultiply-add, maintaining the same signficand precision and dynamic range,\nproving useful for training ASICs as well.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.01721v1"
    },
    {
        "title": "Precision of the ENDGame: Mixed-precision arithmetic in the iterative\n  solver of the Unified Model",
        "authors": [
            "Christopher M Maynard",
            "David N Walters"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The Met Office's weather and climate simulation code the Unified Model is\nused for both operational Numerical Weather Prediction and Climate modelling.\nThe computational performance of the model running on parallel supercomputers\nis a key consideration. A Krylov sub-space solver is employed to solve the\nequations of the dynamical core of the model, known as ENDGame. These describe\nthe evolution of the Earth's atmosphere. Typically, 64-bit precision is used\nthroughout weather and climate applications. This work presents a\nmixed-precision implementation of the solver, the beneficial effect on run-time\nand the impact on solver convergence. The complex interplay of errors arising\nfrom accumulated round-off in floating-point arithmetic and other numerical\neffects is discussed. A careful analysis is required, however, the\nmixed-precision solver is now employed in the operational forecast to satisfy\nrun-time constraints without compromising the accuracy of the solution.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03852v1"
    },
    {
        "title": "How to get meaningful and correct results from your finite element model",
        "authors": [
            "Martin Bäker"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  This document gives guidelines to set up, run, and postprocess correct\nsimulations with the finite element method. It is not an introduction to the\nmethod itself, but rather a list of things to check and possible mistakes to\nwatch out for when doing a finite element simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.05753v1"
    },
    {
        "title": "Randomized Rank-Revealing UZV Decomposition for Low-Rank Approximation\n  of Matrices",
        "authors": [
            "Maboud F. Kaloorazi",
            "Rodrigo C. de Lamare"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Low-rank matrix approximation plays an increasingly important role in signal\nand image processing applications. This paper presents a new rank-revealing\ndecomposition method called randomized rank-revealing UZV decomposition\n(RRR-UZVD). RRR-UZVD is powered by randomization to approximate a low-rank\ninput matrix. Given a large and dense matrix ${\\bf A} \\in \\mathbb R^{m \\times\nn}$ whose numerical rank is $k$, where $k$ is much smaller than $m$ and $n$,\nRRR-UZVD constructs an approximation $\\hat{\\bf A}$ such as $\\hat{\\bf A}={\\bf\nUZV}^T$, where ${\\bf U}$ and ${\\bf V}$ have orthonormal columns, the\nleading-diagonal block of ${\\bf Z}$ reveals the rank of $\\bf A$, and its\noff-diagonal blocks have small $\\ell_2$-norms. RRR-UZVD is simple, accurate,\nand only requires a few passes through $\\bf A$ with an arithmetic cost of\n$O(mnk)$ floating-point operations. To demonstrate the effectiveness of the\nproposed method, we conduct experiments on synthetic data, as well as real data\nin applications of image reconstruction and robust principal component\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.08597v1"
    },
    {
        "title": "On an explicit finite difference method for fractional diffusion\n  equations",
        "authors": [
            "S. B. Yuste",
            "L. Acedo"
        ],
        "category": "cs.NA",
        "published_year": "2003",
        "summary": "  A numerical method to solve the fractional diffusion equation, which could\nalso be easily extended to many other fractional dynamics equations, is\nconsidered. These fractional equations have been proposed in order to describe\nanomalous transport characterized by non-Markovian kinetics and the breakdown\nof Fick's law. In this paper we combine the forward time centered space (FTCS)\nmethod, well known for the numerical integration of ordinary diffusion\nequations, with the Grunwald-Letnikov definition of the fractional derivative\noperator to obtain an explicit fractional FTCS scheme for solving the\nfractional diffusion equation. The resulting method is amenable to a stability\nanalysis a la von Neumann. We show that the analytical stability bounds are in\nexcellent agreement with numerical tests. Comparison between exact analytical\nsolutions and numerical predictions are made.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0311011v1"
    },
    {
        "title": "Using matrices in post-processing phase of CFD simulations",
        "authors": [
            "Gianluca Argentini"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  In this work I present a technique of construction and fast evaluation of a\nfamily of cubic polynomials for analytic smoothing and graphical rendering of\nparticles trajectories for flows in a generic geometry. The principal result of\nthe work was implementation and test of a method for interpolating 3D points by\nregular parametric curves and their fast and efficient evaluation for a good\nresolution of rendering. For the purpose I have used a parallel environment\nusing a multiprocessor cluster architecture. The efficiency of the used method\nis good, mainly reducing the number of floating-points computations by caching\nthe numerical values of some line-parameter's powers, and reducing the\nnecessity of communication among processes. This work has been developed for\nthe Research and Development Department of my company for planning advanced\ncustomized models of industrial burners.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0404047v1"
    },
    {
        "title": "Weighted average finite difference methods for fractional diffusion\n  equations",
        "authors": [
            "Santos B. Yuste"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  Weighted averaged finite difference methods for solving fractional diffusion\nequations are discussed and different formulae of the discretization of the\nRiemann-Liouville derivative are considered. The stability analysis of the\ndifferent numerical schemes is carried out by means of a procedure close to the\nwell-known von Neumann method of ordinary diffusion equations. The stability\nbounds are easily found and checked in some representative examples.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0408053v1"
    },
    {
        "title": "Using sparse matrices and splines-based interpolation in computational\n  fluid dynamics simulations",
        "authors": [
            "Gianluca Argentini"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  In this relation I present a technique of construction and fast evaluation of\na family of cubic polynomials for analytic smoothing and graphical rendering of\nparticles trajectories for flows in a generic geometry. The principal result of\nthe work was implementation and test of a method for interpolating 3D points by\nregular parametric curves and their fast and efficient evaluation for a good\nresolution of rendering. For the purpose a parallel environment using a\nmultiprocessor cluster architecture has been used. This work has been developed\nfor the Research and Development Department of my company for planning advanced\ncustomized models of industrial burners.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0409056v1"
    },
    {
        "title": "Numerical Solutions of 2-D Steady Incompressible Driven Cavity Flow at\n  High Reynolds Numbers",
        "authors": [
            "E. Erturk",
            "T. C. Corke",
            "C. Gokcol"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  Numerical calculations of the 2-D steady incompressible driven cavity flow\nare presented. The Navier-Stokes equations in streamfunction and vorticity\nformulation are solved numerically using a fine uniform grid mesh of 601x601.\nThe steady driven cavity solutions are computed for Re<21,000 with a maximum\nabsolute residuals of the governing equations that were less than 10-10. A new\nquaternary vortex at the bottom left corner and a new tertiary vortex at the\ntop left corner of the cavity are observed in the flow field as the Reynolds\nnumber increases. Detailed results are presented and comparisons are made with\nbenchmark solutions found in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0411047v3"
    },
    {
        "title": "Discussions on Driven Cavity Flow",
        "authors": [
            "E. Erturk"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  The widely studied benchmark problem, 2-D driven cavity flow problem is\ndiscussed in details in terms of physical and mathematical and also numerical\naspects. A very brief literature survey on studies on the driven cavity flow is\ngiven. Based on the several numerical and experimental studies, the fact of the\nmatter is, above moderate Reynolds numbers physically the flow in a driven\ncavity is not two-dimensional. However there exist numerical solutions for 2-D\ndriven cavity flow at high Reynolds numbers.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0411048v3"
    },
    {
        "title": "Fourth Order Compact Formulation of Navier-Stokes Equations and Driven\n  Cavity Flow at High Reynolds Numbers",
        "authors": [
            "E. Erturk",
            "C. Gokcol"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  A new fourth order compact formulation for the steady 2-D incompressible\nNavier-Stokes equations is presented. The formulation is in the same form of\nthe Navier-Stokes equations such that any numerical method that solve the\nNavier-Stokes equations can also be applied to this fourth order compact\nformulation. In particular in this work the formulation is solved with an\nefficient numerical method that requires the solution of tridiagonal systems\nusing a fine grid mesh of 601x601. Using this formulation, the steady 2-D\nincompressible flow in a driven cavity is solved up to Reynolds number of\n20,000 with fourth order spatial accuracy. Detailed solutions are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0411049v2"
    },
    {
        "title": "An estimate of accuracy for interpolant numerical solutions of a PDE\n  problem",
        "authors": [
            "Gianluca Argentini"
        ],
        "category": "cs.NA",
        "published_year": "2004",
        "summary": "  In this paper we present an estimate of accuracy for a piecewise polynomial\napproximation of a classical numerical solution to a non linear differential\nproblem. We suppose the numerical solution U is computed using a grid with a\nsmall linear step and interval time Tu, while the polynomial approximation V is\nan interpolation of the values of a numerical solution on a less fine grid and\ninterval time Tv << Tu. The estimate shows that the interpolant solution V can\nbe, under suitable hypotheses, a good approximation and in general its\ncomputational cost is much lower of the cost of the fine numerical solution. We\npresent two possible applications to linear case and periodic case.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0412120v2"
    },
    {
        "title": "The complexity of class polynomial computation via floating point\n  approximations",
        "authors": [
            "Andreas Enge"
        ],
        "category": "cs.NA",
        "published_year": "2006",
        "summary": "  We analyse the complexity of computing class polynomials, that are an\nimportant ingredient for CM constructions of elliptic curves, via complex\nfloating point approximations of their roots. The heart of the algorithm is the\nevaluation of modular functions in several arguments. The fastest one of the\npresented approaches uses a technique devised by Dupont to evaluate modular\nfunctions by Newton iterations on an expression involving the\narithmetic-geometric mean. It runs in time $O (|D| \\log^5 |D| \\log \\log |D|) =\nO (|D|^{1 + \\epsilon}) = O (h^{2 + \\epsilon})$ for any $\\epsilon > 0$, where\n$D$ is the CM discriminant and $h$ is the degree of the class polynomial.\nAnother fast algorithm uses multipoint evaluation techniques known from\nsymbolic computation; its asymptotic complexity is worse by a factor of $\\log\n|D|$. Up to logarithmic factors, this running time matches the size of the\nconstructed polynomials. The estimate also relies on a new result concerning\nthe complexity of enumerating the class group of an imaginary-quadratic order\nand on a rigorously proven upper bound for the height of class polynomials.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0601104v2"
    },
    {
        "title": "On local symbolic approximation and resolution of ODEs using Implicit\n  Function Theorem",
        "authors": [
            "Gianluca Argentini"
        ],
        "category": "cs.NA",
        "published_year": "2006",
        "summary": "  In this work the implicit function theorem is used for searching local\nsymbolic resolution of differential equations. General results of existence for\nfirst order equations are proven and some examples, one relative to cavitation\nin a fluid, are developed. These examples seem to show that local approximation\nof non linear differential equations can give useful informations about\nsymbolic form of possible solutions, and in the case a global solution is\nknown, locally the accuracy of approximation can be good.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0602025v1"
    },
    {
        "title": "A Multi-server Scheduling Framework for Resource Allocation in Wireless\n  Multi-carrier Networks",
        "authors": [
            "Ying Jun Zhang"
        ],
        "category": "cs.NA",
        "published_year": "2006",
        "summary": "  Multiuser resource allocation has recently been recognized as an effective\nmethodology for enhancing the power and spectrum efficiency in OFDM (orthogonal\nfrequency division multiplexing) systems. It is, however, not directly\napplicable to current packet-switched networks, because (i) most existing\npacket-scheduling schemes are based on a single-server model and do not serve\nmultiple users at the same time; and (ii) the conventional separate design of\nMAC (medium access control) packet scheduling and PHY (physical) resource\nallocation yields inefficient resource utilization. In this paper, we propose a\ncross-layer resource allocation algorithm based on a novel multi-server\nscheduling framework to achieve overall high system power efficiency in\npacket-switched OFDM networks. Our contribution is four fold: (i) we propose\nand analyze a MPGPS (multi-server packetized general processor sharing) service\ndiscipline that serves multiple users at the same time and facilitates\nmultiuser resource allocation; (ii) we present a MPGPS-based joint MAC-PHY\nresource allocation scheme that incorporates packet scheduling, subcarrier\nallocation, and power allocation in an integrated framework; (iii) by\ninvestigating the fundamental tradeoff between multiuser-diversity and queueing\nperformance, we present an A-MPGPS (adaptive MPGPS) service discipline that\nstrikes balance between power efficiency and queueing performance; and (iv) we\nextend MPGPS to an O-MPGPS (opportunistic MPGPS) service discipline to further\nenhance the resource utilization efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0611080v1"
    },
    {
        "title": "Evolutionary Mesh Numbering: Preliminary Results",
        "authors": [
            "Francis Sourd",
            "Marc Schoenauer"
        ],
        "category": "cs.NA",
        "published_year": "2007",
        "summary": "  Mesh numbering is a critical issue in Finite Element Methods, as the\ncomputational cost of one analysis is highly dependent on the order of the\nnodes of the mesh. This paper presents some preliminary investigations on the\nproblem of mesh numbering using Evolutionary Algorithms. Three conclusions can\nbe drawn from these experiments. First, the results of the up-to-date method\nused in all FEM softwares (Gibb's method) can be consistently improved; second,\nnone of the crossover operators tried so far (either general or problem\nspecific) proved useful; third, though the general tendency in Evolutionary\nComputation seems to be the hybridization with other methods (deterministic or\nheuristic), none of the presented attempt did encounter any success yet. The\ngood news, however, is that this algorithm allows an improvement over the\nstandard heuristic method between 12% and 20% for both the 1545 and 5453-nodes\nmeshes used as test-bed. Finally, some strange interaction between the\nselection scheme and the use of problem specific mutation operator was\nobserved, which appeals for further investigation.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.1410v1"
    },
    {
        "title": "Subspace Pursuit for Compressive Sensing Signal Reconstruction",
        "authors": [
            "Wei Dai",
            "Olgica Milenkovic"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  We propose a new method for reconstruction of sparse signals with and without\nnoisy perturbations, termed the subspace pursuit algorithm. The algorithm has\ntwo important characteristics: low computational complexity, comparable to that\nof orthogonal matching pursuit techniques when applied to very sparse signals,\nand reconstruction accuracy of the same order as that of LP optimization\nmethods. The presented analysis shows that in the noiseless setting, the\nproposed algorithm can exactly reconstruct arbitrary sparse signals provided\nthat the sensing matrix satisfies the restricted isometry property with a\nconstant parameter. In the noisy setting and in the case that the signal is not\nexactly sparse, it can be shown that the mean squared error of the\nreconstruction is upper bounded by constant multiples of the measurement and\nsignal perturbation energies.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.0811v3"
    },
    {
        "title": "Nonorthogonal Bases and Phase Decomposition: Properties and Applications",
        "authors": [
            "Sossio Vergara"
        ],
        "category": "cs.NA",
        "published_year": "2008",
        "summary": "  In a previous paper [1] it was discussed the viability of functional analysis\nusing as a basis a couple of generic functions, and hence vectorial\ndecomposition. Here we complete the paradigm exploiting one of the analysis\nmethodologies developed there, but applied to phase coordinates, so needing\nonly one function as a basis. It will be shown that, thanks to the novel\niterative analysis, any function satisfying a rather loose requisite is\nontologically a basis. This in turn generalizes the polar version of the\nFourier theorem to an ample class of nonorthogonal bases. The main advantage of\nthis generalization is that it inherits some of the properties of the original\nFourier theorem. As a result the new transform has a wide range of applications\nand some remarkable consequences. The new tool will be compared with wavelets\nand frames. Examples of analysis and reconstruction of functions using the\ndeveloped algorithms and generic bases will be given. Some of the properties,\nand applications that can promptly benefit from the theory, will be discussed.\nThe implementation of a matched filter for noise suppression will be used as an\nexample of the potential of the theory.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.4347v2"
    },
    {
        "title": "A Fully Equivalent Global Pressure Formulation for Three-Phase\n  Compressible Flow",
        "authors": [
            "Guy Chavent"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  We introduce a new global pressure formulation for immiscible three-phase\ncompressible flows in porous media which is fully equivalent to the original\nequations, unlike the one introduced in \\cite{CJ86}. In this formulation, the\ntotal volumetric flow of the three fluids and the global pressure follow a\nclassical Darcy law, which simplifies the resolution of the pressure equation.\nHowever, this global pressure formulation exists only for Total Differential\n(TD) three-phase data, which depend only on two functions of saturations and\nglobal pressure: the global capillary pressure and the global mobility. Hence\nwe introduce a class of interpolation which constructs such TD-three-phase data\nfrom any set of three two-phase data (for each pair of fluids) which satisfy a\nTD-compatibility condition.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.1462v1"
    },
    {
        "title": "On the upstream mobility scheme for two-phase flow in porous media",
        "authors": [
            "Siddhartha Mishra",
            "Jérôme Jaffré"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  When neglecting capillarity, two-phase incompressible flow in porous media is\nmodelled as a scalar nonlinear hyperbolic conservation law. A change in the\nrock type results in a change of the flux function. Discretizing in\none-dimensional with a finite volume method, we investigate two numerical\nfluxes, an extension of the Godunov flux and the upstream mobility flux, the\nlatter being widely used in hydrogeology and petroleum engineering. Then, in\nthe case of a changing rock type, one can give examples when the upstream\nmobility flux does not give the right answer.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.4032v1"
    },
    {
        "title": "Estimating nonlinearities in twophase flow in porous media",
        "authors": [
            "Jianfeng Zhang",
            "Guy Chavent",
            "Jérôme Jaffré"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  In order to analyze numerically inverse problems several techniques based on\nlinear and nonlinear stability analysis are presented. These techniques are\nillustrated on the problem of estimating mobilities and capillary pressure in\none-dimensional two-phase displacements in porous media that are performed in\nlaboratories. This is an example of the problem of estimating nonlinear\ncoefficients in a system of nonlinear partial differential equations.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.1534v1"
    },
    {
        "title": "A Barzilai-Borwein $l_1$-Regularized Least Squares Algorithm for\n  Compressed Sensing",
        "authors": [
            "R. Broughton",
            "I. Coope",
            "P. Renaud",
            "R. Tappenden"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  Problems in signal processing and medical imaging often lead to calculating\nsparse solutions to under-determined linear systems. Methodologies for solving\nthis problem are presented as background to the method used in this work where\nthe problem is reformulated as an unconstrained convex optimization problem.\nThe least squares approach is modified by an $l_1$-regularization term. A\nsparse solution is sought using a Barzilai-Borwein type projection algorithm\nwith an adaptive step length. New insight into the choice of step length is\nprovided through a study of the special structure of the underlying problem.\nNumerical experiments are conducted and results given, comparing this algorithm\nwith a number of other current algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.3340v1"
    },
    {
        "title": "Applications of the DFLU flux to systems of conservation laws",
        "authors": [
            "Adimurthi Adimurthi",
            "G. D. Veerappa Gowda",
            "Jérôme Jaffré"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  The DFLU numerical flux was introduced in order to solve hyperbolic scalar\nconservation laws with a flux function discontinuous in space. We show how this\nflux can be used to solve systems of conservation laws. The obtained numerical\nflux is very close to a Godunov flux. As an example we consider a system\nmodeling polymer flooding in oil reservoir engineering.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.0320v1"
    },
    {
        "title": "On the Exponentiation of Interval Matrices",
        "authors": [
            "Alexandre Goldsztejn"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  The numerical computation of the exponentiation of a real matrix has been\nintensively studied. The main objective of a good numerical method is to deal\nwith round-off errors and computational cost. The situation is more complicated\nwhen dealing with interval matrices exponentiation: Indeed, the main problem\nwill now be the dependency loss of the different occurrences of the variables\ndue to interval evaluation, which may lead to so wide enclosures that they are\nuseless. In this paper, the problem of computing a sharp enclosure of the\ninterval matrix exponential is proved to be NP-hard. Then the scaling and\nsquaring method is adapted to interval matrices and shown to drastically reduce\nthe dependency loss w.r.t. the interval evaluation of the Taylor series.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.3954v1"
    },
    {
        "title": "A Sparse Bayesian Estimation Framework for Conditioning Prior Geologic\n  Models to Nonlinear Flow Measurements",
        "authors": [
            "Lianlin Li",
            "Behnam Jafarpour"
        ],
        "category": "cs.NA",
        "published_year": "2009",
        "summary": "  We present a Bayesian framework for reconstruction of subsurface hydraulic\nproperties from nonlinear dynamic flow data by imposing sparsity on the\ndistribution of the solution coefficients in a compression transform domain.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.4961v1"
    },
    {
        "title": "George Forsythe's last paper",
        "authors": [
            "Richard P. Brent"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  We describe von Neumann's elegant idea for sampling from the exponential\ndistribution, Forsythe's generalization for sampling from a probability\ndistribution whose density has the form exp(-G(x)), where G(x) is easy to\ncompute (e.g. a polynomial), and my refinement of these ideas to give an\nefficient algorithm for generating pseudo-random numbers with a normal\ndistribution. Later developments are also mentioned.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0909v2"
    },
    {
        "title": "Two-dimensional generalization of the Muller root-finding algorithm and\n  its applications",
        "authors": [
            "Plamen P. Fiziev",
            "Denitsa R. Staicova"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  We propose a new algorithm for solving a system of two nonlinear\ntranscendental equations with two complex variables based on the Muller\nalgorithm. The two-dimensional Muller algorithm is tested on systems of\ndifferent type and is found to work comparably to Newton's method and Broyden's\nmethod in many cases. The new algorithm is particularly useful in systems\nfeaturing the Heun functions whose complexity may make the already known\nalgorithms not efficient enough or not working at all. In those specific cases,\nthe new algorithm gives distinctly better results than the other two methods.\nAs an example for its application in physics, the new algorithm was used to\nfind the quasi-normal modes (QNM) of Schwarzschild black hole described by the\nRegge-Wheeler equation. The numerical results obtained by our method are\ncompared with the already published QNM frequencies and are found to coincide\nto a great extent with them. Also discussed are the QNM of the Kerr black hole,\ndescribed by the Teukolsky Master equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.5375v2"
    },
    {
        "title": "Discrete Variational Calculus for B-spline Approximated Curves",
        "authors": [
            "Jun Zhao",
            "Elizabeth Mansfield"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  We study variational problems for curves approximated by B-spline curves. We\nshow that, one can obtain discrete Euler-Lagrange equations, for the data\ndescribing the approximated curves. Our main application is to the curve\ncompletion problem in 2D and 3D. In this case, the aim is to find various\naesthetically pleasing solutions as opposed to a solution of a physical\nproblem. The Lagrangians of interest are invariant under the special Euclidean\ngroup action for which B-spline approximated curves are well suited. Smooth\nLagrangians with special Euclidean symmetries involve curvature, torsion, and\narc length. Expressions in these, in the original coordinates, are highly\ncomplex. We show that, by contrast, relatively simple discrete Lagrangians\noffer excellent results for the curve completion problem. The methods we\ndevelop for the discrete curve completion problem are general and can be used\nto solve other discrete variational problems for B-spline curves.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.1282v2"
    },
    {
        "title": "Development of three dimensional constitutive theories based on lower\n  dimensional experimental data",
        "authors": [
            "Satish Karra",
            "K. R. Rajagopal"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Most three dimensional constitutive relations that have been developed to\ndescribe the behavior of bodies are correlated against one dimensional and two\ndimensional experiments. What is usually lost sight of is the fact that\ninfinity of such three dimensional models may be able to explain these\nexperiments that are lower dimensional. Recently, the notion of maximization of\nthe rate of entropy production has been used to obtain constitutive relations\nbased on the choice of the stored energy and rate of entropy production, etc.\nIn this paper we show different choices for the manner in which the body stores\nenergy and dissipates energy and satisfies the requirement of maximization of\nthe rate of entropy production that leads to many three dimensional models. All\nof these models, in one dimension, reduce to the model proposed by Burgers to\ndescribe the viscoelastic behavior of bodies.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.3760v1"
    },
    {
        "title": "A thermodynamic framework to develop rate-type models for fluids without\n  instantaneous elasticity",
        "authors": [
            "Satish Karra",
            "K. R. Rajagopal"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  In this paper, we apply the thermodynamic framework recently put into place\nby Rajagopal and co-workers, to develop rate-type models for viscoelastic\nfluids which do not possess instantaneous elasticity. To illustrate the\ncapabilities of such models we make a specific choice for the specific\nHelmholtz potential and the rate of dissipation and consider the creep and\nstress relaxation response associated with the model. Given specific forms for\nthe Helmholtz potential and the rate of dissipation, the rate of dissipation is\nmaximized with the constraint that the difference between the stress power and\nthe rate of change of Helmholtz potential is equal to the rate of dissipation\nand any other constraint that may be applicable such as incompressibility. We\nshow that the model that is developed exhibits fluid-like characteristics and\nis incapable of instantaneous elastic response. It also includes Maxwell-like\nand Kelvin-Voigt-like viscoelastic materials (when certain material moduli take\nspecial values).\n",
        "pdf_link": "http://arxiv.org/pdf/1007.3764v1"
    },
    {
        "title": "On Maxwell fluid with relaxation time and viscosity depending on the\n  pressure",
        "authors": [
            "Satish Karra",
            "Vít Průša",
            "K. R. Rajagopal"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  We study a variant of the well known Maxwell model for viscoelastic fluids,\nnamely we consider the Maxwell fluid with viscosity and relaxation time\ndepending on the pressure. Such a model is relevant for example in modelling\nbehaviour of some polymers and geomaterials. Although it is experimentally\nknown that the material moduli of some viscoelastic fluids can depend on the\npressure, most of the studies concerning the motion of viscoelastic fluids do\nnot take such effects into account despite their possible practical\nsignificance in technological applications. Using a generalized Maxwell model\nwith pressure dependent material moduli we solve a simple boundary value\nproblem and we demonstrate interesting non-classical features exhibited by the\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1751v1"
    },
    {
        "title": "On Modeling the Response of Synovial Fluid: Unsteady Flow of a\n  Shear-Thinning, Chemically-Reacting Fluid Mixture",
        "authors": [
            "Craig Bridges",
            "Satish Karra",
            "K. R. Rajagopal"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  We study the flow of a shear-thinning, chemically-reacting fluid that could\nbe used to model the flow of the synovial fluid. The actual geometry where the\nflow of the synovial fluid takes place is very complicated, and therefore the\ngoverning equations are not amenable to simple mathematical analysis. In order\nto understand the response of the model, we choose to study the flow in a\nsimple geometry. While the flow domain is not a geometry relevant to the flow\nof the synovial fluid in the human body it yet provides a flow which can be\nused to assess the efficacy of different models that have been proposed to\ndescribe synovial fluids. We study the flow in the annular region between two\ncylinders, one of which is undergoing unsteady oscillations about their common\naxis, in order to understand the quintessential behavioral characteristics of\nthe synovial fluid. We use the three models suggested by Hron et al. [ J. Hron,\nJ. M\\'{a}lek, P. Pust\\v{e}jovsk\\'{a}, K. R. Rajagopal, On concentration\ndependent shear-thinning behavior in modeling of synovial fluid flow, Adv. in\nTribol. (In Press).] to study the problem, by appealing to a semi-inverse\nmethod. The assumed structure for the velocity field automatically satisfies\nthe constraint of incompressibility, and the balance of linear momentum is\nsolved together with a convection-diffusion equation. The results are compared\nto those associated with the Newtonian model. We also study the case in which\nan external pressure gradient is applied along the axis of the cylindrical\nannulus.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1756v1"
    },
    {
        "title": "On the simulation of nonlinear bidimensional spiking neuron models",
        "authors": [
            "Jonathan Touboul"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Bidimensional spiking models currently gather a lot of attention for their\nsimplicity and their ability to reproduce various spiking patterns of cortical\nneurons, and are particularly used for large network simulations. These models\ndescribe the dynamics of the membrane potential by a nonlinear differential\nequation that blows up in finite time, coupled to a second equation for\nadaptation. Spikes are emitted when the membrane potential blows up or reaches\na cutoff value. The precise simulation of the spike times and of the adaptation\nvariable is critical for it governs the spike pattern produced, and is hard to\ncompute accurately because of the exploding nature of the system at the spike\ntimes. We thoroughly study the precision of fixed time-step integration schemes\nfor this type of models and demonstrate that these methods produce systematic\nerrors that are unbounded, as the cutoff value is increased, in the evaluation\nof the two crucial quantities: the spike time and the value of the adaptation\nvariable at this time. Precise evaluation of these quantities therefore involve\nvery small time steps and long simulation times. In order to achieve a fixed\nabsolute precision in a reasonable computational time, we propose here a new\nalgorithm to simulate these systems based on a variable integration step method\nthat either integrates the original ordinary differential equation or the\nequation of the orbits in the phase plane, and compare this algorithm with\nfixed time-step Euler scheme and other more accurate simulation algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1954v2"
    },
    {
        "title": "Modeling the Non-linear Viscoelastic Response of High Temperature\n  Polyimides",
        "authors": [
            "Satish Karra",
            "K. R. Rajagopal"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  A constitutive model is developed to predict the viscoelastic response of\npolyimide resins that are used in high temperature applications. This model is\nbased on a thermodynamic framework that uses the notion that the `natural\nconfiguration' of a body evolves as the body undergoes a process and the\nevolution is determined by maximizing the rate of entropy production in general\nand the rate of dissipation within purely mechanical considerations. We\nconstitutively prescribe forms for the specific Helmholtz potential and the\nrate of dissipation (which is the product of density, temperature and the rate\nof entropy production), and the model is derived by maximizing the rate of\ndissipation with the constraint of incompressibility, and the reduced energy\ndissipation equation is also regarded as a constraint in that it is required to\nbe met in every process that the body undergoes. The efficacy of the model is\nascertained by comparing the predictions of the model with the experimental\ndata for PMR-15 and HFPE-II-52 polyimide resins.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.3576v1"
    },
    {
        "title": "Stability analysis of the split-step Fourier method on the background of\n  a soliton of the nonlinear Schrödinger equation",
        "authors": [
            "Taras I. Lakoba"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  We analyze a numerical instability that occurs in the well-known split-step\nFourier method on the background of a soliton. This instability is found to be\nvery sensitive to small changes of the parameters of both the numerical grid\nand the soliton, unlike the instability of most finite-difference schemes. % on\nthe background of a monochromatic wave, considered earlier in the literature.\nMoreover, the principle of ``frozen coefficients\", in which variable\ncoefficients are treated as ``locally constant\" for the purpose of stability\nanalysis, is strongly violated for the instability of the split-step method on\nthe soliton background. Our analysis explains all these features. It is enabled\nby the fact that the period of oscillations of the unstable Fourier modes is\nmuch smaller than the width of the soliton.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.4974v1"
    },
    {
        "title": "Energy-preserving numerical schemes of high accuracy for one-dimensional\n  Hamiltonian systems",
        "authors": [
            "Jan L. Cieśliński",
            "Bogusław Ratkiewicz"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  We present a class of non-standard numerical schemes which are modifications\nof the discrete gradient method. They preserve the energy integral exactly (up\nto the round-off error). The considered class contains locally exact discrete\ngradient schemes and integrators of arbitrary high order. In numerical\nexperiments we compare our integrators with some other numerical schemes,\nincluding the standard discrete gradient method, the leap-frog scheme and a\nsymplectic scheme of 4th order. We study the error accumulation for very long\ntime and the conservation of the energy integral.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.2738v1"
    },
    {
        "title": "Least Squares Ranking on Graphs",
        "authors": [
            "Anil N. Hirani",
            "Kaushik Kalyanaraman",
            "Seth Watts"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Given a set of alternatives to be ranked, and some pairwise comparison data,\nranking is a least squares computation on a graph. The vertices are the\nalternatives, and the edge values comprise the comparison data. The basic idea\nis very simple and old: come up with values on vertices such that their\ndifferences match the given edge data. Since an exact match will usually be\nimpossible, one settles for matching in a least squares sense. This formulation\nwas first described by Leake in 1976 for rankingfootball teams and appears as\nan example in Professor Gilbert Strang's classic linear algebra textbook. If\none is willing to look into the residual a little further, then the problem\nreally comes alive, as shown effectively by the remarkable recent paper of\nJiang et al. With or without this twist, the humble least squares problem on\ngraphs has far-reaching connections with many current areas ofresearch. These\nconnections are to theoretical computer science (spectral graph theory, and\nmultilevel methods for graph Laplacian systems); numerical analysis (algebraic\nmultigrid, and finite element exterior calculus); other mathematics (Hodge\ndecomposition, and random clique complexes); and applications (arbitrage, and\nranking of sports teams). Not all of these connections are explored in this\npaper, but many are. The underlying ideas are easy to explain, requiring only\nthe four fundamental subspaces from elementary linear algebra. One of our aims\nis to explain these basic ideas and connections, to get researchers in many\nfields interested in this topic. Another aim is to use our numerical\nexperiments for guidance on selecting methods and exposing the need for further\ndevelopment.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.1716v4"
    },
    {
        "title": "A Block Lanczos with Warm Start Technique for Accelerating Nuclear Norm\n  Minimization Algorithms",
        "authors": [
            "Zhouchen Lin",
            "Siming Wei"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Recent years have witnessed the popularity of using rank minimization as a\nregularizer for various signal processing and machine learning problems. As\nrank minimization problems are often converted to nuclear norm minimization\n(NNM) problems, they have to be solved iteratively and each iteration requires\ncomputing a singular value decomposition (SVD). Therefore, their solution\nsuffers from the high computation cost of multiple SVDs. To relieve this issue,\nwe propose using the block Lanczos method to compute the partial SVDs, where\nthe principal singular subspaces obtained in the previous iteration are used to\nstart the block Lanczos procedure. To avoid the expensive reorthogonalization\nin the Lanczos procedure, the block Lanczos procedure is performed for only a\nfew steps. Our block Lanczos with warm start (BLWS) technique can be adopted by\ndifferent algorithms that solve NNM problems. We present numerical results on\napplying BLWS to Robust PCA and Matrix Completion problems. Experimental\nresults show that our BLWS technique usually accelerates its host algorithm by\nat least two to three times.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.0365v2"
    },
    {
        "title": "Low-Rank Structure Learning via Log-Sum Heuristic Recovery",
        "authors": [
            "Yue Deng",
            "Qionghai Dai",
            "Risheng Liu",
            "Zengke Zhang",
            "Sanqing Hu"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  Recovering intrinsic data structure from corrupted observations plays an\nimportant role in various tasks in the communities of machine learning and\nsignal processing. In this paper, we propose a novel model, named log-sum\nheuristic recovery (LHR), to learn the essential low-rank structure from\ncorrupted data. Different from traditional approaches, which directly utilize\n$\\ell_1$ norm to measure the sparseness, LHR introduces a more reasonable\nlog-sum measurement to enhance the sparsity in both the intrinsic low-rank\nstructure and in the sparse corruptions. Although the proposed LHR optimization\nis no longer convex, it still can be effectively solved by a\nmajorization-minimization (MM) type algorithm, with which the non-convex\nobjective function is iteratively replaced by its convex surrogate and LHR\nfinally falls into the general framework of reweighed approaches. We prove that\nthe MM-type algorithm can converge to a stationary point after successive\niteration. We test the performance of our proposed model by applying it to\nsolve two typical problems: robust principal component analysis (RPCA) and\nlow-rank representation (LRR).\n  For RPCA, we compare LHR with the benchmark Principal Component Pursuit (PCP)\nmethod from both the perspectives of simulations and practical applications.\nFor LRR, we apply LHR to compute the low-rank representation matrix for motion\nsegmentation and stock clustering. Experimental results on low rank structure\nlearning demonstrate that the proposed Log-sum based model performs much better\nthan the $\\ell_1$-based method on for data with higher rank and with denser\ncorruptions.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.1919v3"
    },
    {
        "title": "Test of some numerical limiters for the conservative PSM scheme for 4D\n  Drift-Kinetic simulations",
        "authors": [
            "Jerome Guterl",
            "Jean-Philippe Braeunig",
            "Nicolas Crouseilles",
            "Virginie Grandgirard",
            "Guillaume Latu",
            "Michel Mehrenberger",
            "Eric Sonnendrücker"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  The purpose of this work is simulation of magnetised plasmas in the ITER\nproject framework. In this context, Vlasov-Poisson like models are used to\nsimulate core turbulence in the tokamak in a toroidal geometry. This leads to\nheavy simulation because a 6D dimensional problem has to be solved, 3D in space\nand 3D in velocity. The model is reduced to a 5D gyrokinetic model, taking\nadvantage of the particular motion of particles due to the presence of a strong\nmagnetic field. However, accurate schemes, parallel algorithms need to be\ndesigned to bear these simulations. This paper describes a Hermite formulation\nof the conservative PSM scheme which is very generic and allows to implement\ndifferent semi-Lagrangian schemes. We also test and propose numerical limiters\nwhich should improve the robustness of the simulations by diminishing spurious\noscillations. We only consider here the 4D drift-kinetic model which is the\nbackbone of the 5D gyrokinetic models and relevant to build a robust and\naccurate numerical method.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.2674v1"
    },
    {
        "title": "On the performance of the variational multiscale formulation for\n  subsurface flow and transport in heterogeneous porous media",
        "authors": [
            "D. Z. Turner",
            "K. B. Nakshatrala",
            "P. K. Notz"
        ],
        "category": "cs.NA",
        "published_year": "2010",
        "summary": "  The following work compares two popular mixed finite elements used to model\nsubsurface flow and transport in heterogeneous porous media; the lowest order\nRaviart-Thomas element and the variational multiscale stabilized element.\nComparison is made based on performance for several problems of engineering\nrelevance that involve highly heterogenous material properties (permeability\nratios of up to $1\\times10^5$), open flow boundary conditions (pressure driven\nflows), and large scale domains in two dimensions. Numerical experiments are\nperformed to show the degree to which mass conservation is violated when a flow\nfield computed using either element is used as the advection velocity in a\ntransport model. The results reveal that the variational multiscale element\nshows considerable mass production or loss for problems that involve flow\ntangential to layers of differing permeability, but marginal violation of local\nmass balance for problems of less orthogonality in the permeability. The\nresults are useful in establishing rudimentary estimates of the error produced\nby using the variational mutliscale element for several different types of\nproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.3440v1"
    },
    {
        "title": "An algebra for signal processing",
        "authors": [
            "Henning Thielemann"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Our paper presents an attempt to axiomatise signal processing. Our long-term\ngoal is to formulate signal processing algorithms for an ideal world of exact\ncomputation and prove properties about them, then interpret these ideal\nformulations and apply them without change to real world discrete data. We give\nmodels of the axioms that are based on Gaussian functions, that allow for exact\ncomputations and automated tests of signal algorithm properties.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.4205v2"
    },
    {
        "title": "Numerical solution of a fuzzy time-optimal control problem",
        "authors": [
            "Şahin Emrah Amrahov",
            "Nizami Gasilov",
            "Afet Golayoglu Fatullayev"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  In this paper, we consider a time-optimal control problem with uncertainties.\nDynamics of controlled object is expressed by crisp linear system of\ndifferential equations with fuzzy initial and final states. We introduce a\nnotion of fuzzy optimal time and reduce its calculation to two crisp optimal\ncontrol problems. We examine the proposed approach on an example.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.3371v1"
    },
    {
        "title": "Numerical Experiments for Darcy Flow on a Surface Using Mixed Exterior\n  Calculus Methods",
        "authors": [
            "Anil N. Hirani",
            "Kaushik Kalyanaraman"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  There are very few results on mixed finite element methods on surfaces. A\ntheory for the study of such methods was given recently by Holst and Stern,\nusing a variational crimes framework in the context of finite element exterior\ncalculus. However, we are not aware of any numerical experiments where mixed\nfinite elements derived from discretizations of exterior calculus are used for\na surface domain. This short note shows results of our preliminary experiments\nusing mixed methods for Darcy flow (hence scalar Poisson's equation in mixed\nform) on surfaces. We demonstrate two numerical methods. One is derived from\nthe primal-dual Discrete Exterior Calculus and the other from lowest order\nfinite element exterior calculus. The programming was done in the language\nPython, using the PyDEC package which makes the code very short and easy to\nread. The qualitative convergence studies seem to be promising.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.4865v1"
    },
    {
        "title": "Unstructured Geometric Multigrid in Two and Three Dimensions on Complex\n  and Graded Meshes",
        "authors": [
            "Peter R. Brune",
            "Matthew G. Knepley",
            "L. Ridgway Scott"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  The use of multigrid and related preconditioners with the finite element\nmethod is often limited by the difficulty of applying the algorithm effectively\nto a problem, especially when the domain has a complex shape or adaptive\nrefinement. We introduce a simplification of a general topologically-motivated\nmesh coarsening algorithm for use in creating hierarchies of meshes for\ngeometric unstructured multigrid methods. The connections between the\nguarantees of this technique and the quality criteria necessary for multigrid\nmethods for non-quasi-uniform problems are noted. The implementation details,\nin particular those related to coarsening, remeshing, and interpolation, are\ndiscussed. Computational tests on pathological test cases from adaptive finite\nelement methods show the performance of the technique.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.0261v2"
    },
    {
        "title": "Hybrid Deterministic-Stochastic Methods for Data Fitting",
        "authors": [
            "Michael P. Friedlander",
            "Mark Schmidt"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Many structured data-fitting applications require the solution of an\noptimization problem involving a sum over a potentially large number of\nmeasurements. Incremental gradient algorithms offer inexpensive iterations by\nsampling a subset of the terms in the sum. These methods can make great\nprogress initially, but often slow as they approach a solution. In contrast,\nfull-gradient methods achieve steady convergence at the expense of evaluating\nthe full objective and gradient on each iteration. We explore hybrid methods\nthat exhibit the benefits of both approaches. Rate-of-convergence analysis\nshows that by controlling the sample size in an incremental gradient algorithm,\nit is possible to maintain the steady convergence rates of full-gradient\nmethods. We detail a practical quasi-Newton implementation based on this\napproach. Numerical experiments illustrate its potential benefits.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.2373v4"
    },
    {
        "title": "A Discrete Adapted Hierarchical Basis Solver For Radial Basis Function\n  Interpolation",
        "authors": [
            "Julio Enrique Castrillon-Candas",
            "Jun Li",
            "Victor Eijkhout"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  In this paper we develop a discrete Hierarchical Basis (HB) to efficiently\nsolve the Radial Basis Function (RBF) interpolation problem with variable\npolynomial order. The HB forms an orthogonal set and is adapted to the kernel\nseed function and the placement of the interpolation nodes. Moreover, this\nbasis is orthogonal to a set of polynomials up to a given order defined on the\ninterpolating nodes. We are thus able to decouple the RBF interpolation problem\nfor any order of the polynomial interpolation and solve it in two steps: (1)\nThe polynomial orthogonal RBF interpolation problem is efficiently solved in\nthe transformed HB basis with a GMRES iteration and a diagonal, or block SSOR\npreconditioner. (2) The residual is then projected onto an orthonormal\npolynomial basis. We apply our approach on several test cases to study its\neffectiveness, including an application to the Best Linear Unbiased Estimator\nregression problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.2504v2"
    },
    {
        "title": "Reconstruction of Fractional Brownian Motion Signals From Its Sparse\n  Samples Based on Compressive Sampling",
        "authors": [
            "Andriyan Bayu Suksmono"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  This paper proposes a new fBm (fractional Brownian motion)\ninterpolation/reconstruction method from partially known samples based on CS\n(Compressive Sampling). Since 1/f property implies power law decay of the fBm\nspectrum, the fBm signals should be sparse in frequency domain. This property\nmotivates the adoption of CS in the development of the reconstruction method.\nHurst parameter H that occurs in the power law determines the sparsity level,\ntherefore the CS reconstruction quality of an fBm signal for a given number of\nknown subsamples will depend on H. However, the proposed method does not\nrequire the information of H to reconstruct the fBm signal from its partial\nsamples. The method employs DFT (Discrete Fourier Transform) as the sparsity\nbasis and a random matrix derived from known samples positions as the\nprojection basis. Simulated fBm signals with various values of H are used to\nshow the relationship between the Hurst parameter and the reconstruction\nquality. Additionally, US-DJIA (Dow Jones Industrial Average) stock index\nmonthly values time-series are also used to show the applicability of the\nproposed method to reconstruct a real-world data.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.5924v1"
    },
    {
        "title": "Algorithm 916: computing the Faddeyeva and Voigt functions",
        "authors": [
            "Mofreh R. Zaghloul",
            "Ahmed N. Ali"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  We present a MATLAB function for the numerical evaluation of the Faddeyeva\nfunction w(z). The function is based on a newly developed accurate algorithm.\nIn addition to its higher accuracy, the software provides a flexible accuracy\nvs efficiency trade-off through a controlling parameter that may be used to\nreduce accuracy and computational time and vice versa. Verification of the\nflexibility, reliability and superior accuracy of the algorithm is provided\nthrough comparison with standard algorithms available in other libraries and\nsoftware packages.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.0151v2"
    },
    {
        "title": "A framework for coupled deformation-diffusion analysis with application\n  to degradation/healing",
        "authors": [
            "M. K. Mudunuru",
            "K. B. Nakshatrala"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  This paper deals with the formulation and numerical implementation of a fully\ncoupled continuum model for deformation-diffusion in linearized elastic solids.\nThe mathematical model takes into account the effect of the deformation on the\ndiffusion process, and the affect of the transport of an inert chemical species\non the deformation of the solid. We then present a robust computational\nframework for solving the proposed mathematical model, which consists of\ncoupled non-linear partial differential equations. It should be noted that many\npopular numerical formulations may produce unphysical negative values for the\nconcentration, particularly, when the diffusion process is anisotropic. The\nviolation of the non-negative constraint by these numerical formulations is not\nmere numerical noise. In the proposed computational framework we employ a novel\nnumerical formulation that will ensure that the concentration of the diffusant\nbe always non-negative, which is one of the main contributions of this paper.\nRepresentative numerical examples are presented to show the robustness,\nconvergence, and performance of the proposed computational framework. Another\ncontribution of this paper is to systematically study the affect of transport\nof the diffusant on the deformation of the solid and vice-versa, and their\nimplication in modeling degradation/healing of materials. We show that the\ncoupled response is both qualitatively and quantitatively different from the\nuncoupled response.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.2327v1"
    },
    {
        "title": "Petascale turbulence simulation using a highly parallel fast multipole\n  method on GPUs",
        "authors": [
            "R. Yokota",
            "L. A. Barba",
            "T. Narumi",
            "K. Yasuoka"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  This paper reports large-scale direct numerical simulations of\nhomogeneous-isotropic fluid turbulence, achieving sustained performance of 1.08\npetaflop/s on gpu hardware using single precision. The simulations use a vortex\nparticle method to solve the Navier-Stokes equations, with a highly parallel\nfast multipole method (FMM) as numerical engine, and match the current record\nin mesh size for this application, a cube of 4096^3 computational points solved\nwith a spectral method. The standard numerical approach used in this field is\nthe pseudo-spectral method, relying on the FFT algorithm as numerical engine.\nThe particle-based simulations presented in this paper quantitatively match the\nkinetic energy spectrum obtained with a pseudo-spectral method, using a trusted\ncode. In terms of parallel performance, weak scaling results show the fmm-based\nvortex method achieving 74% parallel efficiency on 4096 processes (one gpu per\nmpi process, 3 gpus per node of the TSUBAME-2.0 system). The FFT-based spectral\nmethod is able to achieve just 14% parallel efficiency on the same number of\nmpi processes (using only cpu cores), due to the all-to-all communication\npattern of the FFT algorithm. The calculation time for one time step was 108\nseconds for the vortex method and 154 seconds for the spectral method, under\nthese conditions. Computing with 69 billion particles, this work exceeds by an\norder of magnitude the largest vortex method calculations to date.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.5273v3"
    },
    {
        "title": "Finite Projective Geometry based Fast, Conflict-free Parallel Matrix\n  Computations",
        "authors": [
            "Shreeniwas Sapre",
            "Hrishikesh Sharma",
            "Abhishek Patil",
            "B. S. Adiga",
            "Sachin Patkar"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Matrix computations, especially iterative PDE solving (and the sparse matrix\nvector multiplication subproblem within) using conjugate gradient algorithm,\nand LU/Cholesky decomposition for solving system of linear equations, form the\nkernel of many applications, such as circuit simulators, computational fluid\ndynamics or structural analysis etc. The problem of designing approaches for\nparallelizing these computations, to get good speedups as much as possible as\nper Amdahl's law, has been continuously researched upon. In this paper, we\ndiscuss approaches based on the use of finite projective geometry graphs for\nthese two problems. For the problem of conjugate gradient algorithm, the\napproach looks at an alternative data distribution based on projective-geometry\nconcepts. It is proved that this data distribution is an optimal data\ndistribution for scheduling the main problem of dense matrix-vector\nmultiplication. For the problem of parallel LU/Cholesky decomposition of\ngeneral matrices, the approach is motivated by the recently published scheme\nfor interconnects of distributed systems, perfect difference networks. We find\nthat projective-geometry based graphs indeed offer an exciting way of\nparallelizing these computations, and in fact many others. Moreover, their\napplications ranges from architectural ones (interconnect choice) to\nalgorithmic ones (data distributions).\n",
        "pdf_link": "http://arxiv.org/pdf/1107.1127v1"
    },
    {
        "title": "Linear Differential Equations with Fuzzy Boundary Values",
        "authors": [
            "Nizami Gasilov",
            "Şahin Emrah Amrahov",
            "Afet Golayoğlu Fatullayev"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  In this study, we consider a linear differential equation with fuzzy boundary\nvalues. We express the solution of the problem in terms of a fuzzy set of crisp\nreal functions. Each real function from the solution set satisfies differential\nequation, and its boundary values belong to intervals, determined by the\ncorresponding fuzzy numbers. The least possibility among possibilities of\nboundary values in corresponding fuzzy sets is defined as the possibility of\nthe real function in the fuzzy solution. In order to find the fuzzy solution we\npropose a method based on the properties of linear transformations. We show\nthat, if the corresponding crisp problem has a unique solution then the fuzzy\nproblem has unique solution too. We also prove that if the boundary values are\ntriangular fuzzy numbers, then the value of the solution at any time is also a\ntriangular fuzzy number. We find that the fuzzy solution determined by our\nmethod is the same as the one that is obtained from solution of crisp problem\nby the application of the extension principle. We present two examples\ndescribing the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4157v1"
    },
    {
        "title": "A novel and scalable Multigrid algorithm for many-core architectures",
        "authors": [
            "Julian Becerra-Sagredo",
            "Carlos Malaga",
            "Francisco Mandujano"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  Multigrid algorithms are among the fastest iterative methods known today for\nsolving large linear and some non-linear systems of equations. Greatly\noptimized for serial operation, they still have a great potential for\nparallelism not fully realized. In this work, we present a novel multigrid\nalgorithm designed to work entirely inside many-core architectures like the\ngraphics processing units (GPUs), without memory transfers between the GPU and\nthe central processing unit (CPU), avoiding low bandwitdth communications. The\nalgorithm is denoted as the high occupancy multigrid (HOMG) because it makes\nuse of entire grid operations with interpolations and relaxations fused into\none task, providing useful work for every thread in the grid. For a given\naccuracy, its number of operations scale linearly with the total number of\nnodes in the grid. Perfect scalability is observed for a large number of\nprocessors.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.2045v1"
    },
    {
        "title": "FMM-based vortex method for simulation of isotropic turbulence on GPUs,\n  compared with a spectral method",
        "authors": [
            "Rio Yokota",
            "L. A. Barba"
        ],
        "category": "cs.NA",
        "published_year": "2011",
        "summary": "  The Lagrangian vortex method offers an alternative numerical approach for\ndirect numerical simulation of turbulence. The fact that it uses the fast\nmultipole method (FMM)--a hierarchical algorithm for N-body problems with\nhighly scalable parallel implementations--as numerical engine makes it a\npotentially good candidate for exascale systems. However, there have been few\nvalidation studies of Lagrangian vortex simulations and the insufficient\ncomparisons against standard DNS codes has left ample room for skepticism. This\npaper presents a comparison between a Lagrangian vortex method and a\npseudo-spectral method for the simulation of decaying homogeneous isotropic\nturbulence. This flow field is chosen despite the fact that it is not the most\nfavorable flow problem for particle methods (which shine in wake flows or where\nvorticity is compact), due to the fact that it is ideal for the quantitative\nvalidation of DNS codes. We use a 256^3 grid with Re_lambda=50 and 100 and look\nat the turbulence statistics, including high-order moments. The focus is on the\neffect of the various parameters in the vortex method, e.g., order of FMM\nseries expansion, frequency of reinitialization, overlap ratio and time step.\nThe vortex method uses an FMM code (exaFMM) that runs on GPU hardware using\nCUDA, while the spectral code (hit3d) runs on CPU only. Results indicate that,\nfor this application (and with the current code implementations), the spectral\nmethod is an order of magnitude faster than the vortex method when using a\nsingle GPU for the FMM and six CPU cores for the FFT.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.2921v4"
    },
    {
        "title": "Efficient recovery-based error estimation for the smoothed finite\n  element method for smooth and singular linear elasticity",
        "authors": [
            "Octavio A. González-Estrada",
            "Sundararajan Natarajan",
            "Juan José Ródenas",
            "Hung Nguyen-Xuan",
            "Stéphane P. A. Bordas"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  An error control technique aimed to assess the quality of smoothed finite\nelement approximations is presented in this paper. Finite element techniques\nbased on strain smoothing appeared in 2007 were shown to provide significant\nadvantages compared to conventional finite element approximations. In\nparticular, a widely cited strength of such methods is improved accuracy for\nthe same computational cost. Yet, few attempts have been made to directly\nassess the quality of the results obtained during the simulation by evaluating\nan estimate of the discretization error. Here we propose a recovery type error\nestimator based on an enhanced recovery technique. The salient features of the\nrecovery are: enforcement of local equilibrium and, for singular problems a\n\"smooth+singular\" decomposition of the recovered stress. We evaluate the\nproposed estimator on a number of test cases from linear elastic structural\nmechanics and obtain precise error estimations whose effectivities, both at\nlocal and global levels, are improved compared to recovery procedures not\nimplementing these features.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.1278v1"
    },
    {
        "title": "An efficient solver for volumetric scattering based on fast spherical\n  harmonics transforms",
        "authors": [
            "Youngae Han"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The Helmholtz equation arises in the study of electromagnetic radiation,\noptics, acoustics, etc. In spherical coordinates, its general solution can be\nwritten as a spherical harmonic series which satisfies the radiation condition\nat infinity, ensuring that the wave is outgoing. The boundary condition at\ninfinity is hard to enforce with a finite element method since a suitable\napproximation needs to be made within reasonable distance from scatterers.\n  Luckily, the Helmholtz equation can be represented as a Lippmann-Schwinger\nintegral equation which removes the necessity of the boundary approximations\nand its Green's function can be expanded as a spherical harmonic series which\nleads to our numerical scheme based on spherical harmonic polynomial transform.\nIn this paper, we present an efficient solver for the Helmholtz equation which\ncosts $O(N\\log N)$ operations, where $N$ is the number of the discretization\npoints. We use the fast spherical harmonic transforms which are originally\ndeveloped in \\cite{suda}. The convergence order of the method is tied to the\nglobal regularity of the solution. At the lower end, it is second order\naccurate for discontinuous material properties. The order increases with\nincreasing regularity leading to spectral convergence for globally smooth\nsolutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.2621v1"
    },
    {
        "title": "Low Complexity Damped Gauss-Newton Algorithms for CANDECOMP/PARAFAC",
        "authors": [
            "Anh Huy Phan",
            "Petr Tichavský",
            "Andrzej Cichocki"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The damped Gauss-Newton (dGN) algorithm for CANDECOMP/PARAFAC (CP)\ndecomposition can handle the challenges of collinearity of factors and\ndifferent magnitudes of factors; nevertheless, for factorization of an $N$-D\ntensor of size $I_1\\times I_N$ with rank $R$, the algorithm is computationally\ndemanding due to construction of large approximate Hessian of size $(RT \\times\nRT)$ and its inversion where $T = \\sum_n I_n$. In this paper, we propose a fast\nimplementation of the dGN algorithm which is based on novel expressions of the\ninverse approximate Hessian in block form. The new implementation has lower\ncomputational complexity, besides computation of the gradient (this part is\ncommon to both methods), requiring the inversion of a matrix of size\n$NR^2\\times NR^2$, which is much smaller than the whole approximate Hessian, if\n$T \\gg NR$. In addition, the implementation has lower memory requirements,\nbecause neither the Hessian nor its inverse never need to be stored in their\nentirety. A variant of the algorithm working with complex valued data is\nproposed as well. Complexity and performance of the proposed algorithm is\ncompared with those of dGN and ALS with line search on examples of difficult\nbenchmark tensors.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2584v2"
    },
    {
        "title": "A priori error estimates for compatible spectral discretization of the\n  Stokes problem for all admissible boundary conditions",
        "authors": [
            "Jasper Kreeft",
            "Marc Gerritsma"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  This paper describes the recently developed mixed mimetic spectral element\nmethod for the Stokes problem in the vorticity-velocity-pressure formulation.\nThis compatible discretization method relies on the construction of a\nconforming discrete Hodge decomposition, that is based on a bounded projection\noperator that commutes with the exterior derivative. The projection operator is\nthe composition of a reduction and a reconstruction step. The reconstruction in\nterms of mimetic spectral element basis-functions are tensor-based\nconstructions and therefore hold for curvilinear quadrilateral and hexahedral\nmeshes. For compatible discretization methods that contain a conforming\ndiscrete Hodge decomposition, we derive optimal a priori error estimates which\nare valid for all admissible boundary conditions on both Cartesian and\ncurvilinear meshes. These theoretical results are confirmed by numerical\nexperiments. These clearly show that the mimetic spectral elements outperform\nthe commonly used H(div)-compatible Raviart-Thomas elements.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2812v1"
    },
    {
        "title": "Quasi-Newton Methods: A New Direction",
        "authors": [
            "Philipp Hennig",
            "Martin Kiefel"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Four decades after their invention, quasi-Newton methods are still state of\nthe art in unconstrained numerical optimization. Although not usually\ninterpreted thus, these are learning algorithms that fit a local quadratic\napproximation to the objective function. We show that many, including the most\npopular, quasi-Newton methods can be interpreted as approximations of Bayesian\nlinear regression under varying prior assumptions. This new notion elucidates\nsome shortcomings of classical algorithms, and lights the way to a novel\nnonparametric quasi-Newton method, which is able to make more efficient use of\navailable information at computational cost similar to its predecessors.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.4602v1"
    },
    {
        "title": "Stability of matrix factorization for collaborative filtering",
        "authors": [
            "Yu-Xiang Wang",
            "Huan Xu"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  We study the stability vis a vis adversarial noise of matrix factorization\nalgorithm for matrix completion. In particular, our results include: (I) we\nbound the gap between the solution matrix of the factorization method and the\nground truth in terms of root mean square error; (II) we treat the matrix\nfactorization as a subspace fitting problem and analyze the difference between\nthe solution subspace and the ground truth; (III) we analyze the prediction\nerror of individual users based on the subspace stability. We apply these\nresults to the problem of collaborative filtering under manipulator attack,\nwhich leads to useful insights and guidelines for collaborative filtering\nsystem design.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.4640v1"
    },
    {
        "title": "Subdivision Shell Elements with Anisotropic Growth",
        "authors": [
            "Roman Vetter",
            "Norbert Stoop",
            "Thomas Jenni",
            "Falk K. Wittel",
            "Hans J. Herrmann"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  A thin shell finite element approach based on Loop's subdivision surfaces is\nproposed, capable of dealing with large deformations and anisotropic growth. To\nthis end, the Kirchhoff-Love theory of thin shells is derived and extended to\nallow for arbitrary in-plane growth. The simplicity and computational\nefficiency of the subdivision thin shell elements is outstanding, which is\ndemonstrated on a few standard loading benchmarks. With this powerful tool at\nhand, we demonstrate the broad range of possible applications by numerical\nsolution of several growth scenarios, ranging from the uniform growth of a\nsphere, to boundary instabilities induced by large anisotropic growth. Finally,\nit is shown that the problem of a slowly and uniformly growing sheet confined\nin a fixed hollow sphere is equivalent to the inverse process where a sheet of\nfixed size is slowly crumpled in a shrinking hollow sphere in the frictionless,\nquasi-static, elastic limit.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.4434v2"
    },
    {
        "title": "Two-step greedy algorithm for reduced order quadratures",
        "authors": [
            "Harbir Antil",
            "Scott E. Field",
            "Frank Herrmann",
            "Ricardo H. Nochetto",
            "Manuel Tiglio"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  We present an algorithm to generate application-specific, global reduced\norder quadratures (ROQ) for multiple fast evaluations of weighted inner\nproducts between parameterized functions. If a reduced basis (RB) or any other\nprojection-based model reduction technique is applied, the dimensionality of\nintegrands is reduced dramatically; however, the cost of approximating the\nintegrands by projection still scales as the size of the original problem. In\ncontrast, using discrete empirical interpolation (DEIM) points as ROQ nodes\nleads to a computational cost which depends linearly on the dimension of the\nreduced space. Generation of a reduced basis via a greedy procedure requires a\ntraining set, which for products of functions can be very large. Since this\ndirect approach can be impractical in many applications, we propose instead a\ntwo-step greedy targeted towards approximation of such products. We present\nnumerical experiments demonstrating the accuracy and the efficiency of the\ntwo-step approach. The presented ROQ are expected to display very fast\nconvergence whenever there is regularity with respect to parameter variation.\nWe find that for the particular application here considered, one driven by\ngravitational wave physics, the two-step approach speeds up the offline\ncomputations to build the ROQ by more than two orders of magnitude.\nFurthermore, the resulting ROQ rule is found to converge exponentially with the\nnumber of nodes, and a factor of ~50 savings, without loss of accuracy, is\nobserved in evaluations of inner products when ROQ are used as a downsampling\nstrategy for equidistant samples using the trapezoidal rule. While the primary\nfocus of this paper is on quadrature rules for inner products of parameterized\nfunctions, our method can be easily adapted to integrations of single\nparameterized functions, and some examples of this type are considered.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.0577v2"
    },
    {
        "title": "Solving Linear System of Equations Via A Convex Hull Algorithm",
        "authors": [
            "Bahman Kalantari"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  We present new iterative algorithms for solving a square linear system $Ax=b$\nin dimension $n$ by employing the {\\it Triangle Algorithm} \\cite{kal12}, a\nfully polynomial-time approximation scheme for testing if the convex hull of a\nfinite set of points in a Euclidean space contains a given point. By converting\n$Ax=b$ into a convex hull problem and solving via the Triangle Algorithm,\ntogether with a {\\it sensitivity theorem}, we compute in $O(n^2\\epsilon^{-2})$\narithmetic operations an approximate solution satisfying $\\Vert Ax_\\epsilon - b\n\\Vert \\leq \\epsilon \\rho$, where $\\rho= \\max \\{\\Vert a_1 \\Vert,..., \\Vert a_n\n\\Vert, \\Vert b \\Vert \\}$, and $a_i$ is the $i$-th column of $A$. In another\napproach we apply the Triangle Algorithm incrementally, solving a sequence of\nconvex hull problems while repeatedly employing a {\\it distance duality}. The\nsimplicity and theoretical complexity bounds of the proposed algorithms,\nrequiring no structural restrictions on the matrix $A$, suggest their potential\npracticality, offering alternatives to the existing exact and iterative\nmethods, especially for large scale linear systems. The assessment of\ncomputational performance however is the subject of future experimentations.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.7858v1"
    },
    {
        "title": "Belief Propagation Reconstruction for Discrete Tomography",
        "authors": [
            "Emmanuelle Gouillart",
            "Florent Krzakala",
            "Marc Mezard",
            "Lenka Zdeborová"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  We consider the reconstruction of a two-dimensional discrete image from a set\nof tomographic measurements corresponding to the Radon projection. Assuming\nthat the image has a structure where neighbouring pixels have a larger\nprobability to take the same value, we follow a Bayesian approach and introduce\na fast message-passing reconstruction algorithm based on belief propagation.\nFor numerical results, we specialize to the case of binary tomography. We test\nthe algorithm on binary synthetic images with different length scales and\ncompare our results against a more usual convex optimization approach. We\ninvestigate the reconstruction error as a function of the number of tomographic\nmeasurements, corresponding to the number of projection angles. The belief\npropagation algorithm turns out to be more efficient than the\nconvex-optimization algorithm, both in terms of recovery bounds for noise-free\nprojections, and in terms of reconstruction quality when moderate Gaussian\nnoise is added to the projections.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.2379v2"
    },
    {
        "title": "Accelerated Canonical Polyadic Decomposition by Using Mode Reduction",
        "authors": [
            "Guoxu Zhou",
            "Andrzej Cichocki",
            "Shengli Xie"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Canonical Polyadic (or CANDECOMP/PARAFAC, CP) decompositions (CPD) are widely\napplied to analyze high order tensors. Existing CPD methods use alternating\nleast square (ALS) iterations and hence need to unfold tensors to each of the\n$N$ modes frequently, which is one major bottleneck of efficiency for\nlarge-scale data and especially when $N$ is large. To overcome this problem, in\nthis paper we proposed a new CPD method which converts the original $N$th\n($N>3$) order tensor to a 3rd-order tensor first. Then the full CPD is realized\nby decomposing this mode reduced tensor followed by a Khatri-Rao product\nprojection procedure. This way is quite efficient as unfolding to each of the\n$N$ modes are avoided, and dimensionality reduction can also be easily\nincorporated to further improve the efficiency. We show that, under mild\nconditions, any $N$th-order CPD can be converted into a 3rd-order case but\nwithout destroying the essential uniqueness, and theoretically gives the same\nresults as direct $N$-way CPD methods. Simulations show that, compared with\nstate-of-the-art CPD methods, the proposed method is more efficient and escape\nfrom local solutions more easily.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.3500v2"
    },
    {
        "title": "Parameterized Uniform Complexity in Numerics: from Smooth to Analytic,\n  from NP-hard to Polytime",
        "authors": [
            "Akitoshi Kawamura",
            "Norbert Th. Müller",
            "Carsten Rösnick",
            "Martin Ziegler"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  The synthesis of classical Computational Complexity Theory with Recursive\nAnalysis provides a quantitative foundation to reliable numerics. Here the\noperators of maximization, integration, and solving ordinary differential\nequations are known to map (even high-order differentiable) polynomial-time\ncomputable functions to instances which are `hard' for classical complexity\nclasses NP, #P, and CH; but, restricted to analytic functions, map\npolynomial-time computable ones to polynomial-time computable ones --\nnon-uniformly!\n  We investigate the uniform parameterized complexity of the above operators in\nthe setting of Weihrauch's TTE and its second-order extension due to\nKawamura&Cook (2010). That is, we explore which (both continuous and discrete,\nfirst and second order) information and parameters on some given f is\nsufficient to obtain similar data on Max(f) and int(f); and within what running\ntime, in terms of these parameters and the guaranteed output precision 2^(-n).\n  It turns out that Gevrey's hierarchy of functions climbing from analytic to\nsmooth corresponds to the computational complexity of maximization growing from\npolytime to NP-hard. Proof techniques involve mainly the Theory of (discrete)\nComputation, Hard Analysis, and Information-Based Complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.4974v1"
    },
    {
        "title": "Tracking of a Mobile Target Using Generalized Polarization Tensors",
        "authors": [
            "Habib Ammari",
            "Thomas Boulier",
            "Josselin Garnier",
            "Hyeonbae Kang",
            "Han Wang"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  In this paper we apply an extended Kalman filter to track both the location\nand the orientation of a mobile target from multistatic response measurements.\nWe also analyze the effect of the limited-view aspect on the stability and the\nefficiency of our tracking approach. Our algorithm is based on the use of the\ngeneralized polarization tensors, which can be reconstructed from the\nmultistatic response measurements by solving a linear system. The system has\nthe remarkable property that low order generalized polarization tensors are not\naffected by the error caused by the instability of higher orders in the\npresence of measurement noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.3544v1"
    },
    {
        "title": "Fast and RIP-optimal transforms",
        "authors": [
            "Nir Ailon",
            "Holger Rauhut"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We study constructions of $k \\times n$ matrices $A$ that both (1) satisfy the\nrestricted isometry property (RIP) at sparsity $s$ with optimal parameters, and\n(2) are efficient in the sense that only $O(n\\log n)$ operations are required\nto compute $Ax$ given a vector $x$. Our construction is based on repeated\napplication of independent transformations of the form $DH$, where $H$ is a\nHadamard or Fourier transform and $D$ is a diagonal matrix with random\n$\\{+1,-1\\}$ elements on the diagonal, followed by any $k \\times n$ matrix of\northonormal rows (e.g.\\ selection of $k$ coordinates). We provide guarantees\n(1) and (2) for a larger regime of parameters for which such constructions were\npreviously unknown. Additionally, our construction does not suffer from the\nextra poly-logarithmic factor multiplying the number of observations $k$ as a\nfunction of the sparsity $s$, as present in the currently best known RIP\nestimates for partial random Fourier matrices and other classes of structured\nrandom matrices.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.0878v2"
    },
    {
        "title": "Convergence of the D-iteration algorithm: convergence rate and\n  asynchronous distributed scheme",
        "authors": [
            "Dohy Hong",
            "Fabien Mathieu",
            "Gérard Burnside"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this paper, we define the general framework to describe the diffusion\noperators associated to a positive matrix. We define the equations associated\nto diffusion operators and present some general properties of their state\nvectors. We show how this can be applied to prove and improve the convergence\nof a fixed point problem associated to the matrix iteration scheme, including\nfor distributed computation framework. The approach can be understood as a\ndecomposition of the matrix-vector product operation in elementary operations\nat the vector entry level.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.3007v1"
    },
    {
        "title": "A framework for coupling flow and deformation of the porous solid",
        "authors": [
            "D. Z. Turner",
            "K. B. Nakshatrala",
            "M. J. Martinez"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this paper, we consider the flow of an incompressible fluid in a\ndeformable porous solid. We present a mathematical model using the framework\noffered by the theory of interacting continua. In its most general form, this\nframework provides a mechanism for capturing multiphase flow, deformation,\nchemical reactions and thermal processes, as well as interactions between the\nvarious physics in a conveniently implemented fashion. To simplify the\npresentation of the framework, results are presented for a particular model\nthan can be seen as an extension of Darcy's equation (which assumes that the\nporous solid is rigid) that takes into account elastic deformation of the\nporous solid. The model also considers the effect of deformation on porosity.\nWe show that using this model one can recover identical results as in the\nframework proposed by Biot and Terzaghi. Some salient features of the framework\nare as follows: (a) It is a consistent mixture theory model, and adheres to the\nlaws and principles of continuum thermodynamics, (b) the model is capable of\nsimulating various important phenomena like consolidation and surface\nsubsidence, and (c) the model is amenable to several extensions. We also\npresent numerical coupling algorithms to obtain coupled flow-deformation\nresponse. Several representative numerical examples are presented to illustrate\nthe capability of the mathematical model and the performance of the\ncomputational framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.5848v2"
    },
    {
        "title": "Matrix-free GPU implementation of a preconditioned conjugate gradient\n  solver for anisotropic elliptic PDEs",
        "authors": [
            "Eike Mueller",
            "Xu Guo",
            "Robert Scheichl",
            "Sinan Shi"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Many problems in geophysical and atmospheric modelling require the fast\nsolution of elliptic partial differential equations (PDEs) in \"flat\" three\ndimensional geometries. In particular, an anisotropic elliptic PDE for the\npressure correction has to be solved at every time step in the dynamical core\nof many numerical weather prediction models, and equations of a very similar\nstructure arise in global ocean models, subsurface flow simulations and gas and\noil reservoir modelling. The elliptic solve is often the bottleneck of the\nforecast, and an algorithmically optimal method has to be used and implemented\nefficiently. Graphics Processing Units have been shown to be highly efficient\nfor a wide range of applications in scientific computing, and recently\niterative solvers have been parallelised on these architectures. We describe\nthe GPU implementation and optimisation of a Preconditioned Conjugate Gradient\n(PCG) algorithm for the solution of a three dimensional anisotropic elliptic\nPDE for the pressure correction in NWP. Our implementation exploits the strong\nvertical anisotropy of the elliptic operator in the construction of a suitable\npreconditioner. As the algorithm is memory bound, performance can be improved\nsignificantly by reducing the amount of global memory access. We achieve this\nby using a matrix-free implementation which does not require explicit storage\nof the matrix and instead recalculates the local stencil. Global memory access\ncan also be reduced by rewriting the algorithm using loop fusion and we show\nthat this further reduces the runtime on the GPU. We demonstrate the\nperformance of our matrix-free GPU code by comparing it to a sequential CPU\nimplementation and to a matrix-explicit GPU code which uses existing libraries.\nThe absolute performance of the algorithm for different problem sizes is\nquantified in terms of floating point throughput and global memory bandwidth.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.7193v1"
    },
    {
        "title": "Fast Approximate Polynomial Multipoint Evaluation and Applications",
        "authors": [
            "Alexander Kobel",
            "Michael Sagraloff"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  It is well known that, using fast algorithms for polynomial multiplication\nand division, evaluation of a polynomial $F \\in \\mathbb{C}[x]$ of degree $n$ at\n$n$ complex-valued points can be done with $\\tilde{O}(n)$ exact field\noperations in $\\mathbb{C},$ where $\\tilde{O}(\\cdot)$ means that we omit\npolylogarithmic factors. We complement this result by an analysis of\napproximate multipoint evaluation of $F$ to a precision of $L$ bits after the\nbinary point and prove a bit complexity of $\\tilde{O}(n(L + \\tau + n\\Gamma)),$\nwhere $2^\\tau$ and $2^\\Gamma,$ with $\\tau, \\Gamma \\in \\mathbb{N}_{\\ge 1},$ are\nbounds on the magnitude of the coefficients of $F$ and the evaluation points,\nrespectively. In particular, in the important case where the precision demand\ndominates the other input parameters, the complexity is soft-linear in $n$ and\n$L$.\n  Our result on approximate multipoint evaluation has some interesting\nconsequences on the bit complexity of further approximation algorithms which\nall use polynomial evaluation as a key subroutine. Of these applications, we\ndiscuss in detail an algorithm for polynomial interpolation and for computing a\nTaylor shift of a polynomial. Furthermore, our result can be used to derive\nimproved complexity bounds for algorithms to refine isolating intervals for the\nreal roots of a polynomial. For all of the latter algorithms, we derive\nnear-optimal running times.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.8069v2"
    },
    {
        "title": "Online and stochastic Douglas-Rachford splitting method for large scale\n  machine learning",
        "authors": [
            "Ziqiang Shi",
            "Rujie Liu"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Online and stochastic learning has emerged as powerful tool in large scale\noptimization. In this work, we generalize the Douglas-Rachford splitting (DRs)\nmethod for minimizing composite functions to online and stochastic settings (to\nour best knowledge this is the first time DRs been generalized to sequential\nversion). We first establish an $O(1/\\sqrt{T})$ regret bound for batch DRs\nmethod. Then we proved that the online DRs splitting method enjoy an $O(1)$\nregret bound and stochastic DRs splitting has a convergence rate of\n$O(1/\\sqrt{T})$. The proof is simple and intuitive, and the results and\ntechnique can be served as a initiate for the research on the large scale\nmachine learning employ the DRs method. Numerical experiments of the proposed\nmethod demonstrate the effectiveness of the online and stochastic update rule,\nand further confirm our regret and convergence analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.4757v9"
    },
    {
        "title": "Low-rank approximation in the numerical modeling of the Farley-Buneman\n  instability in ionospheric plasma",
        "authors": [
            "S. V. Dolgov",
            "A. P. Smirnov",
            "E. E. Tyrtyshnikov"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  We consider the numerical modeling of the Farley-Buneman instability\ndevelopment in the earth's ionosphere plasma. The ion behavior is governed by\nthe kinetic Landau equation in the four-dimensional phase space, and since the\nfinite difference discretization on a tensor product grid is used, this\nequation becomes the most computationally challenging part of the scheme. To\nrelax the complexity and memory consumption, an adaptive model reduction using\nthe low-rank separation of variables, namely the Tensor Train format, is\nemployed.\n  The approach was verified via the prototype MATLAB implementation. Numerical\nexperiments demonstrate the possibility of efficient separation of space and\nvelocity variables, resulting in the solution storage reduction by a factor of\norder tens.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.5952v1"
    },
    {
        "title": "Parallel-in-time method for calculation of long-range electrostatic\n  interactions",
        "authors": [
            "Jana Pazúriková",
            "Luděk Matyska"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Large molecular dynamics simulations (millions of atoms, tens of\nmicroseconds, thousands of processors) hit the strong scalability wall:\nsimulation on twice as many processors does not take half the time. Inspired by\nlarge N-body space simulations, we suggest to calculate the bottleneck---the\nlong-range interactions---parallel in time. This technical report aims to\npresent the combination of parareal method and multilevel summation method. We\nthoroughly describe both methods and reasons for their particular combination.\nWe also propose several optimizations that should provide the acceleration by\nan order of magnitude.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.1889v2"
    },
    {
        "title": "Identifying Influential Entries in a Matrix",
        "authors": [
            "Abhisek Kundu",
            "Srinivas Nambirajan",
            "Petros Drineas"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  For any matrix A in R^(m x n) of rank \\rho, we present a probability\ndistribution over the entries of A (the element-wise leverage scores of\nequation (2)) that reveals the most influential entries in the matrix. From a\ntheoretical perspective, we prove that sampling at most s = O ((m + n) \\rho^2\nln (m + n)) entries of the matrix (see eqn. (3) for the precise value of s)\nwith respect to these scores and solving the nuclear norm minimization problem\non the sampled entries, reconstructs A exactly. To the best of our knowledge,\nthese are the strongest theoretical guarantees on matrix completion without any\nincoherence assumptions on the matrix A. From an experimental perspective, we\nshow that entries corresponding to high element-wise leverage scores reveal\nstructural properties of the data matrix that are of interest to domain\nscientists.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.3556v2"
    },
    {
        "title": "On Robustness Analysis of Stochastic Biochemical Systems by\n  Probabilistic Model Checking",
        "authors": [
            "Lubos Brim",
            "Milan Ceska",
            "Sven Drazan",
            "David Safranek"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  This report proposes a novel framework for a rigorous robustness analysis of\nstochastic biochemical systems. The technique is based on probabilistic model\nchecking. We adapt the general definition of robustness introduced by Kitano to\nthe class of stochastic systems modelled as continuous time Markov Chains in\norder to extensively analyse and compare robustness of biological models with\nuncertain parameters. The framework utilises novel computational methods that\nenable to effectively evaluate the robustness of models with respect to\nquantitative temporal properties and parameters such as reaction rate constants\nand initial conditions.\n  The framework is applied to gene regulation as an example of a central\nbiological mechanism where intrinsic and extrinsic stochasticity plays crucial\nrole due to low numbers of DNA and RNA molecules. Using our methods we have\nobtained a comprehensive and precise analysis of stochastic dynamics under\nparameter uncertainty. Furthermore, we apply our framework to compare several\nvariants of two-component signalling networks from the perspective of\nrobustness with respect to intrinsic noise caused by low populations of\nsignalling components. We succeeded to extend previous studies performed on\ndeterministic models (ODE) and show that stochasticity may significantly affect\nobtained predictions. Our case studies demonstrate that the framework can\nprovide deeper insight into the role of key parameters in maintaining the\nsystem functionality and thus it significantly contributes to formal methods in\ncomputational systems biology.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.4734v1"
    },
    {
        "title": "High-order algorithms for solving eigenproblems over discrete surfaces",
        "authors": [
            "Sheng-Gwo Chen",
            "Mei-Hsiu Chi",
            "Jyh-Yang Wu"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The eigenvalue problem of the Laplace-Beltrami operators on curved surfaces\nplays an essential role in the convergence analysis of the numerical\nsimulations of some important geometric partial differential equations which\ninvolve this operator. In this note we shall combine the local tangential\nlifting (LTL) method with the configuration equation to develop a new effective\nand convergent algorithm to solve the eigenvalue problems of the\nLaplace-Beltrami operators acting on functions over discrete surfaces. The\nconvergence rates of our algorithms of discrete Laplace-Beltrami operators over\nsurfaces is $O(r^n)$, $n \\geq 1$, where $r$ represents the size of the mesh of\ndiscretization of the surface. The problem of high-order accuracies will also\nbe discussed and used to compute geometric invariants of the underlying\nsurfaces. Some convergence tests and eigenvalue computations on the sphere,\ntori and a dumbbell are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.4807v1"
    },
    {
        "title": "Linearized Alternating Direction Method with Parallel Splitting and\n  Adaptive Penalty for Separable Convex Programs in Machine Learning",
        "authors": [
            "Zhouchen Lin",
            "Risheng Liu",
            "Huan Li"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Many problems in machine learning and other fields can be (re)for-mulated as\nlinearly constrained separable convex programs. In most of the cases, there are\nmultiple blocks of variables. However, the traditional alternating direction\nmethod (ADM) and its linearized version (LADM, obtained by linearizing the\nquadratic penalty term) are for the two-block case and cannot be naively\ngeneralized to solve the multi-block case. So there is great demand on\nextending the ADM based methods for the multi-block case. In this paper, we\npropose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve\nmulti-block separable convex programs efficiently. When all the component\nobjective functions have bounded subgradients, we obtain convergence results\nthat are stronger than those of ADM and LADM, e.g., allowing the penalty\nparameter to be unbounded and proving the sufficient and necessary conditions}\nfor global convergence. We further propose a simple optimality measure and\nreveal the convergence rate of LADMPSAP in an ergodic sense. For programs with\nextra convex set constraints, with refined parameter estimation we devise a\npractical version of LADMPSAP for faster convergence. Finally, we generalize\nLADMPSAP to handle programs with more difficult objective functions by\nlinearizing part of the objective function as well. LADMPSAP is particularly\nsuitable for sparse representation and low-rank recovery problems because its\nsubproblems have closed form solutions and the sparsity and low-rankness of the\niterates can be preserved during the iteration. It is also highly\nparallelizable and hence fits for parallel or distributed computing. Numerical\nexperiments testify to the advantages of LADMPSAP in speed and numerical\naccuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.5035v2"
    },
    {
        "title": "Recovery of Sparse Matrices via Matrix Sketching",
        "authors": [
            "Thakshila Wimalajeewa",
            "Yonina C. Eldar",
            "Pramod K. Varshney"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  In this paper, we consider the problem of recovering an unknown sparse matrix\nX from the matrix sketch Y = AX B^T. The dimension of Y is less than that of X,\nand A and B are known matrices. This problem can be solved using standard\ncompressive sensing (CS) theory after converting it to vector form using the\nKronecker operation. In this case, the measurement matrix assumes a Kronecker\nproduct structure. However, as the matrix dimension increases the associated\ncomputational complexity makes its use prohibitive. We extend two algorithms,\nfast iterative shrinkage threshold algorithm (FISTA) and orthogonal matching\npursuit (OMP) to solve this problem in matrix form without employing the\nKronecker product. While both FISTA and OMP with matrix inputs are shown to be\nequivalent in performance to their vector counterparts with the Kronecker\nproduct, solving them in matrix form is shown to be computationally more\nefficient. We show that the computational gain achieved by FISTA with matrix\ninputs over its vector form is more significant compared to that achieved by\nOMP.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.2448v1"
    },
    {
        "title": "Parallel matrix factorization for low-rank tensor completion",
        "authors": [
            "Yangyang Xu",
            "Ruru Hao",
            "Wotao Yin",
            "Zhixun Su"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Higher-order low-rank tensors naturally arise in many applications including\nhyperspectral data recovery, video inpainting, seismic data recon- struction,\nand so on. We propose a new model to recover a low-rank tensor by\nsimultaneously performing low-rank matrix factorizations to the all-mode ma-\ntricizations of the underlying tensor. An alternating minimization algorithm is\napplied to solve the model, along with two adaptive rank-adjusting strategies\nwhen the exact rank is not known.\n  Phase transition plots reveal that our algorithm can recover a variety of\nsynthetic low-rank tensors from significantly fewer samples than the compared\nmethods, which include a matrix completion method applied to tensor recovery\nand two state-of-the-art tensor completion methods. Further tests on real-\nworld data show similar advantages. Although our model is non-convex, our\nalgorithm performs consistently throughout the tests and give better results\nthan the compared methods, some of which are based on convex models. In\naddition, the global convergence of our algorithm can be established in the\nsense that the gradient of Lagrangian function converges to zero.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.1254v2"
    },
    {
        "title": "A Quadratically Convergent Algorithm for Structured Low-Rank\n  Approximation",
        "authors": [
            "Éric Schost",
            "Pierre-Jean Spaenlehauer"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Structured Low-Rank Approximation is a problem arising in a wide range of\napplications in Numerical Analysis and Engineering Sciences. Given an input\nmatrix $M$, the goal is to compute a matrix $M'$ of given rank $r$ in a linear\nor affine subspace $E$ of matrices (usually encoding a specific structure) such\nthat the Frobenius distance $\\lVert M-M'\\rVert$ is small. We propose a\nNewton-like iteration for solving this problem, whose main feature is that it\nconverges locally quadratically to such a matrix under mild transversality\nassumptions between the manifold of matrices of rank $r$ and the linear/affine\nsubspace $E$. We also show that the distance between the limit of the iteration\nand the optimal solution of the problem is quadratic in the distance between\nthe input matrix and the manifold of rank $r$ matrices in $E$. To illustrate\nthe applicability of this algorithm, we propose a Maple implementation and give\nexperimental results for several applicative problems that can be modeled by\nStructured Low-Rank Approximation: univariate approximate GCDs (Sylvester\nmatrices), low-rank Matrix completion (coordinate spaces) and denoising\nprocedures (Hankel matrices). Experimental results give evidence that this\nall-purpose algorithm is competitive with state-of-the-art numerical methods\ndedicated to these problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.7279v2"
    },
    {
        "title": "Linear Convergence of Variance-Reduced Stochastic Gradient without\n  Strong Convexity",
        "authors": [
            "Pinghua Gong",
            "Jieping Ye"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Stochastic gradient algorithms estimate the gradient based on only one or a\nfew samples and enjoy low computational cost per iteration. They have been\nwidely used in large-scale optimization problems. However, stochastic gradient\nalgorithms are usually slow to converge and achieve sub-linear convergence\nrates, due to the inherent variance in the gradient computation. To accelerate\nthe convergence, some variance-reduced stochastic gradient algorithms, e.g.,\nproximal stochastic variance-reduced gradient (Prox-SVRG) algorithm, have\nrecently been proposed to solve strongly convex problems. Under the strongly\nconvex condition, these variance-reduced stochastic gradient algorithms achieve\na linear convergence rate. However, many machine learning problems are convex\nbut not strongly convex. In this paper, we introduce Prox-SVRG and its\nprojected variant called Variance-Reduced Projected Stochastic Gradient (VRPSG)\nto solve a class of non-strongly convex optimization problems widely used in\nmachine learning. As the main technical contribution of this paper, we show\nthat both VRPSG and Prox-SVRG achieve a linear convergence rate without strong\nconvexity. A key ingredient in our proof is a Semi-Strongly Convex (SSC)\ninequality which is the first to be rigorously proved for a class of\nnon-strongly convex problems in both constrained and regularized settings.\nMoreover, the SSC inequality is independent of algorithms and may be applied to\nanalyze other stochastic gradient algorithms besides VRPSG and Prox-SVRG, which\nmay be of independent interest. To the best of our knowledge, this is the first\nwork that establishes the linear convergence rate for the variance-reduced\nstochastic gradient algorithms on solving both constrained and regularized\nproblems without strong convexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.1102v2"
    },
    {
        "title": "A Generalized Markov-Chain Modelling Approach to $(1,λ)$-ES Linear\n  Optimization: Technical Report",
        "authors": [
            "Alexandre Chotard",
            "Martin Holena"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Several recent publications investigated Markov-chain modelling of linear\noptimization by a $(1,\\lambda)$-ES, considering both unconstrained and linearly\nconstrained optimization, and both constant and varying step size. All of them\nassume normality of the involved random steps, and while this is consistent\nwith a black-box scenario, information on the function to be optimized (e.g.\nseparability) may be exploited by the use of another distribution. The\nobjective of our contribution is to complement previous studies realized with\nnormal steps, and to give sufficient conditions on the distribution of the\nrandom steps for the success of a constant step-size $(1,\\lambda)$-ES on the\nsimple problem of a linear function with a linear constraint. The decomposition\nof a multidimensional distribution into its marginals and the copula combining\nthem is applied to the new distributional assumptions, particular attention\nbeing paid to distributions with Archimedean copulas.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.4619v1"
    },
    {
        "title": "Playing with Duality: An Overview of Recent Primal-Dual Approaches for\n  Solving Large-Scale Optimization Problems",
        "authors": [
            "Nikos Komodakis",
            "Jean-Christophe Pesquet"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Optimization methods are at the core of many problems in signal/image\nprocessing, computer vision, and machine learning. For a long time, it has been\nrecognized that looking at the dual of an optimization problem may drastically\nsimplify its solution. Deriving efficient strategies which jointly brings into\nplay the primal and the dual problems is however a more recent idea which has\ngenerated many important new contributions in the last years. These novel\ndevelopments are grounded on recent advances in convex analysis, discrete\noptimization, parallel processing, and non-smooth optimization with emphasis on\nsparsity issues. In this paper, we aim at presenting the principles of\nprimal-dual approaches, while giving an overview of numerical methods which\nhave been proposed in different contexts. We show the benefits which can be\ndrawn from primal-dual algorithms both for solving large-scale convex\noptimization problems and discrete ones, and we provide various application\nexamples to illustrate their usefulness.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.5429v2"
    },
    {
        "title": "Solution of nonlinear Stokes equations discretized by high-order finite\n  elements on nonconforming and anisotropic meshes, with application to ice\n  sheet dynamics",
        "authors": [
            "Tobin Isaac",
            "Georg Stadler",
            "Omar Ghattas"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Motivated by the need for efficient and accurate simulation of the dynamics\nof the polar ice sheets, we design high-order finite element discretizations\nand scalable solvers for the solution of nonlinear incompressible Stokes\nequations. We focus on power-law, shear thinning rheologies used in modeling\nice dynamics and other geophysical flows. We use nonconforming hexahedral\nmeshes and the conforming inf-sup stable finite element velocity-pressure\npairings $\\mathbb{Q}_k\\times \\mathbb{Q}^\\text{disc}_{k-2}$ or $\\mathbb{Q}_k\n\\times \\mathbb{P}^\\text{disc}_{k-1}$. To solve the nonlinear equations, we\npropose a Newton-Krylov method with a block upper triangular preconditioner for\nthe linearized Stokes systems. The diagonal blocks of this preconditioner are\nsparse approximations of the (1,1)-block and of its Schur complement. The\n(1,1)-block is approximated using linear finite elements based on the nodes of\nthe high-order discretization, and the application of its inverse is\napproximated using algebraic multigrid with an incomplete factorization\nsmoother. This preconditioner is designed to be efficient on anisotropic\nmeshes, which are necessary to match the high aspect ratio domains typical for\nice sheets. We develop and make available extensions to two libraries---a\nhybrid meshing scheme for the p4est parallel AMR library, and a modified\nsmoothed aggregation scheme for PETSc---to improve their support for solving\nPDEs in high aspect ratio domains. In a numerical study, we find that our\nsolver yields fast convergence that is independent of the element aspect ratio,\nthe occurrence of nonconforming interfaces, and of mesh refinement, and that\ndepends only weakly on the polynomial finite element order. We simulate the ice\nflow in a realistic description of the Antarctic ice sheet derived from field\ndata, and study the parallel scalability of our solver for problems with up to\n383M unknowns.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.6573v3"
    },
    {
        "title": "On Data Preconditioning for Regularized Loss Minimization",
        "authors": [
            "Tianbao Yang",
            "Rong Jin",
            "Shenghuo Zhu",
            "Qihang Lin"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  In this work, we study data preconditioning, a well-known and long-existing\ntechnique, for boosting the convergence of first-order methods for regularized\nloss minimization. It is well understood that the condition number of the\nproblem, i.e., the ratio of the Lipschitz constant to the strong convexity\nmodulus, has a harsh effect on the convergence of the first-order optimization\nmethods. Therefore, minimizing a small regularized loss for achieving good\ngeneralization performance, yielding an ill conditioned problem, becomes the\nbottleneck for big data problems. We provide a theory on data preconditioning\nfor regularized loss minimization. In particular, our analysis exhibits an\nappropriate data preconditioner and characterizes the conditions on the loss\nfunction and on the data under which data preconditioning can reduce the\ncondition number and therefore boost the convergence for minimizing the\nregularized loss. To make the data preconditioning practically useful, we\nendeavor to employ and analyze a random sampling approach to efficiently\ncompute the preconditioned data. The preliminary experiments validate our\ntheory.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.3115v4"
    },
    {
        "title": "Application of approximate matrix factorization to high order linearly\n  implicit Runge-Kutta methods",
        "authors": [
            "Hong Zhang",
            "Adrian Sandu",
            "Paul Tranquilli"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Linearly implicit Runge-Kutta methods with approximate matrix factorization\ncan solve efficiently large systems of differential equations that have a stiff\nlinear part, e.g. reaction-diffusion systems. However, the use of approximate\nfactorization usually leads to loss of accuracy, which makes it attractive only\nfor low order time integration schemes. This paper discusses the application of\napproximate matrix factorization with high order methods; an inexpensive\ncorrection procedure applied to each stage allows to retain the high order of\nthe underlying linearly implicit Runge-Kutta scheme. The accuracy and stability\nof the methods are studied. Numerical experiments on reaction-diffusion type\nproblems of different sizes and with different degrees of stiffness illustrate\nthe efficiency of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.3622v2"
    },
    {
        "title": "A Subspace Method for Array Covariance Matrix Estimation",
        "authors": [
            "Mostafa Rahmani",
            "George Atia"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This paper introduces a subspace method for the estimation of an array\ncovariance matrix. It is shown that when the received signals are uncorrelated,\nthe true array covariance matrices lie in a specific subspace whose dimension\nis typically much smaller than the dimension of the full space. Based on this\nidea, a subspace based covariance matrix estimator is proposed. The estimator\nis obtained as a solution to a semi-definite convex optimization problem. While\nthe optimization problem has no closed-form solution, a nearly optimal\nclosed-form solution is proposed making it easy to implement. In comparison to\nthe conventional approaches, the proposed method yields higher estimation\naccuracy because it eliminates the estimation error which does not lie in the\nsubspace of the true covariance matrices. The numerical examples indicate that\nthe proposed covariance matrix estimator can significantly improve the\nestimation quality of the covariance matrix.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.0622v1"
    },
    {
        "title": "The Optimal Arbitrary-Proportional Finite-Set-Partitioning",
        "authors": [
            "Tiancheng Li"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  This paper considers the arbitrary-proportional finite-set-partitioning\nproblem which involves partitioning a finite set into multiple subsets with\nrespect to arbitrary nonnegative proportions. This is the core art of many\nfundamental problems such as determining quotas for different individuals of\ndifferent weights or sampling from a discrete-valued weighted sample set to get\na new identically distributed but non-weighted sample set (e.g. the resampling\nneeded in the particle filter). The challenge raises as the size of each subset\nmust be an integer while its unbiased expectation is often not. To solve this\nproblem, a metric (cost function) is defined on their discrepancies and\ncorrespondingly a solution is proposed to determine the sizes of each subsets,\ngaining the minimal bias. Theoretical proof and simulation demonstrations are\nprovided to demonstrate the optimality of the scheme in the sense of the\nproposed metric.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.6529v1"
    },
    {
        "title": "Fast Finite Field Hartley Transforms Based on Hadamard Decomposition",
        "authors": [
            "H. M. de Oliveira",
            "R. G. F. Távora",
            "R. J. Cintra",
            "R. M. Campello de Souza"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  A new transform over finite fields, the finite field Hartley transform\n(FFHT), was recently introduced and a number of promising applications on the\ndesign of efficient multiple access systems and multilevel spread spectrum\nsequences were proposed. The FFHT exhibits interesting symmetries, which are\nexploited to derive tailored fast transform algorithms. The proposed fast\nalgorithms are based on successive decompositions of the FFHT by means of\nHadamard-Walsh transforms (HWT). The introduced decompositions meet the lower\nbound on the multiplicative complexity for all the cases investigated. The\ncomplexity of the new algorithms is compared with that of traditional\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00277v1"
    },
    {
        "title": "Compactly Supported Wavelets Derived From Legendre Polynomials:\n  Spherical Harmonic Wavelets",
        "authors": [
            "M. M. S. Lira",
            "H. M. de Oliveira",
            "M. A. Carvalho Jr",
            "R. M. Campello de Souza"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  A new family of wavelets is introduced, which is associated with Legendre\npolynomials. These wavelets, termed spherical harmonic or Legendre wavelets,\npossess compact support. The method for the wavelet construction is derived\nfrom the association of ordinary second order differential equations with\nmultiresolution filters. The low-pass filter associated with Legendre\nmultiresolution analysis is a linear phase finite impulse response filter\n(FIR).\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00950v1"
    },
    {
        "title": "A Factorization Scheme for Some Discrete Hartley Transform Matrices",
        "authors": [
            "H. M. de Oliveira",
            "R. J. Cintra",
            "R. M. Campello de Souza"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Discrete transforms such as the discrete Fourier transform (DFT) and the\ndiscrete Hartley transform (DHT) are important tools in numerical analysis. The\nsuccessful application of transform techniques relies on the existence of\nefficient fast transforms. In this paper some fast algorithms are derived. The\ntheoretical lower bound on the multiplicative complexity for the DFT/DHT are\nachieved. The approach is based on the factorization of DHT matrices.\nAlgorithms for short blocklengths such as $N \\in \\{3, 5, 6, 12, 24 \\}$ are\npresented.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.01038v1"
    },
    {
        "title": "eOMP: Finding Sparser Representation by Recursively Orthonormalizing the\n  Remaining Atoms",
        "authors": [
            "Yuanyi Xue",
            "Yao Wang"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Greedy algorithms for minimizing L0-norm of sparse decomposition have\nprofound application impact on many signal processing problems. In the sparse\ncoding setup, given the observations $\\mathrm{y}$ and the redundant dictionary\n$\\mathbf{\\Phi}$, one would seek the most sparse coefficient (signal)\n$\\mathrm{x}$ with a constraint on approximation fidelity. In this work, we\npropose a greedy algorithm based on the classic orthogonal matching pursuit\n(OMP) with improved sparsity on $\\mathrm{x}$ and better recovery rate, which we\nname as eOMP. The key ingredient of the eOMP is recursively performing one-step\northonormalization on the remaining atoms, and evaluating correlations between\nresidual and orthonormalized atoms. We show a proof that the proposed eOMP\nguarantees to maximize the residual reduction at each iteration. Through\nextensive simulations, we show the proposed algorithm has better exact recovery\nrate on i.i.d. Gaussian ensembles with Gaussian signals, and more importantly\nyields smaller L0-norm under the same approximation fidelity compared to the\noriginal OMP, for both synthetic and practical scenarios. The complexity\nanalysis and real running time result also show a manageable complexity\nincrease over the original OMP. We claim that the proposed algorithm has better\npractical perspective for finding more sparse representations than existing\ngreedy algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.03805v1"
    },
    {
        "title": "Mercer kernels and integrated variance experimental design: connections\n  between Gaussian process regression and polynomial approximation",
        "authors": [
            "Alex A. Gorodetsky",
            "Youssef M. Marzouk"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This paper examines experimental design procedures used to develop surrogates\nof computational models, exploring the interplay between experimental designs\nand approximation algorithms. We focus on two widely used approximation\napproaches, Gaussian process (GP) regression and non-intrusive polynomial\napproximation. First, we introduce algorithms for minimizing a posterior\nintegrated variance (IVAR) design criterion for GP regression. Our formulation\ntreats design as a continuous optimization problem that can be solved with\ngradient-based methods on complex input domains, without resorting to greedy\napproximations. We show that minimizing IVAR in this way yields point sets with\ngood interpolation properties, and that it enables more accurate GP regression\nthan designs based on entropy minimization or mutual information maximization.\nSecond, using a Mercer kernel/eigenfunction perspective on GP regression, we\nidentify conditions under which GP regression coincides with pseudospectral\npolynomial approximation. Departures from these conditions can be understood as\nchanges either to the kernel or to the experimental design itself. We then show\nhow IVAR-optimal designs, while sacrificing discrete orthogonality of the\nkernel eigenfunctions, can yield lower approximation error than orthogonalizing\npoint sets. Finally, we compare the performance of adaptive Gaussian process\nregression and adaptive pseudospectral approximation for several classes of\ntarget functions, identifying features that are favorable to the GP + IVAR\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00021v4"
    },
    {
        "title": "Scrambled geometric net integration over general product spaces",
        "authors": [
            "K. Basu",
            "A. B. Owen"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Quasi-Monte Carlo (QMC) sampling has been developed for integration over\n$[0,1]^s$ where it has superior accuracy to Monte Carlo (MC) for integrands of\nbounded variation. Scrambled net quadrature gives allows replication based\nerror estimation for QMC with at least the same accuracy and for smooth enough\nintegrands even better accuracy than plain QMC. Integration over triangles,\nspheres, disks and Cartesian products of such spaces is more difficult for QMC\nbecause the induced integrand on a unit cube may fail to have the desired\nregularity. In this paper, we present a construction of point sets for\nnumerical integration over Cartesian products of $s$ spaces of dimension $d$,\nwith triangles ($d=2$) being of special interest. The point sets are\ntransformations of randomized $(t,m,s)$-nets using recursive geometric\npartitions. The resulting integral estimates are unbiased and their variance is\n$o(1/n)$ for any integrand in $L^2$ of the product space. Under smoothness\nassumptions on the integrand, our randomized QMC algorithm has variance\n$O(n^{-1 - 2/d} (\\log n)^{s-1})$, for integration over $s$-fold Cartesian\nproducts of $d$-dimensional domains, compared to $O(n^{-1})$ for ordinary Monte\nCarlo.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.02737v1"
    },
    {
        "title": "Do current lattice Boltzmann methods for diffusion and diffusion-type\n  equations respect maximum principles and the non-negative constraint?",
        "authors": [
            "S. Karimi",
            "K. B. Nakshatrala"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  The lattice Boltzmann method (LBM) has established itself as a valid\nnumerical method in computational fluid dynamics. Recently,\nmultiple-relaxation-time LBM has been proposed to simulate anisotropic\nadvection-diffusion processes. The governing differential equations of\nadvective-diffusive systems are known to satisfy maximum principles, comparison\nprinciples, the non-negative constraint, and the decay property. In this paper,\nit will be shown that current single- and multiple-relaxation-time lattice\nBoltzmann methods fail to preserve these mathematical properties for transient\ndiffusion-type equations. It will also be shown that the discretization of\nDirichlet boundary conditions will affect the performance of lattice Boltzmann\nmethods in meeting these mathematical principles. A new way of discretizing the\nDirichlet boundary conditions is also proposed. Several benchmark problems have\nbeen solved to illustrate the performance of lattice Boltzmann methods and the\neffect of discretization of boundary conditions with respect to the\naforementioned mathematical properties for transient diffusion and\nadvection-diffusion equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08360v2"
    },
    {
        "title": "A New Selection Operator for the Discrete Empirical Interpolation Method\n  -- improved a priori error bound and extensions",
        "authors": [
            "Zlatko Drmac",
            "Serkan Gugercin"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  This paper introduces a new framework for constructing the Discrete Empirical\nInterpolation Method DEIM projection operator. The interpolation node selection\nprocedure is formulated using the QR factorization with column pivoting, and it\nenjoys a sharper error bound for the DEIM projection error. Furthermore, for a\nsubspace $\\mathcal{U}$ given as the range of an orthonormal $U$, the DEIM\nprojection does not change if $U$ is replaced by $U \\Omega$ with arbitrary\nunitary matrix $\\Omega$. In a large-scale setting, the new approach allows\nmodifications that use only randomly sampled rows of $U$, but with the\npotential of producing good approximations with corresponding probabilistic\nerror bounds. Another salient feature of the new framework is that robust and\nefficient software implementation is easily developed, based on readily\navailable high performance linear algebra packages.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.00370v2"
    },
    {
        "title": "Fast exact summation using small and large superaccumulators",
        "authors": [
            "Radford M. Neal"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  I present two new methods for exactly summing a set of floating-point\nnumbers, and then correctly rounding to the nearest floating-point number.\nHigher accuracy than simple summation (rounding after each addition) is\nimportant in many applications, such as finding the sample mean of data. Exact\nsummation also guarantees identical results with parallel and serial\nimplementations, since the exact sum is independent of order. The new methods\nuse variations on the concept of a \"superaccumulator\" - a large fixed-point\nnumber that can exactly represent the sum of any reasonable number of\nfloating-point values. One method uses a \"small\" superaccumulator with\nsixty-seven 64-bit chunks, each with 32-bit overlap with the next chunk,\nallowing carry propagation to be done infrequently. The small superaccumulator\nis used alone when summing a small number of terms. For big summations, a\n\"large\" superaccumulator is used as well. It consists of 4096 64-bit chunks,\none for every possible combination of exponent bits and sign bit, plus counts\nof when each chunk needs to be transferred to the small superaccumulator. To\nadd a term to the large superaccumulator, only a single chunk and its\nassociated count need to be updated, which takes very few instructions if\ncarefully implemented. On modern 64-bit processors, exactly summing a large\narray using this combination of large and small superaccumulators takes less\nthan twice the time of simple, inexact, ordered summation, with a serial\nimplementation. A parallel implementation using a small number of processor\ncores can be expected to perform exact summation of large arrays at a speed\nthat reaches the limit imposed by memory bandwidth. Some common methods that\nattempt to improve accuracy without being exact may therefore be pointless, at\nleast for large summations, since they are slower than computing the sum\nexactly.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.05571v1"
    },
    {
        "title": "Parallel MMF: a Multiresolution Approach to Matrix Computation",
        "authors": [
            "Risi Kondor",
            "Nedelina Teneva",
            "Pramod K. Mudrakarta"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Multiresolution Matrix Factorization (MMF) was recently introduced as a\nmethod for finding multiscale structure and defining wavelets on\ngraphs/matrices. In this paper we derive pMMF, a parallel algorithm for\ncomputing the MMF factorization. Empirically, the running time of pMMF scales\nlinearly in the dimension for sparse matrices. We argue that this makes pMMF a\nvaluable new computational primitive in its own right, and present experiments\non using pMMF for two distinct purposes: compressing matrices and\npreconditioning large sparse linear systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04396v1"
    },
    {
        "title": "Estimating the Trace of the Matrix Inverse by Interpolating from the\n  Diagonal of an Approximate Inverse",
        "authors": [
            "Lingfei Wu",
            "Jesse Laeuchli",
            "Vassilis Kalantzis",
            "Andreas Stathopoulos",
            "Efstratios Gallopoulos"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  A number of applications require the computation of the trace of a matrix\nthat is implicitly available through a function. A common example of a function\nis the inverse of a large, sparse matrix, which is the focus of this paper.\nWhen the evaluation of the function is expensive, the task is computationally\nchallenging because the standard approach is based on a Monte Carlo method\nwhich converges slowly. We present a different approach that exploits the\npattern correlation, if present, between the diagonal of the inverse of the\nmatrix and the diagonal of some approximate inverse that can be computed\ninexpensively. We leverage various sampling and fitting techniques to fit the\ndiagonal of the approximation to the diagonal of the inverse. Depending on the\nquality of the approximate inverse, our method may serve as a standalone kernel\nfor providing a fast trace estimate with a small number of samples.\nFurthermore, the method can be used as a variance reduction method for Monte\nCarlo in some cases. This is decided dynamically by our algorithm. An extensive\nset of experiments with various technique combinations on several matrices from\nsome real applications demonstrate the potential of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.07227v3"
    },
    {
        "title": "A Scalable and Extensible Framework for Superposition-Structured Models",
        "authors": [
            "Shenjian Zhao",
            "Cong Xie",
            "Zhihua Zhang"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In many learning tasks, structural models usually lead to better\ninterpretability and higher generalization performance. In recent years,\nhowever, the simple structural models such as lasso are frequently proved to be\ninsufficient. Accordingly, there has been a lot of work on\n\"superposition-structured\" models where multiple structural constraints are\nimposed. To efficiently solve these \"superposition-structured\" statistical\nmodels, we develop a framework based on a proximal Newton-type method.\nEmploying the smoothed conic dual approach with the LBFGS updating formula, we\npropose a scalable and extensible proximal quasi-Newton (SEP-QN) framework.\nEmpirical analysis on various datasets shows that our framework is potentially\npowerful, and achieves super-linear convergence rate for optimizing some\npopular \"superposition-structured\" statistical models such as the fused sparse\ngroup lasso.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.02314v2"
    },
    {
        "title": "A Near-Optimal Subdivision Algorithm for Complex Root Isolation based on\n  the Pellet Test and Newton Iteration",
        "authors": [
            "Ruben Becker",
            "Michael Sagraloff",
            "Vikram Sharma",
            "Chee Yap"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We describe a subdivision algorithm for isolating the complex roots of a\npolynomial $F\\in\\mathbb{C}[x]$. Given an oracle that provides approximations of\neach of the coefficients of $F$ to any absolute error bound and given an\narbitrary square $\\mathcal{B}$ in the complex plane containing only simple\nroots of $F$, our algorithm returns disjoint isolating disks for the roots of\n$F$ in $\\mathcal{B}$. Our complexity analysis bounds the absolute error to\nwhich the coefficients of $F$ have to be provided, the total number of\niterations, and the overall bit complexity. It further shows that the\ncomplexity of our algorithm is controlled by the geometry of the roots in a\nnear neighborhood of the input square $\\mathcal{B}$, namely, the number of\nroots, their absolute values and pairwise distances. The number of subdivision\nsteps is near-optimal. For the \\emph{benchmark problem}, namely, to isolate all\nthe roots of a polynomial of degree $n$ with integer coefficients of bit size\nless than $\\tau$, our algorithm needs $\\tilde O(n^3+n^2\\tau)$ bit operations,\nwhich is comparable to the record bound of Pan (2002). It is the first time\nthat such a bound has been achieved using subdivision methods, and independent\nof divide-and-conquer techniques such as Sch\\\"onhage's splitting circle\ntechnique. Our algorithm uses the quadtree construction of Weyl (1924) with two\nkey ingredients: using Pellet's Theorem (1881) combined with Graeffe iteration,\nwe derive a \"soft-test\" to count the number of roots in a disk. Using\nSchr\\\"oder's modified Newton operator combined with bisection, in a form\ninspired by the quadratic interval method from Abbot (2006), we achieve\nquadratic convergence towards root clusters. Relative to the divide-conquer\nalgorithms, our algorithm is quite simple with the potential of being\npractical. This paper is self-contained: we provide pseudo-code for all\nsubroutines used by our algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.06231v4"
    },
    {
        "title": "Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization",
        "authors": [
            "Fanhua Shang",
            "Yuanyuan Liu",
            "James Cheng"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  The Schatten-p quasi-norm $(0<p<1)$ is usually used to replace the standard\nnuclear norm in order to approximate the rank function more accurately.\nHowever, existing Schatten-p quasi-norm minimization algorithms involve\nsingular value decomposition (SVD) or eigenvalue decomposition (EVD) in each\niteration, and thus may become very slow and impractical for large-scale\nproblems. In this paper, we first define two tractable Schatten quasi-norms,\ni.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove\nthat they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively,\nwhich lead to the design of very efficient algorithms that only need to update\ntwo much smaller factor matrices. We also design two efficient proximal\nalternating linearized minimization algorithms for solving representative\nmatrix completion problems. Finally, we provide the global convergence and\nperformance guarantees for our algorithms, which have better convergence\nproperties than existing algorithms. Experimental results on synthetic and\nreal-world data show that our algorithms are more accurate than the\nstate-of-the-art methods, and are orders of magnitude faster.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.01245v1"
    },
    {
        "title": "Multitimescale method for approximating the path action relevant to\n  non-equilibrium statistical physics",
        "authors": [
            "Richard Kleeman"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  A path integral formalism has been proposed recently for non-equilibrium\nstatistical physics applications by the author. In this contribution we outline\nan efficient method for its numerical evaluation. The method used is based on\nthe multiscale MCMC method of Ceperley and co-workers in quantum applications.\nA significant new feature of the method proposed is that the time endpoint is\nnot fixed and indeed the endpoint sample is the principle object of interest.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.01889v1"
    },
    {
        "title": "Tensor Network alternating linear scheme for MIMO Volterra system\n  identification",
        "authors": [
            "Kim Batselier",
            "Zhongming Chen",
            "Ngai Wong"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  This article introduces two Tensor Network-based iterative algorithms for the\nidentification of high-order discrete-time nonlinear multiple-input\nmultiple-output (MIMO) Volterra systems. The system identification problem is\nrewritten in terms of a Volterra tensor, which is never explicitly constructed,\nthus avoiding the curse of dimensionality. It is shown how each iteration of\nthe two identification algorithms involves solving a linear system of low\ncomputational complexity. The proposed algorithms are guaranteed to\nmonotonically converge and numerical stability is ensured through the use of\northogonal matrix factorizations. The performance and accuracy of the two\nidentification algorithms are illustrated by numerical experiments, where\naccurate degree-10 MIMO Volterra models are identified in about 1 second in\nMatlab on a standard desktop pc.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.00127v2"
    },
    {
        "title": "A multilevel framework for sparse optimization with application to\n  inverse covariance estimation and logistic regression",
        "authors": [
            "Eran Treister",
            "Javier S. Turek",
            "Irad Yavneh"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Solving l1 regularized optimization problems is common in the fields of\ncomputational biology, signal processing and machine learning. Such l1\nregularization is utilized to find sparse minimizers of convex functions. A\nwell-known example is the LASSO problem, where the l1 norm regularizes a\nquadratic function. A multilevel framework is presented for solving such l1\nregularized sparse optimization problems efficiently. We take advantage of the\nexpected sparseness of the solution, and create a hierarchy of problems of\nsimilar type, which is traversed in order to accelerate the optimization\nprocess. This framework is applied for solving two problems: (1) the sparse\ninverse covariance estimation problem, and (2) l1-regularized logistic\nregression. In the first problem, the inverse of an unknown covariance matrix\nof a multivariate normal distribution is estimated, under the assumption that\nit is sparse. To this end, an l1 regularized log-determinant optimization\nproblem needs to be solved. This task is challenging especially for large-scale\ndatasets, due to time and memory limitations. In the second problem, the\nl1-regularization is added to the logistic regression classification objective\nto reduce overfitting to the data and obtain a sparse model. Numerical\nexperiments demonstrate the efficiency of the multilevel framework in\naccelerating existing iterative solvers for both of these problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.00315v1"
    },
    {
        "title": "Approximate Joint Matrix Triangularization",
        "authors": [
            "Nicolo Colombo",
            "Nikos Vlassis"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We consider the problem of approximate joint triangularization of a set of\nnoisy jointly diagonalizable real matrices. Approximate joint triangularizers\nare commonly used in the estimation of the joint eigenstructure of a set of\nmatrices, with applications in signal processing, linear algebra, and tensor\ndecomposition. By assuming the input matrices to be perturbations of\nnoise-free, simultaneously diagonalizable ground-truth matrices, the\napproximate joint triangularizers are expected to be perturbations of the exact\njoint triangularizers of the ground-truth matrices. We provide a priori and a\nposteriori perturbation bounds on the `distance' between an approximate joint\ntriangularizer and its exact counterpart. The a priori bounds are theoretical\ninequalities that involve functions of the ground-truth matrices and noise\nmatrices, whereas the a posteriori bounds are given in terms of observable\nquantities that can be computed from the input matrices. From a practical\nperspective, the problem of finding the best approximate joint triangularizer\nof a set of noisy matrices amounts to solving a nonconvex optimization problem.\nWe show that, under a condition on the noise level of the input matrices, it is\npossible to find a good initial triangularizer such that the solution obtained\nby any local descent-type algorithm has certain global guarantees. Finally, we\ndiscuss the application of approximate joint matrix triangularization to\ncanonical tensor decomposition and we derive novel estimation error bounds.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.00514v1"
    },
    {
        "title": "Proximal Quasi-Newton Methods for Regularized Convex Optimization with\n  Linear and Accelerated Sublinear Convergence Rates",
        "authors": [
            "Hiva Ghanbari",
            "Katya Scheinberg"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In [19], a general, inexact, efficient proximal quasi-Newton algorithm for\ncomposite optimization problems has been proposed and a sublinear global\nconvergence rate has been established. In this paper, we analyze the\nconvergence properties of this method, both in the exact and inexact setting,\nin the case when the objective function is strongly convex. We also investigate\na practical variant of this method by establishing a simple stopping criterion\nfor the subproblem optimization. Furthermore, we consider an accelerated\nvariant, based on FISTA [1], to the proximal quasi-Newton algorithm. A similar\naccelerated method has been considered in [7], where the convergence rate\nanalysis relies on very strong impractical assumptions. We present a modified\nanalysis while relaxing these assumptions and perform a practical comparison of\nthe accelerated proximal quasi- Newton algorithm and the regular one. Our\nanalysis and computational results show that acceleration may not bring any\nbenefit in the quasi-Newton setting.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.03081v2"
    },
    {
        "title": "High-Performance Algorithms for Computing the Sign Function of\n  Triangular Matrices",
        "authors": [
            "Vadim Stotland",
            "Oded Schwartz",
            "Sivan Toledo"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Algorithms and implementations for computing the sign function of a\ntriangular matrix are fundamental building blocks in algorithms for computing\nthe sign of arbitrary square real or complex matrices. We present novel\nrecursive and cache efficient algorithms that are based on Higham's stabilized\nspecialization of Parlett's substitution algorithm for computing the sign of a\ntriangular matrix. We show that the new recursive algorithms are asymptotically\noptimal in terms of the number of cache misses that they generate. One of the\nnovel algorithms that we present performs more arithmetic than the\nnon-recursive version, but this allows it to benefit from calling\nhighly-optimized matrix-multiplication routines; the other performs the same\nnumber of operations as the non-recursive version, but it uses custom\ncomputational kernels instead. We present implementations of both, as well as a\ncache-efficient implementation of a block version of Parlett's algorithm. Our\nexperiments show that the blocked and recursive versions are much faster than\nthe previous algorithms, and that the inertia strongly influences their\nrelative performance, as predicted by our analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.06303v1"
    },
    {
        "title": "Fast estimation of approximate matrix ranks using spectral densities",
        "authors": [
            "Shashanka Ubaru",
            "Yousef Saad",
            "Abd-Krim Seghouane"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In many machine learning and data related applications, it is required to\nhave the knowledge of approximate ranks of large data matrices at hand. In this\npaper, we present two computationally inexpensive techniques to estimate the\napproximate ranks of such large matrices. These techniques exploit approximate\nspectral densities, popular in physics, which are probability density\ndistributions that measure the likelihood of finding eigenvalues of the matrix\nat a given point on the real line. Integrating the spectral density over an\ninterval gives the eigenvalue count of the matrix in that interval. Therefore\nthe rank can be approximated by integrating the spectral density over a\ncarefully selected interval. Two different approaches are discussed to estimate\nthe approximate rank, one based on Chebyshev polynomials and the other based on\nthe Lanczos algorithm. In order to obtain the appropriate interval, it is\nnecessary to locate a gap between the eigenvalues that correspond to noise and\nthe relevant eigenvalues that contribute to the matrix rank. A method for\nlocating this gap and selecting the interval of integration is proposed based\non the plot of the spectral density. Numerical experiments illustrate the\nperformance of these techniques on matrices from typical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05754v1"
    },
    {
        "title": "Accelerating Nuclear Configuration Interaction Calculations through a\n  Preconditioned Block Iterative Eigensolver",
        "authors": [
            "Meiyue Shao",
            "Hasan Metin Aktulga",
            "Chao Yang",
            "Esmond G. Ng",
            "Pieter Maris",
            "James P. Vary"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We describe a number of recently developed techniques for improving the\nperformance of large-scale nuclear configuration interaction calculations on\nhigh performance parallel computers. We show the benefit of using a\npreconditioned block iterative method to replace the Lanczos algorithm that has\ntraditionally been used to perform this type of computation. The rapid\nconvergence of the block iterative method is achieved by a proper choice of\nstarting guesses of the eigenvectors and the construction of an effective\npreconditioner. These acceleration techniques take advantage of special\nstructure of the nuclear configuration interaction problem which we discuss in\ndetail. The use of a block method also allows us to improve the concurrency of\nthe computation, and take advantage of the memory hierarchy of modern\nmicroprocessors to increase the arithmetic intensity of the computation\nrelative to data movement. We also discuss implementation details that are\ncritical to achieving high performance on massively parallel multi-core\nsupercomputers, and demonstrate that the new block iterative solver is two to\nthree times faster than the Lanczos based algorithm for problems of moderate\nsizes on a Cray XC30 system.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.01689v3"
    },
    {
        "title": "Tensor Completion by Alternating Minimization under the Tensor Train\n  (TT) Model",
        "authors": [
            "Wenqi Wang",
            "Vaneet Aggarwal",
            "Shuchin Aeron"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Using the matrix product state (MPS) representation of tensor train\ndecompositions, in this paper we propose a tensor completion algorithm which\nalternates over the matrices (tensors) in the MPS representation. This\ndevelopment is motivated in part by the success of matrix completion algorithms\nwhich alternate over the (low-rank) factors. We comment on the computational\ncomplexity of the proposed algorithm and numerically compare it with existing\nmethods employing low rank tensor train approximation for data completion as\nwell as several other recently proposed methods. We show that our method is\nsuperior to existing ones for a variety of real settings.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.05587v1"
    },
    {
        "title": "Convergence of a Grassmannian Gradient Descent Algorithm for Subspace\n  Estimation From Undersampled Data",
        "authors": [
            "Dejiao Zhang",
            "Laura Balzano"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Subspace learning and matrix factorization problems have great many\napplications in science and engineering, and efficient algorithms are critical\nas dataset sizes continue to grow. Many relevant problem formulations are\nnon-convex, and in a variety of contexts it has been observed that solving the\nnon-convex problem directly is not only efficient but reliably accurate. We\ndiscuss convergence theory for a particular method: first order incremental\ngradient descent constrained to the Grassmannian. The output of the algorithm\nis an orthonormal basis for a $d$-dimensional subspace spanned by an input\nstreaming data matrix. We study two sampling cases: where each data vector of\nthe streaming matrix is fully sampled, or where it is undersampled by a\nsampling matrix $A_t\\in \\mathbb{R}^{m\\times n}$ with $m\\ll n$. Our results\ncover two cases, where $A_t$ is Gaussian or a subset of rows of the identity\nmatrix. We propose an adaptive stepsize scheme that depends only on the sampled\ndata and algorithm outputs. We prove that with fully sampled data, the stepsize\nscheme maximizes the improvement of our convergence metric at each iteration,\nand this method converges from any random initialization to the true subspace,\ndespite the non-convex formulation and orthogonality constraints. For the case\nof undersampled data, we establish monotonic expected improvement on the\ndefined convergence metric for each iteration with high probability.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.00199v3"
    },
    {
        "title": "Tensor Computation: A New Framework for High-Dimensional Problems in EDA",
        "authors": [
            "Zheng Zhang",
            "Kim Batselier",
            "Haotian Liu",
            "Luca Daniel",
            "Ngai Wong"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  Many critical EDA problems suffer from the curse of dimensionality, i.e. the\nvery fast-scaling computational burden produced by large number of parameters\nand/or unknown variables. This phenomenon may be caused by multiple spatial or\ntemporal factors (e.g. 3-D field solvers discretizations and multi-rate circuit\nsimulation), nonlinearity of devices and circuits, large number of design or\noptimization parameters (e.g. full-chip routing/placement and circuit sizing),\nor extensive process variations (e.g. variability/reliability analysis and\ndesign for manufacturability). The computational challenges generated by such\nhigh dimensional problems are generally hard to handle efficiently with\ntraditional EDA core algorithms that are based on matrix and vector\ncomputation. This paper presents \"tensor computation\" as an alternative general\nframework for the development of efficient EDA algorithms and tools. A tensor\nis a high-dimensional generalization of a matrix and a vector, and is a natural\nchoice for both storing and solving efficiently high-dimensional EDA problems.\nThis paper gives a basic tutorial on tensors, demonstrates some recent examples\nof EDA applications (e.g., nonlinear circuit modeling and high-dimensional\nuncertainty quantification), and suggests further open EDA problems where the\nuse of tensor computation could be of advantage.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.04272v1"
    },
    {
        "title": "Simultaneous diagonalisation of the covariance and complementary\n  covariance matrices in quaternion widely linear signal processing",
        "authors": [
            "Min Xiang",
            "Shirin Enshaeifar",
            "Alexander E. Stott",
            "Clive Cheong Took",
            "Yili Xia",
            "Sithan Kanna",
            "Danilo P. Mandic"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Recent developments in quaternion-valued widely linear processing have\nestablished that the exploitation of complete second-order statistics requires\nconsideration of both the standard covariance and the three complementary\ncovariance matrices. Although such matrices have a tremendous amount of\nstructure and their decomposition is a powerful tool in a variety of\napplications, the non-commutative nature of the quaternion product has been\nprohibitive to the development of quaternion uncorrelating transforms. To this\nend, we introduce novel techniques for a simultaneous decomposition of the\ncovariance and complementary covariance matrices in the quaternion domain,\nwhereby the quaternion version of the Takagi factorisation is explored to\ndiagonalise symmetric quaternion-valued matrices. This gives new insights into\nthe quaternion uncorrelating transform (QUT) and forms a basis for the proposed\nquaternion approximate uncorrelating transform (QAUT) which simultaneously\ndiagonalises all four covariance matrices associated with improper quaternion\nsignals. The effectiveness of the proposed uncorrelating transforms is\nvalidated by simulations on both synthetic and real-world quaternion-valued\nsignals.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.00058v2"
    },
    {
        "title": "Fourth-order Tensors with Multidimensional Discrete Transforms",
        "authors": [
            "Xiao-Yang Liu",
            "Xiaodong Wang"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  The big data era is swamping areas including data analysis, machine/deep\nlearning, signal processing, statistics, scientific computing, and cloud\ncomputing. The multidimensional feature and huge volume of big data put urgent\nrequirements to the development of multilinear modeling tools and efficient\nalgorithms. In this paper, we build a novel multilinear tensor space that\nsupports useful algorithms such as SVD and QR, while generalizing the matrix\nspace to fourth-order tensors was believed to be challenging. Specifically,\ngiven any multidimensional discrete transform, we show that fourth-order\ntensors are bilinear operators on a space of matrices. First, we take a\ntransform-based approach to construct a new tensor space by defining a new\nmultiplication operation and tensor products, and accordingly the analogous\nconcepts: identity, inverse, transpose, linear combinations, and orthogonality.\nSecondly, we define the $\\mathcal{L}$-SVD for fourth-order tensors and present\nan efficient algorithm, where the tensor case requires a stronger condition for\nunique decomposition than the matrix case. Thirdly, we define the tensor\n$\\mathcal{L}$-QR decomposition and propose a Householder QR algorithm to avoid\nthe catastrophic cancellation problem associated with the conventional\nGram-Schmidt process. Finally, we validate our schemes on video compression and\none-shot face recognition. For video compression, compared with the existing\ntSVD, the proposed $\\mathcal{L}$-SVD achieves $3\\sim 10$dB gains in RSE, while\nthe running time is reduced by about $50\\%$ and $87.5\\%$, respectively. For\none-shot face recognition, the recognition rate is increased by about $10\\%\n\\sim 20\\%$.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01576v1"
    },
    {
        "title": "Recent implementations, applications, and extensions of the Locally\n  Optimal Block Preconditioned Conjugate Gradient method (LOBPCG)",
        "authors": [
            "Andrew Knyazev"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Since introduction [A. Knyazev, Toward the optimal preconditioned\neigensolver: Locally optimal block preconditioned conjugate gradient method,\nSISC (2001) DOI:10.1137/S1064827500366124] and efficient parallel\nimplementation [A. Knyazev et al., Block locally optimal preconditioned\neigenvalue xolvers (BLOPEX) in HYPRE and PETSc, SISC (2007)\nDOI:10.1137/060661624], LOBPCG has been used is a wide range of applications in\nmechanics, material sciences, and data sciences. We review its recent\nimplementations and applications, as well as extensions of the local optimality\nidea beyond standard eigenvalue problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.08354v1"
    },
    {
        "title": "A stencil scaling approach for accelerating matrix-free finite element\n  implementations",
        "authors": [
            "Simon Bauer",
            "Daniel Drzisga",
            "Marcus Mohr",
            "Ulrich Ruede",
            "Christian Waluga",
            "Barbara Wohlmuth"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We present a novel approach to fast on-the-fly low order finite element\nassembly for scalar elliptic partial differential equations of Darcy type with\nvariable coefficients optimized for matrix-free implementations. Our approach\nintroduces a new operator that is obtained by appropriately scaling the\nreference stiffness matrix from the constant coefficient case. Assuming\nsufficient regularity, an a priori analysis shows that solutions obtained by\nthis approach are unique and have asymptotically optimal order convergence in\nthe $H^1$- and the $L^2$-norm on hierarchical hybrid grids. For the\npre-asymptotic regime, we present a local modification that guarantees uniform\nellipticity of the operator. Cost considerations show that our novel approach\nrequires roughly one third of the floating-point operations compared to a\nclassical finite element assembly scheme employing nodal integration. Our\ntheoretical considerations are illustrated by numerical tests that confirm the\nexpectations with respect to accuracy and run-time. A large scale application\nwith more than a hundred billion ($1.6\\cdot10^{11}$) degrees of freedom\nexecuted on 14,310 compute cores demonstrates the efficiency of the new scaling\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.06793v2"
    },
    {
        "title": "Towards a better understanding of the matrix product function\n  approximation algorithm in application to quantum physics",
        "authors": [
            "Moritz August",
            "Thomas Huckle"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We recently introduced a method to approximate functions of Hermitian Matrix\nProduct Operators or Tensor Trains that are of the form $\\mathsf{Tr} f(A)$.\nFunctions of this type occur in several applications, most notably in quantum\nphysics. In this work we aim at extending the theoretical understanding of our\nmethod by showing several properties of our algorithm that can be used to\ndetect and correct errors in its results. Most importantly, we show that there\nexists a more computationally efficient version of our algorithm for certain\ninputs. To illustrate the usefulness of our finding, we prove that several\nclasses of spin Hamiltonians in quantum physics fall into this input category.\nWe finally support our findings with numerical results obtained for an example\nfrom quantum physics.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.06847v2"
    },
    {
        "title": "Sampling and multilevel coarsening algorithms for fast matrix\n  approximations",
        "authors": [
            "Shashanka Ubaru",
            "Yousef Saad"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  This paper addresses matrix approximation problems for matrices that are\nlarge, sparse and/or that are representations of large graphs. To tackle these\nproblems, we consider algorithms that are based primarily on coarsening\ntechniques, possibly combined with random sampling. A multilevel coarsening\ntechnique is proposed which utilizes a hypergraph associated with the data\nmatrix and a graph coarsening strategy based on column matching. Theoretical\nresults are established that characterize the quality of the dimension\nreduction achieved by a coarsening step, when a proper column matching strategy\nis employed. We consider a number of standard applications of this technique as\nwell as a few new ones. Among the standard applications we first consider the\nproblem of computing the partial SVD for which a combination of sampling and\ncoarsening yields significantly improved SVD results relative to sampling\nalone. We also consider the Column subset selection problem, a popular low rank\napproximation method used in data related applications, and show how multilevel\ncoarsening can be adapted for this problem. Similarly, we consider the problem\nof graph sparsification and show how coarsening techniques can be employed to\nsolve it. Numerical experiments illustrate the performances of the methods in\nvarious applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.00439v2"
    },
    {
        "title": "Spectral Methods in the Presence of Discontinuities",
        "authors": [
            "Joanna Piotrowska",
            "Jonah M. Miller",
            "Erik Schnetter"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Spectral methods provide an elegant and efficient way of numerically solving\ndifferential equations of all kinds. For smooth problems, truncation error for\nspectral methods vanishes exponentially in the infinity norm and $L_2$-norm.\nHowever, for non-smooth problems, convergence is significantly worse---the\n$L_2$-norm of the error for a discontinuous problem will converge at a\nsub-linear rate and the infinity norm will not converge at all. We explore and\nimprove upon a post-processing technique---optimally convergent mollifiers---to\nrecover exponential convergence from a poorly-converging spectral\nreconstruction of non-smooth data. This is an important first step towards\nusing these techniques for simulations of realistic systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09952v2"
    },
    {
        "title": "Parallel Active Subspace Decomposition for Scalable and Efficient Tensor\n  Robust Principal Component Analysis",
        "authors": [
            "Jonathan Q. Jiang",
            "Michael K. Ng"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Tensor robust principal component analysis (TRPCA) has received a substantial\namount of attention in various fields. Most existing methods, normally relying\non tensor nuclear norm minimization, need to pay an expensive computational\ncost due to multiple singular value decompositions (SVDs) at each iteration. To\novercome the drawback, we propose a scalable and efficient method, named\nParallel Active Subspace Decomposition (PASD), which divides the unfolding\nalong each mode of the tensor into a columnwise orthonormal matrix (active\nsubspace) and another small-size matrix in parallel. Such a transformation\nleads to a nonconvex optimization problem in which the scale of nulcear norm\nminimization is generally much smaller than that in the original problem.\nFurthermore, we introduce an alternating direction method of multipliers (ADMM)\nmethod to solve the reformulated problem and provide rigorous analyses for its\nconvergence and suboptimality. Experimental results on synthetic and real-world\ndata show that our algorithm is more accurate than the state-of-the-art\napproaches, and is orders of magnitude faster.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09999v1"
    },
    {
        "title": "An efficient algorithm for global interval solution of nonlinear\n  algebraic equations and its GPGPU implementation",
        "authors": [
            "Dang Lin",
            "Liangyu Chen"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Solving nonlinear algebraic equations is a classic mathematics problem, and\ncommon in scientific researches and engineering applications. There are many\nnumeric, symbolic and numeric-symbolic methods of solving (real) solutions.\nUnlucky, these methods are constrained by some factors, e.g., high complexity,\nslow serial calculation, and the notorious intermediate expression expansion.\nEspecially when the count of variables is larger than six, the efficiency is\ndecreasing drastically. In this paper, according to the property of physical\nworld, we pay attention to nonlinear algebraic equations whose variables are in\nfixed constraints, and get meaningful real solutions. Combining with\nparallelism of GPGPU, we present an efficient algorithm, by searching the\nsolution space globally and solving the nonlinear algebraic equations with real\ninterval solutions. Furthermore, we realize the Hansen-Sengupta method on\nGPGPU. The experiments show that our method can solve many nonlinear algebraic\nequations, and the results are accurate and more efficient compared to\ntraditional serial methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.00330v1"
    },
    {
        "title": "L0TV: A Sparse Optimization Method for Impulse Noise Image Restoration",
        "authors": [
            "Ganzhao Yuan",
            "Bernard Ghanem"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Total Variation (TV) is an effective and popular prior model in the field of\nregularization-based image processing. This paper focuses on total variation\nfor removing impulse noise in image restoration. This type of noise frequently\narises in data acquisition and transmission due to many reasons, e.g. a faulty\nsensor or analog-to-digital converter errors. Removing this noise is an\nimportant task in image restoration. State-of-the-art methods such as Adaptive\nOutlier Pursuit(AOP) \\cite{yan2013restoration}, which is based on TV with\n$\\ell_{02}$-norm data fidelity, only give sub-optimal performance. In this\npaper, we propose a new sparse optimization method, called $\\ell_0TV$-PADMM,\nwhich solves the TV-based restoration problem with $\\ell_0$-norm data fidelity.\nTo effectively deal with the resulting non-convex non-smooth optimization\nproblem, we first reformulate it as an equivalent biconvex Mathematical Program\nwith Equilibrium Constraints (MPEC), and then solve it using a proximal\nAlternating Direction Method of Multipliers (PADMM). Our $\\ell_0TV$-PADMM\nmethod finds a desirable solution to the original $\\ell_0$-norm optimization\nproblem and is proven to be convergent under mild conditions. We apply\n$\\ell_0TV$-PADMM to the problems of image denoising and deblurring in the\npresence of impulse noise. Our extensive experiments demonstrate that\n$\\ell_0TV$-PADMM outperforms state-of-the-art image restoration methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.09879v2"
    },
    {
        "title": "High-dimension Tensor Completion via Gradient-based Optimization Under\n  Tensor-train Format",
        "authors": [
            "Longhao Yuan",
            "Qibin Zhao",
            "Lihua Gui",
            "Jianting Cao"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Tensor train (TT) decomposition has drawn people's attention due to its\npowerful representation ability and performance stability in high-order\ntensors. In this paper, we propose a novel approach to recover the missing\nentries of incomplete data represented by higher-order tensors. We attempt to\nfind the low-rank TT decomposition of the incomplete data which captures the\nlatent features of the whole data and then reconstruct the missing entries. By\napplying gradient descent algorithms, tensor completion problem is efficiently\nsolved by optimization models. We propose two TT-based algorithms: Tensor Train\nWeighted Optimization (TT-WOPT) and Tensor Train Stochastic Gradient Descent\n(TT-SGD) to optimize TT decomposition factors. In addition, a method named\nVisual Data Tensorization (VDT) is proposed to transform visual data into\nhigher-order tensors, resulting in the performance improvement of our\nalgorithms. The experiments in synthetic data and visual data show high\nefficiency and performance of our algorithms compared to the state-of-the-art\ncompletion algorithms, especially in high-order, high missing rate, and\nlarge-scale tensor completion situations.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01983v3"
    },
    {
        "title": "Polynomial data compression for large-scale physics experiments",
        "authors": [
            "Pierre Aubert",
            "Thomas Vuillaume",
            "Gilles Maurin",
            "Jean Jacquemier",
            "Giovanni Lamanna",
            "Nahid Emad"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  The new generation research experiments will introduce huge data surge to a\ncontinuously increasing data production by current experiments. This data surge\nnecessitates efficient compression techniques. These compression techniques\nmust guarantee an optimum tradeoff between compression rate and the\ncorresponding compression /decompression speed ratio without affecting the data\nintegrity.\n  This work presents a lossless compression algorithm to compress physics data\ngenerated by Astronomy, Astrophysics and Particle Physics experiments.\n  The developed algorithms have been tuned and tested on a real use case~: the\nnext generation ground-based high-energy gamma ray observatory, Cherenkov\nTelescope Array (CTA), requiring important compression performance.\nStand-alone, the proposed compression method is very fast and reasonably\nefficient. Alternatively, applied as pre-compression algorithm, it can\naccelerate common methods like LZMA, keeping close performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.01844v1"
    },
    {
        "title": "Accelerating Nonnegative Matrix Factorization Algorithms using\n  Extrapolation",
        "authors": [
            "Andersen Man Shun Ang",
            "Nicolas Gillis"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In this paper, we propose a general framework to accelerate significantly the\nalgorithms for nonnegative matrix factorization (NMF). This framework is\ninspired from the extrapolation scheme used to accelerate gradient methods in\nconvex optimization and from the method of parallel tangents. However, the use\nof extrapolation in the context of the two-block exact coordinate descent\nalgorithms tackling the non-convex NMF problems is novel. We illustrate the\nperformance of this approach on two state-of-the-art NMF algorithms, namely,\naccelerated hierarchical alternating least squares (A-HALS) and alternating\nnonnegative least squares (ANLS), using synthetic, image and document data\nsets.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06604v2"
    },
    {
        "title": "Accelerated PDE's for efficient solution of regularized inversion\n  problems",
        "authors": [
            "Minas Benyamin",
            "Jeff Calder",
            "Ganesh Sundaramoorthi",
            "Anthony Yezzi"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We further develop a new framework, called PDE Acceleration, by applying it\nto calculus of variations problems defined for general functions on\n$\\mathbb{R}^n$, obtaining efficient numerical algorithms to solve the resulting\nclass of optimization problems based on simple discretizations of their\ncorresponding accelerated PDE's. While the resulting family of PDE's and\nnumerical schemes are quite general, we give special attention to their\napplication for regularized inversion problems, with particular illustrative\nexamples on some popular image processing applications. The method is a\ngeneralization of momentum, or accelerated, gradient descent to the PDE\nsetting. For elliptic problems, the descent equations are a nonlinear damped\nwave equation, instead of a diffusion equation, and the acceleration is\nrealized as an improvement in the CFL condition from $\\Delta t\\sim \\Delta\nx^{2}$ (for diffusion) to $\\Delta t\\sim \\Delta x$ (for wave equations). We work\nout several explicit as well as a semi-implicit numerical schemes, together\nwith their necessary stability constraints, and include recursive update\nformulations which allow minimal-effort adaptation of existing gradient descent\nPDE codes into the accelerated PDE framework. We explore these schemes more\ncarefully for a broad class of regularized inversion applications, with special\nattention to quadratic, Beltrami, and Total Variation regularization, where the\naccelerated PDE takes the form of a nonlinear wave equation. Experimental\nexamples demonstrate the application of these schemes for image denoising,\ndeblurring, and inpainting, including comparisons against Primal Dual, Split\nBregman, and ADMM algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.00410v1"
    },
    {
        "title": "Find the dimension that counts: Fast dimension estimation and Krylov PCA",
        "authors": [
            "Shashanka Ubaru",
            "Abd-Krim Seghouane",
            "Yousef Saad"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  High dimensional data and systems with many degrees of freedom are often\ncharacterized by covariance matrices. In this paper, we consider the problem of\nsimultaneously estimating the dimension of the principal (dominant) subspace of\nthese covariance matrices and obtaining an approximation to the subspace. This\nproblem arises in the popular principal component analysis (PCA), and in many\napplications of machine learning, data analysis, signal and image processing,\nand others. We first present a novel method for estimating the dimension of the\nprincipal subspace. We then show how this method can be coupled with a Krylov\nsubspace method to simultaneously estimate the dimension and obtain an\napproximation to the subspace. The dimension estimation is achieved at no\nadditional cost. The proposed method operates on a model selection framework,\nwhere the novel selection criterion is derived based on random matrix\nperturbation theory ideas. We present theoretical analyses which (a) show that\nthe proposed method achieves strong consistency (i.e., yields optimal solution\nas the number of data-points $n\\rightarrow \\infty$), and (b) analyze conditions\nfor exact dimension estimation in the finite $n$ case. Using recent results, we\nshow that our algorithm also yields near optimal PCA. The proposed method\navoids forming the sample covariance matrix (associated with the data)\nexplicitly and computing the complete eigen-decomposition. Therefore, the\nmethod is inexpensive, which is particularly advantageous in modern data\napplications where the covariance matrices can be very large. Numerical\nexperiments illustrate the performance of the proposed method in various\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.03733v1"
    },
    {
        "title": "Eigenvalue Analysis via Kernel Density Estimation",
        "authors": [
            "Ahmed Yehia",
            "Mohamed Saleh"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  In this paper, we propose an eigenvalue analysis -- of system dynamics models\n-- based on the Mutual Information measure, which in turn will be estimated via\nthe Kernel Density Estimation method. We postulate that the proposed approach\nrepresents a novel and efficient multivariate eigenvalue sensitivity analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.06276v2"
    },
    {
        "title": "Deep Theory of Functional Connections: A New Method for Estimating the\n  Solutions of PDEs",
        "authors": [
            "Carl Leake"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  This article presents a new methodology called deep Theory of Functional\nConnections (TFC) that estimates the solutions of partial differential\nequations (PDEs) by combining neural networks with TFC. TFC is used to\ntransform PDEs with boundary conditions into unconstrained optimization\nproblems by embedding the boundary conditions into a \"constrained expression.\"\nIn this work, a neural network is chosen as the free function, and used to\nsolve the now unconstrained optimization problem. The loss function is taken as\nthe square of the residual of the PDE. Then, the neural network is trained in\nan unsupervised manner to solve the unconstrained optimization problem. This\nmethodology has two major differences when compared with popular methods used\nto estimate the solutions of PDEs. First, this methodology does not need to\ndiscretize the domain into a grid, rather, this methodology randomly samples\npoints from the domain during the training phase. Second, after training, this\nmethodology represents a closed form, analytical, differentiable approximation\nof the solution throughout the entire training domain. In contrast, other\npopular methods require interpolation if the estimated solution is desired at\npoints that do not lie on the discretized grid. The deep TFC method for\nestimating the solution of PDEs is demonstrated on four problems with a variety\nof boundary conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.08625v3"
    },
    {
        "title": "On the stability of the generalized, finite deformation correspondence\n  model of peridynamics",
        "authors": [
            "Masoud Behzadinasab",
            "John T. Foster"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  A class of peridynamic material models known as constitutive correspondence\nmodels provide a bridge between classical continuum mechanics and peridynamics.\nThese models are useful because they allow well-established local constitutive\ntheories to be used within the nonlocal framework of peridynamics. A recent\nfinite deformation correspondence theory (Foster and Xu, 2018) was developed\nand reported to improve stability properties of the original correspondence\nmodel (Silling et al., 2007). This paper presents a stability analysis that\nindicates the reported advantages of the new theory were overestimated.\nHomogeneous deformations are analyzed and shown to exibit unstable material\nbehavior at the continuum level. Additionally, the effects of a particle\ndiscretization on the stability of the model are reported. Numerical examples\ndemonstrate the large errors induced by the unstable behavior. Stabilization\nstrategies and practical applications of the new finite deformation model are\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.02937v1"
    },
    {
        "title": "A reduction methodology using free-free component eigenmodes and Arnoldi\n  enrichment",
        "authors": [
            "Hadrien Tournaire",
            "Franck Renaud",
            "Jean-Luc Dion"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  In order to perform faster simulations, the model reduction is nowadays used\nin industrial contexts to solve large and complex problems. However, the\nefficiency of such an approach is sometimes cut by the interface size of the\nreduced model and its reusability. In this article, we focus on the development\nof a reduction methodology for the build of modal analysis oriented and\nupdatable reduced order model whose size is not linked to their contacting\ninterface. In order to allow latter model readjusting, we impose the use of\neigenmodes in the reduction basis. Eventually, the method introduced is coupled\nto an Arnoldi based enrichment algorithm in order to improve the accuracy of\nthe reduced model produced. In the last section the proposed methodology is\ndiscussed and compared to the Craig and Bampton reduction method. During this\ncomparison we observed that even when not enriched, our work enables us to\nrecover the Craig and Bampton accuracy with partially updatable and smaller\nreduced order model.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.02943v1"
    },
    {
        "title": "Extended framework of Hamilton's principle applied to Duffing\n  oscillation",
        "authors": [
            "Jinkyu Kim",
            "Hyeonseok Lee",
            "Jinwon Shin"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  The paper begins with a novel variational formulation of Duffing equation\nusing the extended framework of Hamilton's principle (EHP). This formulation\nproperly accounts for initial conditions, and it recovers all the governing\ndifferential equations as its Euler-Lagrange equation. Thus, it provides\nelegant structure for the development of versatile temporal finite element\nmethods. Herein, the simplest temporal finite element method is presented by\nadopting linear temporal shape functions. Numerical examples are included to\nverify and investigate performance of non-iterative algorithm in the developed\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.06524v1"
    },
    {
        "title": "Efficient Nonlinear Fourier Transform Algorithms of Order Four on\n  Equispaced Grid",
        "authors": [
            "Vishal Vaibhav"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  We explore two classes of exponential integrators in this letter to design\nnonlinear Fourier transform (NFT) algorithms with a desired accuracy-complexity\ntrade-off and a convergence order of $4$ on an equispaced grid. The integrating\nfactor based method in the class of Runge-Kutta methods yield algorithms with\ncomplexity $O(N\\log^2N)$ (where $N$ is the number of samples of the signal)\nwhich have superior accuracy-complexity trade-off than any of the fast methods\nknown currently. The integrators based on Magnus series expansion, namely,\nstandard and commutator-free Magnus methods yield algorithms of complexity\n$O(N^2)$ that have superior error behavior even for moderately small step-sizes\nand higher signal strengths.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.11702v1"
    },
    {
        "title": "Parallel Algorithms for Constrained Tensor Factorization via the\n  Alternating Direction Method of Multipliers",
        "authors": [
            "Athanasios P. Liavas",
            "Nicholas D. Sidiropoulos"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Tensor factorization has proven useful in a wide range of applications, from\nsensor array processing to communications, speech and audio signal processing,\nand machine learning. With few recent exceptions, all tensor factorization\nalgorithms were originally developed for centralized, in-memory computation on\na single machine; and the few that break away from this mold do not easily\nincorporate practically important constraints, such as nonnegativity. A new\nconstrained tensor factorization framework is proposed in this paper, building\nupon the Alternating Direction method of Multipliers (ADMoM). It is shown that\nthis simplifies computations, bypassing the need to solve constrained\noptimization problems in each iteration; and it naturally leads to distributed\nalgorithms suitable for parallel implementation on regular high-performance\ncomputing (e.g., mesh) architectures. This opens the door for many emerging big\ndata-enabled applications. The methodology is exemplified using nonnegativity\nas a baseline constraint, but the proposed framework can more-or-less readily\nincorporate many other types of constraints. Numerical experiments are very\nencouraging, indicating that the ADMoM-based nonnegative tensor factorization\n(NTF) has high potential as an alternative to state-of-the-art approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.2383v2"
    },
    {
        "title": "A theoretical contribution to the fast implementation of null linear\n  discriminant analysis method using random matrix multiplication with scatter\n  matrices",
        "authors": [
            "Ting-ting Feng",
            "Gang Wu"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  The null linear discriminant analysis method is a competitive approach for\ndimensionality reduction. The implementation of this method, however, is\ncomputationally expensive. Recently, a fast implementation of null linear\ndiscriminant analysis method using random matrix multiplication with scatter\nmatrices was proposed. However, if the random matrix is chosen arbitrarily, the\norientation matrix may be rank deficient, and some useful discriminant\ninformation will be lost. In this paper, we investigate how to choose the\nrandom matrix properly, such that the two criteria of the null LDA method are\nsatisfied theoretically. We give a necessary and sufficient condition to\nguarantee full column rank of the orientation matrix. Moreover, the geometric\ncharacterization of the condition is also described.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.2579v1"
    },
    {
        "title": "Stochastic Subgradient Algorithms for Strongly Convex Optimization over\n  Distributed Networks",
        "authors": [
            "N. Denizcan Vanli",
            "Muhammed O. Sayin",
            "Suleyman S. Kozat"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  We study diffusion and consensus based optimization of a sum of unknown\nconvex objective functions over distributed networks. The only access to these\nfunctions is through stochastic gradient oracles, each of which is only\navailable at a different node, and a limited number of gradient oracle calls is\nallowed at each node. In this framework, we introduce a convex optimization\nalgorithm based on the stochastic gradient descent (SGD) updates. Particularly,\nwe use a carefully designed time-dependent weighted averaging of the SGD\niterates, which yields a convergence rate of\n$O\\left(\\frac{N\\sqrt{N}}{T}\\right)$ after $T$ gradient updates for each node on\na network of $N$ nodes. We then show that after $T$ gradient oracle calls, the\naverage SGD iterate achieves a mean square deviation (MSD) of\n$O\\left(\\frac{\\sqrt{N}}{T}\\right)$. This rate of convergence is optimal as it\nmatches the performance lower bound up to constant terms. Similar to the SGD\nalgorithm, the computational complexity of the proposed algorithm also scales\nlinearly with the dimensionality of the data. Furthermore, the communication\nload of the proposed method is the same as the communication load of the SGD\nalgorithm. Thus, the proposed algorithm is highly efficient in terms of\ncomplexity and communication load. We illustrate the merits of the algorithm\nwith respect to the state-of-art methods over benchmark real life data sets and\nwidely studied network topologies.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.8277v2"
    },
    {
        "title": "Robust solutions of uncertain mixed-integer linear programs using\n  decomposition techniques",
        "authors": [
            "Roberto Mínguez",
            "Víctor Casero-Alonso"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Robust optimization is a framework for modeling optimization problems\ninvolving data uncertainty and during the last decades has been an area of\nactive research. If we focus on linear programming (LP) problems with i)\nuncertain data, ii) binary decisions and iii) hard constraints within an\nellipsoidal uncertainty set, this paper provides a different interpretation of\ntheir robust counterpart (RC) inspired from decomposition techniques. This new\ninterpretation allows the proposal of an ad-hoc decomposition technique to\nsolve the RC problem with the following advantages: i) it improves\ntractability, specially for large-scale problems, and ii) it provides the exact\nprobability of constraint violation in case the probability distribution of\nuncertain parameters are completely defined by using first and second-order\nprobability moments. An attractive aspect of our method is that it decomposes\nthe second-order cone programming problem, associated with the robust\ncounterpart, into a linear master problem and different quadratically\nconstrained problems (QCP) of considerable lower size. The optimal solution is\nachieved through the solution of these master and subproblems within an\niterative scheme based on cutting plane approximations of the second-order cone\nconstraints. In addition, proof of convergence of the iterative method is\ngiven.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.8593v4"
    },
    {
        "title": "On GROUSE and Incremental SVD",
        "authors": [
            "Laura Balzano",
            "Stephen J. Wright"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an incremental\nalgorithm for identifying a subspace of Rn from a sequence of vectors in this\nsubspace, where only a subset of components of each vector is revealed at each\niteration. Recent analysis has shown that GROUSE converges locally at an\nexpected linear rate, under certain assumptions. GROUSE has a similar flavor to\nthe incremental singular value decomposition algorithm, which updates the SVD\nof a matrix following addition of a single column. In this paper, we modify the\nincremental SVD approach to handle missing data, and demonstrate that this\nmodified approach is equivalent to GROUSE, for a certain choice of an\nalgorithmic parameter.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.5494v1"
    },
    {
        "title": "A space-time parallel solver for the three-dimensional heat equation",
        "authors": [
            "Robert Speck",
            "Daniel Ruprecht",
            "Matthew Emmett",
            "Matthias Bolten",
            "Rolf Krause"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The paper presents a combination of the time-parallel \"parallel full\napproximation scheme in space and time\" (PFASST) with a parallel multigrid\nmethod (PMG) in space, resulting in a mesh-based solver for the\nthree-dimensional heat equation with a uniquely high degree of efficient\nconcurrency. Parallel scaling tests are reported on the Cray XE6 machine \"Monte\nRosa\" on up to 16,384 cores and on the IBM Blue Gene/Q system \"JUQUEEN\" on up\nto 65,536 cores. The efficacy of the combined spatial- and temporal\nparallelization is shown by demonstrating that using PFASST in addition to PMG\nsignificantly extends the strong-scaling limit. Implications of using spatial\ncoarsening strategies in PFASST's multi-level hierarchy in large-scale parallel\nsimulations are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.7867v2"
    },
    {
        "title": "Guaranteed upper-lower bounds on homogenized properties by FFT-based\n  Galerkin method",
        "authors": [
            "Jaroslav Vondřejc",
            "Jan Zeman",
            "Ivo Marek"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  Guaranteed upper-lower bounds on homogenized coefficients, arising from the\nperiodic cell problem, are calculated in a scalar elliptic setting. Our\napproach builds on the recent variational reformulation of the Moulinec-Suquet\n(1994) Fast Fourier Transform (FFT) homogenization scheme by Vond\\v{r}ejc et\nal. (2014), which is based on the conforming Galerkin approximation with\ntrigonometric polynomials. Upper-lower bounds are obtained by adjusting the\nprimal-dual finite element framework developed independently by Dvo\\v{r}\\'{a}k\n(1993) and Wieckowski (1995) to the FFT-based Galerkin setting. We show that\nthe discretization procedure differs for odd and non-odd number of grid points.\nThanks to the Helmholtz decomposition inherited from the continuous\nformulation, the duality structure is fully preserved for the odd\ndiscretizations. In the latter case, a more complex primal-dual structure is\nobserved due to presence of the trigonometric polynomials associated with the\nNyquist frequencies. These theoretical findings are confirmed with numerical\nexamples. To conclude, the main advantage of the FFT-based approach over\nconventional finite-element schemes is that the primal and the dual problems\nare treated on the same basis, and this property can be extended beyond the\nscalar elliptic setting.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.3614v3"
    },
    {
        "title": "A Revised Scheme to Compute Horizontal Covariances in an Oceanographic\n  3D-VAR Assimilation System",
        "authors": [
            "R. Farina",
            "S. Dobricic",
            "A. Storto",
            "S. Masina",
            "S. Cuomo"
        ],
        "category": "cs.NA",
        "published_year": "2014",
        "summary": "  We propose an improvement of an oceanographic three dimensional variational\nassimilation scheme (3D-VAR), named OceanVar, by introducing a recursive filter\n(RF) with the third order of accuracy (3rd-RF), instead of a RF with first\norder of accuracy (1st-RF), to approximate horizontal Gaussian covariances. An\nadvantage of the proposed scheme is that the CPU's time can be substantially\nreduced with benefits on the large scale applications. Experiments estimating\nthe impact of 3rd-RF are performed by assimilating oceanographic data in two\nrealistic oceanographic applications. The results evince benefits in terms of\nassimilation process computational time, accuracy of the Gaussian correlation\nmodeling, and show that the 3rd-RF is a suitable tool for operational data\nassimilation.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.5756v1"
    },
    {
        "title": "Riemannian preconditioning for tensor completion",
        "authors": [
            "Hiroyuki Kasai",
            "Bamdev Mishra"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  We propose a novel Riemannian preconditioning approach for the tensor\ncompletion problem with rank constraint. A Riemannian metric or inner product\nis proposed that exploits the least-squares structure of the cost function and\ntakes into account the structured symmetry in Tucker decomposition. The\nspecific metric allows to use the versatile framework of Riemannian\noptimization on quotient manifolds to develop a preconditioned nonlinear\nconjugate gradient algorithm for the problem. To this end, concrete matrix\nrepresentations of various optimization-related ingredients are listed.\nNumerical comparisons suggest that our proposed algorithm robustly outperforms\nstate-of-the-art algorithms across different problem instances encompassing\nvarious synthetic and real-world datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.02159v2"
    },
    {
        "title": "A consistent solution of the reinitialization equation in the\n  conservative level-set method",
        "authors": [
            "Tomasz Waclawczyk"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this paper, a new re-initialization method for the conservative level-set\nfunction is put forward. First, it has been shown that the re-initialization\nand advection equations of the conservative level-set function are\nmathematically equivalent to the re-initialization and advection equations of\nthe localized signed distance function. Next, a new discretization for the\nspatial derivatives of the conservative level-set function has been proposed.\nThis new discretization is consistent with the re-initialization procedure and\nit guarantees a second-order convergence rate of the interface curvature on\ngradually refined grids. The new re-initialization method does not introduce\nartificial deformations to stationary and non-stationary interfaces, even when\nthe number of re-initialization steps is large.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.04268v1"
    },
    {
        "title": "Task-based adaptive multiresolution for time-space multi-scale\n  reaction-diffusion systems on multi-core architectures",
        "authors": [
            "Stéphane Descombes",
            "Max Duarte",
            "Thierry Dumont",
            "Thomas Guillet",
            "Violaine Louvet",
            "Marc Massot"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  A new solver featuring time-space adaptation and error control has been\nrecently introduced to tackle the numerical solution of stiff\nreaction-diffusion systems. Based on operator splitting, finite volume adaptive\nmultiresolution and high order time integrators with specific stability\nproperties for each operator, this strategy yields high computational\nefficiency for large multidimensional computations on standard architectures\nsuch as powerful workstations. However, the data structure of the original\nimplementation, based on trees of pointers, provides limited opportunities for\nefficiency enhancements, while posing serious challenges in terms of parallel\nprogramming and load balancing. The present contribution proposes a new\nimplementation of the whole set of numerical methods including Radau5 and\nROCK4, relying on a fully different data structure together with the use of a\nspecific library, TBB, for shared-memory, task-based parallelism with\nwork-stealing. The performance of our implementation is assessed in a series of\ntest-cases of increasing difficulty in two and three dimensions on multi-core\nand many-core architectures, demonstrating high scalability.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.04651v3"
    },
    {
        "title": "Global Convergence of a Grassmannian Gradient Descent Algorithm for\n  Subspace Estimation",
        "authors": [
            "Dejiao Zhang",
            "Laura Balzano"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  It has been observed in a variety of contexts that gradient descent methods\nhave great success in solving low-rank matrix factorization problems, despite\nthe relevant problem formulation being non-convex. We tackle a particular\ninstance of this scenario, where we seek the $d$-dimensional subspace spanned\nby a streaming data matrix. We apply the natural first order incremental\ngradient descent method, constraining the gradient method to the Grassmannian.\nIn this paper, we propose an adaptive step size scheme that is greedy for the\nnoiseless case, that maximizes the improvement of our metric of convergence at\neach data index $t$, and yields an expected improvement for the noisy case. We\nshow that, with noise-free data, this method converges from any random\ninitialization to the global minimum of the problem. For noisy data, we provide\nthe expected convergence rate of the proposed algorithm per iteration.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.07405v3"
    },
    {
        "title": "Global Optimality in Tensor Factorization, Deep Learning, and Beyond",
        "authors": [
            "Benjamin D. Haeffele",
            "Rene Vidal"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Techniques involving factorization are found in a wide range of applications\nand have enjoyed significant empirical success in many fields. However, common\nto a vast majority of these problems is the significant disadvantage that the\nassociated optimization problems are typically non-convex due to a multilinear\nform or other convexity destroying transformation. Here we build on ideas from\nconvex relaxations of matrix factorizations and present a very general\nframework which allows for the analysis of a wide range of non-convex\nfactorization problems - including matrix factorization, tensor factorization,\nand deep neural network training formulations. We derive sufficient conditions\nto guarantee that a local minimum of the non-convex optimization problem is a\nglobal minimum and show that if the size of the factorized variables is large\nenough then from any initialization it is possible to find a global minimizer\nusing a purely local descent algorithm. Our framework also provides a partial\ntheoretical justification for the increasingly common use of Rectified Linear\nUnits (ReLUs) in deep neural networks and offers guidance on deep network\narchitectures and regularization strategies to facilitate efficient\noptimization.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.07540v1"
    },
    {
        "title": "Large-scale Optimization-based Non-negative Computational Framework for\n  Diffusion Equations: Parallel Implementation and Performance Studies",
        "authors": [
            "J. Chang",
            "S. Karra",
            "K. B. Nakshatrala"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  It is well-known that the standard Galerkin formulation, which is often the\nformulation of choice under the finite element method for solving self-adjoint\ndiffusion equations, does not meet maximum principles and the non-negative\nconstraint for anisotropic diffusion equations. Recently, optimization-based\nmethodologies that satisfy maximum principles and the non-negative constraint\nfor steady-state and transient diffusion-type equations have been proposed. To\ndate, these methodologies have been tested only on small-scale academic\nproblems. The purpose of this paper is to systematically study the performance\nof the non-negative methodology in the context of high performance computing\n(HPC). PETSc and TAO libraries are, respectively, used for the parallel\nenvironment and optimization solvers. For large-scale problems, it is important\nfor computational scientists to understand the computational performance of\ncurrent algorithms available in these scientific libraries. The numerical\nexperiments are conducted on the state-of-the-art HPC systems, and a\nsingle-core performance model is used to better characterize the efficiency of\nthe solvers. Our studies indicate that the proposed non-negative computational\nframework for diffusion-type equations exhibits excellent strong scaling for\nreal-world large-scale problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.08435v3"
    },
    {
        "title": "Fast Low-Rank Matrix Learning with Nonconvex Regularization",
        "authors": [
            "Quanming Yao",
            "James T. Kwok",
            "Wenliang Zhong"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Low-rank modeling has a lot of important applications in machine learning,\ncomputer vision and social network analysis. While the matrix rank is often\napproximated by the convex nuclear norm, the use of nonconvex low-rank\nregularizers has demonstrated better recovery performance. However, the\nresultant optimization problem is much more challenging. A very recent\nstate-of-the-art is based on the proximal gradient algorithm. However, it\nrequires an expensive full SVD in each proximal step. In this paper, we show\nthat for many commonly-used nonconvex low-rank regularizers, a cutoff can be\nderived to automatically threshold the singular values obtained from the\nproximal operator. This allows the use of power method to approximate the SVD\nefficiently. Besides, the proximal operator can be reduced to that of a much\nsmaller matrix projected onto this leading subspace. Convergence, with a rate\nof O(1/T) where T is the number of iterations, can be guaranteed. Extensive\nexperiments are performed on matrix completion and robust principal component\nanalysis. The proposed method achieves significant speedup over the\nstate-of-the-art. Moreover, the matrix solution obtained is more accurate and\nhas a lower rank than that of the traditional nuclear norm regularizer.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.00984v1"
    },
    {
        "title": "Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization",
        "authors": [
            "Yang Song",
            "Jun Zhu"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  Bayesian matrix completion has been studied based on a low-rank matrix\nfactorization formulation with promising results. However, little work has been\ndone on Bayesian matrix completion based on the more direct spectral\nregularization formulation. We fill this gap by presenting a novel Bayesian\nmatrix completion method based on spectral regularization. In order to\ncircumvent the difficulties of dealing with the orthonormality constraints of\nsingular vectors, we derive a new equivalent form with relaxed constraints,\nwhich then leads us to design an adaptive version of spectral regularization\nfeasible for Bayesian inference. Our Bayesian method requires no parameter\ntuning and can infer the number of latent factors automatically. Experiments on\nsynthetic and real datasets demonstrate encouraging results on rank recovery\nand collaborative filtering, with notably good results for very sparse\nmatrices.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.01110v2"
    },
    {
        "title": "Fast Optimization Algorithm on Riemannian Manifolds and Its Application\n  in Low-Rank Representation",
        "authors": [
            "Haoran Chen",
            "Yanfeng Sun",
            "Junbin Gao",
            "Yongli Hu"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  The paper addresses the problem of optimizing a class of composite functions\non Riemannian manifolds and a new first order optimization algorithm (FOA) with\na fast convergence rate is proposed. Through the theoretical analysis for FOA,\nit has been proved that the algorithm has quadratic convergence. The\nexperiments in the matrix completion task show that FOA has better performance\nthan other first order optimization methods on Riemannian manifolds. A fast\nsubspace pursuit method based on FOA is proposed to solve the low-rank\nrepresentation model based on augmented Lagrange method on the low rank matrix\nvariety. Experimental results on synthetic and real data sets are presented to\ndemonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance in\nterms of faster convergence and higher accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.01927v1"
    },
    {
        "title": "Robust method for finding sparse solutions to linear inverse problems\n  using an L2 regularization",
        "authors": [
            "Gonzalo H Otazu"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We analyzed the performance of a biologically inspired algorithm called the\nCorrected Projections Algorithm (CPA) when a sparseness constraint is required\nto unambiguously reconstruct an observed signal using atoms from an\novercomplete dictionary. By changing the geometry of the estimation problem,\nCPA gives an analytical expression for a binary variable that indicates the\npresence or absence of a dictionary atom using an L2 regularizer. The\nregularized solution can be implemented using an efficient real-time\nKalman-filter type of algorithm. The smoother L2 regularization of CPA makes it\nvery robust to noise, and CPA outperforms other methods in identifying known\natoms in the presence of strong novel atoms in the signal.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.00573v3"
    },
    {
        "title": "Efficient coordinate-wise leading eigenvector computation",
        "authors": [
            "Jialei Wang",
            "Weiran Wang",
            "Dan Garber",
            "Nathan Srebro"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  We develop and analyze efficient \"coordinate-wise\" methods for finding the\nleading eigenvector, where each step involves only a vector-vector product. We\nestablish global convergence with overall runtime guarantees that are at least\nas good as Lanczos's method and dominate it for slowly decaying spectrum. Our\nmethods are based on combining a shift-and-invert approach with coordinate-wise\nalgorithms for linear regression.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.07834v1"
    },
    {
        "title": "CTD: Fast, Accurate, and Interpretable Method for Static and Dynamic\n  Tensor Decompositions",
        "authors": [
            "Jungwoo Lee",
            "Dongjin Choi",
            "Lee Sael"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  How can we find patterns and anomalies in a tensor, or multi-dimensional\narray, in an efficient and directly interpretable way? How can we do this in an\nonline environment, where a new tensor arrives each time step? Finding patterns\nand anomalies in a tensor is a crucial problem with many applications,\nincluding building safety monitoring, patient health monitoring, cyber\nsecurity, terrorist detection, and fake user detection in social networks.\nStandard PARAFAC and Tucker decomposition results are not directly\ninterpretable. Although a few sampling-based methods have previously been\nproposed towards better interpretability, they need to be made faster, more\nmemory efficient, and more accurate.\n  In this paper, we propose CTD, a fast, accurate, and directly interpretable\ntensor decomposition method based on sampling. CTD-S, the static version of\nCTD, provably guarantees a high accuracy that is 17 ~ 83x more accurate than\nthat of the state-of-the-art method. Also, CTD-S is made 5 ~ 86x faster, and 7\n~ 12x more memory-efficient than the state-of-the-art method by removing\nredundancy. CTD-D, the dynamic version of CTD, is the first interpretable\ndynamic tensor decomposition method ever proposed. Also, it is made 2 ~ 3x\nfaster than already fast CTD-S by exploiting factors at previous time step and\nby reordering operations. With CTD, we demonstrate how the results can be\neffectively interpreted in the online distributed denial of service (DDoS)\nattack detection.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.03608v1"
    },
    {
        "title": "Stochastic variance reduced multiplicative update for nonnegative matrix\n  factorization",
        "authors": [
            "Hiroyuki Kasai"
        ],
        "category": "cs.NA",
        "published_year": "2017",
        "summary": "  Nonnegative matrix factorization (NMF), a dimensionality reduction and factor\nanalysis method, is a special case in which factor matrices have low-rank\nnonnegative constraints. Considering the stochastic learning in NMF, we\nspecifically address the multiplicative update (MU) rule, which is the most\npopular, but which has slow convergence property. This present paper introduces\non the stochastic MU rule a variance-reduced technique of stochastic gradient.\nNumerical comparisons suggest that our proposed algorithms robustly outperform\nstate-of-the-art algorithms across different synthetic and real-world datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.10781v2"
    },
    {
        "title": "Tensor Decompositions: A New Concept in Brain Data Analysis?",
        "authors": [
            "Andrzej Cichocki"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  Matrix factorizations and their extensions to tensor factorizations and\ndecompositions have become prominent techniques for linear and multilinear\nblind source separation (BSS), especially multiway Independent Component\nAnalysis (ICA), NonnegativeMatrix and Tensor Factorization (NMF/NTF), Smooth\nComponent Analysis (SmoCA) and Sparse Component Analysis (SCA). Moreover,\ntensor decompositions have many other potential applications beyond multilinear\nBSS, especially feature extraction, classification, dimensionality reduction\nand multiway clustering. In this paper, we briefly overview new and emerging\nmodels and approaches for tensor decompositions in applications to group and\nlinked multiway BSS/ICA, feature extraction, classification andMultiway Partial\nLeast Squares (MPLS) regression problems. Keywords: Multilinear BSS, linked\nmultiway BSS/ICA, tensor factorizations and decompositions, constrained Tucker\nand CP models, Penalized Tensor Decompositions (PTD), feature extraction,\nclassification, multiway PLS and CCA.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.0395v1"
    },
    {
        "title": "A fast randomized Kaczmarz algorithm for sparse solutions of consistent\n  linear systems",
        "authors": [
            "Hassan Mansour",
            "Ozgur Yilmaz"
        ],
        "category": "cs.NA",
        "published_year": "2013",
        "summary": "  The Kaczmarz algorithm is a popular solver for overdetermined linear systems\ndue to its simplicity and speed. In this paper, we propose a modification that\nspeeds up the convergence of the randomized Kaczmarz algorithm for systems of\nlinear equations with sparse solutions. The speedup is achieved by projecting\nevery iterate onto a weighted row of the linear system while maintaining the\nrandom row selection criteria of Strohmer and Vershynin. The weights are chosen\nto attenuate the contribution of row elements that lie outside of the estimated\nsupport of the sparse solution. While the Kaczmarz algorithm and its variants\ncan only find solutions to overdetermined linear systems, our algorithm\nsurprisingly succeeds in finding sparse solutions to underdetermined linear\nsystems as well. We present empirical studies which demonstrate the\nacceleration in convergence to the sparse solution using this modified approach\nin the overdetermined case. We also demonstrate the sparse recovery\ncapabilities of our approach in the underdetermined case and compare the\nperformance with that of $\\ell_1$ minimization.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.3803v1"
    },
    {
        "title": "Spectral Statistics of Lattice Graph Percolation Models",
        "authors": [
            "Stephen Kruzick",
            "Jose M. F. Moura"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  In graph signal processing, the graph adjacency matrix or the graph Laplacian\ncommonly define the shift operator. The spectral decomposition of the shift\noperator plays an important role in that the eigenvalues represent frequencies\nand the eigenvectors provide a spectral basis. This is useful, for example, in\nthe design of filters. However, the graph or network may be uncertain due to\nstochastic influences in construction and maintenance, and, under such\nconditions, the eigenvalues of the shift matrix become random variables. This\npaper examines the spectral distribution of the eigenvalues of random networks\nformed by including each link of a D-dimensional lattice supergraph\nindependently with identical probability, a percolation model. Using the\nstochastic canonical equation methods developed by Girko for symmetric matrices\nwith independent upper triangular entries, a deterministic distribution is\nfound that asymptotically approximates the empirical spectral distribution of\nthe scaled adjacency matrix for a model with arbitrary parameters. The main\nresults characterize the form of the solution to an important system of\nequations that leads to this deterministic distribution function and\nsignificantly reduce the number of equations that must be solved to find the\nsolution for a given set of model parameters. Simulations comparing the\nexpected empirical spectral distributions and the computed deterministic\ndistributions are provided for sample parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.02655v1"
    },
    {
        "title": "Deterministic and Probabilistic Conditions for Finite Completability of\n  Low-Tucker-Rank Tensor",
        "authors": [
            "Morteza Ashraphijuo",
            "Vaneet Aggarwal",
            "Xiaodong Wang"
        ],
        "category": "cs.NA",
        "published_year": "2016",
        "summary": "  We investigate the fundamental conditions on the sampling pattern, i.e.,\nlocations of the sampled entries, for finite completability of a low-rank\ntensor given some components of its Tucker rank. In order to find the\ndeterministic necessary and sufficient conditions, we propose an algebraic\ngeometric analysis on the Tucker manifold, which allows us to incorporate\nmultiple rank components in the proposed analysis in contrast with the\nconventional geometric approaches on the Grassmannian manifold. This analysis\ncharacterizes the algebraic independence of a set of polynomials defined based\non the sampling pattern, which is closely related to finite completion.\nProbabilistic conditions are then studied and a lower bound on the sampling\nprobability is given, which guarantees that the proposed deterministic\nconditions on the sampling patterns for finite completability hold with high\nprobability. Furthermore, using the proposed geometric approach for finite\ncompletability, we propose a sufficient condition on the sampling pattern that\nensures there exists exactly one completion for the sampled tensor.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.01597v3"
    },
    {
        "title": "A Projection Method for Metric-Constrained Optimization",
        "authors": [
            "Nate Veldt",
            "David Gleich",
            "Anthony Wirth",
            "James Saunderson"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We outline a new approach for solving optimization problems which enforce\ntriangle inequalities on output variables. We refer to this as\nmetric-constrained optimization, and give several examples where problems of\nthis form arise in machine learning applications and theoretical approximation\nalgorithms for graph clustering. Although these problem are interesting from a\ntheoretical perspective, they are challenging to solve in practice due to the\nhigh memory requirement of black-box solvers. In order to address this\nchallenge we first prove that the metric-constrained linear program relaxation\nof correlation clustering is equivalent to a special case of the metric\nnearness problem. We then developed a general solver for metric-constrained\nlinear and quadratic programs by generalizing and improving a simple projection\nalgorithm originally developed for metric nearness. We give several novel\napproximation guarantees for using our framework to find lower bounds for\noptimal solutions to several challenging graph clustering problems. We also\ndemonstrate the power of our framework by solving optimizing problems involving\nup to 10^{8} variables and 10^{11} constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01678v1"
    },
    {
        "title": "Convergence study and optimal weight functions of an explicit particle\n  method for the incompressible Navier--Stokes equations",
        "authors": [
            "Y. Imoto",
            "S. Tsuzuki",
            "D. Nishiura"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  To increase the reliability of simulations by particle methods for\nincompressible viscous flow problems, convergence studies and improvements of\naccuracy are considered for a fully explicit particle method for incompressible\nNavier--Stokes equations. The explicit particle method is based on a penalty\nproblem, which converges theoretically to the incompressible Navier--Stokes\nequations, and is discretized in space by generalized approximate operators\ndefined as a wider class of approximate operators than those of the smoothed\nparticle hydrodynamics (SPH) and moving particle semi-implicit (MPS) methods.\nBy considering an analytical derivation of the explicit particle method and\ntruncation error estimates of the generalized approximate operators, sufficient\nconditions of convergence are conjectured.Under these conditions, the\nconvergence of the explicit particle method is confirmed by numerically\ncomparing errors between exact and approximate solutions. Moreover, by focusing\non the truncation errors of the generalized approximate operators, an optimal\nweight function is derived by reducing the truncation errors over general\nparticle distributions. The effectiveness of the generalized approximate\noperators with the optimal weight functions is confirmed using numerical\nresults of truncation errors and driven cavity flow. As an application for flow\nproblems with free surface effects, the explicit particle method is applied to\na dam break flow.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.00867v2"
    },
    {
        "title": "A Dual Symmetric Gauss-Seidel Alternating Direction Method of\n  Multipliers for Hyperspectral Sparse Unmixing",
        "authors": [
            "Longfei Ren",
            "Chengjing Wang",
            "Peipei Tang",
            "Zheng Ma"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  Since sparse unmixing has emerged as a promising approach to hyperspectral\nunmixing, some spatial-contextual information in the hyperspectral images has\nbeen exploited to improve the performance of the unmixing recently. The total\nvariation (TV) has been widely used to promote the spatial homogeneity as well\nas the smoothness between adjacent pixels. However, the computation task for\nhyperspectral sparse unmixing with a TV regularization term is heavy. Besides,\nthe convergence of the primal alternating direction method of multipliers\n(ADMM) for the hyperspectral sparse unmixing with a TV regularization term has\nnot been explained in details. In this paper, we design an efficient and\nconvergent dual symmetric Gauss-Seidel ADMM (sGS-ADMM) for hyperspectral sparse\nunmixing with a TV regularization term. We also present the global convergence\nand local linear convergence rate analysis for this algorithm. As demonstrated\nin numerical experiments, our algorithm can obviously improve the efficiency of\nthe unmixing compared with the state-of-the-art algorithm. More importantly, we\ncan obtain images with higher quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.09135v2"
    },
    {
        "title": "An Efficient and high accuracy finite-difference scheme for the acoustic\n  wave equation in 3D heterogeneous media",
        "authors": [
            "Keran Li",
            "Wenyuan Liao"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  Efficient and accurate numerical simulation of 3D acoustic wave propagation\nin heterogeneous media plays an important role in the success of seismic full\nwaveform inversion (FWI) problem. In this work, we employed the combined scheme\nand developed a new explicit compact high-order finite difference scheme to\nsolve the 3D acoustic wave equation with spatially variable acoustic velocity.\nThe boundary conditions for the second derivatives of spatial variables have\nbeen derived by using the equation itself and the boundary condition for $u$.\nTheoretical analysis shows that the new scheme has an accuracy order of\n$O(\\tau^2) + O(h^4)$, where $\\tau$ is the time step and $h$ is the grid size.\nCombined with Richardson extrapolation or Runge-Kutta method, the new method\ncan be improved to 4th-order in time. Three numerical experiments are conducted\nto validate the efficiency and accuracy of the new scheme. The stability of the\nnew scheme has been proved by an energy method, which shows that the new scheme\nis conditionally stable with a Courant - Friedrichs - Lewy (CFL) number which\nis slightly lower than that of the Pad\\'{e} approximation based method.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03831v1"
    },
    {
        "title": "A Discrete Empirical Interpolation Method for Interpretable Immersion\n  and Embedding of Nonlinear Manifolds",
        "authors": [
            "Samuel E. Otto",
            "Clarence W. Rowley"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  Manifold learning techniques seek to discover structure-preserving mappings\nof high-dimensional data into low-dimensional spaces.\n  While the new sets of coordinates specified by these mappings can closely\nparameterize the data, they are generally complicated nonlinear functions of\nthe original variables. This makes them difficult to interpret physically.\n  Furthermore, in data-driven model reduction applications the governing\nequations may have structure that is destroyed by nonlinear mapping into\ncoordinates on an inertial manifold, creating a computational bottleneck for\nsimulations.\n  Instead, we propose to identify a small collection of the original variables\nwhich are capable of uniquely determining all others either locally via\nimmersion or globally via embedding of the underlying manifold.\n  When the data lies on a low-dimensional subspace the existing discrete\nempirical interpolation method (DEIM) accomplishes this with recent variants\nemploying greedy algorithms based on pivoted QR (PQR) factorizations.\n  However, low-dimensional manifolds coming from a variety of applications,\nparticularly from advection-dominated PDEs, do not lie in or near any\nlow-dimensional subspace.\n  Our proposed approach extends DEIM to data lying near nonlinear manifolds by\napplying a similar pivoted QR procedure simultaneously on collections of\npatches making up locally linear approximations of the manifold, resulting in a\nnovel simultaneously pivoted QR (SimPQR) algorithm.\n  The immersion provided by SimPQR can be extended to an embedding by applying\nSimPQR a second time to a modified collection of vectors.\n  The SimPQR method for computing these `nonlinear DEIM' (NLDEIM) coordinates\nis successfully applied to real-world data lying near an inertial manifold in a\ncylinder wake flow as well as data coming from a viscous Burgers equation with\ndifferent initial conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.07619v2"
    },
    {
        "title": "User-Device Authentication in Mobile Banking using APHEN for Paratuck2\n  Tensor Decomposition",
        "authors": [
            "Jeremy Charlier",
            "Eric Falk",
            "Radu State",
            "Jean Hilger"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  The new financial European regulations such as PSD2 are changing the retail\nbanking services. Noticeably, the monitoring of the personal expenses is now\nopened to other institutions than retail banks. Nonetheless, the retail banks\nare looking to leverage the user-device authentication on the mobile banking\napplications to enhance the personal financial advertisement. To address the\nprofiling of the authentication, we rely on tensor decomposition, a higher\ndimensional analogue of matrix decomposition. We use Paratuck2, which expresses\na tensor as a multiplication of matrices and diagonal tensors, because of the\nimbalance between the number of users and devices. We highlight why Paratuck2\nis more appropriate in this case than the popular CP tensor decomposition,\nwhich decomposes a tensor as a sum of rank-one tensors. However, the\ncomputation of Paratuck2 is computational intensive. We propose a new\nAPproximate HEssian-based Newton resolution algorithm, APHEN, capable of\nsolving Paratuck2 more accurately and faster than the other popular approaches\nbased on alternating least square or gradient descent. The results of Paratuck2\nare used for the predictions of users' authentication with neural networks. We\napply our method for the concrete case of targeting clients for financial\nadvertising campaigns based on the authentication events generated by mobile\nbanking applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.10363v1"
    },
    {
        "title": "Accuracy of unperturbed motion of particles in a gyrokinetic\n  semi-Lagrangian code",
        "authors": [
            "Guillaume Latu",
            "Virginie Grandgirard",
            "Jérémie Abiteboul",
            "Morgane Bergot",
            "Nicolas Crouseilles",
            "Xavier Garbet",
            "Philippe Ghendrih",
            "Michel Mehrenberger",
            "Yanick Sarazin",
            "Hocine Sellama",
            "Eric Sonnendrücker",
            "David Zarzoso"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Inaccurate description of the equilibrium can yield to spurious effects in\ngyrokinetic turbulence simulations. Also, the Vlasov solver and time\nintegration schemes impact the conservation of physical quantities, especially\nin long-term simulations. Equilibrium and Vlasov solver have to be tuned in\norder to preserve constant states (equilibrium) and to provide good\nconservation property along time (mass to begin with). Several illustrative\nsimple test cases are given to show typical spurious effects that one can\nobserves for poor settings. We explain why Forward Semi-Lagrangian scheme bring\nus some benefits. Some toroidal and cylindrical GYSELA runs are shown that use\nFSL.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.0317v1"
    },
    {
        "title": "Discontinuous Galerkin method for Navier-Stokes equations using kinetic\n  flux vector splitting",
        "authors": [
            "Praveen Chandrashekar"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Kinetic schemes for compressible flow of gases are constructed by exploiting\nthe connection between Boltzmann equation and the Navier-Stokes equations. This\nconnection allows us to construct a flux splitting for the Navier-Stokes\nequations based on the direction of molecular motion from which a numerical\nflux can be obtained. The naive use of such a numerical flux function in a\ndiscontinuous Galerkin (DG) discretization leads to an unstable scheme in the\nviscous dominated case. Stable schemes are constructed by adding additional\nterms either in a symmetric or non-symmetric manner which are motivated by the\nDG schemes for elliptic equations. The novelty of the present scheme is the use\nof kinetic fluxes to construct the stabilization terms. In the symmetric case,\ninterior penalty terms have to be added for stability and the resulting schemes\ngive optimal convergence rates in numerical experiments. The non-symmetric\nschemes lead to a cell energy/entropy inequality but exhibit sub-optimal\nconvergence rates. These properties are studied by applying the schemes to a\nscalar convection-diffusion equation and the 1-D compressible Navier-Stokes\nequations. In the case of Navier-Stokes equations, entropy variables are used\nto construct stable schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.4992v1"
    },
    {
        "title": "Kinetic energy preserving and entropy stable finite volume schemes for\n  compressible Euler and Navier-Stokes equations",
        "authors": [
            "Praveen Chandrashekar"
        ],
        "category": "cs.NA",
        "published_year": "2012",
        "summary": "  Centered numerical fluxes can be constructed for compressible Euler equations\nwhich preserve kinetic energy in the semi-discrete finite volume scheme. The\nessential feature is that the momentum flux should be of the form $f^m_\\jph =\n\\tp_\\jph + \\avg{u}_\\jph f^\\rho_\\jph$ where $\\avg{u}_\\jph = (u_j + u_{j+1})/2$\nand $\\tp_\\jph, f^\\rho_\\jph$ are {\\em any} consistent approximations to the\npressure and the mass flux. This scheme thus leaves most terms in the numerical\nflux unspecified and various authors have used simple averaging. Here we\nenforce approximate or exact entropy consistency which leads to a unique choice\nof all the terms in the numerical fluxes. As a consequence novel entropy\nconservative flux that also preserves kinetic energy for the semi-discrete\nfinite volume scheme has been proposed. These fluxes are centered and some\ndissipation has to be added if shocks are present or if the mesh is coarse. We\nconstruct scalar artificial dissipation terms which are kinetic energy stable\nand satisfy approximate/exact entropy condition. Secondly, we use entropy-\nvariable based matrix dissipation flux which leads to kinetic energy and\nentropy stable schemes. These schemes are shown to be free of entropy violating\nsolutions unlike the original Roe scheme. For hypersonic flows a blended scheme\nis proposed which gives carbuncle free solutions for blunt body flows.\nNumerical results for Euler and Navier-Stokes equations are presented to\ndemonstrate the performance of the different schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.4994v1"
    },
    {
        "title": "Isogeometric Boundary Element Analysis with elasto-plastic inclusions.\n  Part 1: Plane problems",
        "authors": [
            "Gernot Beer",
            "Benjamin Marussig",
            "Jürgen Zechner",
            "Christian Dünser",
            "Thomas-Peter Fries"
        ],
        "category": "cs.NA",
        "published_year": "2015",
        "summary": "  In this work a novel approach is presented for the isogeometric Boundary\nElement analysis of domains that contain inclusions with different elastic\nproperties than the ones used for computing the fundamental solutions. In\naddition the inclusion may exhibit inelastic material behavior. In this paper\nonly plane stress/strain problems are considered.\n  In our approach the geometry of the inclusion is described using NURBS basis\nfunctions. The advantage over currently used methods is that no discretization\ninto cells is required in order to evaluate the arising volume integrals. The\nother difference to current approaches is that Kernels of lower singularity are\nused in the domain term. The implementation is verified on simple finite and\ninfinite domain examples with various boundary conditions. Finally a practical\napplication in geomechanics is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.04870v1"
    },
    {
        "title": "Error Analysis and Improving the Accuracy of Winograd Convolution for\n  Deep Neural Networks",
        "authors": [
            "Barbara Barabasz",
            "Andrew Anderson",
            "Kirk M. Soodhalter",
            "David Gregg"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Popular deep neural networks (DNNs) spend the majority of their execution\ntime computing convolutions. The Winograd family of algorithms can greatly\nreduce the number of arithmetic operations required and is present in many DNN\nsoftware frameworks. However, the performance gain is at the expense of a\nreduction in floating point (FP) numerical accuracy. In this paper, we analyse\nthe worst case FP error and prove the estimation of norm and conditioning of\nthe algorithm. We show that the bound grows exponentially with the size of the\nconvolution, but the error bound of the \\textit{modified} algorithm is smaller\nthan the original one. We propose several methods for reducing FP error. We\npropose a canonical evaluation ordering based on Huffman coding that reduces\nsummation error. We study the selection of sampling \"points\" experimentally and\nfind empirically good points for the most important sizes. We identify the main\nfactors associated with good points. In addition, we explore other methods to\nreduce FP error, including mixed-precision convolution, and pairwise summation\nacross DNN channels. Using our methods we can significantly reduce FP error for\na given block size, which allows larger block sizes and reduced computation.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.10986v3"
    },
    {
        "title": "Uncertainty Quantification of Electronic and Photonic ICs with\n  Non-Gaussian Correlated Process Variations",
        "authors": [
            "Chunfeng Cui",
            "Zheng Zhang"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Since the invention of generalized polynomial chaos in 2002, uncertainty\nquantification has impacted many engineering fields, including variation-aware\ndesign automation of integrated circuits and integrated photonics. Due to the\nfast convergence rate, the generalized polynomial chaos expansion has achieved\norders-of-magnitude speedup than Monte Carlo in many applications. However,\nalmost all existing generalized polynomial chaos methods have a strong\nassumption: the uncertain parameters are mutually independent or Gaussian\ncorrelated. This assumption rarely holds in many realistic applications, and it\nhas been a long-standing challenge for both theorists and practitioners.\n  This paper propose a rigorous and efficient solution to address the challenge\nof non-Gaussian correlation. We first extend generalized polynomial chaos, and\npropose a class of smooth basis functions to efficiently handle non-Gaussian\ncorrelations. Then, we consider high-dimensional parameters, and develop a\nscalable tensor method to compute the proposed basis functions. Finally, we\ndevelop a sparse solver with adaptive sample selections to solve\nhigh-dimensional uncertainty quantification problems. We validate our theory\nand algorithm by electronic and photonic ICs with 19 to 57 non-Gaussian\ncorrelated variation parameters. The results show that our approach outperforms\nMonte Carlo by $2500\\times$ to $3000\\times$ in terms of efficiency. Moreover,\nour method can accurately predict the output density functions with multiple\npeaks caused by non-Gaussian correlations, which is hard to handle by existing\nmethods.\n  Based on the results in this paper, many novel uncertainty quantification\nalgorithms can be developed and can be further applied to a broad range of\nengineering domains.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.01778v1"
    },
    {
        "title": "Improved SVD-based Initialization for Nonnegative Matrix Factorization\n  using Low-Rank Correction",
        "authors": [
            "Atif Muhammad Syed",
            "Sameer Qazi",
            "Nicolas Gillis"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  Due to the iterative nature of most nonnegative matrix factorization\n(\\textsc{NMF}) algorithms, initialization is a key aspect as it significantly\ninfluences both the convergence and the final solution obtained. Many\ninitialization schemes have been proposed for NMF, among which one of the most\npopular class of methods are based on the singular value decomposition (SVD).\nHowever, these SVD-based initializations do not satisfy a rather natural\ncondition, namely that the error should decrease as the rank of factorization\nincreases. In this paper, we propose a novel SVD-based \\textsc{NMF}\ninitialization to specifically address this shortcoming by taking into account\nthe SVD factors that were discarded to obtain a nonnegative initialization.\nThis method, referred to as nonnegative SVD with low-rank correction\n(NNSVD-LRC), allows us to significantly reduce the initial error at a\nnegligible additional computational cost using the low-rank structure of the\ndiscarded SVD factors. NNSVD-LRC has two other advantages compared to previous\nSVD-based initializations: (1) it provably generates sparse initial factors,\nand (2) it is faster as it only requires to compute a truncated SVD of rank\n$\\lceil r/2 + 1 \\rceil$ where $r$ is the factorization rank of the sought NMF\ndecomposition (as opposed to a rank-$r$ truncated SVD for other methods). We\nshow on several standard dense and sparse data sets that our new method\ncompetes favorably with state-of-the-art SVD-based initializations for NMF.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.04020v1"
    },
    {
        "title": "Minimizing convex quadratic with variable precision conjugate gradients",
        "authors": [
            "S. Gratton",
            "E. Simon",
            "D. Titley-Peloquin",
            "Ph. L. Toint"
        ],
        "category": "cs.NA",
        "published_year": "2018",
        "summary": "  We investigate the method of conjugate gradients, exploiting inaccurate\nmatrix-vector products, for the solution of convex quadratic optimization\nproblems. Theoretical performance bounds are derived, and the necessary\nquantities occurring in the theoretical bounds estimated, leading to a\npractical algorithm. Numerical experiments suggest that this approach has\nsignificant potential, including in the steadily more important context of\nmulti-precision computations\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07476v3"
    },
    {
        "title": "Towards Solving the Navier-Stokes Equation on Quantum Computers",
        "authors": [
            "Navamita Ray",
            "Tirtha Banerjee",
            "Balasubramanya Nadiga",
            "Satish Karra"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  In this paper, we explore the suitability of upcoming novel computing\ntechnologies, in particular adiabatic annealing based quantum computers, to\nsolve fluid dynamics problems that form a critical component of several science\nand engineering applications. We start with simple flows with well-studied flow\nproperties, and provide a framework to convert such systems to a form amenable\nfor deployment on such quantum annealers. We analyze the solutions obtained\nboth qualitatively and quantitatively as well as the sensitivities of the\nvarious solution selection schemes on the obtained solution.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.09033v1"
    },
    {
        "title": "A theoretical and experimental investigation of a family of immersed\n  finite element methods",
        "authors": [
            "Yongxing Wang",
            "Peter K. Jimack",
            "Mark A. Walkley"
        ],
        "category": "cs.NA",
        "published_year": "2019",
        "summary": "  In this article we consider the widely used immersed finite element method\n(IFEM), in both explicit and implicit form, and its relationship to our more\nrecent one-field fictitious domain method (FDM). We review and extend the\nformulation of these methods, based upon an operator splitting scheme, in order\nto demonstrate that both the explicit IFEM and the one-field FDM can be\nregarded as particular linearizations of the fully implicit IFEM. However, the\none-field FDM can be shown to be more robust than the explicit IFEM and can\nsimulate a wider range of solid parameters with a relatively large time step.\nIn addition, it can produce results almost identical to the implicit IFEM but\nwithout iteration inside each time step. We study the effect on these methods\nof variations in viscosity and density of fluid and solid materials. The\nadvantages of the one-field FDM within the IFEM framework are illustrated\nthrough a selection of parameter sets for two benchmark cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.10027v1"
    }
]