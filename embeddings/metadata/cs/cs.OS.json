[
    {
        "title": "Open Source Real Time Operating Systems Overview",
        "authors": [
            "Till Straumann"
        ],
        "category": "cs.OS",
        "published_year": "2001",
        "summary": "  Modern control systems applications are often built on top of a real time\noperating system (RTOS) which provides the necessary hardware abstraction as\nwell as scheduling, networking and other services. Several open source RTOS\nsolutions are publicly available, which is very attractive, both from an\neconomic (no licensing fees) as well as from a technical (control over the\nsource code) point of view. This contribution gives an overview of the RTLinux\nand RTEMS systems (architecture, development environment, API etc.). Both\nsystems feature most popular CPUs, several APIs (including Posix), networking,\nportability and optional commercial support. Some performance figures are\npresented, focusing on interrupt latency and context switching delay.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0111035v1"
    },
    {
        "title": "Modeling the input history of programs for improved instruction-memory\n  performance",
        "authors": [
            "C. A. G. Assis",
            "E. S. T. Fernandes",
            "V. C. Barbosa"
        ],
        "category": "cs.OS",
        "published_year": "2004",
        "summary": "  When a program is loaded into memory for execution, the relative position of\nits basic blocks is crucial, since loading basic blocks that are unlikely to be\nexecuted first places them high in the instruction-memory hierarchy only to be\ndislodged as the execution goes on. In this paper we study the use of Bayesian\nnetworks as models of the input history of a program. The main point is the\ncreation of a probabilistic model that persists as the program is run on\ndifferent inputs and at each new input refines its own parameters in order to\nreflect the program's input history more accurately. As the model is thus\ntuned, it causes basic blocks to be reordered so that, upon arrival of the next\ninput for execution, loading the basic blocks into memory automatically takes\ninto account the input history of the program. We report on extensive\nexperiments, whose results demonstrate the efficacy of the overall approach in\nprogressively lowering the execution times of a program on identical inputs\nplaced randomly in a sequence of varied inputs. We provide results on selected\nSPEC CINT2000 programs and also evaluate our approach as compared to the gcc\nlevel-3 optimization and to Pettis-Hansen reordering.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0411080v1"
    },
    {
        "title": "Markets are Dead, Long Live Markets",
        "authors": [
            "Kevin Lai"
        ],
        "category": "cs.OS",
        "published_year": "2005",
        "summary": "  Researchers have long proposed using economic approaches to resource\nallocation in computer systems. However, few of these proposals became\noperational, let alone commercial. Questions persist about the economic\napproach regarding its assumptions, value, applicability, and relevance to\nsystem design. The goal of this paper is to answer these questions. We find\nthat market-based resource allocation is useful, and more importantly, that\nmechanism design and system design should be integrated to produce systems that\nare both economically and computationally efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0502027v1"
    },
    {
        "title": "A Survey of Virtualization Techniques Focusing on Secure On-Demand\n  Cluster Computing",
        "authors": [
            "Nadir Kiyanclar"
        ],
        "category": "cs.OS",
        "published_year": "2005",
        "summary": "  Virtualization, a technique once used to multiplex the resources of\nhigh-priced mainframe hardware, is seeing a resurgence in applicability with\nthe increasing computing power of commodity computers. By inserting a layer of\nsoftware between the machine and traditional operating systems, this technology\nallows access to a shared computing medium in a manner that is secure,\nresource-controlled, and efficient. These properties are attractive in the\nfield of on-demand computing, where the fine-grained subdivision of resources\nprovided by virtualized systems allows potentially higher utilization of\ncomputing resources.\n  It this work, we survey a number of virtual machine systems with the goal of\nfinding an appropriate candidate to serve as the basis for the On-Demand Secure\nCluster Computing project at the National Center for Supercomputing\nApplications. Contenders are reviewed on a number of desirable properties\nincluding portability and security. We conclude with a comparison and\njustification of our choice.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0511010v1"
    },
    {
        "title": "A Low-Footprint Class Loading Mechanism for Embedded Java Virtual\n  Machines",
        "authors": [
            "Christophe Rippert",
            "Alexandre Courbot",
            "Gilles Grimaud"
        ],
        "category": "cs.OS",
        "published_year": "2006",
        "summary": "  This paper shows that it is possible to dramatically reduce the memory\nconsumption of classes loaded in an embedded Java virtual machine without\nreducing its functionalities. We describe how to pack the constant pool by\ndeleting entries which are only used during the class loading process. We\npresent some benchmarks which demonstrate the efficiency of this mechanism. We\nfinally suggest some additional optimizations which can be applied if some\nrestrictions to the functionalities of the virtual machine can be tolerated.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0611055v1"
    },
    {
        "title": "Executing the same binary on several operating systems",
        "authors": [
            "Steffen Grønneberg"
        ],
        "category": "cs.OS",
        "published_year": "2006",
        "summary": "  We notice a way to execute a binary file on Windows and ELF-based systems. It\ncan be used to create software installers and other applications not exceeding\n64 kilo bytes.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0612079v2"
    },
    {
        "title": "A Survey of Unix Init Schemes",
        "authors": [
            "Yvan Royon",
            "Stéphane Frénot"
        ],
        "category": "cs.OS",
        "published_year": "2007",
        "summary": "  In most modern operating systems, init (as in \"initialization\") is the\nprogram launched by the kernel at boot time. It runs as a daemon and typically\nhas PID 1. Init is responsible for spawning all other processes and scavenging\nzombies. It is also responsible for reboot and shutdown operations. This\ndocument describes existing solutions that implement the init process and/or\ninit scripts in Unix-like systems. These solutions range from the legacy and\nstill-in-use BSD and SystemV schemes, to recent and promising schemes from\nUbuntu, Apple, Sun and independent developers. Our goal is to highlight their\nfocus and compare their sets of features.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.2748v2"
    },
    {
        "title": "OS Debugging Method Using a Lightweight Virtual Machine Monitor",
        "authors": [
            "Tadashi Takeuchi"
        ],
        "category": "cs.OS",
        "published_year": "2007",
        "summary": "  Demands for implementing original OSs that can achieve high I/O performance\non PC/AT compatible hardware have recently been increasing, but conventional OS\ndebugging environments have not been able to simultaneously assure their\nstability, be easily customized to new OSs and new I/O devices, and assure\nefficient execution of I/O operations. We therefore developed a novel OS\ndebugging method using a lightweight virtual machine. We evaluated this\ndebugging method experimentally and confirmed that it can transfer data about\n5.4 times as fast as the conventional virtual machine monitor.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4635v1"
    },
    {
        "title": "RTK-Spec TRON: A Simulation Model of an ITRON Based RTOS Kernel in\n  SystemC",
        "authors": [
            "M. Abdelsalam Hassan",
            "Keishi Sakanushi",
            "Yoshinori Takeuchi",
            "Masaharu Imai"
        ],
        "category": "cs.OS",
        "published_year": "2007",
        "summary": "  This paper presents the methodology and the modeling constructs we have\ndeveloped to capture the real time aspects of RTOS simulation models in a\nSystem Level Design Language (SLDL) like SystemC. We describe these constructs\nand show how they are used to build a simulation model of an RTOS kernel\ntargeting the $\\mu$-ITRON OS specification standard.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4746v1"
    },
    {
        "title": "Power-Aware Real-Time Scheduling upon Identical Multiprocessor Platforms",
        "authors": [
            "Vincent Nélis",
            "Joël Goossens",
            "Nicolas Navet",
            "Raymond Devillers",
            "Dragomir Milojevic"
        ],
        "category": "cs.OS",
        "published_year": "2007",
        "summary": "  In this paper, we address the power-aware scheduling of sporadic\nconstrained-deadline hard real-time tasks using dynamic voltage scaling upon\nmultiprocessor platforms. We propose two distinct algorithms. Our first\nalgorithm is an off-line speed determination mechanism which provides an\nidentical speed for each processor. That speed guarantees that all deadlines\nare met if the jobs are scheduled using EDF. The second algorithm is an on-line\nand adaptive speed adjustment mechanism which reduces the energy consumption\nwhile the system is running.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.2958v2"
    },
    {
        "title": "Discrete Frequency Selection of Frame-Based Stochastic Real-Time Tasks",
        "authors": [
            "Vandy Berten",
            "Chi-Ju Chang",
            "Tei-Wei Kuo"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  Energy-efficient real-time task scheduling has been actively explored in the\npast decade. Different from the past work, this paper considers schedulability\nconditions for stochastic real-time tasks. A schedulability condition is first\npresented for frame-based stochastic real-time tasks, and several algorithms\nare also examined to check the schedulability of a given strategy. An approach\nis then proposed based on the schedulability condition to adapt a\ncontinuous-speed-based method to a discrete-speed system. The approach is able\nto stay as close as possible to the continuous-speed-based method, but still\nguaranteeing the schedulability. It is shown by simulations that the energy\nsaving can be more than 20% for some system configurations\n",
        "pdf_link": "http://arxiv.org/pdf/0803.4308v2"
    },
    {
        "title": "(m,k)-firm constraints and DBP scheduling: impact of the initial\n  k-sequence and exact schedulability test",
        "authors": [
            "Joël Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  In this paper we study the scheduling of (m,k)-firm synchronous periodic task\nsystems using the Distance Based Priority (DBP) scheduler. We first show three\nphenomena: (i) choosing, for each task, the initial k-sequence 1^k is not\noptimal, (ii) we can even start the scheduling from a (fictive) error state (in\nregard to the initial k-sequence) and (iii) the period of feasible\nDBP-schedules is not necessarily the task hyper-period. We then show that any\nfeasible DBP-schedule is periodic and we upper-bound the length of that period.\nLastly, based on our periodicity result we provide an exact schedulability\ntest.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.0200v2"
    },
    {
        "title": "Integrating Job Parallelism in Real-Time Scheduling Theory",
        "authors": [
            "S. Collette",
            "L. Cucu",
            "J. Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  We investigate the global scheduling of sporadic, implicit deadline,\nreal-time task systems on multiprocessor platforms. We provide a task model\nwhich integrates job parallelism. We prove that the time-complexity of the\nfeasibility problem of these systems is linear relatively to the number of\n(sporadic) tasks for a fixed number of processors. We propose a scheduling\nalgorithm theoretically optimal (i.e., preemptions and migrations neglected).\nMoreover, we provide an exact feasibility utilization bound. Lastly, we propose\na technique to limit the number of migrations and preemptions.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.3237v1"
    },
    {
        "title": "Control-theoretic dynamic voltage scaling for embedded controllers",
        "authors": [
            "Feng Xia",
            "Yu-Chu Tian",
            "Youxian Sun",
            "Jinxiang Dong"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  For microprocessors used in real-time embedded systems, minimizing power\nconsumption is difficult due to the timing constraints. Dynamic voltage scaling\n(DVS) has been incorporated into modern microprocessors as a promising\ntechnique for exploring the trade-off between energy consumption and system\nperformance. However, it remains a challenge to realize the potential of DVS in\nunpredictable environments where the system workload cannot be accurately\nknown. Addressing system-level power-aware design for DVS-enabled embedded\ncontrollers, this paper establishes an analytical model for the DVS system that\nencompasses multiple real-time control tasks. From this model, a feedback\ncontrol based approach to power management is developed to reduce dynamic power\nconsumption while achieving good application performance. With this approach,\nthe unpredictability and variability of task execution times can be attacked.\nThanks to the use of feedback control theory, predictable performance of the\nDVS system is achieved, which is favorable to real-time applications. Extensive\nsimulations are conducted to evaluate the performance of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.0132v1"
    },
    {
        "title": "Feedback Scheduling: An Event-Driven Paradigm",
        "authors": [
            "Feng Xia",
            "Guosong Tian",
            "Youxian Sun"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  Embedded computing systems today increasingly feature resource constraints\nand workload variability, which lead to uncertainty in resource availability.\nThis raises great challenges to software design and programming in multitasking\nenvironments. In this paper, the emerging methodology of feedback scheduling is\nintroduced to address these challenges. As a closed-loop approach to resource\nmanagement, feedback scheduling promises to enhance the flexibility and\nresource efficiency of various software programs through dynamically\ndistributing available resources among concurrent tasks based on feedback\ninformation about the actual usage of the resources. With emphasis on the\nbehavioral design of feedback schedulers, we describe a general framework of\nfeedback scheduling in the context of real-time control applications. A simple\nyet illustrative feedback scheduling algorithm is given. From a programming\nperspective, we describe how to modify the implementation of control tasks to\nfacilitate the application of feedback scheduling. An event-driven paradigm\nthat combines time-triggered and event-triggered approaches is proposed for\nprogramming of the feedback scheduler. Simulation results argue that the\nproposed event-driven paradigm yields better performance than time-triggered\nparadigm in dynamic environments where the workload varies irregularly and\nunpredictably.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.1381v1"
    },
    {
        "title": "Managing Varying Worst Case Execution Times on DVS Platforms",
        "authors": [
            "Vandy Berten",
            "Chi-Ju Chang",
            "Tei-Wei Kuo"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  Energy efficient real-time task scheduling attracted a lot of attention in\nthe past decade. Most of the time, deterministic execution lengths for tasks\nwere considered, but this model fits less and less with the reality, especially\nwith the increasing number of multimedia applications. It's why a lot of\nresearch is starting to consider stochastic models, where execution times are\nonly known stochastically. However, authors consider that they have a pretty\nmuch precise knowledge about the properties of the system, especially regarding\nto the worst case execution time (or worst case execution cycles, WCEC).\n  In this work, we try to relax this hypothesis, and assume that the WCEC can\nvary. We propose miscellaneous methods to react to such a situation, and give\nmany simulation results attesting that with a small effort, we can provide very\ngood results, allowing to keep a low deadline miss rate as well as an energy\nconsumption similar to clairvoyant algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.1132v1"
    },
    {
        "title": "Multiprocessor Global Scheduling on Frame-Based DVFS Systems",
        "authors": [
            "Vandy Berten",
            "Joël Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  In this ongoing work, we are interested in multiprocessor energy efficient\nsystems, where task durations are not known in advance, but are know\nstochastically. More precisely, we consider global scheduling algorithms for\nframe-based multiprocessor stochastic DVFS (Dynamic Voltage and Frequency\nScaling) systems. Moreover, we consider processors with a discrete set of\navailable frequencies.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.4082v1"
    },
    {
        "title": "Mode Change Protocol for Multi-Mode Real-Time Systems upon Identical\n  Multiprocessors",
        "authors": [
            "Vincent Nélis",
            "Joël Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  In this paper, we propose a synchronous protocol without periodicity for\nscheduling multi-mode real-time systems upon identical multiprocessor\nplatforms. Our proposal can be considered to be a multiprocessor extension of\nthe uniprocessor protocol called \"Minimal Single Offset protocol\".\n",
        "pdf_link": "http://arxiv.org/pdf/0809.5238v1"
    },
    {
        "title": "MORA: an Energy-Aware Slack Reclamation Scheme for Scheduling Sporadic\n  Real-Time Tasks upon Multiprocessor Platforms",
        "authors": [
            "Vincent Nelis",
            "Joel Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2009",
        "summary": "  In this paper, we address the global and preemptive energy-aware scheduling\nproblem of sporadic constrained-deadline tasks on DVFS-identical multiprocessor\nplatforms. We propose an online slack reclamation scheme which profits from the\ndiscrepancy between the worst- and actual-case execution time of the tasks by\nslowing down the speed of the processors in order to save energy. Our algorithm\ncalled MORA takes into account the application-specific consumption profile of\nthe tasks. We demonstrate that MORA does not jeopardize the system\nschedulability and we show by performing simulations that it can save up to 32%\nof energy (in average) compared to execution without using any energy-aware\nalgorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.0268v1"
    },
    {
        "title": "Predictability of Fixed-Job Priority Schedulers on Heterogeneous\n  Multiprocessor Real-Time Systems",
        "authors": [
            "Liliana Cucu-Grosjean",
            "Joël Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2009",
        "summary": "  The multiprocessor Fixed-Job Priority (FJP) scheduling of real-time systems\nis studied. An important property for the schedulability analysis, the\npredictability (regardless to the execution times), is studied for\nheterogeneous multiprocessor platforms. Our main contribution is to show that\nany FJP schedulers are predictable on unrelated platforms. A convenient\nconsequence is the fact that any FJP schedulers are predictable on uniform\nmultiprocessors.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.3519v1"
    },
    {
        "title": "A New Scheduling Algorithms For Real Time Tasks",
        "authors": [
            "C. Yaashuwanth",
            "Dr. R. Ramesh"
        ],
        "category": "cs.OS",
        "published_year": "2009",
        "summary": "  The main objective of this paper is to develop the two different ways in\nwhich round robin architecture is modified and made suitable to be implemented\nin real time and embedded systems. The scheduling algorithm plays a significant\nrole in the design of real time embedded systems. Simple round robin\narchitecture is not efficient to be implemented in embedded systems because of\nhigher context switch rate, larger waiting time and larger response time.\nMissing of deadlines will degrade the system performance in soft real time\nsystems. The main objective of this paper is to develop the scheduling\nalgorithm which removes the drawbacks in simple round robin architecture. A\ncomparison with round robin architecture to the proposed architectures has been\nmade. It is observed that the proposed architectures solves the problems\nencountered in round robin architecture in soft real time by decreasing the\nnumber of context switches waiting time and response time thereby increasing\nthe system throughput.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.0606v1"
    },
    {
        "title": "Deterministic Consistency: A Programming Model for Shared Memory\n  Parallelism",
        "authors": [
            "Amittai Aviram",
            "Bryan Ford"
        ],
        "category": "cs.OS",
        "published_year": "2009",
        "summary": "  The difficulty of developing reliable parallel software is generating\ninterest in deterministic environments, where a given program and input can\nyield only one possible result. Languages or type systems can enforce\ndeterminism in new code, and runtime systems can impose synthetic schedules on\nlegacy parallel code. To parallelize existing serial code, however, we would\nlike a programming model that is naturally deterministic without language\nrestrictions or artificial scheduling. We propose \"deterministic consistency\",\na parallel programming model as easy to understand as the \"parallel assignment\"\nconstruct in sequential languages such as Perl and JavaScript, where concurrent\nthreads always read their inputs before writing shared outputs. DC supports\ncommon data- and task-parallel synchronization abstractions such as fork/join\nand barriers, as well as non-hierarchical structures such as producer/consumer\npipelines and futures. A preliminary prototype suggests that software-only\nimplementations of DC can run applications written for popular parallel\nenvironments such as OpenMP with low (<10%) overhead for some applications.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.0926v2"
    },
    {
        "title": "Fault Tolerance in Real Time Multiprocessors - Embedded Systems",
        "authors": [
            "A. Christy Persya",
            "T. R. Gopalakrishnan Nair"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  All real time tasks which are termed as critical tasks by nature have to\ncomplete its execution before its deadline, even in presence of faults. The\nmost popularly used real time task assignment algorithms are First Fit (FF),\nBest Fit (BF), Bin Packing (BP).The common task scheduling algorithms are Rate\nMonotonic (RM), Earliest Deadline First (EDF) etc.All the current approaches\ndeal with either fault tolerance or criticality in real time. In this paper we\nhave proposed an integrated approach with a new algorithm, called SASA (Sorting\nAnd Sequential Assignment) which maps the real time task assignment with task\nschedule and fault tolerance\n",
        "pdf_link": "http://arxiv.org/pdf/1001.3727v1"
    },
    {
        "title": "FIFO anomaly is unbounded",
        "authors": [
            "Peter Fornai",
            "Antal Ivanyi"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  Virtual memory of computers is usually implemented by demand paging. For some\npage replacement algorithms the number of page faults may increase as the\nnumber of page frames increases. Belady, Nelson and Shedler constructed\nreference strings for which page replacement algorithm FIFO produces near twice\nmore page faults in a larger memory than in a smaller one. They formulated the\nconjecture that 2 is a general bound. We prove that this ratio can be\narbitrarily large.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.1336v2"
    },
    {
        "title": "Proficient Pair of Replacement Algorithms on L1 and L2 Cache for Merge\n  Sort",
        "authors": [
            "Richa Gupta",
            "Sanjiv Tokekar"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  Memory hierarchy is used to compete the processors speed. Cache memory is the\nfast memory which is used to conduit the speed difference of memory and\nprocessor. The access patterns of Level 1 cache (L1) and Level 2 cache (L2) are\ndifferent, when CPU not gets the desired data in L1 then it accesses L2. Thus\nthe replacement algorithm which works efficiently on L1 may not be as efficient\non L2. Similarly various applications such as Matrix Multiplication, Web, Fast\nFourier Transform (FFT) etc will have varying access pattern. Thus same\nreplacement algorithm for all types of application may not be efficient. This\npaper works for getting an efficient pair of replacement algorithm on L1 and L2\nfor the algorithm Merge Sort. With the memory reference string of Merge Sort,\nwe have analyzed the behavior of various existing replacement algorithms on L1.\nThe existing replacement algorithms which are taken into consideration are:\nLeast Recently Used (LRU), Least Frequently Used (LFU) and First In First Out\n(FIFO). After Analyzing the memory reference pattern of Merge Sort, we have\nproposed a Partition Based Replacement algorithm (PBR_L1)) on L1 Cache.\nFurthermore we have analyzed various pairs of algorithms on L1 and L2\nrespectively, resulting in finding a suitable pair of replacement algorithms.\nSimulation on L1 shows, among the considered existing replacement algorithms\nFIFO is performing better than others. While the proposed replacement algorithm\nPBR_L1 is working about 1.7% to 44 % better than FIFO for various cache sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.4088v1"
    },
    {
        "title": "Searching publications on operating systems",
        "authors": [
            "C. A. Middelburg"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  This note concerns a search for publications in which one can find statements\nthat explain the concept of an operating system, reasons for introducing\noperating systems, a formalization of the concept of an operating system or\ntheory about operating systems based on such a formalization. It reports on the\nway in which the search has been carried out and the outcome of the search. The\noutcome includes not only what the search was meant for, but also some added\nbonuses.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.5525v1"
    },
    {
        "title": "Scheduling Multi-Mode Real-Time Systems upon Uniform Multiprocessor\n  Platforms",
        "authors": [
            "Patrick Meumeu Yomsi",
            "Vincent Nelis",
            "Joël Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  In this paper, we address the scheduling problem of multi-mode real-time\nsystems upon uniform multiprocessor platforms. We propose two transition\nprotocols, specified together with their schedulability test, and provide the\nreader with two distinct upper bounds for the length of the transient phases\nduring mode transitions, respectively for the cases where jobs priorities are\nknown and unknown beforehand.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.3687v1"
    },
    {
        "title": "Multi-Criteria Evaluation of Partitioning Schemes for Real-Time Systems",
        "authors": [
            "Irina Lupu",
            "Pierre Courbin",
            "Laurent George",
            "Joël Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  In this paper we study the partitioning approach for multiprocessor real-time\nscheduling. This approach seems to be the easiest since, once the partitioning\nof the task set has been done, the problem reduces to well understood\nuniprocessor issues. Meanwhile, there is no optimal and polynomial solution to\npartition tasks on processors. In this paper we analyze partitioning algorithms\nfrom several points of view such that for a given task set and specific\nconstraints (processor number, task set type, etc.) we should be able to\nidentify the best heuristic and the best schedulability test. We also analyze\nthe influence of the heuristics on the performance of the uniprocessor tests\nand the impact of a specific task order on the schedulability. A study on\nperformance difference between Fixed Priority schedulers and EDF in the case of\npartitioning scheduling is also considered.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.3715v1"
    },
    {
        "title": "On the definition of a theoretical concept of an operating system",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  We dwell on how a definition of a theoretical concept of an operating system,\nsuitable to be incorporated in a mathematical theory of operating systems,\ncould look like. This is considered a valuable preparation for the development\nof a mathematical theory of operating systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.0813v1"
    },
    {
        "title": "Perbandingan Shell Unix",
        "authors": [
            "Spits Warnars H. L. H"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  Is it possible for an Information Technology [IT] product to be both mature\nand state-of-theart at the same time? In the case of the UNIX system, the\nanswer is an unqualified \"Yes.\" The UNIX system has continued to develop over\nthe past twenty-five years. In millions of installations running on nearly\nevery hardware platform made, the UNIX system has earned its reputation for\nstability and scalability. Over the years, UNIX system suppliers have steadily\nassimilated new technologies so that UNIX systems today provide more\nfunctionality as any other operating system.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.2104v1"
    },
    {
        "title": "Gang FTP scheduling of periodic and parallel rigid real-time tasks",
        "authors": [
            "Joël Goossens",
            "Vandy Berten"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  In this paper we consider the scheduling of periodic and parallel rigid\ntasks. We provide (and prove correct) an exact schedulability test for Fixed\nTask Priority (FTP) Gang scheduler sub-classes: Parallelism Monotonic, Idling,\nLimited Gang, and Limited Slack Reclaiming. Additionally, we study the\npredictability of our schedulers: we show that Gang FJP schedulers are not\npredictable and we identify several sub-classes which are actually predictable.\nMoreover, we extend the definition of rigid, moldable and malleable jobs to\nrecurrent tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.2617v1"
    },
    {
        "title": "Semi-Partitioned Hard Real-Time Scheduling with Restricted Migrations\n  upon Identical Multiprocessor Platforms",
        "authors": [
            "François Dorin",
            "Patrick Meumeu Yomsi",
            "Joël Goossens",
            "Pascal Richard"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  Algorithms based on semi-partitioned scheduling have been proposed as a\nviable alternative between the two extreme ones based on global and partitioned\nscheduling. In particular, allowing migration to occur only for few tasks which\ncannot be assigned to any individual processor, while most tasks are assigned\nto specific processors, considerably reduces the runtime overhead compared to\nglobal scheduling on the one hand, and improve both the schedulability and the\nsystem utilization factor compared to partitioned scheduling on the other hand.\nIn this paper, we address the preemptive scheduling problem of hard real-time\nsystems composed of sporadic constrained-deadline tasks upon identical\nmultiprocessor platforms. We propose a new algorithm and a scheduling paradigm\nbased on the concept of semi-partitioned scheduling with restricted migrations\nin which jobs are not allowed to migrate, but two subsequent jobs of a task can\nbe assigned to different processors by following a periodic strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.2637v1"
    },
    {
        "title": "Use of Data Mining in Scheduler Optimization",
        "authors": [
            "George Anderson",
            "Tshilidzi Marwala",
            "Fulufhelo V. Nelwamondo"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  The operating system's role in a computer system is to manage the various\nresources. One of these resources is the Central Processing Unit. It is managed\nby a component of the operating system called the CPU scheduler. Schedulers are\noptimized for typical workloads expected to run on the platform. However, a\nsingle scheduler may not be appropriate for all workloads. That is, a scheduler\nmay schedule a workload such that the completion time is minimized, but when\nanother type of workload is run on the platform, scheduling and therefore\ncompletion time will not be optimal; a different scheduling algorithm, or a\ndifferent set of parameters, may work better. Several approaches to solving\nthis problem have been proposed. The objective of this survey is to summarize\nthe approaches based on data mining, which are available in the literature. In\naddition to solutions that can be directly utilized for solving this problem,\nwe are interested in data mining research in related areas that have potential\nfor use in operating system scheduling. We also explain general technical\nissues involved in scheduling in modern computers, including parallel\nscheduling issues related to multi-core CPUs. We propose a taxonomy that\nclassifies the scheduling approaches we discuss into different categories.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.1735v1"
    },
    {
        "title": "Sesame: Self-Constructive System Energy Modeling for Battery-Powered\n  Mobile Systems",
        "authors": [
            "Mian Dong",
            "Lin Zhong"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  System energy models are important for energy optimization and management in\nmobile systems. However, existing system energy models are built in lab with\nthe help from a second computer. Not only are they labor-intensive; but also\nthey will not adequately account for the great diversity in the hardware and\nusage of mobile systems. Moreover, existing system energy models are intended\nfor energy estimation for time intervals of one second or longer; they do not\nprovide the required rate for fine-grain use such as per-application energy\naccounting.\n  In this work, we study a self-modeling paradigm in which a mobile system\nautomatically generates its energy model without any external assistance. Our\nsolution, Se-same, leverages the possibility of self power measurement through\nthe smart battery interface and employs a suite of novel techniques to achieve\naccuracy and rate much higher than that of the smart battery interface.\n  We report the implementation and evaluation of Se-same on a laptop and a\nsmartphone. The experiment results show that Sesame generates system energy\nmodels of 95% accuracy at one estimation per second and 88% accuracy at one\nestimation per 10ms, without any external assistance. A five-day field studies\nwith four laptop and four smartphones users further demonstrate the\neffectiveness, efficiency, and noninvasiveness of Sesame.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.2831v2"
    },
    {
        "title": "Customer Appeasement Scheduling",
        "authors": [
            "Mohammad R Nikseresht",
            "Anil Somayaji",
            "Anil Maheshwari"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  Almost all of the current process scheduling algorithms which are used in\nmodern operating systems (OS) have their roots in the classical scheduling\nparadigms which were developed during the 1970's. But modern computers have\ndifferent types of software loads and user demands. We think it is important to\nrun what the user wants at the current moment. A user can be a human, sitting\nin front of a desktop machine, or it can be another machine sending a request\nto a server through a network connection. We think that OS should become\nintelligent to distinguish between different processes and allocate resources,\nincluding CPU, to those processes which need them most. In this work, as a\nfirst step to make the OS aware of the current state of the system, we consider\nprocess dependencies and interprocess communications. We are developing a\nmodel, which considers the need to satisfy interactive users and other possible\nremote users or customers, by making scheduling decisions based on process\ndependencies and interprocess communications. Our simple proof of concept\nimplementation and experiments show the effectiveness of this approach in the\nreal world applications. Our implementation does not require any change in the\nsoftware applications nor any special kind of configuration in the system,\nMoreover, it does not require any additional information about CPU needs of\napplications nor other resource requirements. Our experiments show significant\nperformance improvement for real world applications. For example, almost\nconstant average response time for Mysql data base server and constant frame\nrate for mplayer under different simulated load values.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.3452v1"
    },
    {
        "title": "Application of Global and One-Dimensional Local Optimization to\n  Operating System Scheduler Tuning",
        "authors": [
            "George Anderson",
            "Tshilidzi Marwala",
            "Fulufhelo Vincent Nelwamondo"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  This paper describes a study of comparison of global and one-dimensional\nlocal optimization methods to operating system scheduler tuning. The operating\nsystem scheduler we use is the Linux 2.6.23 Completely Fair Scheduler (CFS)\nrunning in simulator (LinSched). We have ported the Hackbench scheduler\nbenchmark to this simulator and use this as the workload. The global\noptimization approach we use is Particle Swarm Optimization (PSO). We make use\nof Response Surface Methodology (RSM) to specify optimal parameters for our PSO\nimplementation. The one-dimensional local optimization approach we use is the\nGolden Section method. In order to use this approach, we convert the scheduler\ntuning problem from one involving setting of three parameters to one involving\nthe manipulation of one parameter. Our results show that the global\noptimization approach yields better response but the one- dimensional\noptimization approach converges to a solution faster than the global\noptimization approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.4045v1"
    },
    {
        "title": "Dynamic Scheduling of Skippable Periodic Tasks with Energy Efficiency in\n  Weakly Hard Real-Time System",
        "authors": [
            "Santhi Baskaran",
            "P. Thambidurai"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  Energy consumption is a critical design issue in real-time systems,\nespecially in battery- operated systems. Maintaining high performance, while\nextending the battery life between charges is an interesting challenge for\nsystem designers. Dynamic Voltage Scaling (DVS) allows a processor to\ndynamically change speed and voltage at run time, thereby saving energy by\nspreading run cycles into idle time. Knowing when to use full power and when\nnot, requires the cooperation of the operating system scheduler. Usually,\nhigher processor voltage and frequency leads to higher system throughput while\nenergy reduction can be obtained using lower voltage and frequency. Instead of\nlowering processor voltage and frequency as much as possible, energy efficient\nreal-time scheduling adjusts voltage and frequency according to some\noptimization criteria, such as low energy consumption or high throughput, while\nit meets the timing constraints of the real-time tasks. As the quantity and\nfunctional complexity of battery powered portable devices continues to raise,\nenergy efficient design of such devices has become increasingly important. Many\nreal-time scheduling algorithms have been developed recently to reduce energy\nconsumption in the portable devices that use DVS capable processors. Three\nalgorithms namely Red Tasks Only (RTO), Blue When Possible (BWP) and Red as\nLate as Possible (RLP) are proposed in the literature to schedule the real-time\ntasks in Weakly-hard real-time systems. This paper proposes optimal slack\nmanagement algorithms to make the above existing weakly hard real-time\nscheduling algorithms energy efficient using DVS and DPD techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.5695v1"
    },
    {
        "title": "Exact Schedulability Test for global-EDF Scheduling of Periodic Hard\n  Real-Time Tasks on Identical Multiprocessors",
        "authors": [
            "Joël Goossens",
            "Patrick Meumeu Yomsi"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  In this paper we consider the scheduling problem of hard real-time systems\ncomposed of periodic constrained-deadline tasks upon identical multiprocessor\nplatforms. We assume that tasks are scheduled by using the global-EDF\nscheduler. We establish an exact schedulability test for this scheduler by\nexploiting on the one hand its predictability property and by providing on the\nother hand a feasibility interval so that if it is possible to find a valid\nschedule for all the jobs contained in this interval, then the whole system\nwill be stamped feasible. In addition, we show by means of a counterexample\nthat the feasibility interval, and thus the schedulability test, proposed by\nLeung [Leung 1989] is incorrect and we show which arguments are actually\nincorrect.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.5929v1"
    },
    {
        "title": "Comparison of Loss ratios of different scheduling algorithms",
        "authors": [
            "Sudipta Das",
            "Lawrence Jenkins",
            "Debasis Sengupta"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  It is well known that in a firm real time system with a renewal arrival\nprocess, exponential service times and independent and identically distributed\ndeadlines till the end of service of a job, the earliest deadline first (EDF)\nscheduling policy has smaller loss ratio (expected fraction of jobs, not\ncompleted) than any other service time independent scheduling policy, including\nthe first come first served (FCFS). Various modifications to the EDF and FCFS\npolicies have been proposed in the literature, with a view to improving\nperformance. In this article, we compare the loss ratios of these two policies\nalong with some of the said modifications, as well as their counterparts with\ndeterministic deadlines. The results include some formal inequalities and some\ncounter-examples to establish non-existence of an order. A few relations\ninvolving loss ratios are posed as conjectures, and simulation results in\nsupport of these are reported. These results lead to a complete picture of\ndominance and non-dominance relations between pairs of scheduling policies, in\nterms of loss ratios.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.1466v1"
    },
    {
        "title": "Global Scheduling of Multi-Mode Real-Time Applications upon\n  Multiprocessor Platforms",
        "authors": [
            "Vincent Nelis",
            "Patrick Meumeu Yomsi",
            "Björn Andersson",
            "Joël Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  Multi-mode real-time systems are those which support applications with\ndifferent modes of operation, where each mode is characterized by a specific\nset of tasks. At run-time, such systems can, at any time, be requested to\nswitch from its current operating mode to another mode (called \"new mode\") by\nreplacing the current set of tasks with that of the new-mode. Thereby, ensuring\nthat all the timing requirements are met not only requires that a\nschedulability test is performed on the tasks of each mode but also that (i) a\nprotocol for transitioning from one mode to another is specified and (ii) a\nschedulability test for each transition is performed. We propose two distinct\nprotocols that manage the mode transitions upon uniform and identical\nmultiprocessor platforms at run-time, each specific to distinct task\nrequirements. For each protocol, we formally establish schedulability analyses\nthat indicate beforehand whether all the timing requirements will be met during\nany mode transition of the system. This is performed assuming both\nFixed-Task-Priority and Fixed-Job-Priority schedulers.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.2094v1"
    },
    {
        "title": "Efficient and Playful Tools to Teach Unix to New Students",
        "authors": [
            "Matthieu Moy"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  Teaching Unix to new students is a common tasks in many higher schools. This\npaper presents an approach to such course where the students progress\nautonomously with the help of the teacher. The traditional textbook is\ncomplemented with a wiki, and the main thread of the course is a game, in the\nform of a treasure hunt. The course finishes with a lab exam, where students\nhave to perform practical manipulations similar to the ones performed during\nthe treasure hunt. The exam is graded fully automatically. This paper discusses\nthe motivations and advantages of the approach, and gives an overall view of\nthe tools we developed. The tools are available from the web, and open-source,\nhence re-usable outside the Ensimag.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.1717v1"
    },
    {
        "title": "Building XenoBuntu Linux Distribution for Teaching and Prototyping\n  Real-Time Operating Systems",
        "authors": [
            "Nabil Litayem",
            "Ahmed Ben Achballah",
            "Slim Ben Saoud"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  This paper describes the realization of a new Linux distribution based on\nUbuntu Linux and Xenomai Real-Time framework. This realization is motivated by\nthe eminent need of real-time systems in modern computer science courses. The\nmajority of the technical choices are made after qualitative comparison. The\nmain goal of this distribution is to offer standard Operating Systems (OS) that\ninclude Xenomai infrastructure and the essential tools to begin hard real-time\napplication development inside a convivial desktop environment. The released\nlive/installable DVD can be adopted to emulate several classic RTOS Application\nProgram Interfaces (APIs), directly use and understand real-time Linux in\nconvivial desktop environment and prototyping real-time embedded applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.2336v1"
    },
    {
        "title": "A New Proposed Dynamic Quantum with Re-Adjusted Round Robin Scheduling\n  Algorithm and Its Performance Analysis",
        "authors": [
            "H. S. Behera",
            "Rakesh Mohanty",
            "Debashree Nayak"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  Scheduling is the central concept used frequently in Operating System. It\nhelps in choosing the processes for execution. Round Robin (RR) is one of the\nmost widely used CPU scheduling algorithm. But, its performance degrades with\nrespect to context switching, which is an overhead and it occurs during each\nscheduling. Overall performance of the system depends on choice of an optimal\ntime quantum, so that context switching can be reduced. In this paper, we have\nproposed a new variant of RR scheduling algorithm, known as Dynamic Quantum\nwith Readjusted Round Robin (DQRRR) algorithm. We have experimentally shown\nthat performance of DQRRR is better than RR by reducing number of context\nswitching, average waiting time and average turn around time.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.3831v1"
    },
    {
        "title": "A New Dynamic Round Robin and SRTN Algorithm with Variable Original Time\n  Slice and Intelligent Time Slice for Soft Real Time Systems",
        "authors": [
            "H. S. Behera",
            "Simpi Patel",
            "Bijayalakshmi Panda"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  The main objective of the paper is to improve the Round Robin (RR) algorithm\nusing dynamic ITS by coalescing it with Shortest Remaining Time Next (SRTN)\nalgorithm thus reducing the average waiting time, average turnaround time and\nthe number of context switches. The original time slice has been calculated for\neach process based on its burst time.This is mostly suited for soft real time\nsystems where meeting of deadlines is desirable to increase its performance.\nThe advantage is that processes that are closer to their remaining completion\ntime will get more chances to execute and leave the ready queue. This will\nreduce the number of processes in the ready queue by knocking out short jobs\nrelatively faster in a hope to reduce the average waiting time, turn around\ntime and number of context switches. This paper improves the algorithm [8] and\nthe experimental analysis shows that the proposed algorithm performs better\nthan algorithm [6] and [8] when the processes are having an increasing order,\ndecreasing order and random order of burst time.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.3832v1"
    },
    {
        "title": "An Optimal Real-Time Scheduling Approach: From Multiprocessor to\n  Uniprocessor",
        "authors": [
            "Paul Regnier",
            "George Lima",
            "Ernesto Massa"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  An optimal solution to the problem of scheduling real-time tasks on a set of\nidentical processors is derived. The described approach is based on solving an\nequivalent uniprocessor real-time scheduling problem. Although there are other\nscheduling algorithms that achieve optimality, they usually impose prohibitive\npreemption costs. Unlike these algorithms, it is observed through simulation\nthat the proposed approach produces no more than three preemptions points per\njob.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.3523v1"
    },
    {
        "title": "Priority Based Dynamic Round Robin (PBDRR) Algorithm with Intelligent\n  Time Slice for Soft Real Time Systems",
        "authors": [
            "Rakesh Mohanty",
            "H. S. Behera",
            "Khusbu Patwari",
            "Monisha Dash",
            "M. Lakshmi Prasanna"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  In this paper, a new variant of Round Robin (RR) algorithm is proposed which\nis suitable for soft real time systems. RR algorithm performs optimally in\ntimeshared systems, but it is not suitable for soft real time systems. Because\nit gives more number of context switches, larger waiting time and larger\nresponse time. We have proposed a novel algorithm, known as Priority Based\nDynamic Round Robin Algorithm(PBDRR),which calculates intelligent time slice\nfor individual processes and changes after every round of execution. The\nproposed scheduling algorithm is developed by taking dynamic time quantum\nconcept into account. Our experimental results show that our proposed algorithm\nperforms better than algorithm in [8] in terms of reducing the number of\ncontext switches, average waiting time and average turnaround time.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.1736v1"
    },
    {
        "title": "Scheduling of Hard Real-Time Multi-Thread Periodic Tasks",
        "authors": [
            "Irina Iulia Lupu",
            "Joël Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  In this paper we study the scheduling of parallel and real-time recurrent\ntasks. Firstly, we propose a new parallel task model which allows recurrent\ntasks to be composed of several threads, each thread requires a single\nprocessor for execution and can be scheduled simultaneously. Secondly, we\ndefine several kinds of real-time schedulers that can be applied to our\nparallel task model. We distinguish between two scheduling classes:\nhierarchical schedulers and global thread schedulers. We present and prove\ncorrect an exact schedulability test for each class. Lastly, we also evaluate\nthe performance of our scheduling paradigm in comparison with Gang scheduling\nby means of simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.5080v1"
    },
    {
        "title": "Towards Bridging IoT and Cloud Services: Proposing Smartphones as Mobile\n  and Autonomic Service Gateways",
        "authors": [
            "Roya Golchay",
            "Frédéric Le Mouël",
            "Stéphane Frénot",
            "Julien Ponge"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  Computing is currently getting at the same time incredibly in the small with\nsensors/actuators embedded in our every- day objects and also greatly in the\nlarge with data and ser- vice clouds accessible anytime, anywhere. This\nInternet of Things is physically closed to the user but suffers from weak\nrun-time execution environments. Cloud Environments provide powerful data\nstorage and computing power but can not be easily accessed and integrate the\nfinal-user context- awareness. We consider smartphones are set to become the\nuniversal interface between these two worlds. In this position paper, we\npropose a middleware approach where smartphones provide service gateways to\nbridge the gap between IoT services and Cloud services. Since smartphones are\nmobile gateways, they should be able to (re)configure themself according to\ntheir place, things discovered around, and their own resources such battery.\nSeveral issues are discussed: collaborative event-based context management,\nadaptive and opportunistic service deployment and invocation, multi-criteria\n(user- and performance-oriented) optimization decision algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4786v1"
    },
    {
        "title": "Sufficient FTP Schedulability Test for the Non-Cyclic Generalized\n  Multiframe Task Model",
        "authors": [
            "Vandy Berten",
            "Joël Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  Our goal is to provide a sufficient schedulability test -ideally polynomial-\nfor the scheduling of Non-Cyclic Generalized Multiframe Task Model using\nFixed-Task-Priority schedulers. We report two first results: (i) we present and\nprove correct the critical instant for the Non-Cyclic Generalized Multiframe\nTask Model then (ii) we propose an algorithm which provides a sufficient (but\npseudo-polynomial) schedulability test.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5793v1"
    },
    {
        "title": "A New Round Robin Based Scheduling Algorithm for Operating Systems:\n  Dynamic Quantum Using the Mean Average",
        "authors": [
            "Abbas Noon",
            "Ali Kalakech",
            "Seifedine Kadry"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  Round Robin, considered as the most widely adopted CPU scheduling algorithm,\nundergoes severe problems directly related to quantum size. If time quantum\nchosen is too large, the response time of the processes is considered too high.\nOn the other hand, if this quantum is too small, it increases the overhead of\nthe CPU. In this paper, we propose a new algorithm, called AN, based on a new\napproach called dynamic-time-quantum; the idea of this approach is to make the\noperating systems adjusts the time quantum according to the burst time of the\nset of waiting processes in the ready queue. Based on the simulations and\nexperiments, we show that the new proposed algorithm solves the fixed time\nquantum problem and increases the performance of Round Robin.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.5348v1"
    },
    {
        "title": "What is an OS?",
        "authors": [
            "Abhijat Vichare"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  While the engineering of operating systems is well understood, their formal\nstructure and properties are not. The latter needs a clear definition of the\npurpose of an OS and an identification of the core. In this paper I offer\ndefinitions of the OS, processes and files, and present a few useful\nprinciples. The principles allow us to identify work like closure and\ncontinuation algorithms, in programming languages that is useful for the OS\nproblem. The definitions and principles should yield a symbolic, albeit\nsemiquantitative, framework that encompasses practice. Towards that end I\nspecialise the definitions to describe conventional OSes and identify the core\noperations for a single computer OS that can be used to express their\nalgorithms. The assumptions underlying the algorithms offer the design space\nframework. The paging and segmentation algorithms for conventional OSes are\nextracted from the framework as a check. Among the insights the emerge is that\nan OS is a constructive proof of equivalence between models of computation.\nClear and useful definitions and principles are the first step towards a fully\nquantitative structure of an OS.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.4451v2"
    },
    {
        "title": "Quest-V: A Virtualized Multikernel for High-Confidence Systems",
        "authors": [
            "Ye Li",
            "Matthew Danish",
            "Richard West"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  This paper outlines the design of `Quest-V', which is implemented as a\ncollection of separate kernels operating together as a distributed system on a\nchip. Quest-V uses virtualization techniques to isolate kernels and prevent\nlocal faults from affecting remote kernels. This leads to a high-confidence\nmultikernel approach, where failures of system subcomponents do not render the\nentire system inoperable. A virtual machine monitor for each kernel keeps track\nof shadow page table mappings that control immutable memory access\ncapabilities. This ensures a level of security and fault tolerance in\nsituations where a service in one kernel fails, or is corrupted by a malicious\nattack. Communication is supported between kernels using shared memory regions\nfor message passing. Similarly, device driver data structures are shareable\nbetween kernels to avoid the need for complex I/O virtualization, or\ncommunication with a dedicated kernel responsible for I/O. In Quest-V, device\ninterrupts are delivered directly to a kernel, rather than via a monitor that\ndetermines the destination. Apart from bootstrapping each kernel, handling\nfaults and managing shadow page tables, the monitors are not needed. This\ndiffers from conventional virtual machine systems in which a central monitor,\nor hypervisor, is responsible for scheduling and management of host resources\namongst a set of guest kernels. In this paper we show how Quest-V can implement\nnovel fault isolation and recovery techniques that are not possible with\nconventional systems. We also show how the costs of using virtualization for\nisolation of system services does not add undue overheads to the overall system\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.5136v1"
    },
    {
        "title": "AdSplit: Separating smartphone advertising from applications",
        "authors": [
            "Shashi Shekhar",
            "Michael Dietz",
            "Dan S. Wallach"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  A wide variety of smartphone applications today rely on third-party\nadvertising services, which provide libraries that are linked into the hosting\napplication. This situation is undesirable for both the application author and\nthe advertiser. Advertising libraries require additional permissions, resulting\nin additional permission requests to users. Likewise, a malicious application\ncould simulate the behavior of the advertising library, forging the user's\ninteraction and effectively stealing money from the advertiser. This paper\ndescribes AdSplit, where we extended Android to allow an application and its\nadvertising to run as separate processes, under separate user-ids, eliminating\nthe need for applications to request permissions on behalf of their advertising\nlibraries.\n  We also leverage mechanisms from Quire to allow the remote server to validate\nthe authenticity of client-side behavior. In this paper, we quantify the degree\nof permission bloat caused by advertising, with a study of thousands of\ndownloaded apps. AdSplit automatically recompiles apps to extract their ad\nservices, and we measure minimal runtime overhead. We also observe that most ad\nlibraries just embed an HTML widget within and describe how AdSplit can be\ndesigned with this in mind to avoid any need for ads to have native code.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.4030v1"
    },
    {
        "title": "Windows And Linux Operating Systems From A Security Perspective",
        "authors": [
            "Youssef Bassil"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Operating systems are vital system software that, without them, humans would\nnot be able to manage and use computer systems. In essence, an operating system\nis a collection of software programs whose role is to manage computer resources\nand provide an interface for client applications to interact with the different\ncomputer hardware. Most of the commercial operating systems available today on\nthe market have buggy code and they exhibit security flaws and vulnerabilities.\nIn effect, building a trusted operating system that can mostly resist attacks\nand provide a secure computing environment to protect the important assets of a\ncomputer is the goal of every operating system manufacturer. This paper deeply\ninvestigates the various security features of the two most widespread and\nsuccessful operating systems, Microsoft Windows and Linux. The different\nsecurity features, designs, and components of the two systems are to be covered\nelaborately, pin-pointing the key similarities and differences between them. In\ndue course, a head-to-head comparison is to be drawn for each security aspect,\nexposing the advantage of one system over the other.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.0197v1"
    },
    {
        "title": "Schedulability Test for Soft Real-Time Systems under Multiprocessor\n  Environment by using an Earliest Deadline First Scheduling Algorithm",
        "authors": [
            "Jagbeer Singh",
            "Satyendra Prasad Singh"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  This paper deals with the study of Earliest Deadline First (EDF) which is an\noptimal scheduling algorithm for uniprocessor real time systems use for\nscheduling the periodic task in soft real-time multiprocessor systems. In hard\nreal-time systems, a significant disparity exists EDF-based schemes and RMA\nscheduling (which is the only known way of optimally scheduling recurrent\nreal-time tasks on multiprocessors): on M processors, all known EDF variants\nhave utilization-based schedulability bounds of approximately M/2, while RMA\nalgorithms can fully utilize all processors. This is unfortunate because EDF\nbased algorithms entail lower scheduling and task migration overheads. In work\non hard real-time systems, it has been shown that this disparity in\nSchedulability can be lessened by placing caps on per task utilizations. Our\nmain contribution is a new EDF based scheme that ensures bounded deadline\ntardiness. In this scheme, per-task utilizations must be focused,but overall\nutilization need not be stricted. Our scheme should enable a wide range of soft\nreal-time applications to be scheduled with no constraints on total\nutilization. Also propose techniques and heuristics that can be used to reduce\ntardiness as well as increase the efficiency of task.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.0124v1"
    },
    {
        "title": "Age Based User Interface in Mobile Operating System",
        "authors": [
            "Sumit Sharma",
            "Rohitt Sharma",
            "Paramjit Singh",
            "Aditya Mahajan"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  This paper proposes the creation of different interfaces in the mobile\noperating system for different age groups. The different age groups identified\nare kids, elderly people and all others. The motive behind creating different\ninterfaces is to make the smartphones of today's world usable to all age\ngroups.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.1687v1"
    },
    {
        "title": "Proposed Challenges And Areas of Concern in Operating System Research\n  and Development",
        "authors": [
            "Plawan Kumar Rath",
            "G. N. Anil"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Computers are a very important part of our lives and the major reason why\nthey have been such a success is because of the excellent graphical operating\nsystems that run on these powerful machines. As the computer hardware is\nbecoming more and more powerful, it is also vital to keep the software updated\nin order to utilize the hardware of the system efficiently and make it faster\nand smarter. This paper highlights some core issues that if dealt with in the\noperating system level would make use of the full potential of the computer\nhardware and provide an excellent user experience.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.6423v1"
    },
    {
        "title": "Energy-Aware Task Partitioning on Heterogeneous Multiprocessor Platforms",
        "authors": [
            "Elsayed Saad",
            "Medhat Awadalla",
            "Mohamed Shalan",
            "Abdullah Elewi"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Efficient task partitioning plays a crucial role in achieving high\nperformance at multiprocessor plat forms. This paper addresses the problem of\nenergy-aware static partitioning of periodic real-time tasks on heterogeneous\nmultiprocessor platforms. A Particle Swarm Optimization variant based on\nMin-min technique for task partitioning is proposed. The proposed approach aims\nto minimize the overall energy consumption, meanwhile avoid deadline\nviolations. An energy-aware cost function is proposed to be considered in the\nproposed approach. Extensive simulations and comparisons are conducted in order\nto validate the effectiveness of the proposed technique. The achieved results\ndemonstrate that the proposed partitioning scheme significantly surpasses\nprevious approaches in terms of both number of iterations and energy savings.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.0396v1"
    },
    {
        "title": "A Secure Dynamic Job Scheduling on Smart Grid using RSA Algorithm",
        "authors": [
            "P. Radha Krishna Reddy",
            "Ashim Roy",
            "G. Sireesha",
            "Ismatha Begum",
            "S. Siva Ramaiah"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Grid computing is a computation methodology using group of clusters connected\nover high-speed networks that involves coordinating and sharing computational\npower, data storage and network resources. Integrating a set of clusters of\nworkstations into one large computing environment can improve the availability\nof computing power. The goal of scheduling is to achieve highest possible\nsystem throughput and to match the application need with the available\ncomputing resources. A secure scheduling model is presented, that performs job\ngrouping activity at runtime. In a Grid environment, security is necessary\nbecause grid is a dynamic environment and participates are independent bodies\nwith different policies, objectives and requirements. Authentication should be\nverified for Grid resource owners as well as resource requesters before they\nare allowed to join in scheduling activities. In order to achieve secure\nresource and job scheduling including minimum processing time and maximum\nresource utilization, A Secure Resource by using RSA algorithm on Networking\nand Job Scheduling model with Job Grouping strategy(JGS) in Grid Computing has\nbeen proposed. The result shows significant improvement in the processing time\nof jobs and resource utilization as compared to dynamic job grouping (DJG)\nbased scheduling on smart grids (SG).\n",
        "pdf_link": "http://arxiv.org/pdf/1207.1591v1"
    },
    {
        "title": "Performance Evaluation of Flash File Systems",
        "authors": [
            "Pierre Olivier",
            "Jalil Boukhobza",
            "Eric Senn"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Today, flash memory are strongly used in the embedded system domain. NAND\nflash memories are the building block of main secondary storage systems. Such\nmemories present many benefits in terms of data density, I/O performance, shock\nresistance and power consumption. Nevertheless, flash does not come without\nconstraints: the write / erase granularity asymmetry and the limited lifetime\nbring the need for specific management. This can be done through the operating\nsystem using dedicated Flash File Systems (FFSs). In this document, we present\ngeneral concepts about FFSs, and implementations example that are JFFS2, YAFFS2\nand UBIFS, the most commonly used flash file systems. Then we give performance\nevaluation results for these FFSs.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.6390v1"
    },
    {
        "title": "Disk Scheduling: Selection of Algorithm",
        "authors": [
            "S. Yashvir",
            "Om Prakash"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  The objective of this paper is to take some aspects of disk scheduling and\nscheduling algorithms. The disk scheduling is discussed with a sneak peak in\ngeneral and selection of algorithm in particular.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.6447v1"
    },
    {
        "title": "Multicore Dynamic Kernel Modules Attachment Technique for Kernel\n  Performance Enhancement",
        "authors": [
            "Mohamed Farag"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Traditional monolithic kernels dominated kernel structures for long time\nalong with small sized kernels,few hardware companies and limited kernel\nfunctionalities. Monolithic kernel structure was not applicable when the number\nof hardware companies increased and kernel services consumed by different users\nfor many purposes. One of the biggest disadvantages of the monolithic kernels\nis the inflexibility due to the need to include all the available modules in\nkernel compilation causing high time consuming. Lately, new kernel structure\nwas introduced through multicore operating systems. Unfortunately, many\nmulticore operating systems such as barrelfish and FOS are experimental. This\npaper aims to simulate the performance of multicore hybrid kernels through\ndynamic kernel module customized attachment/ deattachment for multicore\nmachines. In addition, this paper proposes a new technique for loading dynamic\nkernel modules based on the user needs and machine capabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.4840v1"
    },
    {
        "title": "A Generic Checkpoint-Restart Mechanism for Virtual Machines",
        "authors": [
            "Rohan Garg",
            "Komal Sodha",
            "Gene Cooperman"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  It is common today to deploy complex software inside a virtual machine (VM).\nSnapshots provide rapid deployment, migration between hosts, dependability\n(fault tolerance), and security (insulating a guest VM from the host). Yet, for\neach virtual machine, the code for snapshots is laboriously developed on a\nper-VM basis. This work demonstrates a generic checkpoint-restart mechanism for\nvirtual machines. The mechanism is based on a plugin on top of an unmodified\nuser-space checkpoint-restart package, DMTCP. Checkpoint-restart is\ndemonstrated for three virtual machines: Lguest, user-space QEMU, and KVM/QEMU.\nThe plugins for Lguest and KVM/QEMU require just 200 lines of code. The Lguest\nkernel driver API is augmented by 40 lines of code. DMTCP checkpoints\nuser-space QEMU without any new code. KVM/QEMU, user-space QEMU, and DMTCP need\nno modification. The design benefits from other DMTCP features and plugins.\nExperiments demonstrate checkpoint and restart in 0.2 seconds using forked\ncheckpointing, mmap-based fast-restart, and incremental Btrfs-based snapshots.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.1787v1"
    },
    {
        "title": "Adaptive Scheduling in Real-Time Systems Through Period Adjustment",
        "authors": [
            "Shri Prakash Dwivedi"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Real time system technology traditionally developed for safety critical\nsystems, has now been extended to support multimedia systems and virtual\nreality. A large number of real-time application, related to multimedia and\nadaptive control system, require more flexibility than classical real-time\ntheory usually permits. This paper proposes an efficient adaptive scheduling\nframework in real-time systems based on period adjustment. Under this model\nperiodic task can change their execution rates based on their importance value\nto keep the system underloaded. We propose Period_Adjust algorithm, which\nconsider the tasks whose periods are bounded as well as the tasks whose periods\nare not bounded.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.3502v1"
    },
    {
        "title": "Energy Minimization for Parallel Real-Time Systems with Malleable Jobs\n  and Homogeneous Frequencies",
        "authors": [
            "Nathan Fisher",
            "Joël Goossens",
            "Pradeep M. Hettiarachchi",
            "Antonio Paolillo"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  In this work, we investigate the potential utility of parallelization for\nmeeting real-time constraints and minimizing energy. We consider malleable Gang\nscheduling of implicit-deadline sporadic tasks upon multiprocessors. We first\nshow the non-necessity of dynamic voltage/frequency regarding optimality of our\nscheduling problem. We adapt the canonical schedule for DVFS multiprocessor\nplatforms and propose a polynomial-time optimal processor/frequency-selection\nalgorithm. We evaluate the performance of our algorithm via simulations using\nparameters obtained from a hardware testbed implementation. Our algorithm has\nup to a 60 watt decrease in power consumption over the optimal non-parallel\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.1747v1"
    },
    {
        "title": "Capturing Information Flows inside Android and Qemu Environments",
        "authors": [
            "Marco Sironi",
            "Francesco Tisato"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  The smartphone market has grown so wide that it assumed a strategic\nrelevance. Today the most common smartphone OSs are Google's Android and\nApple's iOS. The former is particularly interesting due to its open source\nnature, that allows everyone to deeply inspect every aspect of the OS. Android\nsource code is also bundled with an hardware emulator, based on the open source\nsoftware Qemu, that allows the user to run the Android OS without the need of a\nphysical device. We first present a procedure to extract information flows from\na generic system. We then focus on Android and Qemu architectures and their\nlogging infrastructures. Finally, we detail what happens inside an Android\ndevice in a particular scenario: the system boot.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.5109v1"
    },
    {
        "title": "LFTL: A multi-threaded FTL for a Parallel IO Flash Card under Linux",
        "authors": [
            " Srimugunthan",
            "K. Gopinath",
            "Giridhar Appaji Nag Yasa"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  New PCI-e flash cards and SSDs supporting over 100,000 IOPs are now\navailable, with several usecases in the design of a high performance storage\nsystem. By using an array of flash chips, arranged in multiple banks, large\ncapacities are achieved. Such multi-banked architecture allow parallel read,\nwrite and erase operations. In a raw PCI-e flash card, such parallelism is\ndirectly available to the software layer. In addition, the devices have\nrestrictions such as, pages within a block can only be written sequentially.\nThe devices also have larger minimum write sizes (greater than 4KB). Current\nflash translation layers (FTLs) in Linux are not well suited for such devices\ndue to the high device speeds, architectural restrictions as well as other\nfactors such as high lock contention. We present a FTL for Linux that takes\ninto account the hardware restrictions, that also exploits the parallelism to\nachieve high speeds. We also consider leveraging the parallelism for garbage\ncollection by scheduling the garbage collection activities on idle banks. We\npropose and evaluate an adaptive method to vary the amount of garbage\ncollection according to the current I/O load on the device.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.5502v1"
    },
    {
        "title": "Survey of Server Virtualization",
        "authors": [
            "Radhwan Y Ameen",
            "Asmaa Y. Hamo"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Virtualization is a term that refers to the abstraction of computer\nresources. The purpose of virtual computing environment is to improve resource\nutilization by providing a unified integrated operating platform for users and\napplications based on aggregation of heterogeneous and autonomous resources.\nMore recently, virtualization at all levels (system, storage, and network)\nbecame important again as a way to improve system security, reliability and\navailability, reduce costs, and provide greater flexibility. Virtualization has\nrapidly become a go-to technology for increasing efficiency in the data center.\nWith virtualization technologies providing tremendous flexibility, even\ndisparate architectures may be deployed on a single machine without\ninterference This paper explains the basics of server virtualization and\naddresses pros and cons of virtualization\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3557v1"
    },
    {
        "title": "Making I/O Virtualization Easy with Device Files",
        "authors": [
            "Ardalan Amiri Sani",
            "Sreekumar Nair",
            "Lin Zhong",
            "Quinn Jacobson"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Personal computers have diverse and fast-evolving I/O devices, making their\nI/O virtualization different from that of servers and data centers. In this\npaper, we present our recent endeavors in simplifying I/O virtualization for\npersonal computers. Our key insight is that many operating systems, including\nUnix-like ones, abstract I/O devices as device files. There is a small and\nstable set of operations on device files, therefore, I/O virtualization at the\ndevice file boundary requires a one-time effort to support various I/O devices.\n  We present devirtualization, our design of I/O virtualization at the device\nfile boundary and its implementation for Linux/x86 systems. We are able to\nvirtualize various GPUs, input devices, cameras, and audio devices with fewer\nthan 4900 LoC, of which only about 300 are specific to I/O device classes. Our\nmeasurements show that devirtualized devices achieve interactive performance\nindistinguishable from native ones by human users, even when running 3D HD\ngames.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3771v1"
    },
    {
        "title": "Invasive Computing - Common Terms and Granularity of Invasion",
        "authors": [
            "Jürgen Teich",
            "Wolfgang Schröder-Preikschat",
            "Andreas Herkersdorf"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Future MPSoCs with 1000 or more processor cores on a chip require new means\nfor resource-aware programming in order to deal with increasing imperfections\nsuch as process variation, fault rates, aging effects, and power as well as\nthermal problems. On the other hand, predictable program executions are\nthreatened if not impossible if no proper means of resource isolation and\nexclusive use may be established on demand. In view of these problems and\nmenaces, invasive computing enables an application programmer to claim for\nprocessing resources and spread computations to claimed processors dynamically\nat certain points of the program execution.\n  Such decisions may be depending on the degree of application parallelism and\nthe state of the underlying resources such as utilization, load, and\ntemperature, but also with the goal to provide predictable program execution on\nMPSoCs by claiming processing resources exclusively as the default and thus\neliminating interferences and creating the necessary isolation between multiple\nconcurrently running applications. For achieving this goal, invasive computing\nintroduces new programming constructs for resource-aware programming that\nmeanwhile, for testing purpose, have been embedded into the parallel computing\nlanguage X10 as developed by IBM using a library-based approach.\n  This paper presents major ideas and common terms of invasive computing as\ninvestigated by the DFG Transregional Collaborative Research Centre TR89.\nMoreoever, a reflection is given on the granularity of resources that may be\nrequested by invasive programs.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.6067v1"
    },
    {
        "title": "Network Control Systems RTAI framework A Review",
        "authors": [
            "Deepika Bhatia",
            "Urmila Shrawankar"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  With the advancement in the automation industry, to perform complex remote\noperations is required. Advancements in the networking technology has led to\nthe development of different architectures to implement control from a large\ndistance. In various control applications of the modern industry, the agents,\nsuch as sensors, actuators, and controllers are basically geographically\ndistributed. For efficient working of a control application, all of the agents\nhave to exchange information through a communication media. At present, an\nincreasing number of distributed control systems are based on platforms made up\nof conventional PCs running open-source real-time operating systems. Often,\nthese systems needed to have networked devices supporting synchronized\noperations with respect to each node. A framework is studied that relies on\nstandard software and protocol as RTAI, EtherCAT, RTnet and IEEE 1588. RTAI and\nits various protocols are studied in network control systems environment.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7001v1"
    },
    {
        "title": "Partitioned scheduling of multimode multiprocessor real-time systems\n  with temporal isolation",
        "authors": [
            "Joël Goossens",
            "Pascal Richard"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  We consider the partitioned scheduling problem of multimode real-time systems\nupon identical multiprocessor platforms. During the execution of a multimode\nsystem, the system can change from one mode to another such that the current\ntask set is replaced with a new one. In this paper, we consider a synchronous\ntransition protocol in order to take into account mode-independent tasks, i.e.,\ntasks of which the execution pattern must not be jeopardized by the mode\nchanges. We propose two methods for handling mode changes in partitioned\nscheduling. The first method is offline/optimal and computes a static\nallocation of tasks schedulable and respecting both tasks and transition\ndeadlines (if any). The second approach is subject to a sufficient condition in\norder to ensure online First Fit based allocation to satisfy the timing\nconstraints.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.1316v1"
    },
    {
        "title": "Intensional view of General Single Processor Operating Systems",
        "authors": [
            "Abhijat Vichare"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Operating systems are currently viewed ostensively. As a result they mean\ndifferent things to different people. The ostensive character makes it is hard\nto understand OSes formally. An intensional view can enable better formal work,\nand also offer constructive support for some important problems, e.g. OS\narchitecture. This work argues for an intensional view of operating systems. It\nproposes to overcome the current ostensive view by defining an OS based on\nformal models of computation, and also introduces some principles. Together\nthese are used to develop a framework of algorithms of single processor OS\nstructure using an approach similar to function level programming. In this\nabridged paper we illustrate the essential approach, discuss some advantages\nand limitations and point out some future possibilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.1199v1"
    },
    {
        "title": "Simulation of an Optimum Multilevel Dynamic Round Robin Scheduling\n  Algorithm",
        "authors": [
            "Neetu Goel",
            "R. B. Garg"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  CPU scheduling has valiant effect on resource utilization as well as overall\nquality of the system. Round Robin algorithm performs optimally in time shared\nsystems, but it performs more number of context switches, larger waiting time\nand larger response time. In order to simulate the behavior of various CPU\nscheduling algorithms and to improve Round Robin scheduling algorithm using\ndynamic time slice concept, in this paper we produce the implementation of new\nCPU scheduling algorithm called An Optimum Multilevel Dynamic Round Robin\nScheduling (OMDRRS), which calculates intelligent time slice and warps after\nevery round of execution. The results display the robustness of this software,\nespecially for academic, research and experimental use, as well as proving the\ndesirability and efficiency of the probabilistic algorithm over the other\nexisting techniques and it is observed that this OMDRRS projects good\nperformance as compared to the other existing CPU scheduling algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.3096v1"
    },
    {
        "title": "The Quest-V Separation Kernel for Mixed Criticality Systems",
        "authors": [
            "Ye Li",
            "Richard West",
            "Eric Missimer"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Multi- and many-core processors are becoming increasingly popular in embedded\nsystems. Many of these processors now feature hardware virtualization\ncapabilities, such as the ARM Cortex A15, and x86 processors with Intel VT-x or\nAMD-V support. Hardware virtualization offers opportunities to partition\nphysical resources, including processor cores, memory and I/O devices amongst\nguest virtual machines. Mixed criticality systems and services can then\nco-exist on the same platform in separate virtual machines. However,\ntraditional virtual machine systems are too expensive because of the costs of\ntrapping into hypervisors to multiplex and manage machine physical resources on\nbehalf of separate guests. For example, hypervisors are needed to schedule\nseparate VMs on physical processor cores. In this paper, we discuss the design\nof the Quest-V separation kernel, that partitions services of different\ncriticalities in separate virtual machines, or sandboxes. Each sandbox\nencapsulates a subset of machine physical resources that it manages without\nrequiring intervention of a hypervisor. Moreover, a hypervisor is not needed\nfor normal operation, except to bootstrap the system and establish\ncommunication channels between sandboxes.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.6298v1"
    },
    {
        "title": "Predictable Migration and Communication in the Quest-V Multikernel",
        "authors": [
            "Ye Li",
            "Eric Missimer",
            "Richard West"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Quest-V is a system we have been developing from the ground up, with\nobjectives focusing on safety, predictability and efficiency. It is designed to\nwork on emerging multicore processors with hardware virtualization support.\nQuest-V is implemented as a \"distributed system on a chip\" and comprises\nmultiple sandbox kernels. Sandbox kernels are isolated from one another in\nseparate regions of physical memory, having access to a subset of processing\ncores and I/O devices. This partitioning prevents system failures in one\nsandbox affecting the operation of other sandboxes. Shared memory channels\nmanaged by system monitors enable inter-sandbox communication.\n  The distributed nature of Quest-V means each sandbox has a separate physical\nclock, with all event timings being managed by per-core local timers. Each\nsandbox is responsible for its own scheduling and I/O management, without\nrequiring intervention of a hypervisor.\n  In this paper, we formulate bounds on inter-sandbox communication in the\nabsence of a global scheduler or global system clock. We also describe how\naddress space migration between sandboxes can be guaranteed without violating\nservice constraints. Experimental results on a working system show the\nconditions under which Quest-V performs real-time communication and migration.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.6301v1"
    },
    {
        "title": "Quest-V: A Virtualized Multikernel for Safety-Critical Real-Time Systems",
        "authors": [
            "Richard West",
            "Ye Li",
            "Eric Missimer"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Modern processors are increasingly featuring multiple cores, as well as\nsupport for hardware virtualization. While these processors are common in\ndesktop and server-class computing, they are less prevalent in embedded and\nreal-time systems. However, smartphones and tablet PCs are starting to feature\nmulticore processors with hardware virtualization. If the trend continues, it\nis possible that future real-time systems will feature more sophisticated\nprocessor architectures. Future automotive or avionics systems, for example,\ncould replace complex networks of uniprocessors with consolidated services on a\nsmaller number of multicore processors. Likewise, virtualization could be used\nto isolate services and increase the availability of a system even when\nfailures occur.\n  This paper investigates whether advances in modern processor technologies\noffer new opportunities to rethink the design of real-time operating systems.\nWe describe some of the design principles behind Quest-V, which is being used\nas an exploratory vehicle for real-time system design on multicore processors\nwith hardware virtualization capabilities. While not all embedded systems\nshould assume such features, a case can be made that more robust,\nsafety-critical systems can be built to use hardware virtualization without\nincurring significant overheads.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.6349v1"
    },
    {
        "title": "File System - A Component of Operating System",
        "authors": [
            "Brijender Kahanwal",
            "Tejinder Pal Singh",
            "Ruchira Bhargava",
            "Girish Pal Singh"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  The file system provides the mechanism for online storage and access to file\ncontents, including data and programs. This paper covers the high-level details\nof file systems, as well as related topics such as the disk cache, the file\nsystem interface to the kernel, and the user-level APIs that use the features\nof the file system. It will give you a thorough understanding of how a file\nsystem works in general. The main component of the operating system is the file\nsystem. It is used to create, manipulate, store, and retrieve data. At the\nhighest level, a file system is a way to manage information on a secondary\nstorage medium. There are so many layers under and above the file system. All\nthe layers are to be fully described here. This paper will give the explanatory\nknowledge of the file system designers and the researchers in the area. The\ncomplete path from the user process to secondary storage device is to be\nmentioned. File system is the area where the researchers are doing lot of job\nand there is always a need to do more work. The work is going on for the\nefficient, secure, energy saving techniques for the file systems. As we know\nthat the hardware is going to be fast in performance and low-priced day by day.\nThe software is not built to comeback with the hardware technology. So there is\na need to do research in this area to bridge the technology gap.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.1810v1"
    },
    {
        "title": "Towards the Framework of the File Systems Performance Evaluation\n  Techniques and the Taxonomy of Replay Traces",
        "authors": [
            "Brijender Kahanwal",
            "Tejinder Pal Singh"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  This is the era of High Performance Computing (HPC). There is a great demand\nof the best performance evaluation techniques for the file and storage systems.\nThe task of evaluation is both necessary and hard. It gives in depth analysis\nof the target system and that becomes the decision points for the users. That\nis also helpful for the inventors or developers to find out the bottleneck in\ntheir systems. In this paper many performance evaluation techniques are\ndescribed for file and storage system evaluation and the main stress is given\non the important one that is replay traces. A survey has been done for the\nperformance evaluation techniques used by the researchers and on the replay\ntraces. And the taxonomy of the replay traces is described. The some of the\npopular replay traces are just like, Tracefs [1], //Trace [2], Replayfs [3] and\nVFS Interceptor [12]. At last we have concluded all the features that must be\nconsidered when we are going to develop the new tool for the replay traces. The\ncomplete work of this paper shows that the storage system developers must care\nabout all the techniques which can be used for the performance evaluation of\nthe file systems. So they can develop highly efficient future file and storage\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.1822v1"
    },
    {
        "title": "Cache-aware static scheduling for hard real-time multicore systems based\n  on communication affinities",
        "authors": [
            "Lilia Zaourar",
            "Mathieu Jan",
            "Maurice Pitel"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  The growing need for continuous processing capabilities has led to the\ndevelopment of multicore systems with a complex cache hierarchy. Such multicore\nsystems are generally designed for improving the performance in average case,\nwhile hard real-time systems must consider worst-case scenarios. An open\nchallenge is therefore to efficiently schedule hard real-time tasks on a\nmulticore architecture. In this work, we propose a mathematical formulation for\ncomputing a static scheduling that minimize L1 data cache misses between hard\nreal-time tasks on a multicore architecture using communication affinities.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.4509v1"
    },
    {
        "title": "Rio: A System Solution for Sharing I/O between Mobile Systems",
        "authors": [
            "Ardalan Amiri Sani",
            "Kevin Boos",
            "Min Hong Yun",
            "Lin Zhong"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Mobile systems are equipped with a diverse collection of I/O devices,\nincluding cameras, microphones, sensors, and modems. There exist many novel use\ncases for allowing an application on one mobile system to utilize I/O devices\nfrom another. This paper presents Rio, an I/O sharing solution that supports\nunmodified applications and exposes all the functionality of an I/O device for\nsharing. Rio's design is common to many classes of I/O devices, thus\nsignificantly reducing the engineering effort to support new I/O devices. Our\nimplementation of Rio on Android consists of 6700 total lines of code and\nsupports four I/O classes with fewer than 450 class-specific lines of code. Rio\nalso supports I/O sharing between mobile systems of different form factors,\nincluding smartphones and tablets. We show that Rio achieves performance close\nto that of local I/O for audio, sensors, and modems, but suffers noticeable\nperformance degradation for camera due to network throughput limitations\nbetween the two systems, which is likely to be alleviated by emerging wireless\nstandards.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.4931v1"
    },
    {
        "title": "Transparent Checkpoint-Restart for Hardware-Accelerated 3D Graphics",
        "authors": [
            "Samaneh Kazemi Nafchi",
            "Rohan Garg",
            "Gene Cooperman"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Providing fault-tolerance for long-running GPU-intensive jobs requires\napplication-specific solutions, and often involves saving the state of complex\ndata structures spread among many graphics libraries. This work describes a\nmechanism for transparent GPU-independent checkpoint-restart of 3D graphics.\nThe approach is based on a record-prune-replay paradigm: all OpenGL calls\nrelevant to the graphics driver state are recorded; calls not relevant to the\ninternal driver state as of the last graphics frame prior to checkpoint are\ndiscarded; and the remaining calls are replayed on restart. A previous approach\nfor OpenGL 1.5, based on a shadow device driver, required more than 78,000\nlines of OpenGL-specific code. In contrast, the new approach, based on\nrecord-prune-replay, is used to implement the same case in just 4,500 lines of\ncode. The speed of this approach varies between 80 per cent and nearly 100 per\ncent of the speed of the native hardware acceleration for OpenGL 1.5, as\nmeasured when running the ioquake3 game under Linux. This approach has also\nbeen extended to demonstrate checkpointing of OpenGL 3.0 for the first time,\nwith a demonstration for PyMol, for molecular visualization.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.6650v2"
    },
    {
        "title": "LWRP: Low Power Consumption Weighting Replacement Policy using Buffer\n  Memory",
        "authors": [
            "S. R. Bhalgama",
            "C. C. Kavar",
            "S. S. Parmar"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  As the performance gap between memory and processors has increased, then it\nleads to the poor performance. Efficient virtual memory can overcome this\nproblem. And the efficiency of virtual memory depends on the replacement policy\nused for cache. In this paper, our algorithm not only based on the time to last\naccess and frequency index but, we also consider the power consumption. We show\nthat Low Power Consumption Weighting Replacement Policy (LWRP) has better\nperformance and low power consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.0631v1"
    },
    {
        "title": "Formal Description of Components in Operating Systems",
        "authors": [
            "Asen Petkov Iliev"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  The contemporary development of hardware components is a prerequisite for\nincreasing the concentration of computing power. System software is developing\nat a much slower pace. To use available resources efficiently modeling is\nrequired. Formalization of elements, present in the material, provides the\nbasis for modeling. Examples are presented to demonstrate the efficiency of the\nconcept.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.4929v1"
    },
    {
        "title": "Design and Performance Evaluation of an Optimized Disk Scheduling\n  Algorithm (ODSA)",
        "authors": [
            "Sourav Kumar Bhoi",
            "Sanjaya Kumar Panda",
            "Imran Hossain Faruk"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  Management of disk scheduling is a very important aspect of operating system.\nPerformance of the disk scheduling completely depends on how efficient is the\nscheduling algorithm to allocate services to the request in a better manner.\nMany algorithms (FIFO, SSTF, SCAN, C-SCAN, LOOK, etc.) are developed in the\nrecent years in order to optimize the system disk I/O performance. By reducing\nthe average seek time and transfer time, we can improve the performance of disk\nI/O operation. In our proposed algorithm, Optimize Disk Scheduling Algorithm\n(ODSA) is taking less average seek time and transfer time as compare to other\ndisk scheduling algorithms (FIFO, SSTF, SCAN, C-SCAN, LOOK, etc.), which\nenhances the efficiency of the disk performance in a better manner.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.0334v1"
    },
    {
        "title": "A Group based Time Quantum Round Robin Algorithm using Min-Max Spread\n  Measure",
        "authors": [
            "Sanjaya Kumar Panda",
            "Debasis Dash",
            "Jitendra Kumar Rout"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  Round Robin (RR) Scheduling is the basis of time sharing environment. It is\nthe combination of First Come First Served (FCFS) scheduling algorithm and\npreemption among processes. It is basically used in a time sharing operating\nsystem. It switches from one process to another process in a time interval. The\ntime interval or Time Quantum (TQ) is fixed for all available processes. So,\nthe larger process suffers from Context Switches (CS). To increase efficiency,\nwe have to select different TQ for processes. The main objective of RR is to\nreduce the CS, maximize the utilization of CPU and minimize the turn around and\nthe waiting time. In this paper, we have considered different TQ for a group of\nprocesses. It reduces CS as well as enhancing the performance of RR algorithm.\nTQ can be calculated using min-max dispersion measure. Our experimental\nanalysis shows that Group Based Time Quantum (GBTQ) RR algorithm performs\nbetter than existing RR algorithm with respect to Average Turn Around Time\n(ATAT), Average Waiting Time (AWT) and CS.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.0335v1"
    },
    {
        "title": "Task & Resource Self-adaptive Embedded Real-time Operating System\n  Microkernel for Wireless Sensor Nodes",
        "authors": [
            "Kexing Xing",
            "Decheng Zuo",
            "Haiying Zhou",
            "Hou Kun-Mean"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  Wireless Sensor Networks (WSNs) are used in many application fields, such as\nmilitary, healthcare, environment surveillance, etc. The WSN OS based on\nevent-driven model doesn't support real-time and multi-task application types\nand the OSs based on thread-driven model consume much energy because of\nfrequent context switch. Due to the high-dense and large-scale deployment of\nsensor nodes, it is very difficult to collect sensor nodes to update their\nsoftware. Furthermore, the sensor nodes are vulnerable to security attacks\nbecause of the characteristics of broadcast communication and unattended\napplication. This paper presents a task and resource self-adaptive embedded\nreal-time microkernel, which proposes hybrid programming model and offers a\ntwo-level scheduling strategy to support real-time multi-task correspondingly.\nA communication scheme, which takes the \"tuple\" space and \"IN/OUT\" primitives\nfrom \"LINDA\", is proposed to support some collaborative and distributed tasks.\nIn addition, this kernel implements a run-time over-the-air updating mechanism\nand provides a security policy to avoid the attacks and ensure the reliable\noperation of nodes. The performance evaluation is proposed and the experiential\nresults show this kernel is task-oriented and resource-aware and can be used\nfor the applications of event-driven and real-time multi-task.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.5010v1"
    },
    {
        "title": "File System Design Approaches",
        "authors": [
            "Brijender Kahanwal"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  In this article, the file system development design approaches are discussed.\nThe selection of the file system design approach is done according to the needs\nof the developers what are the needed requirements and specifications for the\nnew design. It allowed us to identify where our proposal fitted in with\nrelation to current and past file system development. Our experience with file\nsystem development is limited so the research served to identify the different\ntechniques that can be used. The variety of file systems encountered show what\nan active area of research file system development is. The file systems may be\nfrom one of the two fundamental categories. In one category, the file system is\ndeveloped in user space and runs as a user process. Another file system may be\ndeveloped in the kernel space and runs as a privileged process. Another one is\nthe mixed approach in which we can take the advantages of both aforesaid\napproaches. Each development option has its own pros and cons. In this article,\nthese design approaches are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.5976v1"
    },
    {
        "title": "Assessment of Response Time for New Multi Level Feedback Queue Scheduler",
        "authors": [
            "M. V. Panduranga Rao",
            "K. C. Shet"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  Response time is one of the characteristics of scheduler, happens to be a\nprominent attribute of any CPU scheduling algorithm. The proposed New Multi\nLevel Feedback Queue [NMLFQ] Scheduler is compared with dynamic, real time,\nDependent Activity Scheduling Algorithm (DASA) and Lockes Best Effort\nScheduling Algorithm (LBESA). We abbreviated beneficial result of NMLFQ\nscheduler in comparison with dynamic best effort schedulers with respect to\nresponse time.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.0990v1"
    },
    {
        "title": "Glider: A GPU Library Driver for Improved System Security",
        "authors": [
            "Ardalan Amiri Sani",
            "Lin Zhong",
            "Dan S. Wallach"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  Legacy device drivers implement both device resource management and\nisolation. This results in a large code base with a wide high-level interface\nmaking the driver vulnerable to security attacks. This is particularly\nproblematic for increasingly popular accelerators like GPUs that have large,\ncomplex drivers. We solve this problem with library drivers, a new driver\narchitecture. A library driver implements resource management as an untrusted\nlibrary in the application process address space, and implements isolation as a\nkernel module that is smaller and has a narrower lower-level interface (i.e.,\ncloser to hardware) than a legacy driver. We articulate a set of device and\nplatform hardware properties that are required to retrofit a legacy driver into\na library driver. To demonstrate the feasibility and superiority of library\ndrivers, we present Glider, a library driver implementation for two GPUs of\npopular brands, Radeon and Intel. Glider reduces the TCB size and attack\nsurface by about 35% and 84% respectively for a Radeon HD 6450 GPU and by about\n38% and 90% respectively for an Intel Ivy Bridge GPU. Moreover, it incurs no\nperformance cost. Indeed, Glider outperforms a legacy driver for applications\nrequiring intensive interactions with the device driver, such as applications\nusing the OpenGL immediate mode API.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.3777v1"
    },
    {
        "title": "OS-level Failure Injection with SystemTap",
        "authors": [
            "Camille Coti",
            "Nicolas Greneche"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Failure injection in distributed systems has been an important issue to\nexperiment with robust, resilient distributed systems. In order to reproduce\nreal-life conditions, parts of the application must be killed without letting\nthe operating system close the existing network communications in a \"clean\"\nway. When a process is simply killed, the OS closes them. SystemTap is a an\ninfrastructure that probes the Linux kernel's internal calls. If processes are\nkilled at kernel-level, they can be destroyed without letting the OS do\nanything else. In this paper, we present a kernel-level failure injection\nsystem based on SystemTap. We present how it can be used to implement\ndeterministic and probabilistic failure scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.01509v1"
    },
    {
        "title": "Protecting Memory-Performance Critical Sections in Soft Real-Time\n  Applications",
        "authors": [
            "Heechul Yun",
            "Santosh Gondi",
            "Siddhartha Biswas"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Soft real-time applications such as multimedia applications often show bursty\nmemory access patterns---regularly requiring a high memory bandwidth for a\nshort duration of time. Such a period is often critical for timely data\nprocessing. Hence, we call it a memory-performance critical section.\nUnfortunately, in multicore architecture, non-real-time applications on\ndifferent cores may also demand high memory bandwidth at the same time, which\ncan substantially increase the time spent on the memory performance critical\nsections.\n  In this paper, we present BWLOCK, user-level APIs and a memory bandwidth\ncontrol mechanism that can protect such memory performance critical sections of\nsoft real-time applications. BWLOCK provides simple lock like APIs to declare\nmemory-performance critical sections. If an application enters a\nmemory-performance critical section, the memory bandwidth control system then\ndynamically limit other cores' memory access rates to protect memory\nperformance of the application until the critical section finishes.\n  From case studies with real-world soft real-time applications, we found (1)\nsuch memory-performance critical sections do exist and are often easy to\nidentify; and (2) applying BWLOCK for memory critical sections significantly\nimprove performance of the soft real-time applications at a small or no cost in\nthroughput of non real-time applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.02287v1"
    },
    {
        "title": "Survey of Operating Systems for the IoT Environment",
        "authors": [
            "Tuhin Borgohain",
            "Uday Kumar",
            "Sugata Sanyal"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  This paper is a comprehensive survey of the various operating systems\navailable for the Internet of Things environment. At first the paper introduces\nthe various aspects of the operating systems designed for the IoT environment\nwhere resource constraint poses a huge problem for the operation of the general\nOS designed for the various computing devices. The latter part of the paper\ndescribes the various OS available for the resource constraint IoT environment\nalong with the various platforms each OS supports, the software development\nkits available for the development of applications in the respective OS along\nwith the various protocols implemented in these OS for the purpose of\ncommunication and networking.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.02517v2"
    },
    {
        "title": "The Influence of Malloc Placement on TSX Hardware Transactional Memory",
        "authors": [
            "Dave Dice",
            "Tim Harris",
            "Alex Kogan",
            "Yossi Lev"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  The hardware transactional memory (HTM) implementation in Intel's i7-4770\n\"Haswell\" processor tracks the transactional read-set in the L1 (level-1), L2\n(level-2) and L3 (level-3) caches and the write-set in the L1 cache.\nDisplacement or eviction of read-set entries from the cache hierarchy or\nwrite-set entries from the L1 results in abort. We show that the placement\npolicies of dynamic storage allocators -- such as those found in common\n\"malloc\" implementations -- can influence the L1 conflict miss rate in the L1.\nConflict misses -- sometimes called mapping misses -- arise because of less\nthan ideal associativity and represent imbalanced distribution of active memory\nblocks over the set of available L1 indices. Under transactional execution\nconflict misses may manifest as aborts, representing wasted or futile effort\ninstead of a simple stall as would occur in normal execution mode.\n  Furthermore, when HTM is used for transactional lock elision (TLE),\npersistent aborts arising from conflict misses can force the offending thread\nthrough the so-called \"slow path\". The slow path is undesirable as the thread\nmust acquire the lock and run the critical section in normal execution mode,\nprecluding the concurrent execution of threads in the \"fast path\" that monitor\nthat same lock and run their critical sections in transactional mode. For a\ngiven lock, multiple threads can concurrently use the transactional fast path,\nbut at most one thread can use the non-transactional slow path at any given\ntime. Threads in the slow path preclude safe concurrent fast path execution.\nAborts rising from placement policies and L1 index imbalance can thus result in\nloss of concurrency and reduced aggregate throughput.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.04640v2"
    },
    {
        "title": "Deterministically Deterring Timing Attacks in Deterland",
        "authors": [
            "Weiyi Wu",
            "Bryan Ford"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  The massive parallelism and resource sharing embodying today's cloud business\nmodel not only exacerbate the security challenge of timing channels, but also\nundermine the viability of defenses based on resource partitioning. We propose\nhypervisor-enforced timing mitigation to control timing channels in cloud\nenvironments. This approach closes \"reference clocks\" internal to the cloud by\nimposing a deterministic view of time on guest code, and uses timing mitigators\nto pace I/O and rate-limit potential information leakage to external observers.\nOur prototype hypervisor is the first system to mitigate timing-channel leakage\nacross full-scale existing operating systems such as Linux and applications in\narbitrary languages. Mitigation incurs a varying performance cost, depending on\nworkload and tunable leakage-limiting parameters, but this cost may be\njustified for security-critical cloud applications and data.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.07070v2"
    },
    {
        "title": "A Survey Report on Operating Systems for Tiny Networked Sensors",
        "authors": [
            "Alok Ranjan",
            "H. B. Sahu",
            "Prasant Misra"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Wireless sensor network (WSN) has attracted researchers worldwide to explore\nthe research opportunities, with application mainly in health monitoring,\nindustry automation, battlefields, home automation and environmental\nmonitoring. A WSN is highly resource constrained in terms of energy,\ncomputation and memory. WSNs deployment ranges from the normal working\nenvironment up to hostile and hazardous environment such as in volcano\nmonitoring and underground mines. These characteristics of WSNs hold additional\nset of challenges in front of the operating system designer. The objective of\nthis survey is to highlight the features and weakness of the opearting system\navailable for WSNs, with the focus on the current application demands. The\npaper also discusses the operating system design issues in terms of\narchitecture, programming model, scheduling and memory management and support\nfor real time applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.05269v1"
    },
    {
        "title": "EOS: Automatic In-vivo Evolution of Kernel Policies for Better\n  Performance",
        "authors": [
            "Yan Cui",
            "Quan Chen",
            "Junfeng Yang"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Today's monolithic kernels often implement a small, fixed set of policies\nsuch as disk I/O scheduling policies, while exposing many parameters to let\nusers select a policy or adjust the specific setting of the policy. Ideally,\nthe parameters exposed should be flexible enough for users to tune for good\nperformance, but in practice, users lack domain knowledge of the parameters and\nare often stuck with bad, default parameter settings.\n  We present EOS, a system that bridges the knowledge gap between kernel\ndevelopers and users by automatically evolving the policies and parameters in\nvivo on users' real, production workloads. It provides a simple policy\nspecification API for kernel developers to programmatically describe how the\npolicies and parameters should be tuned, a policy cache to make in-vivo tuning\neasy and fast by memorizing good parameter settings for past workloads, and a\nhierarchical search engine to effectively search the parameter space.\nEvaluation of EOS on four main Linux subsystems shows that it is easy to use\nand effectively improves each subsystem's performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.06356v2"
    },
    {
        "title": "A Software-only Mechanism for Device Passthrough and Sharing",
        "authors": [
            "Piyus Kedia",
            "Sorav Bansal"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Network processing elements in virtual machines, also known as Network\nFunction Virtualization (NFV) often face CPU bottlenecks at the virtualization\ninterface. Even highly optimized paravirtual device interfaces fall short of\nthe throughput requirements of modern devices. Passthrough devices, together\nwith SR-IOV support for multiple device virtual functions (VF) and IOMMU\nsupport, mitigate this problem somewhat, by allowing a VM to directly control a\ndevice partition bypassing the virtualization stack. However, device\npassthrough requires high-end (expensive and power-hungry) hardware, places\nscalability limits on consolidation ratios, and does not support efficient\nswitching between multiple VMs on the same host.\n  We present a paravirtual interface that securely exposes an I/O device\ndirectly to the guest OS running inside the VM, and yet allows that device to\nbe securely shared among multiple VMs and the host. Compared to the best-known\nparavirtualization interfaces, our paravirtual interface supports up to 2x\nhigher throughput, and is closer in performance to device passthrough. Unlike\ndevice passthrough however, we do not require SR-IOV or IOMMU support, and\nallow fine-grained dynamic resource allocation, significantly higher\nconsolidation ratios, and seamless VM migration. Our security mechanism is\nbased on a novel approach called dynamic binary opcode subtraction.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.06367v2"
    },
    {
        "title": "Folding a Tree into a Map",
        "authors": [
            "Victor Yodaiken"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Analysis of the retrieval architecture of the highly influential UNIX file\nsystem (\\cite{Ritchie}\\cite{multicsfs}) provides insight into design methods,\nconstraints, and possible alternatives. The basic architecture can be\nunderstood in terms of function composition and recursion by anyone with some\nmathematical maturity. Expertise in operating system coding or in any\nspecialized \"formal method\" is not required.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.07694v1"
    },
    {
        "title": "Multitasking Programming of OBDH Satellite Based On PC-104",
        "authors": [
            "Haryono Haryono"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  On Board Data Handling (OBDH) has functions to monitor, control, acquire,\nanalyze, take a decision, and execute the command. OBDH should organize the\ntask between sub system. OBDH like a heart which has a vital function. Because\nthe function is seriously important therefore designing and implementing the\nOBDH should be carefully, in order to have a good reliability. Many OBDHs have\nbeen made to support the satellite mission using primitive programming. In\nhandling the data from various input, OBDH should always be available to all\nsub systems, when the tasks are many, it is not easy to program using primitive\nprogramming. Sometimes the data become corrupt because the data which come to\nthe OBDH is in the same time. Therefore it is required to have a way to handle\nthe data safely and also easy in programming perspective. In this research,\nOBDH is programmed using multi tasking programming perspective has been\ncreated. The Operating System (OS) has been implemented so that can run the\ntasks simultaneously. The OS is prepared by configuring the Linux Kernel for\nthe specific processor, creating Root File System (RFS), installing the\nBusyBox. In order to do the above method, preparing the environment in our\nmachine has been done, they are installing the Cross Tool Chain, U-Boot,\nGNU-Linux Kernel Source etc. After that, programming using c code with\nmultitasking programming can be implemented. By using above method, it is found\nthat programming is easier and the corruption data because of reentrancy can be\nminimized. Keywords- Operating System, PC-104, Kernel, C Programming\n",
        "pdf_link": "http://arxiv.org/pdf/1510.02552v1"
    },
    {
        "title": "Isolate First, Then Share: a New OS Architecture for Datacenter\n  Computing",
        "authors": [
            "Gang Lu",
            "Jianfeng Zhan",
            "Chongkang Tan",
            "Xinlong Lin",
            "Defei Kong",
            "Chen Zheng",
            "Fei Tang",
            "Cheng Huang",
            "Lei Wang",
            "Tianshu Hao"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  This paper presents the \"isolate first, then share\" OS model in which the\nprocessor cores, memory, and devices are divided up between disparate OS\ninstances and a new abstraction, subOS, is proposed to encapsulate an OS\ninstance that can be created, destroyed, and resized on-the-fly. The intuition\nis that this avoids shared kernel states between applications, which in turn\nreduces performance loss caused by contention. We decompose the OS into the\nsupervisor and several subOSes running at the same privilege level: a subOS\ndirectly manages physical resources, while the supervisor can create, destroy,\nresize a subOS on-the-fly. The supervisor and subOSes have few state sharing,\nbut fast inter-subOS communication mechanisms are provided on demand.\n  We present the first implementation, RainForest, which supports unmodified\nLinux binaries. Our comprehensive evaluation shows RainForest outperforms Linux\nwith four different kernels, LXC, and Xen in terms of worst-case and average\nperformance most of time when running a large number of benchmarks. The source\ncode is available soon.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01378v5"
    },
    {
        "title": "Aware: Controlling App Access to I/O Devices on Mobile Platforms",
        "authors": [
            "Giuseppe Petracca",
            "Ahmad Atamli",
            "Yuqiong Sun",
            "Jens Grossklags",
            "Trent Jaeger"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  Smartphones' cameras, microphones, and device displays enable users to\ncapture and view memorable moments of their lives. However, adversaries can\ntrick users into authorizing malicious apps that exploit weaknesses in current\nmobile platforms to misuse such on-board I/O devices to stealthily capture\nphotos, videos, and screen content without the users' consent. Contemporary\nmobile operating systems fail to prevent such misuse of I/O devices by\nauthorized apps due to lack of binding between users' interactions and accesses\nto I/O devices performed by these apps. In this paper, we propose Aware, a\nsecurity framework for authorizing app requests to perform operations using I/O\ndevices, which binds app requests with user intentions to make all uses of\ncertain I/O devices explicit. We evaluate our defense mechanisms through\nlaboratory-based experimentation and a user study, involving 74 human subjects,\nwhose ability to identify undesired operations targeting I/O devices increased\nsignificantly. Without Aware, only 18% of the participants were able to\nidentify attacks from tested RAT apps. Aware systematically blocks all the\nattacks in absence of user consent and supports users in identifying 82% of\nsocial-engineering attacks tested to hijack approved requests, including some\nmore sophisticated forms of social engineering not yet present in available\nRATs. Aware introduces only 4.79% maximum performance overhead over operations\ntargeting I/O devices. Aware shows that a combination of system defenses and\nuser interface can significantly strengthen defenses for controlling the use of\non-board I/O devices.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.02171v1"
    },
    {
        "title": "It's Time: OS Mechanisms for Enforcing Asymmetric Temporal Integrity",
        "authors": [
            "Anna Lyons",
            "Gernot Heiser"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  Mixed-criticality systems combine real-time components of different levels of\ncriticality, i.e. severity of failure, on the same processor, in order to\nobtain good resource utilisation. They must guarantee deadlines of\nhighly-critical tasks at the expense of lower-criticality ones in the case of\noverload. Present operating systems provide inadequate support for this kind of\nsystem, which is of growing importance in avionics and other verticals. We\npresent an approach that provides the required asymmetric integrity and its\nimplementation in the high-assurance seL4 microkernel.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00111v2"
    },
    {
        "title": "POLYPATH: Supporting Multiple Tradeoffs for Interaction Latency",
        "authors": [
            "Min Hong Yun",
            "Songtao He",
            "Lin Zhong"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  Modern mobile systems use a single input-to-display path to serve all\napplications. In meeting the visual goals of all applications, the path has a\nlatency inadequate for many important interactions. To accommodate the\ndifferent latency requirements and visual constraints by different\ninteractions, we present POLYPATH, a system design in which application\ndevelopers (and users) can choose from multiple path designs for their\napplication at any time. Because a POLYPATH system asks for two or more path\ndesigns, we present a novel fast path design, called Presto. Presto reduces\nlatency by judiciously allowing frame drops and tearing.\n  We report an Android 5-based prototype of POLYPATH with two path designs:\nAndroid legacy and Presto. Using this prototype, we quantify the effectiveness,\noverhead, and user experience of POLYPATH, especially Presto, through both\nobjective measurements and subjective user assessment. We show that Presto\nreduces the latency of legacy touchscreen drawing applications by almost half;\nand more importantly, this reduction is orthogonal to that of other popular\napproaches and is achieved without any user-noticeable negative visual effect.\nWhen combined with touch prediction, Presto is able to reduce the touch latency\nbelow 10 ms, a remarkable achievement without any hardware support.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05654v1"
    },
    {
        "title": "Duplication of Windows Services",
        "authors": [
            "Zhiyong Shan",
            "Xin Wang",
            "Tzi-cker Chiueh",
            "Rajiv Bagai"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  OS-level virtualization techniques virtualize system resources at the system\ncall interface, has the distinct advantage of smaller run-time resource\nrequirements as compared to HAL-level virtualization techniques, and thus forms\nan important building block for virtualizing parallel and distributed\napplications such as a HPC clusters. Because the Windows operating system puts\ncertain critical functionalities in privileged user-level system service\nprocesses, a complete OS-level virtualization solution for the Windows platform\nrequires duplication of such Windows service as Remote Procedure Call Server\nService (RPCSS). As many implementation details of the Windows system services\nare proprietary, duplicating Windows system services becomes the key technical\nchallenge for virtualizing the Windows platform at the OS level. Moreover, as a\ncore component of cloud computing, IIS web server-related services need to be\nduplicated in containers (i.e., OS-level virtual machines), but so far there is\nno such scheme. In this paper, we thoroughly identify all issues that affect\nservice duplication, and then propose the first known methodology to\nsystematically duplicate both system and ordinary Windows services. Our\nexperiments show that the methodology can duplicate a set of system and\nordinary services on different versions of Windows OS.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.08051v3"
    },
    {
        "title": "Compatible and Usable Mandatory Access Control for Good-enough OS\n  Security",
        "authors": [
            "Zhiyong Shan"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  OS compromise is one of the most serious computer security problems today,\nbut still not being resolved. Although people proposed different kinds of\nmethods, they could not be accepted by most users who are non-expert due to the\nlack of compatibility and usability. In this paper, we introduce a kind of new\nmandatory access control model, named CUMAC, that aims to achieve good-enough\nsecurity, high compatibility and usability. It has two novel features. One is\naccess control based on tracing potential intrusion that can reduce false\nnegatives and facilitate security configuration, in order to improve both\ncompatibility and usability; the other is automatically figuring out all of the\ncompatibility exceptions that usually incurs incompatible problems. The\nexperiments performed on the prototype show that CUMAC can defense attacks from\nnetwork, mobile disk and local untrustable users while keeping good\ncompatibility and usability.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.00875v1"
    },
    {
        "title": "Confining Windows Inter-Process Communications for OS-Level Virtual\n  Machine",
        "authors": [
            "Zhiyong Shan",
            "Yang Yu",
            "Tzi-cker Chiueh"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  As OS-level virtualization technology usually imposes little overhead on\nvirtual machine start-up and running, it provides an excellent choice for\nbuilding intrusion/fault tolerant applications that require redundancy and\nfrequent invocation. When developing Windows OS-level virtual machine, however,\npeople will inevitably face the challenge of confining Windows Inter-Process\nCommunications (IPC). As IPC on Windows platform is more complex than UNIX\nstyle OS and most of the programs on Windows are not open-source, it is\ndifficult to discover all of the performed IPCs and confine them. In this\npaper, we propose three general principles to confine IPC on Windows OS and a\nnovel IPC confinement mechanism based on the principles. With the mechanism,\nfor the first time from the literature, we successfully virtualized RPC System\nService (RPCSS) and Internet Information Server (IIS) on Feather-weight Virtual\nMachine (FVM). Experimental results demonstrate that multiple IIS web server\ninstances can simultaneously run on single Windows OS with much less\nperformance overhead than other popular VM technology, offering a good basis\nfor constructing dependable system.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.04781v1"
    },
    {
        "title": "Virtualizing System and Ordinary Services in Windows-based OS-Level\n  Virtual Machines",
        "authors": [
            "Zhiyong Shan",
            "Tzi-cker Chiueh",
            "Xin Wang"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  OS-level virtualization incurs smaller start-up and run-time overhead than\nHAL-based virtualization and thus forms an important building block for\ndeveloping fault-tolerant and intrusion-tolerant applications. A complete\nimplementation of OS-level virtualization on the Windows platform requires\nvirtualization of Windows services, such as system services like the Remote\nProcedure Call Server Service (RPCSS), because they are essentially extensions\nof the kernel. As Windows system services work very differently from their\ncounterparts on UNIX-style OS, i.e., daemons, and many of their implementation\ndetails are proprietary, virtualizing Windows system services turned out to be\nthe most challenging technical barrier for OS-level virtualization for the\nWindows platform. In this paper, we describe a general technique to virtualize\nWindows services, and demonstrate its effectiveness by applying it to\nsuccessfully virtualize a set of important Windows system services and ordinary\nservices on different versions of Windows OS, including RPCSS, DcomLaunch, IIS\nservice group, Tlntsvr, MySQL, Apache2.2, CiSvc, ImapiService, etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.04785v1"
    },
    {
        "title": "Implementing RBAC model in An Operating System Kernel",
        "authors": [
            "Zhiyong Shan",
            "Yu-fang Sun"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  In this paper, the implementation of an operating system oriented RBAC model\nis discussed. Firstly, on the basis of RBAC96 model, a new RBAC model named OSR\nis presented. Secondly, the OSR model is enforced in RFSOS kernel by the way of\nintegrating GFAC method and Capability mechanism together. All parts of the OSR\nimplementation are described in detail.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.08154v1"
    },
    {
        "title": "An Evaluation of Coarse-Grained Locking for Multicore Microkernels",
        "authors": [
            "Kevin Elphinstone",
            "Amirreza Zarrabi",
            "Adrian Danis",
            "Yanyan Shen",
            "Gernot Heiser"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  The trade-off between coarse- and fine-grained locking is a well understood\nissue in operating systems. Coarse-grained locking provides lower overhead\nunder low contention, fine-grained locking provides higher scalability under\ncontention, though at the expense of implementation complexity and re- duced\nbest-case performance.\n  We revisit this trade-off in the context of microkernels and tightly-coupled\ncores with shared caches and low inter-core migration latencies. We evaluate\nperformance on two architectures: x86 and ARM MPCore, in the former case also\nutilising transactional memory (Intel TSX). Our thesis is that on such\nhardware, a well-designed microkernel, with short system calls, can take\nadvantage of coarse-grained locking on modern hardware, avoid the run-time and\ncomplexity cost of multiple locks, enable formal verification, and still\nachieve scalability comparable to fine-grained locking.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.08372v2"
    },
    {
        "title": "Memshare: a Dynamic Multi-tenant Memory Key-value Cache",
        "authors": [
            "Asaf Cidon",
            "Daniel Rushton",
            "Stephen M. Rumble",
            "Ryan Stutsman"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  Web application performance is heavily reliant on the hit rate of\nmemory-based caches. Current DRAM-based web caches statically partition their\nmemory across multiple applications sharing the cache. This causes under\nutilization of memory which negatively impacts cache hit rates. We present\nMemshare, a novel web memory cache that dynamically manages memory across\napplications. Memshare provides a resource sharing model that guarantees\nprivate memory to different applications while dynamically allocating the\nremaining shared memory to optimize overall hit rate. Today's high cost of DRAM\nstorage and the availability of high performance CPU and memory bandwidth, make\nweb caches memory capacity bound. Memshare's log-structured design allows it to\nprovide significantly higher hit rates and dynamically partition memory among\napplications at the expense of increased CPU and memory bandwidth consumption.\nIn addition, Memshare allows applications to use their own eviction policy for\ntheir objects, independent of other applications. We implemented Memshare and\nran it on a week-long trace from a commercial memcached provider. We\ndemonstrate that Memshare increases the combined hit rate of the applications\nin the trace by an 6.1% (from 84.7% hit rate to 90.8% hit rate) and reduces the\ntotal number of misses by 39.7% without affecting system throughput or latency.\nEven for single-tenant applications, Memshare increases the average hit rate of\nthe current state-of-the-art memory cache by an additional 2.7% on our\nreal-world trace.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.08129v1"
    },
    {
        "title": "Tackling Diversity and Heterogeneity by Vertical Memory Management",
        "authors": [
            "Lei Liu"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  Existing memory management mechanisms used in commodity computing machines\ntypically adopt hardware based address interleaving and OS directed random\nmemory allocation to service generic application requests. These conventional\nmemory management mechanisms are challenged by contention at multiple memory\nlevels, a daunting variety of workload behaviors, and an increasingly\ncomplicated memory hierarchy. Our ISCA-41 paper proposes vertical partitioning\nto eliminate shared resource contention at multiple levels in the memory\nhierarchy. Combined with horizontal memory management policies, our framework\nsupports a flexible policy space for tackling diverse application needs in\nproduction environment and is suitable for future heterogeneous memory systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01198v1"
    },
    {
        "title": "Mixed-criticality Scheduling with Dynamic Redistribution of Shared Cache",
        "authors": [
            "Muhammad Ali Awan",
            "Konstantinos Bletsas",
            "Pedro F. Souto",
            "Benny Akesson",
            "Eduardo Tovar"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  The design of mixed-criticality systems often involvespainful tradeoffs\nbetween safety guarantees and performance.However, the use of more detailed\narchitectural modelsin the design and analysis of scheduling arrangements for\nmixedcriticalitysystems can provide greater confidence in the analysis,but also\nopportunities for better performance. Motivated by thisview, we propose an\nextension of Vestal 19s model for mixedcriticalitymulticore systems that (i)\naccounts for the per-taskpartitioning of the last-level cache and (ii) supports\nthe dynamicreassignment, for better schedulability, of cache portions\ninitiallyreserved for lower-criticality tasks to the higher-criticalitytasks,\nwhen the system switches to high-criticality mode. Tothis model, we apply\npartitioned EDF scheduling with Ekbergand Yi 19s deadline-scaling technique.\nOur schedulability analysisand scalefactor calculation is cognisant of the\ncache resourcesassigned to each task, by using WCET estimates that take\nintoaccount these resources. It is hence able to leverage the\ndynamicreconfiguration of the cache partitioning, at mode change, forbetter\nperformance, in terms of provable schedulability. We alsopropose heuristics for\npartitioning the cache in low- and highcriticalitymode, that promote\nschedulability. Our experimentswith synthetic task sets, indicate tangible\nimprovements inschedulability compared to a baseline cache-aware\narrangementwhere there is no redistribution of cache resources from low-\ntohigh-criticality tasks in the event of a mode change.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.08876v1"
    },
    {
        "title": "IOTune: A G-states Driver for Elastic Performance of Block Storage",
        "authors": [
            "Tao Lu",
            "Ping Huang",
            "Xubin He",
            "Matthew Welch",
            "Steven Gonzales",
            "Ming Zhang"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  Imagining a disk which provides baseline performance at a relatively low\nprice during low-load periods, but when workloads demand more resources, the\ndisk performance is automatically promoted in situ and in real time. In a\nhardware era, this is hardly achievable. However, this imagined disk is\nbecoming reality due to the technical advances of software-defined storage,\nwhich enable volume performance to be adjusted on the fly. We propose IOTune, a\nresource management middleware which employs software-defined storage\nprimitives to implement G-states of virtual block devices. G-states enable\nvirtual block devices to serve at multiple performance gears, getting rid of\nconflicts between immutable resource reservation and dynamic resource demands,\nand always achieving resource right-provisioning for workloads. Accompanying\nG-states, we also propose a new block storage pricing policy for cloud\nproviders. Our case study for applying G-states to cloud block storage verifies\nthe effectiveness of the IOTune framework. Trace-replay based evaluations\ndemonstrate that storage volumes with G-states adapt to workload fluctuations.\nFor tenants, G-states enable volumes to provide much better QoS with a same\ncost of ownership, comparing with static IOPS provisioning and the I/O credit\nmechanism. G-states also reduce I/O tail latencies by one to two orders of\nmagnitude. From the standpoint of cloud providers, G-states promote storage\nutilization, creating values and benefiting competitiveness. G-states supported\nby IOTune provide a new paradigm for storage resource management and pricing in\nmulti-tenant clouds.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.03591v1"
    },
    {
        "title": "Comments on \"Gang EDF Schedulability Analysis\"",
        "authors": [
            "Pascal Richard",
            "Joël Goossens",
            "Shinpei Kato"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  This short report raises a correctness issue in the schedulability test\npresented in Kato et al., \"Gang EDF Scheduling of Parallel Task Systems\", 30th\nIEEE Real-Time Systems Symposium, 2009, pp. 459-468.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.05798v1"
    },
    {
        "title": "Look Mum, no VM Exits! (Almost)",
        "authors": [
            "Ralf Ramsauer",
            "Jan Kiszka",
            "Daniel Lohmann",
            "Wolfgang Mauerer"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  Multi-core CPUs are a standard component in many modern embedded systems.\nTheir virtualisation extensions enable the isolation of services, and gain\npopularity to implement mixed-criticality or otherwise split systems. We\npresent Jailhouse, a Linux-based, OS-agnostic partitioning hypervisor that uses\nnovel architectural approaches to combine Linux, a powerful general-purpose\nsystem, with strictly isolated special-purpose components. Our design goals\nfavour simplicity over features, establish a minimal code base, and minimise\nhypervisor activity.\n  Direct assignment of hardware to guests, together with a deferred\ninitialisation scheme, offloads any complex hardware handling and bootstrapping\nissues from the hypervisor to the general purpose OS. The hypervisor\nestablishes isolated domains that directly access physical resources without\nthe need for emulation or paravirtualisation. This retains, with negligible\nsystem overhead, Linux's feature-richness in uncritical parts, while frugal\nsafety and real-time critical workloads execute in isolated, safe domains.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.06932v1"
    },
    {
        "title": "GPU System Calls",
        "authors": [
            "Ján Veselý",
            "Arkaprava Basu",
            "Abhishek Bhattacharjee",
            "Gabriel Loh",
            "Mark Oskin",
            "Steven K. Reinhardt"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  GPUs are becoming first-class compute citizens and are being tasked to\nperform increasingly complex work. Modern GPUs increasingly support\nprogrammability- enhancing features such as shared virtual memory and hardware\ncache coherence, enabling them to run a wider variety of programs. But a key\naspect of general-purpose programming where GPUs are still found lacking is the\nability to invoke system calls. We explore how to directly invoke generic\nsystem calls in GPU programs. We examine how system calls should be meshed with\nprevailing GPGPU programming models where thousands of threads are organized in\na hierarchy of execution groups: Should a system call be invoked at the\nindividual GPU task, or at different execution group levels? What are\nreasonable ordering semantics for GPU system calls across these hierarchy of\nexecution groups? To study these questions, we implemented GENESYS -- a\nmechanism to allow GPU pro- grams to invoke system calls in the Linux operating\nsystem. Numerous subtle changes to Linux were necessary, as the existing kernel\nassumes that only CPUs invoke system calls. We analyze the performance of\nGENESYS using micro-benchmarks and three applications that exercise the\nfilesystem, networking, and memory allocation subsystems of the kernel. We\nconclude by analyzing the suitability of all of Linux's system calls for the\nGPU.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.06965v2"
    },
    {
        "title": "SMORE: A Cold Data Object Store for SMR Drives (Extended Version)",
        "authors": [
            "Peter Macko",
            "Xiongzi Ge",
            "John Haskins Jr.",
            "James Kelley",
            "David Slik",
            "Keith A. Smith",
            "Maxim G. Smith"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  Shingled magnetic recording (SMR) increases the capacity of magnetic hard\ndrives, but it requires that each zone of a disk be written sequentially and\nerased in bulk. This makes SMR a good fit for workloads dominated by large data\nobjects with limited churn. To explore this possibility, we have developed\nSMORE, an object storage system designed to reliably and efficiently store\nlarge, seldom-changing data objects on an array of host-managed or host-aware\nSMR disks.\n  SMORE uses a log-structured approach to accommodate the constraint that all\nwrites to an SMR drive must be sequential within large shingled zones. It\nstripes data across zones on separate disks, using erasure coding to protect\nagainst drive failure. A separate garbage collection thread reclaims space by\nmigrating live data out of the emptiest zones so that they can be trimmed and\nreused. An index stored on flash and backed up to the SMR drives maps object\nidentifiers to on-disk locations. SMORE interleaves log records with object\ndata within SMR zones to enable index recovery after a system crash (or failure\nof the flash device) without any additional logging mechanism.\n  SMORE achieves full disk bandwidth when ingesting data---with a variety of\nobject sizes---and when reading large objects. Read performance declines for\nsmaller object sizes where inter- object seek time dominates. With a worst-case\npattern of random deletions, SMORE has a write amplification (not counting RAID\nparity) of less than 2.0 at 80% occupancy. By taking an index snapshot every\ntwo hours, SMORE recovers from crashes in less than a minute. More frequent\nsnapshots allow faster recovery.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.09701v1"
    },
    {
        "title": "Entirely protecting operating systems against transient errors in space\n  environment",
        "authors": [
            "Mahoukpégo Parfait Tokponnon",
            "Marc Lobelle",
            "Eugene C. Ezin"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  In this article, we propose a mainly-software hardening technique to totally\nprotect unmodified running operating systems on COTS hardware against transient\nerrors in heavily radiation - flooded environment like high altitude space. The\ntechnique is currently being implemented in a hypervisor and allows to control\nthe upper layers of the software stack (operating system and applications). The\nrest of the system, the hypervisor, will be protected by other means, thus\nresulting in a completely protected system against transient errors. The\ninduced overhead turns around 200% but this is expected to decrease with future\nimprovements.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.06450v1"
    },
    {
        "title": "Barrier Enabled IO Stack for Flash Storage",
        "authors": [
            "Youjip Won",
            "Jaemin Jung",
            "Gyeongyeol Choi",
            "Joontaek Oh",
            "Seongbae Son",
            "Jooyoung Hwang",
            "Sangyeun Cho"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  This work is dedicated to eliminating the overhead of guaranteeing the\nstorage order in modern IO stack. The existing block device adopts\nprohibitively expensive resort in ensuring the storage order among write\nrequests: interleaving successive write requests with transfer and flush.\nExploiting the cache barrier command for the Flash storage, we overhaul the IO\nscheduler, the dispatch module and the filesystem so that these layers are\norchestrated to preserve the ordering condition imposed by the application can\nbe delivered to the storage. Key ingredients of Barrier Enabled IO stack are\nEpoch based IO scheduling, Order Preserving Dispatch, and Dual Mode Journaling.\nBarrier enabled IO stack successfully eliminates the root cause of excessive\noverhead in enforcing the storage order. Dual Mode Journaling in BarrierFS\ndedicates the separate threads to effectively decouple the control plane and\ndata plane of the journal commit. We implement Barrier Enabled IO Stack in\nserver as well as in mobile platform. SQLite performance increases by 270% and\n75%, in server and in smartphone, respectively. Relaxing the durability of a\ntransaction, SQLite performance and MySQL performance increases as much as by\n73X and by 43X, respectively, in server storage.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02258v1"
    },
    {
        "title": "Implementation of an Android Framework for USB storage access without\n  root rights",
        "authors": [
            "Magnus Jahnen"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  This bachelor thesis describes the implementation of an Android framework to\naccess mass storage devices over the USB interface of a smartphone. First the\nbasics of USB (i.e. interfaces, endpoints and USB On the go) and accessing USB\ndevices via the official Android API are discussed. Next the USB mass storage\nclass is explained, which was de- signed by the USB-IF to access mobile mass\nstorage like USB pen drives or external HDDs. For communication with mass\nstorage devices, most important are the bulk-only transfer and the SCSI\ntransparent command set. Furthermore file systems, for accessing directo- ries\nand files, are described. This thesis focuses on the FAT32 file system from\nMicrosoft, because it is the most commonly used file system on such devices.\nAfter the theory part it is time to look at the implementation of the\nframework. In this section, the first concern is the purpose in general. Then\nthe architecture of the framework and the actual implementation are presented.\nImportant parts are discussed in detail. The thesis finishes with an overview\nof the test results on various Android devices, a short conclusion and an\noutlook to future developments. Moreover the current status of the developed\nframework is visualized.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08139v1"
    },
    {
        "title": "Migrate when necessary: toward partitioned reclaiming for soft real-time\n  tasks",
        "authors": [
            "Houssam Eddine Zahaf",
            "Giuseppe Lipari",
            "Luca Abeni",
            "Houssam-Eddine Zahaf"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  This paper presents a new strategy for scheduling soft real-time tasks on\nmultiple identical cores. The proposed approach is based on partitioned CPU\nreservations and it uses a reclaiming mechanism to reduce the number of missed\ndeadlines. We introduce the possibility for a task to temporarily migrate to\nanother, less charged, CPU when it has exhausted the reserved bandwidth on its\nallocated CPU. In addition, we propose a simple load balancing method to\ndecrease the number of deadlines missed by the tasks. The proposed algorithm\nhas been evaluated through simulations, showing its effectiveness (compared to\nother multi-core reclaiming approaches) and comparing the performance of\ndifferent partitioning heuristics (Best Fit, Worst Fit and First Fit).\n",
        "pdf_link": "http://arxiv.org/pdf/1712.06276v1"
    },
    {
        "title": "POSIX-based Operating System in the environment of NVM/SCM memory",
        "authors": [
            "Vyacheslav Dubeyko",
            "Cyril Guyot",
            "Luis Cargnini",
            "Adam Manzanares"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  Modern Operating Systems are typically POSIX-compliant. The system calls are\nthe fundamental layer of interaction between user-space applications and the OS\nkernel and its implementation of fundamental abstractions and primitives used\nin modern computing. The next generation of NVM/SCM memory raises critical\nquestions about the efficiency of modern OS architecture. This paper\ninvestigates how the POSIX API drives performance for a system with NVM/SCM\nmemory. We show that OS and metadata related system calls represent the most\nimportant area of optimization. However, the synchronization related system\ncalls (poll(), futex(), wait4()) are the most time-consuming overhead that even\na RAMdisk platform fails to eliminate. Attempting to preserve the POSIX-based\napproach will likely result in fundamental inefficiencies for any future\napplications of NVM/SCM memory.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.07759v2"
    },
    {
        "title": "Elevating commodity storage with the SALSA host translation layer",
        "authors": [
            "Nikolas Ioannou",
            "Kornilios Kourtis",
            "Ioannis Koltsidas"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  To satisfy increasing storage demands in both capacity and performance,\nindustry has turned to multiple storage technologies, including Flash SSDs and\nSMR disks. These devices employ a translation layer that conceals the\nidiosyncrasies of their mediums and enables random access. Device translation\nlayers are, however, inherently constrained: resources on the drive are scarce,\nthey cannot be adapted to application requirements, and lack visibility across\nmultiple devices. As a result, performance and durability of many storage\ndevices is severely degraded.\n  In this paper, we present SALSA: a translation layer that executes on the\nhost and allows unmodified applications to better utilize commodity storage.\nSALSA supports a wide range of single- and multi-device optimizations and,\nbecause is implemented in software, can adapt to specific workloads. We\ndescribe SALSA's design, and demonstrate its significant benefits using\nmicrobenchmarks and case studies based on three applications: MySQL, the Swift\nobject store, and a video server.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.05637v2"
    },
    {
        "title": "vLibOS: Babysitting OS Evolution with a Virtualized Library OS",
        "authors": [
            "Ying Ye",
            "Zhuoqun Cheng",
            "Soham Sinha",
            "Richard West"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Many applications have service requirements that are not easily met by\nexisting operating systems. Real-time and security-critical tasks, for example,\noften require custom OSes to meet their needs. However, development of special\npurpose OSes is a time-consuming and difficult exercise. Drivers, libraries and\napplications have to be written from scratch or ported from existing sources.\nMany researchers have tackled this problem by developing ways to extend\nexisting systems with application-specific services. However, it is often\ndifficult to ensure an adequate degree of separation between legacy and new\nservices, especially when security and timing requirements are at stake.\nVirtualization, for example, supports logical isolation of separate guest\nservices, but suffers from inadequate temporal isolation of time-critical code\nrequired for real-time systems. This paper presents vLibOS, a master-slave\nparadigm for new systems, whose services are built on legacy code that is\ntemporally and spatially isolated in separate VM domains. Existing OSes are\ntreated as sandboxed libraries, providing legacy services that are requested by\ninter-VM calls, which execute with the time budget of the caller. We evaluate a\nreal-time implementation of vLibOS. Empirical results show that vLibOS achieves\nas much as a 50\\% reduction in performance slowdown for real-time threads, when\ncompeting for a shared memory bus with a Linux VM.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.07880v1"
    },
    {
        "title": "iReplayer: In-situ and Identical Record-and-Replay for Multithreaded\n  Applications",
        "authors": [
            "Hongyu Liu",
            "Sam Silvestro",
            "Wei Wang",
            "Chen Tian",
            "Tongping Liu"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Reproducing executions of multithreaded programs is very challenging due to\nmany intrinsic and external non-deterministic factors. Existing RnR systems\nachieve significant progress in terms of performance overhead, but none targets\nthe in-situ setting, in which replay occurs within the same process as the\nrecording process. Also, most existing work cannot achieve identical replay,\nwhich may prevent the reproduction of some errors.\n  This paper presents iReplayer, which aims to identically replay multithreaded\nprograms in the original process (under the \"in-situ\" setting). The novel\nin-situ and identical replay of iReplayer makes it more likely to reproduce\nerrors, and allows it to directly employ debugging mechanisms (e.g.\nwatchpoints) to aid failure diagnosis. Currently, iReplayer only incurs 3%\nperformance overhead on average, which allows it to be always enabled in the\nproduction environment. iReplayer enables a range of possibilities, and this\npaper presents three examples: two automatic tools for detecting buffer\noverflows and use-after-free bugs, and one interactive debugging tool that is\nintegrated with GDB.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01226v1"
    },
    {
        "title": "An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM\n  Memory Architecture",
        "authors": [
            "Reza Salkhordeh",
            "Hossein Asadi"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  With the emergence of Non-Volatile Memories (NVMs) and their shortcomings\nsuch as limited endurance and high power consumption in write requests, several\nstudies have suggested hybrid memory architecture employing both Dynamic Random\nAccess Memory (DRAM) and NVM in a memory system. By conducting a comprehensive\nexperiments, we have observed that such studies lack to consider very important\naspects of hybrid memories including the effect of: a) data migrations on\nperformance, b) data migrations on power, and c) the granularity of data\nmigration. This paper presents an efficient data migration scheme at the\nOperating System level in a hybrid DRAMNVM memory architecture. In the proposed\nscheme, two Least Recently Used (LRU) queues, one for DRAM section and one for\nNVM section, are used for the sake of data migration. With careful\ncharacterization of the workloads obtained from PARSEC benchmark suite, the\nproposed scheme prevents unnecessary migrations and only allows migrations\nwhich benefits the system in terms of power and performance. The experimental\nresults show that the proposed scheme can reduce the power consumption up to\n79% compared to DRAM-only memory and up to 48% compared to the state-of-the art\ntechniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.02514v1"
    },
    {
        "title": "Platform-Agnostic Steal-Time Measurement in a Guest Operating System",
        "authors": [
            "Javier Verdu",
            "Juan Jose Costa",
            "Beatriz Otero",
            "Eva Rodriguez",
            "Alex Pajuelo",
            "Ramon Canal"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Steal time is a key performance metric for applications executed in a\nvirtualized environment. Steal time measures the amount of time the processor\nis preempted by code outside the virtualized environment. This, in turn, allows\nto compute accurately the execution time of an application inside a virtual\nmachine (i.e. it eliminates the time the virtual machine is suspended).\nUnfortunately, this metric is only available in particular scenarios in which\nthe host and the guest OS are tightly coupled. Typical examples are the Xen\nhypervisor and Linux-based guest OSes. In contrast, in scenarios where the\nsteal time is not available inside the virtualized environment, performance\nmeasurements are, most often, incorrect.\n  In this paper, we introduce a novel and platform agnostic approach to\ncalculate this steal time within the virtualized environment and without the\ncooperation of the host OS. The theoretical execution time of a deterministic\nmicrobenchmark is compared to its execution time in a virtualized environment.\nWhen factoring in the virtual machine load, this solution -as simple as it is-\ncan compute the steal time. The preliminary results show that we are able to\ncompute the load of the physical processor within the virtual machine with high\naccuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.01139v1"
    },
    {
        "title": "BRAVO -- Biased Locking for Reader-Writer Locks",
        "authors": [
            "David Dice",
            "Alex Kogan"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Designers of modern reader-writer locks confront a difficult trade-off\nrelated to reader scalability. Locks that have a compact memory representation\nfor active readers will typically suffer under high intensity read-dominated\nworkloads when the \"reader indicator\"' state is updated frequently by a diverse\nset of threads, causing cache invalidation and coherence traffic. Other\ndesigns, such as cohort reader-writer locks, use distributed reader indicators,\none per NUMA node. This improves reader-reader scalability, but also increases\nthe size of each lock instance. We propose a simple transformation BRAVO, that\naugments any existing reader-writer lock, adding just two integer fields to the\nlock instance. Readers make their presence known to writers by hashing their\nthread's identity with the lock address, forming an index into a visible\nreaders table. Readers attempt to install the lock address into that element in\nthe table, making their existence known to potential writers. All locks and\nthreads in an address space can share the visible readers table. Updates by\nreaders tend to be diffused over the table, resulting in a NUMA-friendly\ndesign. Crucially, readers of the same lock tend to write to different\nlocations in the array, reducing coherence traffic. Specifically, BRAVO allows\na simple compact lock to be augmented so as to provide scalable concurrent\nreading but with only a modest increase in footprint.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.01553v3"
    },
    {
        "title": "TWA -- Ticket Locks Augmented with a Waiting Array",
        "authors": [
            "Dave Dice",
            "Alex Kogan"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  The classic ticket lock consists of ticket and grant fields. Arriving threads\natomically fetch-and-increment ticket and then wait for grant to become equal\nto the value returned by the fetch-and-increment primitive, at which point the\nthread holds the lock. The corresponding unlock operation simply increments\ngrant. This simple design has short code paths and fast handover (transfer of\nownership) under light contention, but may suffer degraded scalability under\nhigh contention when multiple threads busy wait on the grant field -- so-called\nglobal spinning. We propose a variation on ticket locks where long-term waiting\nthreads wait on locations in a waiting array instead of busy waiting on the\ngrant field. The single waiting array is shared among all locks. Short-term\nwaiting is accomplished in the usual manner on the grant field. The resulting\nalgorithm, TWA, improves on ticket locks by limiting the number of threads\nspinning on the grant field at any given time, reducing the number of remote\ncaches requiring invalidation from the store that releases the lock. In turn,\nthis accelerates handover, and since the lock is held throughout the handover\noperation, scalability improves. Under light or no contention, TWA yields\nperformance comparable to the classic ticket lock, avoiding the complexity and\nextra accesses incurred by MCS locks in the handover path, but providing\nperformance above or beyond that of MCS at high contention.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.01573v4"
    },
    {
        "title": "Finding Crash-Consistency Bugs with Bounded Black-Box Crash Testing",
        "authors": [
            "Jayashree Mohan",
            "Ashlie Martinez",
            "Soujanya Ponnapalli",
            "Pandian Raju",
            "Vijay Chidambaram"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  We present a new approach to testing file-system crash consistency: bounded\nblack-box crash testing (B3). B3 tests the file system in a black-box manner\nusing workloads of file-system operations. Since the space of possible\nworkloads is infinite, B3 bounds this space based on parameters such as the\nnumber of file-system operations or which operations to include, and\nexhaustively generates workloads within this bounded space. Each workload is\ntested on the target file system by simulating power-loss crashes while the\nworkload is being executed, and checking if the file system recovers to a\ncorrect state after each crash. B3 builds upon insights derived from our study\nof crash-consistency bugs reported in Linux file systems in the last five\nyears. We observed that most reported bugs can be reproduced using small\nworkloads of three or fewer file-system operations on a newly-created file\nsystem, and that all reported bugs result from crashes after fsync() related\nsystem calls. We build two tools, CrashMonkey and ACE, to demonstrate the\neffectiveness of this approach. Our tools are able to find 24 out of the 26\ncrash-consistency bugs reported in the last five years. Our tools also revealed\n10 new crash-consistency bugs in widely-used, mature Linux file systems, seven\nof which existed in the kernel since 2014. Our tools also found a\ncrash-consistency bug in a verified file system, FSCQ. The new bugs result in\nsevere consequences like broken rename atomicity and loss of persisted files.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.02904v1"
    },
    {
        "title": "Revitalizing Copybacks in Modern SSDs: Why and How",
        "authors": [
            "Duwon Hong",
            "Myungsuk Kim",
            "Jisung Park",
            "Myoungsoo Jung",
            "Jihong Kim"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  For modern flash-based SSDs, the performance overhead of internal data\nmigrations is dominated by the data transfer time, not by the flash program\ntime as in old SSDs. In order to mitigate the performance impact of data\nmigrations, we propose rCopyback, a restricted version of copyback. Rcopyback\nworks like the original copyback except that only n consecutive copybacks are\nallowed. By limiting the number of successive copybacks, it guarantees that no\ndata reliability problem occurs when data is internally migrated using\nrCopyback. In order to take a full advantage of rCopyback, we developed a\nrCopyback-aware FTL, rcFTL, which intelligently decides whether rCopyback\nshould be used or not by exploiting varying host workloads. Our evaluation\nresults show that rcFTL can improve the overall I/O throughput by 54% on\naverage over an existing FTL which does not use copybacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04603v1"
    },
    {
        "title": "T-Visor: A Hypervisor for Mixed Criticality Embedded Real-time System\n  with Hardware Virtualization Support",
        "authors": [
            "Takumi Shimada",
            "Takeshi Yashiro",
            "Ken Sakamura"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Recently, embedded systems have not only requirements for hard real-time\nbehavior and reliability, but also diversified functional demands, such as\nnetwork functions. To satisfy these requirements, virtualization using\nhypervisors is promising for embedded systems. However, as most of existing\nhypervisors are designed for general-purpose information processing systems,\nthey rely on large system stacks, so that they are not suitable for mixed\ncriticality embedded real-time systems. Even in hypervisors designed for\nembedded systems, their schedulers do not consider the diversity of real-time\nrequirements and rapid change in scheduling theory.\n  We present the design and implementation of T-Visor, a hypervisor specialized\nfor mixed criticality embedded real-time systems. T-Visor supports ARM\narchitecture and realizes full virtualization using ARM Virtualization\nExtensions. To guarantee real-time behavior, T-Visor provides a flexible\nscheduling framework so that developers can select the most suitable scheduling\nalgorithm for their systems. Our evaluation showed that it performed better\ncompared to Xen/ARM. From these results, we conclude that our design and\nimplementation are more suitable for embedded real-time systems than the\nexisting hypervisors.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.05068v1"
    },
    {
        "title": "Time Protection: the Missing OS Abstraction",
        "authors": [
            "Qian Ge",
            "Yuval Yarom",
            "Tom Chothia",
            "Gernot Heiser"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Timing channels enable data leakage that threatens the security of computer\nsystems, from cloud platforms to smartphones and browsers executing untrusted\nthird-party code. Preventing unauthorised information flow is a core duty of\nthe operating system, however, present OSes are unable to prevent timing\nchannels. We argue that OSes must provide time protection in addition to the\nestablished memory protection. We examine the requirements of time protection,\npresent a design and its implementation in the seL4 microkernel, and evaluate\nits efficacy as well as performance overhead on Arm and x86 processors.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.05345v2"
    },
    {
        "title": "Compact NUMA-Aware Locks",
        "authors": [
            "Dave Dice",
            "Alex Kogan"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Modern multi-socket architectures exhibit non-uniform memory access (NUMA)\nbehavior, where access by a core to data cached locally on a socket is much\nfaster than access to data cached on a remote socket. Prior work offers several\nefficient NUMA-aware locks that exploit this behavior by keeping the lock\nownership on the same socket, thus reducing remote cache misses and\ninter-socket communication. Virtually all those locks, however, are\nhierarchical in their nature, thus requiring space proportional to the number\nof sockets. The increased memory cost renders NUMA-aware locks unsuitable for\nsystems that are conscious to space requirements of their synchronization\nconstructs, with the Linux kernel being the chief example.\n  In this work, we present a compact NUMA-aware lock that requires only one\nword of memory, regardless of the number of sockets in the underlying machine.\nThe new lock is a variant of an efficient (NUMA-oblivious) MCS lock, and\ninherits its performant features, such as local spinning and a single atomic\ninstruction in the acquisition path. Unlike MCS, the new lock organizes waiting\nthreads in two queues, one composed of threads running on the same socket as\nthe current lock holder, and another composed of threads running on a different\nsocket(s).\n  We integrated the new lock in the Linux kernel's qspinlock, one of the major\nsynchronization constructs in the kernel. Our evaluation using both user-space\nand kernel benchmarks shows that the new lock has a single-thread performance\nof MCS, but significantly outperforms the latter under contention, achieving a\nsimilar level of performance when compared to other, state-of-the-art\nNUMA-aware locks that require substantially more space.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.05600v2"
    },
    {
        "title": "Pyronia: Intra-Process Access Control for IoT Applications",
        "authors": [
            "Marcela S. Melara",
            "David H. Liu",
            "Michael J. Freedman"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Third-party code plays a critical role in IoT applications, which generate\nand analyze highly privacy-sensitive data. Unlike traditional desktop and\nserver settings, IoT devices mostly run a dedicated, single application. As a\nresult, vulnerabilities in third-party libraries within a process pose a much\nbigger threat than on traditional platforms.\n  We present Pyronia, a fine-grained access control system for IoT applications\nwritten in high-level languages. Pyronia exploits developers' coarse-grained\nexpectations about how imported third-party code operates to restrict access to\nfiles, devices, and specific network destinations, at the granularity of\nindividual functions. To efficiently protect such sensitive OS resources,\nPyronia combines three techniques: system call interposition, stack inspection,\nand memory domains. This design avoids the need for application refactoring, or\nunintuitive data flow analysis, while enforcing the developer's access policy\nat run time. Our Pyronia prototype for Python runs on a custom Linux kernel,\nand incurs moderate performance overhead on unmodified Python applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01950v2"
    },
    {
        "title": "The Lustre Storage Architecture",
        "authors": [
            "Peter Braam"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  This lengthy document often referred to as the \"Lustre Book\", contains a\ndetailed outline of Lustre file system architecture, as it was created between\n2001 and 2005, in accordance with the requirements from various users. Now, in\n2019, most features have been implemented, but some only recently, and some\nalong different lines of thought.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01955v1"
    },
    {
        "title": "Processor in Non-Volatile Memory (PiNVSM): Towards to Data-centric\n  Computing in Decentralized Environment",
        "authors": [
            "Viacheslav Dubeyko"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  The AI problem has no solution in the environment of existing hardware stack\nand OS architecture. CPU-centric model of computation has a huge number of\ndrawbacks that originate from memory hierarchy and obsolete architecture of the\ncomputing core. The concept of mixing memory and logic has been around since\n1960s. However, the concept of Processor-In-Memory (PIM) is unable to resolve\nthe critical issues of the CPU-centric computing model because of inevitable\nreplication of von Neumann architecture's drawbacks. The next generation of\nNVM/SCM memory is able to give the second birth to the data-centric computing\nparadigm. This paper presents a concept of Processor in Non-Volatile Memory\n(PiNVSM) architecture. The basis of PiNVSM architecture is the concept of DPU\nthat contains the NVM memory and dedicated PU. All necessary PU's registers can\nbe implemented in the space of NVM memory. NVM memory of DPU is the single\nspace for storing and transformation of data. In the basis of PiNVSM\narchitecture lies the DPU array is able to overcome the limitations as Turing\nmachine model as von Neumann architecture. The DPU array hasn't a centralized\ncomputing core. Every data portion has dedicated computing core that excludes\nthe necessity to transfer data to the place of data processing. Every DPU\ncontains data portion that is associated with the set of keywords. Any complex\ndata structure can be split on elementary items that can be stored into\nindependent DPU with dedicated computing core(s). One DPU is able to apply the\nelementary transformation on one item. But the DPU array is able to make the\ntransformation of complex structure by means of concurrent execution of\nelementary transformations in different DPUs. The PiNVSM architecture suggests\na principally new architecture of the computing core that creates a new\nopportunity for data self-organization, data and code synthesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.03701v1"
    },
    {
        "title": "Nature of System Calls in CPU-centric Computing Paradigm",
        "authors": [
            "Viacheslav Dubeyko",
            "Om Rameshwar Gatla",
            "Mai Zheng"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Modern operating systems are typically POSIX-compliant with major system\ncalls specified decades ago. The next generation of non-volatile memory (NVM)\ntechnologies raise concerns about the efficiency of the traditional POSIX-based\nsystems. As one step toward building high performance NVM systems, we explore\nthe potential dependencies between system call performance and major hardware\ncomponents (e.g., CPU, memory, storage) under typical user cases (e.g.,\nsoftware compilation, installation, web browser, office suite) in this paper.\nWe build histograms for the most frequent and time-consuming system calls with\nthe goal to understand the nature of distribution on different platforms. We\nfind that there is a strong dependency between the system call performance and\nthe CPU architecture. On the other hand, the type of persistent storage plays a\nless important role in affecting the performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.04075v1"
    },
    {
        "title": "MultiK: A Framework for Orchestrating Multiple Specialized Kernels",
        "authors": [
            "Hsuan-Chi Kuo",
            "Akshith Gunasekaran",
            "Yeongjin Jang",
            "Sibin Mohan",
            "Rakesh B. Bobba",
            "David Lie",
            "Jesse Walker"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  We present, MultiK, a Linux-based framework 1 that reduces the attack surface\nfor operating system kernels by reducing code bloat. MultiK \"orchestrates\"\nmultiple kernels that are specialized for individual applications in a\ntransparent manner. This framework is flexible to accommodate different kernel\ncode reduction techniques and, most importantly, run the specialized kernels\nwith near-zero additional runtime overheads. MultiK avoids the overheads of\nvirtualization and runs natively on the system. For instance, an Apache\ninstance is shown to run on a kernel that has (a) 93.68% of its code reduced,\n(b) 19 of 23 known kernel vulnerabilities eliminated and (c) with negligible\nperformance overheads (0.19%). MultiK is a framework that can integrate with\nexisting code reduction and OS security techniques. We demonstrate this by\nusing D-KUT and S-KUT -- two methods to profile and eliminate unwanted kernel\ncode. The whole process is transparent to the user applications because MultiK\ndoes not require a recompilation of the application.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.06889v1"
    },
    {
        "title": "Interprocess Communication in FreeBSD 11: Performance Analysis",
        "authors": [
            "A. H. Bell-Thomas"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Interprocess communication, IPC, is one of the most fundamental functions of\na modern operating system, playing an essential role in the fabric of\ncontemporary applications. This report conducts an investigation in FreeBSD of\nthe real world performance considerations behind two of the most common IPC\nmechanisms; pipes and sockets. A simple benchmark provides a fair sense of\neffective bandwidth for each, and analysis using DTrace, hardware performance\ncounters and the operating system's source code is presented. We note that\npipes outperform sockets by 63% on average across all configurations, and\nfurther that the size of userspace transmission buffers has a profound effect\non performance - larger buffers are beneficial up to a point (~32-64 KiB) after\nwhich performance collapses as a result of devastating cache exhaustion. A deep\nscrutiny of the probe effects at play is also presented, justifying the\nvalidity of conclusions drawn from these experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.02145v1"
    },
    {
        "title": "eXpOS: A Simple Pedagogical Operating System for Undergraduate\n  Instruction",
        "authors": [
            "K. Murali Krishnan"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  An operating system project suitable for undergraduate computing/electrical\nsciences students is presented. The project can be used as a course project in\na one semester course, or as a self-study project for motivated students. The\ncourse is organized such that a student with a basic background in programming\nand computer organization can follow the implementation road map available\nonline, and build the OS from scratch on her personal machine/laptop, with\nminimal instructional supervision. The student is provided with a simulated\nabstract machine, an application interface specification, specification and\ndesign of the OS, and a step by step project implementation road map. The\nfunctionalities of the OS include multitasking, virtual memory, semaphores,\nshared memory, an elementary file system, interrupt driven disk and console\nI/O, and a limited multi-user support. The final stage of the project involves\nporting the OS to a two-core machine. An independent one semester compiler\ndesign project, where the student builds a compiler for a tiny object oriented\nprogramming language that generates target code that can be loaded and executed\nby the OS is also briefly discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.03563v1"
    },
    {
        "title": "Optimizations of Management Algorithms for Multi-Level Memory Hierarchy",
        "authors": [
            "Gal Oren"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  In the near future the SCM is predicted to modify the form of new programs,\nthe access form to storage, and the way that storage devices themselves are\nbuilt. Therefore, a combination between the SCM and a designated Memory\nAllocation Manager (MAM) that will allow the programmer to manually control the\ndifferent memories in the memory hierarchy will be likely to achieve a new\nlevel of performance for memory-aware data structures. Although the manual MAM\nseems to be the optimal approach for multi-level memory hierarchy management,\nthis technique is still very far from being realistic, and the chances that it\nwould be implemented in current codes using High Performance Computing (HPC)\nplatforms is quite low. This premise means that the most reasonable way to\nintroduce the SCM into any usable and popular memory system would be by\nimplementing an automated version of the MAM using the fundamentals of paging\nalgorithms, as used for two-level memory hierarchy. Our hypothesis is that\nachieving appropriate transferability between memory levels may be possible\nusing ideas of algorithms employed in current virtual memory systems, and that\nthe adaptation of those algorithms from a two-level memory hierarchy to an\nN-level memory hierarchy is possible. In order to reach the conclusion that our\nhypothesis is correct, we investigated various paging algorithms, and found the\nones that could be adapted successfully from two-level memory hierarchy to an\nN-level memory hierarchy. We discovered that using an adaptation of the Aging\npaging algorithm to an N-level memory hierarchy results in the best\nperformances in terms of Hit/Miss ratio. In order to verify our hypothesis we\nbuild a simulator called \"DeMemory simulator\" for analyzing our algorithms as\nwell as for other algorithms that will be devised in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.07161v1"
    },
    {
        "title": "FluidMem: Memory as a Service for the Datacenter",
        "authors": [
            "Blake Caldwell",
            "Youngbin Im",
            "Sangtae Ha",
            "Richard Han",
            "Eric Keller"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  Disaggregating resources in data centers is an emerging trend. Recent work\nhas begun to explore memory disaggregation, but suffers limitations including\nlack of consideration of the complexity of cloud-based deployment, including\nheterogeneous hardware and APIs for cloud users and operators. In this paper,\nwe present FluidMem, a complete system to realize disaggregated memory in the\ndatacenter. Going beyond simply demonstrating remote memory is possible, we\ncreate an entire Memory as a Service. We define the requirements of Memory as a\nService and build its implementation in Linux as FluidMem. We present a\nperformance analysis of FluidMem and demonstrate that it transparently supports\nremote memory for standard applications such as MongoDB and genome sequencing\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.07780v1"
    },
    {
        "title": "Analyzing IO Amplification in Linux File Systems",
        "authors": [
            "Jayashree Mohan",
            "Rohan Kadekodi",
            "Vijay Chidambaram"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  We present the first systematic analysis of read, write, and space\namplification in Linux file systems. While many researchers are tackling write\namplification in key-value stores, IO amplification in file systems has been\nlargely unexplored. We analyze data and metadata operations on five widely-used\nLinux file systems: ext2, ext4, XFS, btrfs, and F2FS. We find that data\noperations result in significant write amplification (2-32X) and that metadata\noperations have a large IO cost. For example, a single rename requires 648 KB\nwrite IO in btrfs. We also find that small random reads result in read\namplification of 2-13X. Based on these observations, we present the CReWS\nconjecture about the relationship between IO amplification, consistency, and\nstorage space utilization. We hope this paper spurs people to design future\nfile systems with less IO amplification, especially for non-volatile memory\ntechnologies.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.08514v1"
    },
    {
        "title": "Characterizing Synchronous Writes in Stable Memory Devices",
        "authors": [
            "William B. Mingardi",
            "Gustavo M. D. Vieira"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Distributed algorithms that operate in the fail-recovery model rely on the\nstate stored in stable memory to guarantee the irreversibility of operations\neven in the presence of failures. The performance of these algorithms lean\nheavily on the performance of stable memory. Current storage technologies have\na defined performance profile: data is accessed in blocks of hundreds or\nthousands of bytes, random access to these blocks is expensive and sequential\naccess is somewhat better. File system implementations hide some of the\nperformance limitations of the underlying storage devices using buffers and\ncaches. However, fail-recovery distributed algorithms bypass some of these\ntechniques and perform synchronous writes to be able to tolerate a failure\nduring the write itself. Assuming the distributed system designer is able to\nbuffer the algorithm's writes, we ask how buffer size and latency complement\neach other. In this paper we start to answer this question by characterizing\nthe performance (throughput and latency) of typical stable memory devices using\na representative set of current file systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.07515v1"
    },
    {
        "title": "LibrettOS: A Dynamically Adaptable Multiserver-Library OS",
        "authors": [
            "Ruslan Nikolaev",
            "Mincheol Sung",
            "Binoy Ravindran"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  We present LibrettOS, an OS design that fuses two paradigms to simultaneously\naddress issues of isolation, performance, compatibility, failure\nrecoverability, and run-time upgrades. LibrettOS acts as a microkernel OS that\nruns servers in an isolated manner. LibrettOS can also act as a library OS\nwhen, for better performance, selected applications are granted exclusive\naccess to virtual hardware resources such as storage and networking.\nFurthermore, applications can switch between the two OS modes with no\ninterruption at run-time. LibrettOS has a uniquely distinguishing advantage in\nthat, the two paradigms seamlessly coexist in the same OS, enabling users to\nsimultaneously exploit their respective strengths (i.e., greater isolation,\nhigh performance). Systems code, such as device drivers, network stacks, and\nfile systems remain identical in the two modes, enabling dynamic mode switching\nand reducing development and maintenance costs.\n  To illustrate these design principles, we implemented a prototype of\nLibrettOS using rump kernels, allowing us to reuse existent, hardened NetBSD\ndevice drivers and a large ecosystem of POSIX/BSD-compatible applications. We\nuse hardware (VM) virtualization to strongly isolate different rump kernel\ninstances from each other. Because the original rumprun unikernel targeted a\nmuch simpler model for uniprocessor systems, we redesigned it to support\nmulticore systems. Unlike kernel-bypass libraries such as DPDK, applications\nneed not be modified to benefit from direct hardware access. LibrettOS also\nsupports indirect access through a network server that we have developed.\nApplications remain uninterrupted even when network components fail or need to\nbe upgraded. Finally, to efficiently use hardware resources, applications can\ndynamically switch between the indirect and direct modes based on their I/O\nload at run-time.\n  [full abstract is in the paper]\n",
        "pdf_link": "http://arxiv.org/pdf/2002.08928v1"
    },
    {
        "title": "Exact Feasibility Tests for Real-Time Scheduling of Periodic Tasks upon\n  Multiprocessor Platforms",
        "authors": [
            "Liliana Cucu",
            "Joël Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  In this paper we study the global scheduling of periodic task systems upon\nmultiprocessor platforms. We first show two very general properties which are\nwell-known for uniprocessor platforms and which remain for multiprocessor\nplatforms: (i) under few and not so restrictive assumptions, we show that\nfeasible schedules of periodic task systems are periodic from some point with a\nperiod equal to the least common multiple of task periods and (ii) for the\nspecific case of synchronous periodic task systems, we show that feasible\nschedules repeat from the origin. We then present our main result: we\ncharacterize, for task-level fixed-priority schedulers and for asynchronous\nconstrained or arbitrary deadline periodic task models, upper bounds of the\nfirst time instant where the schedule repeats. We show that job-level\nfixed-priority schedulers are predictable upon unrelated multiprocessor\nplatforms. For task-level fixed-priority schedulers, based on the upper bounds\nand the predictability property, we provide for asynchronous constrained or\narbitrary deadline periodic task sets, exact feasibility tests. Finally, for\nthe job-level fixed-priority EDF scheduler, for which such an upper bound\nremains unknown, we provide an exact feasibility test as well.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.4292v1"
    },
    {
        "title": "A Comparative Study of CPU Scheduling Algorithms",
        "authors": [
            "Neetu Goel",
            "R. B. Garg"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Developing CPU scheduling algorithms and understanding their impact in\npractice can be difficult and time consuming due to the need to modify and test\noperating system kernel code and measure the resulting performance on a\nconsistent workload of real applications. As processor is the important\nresource, CPU scheduling becomes very important in accomplishing the operating\nsystem (OS) design goals. The intention should be allowed as many as possible\nrunning processes at all time in order to make best use of CPU. This paper\npresents a state diagram that depicts the comparative study of various\nscheduling algorithms for a single CPU and shows which algorithm is best for\nthe particular situation. Using this representation, it becomes much easier to\nunderstand what is going on inside the system and why a different set of\nprocesses is a candidate for the allocation of the CPU at different time. The\nobjective of the study is to analyze the high efficient CPU scheduler on design\nof the high quality scheduling algorithms which suits the scheduling goals. Key\nWords:-Scheduler, State Diagrams, CPU-Scheduling, Performance\n",
        "pdf_link": "http://arxiv.org/pdf/1307.4165v1"
    },
    {
        "title": "An Optimum Multilevel Dynamic Round Robin Scheduling Algorithm",
        "authors": [
            "Neetu Goel",
            "R. B. Garg"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  The main objective of this paper is to improve the Round Robin scheduling\nalgorithm using the dynamic time slice concept. CPU scheduling becomes very\nimportant in accomplishing the operating system (OS) design goals. The\nintention should be allowed as many as possible running processes at all time\nin order to make best use of CPU. CPU scheduling has strong effect on resource\nutilization as well as overall performance of the system. Round Robin algorithm\nperforms optimally in time-shared systems, but it is not suitable for soft real\ntime systems, because it gives more number of context switches, larger waiting\ntime and larger response time. In this paper, a new CPU scheduling algorithm\ncalled An Optimum Multilevel Dynamic Round Robin Scheduling Algorithm is\nproposed, which calculates intelligent time slice and changes after every round\nof execution. The suggested algorithm was evaluated on some CPU scheduling\nobjectives and it was observed that this algorithm gave good performance as\ncompared to the other existing CPU scheduling algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.4167v1"
    },
    {
        "title": "Toward Parametric Timed Interfaces for Real-Time Components",
        "authors": [
            "Youcheng Sun",
            "Giuseppe Lipari",
            "Étienne André",
            "Laurent Fribourg"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  We propose here a framework to model real-time components consisting of\nconcurrent real-time tasks running on a single processor, using parametric\ntimed automata. Our framework is generic and modular, so as to be easily\nadapted to different schedulers and more complex task models. We first perform\na parametric schedulability analysis of the components using the inverse\nmethod. We show that the method unfortunately does not provide satisfactory\nresults when the task periods are consid- ered as parameters. After identifying\nand explaining the problem, we present a solution adapting the model by making\nuse of the worst-case scenario in schedulability analysis. We show that the\nanalysis with the inverse method always converges on the modified model when\nthe system load is strictly less than 100%. Finally, we show how to use our\nparametric analysis for the generation of timed interfaces in compositional\nsystem design.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.0088v1"
    },
    {
        "title": "An Enhanced Multi-Pager Environment Support for Second Generation\n  Microkernels",
        "authors": [
            "Yauhen Klimiankou"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  The main objective of this paper is to present a mechanism of enhanced paging\nsupport for the second generation microkernels in the form of explicit support\nof multi-pager environment for the tasks running in the system. Proposed\nmechanism is based on the intra-kernel high granularity pagers assignments per\nvirtual address space, which allow efficient and simple dispatching of page\nfaults to the appropriate pagers. The paging is one of the major features of\nthe virtual memory, which is extensively used by advanced operating systems to\nprovide an illusion of elastic memory. Original and present second generation\nmicrokernels provide only limited, inflexible and unnatural support for paging.\nFurthermore, facilities provided by current solutions for multi-pager support\non the runtime level introduce an overhead in terms of mode switches and thread\ncontext switches which can be significantly reduced. Limited paging support\nlimits the attractiveness of the second generation microkernel based systems\nuse in real-life applications, in which processes usually have concurrent\nservicing of multiple paging servers. The purpose of this paper is to present a\nfacilities for the efficient and flexible support of multi-pager environments\nfor the second generation microkernels. A comparison of the proposed solution\nto the present architecture L4 + L4Re has been made and overhead of the page\nfault handling critical path has been evaluated. Proposed solution is simple\nenough and provides a natural and flexible support of multi-pager environments\nfor second generation microkernels in efficient way. It introduces a third less\noverhead in terms of the mode switches and thread context switches in\ncomparison to the present L4 + L4Re solution implemented in the Fiasco.OC.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.1637v1"
    },
    {
        "title": "An Effective Round Robin Algorithm using Min-Max Dispersion Measure",
        "authors": [
            "Sanjaya Kumar Panda",
            "Sourav Kumar Bhoi"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  Round Robin (RR) scheduling algorithm is a preemptive scheduling algorithm.\nIt is designed especially for time sharing Operating System (OS). In RR\nscheduling algorithm the CPU switches between the processes when the static\nTime Quantum (TQ) expires. RR scheduling algorithm is considered as the most\nwidely used scheduling algorithm in research because the TQ is equally shared\namong the processes. In this paper a newly proposed variant of RR algorithm\ncalled Min-Max Round Robin (MMRR) scheduling algorithm is presented. The idea\nof this MMRR is to make the TQ repeatedly adjusted using Min-Max dispersion\nmeasure in accordance with remaining CPU burst time. Our experimental analysis\nshows that MMRR performs much better than RR algorithm in terms of average\nturnaround time, average waiting time and number of context switches.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.5869v1"
    },
    {
        "title": "Enhancing CPU Performance using Subcontrary Mean Dynamic Round Robin\n  (SMDRR) Scheduling Algorithm",
        "authors": [
            "Sourav Kumar Bhoi",
            "Sanjaya Kumar Panda",
            "Debashee Tarai"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  Round Robin (RR) Algorithm is considered as optimal in time shared\nenvironment because the static time is equally shared among the processes. If\nthe time quantum taken is static then it undergoes degradation of the CPU\nperformance and leads to so many context switches. In this paper, we have\nproposed a new effective dynamic RR algorithm SMDRR (Subcontrary Mean Dynamic\nRound Robin) based on dynamic time quantum where we use the subcontrary mean or\nharmonic mean to find the time quantum. The idea of this approach is to make\nthe time quantum repeatedly adjusted according to the burst time of the\ncurrently running processes. Our experimental analysis shows that SMDRR\nperforms better than RR algorithm in terms of reducing the number of context\nswitches, average turnaround time and average waiting time.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.6087v1"
    },
    {
        "title": "Optimize Unsynchronized Garbage Collection in an SSD Array",
        "authors": [
            "Da Zheng",
            "Randal Burns",
            "Alexander S. Szalay"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Solid state disks (SSDs) have advanced to outperform traditional hard drives\nsignificantly in both random reads and writes. However, heavy random writes\ntrigger fre- quent garbage collection and decrease the performance of SSDs. In\nan SSD array, garbage collection of individ- ual SSDs is not synchronized,\nleading to underutilization of some of the SSDs.\n  We propose a software solution to tackle the unsyn- chronized garbage\ncollection in an SSD array installed in a host bus adaptor (HBA), where\nindividual SSDs are exposed to an operating system. We maintain a long I/O\nqueue for each SSD and flush dirty pages intelligently to fill the long I/O\nqueues so that we hide the performance imbalance among SSDs even when there are\nfew parallel application writes. We further define a policy of select- ing\ndirty pages to flush and a policy of taking out stale flush requests to reduce\nthe amount of data written to SSDs. We evaluate our solution in a real system.\nExperi- ments show that our solution fully utilizes all SSDs in an array under\nrandom write-heavy workloads. It improves I/O throughput by up to 62% under\nrandom workloads of mixed reads and writes when SSDs are under active garbage\ncollection. It causes little extra data writeback and increases the cache hit\nrate.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.07566v1"
    },
    {
        "title": "TinyLFU: A Highly Efficient Cache Admission Policy",
        "authors": [
            "Gil Einziger",
            "Roy Friedman",
            "Ben Manes"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  This paper proposes to use a frequency based cache admission policy in order\nto boost the effectiveness of caches subject to skewed access distributions.\nGiven a newly accessed item and an eviction candidate from the cache, our\nscheme decides, based on the recent access history, whether it is worth\nadmitting the new item into the cache at the expense of the eviction candidate.\n  Realizing this concept is enabled through a novel approximate LFU structure\ncalled TinyLFU, which maintains an approximate representation of the access\nfrequency of a large sample of recently accessed items. TinyLFU is very compact\nand light-weight as it builds upon Bloom filter theory.\n  We study the properties of TinyLFU through simulations of both synthetic\nworkloads as well as multiple real traces from several sources. These\nsimulations demonstrate the performance boost obtained by enhancing various\nreplacement policies with the TinyLFU eviction policy. Also, a new combined\nreplacement and eviction policy scheme nicknamed W-TinyLFU is presented.\nW-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state\nof the art replacement policies on these traces. It is the only scheme to\nobtain such good results on all traces.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.00727v2"
    },
    {
        "title": "Real-Time scheduling: from hard to soft real-time systems",
        "authors": [
            "Giuseppe Lipari",
            "Luigi Palopoli"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Real-time systems are traditionally classified into hard real-time and soft\nreal-time: in the first category we have safety critical real-time systems\nwhere missing a deadline can have catastrophic consequences, whereas in the\nsecond class we find systems or which we need to optimise the Quality of\nservice provided to the user. However, the frontier between these two classes\nis thinner than one may think, and many systems that were considered as hard\nreal-time in the past should now be reconsidered under a different light. In\nthis paper we shall first recall the fundamental notion of time-predictability\nand criticality, in order to understand where the real-time deadlines that we\nuse in our theoretical models come from. We shall then introduce the model of a\nsoft real-time system and present one popular method for scheduling hard and\nsoft real-time tasks, the resource reservation framework. Finally, we shall\nshow how resource reservation techniques can be successfully applied to the\ndesign of classical control systems, thus adding robustness to the system and\nincreasing resource utilisation and performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.01978v1"
    },
    {
        "title": "Parallel and sequential reclaiming in multicore real-time global\n  scheduling",
        "authors": [
            "Luca Abeni",
            "Giuseppe Lipari",
            "Andrea Parri",
            "Youcheng Sun"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  When integrating hard, soft and non-real-time tasks in general purpose\noperating systems, it is necessary to provide temporal isolation so that the\ntiming properties of one task do not depend on the behaviour of the others.\nHowever, strict budget enforcement can lead to inefficient use of the\ncomputational resources in the presence of tasks with variable workload. Many\nresource reclaiming algorithms have been proposed in the literature for single\nprocessor scheduling, but not enough work exists for global scheduling in\nmultiprocessor systems. In this report, we propose two reclaiming algorithms\nfor multiprocessor global scheduling and we prove their correctness.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.01984v2"
    },
    {
        "title": "Research on Scalability of Operating Systems on Multicore Processors",
        "authors": [
            "Yan Cui"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Large number of cores and hardware resource sharing are two characteristics\non multicore processors, which bring new challenges for the design of operating\nsystems. How to locate and analyze the speedup restrictive factors in operating\nsystems, how to simulate and avoid the phenomenon that speedup decreases with\nthe number of cores because of lock contention (i.e., lock thrashing) and how\nto avoid the contention of shared resources such as the last level cache are\nkey challenges for the operating system scalability research on multicore\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.06908v1"
    },
    {
        "title": "Energy-aware Fixed-Priority Multi-core Scheduling for Real-time Systems",
        "authors": [
            "Yao Guo",
            "Junyang Lu"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Multi-core processors are becoming more and more popular in embedded and\nreal-time systems. While fixed-priority scheduling with task-splitting in\nreal-time systems are widely applied, current approaches have not taken into\nconsideration energy-aware aspects such as dynamic voltage/frequency scheduling\n(DVS). In this paper, we propose two strategies to apply dynamic voltage\nscaling (DVS) to fixed-priority scheduling algorithms with task-splitting for\nperiodic real-time tasks on multi-core processors. The first strategy\ndetermines voltage scales for each processor after scheduling (Static DVS),\nwhich ensures all tasks meet the timing requirements on synchronization. The\nsecond strategy adaptively determines the frequency of each task before\nscheduling (Adaptive DVS) according to the total utilization of task-set and\nnumber of cores available. The combination of frequency pre-allocation and\ntask-splitting makes it possible to maximize energy savings with DVS.\nSimulation results show that it is possible to achieve significant energy\nsavings with DVS while preserving the schedulability requirements of real-time\nschedulers for multi-core processors.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.07351v1"
    },
    {
        "title": "Mixed-Criticality Scheduling with I/O",
        "authors": [
            "Eric Missimer",
            "Katherine Zhao",
            "Richard West"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  This paper addresses the problem of scheduling tasks with different\ncriticality levels in the presence of I/O requests. In mixed-criticality\nscheduling, higher criticality tasks are given precedence over those of lower\ncriticality when it is impossible to guarantee the schedulability of all tasks.\nWhile mixed-criticality scheduling has gained attention in recent years, most\napproaches typically assume a periodic task model. This assumption does not\nalways hold in practice, especially for real-time and embedded systems that\nperform I/O. For example, many tasks block on I/O requests until devices signal\ntheir completion via interrupts; both the arrival of interrupts and the waking\nof blocked tasks can be aperiodic. In our prior work, we developed a scheduling\ntechnique in the Quest real-time operating system, which integrates the\ntime-budgeted management of I/O operations with Sporadic Server scheduling of\ntasks. This paper extends our previous scheduling approach with support for\nmixed-criticality tasks and I/O requests on the same processing core. Results\nshow the effective schedulability of different task sets in the presence of I/O\nrequests is superior in our approach compared to traditional methods that\nmanage I/O using techniques such as Sporadic Servers.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.07654v3"
    },
    {
        "title": "Flashield: a Key-value Cache that Minimizes Writes to Flash",
        "authors": [
            "Assaf Eisenman",
            "Asaf Cidon",
            "Evgenya Pergament",
            "Or Haimovich",
            "Ryan Stutsman",
            "Mohammad Alizadeh",
            "Sachin Katti"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  As its price per bit drops, SSD is increasingly becoming the default storage\nmedium for cloud application databases. However, it has not become the\npreferred storage medium for key-value caches, even though SSD offers more than\n10x lower price per bit and sufficient performance compared to DRAM. This is\nbecause key-value caches need to frequently insert, update and evict small\nobjects. This causes excessive writes and erasures on flash storage, since\nflash only supports writes and erasures of large chunks of data. These\nexcessive writes and erasures significantly shorten the lifetime of flash,\nrendering it impractical to use for key-value caches. We present Flashield, a\nhybrid key-value cache that uses DRAM as a \"filter\" to minimize writes to SSD.\nFlashield performs light-weight machine learning profiling to predict which\nobjects are likely to be read frequently before getting updated; these objects,\nwhich are prime candidates to be stored on SSD, are written to SSD in large\nchunks sequentially. In order to efficiently utilize the cache's available\nmemory, we design a novel in-memory index for the variable-sized objects stored\non flash that requires only 4 bytes per object in DRAM. We describe Flashield's\ndesign and implementation and, we evaluate it on a real-world cache trace.\nCompared to state-of-the-art systems that suffer a write amplification of 2.5x\nor more, Flashield maintains a median write amplification of 0.5x without any\nloss of hit rate or throughput.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.02588v1"
    },
    {
        "title": "The Case for a Single System Image for Personal Devices",
        "authors": [
            "Beom Heyn Kim",
            "Eyal de Lara",
            "David Lie"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  Computing technology has gotten cheaper and more powerful, allowing users to\nhave a growing number of personal computing devices at their disposal. While\nthis trend is beneficial for the user, it also creates a growing management\nburden for the user. Each device must be managed independently and users must\nrepeat the same management tasks on the each device, such as updating software,\nchanging configurations, backup, and replicating data for availability. To\nprevent the management burden from increasing with the number of devices, we\npropose that all devices run a single system image called a personal computing\nimage. Personal computing images export a device-specific user interface on\neach device, but provide a consistent view of application and operating state\nacross all devices. As a result, management tasks can be performed once on any\ndevice and will be automatically propagated to all other devices belonging to\nthe user. We discuss evolutionary steps that can be taken to achieve personal\ncomputing images for devices and elaborate on challenges that we believe\nbuilding such systems will face.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.03789v1"
    },
    {
        "title": "Slicing the IO execution with ReLayTracer",
        "authors": [
            "Ganguk Lee",
            "Yeaseul Park",
            "Jeongseob Ahn",
            "Youngjin Kwon"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Analyzing IO performance anomalies is a crucial task in various computing\nenvironments, ranging from large-scale cloud applications to desktop\napplications. However, the IO stack of modern operating systems is complicated,\nmaking it hard to understand the performance anomalies with existing tools.\nKernel IO executions are frequently interrupted by internal kernel activities,\nrequiring a sophisticated IO profile tool to deal with the noises. Furthermore,\ncomplicated interactions of concurrent IO requests cause different sources of\ntail latencies in kernel IO stack. As a consequence, developers want to know\nfine-grained latency profile across IO layers, which may differ in each IO\nrequests. To meet the requirements, this paper suggests ReLayTracer, a\nper-request, per-layer IO profiler. ReLayTracer enables a detailed analysis to\nidentify root causes of IO performance anomalies by providing per-layer latency\ndistributions of each IO request, hardware performance behavior, and time spent\nby kernel activities such as an interrupt.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.07124v1"
    },
    {
        "title": "toki: A Build- and Test-Platform for Prototyping and Evaluating\n  Operating System Concepts in Real-Time Environments",
        "authors": [
            "Oliver Horst",
            "Uwe Baumgarten"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Typically, even low-level operating system concepts, such as resource sharing\nstrategies and predictability measures, are evaluated with Linux on PC\nhardware. This leaves a large gap to real industrial applications. Hence, the\ndirect transfer of the results might be difficult. As a solution, we present\ntoki, a prototyping and evaluation platform based on FreeRTOS and several\nopen-source libraries. toki comes with a unified build- and test-environment\nbased on Yocto and Qemu, which makes it well suited for rapid prototyping. With\nits architecture chosen similar to production industrial systems, toki provides\nthe ground work to implement early prototypes of real-time systems research\nresults, up to technology readiness level 7, with little effort.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00466v1"
    },
    {
        "title": "Quantifying the Latency and Possible Throughput of External Interrupts\n  on Cyber-Physical Systems",
        "authors": [
            "Oliver Horst",
            "Johannes Wiesböck",
            "Raphael Wild",
            "Uwe Baumgarten"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  An important characteristic of cyber-physical systems is their capability to\nrespond, in-time, to events from their physical environment. However, to the\nbest of our knowledge there exists no benchmark for assessing and comparing the\ninterrupt handling performance of different software stacks. Hence, we present\na flexible evaluation method for measuring the interrupt latency and throughput\non ARMv8-A based platforms. We define and validate seven test-cases that stress\nindividual parts of the overall process and combine them to three benchmark\nfunctions that provoke the minimal and maximal interrupt latency, and maximal\ninterrupt throughput.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00506v1"
    },
    {
        "title": "Secure Memory Management on Modern Hardware",
        "authors": [
            "Reto Achermann",
            "Nora Hossle",
            "Lukas Humbel",
            "Daniel Schwyn",
            "David Cock",
            "Timothy Roscoe"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Almost all modern hardware, from phone SoCs to high-end servers with\naccelerators, contain memory translation and protection hardware like IOMMUs,\nfirewalls, and lookup tables which make it impossible to reason about, and\nenforce protection and isolation based solely on the processor's MMUs. This has\nled to numerous bugs and security vulnerabilities in today's system software.\n  In this paper we regain the ability to reason about and enforce access\ncontrol using the proven concept of a reference monitor mediating accesses to\nmemory resources. We present a fine-grained, realistic memory protection model\nthat makes this traditional concept applicable today, and bring system software\nin line with the complexity of modern, heterogeneous hardware.\n  Our design is applicable to any operating system, regardless of architecture.\nWe show that it not only enforces the integrity properties of a system, but\ndoes so with no inherent performance overhead and it is even amenable to\nautomation through code generation from trusted hardware specifications.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02737v1"
    },
    {
        "title": "Akita: A CPU scheduler for virtualized Clouds",
        "authors": [
            "Esmail Asyabi",
            "Azer Bestavros",
            "Renato Mancuso",
            "Richard West",
            "Erfan Sharafzadeh"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Clouds inherit CPU scheduling policies of operating systems. These policies\nenforce fairness while leveraging best-effort mechanisms to enhance\nresponsiveness of all schedulable entities, irrespective of their service level\nobjectives (SLOs). This leads to unpredictable performance that forces cloud\nproviders to enforce strict reservation and isolation policies to prevent\nhigh-criticality services (e.g., Memcached) from being impacted by\nlow-criticality ones (e.g., logging), which results in low utilization.\n  In this paper, we present Akita, a hypervisor CPU scheduler that delivers\npredictable performance at high utilization. Akita allows virtual machines\n(VMs) to be categorized into high- and low-criticality VMs. Akita provides\nstrong guarantees on the ability of cloud providers to meet SLOs of\nhigh-criticality VMs, by temporarily slowing down low-criticality VMs if\nnecessary. Akita, therefore, allows the co-existence of high and\nlow-criticality VMs on the same physical machine, leading to higher\nutilization. The effectiveness of Akita is demonstrated by a prototype\nimplementation in the Xen hypervisor. We present experimental results that show\nthe many advantages of adopting Akita as the hypervisor CPU scheduler. In\nparticular, we show that high-criticality Memcached VMs are able to deliver\npredictable performance despite being co-located with low-criticality CPU-bound\nVMs.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09104v1"
    },
    {
        "title": "Augmenting Operating Systems With the GPU",
        "authors": [
            "Weibin Sun",
            "Robert Ricci"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  The most popular heterogeneous many-core platform, the CPU+GPU combination,\nhas received relatively little attention in operating systems research. This\nplatform is already widely deployed: GPUs can be found, in some form, in most\ndesktop and laptop PCs. Used for more than just graphics processing, modern\nGPUs have proved themselves versatile enough to be adapted to other\napplications as well. Though GPUs have strengths that can be exploited in\nsystems software, this remains a largely untapped resource. We argue that\naugmenting the OS kernel with GPU computing power opens the door to a number of\nnew opportunities. GPUs can be used to speed up some kernel functions, make\nother scale better, and make it feasible to bring some computation-heavy\nfunctionality into the kernel. We present our framework for using the GPU as a\nco-processor from an OS kernel, and demonstrate a prototype in Linux.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.3345v1"
    },
    {
        "title": "On the periodic behavior of real-time schedulers on identical\n  multiprocessor platforms",
        "authors": [
            "Emmanuel Grolleau",
            "Joël Goossens",
            "Liliana Cucu-Grosjean"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  This paper is proposing a general periodicity result concerning any\ndeterministic and memoryless scheduling algorithm (including\nnon-work-conserving algorithms), for any context, on identical multiprocessor\nplatforms. By context we mean the hardware architecture (uniprocessor,\nmulticore), as well as task constraints like critical sections, precedence\nconstraints, self-suspension, etc. Since the result is based only on the\nreleases and deadlines, it is independent from any other parameter. Note that\nwe do not claim that the given interval is minimal, but it is an upper bound\nfor any cycle of any feasible schedule provided by any deterministic and\nmemoryless scheduler.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.3849v1"
    },
    {
        "title": "CannyFS: Opportunistically Maximizing I/O Throughput Exploiting the\n  Transactional Nature of Batch-Mode Data Processing",
        "authors": [
            "Jessica Nettelblad",
            "Carl Nettelblad"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  We introduce a user mode file system, CannyFS, that hides latency by assuming\nall I/O operations will succeed. The user mode process will in turn report\nerrors, allowing proper cleanup and a repeated attempt to take place. We\ndemonstrate benefits for the model tasks of extracting archives and removing\ndirectory trees in a real-life HPC environment, giving typical reductions in\ntime use of over 80%.\n  This approach can be considered a view of HPC jobs and their I/O activity as\ntransactions. In general, file systems lack clearly defined transaction\nsemantics. Over time, the competing trends to add cache and maintain data\nintegrity have resulted in different practical tradeoffs.\n  High-performance computing is a special case where overall throughput demands\nare high. Latency can also be high, with non-local storage. In addition, a\ntheoretically possible I/O error (like permission denied, loss of connection,\nexceeding disk quota) will frequently warrant the resubmission of a full job or\ntask, rather than traditional error reporting or handling. Therefore,\nopportunistically treating each I/O operation as successful, and part of a\nlarger transaction, can speed up some applications that do not leverage\nasynchronous I/O.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.06830v1"
    },
    {
        "title": "Datacenter RPCs can be General and Fast",
        "authors": [
            "Anuj Kalia",
            "Michael Kaminsky",
            "David G. Andersen"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  It is commonly believed that datacenter networking software must sacrifice\ngenerality to attain high performance. The popularity of specialized\ndistributed systems designed specifically for niche technologies such as RDMA,\nlossless networks, FPGAs, and programmable switches testifies to this belief.\nIn this paper, we show that such specialization is not necessary. eRPC is a new\ngeneral-purpose remote procedure call (RPC) library that offers performance\ncomparable to specialized systems, while running on commodity CPUs in\ntraditional datacenter networks based on either lossy Ethernet or lossless\nfabrics. eRPC performs well in three key metrics: message rate for small\nmessages; bandwidth for large messages; and scalability to a large number of\nnodes and CPU cores. It handles packet loss, congestion, and background request\nexecution. In microbenchmarks, one CPU core can handle up to 10 million small\nRPCs per second, or send large messages at 75 Gbps. We port a production-grade\nimplementation of Raft state machine replication to eRPC without modifying the\ncore Raft source code. We achieve 5.5 microseconds of replication latency on\nlossy Ethernet, which is faster than or comparable to specialized replication\nsystems that use programmable switches, FPGAs, or RDMA.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.00680v2"
    },
    {
        "title": "Minimizing Event-Handling Latencies in Secure Virtual Machines",
        "authors": [
            "Janis Danisevskis",
            "Michael Peter",
            "Jan Nordholz"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Virtualization, after having found widespread adoption in the server and\ndesktop arena, is poised to change the architecture of embedded systems as\nwell. The benefits afforded by virtualization - enhanced isolation,\nmanageability, flexibility, and security - could be instrumental for developers\nof embedded systems as an answer to the rampant increase in complexity.\n  While mature desktop and server solutions exist, they cannot be easily reused\non embedded systems because of markedly different requirements. Unfortunately,\noptimizations aimed at throughput, important for servers, often compromise on\naspects like predictable real-time behavior, which are crucial to many embedded\nsystems. In a similar vein, the requirements for small trusted computing bases,\nlightweight inter-VM communication, and small footprints are often not\naccommodated. This observation suggests that virtual machines for embedded\nsystems should be constructed from scratch with particular attention paid to\nthe specific requirements.\n  In this paper, we set out with a virtual machine designed for\nsecurity-conscious workloads and describe the steps necessary to achieve good\nevent-handling latencies. That evolution is possible because the underlying\nmicrokernel is well suited to satisfy real-time requirements. As the guest\nsystem we chose Linux with the PREEMPT_RT configuration, which itself was\ndeveloped in an effort to bring down event-handling latencies in a general\npurpose system. Our results indicate that the increase of event-handling\nlatencies of a guest running in a virtual machine does not, compared to native\nexecution, exceed a factor of two.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01147v1"
    },
    {
        "title": "Blocking time under basic priority inheritance: Polynomial bound and\n  exact computation",
        "authors": [
            "Paolo Torroni",
            "Zeynep Kiziltan",
            "Eugenio Faldella"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  The Priority Inheritance Protocol (PIP) is arguably the best-known protocol\nfor resource sharing under real-time constraints. Its importance in modern\napplications is undisputed. Nevertheless, because jobs may be blocked under PIP\nfor a variety of reasons, determining a job's maximum blocking time could be\ndifficult, and thus far no exact method has been proposed that does it.\nExisting analysis methods are inefficient, inaccurate, and of limited\napplicability. This article proposes a new characterization of the problem,\nthus allowing a polynomial method for bounding the blocking time, and an exact,\noptimally efficient method for blocking time computation under priority\ninheritance that have a general applicability.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01589v2"
    },
    {
        "title": "Integrating Proactive Mode Changes in Mixed Criticality Systems",
        "authors": [
            "Flavio R Massaro Jr.",
            "Paulo S. Martins",
            "Edson L. Ursini"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  In this work, we propose to integrate prediction algorithms to the scheduling\nof mode changes under the Earliest-Deadline-First and Fixed-priority scheduling\nin mixed-criticality real-time systems. The method proactively schedules a mode\nchange in the system based on state variables such as laxity, to the percentage\ndifference in the temporal distance between the completion time of the instance\nof a task and its respective deadline, by the deadline (D) stipulated for the\ntask, in order to minimize deadline misses. The simulation model was validated\nagainst an analytical model prior to the logical integration of the\nKalman-based prediction algorithm. Two study cases were presented, one covering\nearliest-deadline first and the other the fixed-priority scheduling approach.\nThe results showed the gains in the adoption of the prediction approach for\nboth scheduling paradigms by presenting a significant reduction of the number\nof missed deadlines for low-criticality tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.11431v1"
    },
    {
        "title": "Cloud Programming Simplified: A Berkeley View on Serverless Computing",
        "authors": [
            "Eric Jonas",
            "Johann Schleier-Smith",
            "Vikram Sreekanti",
            "Chia-Che Tsai",
            "Anurag Khandelwal",
            "Qifan Pu",
            "Vaishaal Shankar",
            "Joao Carreira",
            "Karl Krauth",
            "Neeraja Yadwadkar",
            "Joseph E. Gonzalez",
            "Raluca Ada Popa",
            "Ion Stoica",
            "David A. Patterson"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Serverless cloud computing handles virtually all the system administration\noperations needed to make it easier for programmers to use the cloud. It\nprovides an interface that greatly simplifies cloud programming, and represents\nan evolution that parallels the transition from assembly language to high-level\nprogramming languages. This paper gives a quick history of cloud computing,\nincluding an accounting of the predictions of the 2009 Berkeley View of Cloud\nComputing paper, explains the motivation for serverless computing, describes\napplications that stretch the current limits of serverless, and then lists\nobstacles and research opportunities required for serverless computing to\nfulfill its full potential. Just as the 2009 paper identified challenges for\nthe cloud and predicted they would be addressed and that cloud use would\naccelerate, we predict these issues are solvable and that serverless computing\nwill grow to dominate the future of cloud computing.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.03383v1"
    },
    {
        "title": "Avoiding Scalability Collapse by Restricting Concurrency",
        "authors": [
            "Dave Dice",
            "Alex Kogan"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Saturated locks often degrade the performance of a multithreaded application,\nleading to a so-called scalability collapse problem. This problem arises when a\ngrowing number of threads circulating through a saturated lock causes the\noverall application performance to fade or even drop abruptly. This problem is\nparticularly (but not solely) acute on oversubscribed systems (systems with\nmore threads than available hardware cores). In this paper, we introduce GCR\n(generic concurrency restriction), a mechanism that aims to avoid the\nscalability collapse. GCR, designed as a generic, lock-agnostic wrapper,\nintercepts lock acquisition calls, and decides when threads would be allowed to\nproceed with the acquisition of the underlying lock. Furthermore, we present\nGCR-NUMA, a non-uniform memory access (NUMA)-aware extension of GCR, that\nstrives to ensure that threads allowed to acquire the lock are those that run\non the same socket. The extensive evaluation that includes more than two dozen\nlocks, three machines and three benchmarks shows that GCR brings substantial\nspeedup (in many cases, up to three orders of magnitude) in case of contention\nand growing thread counts, while introducing nearly negligible slowdown when\nthe underlying lock is not contended. GCR-NUMA brings even larger performance\ngains starting at even lighter lock contention.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.10818v2"
    },
    {
        "title": "Exact Polynomial Time Algorithm for the Response Time Analysis of\n  Harmonic Tasks with Constrained Release Jitter",
        "authors": [
            "Thi Huyen Chau Nguyen",
            "Werner Grass",
            "Klaus Jansen"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  In some important application areas of hard real-time systems, preemptive\nsporadic tasks with harmonic periods and constraint deadlines running upon a\nuni-processor platform play an important role. We propose a new algorithm for\ndetermining the exact worst-case response time for a task that has a lower\ncomputational complexity (linear in the number of tasks) than the known\nalgorithm developed for the same system class. We also allow the task\nexecutions to start delayed due to release jitter if they are within certain\nvalue ranges. For checking if these constraints are met we define a constraint\nprogramming problem that has a special structure and can be solved with\nheuristic components in a time that is linear in the task number. If the check\ndetermines the admissibility of the jitter values, the linear time algorithm\ncan be used to determine the worst-case response time also for jitter-aware\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.01161v1"
    },
    {
        "title": "Faster than Flash: An In-Depth Study of System Challenges for Emerging\n  Ultra-Low Latency SSDs",
        "authors": [
            "Sungjoon Koh",
            "Junhyeok Jang",
            "Changrim Lee",
            "Miryeong Kwon",
            "Jie Zhang",
            "Myoungsoo Jung"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Emerging storage systems with new flash exhibit ultra-low latency (ULL) that\ncan address performance disparities between DRAM and conventional solid state\ndrives (SSDs) in the memory hierarchy. Considering the advanced low-latency\ncharacteristics, different types of I/O completion methods (polling/hybrid) and\nstorage stack architecture (SPDK) are proposed. While these new techniques are\nexpected to take costly software interventions off the critical path in\nULL-applied systems, unfortunately no study exists to quantitatively analyze\nsystem-level characteristics and challenges of combining such newly-introduced\ntechniques with real ULL SSDs. In this work, we comprehensively perform\nempirical evaluations with 800GB ULL SSD prototypes and characterize ULL\nbehaviors by considering a wide range of I/O path parameters, such as different\nqueues and access patterns. We then analyze the efficiencies and challenges of\nthe polled-mode and hybrid polling I/O completion methods (added into Linux\nkernels 4.4 and 4.10, respectively) and compare them with the efficiencies of a\nconventional interrupt-based I/O path. In addition, we revisit the common\nexpectations of SPDK by examining all the system resources and parameters.\nFinally, we demonstrate the challenges of ULL SSDs in a real SPDK-enabled\nserver-client system. Based on the performance behaviors that this study\nuncovers, we also discuss several system implications, which are required to\ntake a full advantage of ULL SSD in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06998v1"
    },
    {
        "title": "On Schedulability Analysis of EDF Scheduling by Considering Suspension\n  as Blocking",
        "authors": [
            "Mario Günzel",
            "Jian-Jia Chen"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  During the execution of a job, it may suspend itself, i.e., its computation\nceases to process until certain activities are complete to be resumed. This\npaper provides a counterexample of the schedulability analysis by Devi in\nEuromicro Conference on Real-Time Systems (ECRTS) in 2003, which is the only\nexisting suspension-aware analysis specialized for uniprocessor systems when\npreemptive earliest-deadline-first (EDF) is applied for scheduling dynamic\nselfsuspending tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.05747v2"
    },
    {
        "title": "Dim Silicon and the Case for Improved DVFS Policies",
        "authors": [
            "Mathias Gottschlag",
            "Yussuf Khalil",
            "Frank Bellosa"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Due to thermal and power supply limits, modern Intel CPUs reduce their\nfrequency when AVX2 and AVX-512 instructions are executed. As the CPUs wait for\n670{\\mu}s before increasing the frequency again, the performance of some\nheterogeneous workloads is reduced. In this paper, we describe parallels\nbetween this situation and dynamic power management as well as between the\npolicy implemented by these CPUs and fixed-timeout device shutdown policies. We\nshow that the policy implemented by Intel CPUs is not optimal and describe\npotential better policies. In particular, we present a mechanism to classify\napplications based on their likeliness to cause frequency reduction. Our\napproach takes either the resulting classification information or information\nprovided by the application and generates hints for the DVFS policy. We show\nthat faster frequency changes based on these hints are able to improve\nperformance for a web server using the OpenSSL library.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01498v1"
    },
    {
        "title": "On Failure Diagnosis of the Storage Stack",
        "authors": [
            "Duo Zhang",
            "Om Rameshwar Gatla",
            "Runzhou Han",
            "Mai Zheng"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Diagnosing storage system failures is challenging even for professionals. One\nexample is the \"When Solid State Drives Are Not That Solid\" incident occurred\nat Algolia data center, where Samsung SSDs were mistakenly blamed for failures\ncaused by a Linux kernel bug. With the system complexity keeps increasing, such\nobscure failures will likely occur more often. As one step to address the\nchallenge, we present our on-going efforts called X-Ray. Different from\ntraditional methods that focus on either the software or the hardware, X-Ray\nleverages virtualization to collects events across layers, and correlates them\nto generate a correlation tree. Moreover, by applying simple rules, X-Ray can\nhighlight critical nodes automatically. Preliminary results based on 5 failure\ncases shows that X-Ray can effectively narrow down the search space for\nfailures.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.02547v2"
    },
    {
        "title": "High Velocity Kernel File Systems with Bento",
        "authors": [
            "Samantha Miller",
            "Kaiyuan Zhang",
            "Mengqi Chen",
            "Ryan Jennings",
            "Ang Chen",
            "Danyang Zhuo",
            "Tom Anderson"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  High development velocity is critical for modern systems. This is especially\ntrue for Linux file systems which are seeing increased pressure from new\nstorage devices and new demands on storage systems. However, high velocity\nLinux kernel development is challenging due to the ease of introducing bugs,\nthe difficulty of testing and debugging, and the lack of support for\nredeployment without service disruption. Existing approaches to high-velocity\ndevelopment of file systems for Linux have major downsides, such as the high\nperformance penalty for FUSE file systems, slowing the deployment cycle for new\nfile system functionality.\n  We propose Bento, a framework for high velocity development of Linux kernel\nfile systems. It enables file systems written in safe Rust to be installed in\nthe Linux kernel, with errors largely sandboxed to the file system. Bento file\nsystems can be replaced with no disruption to running applications, allowing\ndaily or weekly upgrades in a cloud server setting. Bento also supports\nuserspace debugging. We implement a simple file system using Bento and show\nthat it performs similarly to VFS-native ext4 on a variety of benchmarks and\noutperforms a FUSE version by 7x on 'git clone'. We also show that we can\ndynamically add file provenance tracking to a running kernel file system with\nonly 15ms of service interruption.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09723v3"
    },
    {
        "title": "Light-weight Locks",
        "authors": [
            "Nitin Garg",
            "Ed Zhu",
            "Fabiano C. Botelho"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  In this paper, we propose a new approach to building synchronization\nprimitives, dubbed \"lwlocks\" (short for light-weight locks). The primitives are\noptimized for small memory footprint while maintaining efficient performance in\nlow contention scenarios. A read-write lwlock occupies 4 bytes, a mutex\noccupies 4 bytes (2 if deadlock detection is not required), and a condition\nvariable occupies 4 bytes. The corresponding primitives of the popular pthread\nlibrary occupy 56 bytes, 40 bytes and 48 bytes respectively on the x86-64\nplatform. The API for lwlocks is similar to that of the pthread library but\ncovering only the most common use cases. Lwlocks allow explicit control of\nqueuing and scheduling decisions in contention situations and support\n\"asynchronous\" or \"deferred blocking\" acquisition of locks. Asynchronous\nlocking helps in working around the constraints of lock-ordering which\notherwise limits concurrency. The small footprint of lwlocks enables the\nconstruction of data structures with very fine-grained locking, which in turn\nis crucial for lowering contention and supporting highly concurrent access to a\ndata structure. Currently, the Data Domain File System uses lwlocks for its\nin-memory inode cache as well as in a generic doubly-linked concurrent list\nwhich forms the building block for more sophisticated structures.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.2638v1"
    },
    {
        "title": "Design and Performance Evaluation of A New Proposed Fittest Job First\n  Dynamic Round Robin(FJFDRR) Scheduling Algorithm",
        "authors": [
            "Rakesh Mohanty",
            "Manas Das",
            "M. Lakshmi Prasanna",
            " Sudhashree"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  In this paper, we have proposed a new variant of Round Robin scheduling\nalgorithm by executing the processes according to the new calculated Fit Factor\nf and using the concept of dynamic time quantum. We have compared the\nperformance of our proposed Fittest Job First Dynamic Round Robin(FJFDRR)\nalgorithm with the Priority Based Static Round Robin(PBSRR) algorithm.\nExperimental results show that our proposed algorithm performs better than\nPBSRR in terms of reducing the number of context switches, average waiting time\nand average turnaround time.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.3075v1"
    },
    {
        "title": "Comparative performance analysis of multi dynamic time quantum Round\n  Robin(MDTQRR) algorithm with arrival time",
        "authors": [
            "H. S. Behera",
            "Rakesh Mohanty",
            "Sabyasachi Sahu",
            "Sourav Kumar Bhoi"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  CPU being considered a primary computer resource, its scheduling is central\nto operating-system design. A thorough performance evaluation of various\nscheduling algorithms manifests that Round Robin Algorithm is considered as\noptimal in time shared environment because the static time is equally shared\namong the processes. We have proposed an efficient technique in the process\nscheduling algorithm by using dynamic time quantum in Round Robin. Our approach\nis based on the calculation of time quantum twice in single round robin cycle.\nTaking into consideration the arrival time, we implement the algorithm.\nExperimental analysis shows better performance of this improved algorithm over\nthe Round Robin algorithm and the Shortest Remaining Burst Round Robin\nalgorithm. It minimizes the overall number of context switches, average waiting\ntime and average turn-around time. Consequently the throughput and CPU\nutilization is better.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.3076v1"
    },
    {
        "title": "Classification Of Heterogeneous Operating System",
        "authors": [
            "Kamlesh Sharma",
            "T. V. Prasad"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Operating system is a bridge between system and user. An operating system\n(OS) is a software program that manages the hardware and software resources of\na computer. The OS performs basic tasks, such as controlling and allocating\nmemory, prioritizing the processing of instructions, controlling input and\noutput devices, facilitating networking, and managing files. It is difficult to\npresent a complete as well as deep account of operating systems developed till\ndate. So, this paper tries to overview only a subset of the available operating\nsystems and its different categories. OS are being developed by a large number\nof academic and commercial organizations for the last several decades. This\npaper, therefore, concentrates on the different categories of OS with special\nemphasis to those that had deep impact on the evolution process. The aim of\nthis paper is to provide a brief timely commentary on the different categories\nimportant operating systems available today.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.4600v1"
    },
    {
        "title": "Characteristic specific prioritized dynamic average burst round robin\n  scheduling for uniprocessor and multiprocessor environment",
        "authors": [
            "Amar Ranjan Dash",
            "Sandipta Kumar Sahu",
            "Sanjay Kumar Samantra",
            "Sradhanjali Sabat"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  CPU scheduling is one of the most crucial operations performed by operating\nsystems. Different conventional algorithms like FCFS, SJF, Priority, and RR\n(Round Robin) are available for CPU Scheduling. The effectiveness of Priority\nand Round Robin scheduling algorithm completely depends on selection of\npriority features of processes and on the choice of time quantum. In this paper\na new CPU scheduling algorithm has been proposed, named as CSPDABRR\n(Characteristic specific Prioritized Dynamic Average Burst Round Robin), that\nuses seven priority features for calculating priority of processes and uses\ndynamic time quantum instead of static time quantum used in RR. The performance\nof the proposed algorithm is experimentally compared with traditional RR and\nPriority scheduling algorithm in both uni-processor and multi-processor\nenvironment. The results of our approach presented in this paper demonstrate\nimproved performance in terms of average waiting time, average turnaround time,\nand optimal priority feature.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.02498v1"
    },
    {
        "title": "PAStime: Progress-aware Scheduling for Time-critical Computing",
        "authors": [
            "Soham Sinha",
            "Richard West",
            "Ahmad Golchin"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Over-estimation of worst-case execution times (WCETs) of real-time tasks\nleads to poor resource utilization. In a mixed-criticality system (MCS), the\nover-provisioning of CPU time to accommodate the WCETs of highly critical tasks\nmay lead to degraded service for less critical tasks. In this paper, we present\nPAStime, a novel approach to monitor and adapt the runtime progress of highly\ntime-critical applications, to allow for improved service to lower criticality\ntasks. In PAStime, CPU time is allocated to time-critical tasks according to\nthe delays they experience as they progress through their control flow graphs.\nThis ensures that as much time as possible is made available to improve the\nQuality-of-Service of less critical tasks, while high-criticality tasks are\ncompensated after their delays.\n  In this paper, we integrate PAStime with Adaptive Mixed-criticality (AMC)\nscheduling. The LO-mode budget of a high-criticality task is adjusted according\nto the delay observed at execution checkpoints. This is the first\nimplementation of AMC in the scheduling framework Using LITMUS-RT, which is\nextended with our PAStime runtime policy and tested with real-time Linux\napplications such as object classification and detection. We observe in our\nexperimental evaluation that AMC-PAStime significantly improves the utilization\nof the low-criticality tasks while guaranteeing service to high-criticality\ntasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.06211v2"
    },
    {
        "title": "Boomerang: Real-Time I/O Meets Legacy Systems",
        "authors": [
            "Ahmad Golchin",
            "Soham Sinha",
            "Richard West"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  This paper presents Boomerang, an I/O system that integrates a legacy\nnon-real-time OS with one that is customized for timing-sensitive tasks. A\nrelatively small RTOS benefits from the pre-existing libraries, drivers and\nservices of the legacy system. Additionally, timing-critical tasks are isolated\nfrom less critical tasks by securely partitioning machine resources among the\nseparate OSes. Boomerang guarantees end-to-end processing delays on input data\nthat requires outputs to be generated within specific time bounds.\n  We show how to construct composable task pipelines in Boomerang that combine\nfunctionality spanning a custom RTOS and a legacy Linux system. By dedicating\ntime-critical I/O to the RTOS, we ensure that complementary services provided\nby Linux are sufficiently predictable to meet end-to-end service guarantees.\nWhile Boomerang benefits from spatial isolation, it also outperforms a\nstandalone Linux system using deadline-based CPU reservations for pipeline\ntasks. We also show how Boomerang outperforms a virtualized system called ACRN,\ndesigned for automotive systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.06807v2"
    },
    {
        "title": "A Least-Privilege Memory Protection Model for Modern Hardware",
        "authors": [
            "Reto Achermann",
            "Nora Hossle",
            "Lukas Humbel",
            "Daniel Schwyn",
            "David Cock",
            "Timothy Roscoe"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  We present a new least-privilege-based model of addressing on which to base\nmemory management functionality in an OS for modern computers like phones or\nserver-based accelerators. Existing software assumptions do not account for\nheterogeneous cores with different views of the address space, leading to the\nrelated problems of numerous security bugs in memory management code (for\nexample programming IOMMUs), and an inability of mainstream OSes to securely\nmanage the complete set of hardware resources on, say, a phone System-on-Chip.\n  Our new work is based on a recent formal model of address translation\nhardware which views the machine as a configurable network of address spaces.\nWe refine this to capture existing address translation hardware from modern\nSoCs and accelerators at a sufficiently fine granularity to model minimal\nrights both to access memory and configure translation hardware. We then build\nan executable specification in Haskell, which expresses the model and metadata\nstructures in terms of partitioned capabilities. Finally, we show a fully\nfunctional implementation of the model in C created by extending the capability\nsystem of the Barrelfish research OS.\n  Our evaluation shows that our unoptimized implementation has comparable (and\nin some cases) better performance than the Linux virtual memory system, despite\nboth capturing all the functionality of modern hardware addressing and enabling\nleast-privilege, decentralized authority to access physical memory and devices.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.08707v1"
    },
    {
        "title": "Kernel/User-level Collaborative Persistent Memory File System with\n  Efficiency and Protection",
        "authors": [
            "Youmin Chen",
            "Youyou Lu",
            "Bohong Zhu",
            "Jiwu Shu"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Emerging high performance non-volatile memories recall the importance of\nefficient file system design. To avoid the virtual file system (VFS) and\nsyscall overhead as in these kernel-based file systems, recent works deploy\nfile systems directly in user level. Unfortunately, a userlevel file system can\neasily be corrupted by a buggy program with misused pointers, and is hard to\nscale on multi-core platforms which incorporates a centralized coordination\nservice. In this paper, we propose KucoFS, a Kernel and user-level\ncollaborative file system. It consists of two parts: a user-level library with\ndirect-access interfaces, and a kernel thread, which performs metadata updates\nand enforces write protection by toggling the permission bits in the page\ntable. Hence, KucoFS achieves both direct-access of user-level designs and\nfine-grained write protection of kernel-level ones. We further explore its\nscalability to multicores: For metadata scalability, KucoFS rebalances the\npathname resolution overhead between the kernel and userspace, by adopting the\nindex offloading technique. For data access efficiency, it coordinates the data\nallocation between kernel and userspace, and uses range-lock write and\nlock-free read to improve concurrency. Experiments on Optane DC persistent\nmemory show that KucoFS significantly outperforms existing file systems and\nshows better scalability.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.10740v1"
    },
    {
        "title": "Porting of eChronos RTOS on RISC-V Architecture",
        "authors": [
            "Shubhendra Pal Singhal",
            "M. Sridevi",
            "N Sathya Narayanan",
            "M J Shankar Raman"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  eChronos is a formally verified Real Time Operating System(RTOS) designed for\nembedded micro-controllers. eChronos was targeted for tightly constrained\ndevices without memory management units. Currently, eChronos is available on\nproprietary designs like ARM, PowerPC and Intel architectures. eChronos is\nadopted in safety critical systems like aircraft control system and medical\nimplant devices. eChronos is one of the very few system software not been\nported to RISC-V. RISC-V is an open-source Instruction Set Architecture (ISA)\nthat enables new era of processor development. Many standard Operating Systems,\nsoftware tool chain have migrated to the RISC-V architecture. According to the\nlatest trends, RISC-V is replacing many proprietary chips. As a secure RTOS, it\nis attractive to port on an open-source ISA. SHAKTI and PicoRV32 are some of\nthe proven open-source RISC-V designs available. Now having a secure RTOS on an\nopen-source hardware design, designed based on an open-source ISA makes it more\ninteresting. In addition to this, the current architectures supported by\neChronos are all proprietary designs, and porting eChronos to the RISC-V\narchitecture increases the secure system development as a whole. This paper,\npresents an idea of porting eChronos on a chip which is open-source and\neffective, thus reducing the cost of embedded systems. Designing a open-source\nsystem that is completely open-source reduces the overall cost, increased the\nsecurity and can be critically reviewed. This paper explores the design and\narchitecture aspect involved in porting eChronos to RISC-V. The authors have\nsuccessfully ported eChronos to RISC-V architecture and verified it on spike.\nThe port of RISC-V to eChronos is made available open-source by authors. Along\nwith that, the safe removal of architectural dependencies and subsequent\nchanges in eChronos are also analyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.11648v3"
    },
    {
        "title": "SIVSHM: Secure Inter-VM Shared Memory",
        "authors": [
            "Shesha Sreenivasamurthy",
            "Ethan Miller"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  With wide spread acceptance of virtualization, virtual machines (VMs) find\ntheir presence in various applications such as Network Address Translation\n(NAT) servers, firewall servers and MapReduce applications. Typically, in these\napplications a data manager collects data from the external world and\ndistributes it to multiple workers for further processing. Currently, data\nmanagers distribute data with workers either using inter-VM shared memory\n(IVSHMEM) or network communication. IVSHMEM provides better data distribution\nthroughput sacrificing security as all untrusted workers have full access to\nthe shared memory region and network communication provides better security at\nthe cost of throughput. Secondly, IVSHMEM uses a central distributor to\nexchange eventfd - a file descriptor to an event queue of length one, which is\nused for inter-VM signaling. This central distributor becomes a bottleneck and\nincreases boot time of VMs. Secure Inter-VM Shared Memory (SIVSHM) provided\nboth security and better throughout by segmenting inter-VM shared memory, so\nthat each worker has access to segment that belong only to it, thereby enabling\nsecurity without sacrificing throughput. SIVSHM boots VMs in 30% less time\ncompared to IVSHMEM by eliminating central distributor from its architecture\nand enabling direct exchange of eventfds amongst VMs.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.10377v1"
    },
    {
        "title": "An Improvement Over Threads Communications on Multi-Core Processors",
        "authors": [
            "Reza Fotohi",
            "Mehdi Effatparvar",
            "Fateme Sarkohaki",
            "Shahram Behzad",
            "Jaber Hoseini balov"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Multicore is an integrated circuit chip that uses two or more computational\nengines (cores) places in a single processor. This new approach is used to\nsplit the computational work of a threaded application and spread it over\nmultiple execution cores, so that the computer system can benefits from a\nbetter performance and better responsiveness of the system. A thread is a unit\nof execution inside a process that is created and maintained to execute a set\nof actions/ instructions. Threads can be implemented differently from an\noperating system to another, but the operating system is in most cases\nresponsible to schedule the execution of different threads. Multi-threading\nimproving efficiency of processor performance with a cost-effective memory\nsystem. In this paper, we explore one approach to improve communications for\nmultithreaded. Pre-send is a software Controlled data forwarding technique that\nsends data to destination's cache before it is needed, eliminating cache misses\nin the destination's cache as well as reducing the coherence traffic on the\nbus. we show how we could improve the overall system performance by addition of\nthese architecture optimizations to multi-core processors.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.11644v2"
    },
    {
        "title": "Cichlid: Explicit physical memory management for large machines",
        "authors": [
            "Simon Gerber",
            "Gerd Zellweger",
            "Reto Achermann",
            "Moritz Hoffmann",
            "Kornilios Kourtis",
            "Timothy Roscoe",
            "Dejan Milojicic"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  In this paper, we rethink how an OS supports virtual memory. Classical VM is\nan opaque abstraction of RAM, backed by demand paging. However, most systems\ntoday (from phones to data-centers) do not page, and indeed may require the\nperformance benefits of non-paged physical memory, precise NUMA allocation,\netc. Moreover, MMU hardware is now useful for other purposes, such as detecting\npage access or providing large page translation. Accordingly, the venerable VM\nabstraction in OSes like Windows and Linux has acquired a plethora of extra\nAPIs to poke at the policy behind the illusion of a virtual address space.\n  Instead, we present Cichlid, a memory system which inverts this model.\nApplications explicitly manage their physical RAM of different types, and\ndirectly (though safely) program the translation hardware. Cichlid is\nimplemented in Barrelfish, requires no virtualization support, and outperforms\nVMM-based approaches for all but the smallest working sets. We show that\nCichlid enables use-cases for virtual memory not possible in Linux today, and\nother use-cases are simple to program and significantly faster.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.08367v1"
    },
    {
        "title": "CleanQ: a lightweight, uniform, formally specified interface for\n  intra-machine data transfer",
        "authors": [
            "Roni Haecki",
            "Lukas Humbel",
            "Reto Achermann",
            "David Cock",
            "Daniel Schwyn",
            "Timothy Roscoe"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  We present CleanQ, a high-performance operating-system interface for\ndescriptor-based data transfer with rigorous formal semantics, based on a\nsimple, formally-verified notion of ownership transfer, with a fast reference\nimplementation. CleanQ aims to replace the current proliferation of similar,\nbut subtly diverse, and loosely specified, descriptor-based interfaces in OS\nkernels and device drivers. CleanQ has strict semantics that not only clarify\nboth the implementation of the interface for different hardware devices and\nsoftware usecases, but also enable composition of modules as in more\nheavyweight frameworks like Unix streams. We motivate CleanQ by showing that\nloose specifications derived from implementation lead to security and\ncorrectness bugs in production systems that a clean, formal, and\neasilyunderstandable abstraction helps eliminate. We further demonstrate by\nexperiment that there is negligible performance cost for a clean design: we\nshow overheads in the tens of cycles for operations, and comparable end-to-end\nperformance to the highly-tuned Virtio and DPDK implementations on Linux.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.08773v1"
    },
    {
        "title": "Memory virtualization in virtualized systems: segmentation is better\n  than paging",
        "authors": [
            "Boris Teabe",
            "Peterson Yuhala",
            "Alain Tchana",
            "Fabien Hermenier",
            "Daniel Hagimont",
            "Gilles Muller"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  The utilization of paging for virtual machine (VM) memory management is the\nroot cause of memory virtualization overhead. This paper shows that paging is\nnot necessary in the hypervisor. In fact, memory fragmentation, which explains\npaging utilization, is not an issue in virtualized datacenters thanks to VM\nmemory demand patterns. Our solution Compromis, a novel Memory Management Unit,\nuses direct segment for VM memory management combined with paging for VM's\nprocesses. The paper presents a systematic methodology for implementing\nCompromis in the hardware, the hypervisor and the datacenter scheduler.\nEvaluation results show that Compromis outperforms the two popular memory\nvirtualization solutions: shadow paging and Extended Page Table by up to 30%\nand 370% respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.00380v1"
    },
    {
        "title": "FastDrain: Removing Page Victimization Overheads in NVMe Storage Stack",
        "authors": [
            "Jie Zhang",
            "Miryeong Kwon",
            "Sanghyun Han",
            "Nam Sung Kim",
            "Mahmut Kandemir",
            "Myoungsoo Jung"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Host-side page victimizations can easily overflow the SSD internal buffer,\nwhich interferes I/O services of diverse user applications thereby degrading\nuser-level experiences. To address this, we propose FastDrain, a co-design of\nOS kernel and flash firmware to avoid the buffer overflow, caused by page\nvictimizations. Specifically, FastDrain can detect a triggering point where a\nnear-future page victimization introduces an overflow of the SSD internal\nbuffer. Our new flash firmware then speculatively scrubs the buffer space to\naccommodate the requests caused by the page victimization. In parallel, our new\nOS kernel design controls the traffic of page victimizations by considering the\ntarget device buffer status, which can further reduce the risk of buffer\noverflow. To secure more buffer spaces, we also design a latency-aware FTL,\nwhich dumps the dirty data only to the fast flash pages. Our evaluation results\nreveal that FastDrain reduces the 99th response time of user applications by\n84%, compared to a conventional system.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.08966v2"
    },
    {
        "title": "LeaFTL: A Learning-Based Flash Translation Layer for Solid-State Drives",
        "authors": [
            "Jinghan Sun",
            "Shaobo Li",
            "Yunxin Sun",
            "Chao Sun",
            "Dejan Vucinic",
            "Jian Huang"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  In modern solid-state drives (SSDs), the indexing of flash pages is a\ncritical component in their storage controllers. It not only affects the data\naccess performance, but also determines the efficiency of the precious\nin-device DRAM resource. A variety of address mapping schemes and optimization\ntechniques have been proposed. However, most of them were developed with\nhuman-driven heuristics. They cannot automatically capture diverse data access\npatterns at runtime in SSD controllers, which leaves a large room for\nimprovement. In this paper, we present a learning-based flash translation layer\n(FTL), named LeaFTL, which learns the address mapping to tolerate dynamic data\naccess patterns via linear regression at runtime. By grouping a large set of\nmapping entries into a learned segment, it significantly reduces the memory\nfootprint of the address mapping table, which further benefits the data caching\nin SSD controllers. LeaFTL also employs various optimization techniques,\nincluding out-of-band metadata verification to tolerate mispredictions,\noptimized flash allocation, and dynamic compaction of learned index segments.\nWe implement LeaFTL with an SSD simulator and evaluate it with various storage\nworkloads. LeaFTL saves the memory consumption of the mapping table by 2.9x on\naverage and improves the storage performance by 1.4x on average, in comparison\nwith state-of-the-art FTL schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.00072v1"
    },
    {
        "title": "From MMU to MPU: adaptation of the Pip kernel to constrained devices",
        "authors": [
            "Nicolas Dejon",
            "Chrystel Gaber",
            "Gilles Grimaud"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  This article presents a hardware-based memory isolation solution for\nconstrained devices. Existing solutions target high-end embedded systems\n(typically ARM Cortex-A with a Memory Management Unit, MMU) such as seL4 or Pip\n(formally verified kernels) or target low-end devices such as ACES, MINION,\nTrustLite, EwoK but with limited flexibility by proposing a single level of\nisolation. Our approach consists in adapting Pip to inherit its flexibility\n(multiple levels of isolation) but using the Memory Protection Unit (MPU)\ninstead of the MMU since the MPU is commonly available on constrained embedded\nsystems (typically ARMv7 Cortex-M4 or ARMv8 Cortex-M33 and similar devices).\nThis paper describes our design of Pip-MPU (Pip's variant based on the MPU) and\nthe rationale behind our choices. We validate our proposal with an\nimplementation on an nRF52840 development kit and we perform various\nevaluations such as memory footprint, CPU cycles and energy consumption. We\ndemonstrate that although our prototyped Pip-MPU causes a 16% overhead on both\nperformance and energy consumption, it can reduce the attack surface of the\naccessible application memory from 100% down to 2% and the privileged\noperations by 99%. Pip-MPU takes less than 10 kB of Flash (6 kB for its core\ncomponents) and 550 B of RAM.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04546v1"
    },
    {
        "title": "Heterogeneity-aware Fault Tolerance using a Self-Organizing Runtime\n  System",
        "authors": [
            "Mario Kicherer",
            "Wolfgang Karl"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  Due to the diversity and implicit redundancy in terms of processing units and\ncompute kernels, off-the-shelf heterogeneous systems offer the opportunity to\ndetect and tolerate faults during task execution in hardware as well as in\nsoftware. To automatically leverage this diversity, we introduce an extension\nof an online-learning runtime system that combines the benefits of the existing\nperformance-oriented task mapping with task duplication, a diversity-oriented\nmapping strategy and heterogeneity-aware majority voter. This extension uses a\nnew metric to dynamically rate the remaining benefit of unreliable processing\nunits and a memory management mechanism for automatic data transfers and\ncheckpointing in the host and device memories.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.2912v1"
    },
    {
        "title": "Supporting Soft Real-Time Sporadic Task Systems on Heterogeneous\n  Multiprocessors with No Utilization Loss",
        "authors": [
            "Guangmo Tong",
            "Cong Liu"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  Heterogeneous multicore architectures are becoming increasingly popular due\nto their potential of achieving high performance and energy efficiency compared\nto the homogeneous multicore architectures. In such systems, the real-time\nscheduling problem becomes more challenging in that processors have different\nspeeds. A job executing on a processor with speed $x$ for $t$ time units\ncompletes $(x \\cdot t)$ units of execution. Prior research on heterogeneous\nmultiprocessor real-time scheduling has focused on hard real-time systems,\nwhere, significant processing capacity may have to be sacrificed in the\nworst-case to ensure that all deadlines are met. As meeting hard deadlines is\noverkill for many soft real-time systems in practice, this paper shows that on\nsoft real-time heterogeneous multiprocessors, bounded response times can be\nensured for globally-scheduled sporadic task systems with no utilization loss.\nA GEDF-based scheduling algorithm, namely GEDF-H, is presented and response\ntime bounds are established under both preemptive and non-preemptive GEDF-H\nscheduling. Extensive experiments show that the magnitude of the derived\nresponse time bound is reasonable, often smaller than three task periods. To\nthe best of our knowledge, this paper is the first to show that soft real-time\nsporadic task systems can be supported on heterogeneous multiprocessors without\nutilization loss, and with reasonable predicted response time.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.7322v1"
    },
    {
        "title": "k2U: A General Framework from k-Point Effective Schedulability Analysis\n  to Utilization-Based Tests",
        "authors": [
            "Jian-Jia Chen",
            "Wen-Hung Huang",
            "Cong Liu"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  To deal with a large variety of workloads in different application domains in\nreal-time embedded systems, a number of expressive task models have been\ndeveloped. For each individual task model, researchers tend to develop\ndifferent types of techniques for deriving schedulability tests with different\ncomputation complexity and performance. In this paper, we present a general\nschedulability analysis framework, namely the k2U framework, that can be\npotentially applied to analyze a large set of real-time task models under any\nfixed-priority scheduling algorithm, on both uniprocessor and multiprocessor\nscheduling. The key to k2U is a k-point effective schedulability test, which\ncan be viewed as a \"blackbox\" interface. For any task model, if a corresponding\nk-point effective schedulability test can be constructed, then a sufficient\nutilization-based test can be automatically derived. We show the generality of\nk2U by applying it to different task models, which results in new and improved\ntests compared to the state-of-the-art.\n  Analogously, a similar concept by testing only k points with a different\nformulation has been studied by us in another framework, called k2Q, which\nprovides quadratic bounds or utilization bounds based on a different\nformulation of schedulability test. With the quadratic and hyperbolic forms,\nk2Q and k2U frameworks can be used to provide many quantitive features to be\nmeasured, like the total utilization bounds, speed-up factors, etc., not only\nfor uniprocessor scheduling but also for multiprocessor scheduling. These\nframeworks can be viewed as a \"blackbox\" interface for schedulability tests and\nresponse-time analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.07084v3"
    },
    {
        "title": "An Implementation and Analysis of a Kernel Network Stack in Go with the\n  CSP Style",
        "authors": [
            "Harshal Sheth",
            "Aashish Welling"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  Modern operating system kernels are written in lower-level languages such as\nC. Although the low-level functionalities of C are often useful within kernels,\nthey also give rise to several classes of bugs. Kernels written in higher level\nlanguages avoid many of these potential problems, at the possible cost of\ndecreased performance. This research evaluates the advantages and disadvantages\nof a kernel written in a higher level language. To do this, the network stack\nsubsystem of the kernel was implemented in Go with the Communicating Sequential\nProcesses (CSP) style. Go is a high-level programming language that supports\nthe CSP style, which recommends splitting large tasks into several smaller ones\nrunning in independent \"threads\". Modules for the major networking protocols,\nincluding Ethernet, ARP, IPv4, ICMP, UDP, and TCP, were implemented. In this\nstudy, the implemented Go network stack, called GoNet, was compared to a\nrepresentative network stack written in C. The GoNet code is more readable and\ngenerally performs better than that of its C stack counterparts. From this, it\ncan be concluded that Go with CSP style is a viable alternative to C for the\nlanguage of kernel implementations.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.05636v1"
    },
    {
        "title": "An optimized round robin cpu scheduling algorithm with dynamic time\n  quantum",
        "authors": [
            "Amar Ranjan Dash",
            "Sandipta kumar Sahu",
            "Sanjay Kumar Samantra"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  CPU scheduling is one of the most crucial operations performed by operating\nsystem. Different algorithms are available for CPU scheduling amongst them RR\n(Round Robin) is considered as optimal in time shared environment. The\neffectiveness of Round Robin completely depends on the choice of time quantum.\nIn this paper a new CPU scheduling algorithm has been proposed, named as DABRR\n(Dynamic Average Burst Round Robin). That uses dynamic time quantum instead of\nstatic time quantum used in RR. The performance of the proposed algorithm is\nexperimentally compared with traditional RR and some existing variants of RR.\nThe results of our approach presented in this paper demonstrate improved\nperformance in terms of average waiting time, average turnaround time, and\ncontext switching.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.00362v1"
    },
    {
        "title": "A Qualitative Comparison of MPSoC Mobile and Embedded Virtualization\n  Techniques",
        "authors": [
            "Junaid Shuja",
            "Abdullah Gani",
            "Sajjad A. Madani"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  Virtualization is generally adopted in server and desktop environments to\nprovide for fault tolerance, resource management, and energy efficiency.\nVirtualization enables parallel execution of multiple operating systems (OSs)\nwhile sharing the hardware resources. Virtualization was previously not deemed\nas feasible technology for mobile and embedded devices due to their limited\nprocessing and memory resource. However, the enterprises are advocating Bring\nYour Own Device (BYOD) applications that enable co-existence of heterogeneous\nOSs on a single mobile device. Moreover, embedded device require virtualization\nfor logical isolation of secure and general purpose OSs on a single device. In\nthis paper, we investigate the processor architectures in the mobile and\nembedded space while examining their formal visualizability. We also compare\nthe virtualization solutions enabling coexistence of multiple OSs in Multicore\nProcessor System-on-Chip (MPSoC) mobile and embedded systems. We advocate that\nvirtualization is necessary to manage resource in MPSoC designs and to enable\nBYOD, security, and logical isolation use cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.01168v1"
    },
    {
        "title": "The Design of the NetBSD I/O Subsystems",
        "authors": [
            "SungWon Chung"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  This book describes the source code of the NetBSD Operating System Release\n1.6 in SUN UltraSPARC 64-bit platform by annotating related excerpts from\nreferences and user manuals on the NetBSD Operating System. The goal of this\nbook is to provide necessary information to understand the operation and the\nimplementation of I/O subsystems in the kernel as well as to design and\nimplement a new filesystem on the NetBSD platform.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05810v1"
    },
    {
        "title": "New Analysis Techniques for Supporting Hard Real-Time Sporadic DAG Task\n  Systems on Multiprocessors",
        "authors": [
            "Zheng Dong",
            "Cong Liu"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  The scheduling and schedulability analysis of real-time directed acyclic\ngraph (DAG) task systems have received much recent attention. The DAG model can\naccurately represent intra-task parallelim and precedence constraints existing\nin many application domains. Existing techniques show that analyzing the DAG\nmodel is fundamentally more challenging compared to the ordinary sporadic task\nmodel, due to the complex intra-DAG precedence constraints which may cause\nrather pessimistic schedulability loss. However,such increased loss is\ncounter-intuitive because the DAG structure shall better exploit the\nparallelism provided by the multiprocessor platform. Our observation is that\nthe intra-DAG precedence constraints, if not carefully considered by the\nscheduling algorithm, may cause very unpredictable execution behaviors of\nsubtasks in a DAG and further cause pessimistic analysis. In this paper, we\npresent a set of novel scheduling and analysis techniques for better supporting\nhard real-time sporadic DAG tasks on multiprocessors, through smartly defining\nand analyzing the execution order of subtasks in each DAG. Evaluation\ndemonstrates that our developed utilization-based schedulability test is highly\nefficient, which dramatically improves schedulability of existing\nutilization-based tests by over 60% on average. Interestingly, when each DAG in\nthe system is an ordinary sporadic task, our test becomes identical to the\nclassical density test designed for the sporadic task model.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.00017v1"
    },
    {
        "title": "Real-time Linux communications: an evaluation of the Linux communication\n  stack for real-time robotic applications",
        "authors": [
            "Carlos San Vicente Gutiérrez",
            "Lander Usategui San Juan",
            "Irati Zamalloa Ugarte",
            "Víctor Mayoral Vilches"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  As robotics systems become more distributed, the communications between\ndifferent robot modules play a key role for the reliability of the overall\nrobot control. In this paper, we present a study of the Linux communication\nstack meant for real-time robotic applications. We evaluate the real-time\nperformance of UDP based communications in Linux on multi-core embedded devices\nas test platforms. We prove that, under an appropriate configuration, the Linux\nkernel greatly enhances the determinism of communications using the UDP\nprotocol. Furthermore, we demonstrate that concurrent traffic disrupts the\nbounded latencies and propose a solution by separating the real-time\napplication and the corresponding interrupt in a CPU.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.10821v1"
    },
    {
        "title": "Dependency Graph Approach for Multiprocessor Real-Time Synchronization",
        "authors": [
            "Jian-Jia Chen",
            "Georg von der Brüggen",
            "Junjie Shi",
            "Niklas Uete"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Over the years, many multiprocessor locking protocols have been designed and\nanalyzed. However, the performance of these protocols highly depends on how the\ntasks are partitioned and prioritized and how the resources are shared locally\nand globally. This paper answers a few fundamental questions when real-time\ntasks share resources in multiprocessor systems. We explore the fundamental\ndifficulty of the multiprocessor synchronization problem and show that a very\nsimplified version of this problem is ${\\mathcal NP}$-hard in the strong sense\nregardless of the number of processors and the underlying scheduling paradigm.\nTherefore, the allowance of preemption or migration does not reduce the\ncomputational complexity. For the positive side, we develop a dependency-graph\napproach, that is specifically useful for frame-based real-time tasks, in which\nall tasks have the same period and release their jobs always at the same time.\nWe present a series of algorithms with speedup factors between $2$ and $3$\nunder semi-partitioned scheduling. We further explore methodologies and\ntradeoffs of preemptive against non-preemptive scheduling algorithms and\npartitioned against semi-partitioned scheduling algorithms. The approach is\nextended to periodic tasks under certain conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.02892v1"
    },
    {
        "title": "DurableFS: A File System for Persistent Memory",
        "authors": [
            "Chandan Kalita",
            "Gautam Barua",
            "Priya Sehgal"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  With the availability of hybrid DRAM-NVRAM memory on the memory bus of CPUs,\na number of file systems on NVRAM have been designed and implemented. In this\npaper we present the design and implementation of a file system on NVRAM called\nDurableFS, which provides atomicity and durability of file operations to\napplications. Due to the byte level random accessibility of memory, it is\npossible to provide these guarantees without much overhead. We use standard\ntechniques like copy on write for data, and a redo log for metadata changes to\nbuild an efficient file system which provides durability and atomicity\nguarantees at the time a file is closed. Benchmarks on the implementation shows\nthat there is only a 7 %degradation in performance due to providing these\nguarantees.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.00757v1"
    },
    {
        "title": "Transkernel: Bridging Monolithic Kernels to Peripheral Cores",
        "authors": [
            "Liwei Guo",
            "Shuang Zhai",
            "Yi Qiao",
            "Felix Xiaozhu Lin"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Smart devices see a large number of ephemeral tasks driven by background\nactivities. In order to execute such a task, the OS kernel wakes up the\nplatform beforehand and puts it back to sleep afterwards. In doing so, the\nkernel operates various IO devices and orchestrates their power state\ntransitions. Such kernel executions are inefficient as they mismatch typical\nCPU hardware. They are better off running on a low-power, microcontroller-like\ncore, i.e., peripheral core, relieving CPU from the inefficiency.\n  We therefore present a new OS structure, in which a lightweight virtual\nexecutor called transkernel offloads specific phases from a monolithic kernel.\nThe transkernel translates stateful kernel execution through cross-ISA, dynamic\nbinary translation (DBT); it emulates a small set of stateless kernel services\nbehind a narrow, stable binary interface; it specializes for hot paths; it\nexploits ISA similarities for lowering DBT cost.\n  Through an ARM-based prototype, we demonstrate transkernel's feasibility and\nbenefit. We show that while cross-ISA DBT is typically used under the\nassumption of efficiency loss, it can enable efficiency gain, even on\noff-the-shelf hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.05000v3"
    },
    {
        "title": "MiniOS: an instructional platform for teaching operating systems labs",
        "authors": [
            "Rafael Roman Otero",
            "Alex Aravind"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Delivering hands-on practice laboratories for introductory courses on\noperating systems is a difficult task. One of the main sources of the\ndifficulty is the sheer size and complexity of the operating systems software.\nConsequently, some of the solutions adopted in the literature to teach\noperating systems laboratory consider smaller and simpler systems, generally\nreferred to as instructional operating systems. This work continues in the same\ndirection and is threefold. First, it considers a simpler hardware platform.\nSecond, it argues that a minimal operating system is a viable option for\ndelivering laboratories. Third, it presents a laboratory teaching platform,\nwhereby students build a minimal operating system for an embedded hardware\nplatform. The proposed platform is called MiniOS. An important aspect of MiniOS\nis that it is sufficiently supported with additional technical and pedagogic\nmaterial. Finally, the effectiveness of the proposed approach to teach\noperating systems laboratories is illustrated through the experience of using\nit to deliver laboratory projects in the Operating Systems course at the\nUniversity of Northern British Columbia. Finally, we discuss experimental\nresearch in computing education and considered the qualitative results of this\nwork as part of a larger research endeavour.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.09792v2"
    },
    {
        "title": "XOS: An Application-Defined Operating System for Data Center Servers",
        "authors": [
            "Chen Zheng",
            "Lei Wang",
            "Sally A. McKee",
            "Lixin Zhang",
            "Hainan Ye",
            "Jianfeng Zhan"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Rapid growth of datacenter (DC) scale, urgency of cost control, increasing\nworkload diversity, and huge software investment protection place unprecedented\ndemands on the operating system (OS) efficiency, scalability, performance\nisolation, and backward-compatibility. The traditional OSes are not built to\nwork with deep-hierarchy software stacks, large numbers of cores, tail latency\nguarantee, and increasingly rich variety of applications seen in modern DCs,\nand thus they struggle to meet the demands of such workloads.\n  This paper presents XOS, an application-defined OS for modern DC servers. Our\ndesign moves resource management out of the OS kernel, supports customizable\nkernel subsystems in user space, and enables elastic partitioning of hardware\nresources. Specifically, XOS leverages modern hardware support for\nvirtualization to move resource management functionality out of the\nconventional kernel and into user space, which lets applications achieve near\nbare-metal performance. We implement XOS on top of Linux to provide backward\ncompatibility. XOS speeds up a set of DC workloads by up to 1.6X over our\nbaseline Linux on a 24-core server, and outperforms the state-of-the-art Dune\nby up to 3.3X in terms of virtual memory management. In addition, XOS\ndemonstrates good scalability and strong performance isolation.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.00825v1"
    },
    {
        "title": "Efficient, Dynamic Multi-tenant Edge Computation in EdgeOS",
        "authors": [
            "Yuxin Ren",
            "Vlad Nitu",
            "Guyue Liu",
            "Gabriel Parmer",
            "Timothy Wood",
            "Alain Tchana",
            "Riley Kennedy"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  In the future, computing will be immersed in the world around us -- from\naugmented reality to autonomous vehicles to the Internet of Things. Many of\nthese smart devices will offer services that respond in real time to their\nphysical surroundings, requiring complex processing with strict performance\nguarantees. Edge clouds promise a pervasive computational infrastructure a\nshort network hop away from end devices, but today's operating systems are a\npoor fit to meet the goals of scalable isolation, dense multi-tenancy, and\npredictable performance required by these emerging applications. In this paper\nwe present EdgeOS, a micro-kernel based operating system that meets these goals\nby blending recent advances in real-time systems and network function\nvirtualization. EdgeOS introduces a Featherweight Process model that offers\nlightweight isolation and supports extreme scalability even under high churn.\nOur architecture provides efficient communication mechanisms, and low-overhead\nper-client isolation. To achieve high performance networking, EdgeOS employs\nkernel bypass paired with the isolation properties of Featherweight Processes.\nWe have evaluated our EdgeOS prototype for running high scale network\nmiddleboxes using the Click software router and endpoint applications using\nmemcached. EdgeOS reduces startup latency by 170X compared to Linux processes\nand over five orders of magnitude compared to containers, while providing three\norders of magnitude latency improvement when running 300 to 1000 edge-cloud\nmemcached instances on one server.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.01222v1"
    },
    {
        "title": "File System in Data-Centric Computing",
        "authors": [
            "Viacheslav Dubeyko"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  The moving computation on the edge or near to data is the new trend that can\nbreak the bandwidth wall and to unleash the power of next generation NVM or SCM\nmemory. File system is the important OS subsystem that plays the role of\nmediator between the user-space application and storage device. The key goal of\nthe file system is to represent the file abstraction and to build the files'\nnamespace. In the current paradigm the file system needs to copy the metadata\nand user data in the DRAM of the host with the goal to access and to modify the\nuser data on the host side. The DAX approach doesn't change the concept but to\nbuild the way to bypass the page cache via the direct access to file's content\nin persistent memory. Generally speaking, for the case of data-centric\ncomputing, the file system needs to solve the opposite task not to copy data\ninto page cache but to deliver the processing activity near data on the storage\ndevice side.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.01340v1"
    },
    {
        "title": "A C-DAG task model for scheduling complex real-time tasks on\n  heterogeneous platforms: preemption matters",
        "authors": [
            "Houssam-Eddine Zahaf",
            "Nicola Capodieci",
            "Roberto Cavicchioli",
            "Marko Bertogna",
            "Giuseppe Lipari"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Recent commercial hardware platforms for embedded real-time systems feature\nheterogeneous processing units and computing accelerators on the same\nSystem-on-Chip. When designing complex real-time application for such\narchitectures, the designer needs to make a number of difficult choices: on\nwhich processor should a certain task be implemented? Should a component be\nimplemented in parallel or sequentially? These choices may have a great impact\non feasibility, as the difference in the processor internal architectures\nimpact on the tasks' execution time and preemption cost. To help the designer\nexplore the wide space of design choices and tune the scheduling parameters, in\nthis paper we propose a novel real-time application model, called C-DAG,\nspecifically conceived for heterogeneous platforms. A C-DAG allows to specify\nalternative implementations of the same component of an application for\ndifferent processing engines to be selected off-line, as well as conditional\nbranches to model if-then-else statements to be selected at run-time. We also\npropose a schedulability analysis for the C-DAG model and a heuristic\nallocation algorithm so that all deadlines are respected. Our analysis takes\ninto account the cost of preempting a task, which can be non-negligible on\ncertain processors. We demonstrate the effectiveness of our approach on a large\nset of synthetic experiments by comparing with state of the art algorithms in\nthe literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.02450v1"
    },
    {
        "title": "Multiverse: Easy Conversion of Runtime Systems into OS Kernels via\n  Automatic Hybridization",
        "authors": [
            "Kyle C. Hale",
            "Conor Hetland",
            "Peter Dinda"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  The hybrid runtime (HRT) model offers a path towards high performance and\nefficiency. By integrating the OS kernel, runtime, and application, an HRT\nallows the runtime developer to leverage the full feature set of the hardware\nand specialize OS services to the runtime's needs. However, conforming to the\nHRT model currently requires a port of the runtime to the kernel level, for\nexample to the Nautilus kernel framework, and this requires knowledge of kernel\ninternals. In response, we developed Multiverse, a system that bridges the gap\nbetween a built-from-scratch HRT and a legacy runtime system. Multiverse allows\nunmodified applications and runtimes to be brought into the HRT model without\nany porting effort whatsoever by splitting the execution of the application\nbetween the domains of a legacy OS and an HRT environment. We describe the\ndesign and implementation of Multiverse and illustrate its capabilities using\nthe massive, widely-used Racket runtime system.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.06360v1"
    },
    {
        "title": "Can We Prove Time Protection?",
        "authors": [
            "Gernot Heiser",
            "Gerwin Klein",
            "Toby Murray"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Timing channels are a significant and growing security threat in computer\nsystems, with no established solution. We have recently argued that the OS must\nprovide time protection, in analogy to the established memory protection, to\nprotect applications from information leakage through timing channels. Based on\na recently-proposed implementation of time protection in the seL4 microkernel,\nwe investigate how such an implementation could be formally proved to prevent\ntiming channels. We postulate that this should be possible by reasoning about a\nhighly abstracted representation of the shared hardware resources that cause\ntiming channels.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.08338v1"
    },
    {
        "title": "Reproducible Execution of POSIX Programs with DiOS",
        "authors": [
            "Petr Ročkai",
            "Zuzana Baranová",
            "Jan Mrázek",
            "Katarína Kejstová",
            "Jiří Barnat"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  In this paper, we describe DiOS, a lightweight model operating system which\ncan be used to execute programs that make use of POSIX APIs. Such executions\nare fully reproducible: running the same program with the same inputs twice\nwill result in two exactly identical instruction traces, even if the program\nuses threads for parallelism.\n  DiOS is implemented almost entirely in portable C and C++: although its\nprimary platform is DiVM, a verification-oriented virtual machine, it can be\nconfigured to also run in KLEE, a symbolic executor. Finally, it can be\ncompiled into machine code to serve as a user-mode kernel.\n  Additionally, DiOS is modular and extensible. Its various components can be\ncombined to match both the capabilities of the underlying platform and to\nprovide services required by a particular program. New components can be added\nto cover additional system calls or APIs.\n  The experimental evaluation has two parts. DiOS is first evaluated as a\ncomponent of a program verification platform based on DiVM. In the second part,\nwe consider its portability and modularity by combining it with the symbolic\nexecutor KLEE.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.03356v1"
    },
    {
        "title": "SSDFS: Towards LFS Flash-Friendly File System without GC operation",
        "authors": [
            "Viacheslav Dubeyko"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Solid state drives have a number of interesting characteristics. However,\nthere are numerous file system and storage design issues for SSDs that impact\nthe performance and device endurance. Many flash-oriented and flash-friendly\nfile systems introduce significant write amplification issue and GC overhead\nthat results in shorter SSD lifetime and necessity to use the NAND flash\noverprovisioning. SSDFS file system introduces several authentic concepts and\nmechanisms: logical segment, logical extent, segment's PEBs pool,\nMain/Diff/Journal areas in the PEB's log, Diff-On-Write approach, PEBs\nmigration scheme, hot/warm data self-migration, segment bitmap, hybrid b-tree,\nshared dictionary b-tree, shared extents b-tree. Combination of all suggested\nconcepts are able: (1) manage write amplification in smart way, (2) decrease GC\noverhead, (3) prolong SSD lifetime, and (4) provide predictable file system's\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11825v1"
    },
    {
        "title": "SEUSS: Rapid serverless deployment using environment snapshots",
        "authors": [
            "James Cadden",
            "Thomas Unger",
            "Yara Awad",
            "Han Dong",
            "Orran Krieger",
            "Jonathan Appavoo"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Modern FaaS systems perform well in the case of repeat executions when\nfunction working sets stay small. However, these platforms are less effective\nwhen applied to more complex, large-scale and dynamic workloads. In this paper,\nwe introduce SEUSS (serverless execution via unikernel snapshot stacks), a new\nsystem-level approach for rapidly deploying serverless functions. Through our\napproach, we demonstrate orders of magnitude improvements in function start\ntimes and cacheability, which improves common re-execution paths while also\nunlocking previously-unsupported large-scale bursty workloads.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.01558v1"
    },
    {
        "title": "APEX: Adaptive Ext4 File System for Enhanced Data Recoverability in Edge\n  Devices",
        "authors": [
            "Shreshth Tuli",
            "Shikhar Tuli",
            "Udit Jain",
            "Rajkumar Buyya"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Recently Edge Computing paradigm has gained significant popularity both in\nindustry and academia. With its increased usage in real-life scenarios,\nsecurity, privacy and integrity of data in such environments have become\ncritical. Malicious deletion of mission-critical data due to ransomware,\ntrojans and viruses has been a huge menace and recovering such lost data is an\nactive field of research. As most of Edge computing devices have compute and\nstorage limitations, difficult constraints arise in providing an optimal scheme\nfor data protection. These devices mostly use Linux/Unix based operating\nsystems. Hence, this work focuses on extending the Ext4 file system to APEX\n(Adaptive Ext4): a file system based on novel on-the-fly learning model that\nprovides an Adaptive Recover-ability Aware file allocation platform for\nefficient post-deletion data recovery and therefore maintaining data integrity.\nOur recovery model and its lightweight implementation allow significant\nimprovement in recover-ability of lost data with lower compute, space, time,\nand cost overheads compared to other methods. We demonstrate the effectiveness\nof APEX through a case study of overwriting surveillance videos by CryPy\nmalware on Raspberry-Pi based Edge deployment and show 678% and 32% higher\nrecovery than Ext4 and current state-of-the-art File Systems. We also evaluate\nthe overhead characteristics and experimentally show that they are lower than\nother related works.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.01642v1"
    },
    {
        "title": "Enabling Failure-resilient Intermittent Systems Without Runtime\n  Checkpointing",
        "authors": [
            "Wei-Ming Chen",
            " Tei-Wei-Kuo",
            "Pi-Cheng Hsiu"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Self-powered intermittent systems typically adopt runtime checkpointing as a\nmeans to accumulate computation progress across power cycles and recover system\nstatus from power failures. However, existing approaches based on the\ncheckpointing paradigm normally require system suspension and/or logging at\nruntime. This paper presents a design which overcomes the drawbacks of\ncheckpointing-based approaches, to enable failure-resilient intermittent\nsystems. Our design allows accumulative execution and instant system recovery\nunder frequent power failures while enforcing the serializability of concurrent\ntask execution to improve computation progress and ensuring data consistency\nwithout system suspension during runtime, by leveraging the characteristics of\ndata accessed in hybrid memory. We integrated the design into FreeRTOS running\non a Texas Instruments device. Experimental results show that our design can\nstill accumulate progress when the power source is too weak for\ncheckpointing-based approaches to make progress, and improves the computation\nprogress by up to 43% under a relatively strong power source, while reducing\nthe recovery time by at least 90%.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.04949v1"
    },
    {
        "title": "Fissile Locks",
        "authors": [
            "Dave Dice",
            "Alex Kogan"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Classic test-and-test (TS) mutual exclusion locks are simple, and enjoy high\nperformance and low latency of ownership transfer under light or no contention.\nHowever, they do not scale gracefully under high contention and do not provide\nany admission order guarantees. Such concerns led to the development of\nscalable queue-based locks, such as a recent Compact NUMA-aware (CNA) lock, a\nvariant of another popular queue-based MCS lock. CNA scales well under load and\nprovides certain admission guarantees, but has more complicated lock handover\noperations than TS and incurs higher latencies at low contention. We propose\nFissile locks, which capture the most desirable properties of both TS and CNA.\nA Fissile lock consists of two underlying locks: a TS lock, which serves as a\nfast path, and a CNA lock, which serves as a slow path. The key feature of\nFissile locks is the ability of threads on the fast path to bypass threads\nenqueued on the slow path, and acquire the lock with less overhead than CNA.\nBypass is bounded (by a tunable parameter) to avoid starvation and ensure\nlong-term fairness. The result is a highly scalable NUMA-aware lock with\nprogress guarantees that performs like TS at low contention and like CNA at\nhigh contention.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05025v2"
    },
    {
        "title": "Combining Task-level and System-level Scheduling Modes for Mixed\n  Criticality Systems",
        "authors": [
            "Jalil Boudjadar",
            "Saravanan Ramanathan",
            "Arvind Easwaran",
            "Ulrik Nyman"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Different scheduling algorithms for mixed criticality systems have been\nrecently proposed. The common denominator of these algorithms is to discard low\ncritical tasks whenever high critical tasks are in lack of computation\nresources. This is achieved upon a switch of the scheduling mode from Normal to\nCritical. We distinguish two main categories of the algorithms: system-level\nmode switch and task-level mode switch. System-level mode algorithms allow low\ncriticality (LC) tasks to execute only in normal mode. Task-level mode switch\nalgorithms enable to switch the mode of an individual high criticality task\n(HC), from low (LO) to high (HI), to obtain priority over all LC tasks. This\npaper investigates an online scheduling algorithm for mixed-criticality systems\nthat supports dynamic mode switches for both task level and system level. When\na HC task job overruns its LC budget, then only that particular job is switched\nto HI mode. If the job cannot be accommodated, then the system switches to\nCritical mode. To accommodate for resource availability of the HC jobs, the LC\ntasks are degraded by stretching their periods until the Critical mode\nexhibiting job complete its execution. The stretching will be carried out until\nthe resource availability is met. We have mechanized and implemented the\nproposed algorithm using Uppaal. To study the efficiency of our scheduling\nalgorithm, we examine a case study and compare our results to the state of the\nart algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05442v1"
    },
    {
        "title": "Demand-based Scheduling of Mixed-Criticality Sporadic Tasks on One\n  Processor",
        "authors": [
            "Arvind Easwaran"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Strategies that artificially tighten high-criticality task deadlines in\nlow-criticality behaviors have been successfully employed for scheduling\nmixed-criticality systems. Although efficient scheduling algorithms have been\ndeveloped for implicit deadline task systems, the same is not true for more\ngeneral sporadic tasks. In this paper we develop a new demand-based\nschedulability test for such general mixed-criticality task systems, in which\nwe collectively bound the low- and high-criticality demand of tasks. We show\nthat the new test strictly dominates the only other known demand-based test for\nsuch systems. We also propose a new deadline tightening strategy based on this\ntest, and show through simulations that the strategy significantly outperforms\nall known scheduling algorithms for a variety of sporadic task systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05444v1"
    },
    {
        "title": "Utilization Difference Based Partitioned Scheduling of Mixed-Criticality\n  Systems",
        "authors": [
            "Saravanan Ramanathan",
            "Arvind Easwaran"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Mixed-Criticality (MC) systems consolidate multiple functionalities with\ndifferent criticalities onto a single hardware platform. Such systems improve\nthe overall resource utilization while guaranteeing resources to critical\ntasks. In this paper, we focus on the problem of partitioned multiprocessor MC\nscheduling, in particular the problem of designing efficient partitioning\nstrategies. We develop two new partitioning strategies based on the principle\nof evenly distributing the difference between total high-critical utilization\nand total low-critical utilization for the critical tasks among all processors.\nBy balancing this difference, we are able to reduce the pessimism in\nuniprocessor MC schedulability tests that are applied on each processor, thus\nimproving overall schedulability. To evaluate the schedulability performance of\nthe proposed strategies, we compare them against existing partitioned\nalgorithms using extensive experiments. We show that the proposed strategies\nare effective with both dynamic-priority Earliest Deadline First with Virtual\nDeadlines (EDF-VD) and fixed-priority Adaptive Mixed-Criticality (AMC)\nalgorithms. Specifically, our results show that the proposed strategies improve\nschedulability by as much as 28.1% and 36.2% for implicit and\nconstrained-deadline task systems respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05445v1"
    },
    {
        "title": "A File System For Write-Once Media",
        "authors": [
            "Simson L. Garfinkel",
            "J. Spencer Love"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  A file system standard for use with write-once media such as digital compact\ndisks is proposed. The file system is designed to work with any operating\nsystem and a variety of physical media. Although the implementation is simple,\nit provides a a full-featured and high-performance alternative to conventional\nfile systems on traditional, multiple-write media such as magnetic disks.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.00402v1"
    },
    {
        "title": "Resource Efficient Isolation Mechanisms in Mixed-Criticality Scheduling",
        "authors": [
            "Xiaozhe Gu",
            "Arvind Easwaran",
            "Kieu-My Phan",
            "Insik Shin"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Mixed-criticality real-time scheduling has been developed to improve resource\nutilization while guaranteeing safe execution of critical applications. These\nstudies use optimistic resource reservation for all the applications to improve\nutilization, but prioritize critical applications when the reservations become\ninsufficient at runtime. Many of them however share an impractical assumption\nthat all the critical applications will simultaneously demand additional\nresources. As a consequence, they under-utilize resources by penalizing all the\nlow-criticality applications. In this paper we overcome this shortcoming using\na novel mechanism that comprises a parameter to model the expected number of\ncritical applications simultaneously demanding more resources, and an execution\nstrategy based on the parameter to improve resource utilization. Since most\nmixed-criticality systems in practice are component-based, we design our\nmechanism such that the component boundaries provide the isolation necessary to\nsupport the execution of low-criticality applications, and at the same time\nprotect the critical ones. We also develop schedulability tests for the\nproposed mechanism under both a flat as well as a hierarchical scheduling\nframework. Finally, through simulations, we compare the performance of the\nproposed approach with existing studies in terms of schedulability and the\ncapability to support low-criticality applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.02400v1"
    },
    {
        "title": "Efficient Kernel Object Management for Tiered Memory Systems with KLOC",
        "authors": [
            "Sudarsun Kannan",
            "Yujie Ren",
            "Abhishek Bhatacharjee"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Software-controlled heterogeneous memory systems have the potential to\nimprove performance, efficiency, and cost tradeoffs in emerging systems.\nDelivering on this promise requires an efficient operating system (OS)\nmechanisms and policies for data management. Unfortunately, modern OSes do not\nsupport efficient tiering of data between heterogeneous memories. While this\nproblem is known (and is being studied) for application-level data pages, the\nquestion of how best to tier OS kernel objects has largely been ignored. We\nshow that careful kernel object management is vital to the performance of\nsoftware-controlled tiered memory systems. We find that the state-of-the-art OS\npage management research leaves considerable performance on the table by\noverlooking how best to tier, migrate, and manage kernel objects like inodes,\ndentry caches, journal blocks, network socket buffers, etc., associated with\nthe filesystem and networking stack. In response, we characterize hotness,\nreuse, and liveness properties of kernel objects to develop appropriate\ntiering/migration mechanisms and policies. We evaluate our proposal using a\nreal-system emulation framework on large-scale workloads like RocksDB, Redis,\nCassandra, and Spark and achieve 1.4X to 4X higher throughput compared to the\nprior art.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.04760v1"
    },
    {
        "title": "$μ$Tiles: Efficient Intra-Process Privilege Enforcement of Memory\n  Regions",
        "authors": [
            "Zahra Tarkhani",
            "Anil Madhavapeddy"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  With the alarming rate of security advisories and privacy concerns on\nconnected devices, there is an urgent need for strong isolation guarantees in\nresource-constrained devices that demand very lightweight solutions. However,\nthe status quo is that Unix-like operating systems do not offer privilege\nseparation inside a process. Lack of practical fine-grained\ncompartmentalization inside a shared address space leads to private data\nleakage through applications' untrusted dependencies and compromised threads.\nTo this end, we propose $\\mu$Tiles, a lightweight kernel abstraction and set of\nsecurity primitives based on mutual distrust for intra-process privilege\nseparation, memory protection, and secure multithreading. $\\mu$Tiles takes\nadvantage of hardware support for virtual memory tagging (e.g., ARM memory\ndomains) to achieve significant performance gain while eliminating various\nhardware limitations. Our results (based on OpenSSL, the Apache HTTP server,\nand LevelDB) show that $\\mu$Tiles is extremely lightweight (adds $\\approx 10KB$\nto kernel image) for IoT use cases. It adds negligible runtime overhead\n($\\approx 0.5\\%-3.5\\%$) and is easy to integrate with existing applications for\nproviding strong privilege separation.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.04846v1"
    },
    {
        "title": "Accelerating Filesystem Checking and Repair with pFSCK",
        "authors": [
            "David Domingo",
            "Kyle Stratton",
            "Sudarsun Kannan"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  File system checking and recovery (C/R) tools play a pivotal role in\nincreasing the reliability of storage software, identifying and correcting file\nsystem inconsistencies. However, with increasing disk capacity and data\ncontent, file system C/R tools notoriously suffer from long runtimes. We posit\nthat current file system checkers fail to exploit CPU parallelism and high\nthroughput offered by modern storage devices. To overcome these challenges, we\npropose pFSCK, a tool that redesigns C/R to enable fine-grained parallelism at\nthe granularity of inodes without impacting the correctness of C/R's\nfunctionality. To accelerate C/R, pFSCK first employs data parallelism by\nidentifying functional operations in each stage of the checker and isolating\ndependent operation and their shared data structures. However, fully isolating\nshared structures is infeasible, consequently requiring serialization that\nlimits scalability. To reduce the impact of synchronization bottlenecks and\nexploit CPU parallelism, pFSCK designs pipeline parallelism allowing multiple\nstages of C/R to run simultaneously without impacting correctness. To realize\nefficient pipeline parallelism for different file system data configurations,\npFSCK provides techniques for ordering updates to global data structures,\nefficient per-thread I/O cache management, and dynamic thread placement across\ndifferent passes of a C/R. Finally, pFSCK designs a resource-aware scheduler\naimed towards reducing the impact of C/R on other applications sharing CPUs and\nthe file system. Evaluation of pFSCK shows more than 2.6x gains of e2fsck and\nmore than 1.8x over XFS's checker that provides coarse-grained parallelism.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.05524v1"
    },
    {
        "title": "Vilamb: Low Overhead Asynchronous Redundancy for Direct Access NVM",
        "authors": [
            "Rajat Kateja",
            "Andy Pavlo",
            "Gregory R. Ganger"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Vilamb provides efficient asynchronous systemredundancy for direct access\n(DAX) non-volatile memory (NVM) storage. Production storage deployments often\nuse system-redundancy in form of page checksums and cross-page parity.\nState-of-the-art solutions for maintaining system-redundancy for DAX NVM either\nincur a high performance overhead or require specialized hardware. The Vilamb\nuser-space library maintains system-redundancy with low overhead by delaying\nand amortizing the system-redundancy updates over multiple data writes. As a\nresult, Vilamb provides 3--5x the throughput of the state-of-the-art software\nsolution at high operation rates. For applications that need system-redundancy\nwith high performance, and can tolerate some delaying of data redundancy,\nVilamb provides a tunable knob between performance and quicker redundancy. Even\nwith the delayed coverage, Vilamb increases the mean time to data loss due to\nfirmware-induced corruptions by up to two orders of magnitude in comparison to\nmaintaining no system-redundancy.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.09619v1"
    },
    {
        "title": "DPCP-p: A Distributed Locking Protocol for Parallel Real-Time Tasks",
        "authors": [
            "Maolin Yang",
            "Zewei Chen",
            "Xu Jiang",
            "Nan Guan",
            "Hang Lei"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Real-time scheduling and locking protocols are fundamental facilities to\nconstruct time-critical systems. For parallel real-time tasks, predictable\nlocking protocols are required when concurrent sub-jobs mutually exclusive\naccess to shared resources. This paper for the first time studies the\ndistributed synchronization framework of parallel real-time tasks, where both\ntasks and global resources are partitioned to designated processors, and\nrequests to each global resource are conducted on the processor on which the\nresource is partitioned. We extend the Distributed Priority Ceiling Protocol\n(DPCP) for parallel tasks under federated scheduling, with which we proved that\na request can be blocked by at most one lower-priority request. We develop task\nand resource partitioning heuristics and propose analysis techniques to safely\nbound the task response times. Numerical evaluation (with heavy tasks on 8-,\n16-, and 32-core processors) indicates that the proposed methods improve the\nschedulability significantly compared to the state-of-the-art locking protocols\nunder federated scheduling.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00706v1"
    },
    {
        "title": "LINTS^RT: A Learning-driven Testbed for Intelligent Scheduling in\n  Embedded Systems",
        "authors": [
            "Zelun Kong",
            "Yaswanth Yadlapalli",
            "Soroush Bateni",
            "Junfeng Guo",
            "Cong Liu"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Due to the increasing complexity seen in both workloads and hardware\nresources in state-of-the-art embedded systems, developing efficient real-time\nschedulers and the corresponding schedulability tests becomes rather\nchallenging. Although close to optimal schedulability performance can be\nachieved for supporting simple system models in practice, adding any small\ncomplexity element into the problem context such as non-preemption or resource\nheterogeneity would cause significant pessimism, which may not be eliminated by\nany existing scheduling technique. In this paper, we present LINTS^RT, a\nlearning-based testbed for intelligent real-time scheduling, which has the\npotential to handle various complexities seen in practice. The design of\nLINTS^RT is fundamentally motivated by AlphaGo Zero for playing the board game\nGo, and specifically addresses several critical challenges due to the real-time\nscheduling context. We first present a clean design of LINTS^RT for supporting\nthe basic case: scheduling sporadic workloads on a homogeneous multiprocessor,\nand then demonstrate how to easily extend the framework to handle further\ncomplexities such as non-preemption and resource heterogeneity. Both\napplication and OS-level implementation and evaluation demonstrate that\nLINTS^RT is able to achieve significantly higher runtime schedulability under\ndifferent settings compared to perhaps the most commonly applied schedulers,\nglobal EDF, and RM. To our knowledge, this work is the first attempt to design\nand implement an extensible learning-based testbed for autonomously making\nreal-time scheduling decisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.05136v1"
    },
    {
        "title": "Scheduling of Real-Time Tasks with Multiple Critical Sections in\n  Multiprocessor Systems",
        "authors": [
            "Jian-Jia Chen",
            "Junjie Shi",
            "Georg von der Brüggen",
            "Niklas Ueter"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  The performance of multiprocessor synchronization and locking protocols is a\nkey factor to utilize the computation power of multiprocessor systems under\nreal-time constraints. While multiple protocols have been developed in the past\ndecades, their performance highly depends on the task partition and\nprioritization. The recently proposed Dependency Graph Approach showed its\nadvantages and attracted a lot of interest. It is, however, restricted to task\nsets where each task has at most one critical section. In this paper, we remove\nthis restriction and demonstrate how to utilize algorithms for the classical\njob shop scheduling problem to construct a dependency graph for tasks with\nmultiple critical sections. To show the applicability, we discuss the\nimplementation in Litmus^{RT} and report the overheads. Moreover, we provide\nextensive numerical evaluations under different configurations, which in many\nsituations show significant improvement compared to the state-of-the-art.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.08302v1"
    },
    {
        "title": "HeRTA: Heaviside Real-Time Analysis",
        "authors": [
            "Frank Slomka",
            "Mohammadreza Sadeghi"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  We investigate the mathematical properties of event bound functions as they\nare used in the worst-case response time analysis and utilization tests. We\nfigure out the differences and similarities between the two approaches. Based\non this analysis, we derive a more general form do describe events and event\nbounds. This new unified approach gives clear new insights in the investigation\nof real-time systems, simplifies the models and will support algebraic proofs\nin future work. In the end, we present a unified analysis which allows the\nalgebraic definition of any scheduler. Introducing such functions to the\nreal-time scheduling theory will lead two a more systematic way to integrate\nnew concepts and applications to the theory. Last but not least, we show how\nthe response time analysis in dynamic scheduling can be improved.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.12112v1"
    },
    {
        "title": "Stage Lookup: Accelerating Path Lookup using Directory Shortcuts",
        "authors": [
            "Yanliang Zou",
            "Tongliang Deng",
            "Jian Zhang",
            "Chen Chen",
            "Shu Yin"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  The lookup procedure in Linux costs a significant portion of file accessing\ntime as the virtual file system (VFS) traverses the file path components one\nafter another. The lookup procedure becomes more time consuming when\napplications frequently access files, especially those with small sizes. We\npropose Stage Lookup, which dynamically caches popular directories to speed up\nlookup procedures and further reduce file accessing latency. The core of Stage\nLookup is to cache popular dentries as shortcuts, so that path walks do not\nbother to traverse directory trees from the root. Furthermore, Stage Lookup\nenriches backward path walks as it treats the directory tree in a VFS as an\nundirected map. We implement a Stage Lookup prototype and integrate it into\nLinux Kernel v3.14. Our extensive performance evaluation studies show that\nStage Lookup offers up to 46.9% performance gain compared to ordinary path\nlookup schemes. Furthermore, Stage Lookup shows smaller performance overheads\nin rename and chmod operations compared to the original method of the kernel.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08741v1"
    },
    {
        "title": "Disaggregated Accelerator Management System for Cloud Data Centers",
        "authors": [
            "Ryousei Takano",
            "Kuniyasu Suzaki"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  A conventional data center that consists of monolithic-servers is confronted\nwith limitations including lack of operational flexibility, low resource\nutilization, low maintainability, etc. Resource disaggregation is a promising\nsolution to address the above issues. We propose a concept of disaggregated\ncloud data center architecture called Flow-in-Cloud (FiC) that enables an\nexisting cluster computer system to expand an accelerator pool through a\nhigh-speed network. FlowOS-RM manages the entire pool resources, and deploys a\nuser job on a dynamically constructed slice according to a user request. This\nslice consists of compute nodes and accelerators where each accelerator is\nattached to the corresponding compute node. This paper demonstrates the\nfeasibility of FiC in a proof of concept experiment running a distributed deep\nlearning application on the prototype system. The result successfully warrants\nthe applicability of the proposed system.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.13594v1"
    },
    {
        "title": "On the Applicability of PEBS based Online Memory Access Tracking for\n  Heterogeneous Memory Management at Scale",
        "authors": [
            "Aleix Roca Nonell",
            "Balazs Gerofi",
            "Leonardo Bautista-Gomez",
            "Dominique Martinet",
            "Vicenç Beltran Querol",
            "Yutaka Ishikawa"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Operating systems have historically had to manage only a single type of\nmemory device. The imminent availability of heterogeneous memory devices based\non emerging memory technologies confronts the classic single memory model and\nopens a new spectrum of possibilities for memory management. Transparent data\nmovement between different memory devices based on access patterns of\napplications is a desired feature to make optimal use of such devices and to\nhide the complexity of memory management to the end-user. However, capturing\nmemory access patterns of an application at runtime comes at a cost, which is\nparticularly challenging for large scale parallel applications that may be\nsensitive to system noise.\n  In this work, we focus on the access pattern profiling phase prior to the\nactual memory relocation. We study the feasibility of using Intel's Processor\nEvent-Based Sampling (PEBS) feature to record memory accesses by sampling at\nruntime and study the overhead at scale. We have implemented a custom PEBS\ndriver in the IHK/McKernel lightweight multi-kernel operating system, one of\nwhose advantages is minimal system interference due to the lightweight kernel's\nsimple design compared to other OS kernels such as Linux. We present the PEBS\noverhead of a set of scientific applications and show the access patterns\nidentified in noise-sensitive HPC applications. Our results show that clear\naccess patterns can be captured with a 10% overhead in the worst-case and 1% in\nthe best case when running on up to 128k CPU cores (2,048 Intel Xeon Phi\nKnights Landing nodes). We conclude that online memory access profiling using\nPEBS at large scale is promising for memory management in heterogeneous memory\nenvironments.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.13432v1"
    },
    {
        "title": "WLFC: Write Less in Flash-based Cache",
        "authors": [
            "Chaos Dong",
            "Fang Wang",
            "Jianshun Zhang"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Flash-based disk caches, for example Bcache and Flashcache, has gained\ntremendous popularity in industry in the last decade because of its low energy\nconsumption, non-volatile nature and high I/O speed. But these cache systems\nhave a worse write performance than the read performance because of the\nasymmetric I/O costs and the the internal GC mechanism. In addition to the\nperformance issues, since the NAND flash is a type of EEPROM device, the\nlifespan is also limited by the Program/Erase (P/E) cycles. So how to improve\nthe performance and the lifespan of flash-based caches in write-intensive\nscenarios has always been a hot issue. Benefiting from Open-Channel SSDs\n(OCSSDs), we propose a write-friendly flash-based disk cache system, which is\ncalled WLFC (Write Less in the Flash-based Cache). In WLFC, a strictly\nsequential writing method is used to minimize the write amplification. A new\nreplacement algorithm for the write buffer is designed to minimize the erase\ncount caused by the evicting. And a new data layout strategy is designed to\nminimize the metadata size persisted in SSDs. As a result, the Over-Provisioned\n(OP) space is completely removed, the erase count of the flash is greatly\nreduced, and the metadata size is 1/10 or less than that in BCache. Even with a\nsmall amount of metadata, the data consistency after the crash is still\nguaranteed. Compared with the existing mechanism, WLFC brings a 7%-80%\nreduction in write latency, a 1.07*-4.5* increment in write throughput, and a\n50%-88.9% reduction in erase count, with a moderate overhead in read\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.05306v5"
    },
    {
        "title": "Supporting Multiprocessor Resource Synchronization Protocols in RTEMS",
        "authors": [
            "Junjie Shi",
            "Jan Duy Thien Pham",
            "Malte Münch",
            "Jan Viktor Hafemeister",
            "Jian-Jia Chen",
            "Kuan-Hsun Chen"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  When considering recurrent tasks in real-time systems, concurrent accesses to\nshared resources, can cause race conditions or data corruptions. Such a problem\nhas been extensively studied since the 1990s, and numerous resource\nsynchronization protocols have been developed for both uni-processor and\nmultiprocessor real-time systems, with the assumption that the implementation\noverheads are negligible. However, in practice, the implementation overheads\nmay impact the performance of different protocols depending upon the practiced\nscenarios, e.g., resources are accessed locally or remotely, and tasks spin or\nsuspend themselves when the requested resources are not available. In this\npaper, to show the applicability of different protocols in real-world systems,\nwe detail the implementation of several state-of-the-art multiprocessor\nresource synchronization protocols in RTEMS. To study the impact of the\nimplementation overheads, we deploy these implemented protocols on a real\nplatform with synthetic task set. The measured results illustrate that the\ndeveloped resource synchronization protocols in RTEMS are comparable to the\nexisted protocol, i.e., MrsP.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.06366v2"
    },
    {
        "title": "Unikraft: Fast, Specialized Unikernels the Easy Way",
        "authors": [
            "Simon Kuenzer",
            "Vlad-Andrei Bădoiu",
            "Hugo Lefeuvre",
            "Sharan Santhanam",
            "Alexander Jung",
            "Gaulthier Gain",
            "Cyril Soldani",
            "Costin Lupu",
            "Ştefan Teodorescu",
            "Costi Răducanu",
            "Cristian Banu",
            "Laurent Mathy",
            "Răzvan Deaconescu",
            "Costin Raiciu",
            "Felipe Huici"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Unikernels are famous for providing excellent performance in terms of boot\ntimes, throughput and memory consumption, to name a few metrics. However, they\nare infamous for making it hard and extremely time consuming to extract such\nperformance, and for needing significant engineering effort in order to port\napplications to them. We introduce Unikraft, a novel micro-library OS that (1)\nfully modularizes OS primitives so that it is easy to customize the unikernel\nand include only relevant components and (2) exposes a set of composable,\nperformance-oriented APIs in order to make it easy for developers to obtain\nhigh performance.\n  Our evaluation using off-the-shelf applications such as nginx, SQLite, and\nRedis shows that running them on Unikraft results in a 1.7x-2.7x performance\nimprovement compared to Linux guests. In addition, Unikraft images for these\napps are around 1MB, require less than 10MB of RAM to run, and boot in around\n1ms on top of the VMM time (total boot time 3ms-40ms). Unikraft is a Linux\nFoundation open source project and can be found at www.unikraft.org.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.12721v1"
    },
    {
        "title": "Revisiting Swapping in User-space with Lightweight Threading",
        "authors": [
            "Kan Zhong",
            "Wenlin Cui",
            "Youyou Lu",
            "Quanzhang Liu",
            "Xiaodan Yan",
            "Qizhao Yuan",
            "Siwei Luo",
            "Keji Huang"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Memory-intensive applications, such as in-memory databases, caching systems\nand key-value stores, are increasingly demanding larger main memory to fit\ntheir working sets. Conventional swapping can enlarge the memory capacity by\npaging out inactive pages to disks. However, the heavy I/O stack makes the\ntraditional kernel-based swapping suffers from several critical performance\nissues.\n  In this paper, we redesign the swapping system and propose LightSwap, an\nhigh-performance user-space swapping scheme that supports paging with both\nlocal SSDs and remote memories. First, to avoids kernel-involving, a novel page\nfault handling mechanism is proposed to handle page faults in user-space and\nfurther eliminates the heavy I/O stack with the help of user-space I/O drivers.\nSecond, we co-design Lightswap with light weight thread (LWT) to improve system\nthroughput and make it be transparent to user applications. Finally, we propose\na try-catch framework in Lightswap to deal with paging errors which are\nexacerbated by the scaling in process technology.\n  We implement Lightswap in our production-level system and evaluate it with\nYCSB workloads running on memcached. Results show that Ligthswap reduces the\npage faults handling latency by 3--5 times, and improves the throughput of\nmemcached by more than 40% compared with the stat-of-art swapping systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.13848v1"
    },
    {
        "title": "EDF-Like Scheduling for Self-Suspending Real-Time Tasks",
        "authors": [
            "Mario Günzel",
            "Kuan-Hsun Chen",
            "Jian-Jia Chen"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  In real-time systems, schedulability tests are utilized to provide timing\nguarantees. However, for self-suspending task sets, current suspension-aware\nschedulability tests are limited to Task-Level Fixed-Priority~(TFP) scheduling\nor Earliest-Deadline-First~(EDF) with constrained-deadline task systems. In\nthis work we provide a unifying schedulability test for the uniprocessor\nversion of Global EDF-Like (GEL) schedulers and arbitrary-deadline task sets. A\nlarge body of existing scheduling algorithms can be considered as EDF-Like,\nsuch as EDF, First-In-First-Out~(FIFO), Earliest-Quasi-Deadline-First~(EQDF)\nand Suspension-Aware EDF~(SAEDF). Therefore, the unifying schedulability test\nis applicable to those algorithms. Moreover, the schedulability test can be\napplied to TFP scheduling as well.\n  Our analysis is the first suspension-aware schedulability test applicable to\narbitrary-deadline sporadic real-time task systems under Job-Level\nFixed-Priority (JFP) scheduling, such as EDF. Moreover, it is the first\nunifying suspension-aware schedulability test framework that covers a wide\nrange of scheduling algorithms. Through numerical simulations, we show that the\nschedulability test outperforms the state of the art for EDF under\nconstrained-deadline scenarios. Moreover, we demonstrate the performance of\ndifferent configurations under EQDF and SAEDF.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.09725v1"
    },
    {
        "title": "Verifying and Optimizing Compact NUMA-Aware Locks on Weak Memory Models",
        "authors": [
            "Antonio Paolillo",
            "Hernán Ponce-de-León",
            "Thomas Haas",
            "Diogo Behrens",
            "Rafael Chehab",
            "Ming Fu",
            "Roland Meyer"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Developing concurrent software is challenging, especially if it has to run on\nmodern architectures with Weak Memory Models (WMMs) such as ARMv8, Power, or\nRISC-V. For the sake of performance, WMMs allow hardware and compilers to\naggressively reorder memory accesses. To guarantee correctness, developers have\nto carefully place memory barriers in the code to enforce ordering among\ncritical memory operations.\n  While WMM architectures are growing in popularity, identifying the necessary\nand sufficient barriers of complex synchronization primitives is notoriously\ndifficult. Unfortunately, publications often consider barriers to be just\nimplementation details and omit them. In this technical note, we report our\nefforts in verifying the correctness of the Compact NUMA-Aware (CNA) lock\nalgorithm on WMMs. The CNA lock is of special interest because it has been\nproposed as a new slowpath for Linux qspinlock, the main spinlock in Linux.\nBesides determining a correct and efficient set of barriers for the original\nCNA algorithm on WMMs, we investigate the correctness of Linux qspinlock and\nthe latest Linux CNA patch (v15) on the memory models LKMM, ARMv8, and Power.\nSurprisingly, we have found that Linux qspinlock and, consequently, Linux CNA\nare incorrect according to LKMM, but are still correct when compiled to ARMv8\nor Power.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.15240v2"
    },
    {
        "title": "FlexOS: Towards Flexible OS Isolation",
        "authors": [
            "Hugo Lefeuvre",
            "Vlad-Andrei Bădoiu",
            "Alexander Jung",
            "Stefan Teodorescu",
            "Sebastian Rauch",
            "Felipe Huici",
            "Costin Raiciu",
            "Pierre Olivier"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  At design time, modern operating systems are locked in a specific safety and\nisolation strategy that mixes one or more hardware/software protection\nmechanisms (e.g. user/kernel separation); revisiting these choices after\ndeployment requires a major refactoring effort. This rigid approach shows its\nlimits given the wide variety of modern applications' safety/performance\nrequirements, when new hardware isolation mechanisms are rolled out, or when\nexisting ones break.\n  We present FlexOS, a novel OS allowing users to easily specialize the safety\nand isolation strategy of an OS at compilation/deployment time instead of\ndesign time. This modular LibOS is composed of fine-grained components that can\nbe isolated via a range of hardware protection mechanisms with various data\nsharing strategies and additional software hardening. The OS ships with an\nexploration technique helping the user navigate the vast safety/performance\ndesign space it unlocks. We implement a prototype of the system and\ndemonstrate, for several applications (Redis/Nginx/SQLite), FlexOS' vast\nconfiguration space as well as the efficiency of the exploration technique: we\nevaluate 80 FlexOS configurations for Redis and show how that space can be\nprobabilistically subset to the 5 safest ones under a given performance budget.\nWe also show that, under equivalent configurations, FlexOS performs similarly\nor better than several baselines/competitors.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.06566v3"
    },
    {
        "title": "Slowing Down for Performance and Energy: An OS-Centric Study in Network\n  Driven Workloads",
        "authors": [
            "Han Dong",
            "Sanjay Arora",
            "Yara Awad",
            "Tommy Unger",
            "Orran Krieger",
            "Jonathan Appavoo"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  This paper studies three fundamental aspects of an OS that impact the\nperformance and energy efficiency of network processing: 1) batching, 2)\nprocessor energy settings, and 3) the logic and instructions of the OS\nnetworking paths. A network device's interrupt delay feature is used to induce\nbatching and processor frequency is manipulated to control the speed of\ninstruction execution. A baremetal library OS is used to explore OS path\nspecialization. This study shows how careful use of batching and interrupt\ndelay results in 2X energy and performance improvements across different\nworkloads. Surprisingly, we find polling can be made energy efficient and can\nresult in gains up to 11X over baseline Linux. We developed a methodology and a\nset of tools to collect system data in order to understand how energy is\nimpacted at a fine-grained granularity. This paper identifies a number of other\nnovel findings that have implications in OS design for networked applications\nand suggests a path forward to consider energy as a focal point of systems\nresearch.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.07010v1"
    },
    {
        "title": "New Mechanism for Fast System Calls",
        "authors": [
            "Till Miemietz",
            "Maksym Planeta",
            "Viktor Laurin Reusch"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  System calls have no place on the fast path of microsecond-scale systems.\nHowever, kernel bypass prevents the OS from controlling and supervising access\nto the hardware. In this paper we introduce the fastcall space, a new layer in\nthe traditional OS architecture, that hosts fastcalls. A fastcall implements\nthe fast path of a traditional kernel operation and can stay on the fast path,\nbecause the transition to the fastcall space is $\\approx\\times 15$ faster than\nto the kernel space. This way the OS does not give up the control over device\naccess, whereas the applications maintain their performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.10106v1"
    },
    {
        "title": "SOL: Safe On-Node Learning in Cloud Platforms",
        "authors": [
            "Yawen Wang",
            "Daniel Crankshaw",
            "Neeraja J. Yadwadkar",
            "Daniel Berger",
            "Christos Kozyrakis",
            "Ricardo Bianchini"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Cloud platforms run many software agents on each server node. These agents\nmanage all aspects of node operation, and in some cases frequently collect data\nand make decisions. Unfortunately, their behavior is typically based on\npre-defined static heuristics or offline analysis; they do not leverage on-node\nmachine learning (ML). In this paper, we first characterize the spectrum of\nnode agents in Azure, and identify the classes of agents that are most likely\nto benefit from on-node ML. We then propose SOL, an extensible framework for\ndesigning ML-based agents that are safe and robust to the range of failure\nconditions that occur in production. SOL provides a simple API to agent\ndevelopers and manages the scheduling and running of the agent-specific\nfunctions they write. We illustrate the use of SOL by implementing three\nML-based agents that manage CPU cores, node power, and memory placement. Our\nexperiments show that (1) ML substantially improves our agents, and (2) SOL\nensures that agents operate safely under a variety of failure conditions. We\nconclude that ML-based agents show significant potential and that SOL can help\nbuild them.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.10477v1"
    },
    {
        "title": "CAP-VMs: Capability-Based Isolation and Sharing for Microservices",
        "authors": [
            "Vasily A. Sartakov",
            "Lluís Vilanova",
            "David Eyers",
            "Takahiro Shinagawa",
            "Peter Pietzuch"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Cloud stacks must isolate application components, while permitting efficient\ndata sharing between components deployed on the same physical host.\nTraditionally, the MMU enforces isolation and permits sharing at page\ngranularity. MMU approaches, however, lead to cloud stacks with large TCBs in\nkernel space, and page granularity requires inefficient OS interfaces for data\nsharing. Forthcoming CPUs with hardware support for memory capabilities offer\nnew opportunities to implement isolation and sharing at a finer granularity.\n  We describe cVMs, a new VM-like abstraction that uses memory capabilities to\nisolate application components while supporting efficient data sharing, all\nwithout mandating application code to be capability-aware. cVMs share a single\nvirtual address space safely, each having only capabilities to access its own\nmemory. A cVM may include a library OS, thus minimizing its dependency on the\ncloud environment. cVMs efficiently exchange data through two capability-based\nprimitives assisted by a small trusted monitor: (i) an asynchronous read-write\ninterface to buffers shared between cVMs; and (ii) a call interface to transfer\ncontrol between cVMs. Using these two primitives, we build more expressive\nmechanisms for efficient cross-cVM communication. Our prototype implementation\nusing CHERI RISC-V capabilities shows that cVMs isolate services (Redis and\nPython) with low overhead while improving data sharing.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05732v2"
    },
    {
        "title": "Migration-Based Synchronization",
        "authors": [
            "Stefan Reif",
            "Phillip Raffeck",
            "Luis Gerhorst",
            "Wolfgang Schröder-Preikschat",
            "Timo Hönig"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  A fundamental challenge in multi- and many-core systems is the correct\nexecution of concurrent access to shared data. A common drawback from existing\nsynchronization mechanisms is the loss of data locality as the shared data is\ntransferred between the accessing cores. In real-time systems, this is\nespecially important as knowledge about data access times is crucial to\nestablish bounds on execution times and guarantee the meeting of deadlines.We\npropose in this paper a refinement of our previously sketched approach of\nMigration-Based Synchronization (MBS) as well as its first practical\nimplementation. The core concept of MBS is the replacement of data migration\nwith control-flow migration to achieve synchronized memory accesses with\nguaranteed data locality. This leads to both shorter and more predictable\nexecution times for critical sections. As MBS can be used as a substitute for\nclassical locks, it can be employed in legacy applications without code\nalterations.We further examine how the gained data locality improves the\nresults of worst-case timing analyses and results in tighter bounds on\nexecution and response time. We reason about the similarity of MBS to existing\nsynchronization approaches and how it enables us to reuse existing analysis\ntechniques.Finally, we evaluate our prototype implementation, showing that MBS\ncan exploit data locality with similar overheads as traditional locking\nmechanisms.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.09365v1"
    },
    {
        "title": "Persistent Memory Objects: Fast and Easy Crash Consistency for\n  Persistent Memory",
        "authors": [
            "Derrick Greenspan",
            "Naveed Ul Mustafa",
            "Zoran Kolega",
            "Mark Heinrich",
            "Yan Solihin"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  DIMM-compatible persistent memory unites memory and storage. Prior works\nutilize persistent memory either by combining the filesystem with direct access\non memory mapped files or by managing it as a collection of objects while\nabolishing the POSIX abstraction. In contrast, we propose retaining the POSIX\nabstraction and extending it to provide support for persistent memory, using\nPersistent Memory Objects (PMOs). In this work, we design and implement PMOs, a\ncrash-consistent abstraction for managing persistent memory. We introduce\npsync, a single system call, that a programmer can use to specify crash\nconsistency points in their code, without needing to orchestrate durability\nexplicitly. When rendering data crash consistent, our design incurs a overhead\nof $\\approx 25\\%$ and $\\approx 21\\%$ for parallel workloads and FileBench,\nrespectively, compared to a system without crash consistency. Compared to\nNOVA-Fortis, our design provides a speedup of $\\approx 1.67\\times$ and $\\approx\n3\\times$ for the two set of benchmarks, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.03289v1"
    },
    {
        "title": "Dynamic Ready Queue Based Process Priority Scheduling Algorithm",
        "authors": [
            "Raghav Dalmia",
            "Aryaman Sinha",
            "Ruchi Verma",
            "P. K. Gupta"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  CPU scheduling is the reason behind the performance of multiprocessing and in\ntime-shared operating systems. Different scheduling criteria are used to\nevaluate Central Processing Unit Scheduling algorithms which are based on\ndifferent properties of the system. Round Robin is known to be the most\nrecurrent pre-emptive algorithm used in an environment where processes are\nallotted a unit of time and multiprocessing operating systems. In this paper, a\nreformed variation of the Round Robin algorithm has been introduced to minimise\nthe completion time, turnaround time, waiting time and number of context\nswitches that results in the better performance of the system. The proposed\nwork consists of calculation of priority on the basis of the difference between\ntime spent in ready upto the moment and arrival time of the process, to ease up\nthe burden on the ready queue. We have also evaluated the performance of the\nproposed approach on different datasets and measured the different scheduling\ncriteria.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07314v1"
    },
    {
        "title": "Unikernel Linux (UKL)",
        "authors": [
            "Ali Raza",
            "Thomas Unger",
            "Matthew Boyd",
            "Eric Munson",
            "Parul Sohal",
            "Ulrich Drepper",
            "Richard Jones",
            "Daniel Bristot de Oliveira",
            "Larry Woodman",
            "Renato Mancuso",
            "Jonathan Appavoo",
            "Orran Krieger"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  This paper presents Unikernel Linux (UKL), a path toward integrating\nunikernel optimization techniques in Linux, a general purpose operating system.\nUKL adds a configuration option to Linux allowing for a single, optimized\nprocess to link with the kernel directly, and run at supervisor privilege. This\nUKL process does not require application source code modification, only a\nre-link with our, slightly modified, Linux kernel and glibc. Unmodified\napplications show modest performance gains out of the box, and developers can\nfurther optimize applications for more significant gains (e.g. 26% throughput\nimprovement for Redis). UKL retains support for co-running multiple user level\nprocesses capable of communicating with the UKL process using standard IPC. UKL\npreserves Linux's battle-tested codebase, community, and ecosystem of tools,\napplications, and hardware support. UKL runs both on bare-metal and virtual\nservers and supports multi-core execution. The changes to the Linux kernel are\nmodest (1250 LOC).\n",
        "pdf_link": "http://arxiv.org/pdf/2206.00789v2"
    },
    {
        "title": "Implementation of SquashFS Support in U-Boot",
        "authors": [
            "Mariana Villarim",
            "João Marcos Costa",
            "Diomadson Belfort"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  U-Boot is a notorious bootloader and Open Source project. This work had as\nobjective adding support for the SquashFS filesystem to U-Boot and the support\ndeveloped was submitted as a contribution to the project. The bootloader is\nresponsible, in this context, for loading the kernel and the device tree blob\ninto RAM. It needs to be capable of reading a storage device's partition\nformatted with a specific filesystem type. Adding this support allows U-Boot to\nread from SquashFS partitions. The source code was submitted to U-Boot's\nmailing list through a series of patches to be reviewed by one of the project's\nmaintainer. Once it gets merged, the support will be used and modified by\nU-Boot's international community.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.12751v1"
    },
    {
        "title": "Multilevel Bidirectional Cache Filter",
        "authors": [
            "Ohad Eytan",
            "Roy Friedman"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Modern caches are often required to handle a massive amount of data, which\nexceeds the amount of available memory; thus, hybrid caches, specifically\nDRAM/SSD combination, become more and more prevalent. In such environments, in\naddition to the classical hit-ratio target, saving writes to the second-level\ncache is a dominant factor to avoid write amplification and wear out, two\nnotorious phenomena of SSD.\n  This paper presents BiDiFilter, a novel multilevel caching scheme that\ncontrols demotions and promotions between cache levels using a frequency sketch\nfilter. Further, it splits the higher cache level into two areas to keep the\nmost recent and the most frequent items close to the user.\n  We conduct an extensive evaluation over real-world traces, comparing to\nprevious multilevel policies. We show that using our mechanism yields an x10\nsaving of writes in almost all cases and often improving latencies by up to\n20%.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.13367v1"
    },
    {
        "title": "3PO: Programmed Far-Memory Prefetching for Oblivious Applications",
        "authors": [
            "Christopher Branner-Augmon",
            "Narek Galstyan",
            "Sam Kumar",
            "Emmanuel Amaro",
            "Amy Ousterhout",
            "Aurojit Panda",
            "Sylvia Ratnasamy",
            "Scott Shenker"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Using memory located on remote machines, or far memory, as a swap space is a\npromising approach to meet the increasing memory demands of modern datacenter\napplications. Operating systems have long relied on prefetchers to mask the\nincreased latency of fetching pages from swap space to main memory.\nUnfortunately, with traditional prefetching heuristics, performance still\ndegrades when applications use far memory. In this paper we propose a new\nprefetching technique for far-memory applications. We focus our efforts on\nmemory-intensive, oblivious applications whose memory access patterns are\nindependent of their inputs, such as matrix multiplication. For this class of\napplications we observe that we can perfectly prefetch pages without relying on\nheuristics. However, prefetching perfectly without requiring significant\napplication modifications is challenging.\n  In this paper we describe the design and implementation of 3PO, a system that\nprovides pre-planned prefetching for general oblivious applications. We\ndemonstrate that 3PO can accelerate applications, e.g., running them 30-150%\nfaster than with Linux's prefetcher with 20% local memory. We also use 3PO to\nunderstand the fundamental software overheads of prefetching in a paging-based\nsystem, and the minimum performance penalty that they impose when we run\napplications under constrained local memory.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.07688v1"
    },
    {
        "title": "SFS: Smart OS Scheduling for Serverless Functions",
        "authors": [
            "Yuqi Fu",
            "Li Liu",
            "Haoliang Wang",
            "Yue Cheng",
            "Songqing Chen"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Serverless computing enables a new way of building and scaling cloud\napplications by allowing developers to write fine-grained serverless or cloud\nfunctions. The execution duration of a cloud function is typically\nshort-ranging from a few milliseconds to hundreds of seconds. However, due to\nresource contentions caused by public clouds' deep consolidation, the function\nexecution duration may get significantly prolonged and fail to accurately\naccount for the function's true resource usage. We observe that the function\nduration can be highly unpredictable with huge amplification of more than 50x\nfor an open-source FaaS platform (OpenLambda). Our experiments show that the OS\nscheduling policy of cloud functions' host server can have a crucial impact on\nperformance. The default Linux scheduler, CFS (Completely Fair Scheduler),\nbeing oblivious to workloads, frequently context-switches short functions,\ncausing a turnaround time that is much longer than their service time.\n  We propose SFS (Smart Function Scheduler),which works entirely in the user\nspace and carefully orchestrates existing Linux FIFO and CFS schedulers to\napproximate Shortest Remaining Time First (SRTF). SFS uses two-level scheduling\nthat seamlessly combines a new FILTER policy with Linux CFS, to trade off\nincreased duration of long functions for significant performance improvement\nfor short functions. We implement {\\proj} in the Linux user space and port it\nto OpenLambda. Evaluation results show that SFS significantly improves short\nfunctions' duration with a small impact on relatively longer functions,\ncompared to CFS.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.01709v2"
    },
    {
        "title": "HMM-V: Heterogeneous Memory Management for Virtualization",
        "authors": [
            "Sai sha",
            "Chuandong Li",
            "Yingwei Luo",
            "Xiaolin Wang",
            "Zhenlin Wang"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  The memory demand of virtual machines (VMs) is increasing, while DRAM has\nlimited capacity and high power consumption. Non-volatile memory (NVM) is an\nalternative to DRAM, but it has high latency and low bandwidth. We observe that\nthe VM with heterogeneous memory may incur up to a $1.5\\times$ slowdown\ncompared to a DRAM VM, if not managed well. However, none of the\nstate-of-the-art heterogeneous memory management designs are customized for\nvirtualization on a real system.\n  In this paper, we propose HMM-V, a Heterogeneous Memory Management system for\nVirtualization. HMM-V automatically determines page hotness and migrates pages\nbetween DRAM and NVM to achieve performance close to the DRAM system. First,\nHMM-V tracks memory accesses through page table manipulation, but reduces the\ncost by leveraging Intel page-modification logging (PML) and a multi-level\nqueue. Second, HMM-V quantifies the ``temperature'' of page and determines the\nhot set with bucket-sorting. HMM-V then efficiently migrates pages with minimal\naccess pause and handles dirty pages with the assistance of PML. Finally, HMM-V\nprovides pooling management to balance precious DRAM across multiple VMs to\nmaximize utilization and overall performance. HMM-V is implemented on a real\nsystem with Intel Optane DC persistent memory. The four-VM co-running results\nshow that HMM-V outperforms NUMA balancing and hardware management (Intel\nOptane memory mode) by $51\\%$ and $31\\%$, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.13111v1"
    },
    {
        "title": "Femto-Containers: Lightweight Virtualization and Fault Isolation For\n  Small Software Functions on Low-Power IoT Microcontrollers",
        "authors": [
            "Koen Zandberg",
            "Emmanuel Baccelli",
            "Shenghao Yuan",
            "Frédéric Besson",
            "Jean-Pierre Talpin"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Low-power operating system runtimes used on IoT microcontrollers typically\nprovide rudimentary APIs, basic connectivity and, sometimes, a (secure)\nfirmware update mechanism. In contrast, on less constrained hardware, networked\nsoftware has entered the age of serverless, microservices and agility. With a\nview to bridge this gap, in the paper we design Femto-Containers, a new\nmiddleware runtime which can be embedded on heterogeneous low-power IoT\ndevices. Femto-Containers enable the secure deployment, execution and isolation\nof small virtual software functions on low-power IoT devices, over the network.\nWe implement Femto-Containers, and provide integration in RIOT, a popular open\nsource IoT operating system. We then evaluate the performance of our\nimplementation, which was formally verified for fault-isolation, guaranteeing\nthat RIOT is shielded from logic loaded and executed in a Femto-Container. Our\nexperiments on various popular microcontroller architectures (Arm Cortex-M,\nESP32 and RISC-V) show that Femto-Containers offer an attractive trade-off in\nterms of memory footprint overhead, energy consumption, and security\n",
        "pdf_link": "http://arxiv.org/pdf/2210.03432v1"
    },
    {
        "title": "The Digital Foundation Platform -- A Multi-layered SOA Architecture for\n  Intelligent Connected Vehicle Operating System",
        "authors": [
            "David Yu",
            "Andy Xiao"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Legacy AD/ADAS development from OEMs centers around developing functions on\nECUs using services provided by AUTOSAR Classic Platform (CP) to meet\nautomotive-grade and mass-production requirements. The AUTOSAR CP couples\nhardware and software components statically and encounters challenges to\nprovide sufficient capacities for the processing of high-level intelligent\ndriving functions, whereas the new platform, AUTOSAR Adaptive Platform (AP) is\ndesigned to support dynamically communication and provide richer services and\nfunction abstractions for those resource-intensive (memory, CPU) applications.\nYet for both platforms, application development and the supporting system\nsoftware are still closely coupled together, and this makes application\ndevelopment and the enhancement less scalable and flexible, resulting in longer\ndevelopment cycles and slower time-to-market. This paper presents a\nmulti-layered, service-oriented intelligent driving operating system foundation\n(we named it as Digital Foundation Platform) that provides abstractions for\neasier adoption of heterogeneous computing hardware. It features a multi-layer\nSOA software architecture with each layer providing adaptive service API at\nnorth-bound for application developers. The proposed Digital Foundation\nPlatform (DFP) has significant advantages of decoupling hardware, operating\nsystem core, middle-ware, functional software and application software\ndevelopment. It provides SOA at multiple layers and enables application\ndevelopers from OEMs, to customize and develop new applications or enhance\nexisting applications with new features, either in autonomous domain or\nintelligent cockpit domain, with great agility, and less code through\nre-usability, and thus reduce the time-to-market.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.08818v1"
    },
    {
        "title": "RIO: Order-Preserving and CPU-Efficient Remote Storage Access",
        "authors": [
            "Xiaojian Liao",
            "Zhe Yang",
            "Jiwu Shu"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Modern NVMe SSDs and RDMA networks provide dramatically higher bandwidth and\nconcurrency. Existing networked storage systems (e.g., NVMe over Fabrics) fail\nto fully exploit these new devices due to inefficient storage ordering\nguarantees. Severe synchronous execution for storage order in these systems\nstalls the CPU and I/O devices and lowers the CPU and I/O performance\nefficiency of the storage system.\n  We present Rio, a new approach to the storage order of remote storage access.\nThe key insight in Rio is that the layered design of the software stack, along\nwith the concurrent and asynchronous network and storage devices, makes the\nstorage stack conceptually similar to the CPU pipeline. Inspired by the CPU\npipeline that executes out-of-order and commits in-order, Rio introduces the\nI/O pipeline that allows internal out-of-order and asynchronous execution for\nordered write requests while offering intact external storage order to\napplications. Together with merging consecutive ordered requests, these design\ndecisions make for write throughput and CPU efficiency close to that of\norderless requests.\n  We implement Rio in Linux NVMe over RDMA stack, and further build a file\nsystem named RioFS atop Rio. Evaluations show that Rio outperforms Linux NVMe\nover RDMA and a state-of-the-art storage stack named Horae by two orders of\nmagnitude and 4.9 times on average in terms of throughput of ordered write\nrequests, respectively. RioFS increases the throughput of RocksDB by 1.9 times\nand 1.5 times on average, against Ext4 and HoraeFS, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.08934v1"
    },
    {
        "title": "Protected Data Plane OS Using Memory Protection Keys and Lightweight\n  Activation",
        "authors": [
            "Yihan Yang",
            "Zhuobin Huang",
            "Antoine Kaufmann",
            "Jialin Li"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Increasing data center network speed coupled with application requirements\nfor high throughput and low latencies have raised the efficiency bar for\nnetwork stacks. To reduce substantial kernel overhead in network processing,\nrecent proposals bypass the kernel or implement the stack as user space OS\nservice -- both with performance isolation, security, and resource efficiency\ntrade-offs. We present Tardis, a new network stack architecture that combines\nthe performance and resource efficiency benefits of kernel-bypass and the\nsecurity and performance enforcement of in-kernel stacks. Tardis runs the OS\nI/O stack in user-level threads that share both address spaces and kernel\nthreads with applications, avoiding almost all kernel context switch and\ncross-core communication overheads. To provide sufficient protection, Tardis\nleverages x86 protection keys (MPK) extension to isolate the I/O stack from\napplication code. And to enforce timely scheduling of network processing and\nfine-grained performance isolation, Tardis implements lightweight scheduler\nactivations with preemption timers.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14417v2"
    },
    {
        "title": "ESP32: QEMU Emulation within a Docker Container",
        "authors": [
            "Michael Howard",
            "R. Bruce Irvin"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  The ESP32 is a popular microcontroller from Espressif that can be used in\nmany embedded applications. Robotic joints, smart car chargers, beer vat\nagitators and automated bread mixers are a few examples where this\nsystem-on-a-chip excels. It is cheap to buy and has a number of vendors\nproviding low-cost development board kits that come with the microcontroller\nand many external connection points with peripherals. There is a large software\necosystem for the ESP32. Espressif maintains an SDK containing many C-language\nsample projects providing a starting point for a huge variety of software\nservices and I/O needs. Third party projects provide additional sample code as\nwell as support for other programming languages. For example, MicroPython is a\nmature project with sample code and officially supported by Espressif. The SDK\nprovides tools to not just build an application but also merge a flash image,\nflash to the microcontroller and monitor the output. Is it possible to build\nthe ESP32 load and emulate on another host OS? This paper explores the QEMU\nemulator and its ability to emulate the ethernet interface for the guest OS.\nAdditionally, we look into the concept of containerizing the entire emulator\nand ESP32 load package such that a microcontroller flash image can successfully\nrun with a one-step deployment of a Docker container.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10204v2"
    },
    {
        "title": "Shedding Light on Static Partitioning Hypervisors for Arm-based\n  Mixed-Criticality Systems",
        "authors": [
            "José Martins",
            "Sandro Pinto"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  In this paper, we aim to understand the properties and guarantees of static\npartitioning hypervisors (SPH) for Arm-based mixed-criticality systems (MCS).\nTo this end, we performed a comprehensive empirical evaluation of popular\nopen-source SPH, i.e., Jailhouse, Xen (Dom0-less), Bao, and seL4 CAmkES VMM,\nfocusing on two key requirements of modern MCS: real-time and safety. The goal\nof this study is twofold. Firstly, to empower industrial practitioners with\nhard data to reason about the different trade-offs of SPH. Secondly, we aim to\nraise awareness of the research and open-source communities to the still open\nproblems in SPH by unveiling new insights regarding lingering weaknesses. All\nartifacts will be open-sourced to enable independent validation of results and\nencourage further exploration on SPH.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.11186v2"
    },
    {
        "title": "Virtio-FPGA: a virtualization solution for SoC-attached FPGAs",
        "authors": [
            "Anna Panagopoulou",
            "Michele Paolino",
            "Daniel Raho"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Recently, FPGA accelerators have risen in popularity as they present a\nsuitable way of satisfying the high-computation and low-power demands of real\ntime applications. The modern electric transportation systems (such as\naircraft, road vehicles) can greatly profit from embedded FPGAs, which\nincorporate both high-performance and flexibility features into a single SoC.\nAt the same time, the virtualization of FPGA resources aims to reinforce these\nsystems with strong isolation, consolidation and security. In this paper, we\npresent a novel virtualization framework aimed for SoC-attached FPGA devices,\nin a Linux and QEMU/KVM setup. We use Virtio as a means to enable the\nconfiguration of FPGA resources from guest systems in an efficient way. Also,\nwe employ the Linux VFIO and Device Tree Overlays technologies in order to\nrender the FPGA resources dynamically accessible to guest systems. The ability\nto dynamically configure and utilize the FPGA resources from a virtualization\nenvironment is described in details. The evaluation procedure of the solution\nis presented and the virtualization overhead is benchmarked as minimal (around\n10%) when accessing the FPGA devices from guest systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.01721v1"
    },
    {
        "title": "Karma: Resource Allocation for Dynamic Demands",
        "authors": [
            "Midhul Vuppalapati",
            "Giannis Fikioris",
            "Rachit Agarwal",
            "Asaf Cidon",
            "Anurag Khandelwal",
            "Eva Tardos"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  We consider the problem of fair resource allocation in a system where user\ndemands are dynamic, that is, where user demands vary over time. Our key\nobservation is that the classical max-min fairness algorithm for resource\nallocation provides many desirable properties (e.g., Pareto efficiency,\nstrategy-proofness, and fairness), but only under the strong assumption of user\ndemands being static over time. For the realistic case of dynamic user demands,\nthe max-min fairness algorithm loses one or more of these properties.\n  We present Karma, a new resource allocation mechanism for dynamic user\ndemands. The key technical contribution in Karma is a credit-based resource\nallocation algorithm: in each quantum, users donate their unused resources and\nare assigned credits when other users borrow these resources; Karma carefully\norchestrates the exchange of credits across users (based on their instantaneous\ndemands, donated resources and borrowed resources), and performs prioritized\nresource allocation based on users' credits. We theoretically establish Karma\nguarantees related to Pareto efficiency, strategy-proofness, and fairness for\ndynamic user demands. Empirical evaluations over production workloads show that\nthese properties translate well into practice: Karma is able to reduce\ndisparity in performance across users to a bare minimum while maintaining\nPareto-optimal system-wide performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.17222v2"
    },
    {
        "title": "A Survey on User-Space Storage and Its Implementations",
        "authors": [
            "Junzhe Li",
            "Xiurui Pan",
            "Shushu Yi",
            "Jie Zhang"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  The storage stack in the traditional operating system is primarily optimized\ntowards improving the CPU utilization and hiding the long I/O latency imposed\nby the slow I/O devices such as hard disk drivers (HDDs). However, the emerging\nstorage media experience significant technique shifts in the past decade, which\nexhibit high bandwidth and low latency. These high-performance storage devices,\nunfortunately, suffer from the huge overheads imposed by the system software\nincluding the long storage stack and the frequent context switch between the\nuser and kernel modes. Many researchers have investigated huge efforts in\naddressing this challenge by constructing a direct software path between a user\nprocess and the underlying storage devices. We revisit such novel designs in\nthe prior work and present a survey in this paper. Specifically, we classify\nthe former research into three categories according to their commonalities. We\nthen present the designs of each category based on the timeline and analyze\ntheir uniqueness and contributions. This paper also reviews the applications\nthat exploit the characteristics of theses designs. Given that the user-space\nstorage is a growing research field, we believe this paper can be an\ninspiration for future researchers, who are interested in the user-space\nstorage system designs.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.10503v1"
    },
    {
        "title": "Agile Development of Linux Schedulers with Ekiben",
        "authors": [
            "Samantha Miller",
            "Anirudh Kumar",
            "Tanay Vakharia",
            "Tom Anderson",
            "Ang Chen",
            "Danyang Zhuo"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Kernel task scheduling is important for application performance, adaptability\nto new hardware, and complex user requirements. However, developing, testing,\nand debugging new scheduling algorithms in Linux, the most widely used cloud\noperating system, is slow and difficult. We developed Ekiben, a framework for\nhigh velocity development of Linux kernel schedulers. Ekiben schedulers are\nwritten in safe Rust, and the system supports live upgrade of new scheduling\npolicies into the kernel, userspace debugging, and bidirectional communication\nwith applications. A scheduler implemented with Ekiben achieved near identical\nperformance (within 1% on average) to the default Linux scheduler CFS on a wide\nrange of benchmarks. Ekiben is also able to support a range of research\nschedulers, specifically the Shinjuku scheduler, a locality aware scheduler,\nand the Arachne core arbiter, with good performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.15076v1"
    },
    {
        "title": "Joint Time-and Event-Triggered Scheduling in the Linux Kernel",
        "authors": [
            "Gautam Gala",
            "Isser Kadusale",
            "Gerhard Fohler"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  There is increasing interest in using Linux in the real-time domain due to\nthe emergence of cloud and edge computing, the need to decrease costs, and the\ngrowing number of complex functional and non-functional requirements of\nreal-time applications. Linux presents a valuable opportunity as it has rich\nhardware support, an open-source development model, a well-established\nprogramming environment, and avoids vendor lock-in. Although Linux was\ninitially developed as a general-purpose operating system, some real-time\ncapabilities have been added to the kernel over many years to increase its\npredictability and reduce its scheduling latency. Unfortunately, Linux\ncurrently has no support for time-triggered (TT) scheduling, which is widely\nused in the safety-critical domain for its determinism, low run-time scheduling\nlatency, and strong isolation properties. We present an enhancement of the\nLinux scheduler as a new low-overhead TT scheduling class to support offline\ntable-driven scheduling of tasks on multicore Linux nodes. Inspired by the Slot\nshifting algorithm, we complement the new scheduling class with a low overhead\nslot shifting manager running on a non-time-triggered core to provide\nguaranteed execution time to real-time aperiodic tasks by using the slack of\nthe time-triggered tasks and avoiding high-overhead table regeneration for\nadding new periodic tasks. Furthermore, we evaluate our implementation on\nserver-grade hardware with Intel Xeon Scalable Processor.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.16271v2"
    },
    {
        "title": "Energy-aware Time- and Event-triggered KVM Nodes",
        "authors": [
            "Isser Kadusale",
            "Gautam Gala",
            "Gerhard Fohler"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Industries are considering the adoption of cloud and edge computing for\nreal-time applications due to current improvements in network latencies and the\nadvent of Fog and Edge computing. Current cloud paradigms are not designed for\nreal-time applications, as they neither provide low latencies/jitter nor the\nguarantees and determinism required by real-time applications. Experts estimate\nthat data centers use 1% of global electricity for powering the equipment, and\nin turn, for dealing with the produced heat. Hence, energy consumption is a\ncrucial metric in cloud technologies. Applying energy conservation techniques\nis not straightforward due to the increased scheduling overheads and\napplication execution times. Inspired by slot shifting, we propose an algorithm\nto support energy-aware time-triggered execution of periodic real-time VMs\nwhile still providing the ability to execute aperiodic real-time and\nbest-effort VMs in the slack of the time-triggered ones. The algorithm\nconsiders energy reduction techniques based on dynamic power management and\ndynamic voltage and frequency scaling. We implement our algorithm as an\nextension to the Linux kernel scheduler (for use with the KVM hypervisor) and\nevaluate it on a server-grade Intel Xeon node.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.00950v1"
    },
    {
        "title": "Understanding Persistent-Memory Related Issues in the Linux Kernel",
        "authors": [
            "Om Rameshwar Gatla",
            "Duo Zhang",
            "Wei Xu",
            "Mai Zheng"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Persistent memory (PM) technologies have inspired a wide range of PM-based\nsystem optimizations. However, building correct PM-based systems is difficult\ndue to the unique characteristics of PM hardware. To better understand the\nchallenges as well as the opportunities to address them, this paper presents a\ncomprehensive study of PM-related issues in the Linux kernel. By analyzing\n1,553 PM-related kernel patches in-depth and conducting experiments on\nreproducibility and tool extension, we derive multiple insights in terms of PM\npatch categories, PM bug patterns, consequences, fix strategies, triggering\nconditions, and remedy solutions. We hope our results could contribute to the\ndevelopment of robust PM-based storage systems\n",
        "pdf_link": "http://arxiv.org/pdf/2307.04095v1"
    },
    {
        "title": "FHPM: Fine-grained Huge Page Management For Virtualization",
        "authors": [
            "Chuandong Li",
            "Sai Sha",
            "Yangqing Zeng",
            "Xiran Yang",
            "Yingwei Luo",
            "Xiaolin Wang",
            "Zhenlin Wang"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  As more data-intensive tasks with large footprints are deployed in virtual\nmachines (VMs), huge pages are widely used to eliminate the increasing address\ntranslation overhead. However, once the huge page mapping is established, all\nthe base page regions in the huge page share a single extended page table (EPT)\nentry, so that the hypervisor loses awareness of accesses to base page regions.\nNone of the state-of-the-art solutions can obtain access information at base\npage granularity for huge pages. We observe that this can lead to incorrect\ndecisions by the hypervisor, such as incorrect data placement in a tiered\nmemory system and unshared base page regions when sharing pages.\n  This paper proposes FHPM, a fine-grained huge page management for\nvirtualization without hardware and guest OS modification. FHPM can identify\naccess information at base page granularity, and dynamically promote and demote\npages. A key insight of FHPM is to redirect the EPT huge page directory entries\n(PDEs) to new companion pages so that the MMU can track access information\nwithin huge pages. Then, FHPM can promote and demote pages according to the\ncurrent hot page pressure to balance address translation overhead and memory\nusage. At the same time, FHPM proposes a VM-friendly page splitting and\ncollapsing mechanism to avoid extra VM-exits. In combination, FHPM minimizes\nthe monitoring and management overhead and ensures that the hypervisor gets\nfine-grained VM memory accesses to make the proper decision. We apply FHPM to\nimprove tiered memory management (FHPM-TMM) and to promote page sharing\n(FHPM-Share). FHPM-TMM achieves a performance improvement of up to 33% and 61%\nover the pure huge page and base page management. FHPM-Share can save 41% more\nmemory than Ingens, a state-of-the-art page sharing solution, with comparable\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.10618v1"
    },
    {
        "title": "Understanding (Un)Written Contracts of NVMe ZNS Devices with zns-tools",
        "authors": [
            "Nick Tehrany",
            "Krijn Doekemeijer",
            "Animesh Trivedi"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Operational and performance characteristics of flash SSDs have long been\nassociated with a set of Unwritten Contracts due to their hidden, complex\ninternals and lack of control from the host software stack. These unwritten\ncontracts govern how data should be stored, accessed, and garbage collected.\nThe emergence of Zoned Namespace (ZNS) flash devices with their open and\nstandardized interface allows us to write these unwritten contracts for the\nstorage stack. However, even with a standardized storage-host interface, due to\nthe lack of appropriate end-to-end operational data collection tools, the\nquantification and reasoning of such contracts remain a challenge. In this\npaper, we propose zns.tools, an open-source framework for end-to-end event and\nmetadata collection, analysis, and visualization for the ZNS SSDs contract\nanalysis. We showcase how zns.tools can be used to understand how the\ncombination of RocksDB with the F2FS file system interacts with the underlying\nstorage. Our tools are available openly at\n\\url{https://github.com/stonet-research/zns-tools}.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.11860v1"
    },
    {
        "title": "A Survey on the Integration of NAND Flash Storage in the Design of File\n  Systems and the Host Storage Software Stack",
        "authors": [
            "Nick Tehrany",
            "Krijn Doekemeijer",
            "Animesh Trivedi"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  With the ever-increasing amount of data generate in the world, estimated to\nreach over 200 Zettabytes by 2025, pressure on efficient data storage systems\nis intensifying. The shift from HDD to flash-based SSD provides one of the most\nfundamental shifts in storage technology, increasing performance capabilities\nsignificantly. However, flash storage comes with different characteristics than\nprior HDD storage technology. Therefore, storage software was unsuitable for\nleveraging the capabilities of flash storage. As a result, a plethora of\nstorage applications have been design to better integrate with flash storage\nand align with flash characteristics.\n  In this literature study we evaluate the effect the introduction of flash\nstorage has had on the design of file systems, which providing one of the most\nessential mechanisms for managing persistent storage. We analyze the mechanisms\nfor effectively managing flash storage, managing overheads of introduced design\nrequirements, and leverage the capabilities of flash storage. Numerous methods\nhave been adopted in file systems, however prominently revolve around similar\ndesign decisions, adhering to the flash hardware constrains, and limiting\nsoftware intervention. Future design of storage software remains prominent with\nthe constant growth in flash-based storage devices and interfaces, providing an\nincreasing possibility to enhance flash integration in the host storage\nsoftware stack.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.11866v1"
    },
    {
        "title": "HotOS XIX Panel Report: Panel on Future of Reproduction and Replication\n  of Systems Research",
        "authors": [
            "Roberta De Viti",
            "Solal Pirelli",
            "Vaastav Anand"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  At HotOS XIX (2023), we organized a panel to discuss the future of\nreproducibility and replication in systems research. In this document, we\nhighlight the key points and themes that were discussed in the panel and\nsummarize the various opinions shared by both the panelists as well as the\nHotOS attendees.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.05762v1"
    },
    {
        "title": "Revitalising the Single Batch Environment: A 'Quest' to Achieve Fairness\n  and Efficiency",
        "authors": [
            "Supriya Manna",
            "Krishna Siva Prasad Mudigonda"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  In the realm of computer systems, efficient utilisation of the CPU (Central\nProcessing Unit) has always been a paramount concern. Researchers and engineers\nhave long sought ways to optimise process execution on the CPU, leading to the\nemergence of CPU scheduling as a field of study. This research proposes a novel\nalgorithm for batch processing that operates on a preemptive model, dynamically\nassigning priorities based on a robust ratio, employing a dynamic time slice,\nand utilising periodic sorting technique to achieve fairness. By engineering\nthis responsive and fair model, the proposed algorithm strikes a delicate\nbalance between efficiency and fairness, providing an optimised solution for\nbatch scheduling while ensuring system responsiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.10062v7"
    },
    {
        "title": "CoRD: Converged RDMA Dataplane for High-Performance Clouds",
        "authors": [
            "Maksym Planeta",
            "Jan Bierbaum",
            "Michael Roitzsch",
            "Hermann Härtig"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  High-performance networking is often characterized by kernel bypass which is\nconsidered mandatory in high-performance parallel and distributed applications.\nBut kernel bypass comes at a price because it breaks the traditional OS\narchitecture, requiring applications to use special APIs and limiting the OS\ncontrol over existing network connections. We make the case, that kernel bypass\nis not mandatory. Rather, high-performance networking relies on multiple\nperformance-improving techniques, with kernel bypass being the least effective.\nCoRD removes kernel bypass from RDMA networks, enabling efficient OS-level\ncontrol over RDMA dataplane.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.00898v1"
    },
    {
        "title": "Loupe: Driving the Development of OS Compatibility Layers",
        "authors": [
            "Hugo Lefeuvre",
            "Gaulthier Gain",
            "Vlad-Andrei Bădoiu",
            "Daniel Dinca",
            "Vlad-Radu Schiller",
            "Costin Raiciu",
            "Felipe Huici",
            "Pierre Olivier"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Supporting mainstream applications is fundamental for a new OS to have\nimpact. It is generally achieved by developing a layer of compatibility\nallowing applications developed for a mainstream OS like Linux to run\nunmodified on the new OS. Building such a layer, as we show, results in large\nengineering inefficiencies due to the lack of efficient methods to precisely\nmeasure the OS features required by a set of applications.\n  We propose Loupe, a novel method based on dynamic analysis that determines\nthe OS features that need to be implemented in a prototype OS to bring support\nfor a target set of applications and workloads. Loupe guides and boosts OS\ndevelopers as they build compatibility layers, prioritizing which features to\nimplement in order to quickly support many applications as early as possible.\nWe apply our methodology to 100+ applications and several OSes currently under\ndevelopment, demonstrating high engineering effort savings vs. existing\napproaches: for example, for the 62 applications supported by the OSv kernel,\nwe show that using Loupe, would have required implementing only 37 system calls\nvs. 92 for the non-systematic process followed by OSv developers.\n  We study our measurements and extract novel key insights. Overall, we show\nthat the burden of building compatibility layers is significantly less than\nwhat previous works suggest: in some cases, only as few as 20% of system calls\nreported by static analysis, and 50% of those reported by naive dynamic\nanalysis need an implementation for an application to successfully run standard\nbenchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.15996v1"
    },
    {
        "title": "First Principles of Big Memory Systems",
        "authors": [
            "Yu Hua"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  In this paper, we comprehensively analyze the vertical and horizontal\nextensions of existing memory hierarchy. The difference between memory and big\nmemory is well reported. We present the state-of-the-art studies upon the big\nmemory systems, together with design methodology and implementations.\nPersistence is the first principle of big memory systems. We further show the\nfull-stack and moving persistence.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.00428v2"
    },
    {
        "title": "Persistent Memory File Systems: A Survey",
        "authors": [
            "Wiebe van Breukelen",
            "Animesh Trivedi"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Persistent Memory (PM) is non-volatile byte-addressable memory that offers\nread and write latencies in the order of magnitude smaller than flash storage,\nsuch as SSDs. This survey discusses how file systems address the most prominent\nchallenges in the implementation of file systems for Persistent Memory. First,\nwe discuss how the properties of Persistent Memory change file system design.\nSecond, we discuss work that aims to optimize small file I/O and the associated\nmeta-data resolution. Third, we address how existing Persistent Memory file\nsystems achieve (meta) data persistence and consistency.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02880v1"
    },
    {
        "title": "Motivating Next-Generation OS Physical Memory Management for\n  Terabyte-Scale NVMMs",
        "authors": [
            "Shivank Garg",
            "Aravinda Prasad",
            "Debadatta Mishra",
            "Sreenivas Subramoney"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Software managed byte-addressable hybrid memory systems consisting of DRAMs\nand NVMMs offer a lot of flexibility to design efficient large scale data\nprocessing applications. Operating systems (OS) play an important role in\nenabling the applications to realize the integrated benefits of DRAMs' low\naccess latency and NVMMs' large capacity along with its persistent\ncharacteristics. In this paper, we comprehensively analyze the performance of\nconventional OS physical memory management subsystems that were designed only\nbased on the DRAM memory characteristics in the context of modern hybrid\nbyte-addressable memory systems.\n  To study the impact of high access latency and large capacity of NVMMs on\nphysical memory management, we perform an extensive evaluation on Linux with\nIntel's Optane NVMM. We observe that the core memory management functionalities\nsuch as page allocation are negatively impacted by high NVMM media latency,\nwhile functionalities such as conventional fragmentation management are\nrendered inadequate. We also demonstrate that certain traditional memory\nmanagement functionalities are affected by neither aspects of modern NVMMs. We\nconclusively motivate the need to overhaul fundamental aspects of traditional\nOS physical memory management in order to fully exploit terabyte-scale NVMMs.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.03370v1"
    },
    {
        "title": "Towards a debuggable kernel design",
        "authors": [
            "Chandrika Parimoo",
            "Ashish Gupta"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  This paper describes what it means for a kernel to be debuggable and proposes\na kernel design with debuggability in mind. We evaluate the proposed kernel\ndesign by comparing the iterations required in cyclic debugging for different\nclasses of bugs in a vanilla monolithic kernel to a variant enhanced with our\ndesign rules for debuggability. We discuss the trade offs involved in designing\na debuggable kernel.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.05399v1"
    },
    {
        "title": "GMEM: Generalized Memory Management for Peripheral Devices",
        "authors": [
            "Weixi Zhu",
            "Alan L. Cox",
            "Scott Rixner"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  This paper presents GMEM, generalized memory management, for peripheral\ndevices. GMEM provides OS support for centralized memory management of both CPU\nand devices. GMEM provides a high-level interface that decouples MMU-specific\nfunctions. Device drivers can thus attach themselves to a process's address\nspace and let the OS take charge of their memory management. This eliminates\nthe need for device drivers to \"reinvent the wheel\" and allows them to benefit\nfrom general memory optimizations integrated by GMEM. Furthermore, GMEM\ninternally coordinates all attached devices within each virtual address space.\nThis drastically improves user-level programmability, since programmers can use\na single address space within their program, even when operating across the CPU\nand multiple devices. A case study on device drivers demonstrates these\nbenefits. A GMEM-based IOMMU driver eliminates around seven hundred lines of\ncode and obtains 54% higher network receive throughput utilizing 32% less CPU\ncompared to the state-of-the-art. In addition, the GMEM-based driver of a\nsimulated GPU takes less than 70 lines of code, excluding its MMU functions.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.12554v1"
    },
    {
        "title": "Adaptive CPU Resource Allocation for Emulator in Kernel-based Virtual\n  Machine",
        "authors": [
            "Yecheng Yang",
            "Pu Pang",
            "Jiawen Wang",
            "Quan Chen",
            "Minyi Guo"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  The technologies of heterogeneous multi-core architectures, co-location, and\nvirtualization can be used to reduce server power consumption and improve\nsystem utilization, which are three important technologies for data centers.\nThis article explores the scheduling strategy of Emulator threads within\nvirtual machine processes in a scenario of co-location of multiple virtual\nmachines on heterogeneous multi-core architectures. In this co-location\nscenario, the scheduling strategy for Emulator threads significantly affects\nthe performance of virtual machines. This article focuses on this thread for\nthe first time in the relevant field. This article found that the scheduling\nlatency metric can well indicate the running status of the vCPU threads and\nEmulator threads in the virtualization environment, and applied this metric to\nthe design of the scheduling strategy. This article designed an Emulator thread\nscheduler based on heuristic rules, which, in coordination with the host\noperating system's scheduler, dynamically adjusts the scheduling scope of\nEmulator threads to improve the overall performance of virtual machines. The\narticle found that in real application scenarios, the scheduler effectively\nimproved the performance of applications within virtual machines, with a\nmaximum performance improvement of 40.7%.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.14741v1"
    },
    {
        "title": "bpftime: userspace eBPF Runtime for Uprobe, Syscall and Kernel-User\n  Interactions",
        "authors": [
            "Yusheng Zheng",
            "Tong Yu",
            "Yiwei Yang",
            "Yanpeng Hu",
            "Xiaozheng Lai",
            "Andrew Quinn"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  In kernel-centric operations, the uprobe component of eBPF frequently\nencounters performance bottlenecks, largely attributed to the overheads borne\nby context switches. Transitioning eBPF operations to user space bypasses these\nhindrances, thereby optimizing performance. This also enhances configurability\nand obviates the necessity for root access or privileges for kernel eBPF,\nsubsequently minimizing the kernel attack surface. This paper introduces\nbpftime, a novel user-space eBPF runtime, which leverages binary rewriting to\nimplement uprobe and syscall hook capabilities. Through bpftime, userspace\nuprobes achieve a 10x speed enhancement compared to their kernel counterparts\nwithout requiring dual context switches. Additionally, this runtime facilitates\nthe programmatic hooking of syscalls within a process, both safely and\nefficiently. Bpftime can be seamlessly attached to any running process,\nlimiting the need for either a restart or manual recompilation. Our\nimplementation also extends to interprocess eBPF Maps within shared memory,\ncatering to summary aggregation or control plane communication requirements.\nCompatibility with existing eBPF toolchains such as clang and libbpf is\nmaintained, not only simplifying the development of user-space eBPF without\nnecessitating any modifications but also supporting CO-RE through BTF. Through\nbpftime, we not only enhance uprobe performance but also extend the versatility\nand user-friendliness of eBPF runtime in user space, paving the way for more\nefficient and secure kernel operations.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.07923v2"
    },
    {
        "title": "Nahida: In-Band Distributed Tracing with eBPF",
        "authors": [
            "Wanqi Yang",
            "Pengfei Chen",
            "Kai Liu",
            "Huxing Zhang"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Microservices are commonly used in modern cloud-native applications to\nachieve agility. However, the complexity of service dependencies in large-scale\nmicroservices systems can lead to anomaly propagation, making fault\ntroubleshooting a challenge. To address this issue, distributed tracing systems\nhave been proposed to trace complete request execution paths, enabling\ndevelopers to troubleshoot anomalous services. However, existing distributed\ntracing systems have limitations such as invasive instrumentation, trace loss,\nor inaccurate trace correlation. To overcome these limitations, we propose a\nnew tracing system based on eBPF (extended Berkeley Packet Filter), named\nNahida, that can track complete requests in the kernel without intrusion,\nregardless of programming language or implementation. Our evaluation results\nshow that Nahida can track over 92% of requests with stable accuracy, even\nunder the high concurrency of user requests, while the state-of-the-art\nnon-invasive approaches can not track any of the requests. Importantly, Nahida\ncan track requests served by a multi-threaded application that none of the\nexisting invasive tracing systems can handle by instrumenting tracing codes\ninto libraries. Moreover, the overhead introduced by Nahida is negligible,\nincreasing service latency by only 1.55%-2.1%. Overall, Nahida provides an\neffective and non-invasive solution for distributed tracing.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.09032v1"
    },
    {
        "title": "Trace-enabled Timing Model Synthesis for ROS2-based Autonomous\n  Applications",
        "authors": [
            "Hazem Abaza",
            "Debayan Roy",
            "Shiqing Fan",
            "Selma Saidi",
            "Antonios Motakis"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Autonomous applications are typically developed over Robot Operating System\n2.0 (ROS2) even in time-critical systems like automotive. Recent years have\nseen increased interest in developing model-based timing analysis and schedule\noptimization approaches for ROS2-based applications. To complement these\napproaches, we propose a tracing and measurement framework to obtain timing\nmodels of ROS2-based applications. It offers a tracer based on extended\nBerkeley Packet Filter (eBPF) that probes different functions in ROS2\nmiddleware and reads their arguments or return values to reason about the data\nflow in applications. It combines event traces from ROS2 and the operating\nsystem to generate a directed acyclic graph showing ROS2 callbacks, precedence\nrelations between them, and their timing attributes. While being compatible\nwith existing analyses, we also show how to model (i)~message synchronization,\ne.g., in sensor fusion, and (ii)~service requests from multiple clients, e.g.,\nin motion planning. Considering that, in real-world scenarios, the application\ncode might be confidential and formal models are unavailable, our framework\nstill enables the application of existing analysis and optimization techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.13333v2"
    },
    {
        "title": "MaxMem: Colocation and Performance for Big Data Applications on Tiered\n  Main Memory Servers",
        "authors": [
            "Amanda Raybuck",
            "Wei Zhang",
            "Kayvan Mansoorshahi",
            "Aditya K. Kamath",
            "Mattan Erez",
            "Simon Peter"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  We present MaxMem, a tiered main memory management system that aims to\nmaximize Big Data application colocation and performance. MaxMem uses an\napplication-agnostic and lightweight memory occupancy control mechanism based\non fast memory miss ratios to provide application QoS under increasing\ncolocation. By relying on memory access sampling and binning to quickly\nidentify per-process memory heat gradients, MaxMem maximizes performance for\nmany applications sharing tiered main memory simultaneously. MaxMem is designed\nas a user-space memory manager to be easily modifiable and extensible, without\ncomplex kernel code development. On a system with tiered main memory consisting\nof DRAM and Intel Optane persistent memory modules, our evaluation confirms\nthat MaxMem provides 11% and 38% better throughput and up to 80% and an order\nof magnitude lower 99th percentile latency than HeMem and Linux AutoNUMA,\nrespectively, with a Big Data key-value store in dynamic colocation scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.00647v1"
    },
    {
        "title": "Robust Resource Partitioning Approach for ARINC 653 RTOS",
        "authors": [
            "Vitaly Cheptsov",
            "Alexey Khoroshilov"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Modern airborne operating systems implement the concept of robust time and\nresource partitioning imposed by the standards for aerospace and\nairborne-embedded software systems, such as ARINC 653. While these standards do\nprovide a considerable amount of design choices in regards to resource\npartitioning on the architectural and API levels, such as isolated memory\nspaces between the application partitions, predefined resource configuration,\nand unidirectional ports with limited queue and message sizes for\ninter-partition communication, they do not specify how an operating system\nshould implement them in software. Furthermore, they often tend to set the\nminimal level of the required guarantees, for example, in terms of memory\npermissions, and disregard the hardware state of the art, which presently can\nprovide considerably stronger guarantees at no extra cost. In the paper we\npresent an architecture of robust resource partitioning for ARINC 653 real-time\noperating systems based on completely static MMU configuration. The\narchitecture was implemented on different types of airborne hardware, including\nplatforms with TLB-based and page table-based MMU. Key benefits of the proposed\napproach include minimised run-time overhead and simpler verification of the\nmemory subsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.01436v1"
    },
    {
        "title": "BPF-oF: Storage Function Pushdown Over the Network",
        "authors": [
            "Ioannis Zarkadas",
            "Tal Zussman",
            "Jeremy Carin",
            "Sheng Jiang",
            "Yuhong Zhong",
            "Jonas Pfefferle",
            "Hubertus Franke",
            "Junfeng Yang",
            "Kostis Kaffes",
            "Ryan Stutsman",
            "Asaf Cidon"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Storage disaggregation, wherein storage is accessed over the network, is\npopular because it allows applications to independently scale storage capacity\nand bandwidth based on dynamic application demand. However, the added network\nprocessing introduced by disaggregation can consume significant CPU resources.\nIn many storage systems, logical storage operations (e.g., lookups,\naggregations) involve a series of simple but dependent I/O access patterns.\nTherefore, one way to reduce the network processing overhead is to execute\ndependent series of I/O accesses at the remote storage server, reducing the\nback-and-forth communication between the storage layer and the application. We\nrefer to this approach as \\emph{remote-storage pushdown}. We present BPF-oF, a\nnew remote-storage pushdown protocol built on top of NVMe-oF, which enables\napplications to safely push custom eBPF storage functions to a remote storage\nserver.\n  The main challenge in integrating BPF-oF with storage systems is preserving\nthe benefits of their client-based in-memory caches. We address this challenge\nby designing novel caching techniques for storage pushdown, including splitting\nqueries into separate in-memory and remote-storage phases and periodically\nrefreshing the client cache with sampled accesses from the remote storage\ndevice. We demonstrate the utility of BPF-oF by integrating it with three\nstorage systems, including RocksDB, a popular persistent key-value store that\nhas no existing storage pushdown capability. We show BPF-oF provides\nsignificant speedups in all three systems when accessed over the network, for\nexample improving RocksDB's throughput by up to 2.8$\\times$ and tail latency by\nup to 2.6$\\times$.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.06808v1"
    },
    {
        "title": "File System Aging",
        "authors": [
            "Alex Conway",
            "Ainesh Bakshi",
            "Arghya Bhattacharya",
            "Rory Bennett",
            "Yizheng Jiao",
            "Eric Knorr",
            "Yang Zhan",
            "Michael A. Bender",
            "William Jannen",
            "Rob Johnson",
            "Bradley C. Kuszmaul",
            "Donald E. Porter",
            "Jun Yuan",
            "Martin Farach-Colton"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  File systems must allocate space for files without knowing what will be added\nor removed in the future. Over the life of a file system, this may cause\nsuboptimal file placement decisions that eventually lead to slower performance,\nor aging. Conventional wisdom suggests that file system aging is a solved\nproblem in the common case; heuristics to avoid aging, such as colocating\nrelated files and data blocks, are effective until a storage device fills up,\nat which point space pressure exacerbates fragmentation-based aging. However,\nthis article describes both realistic and synthetic workloads that can cause\nthese heuristics to fail, inducing large performance declines due to aging,\neven when the storage device is nearly empty.\n  We argue that these slowdowns are caused by poor layout. We demonstrate a\ncorrelation between the read performance of a directory scan and the locality\nwithin a file system's access patterns, using a dynamic layout score. We\ncomplement these results with microbenchmarks that show that space pressure can\ncause a substantial amount of inter-file and intra-file fragmentation. However,\nour results suggest that the effect of free-space fragmentation on read\nperformance is best described as accelerating the file system aging process.\nThe effect on write performance is non-existent in some cases, and, in most\ncases, an order of magnitude smaller than the read degradation from\nfragmentation caused by normal usage.\n  In short, many file systems are exquisitely prone to read aging after a\nvariety of write patterns. We show, however, that aging is not inevitable.\nBetrFS, a file system based on write-optimized dictionaries, exhibits almost no\naging in our experiments. We present a framework for understanding and\npredicting aging, and identify the key features of BetrFS that avoid aging.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.08858v1"
    },
    {
        "title": "Nomad: Non-Exclusive Memory Tiering via Transactional Page Migration",
        "authors": [
            "Lingfeng Xiang",
            "Zhen Lin",
            "Weishu Deng",
            "Hui Lu",
            "Jia Rao",
            "Yifan Yuan",
            "Ren Wang"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  With the advent of byte-addressable memory devices, such as CXL memory,\npersistent memory, and storage-class memory, tiered memory systems have become\na reality. Page migration is the de facto method within operating systems for\nmanaging tiered memory. It aims to bring hot data whenever possible into fast\nmemory to optimize the performance of data accesses while using slow memory to\naccommodate data spilled from fast memory. While the existing research has\ndemonstrated the effectiveness of various optimizations on page migration, it\nfalls short of addressing a fundamental question: Is exclusive memory tiering,\nin which a page is either present in fast memory or slow memory, but not both\nsimultaneously, the optimal strategy for tiered memory management?\n  We demonstrate that page migration-based exclusive memory tiering suffers\nsignificant performance degradation when fast memory is under pressure. In this\npaper, we propose non-exclusive memory tiering, a page management strategy that\nretains a copy of pages recently promoted from slow memory to fast memory to\nmitigate memory thrashing. To enable non-exclusive memory tiering, we develop\nNomad, a new page management mechanism for Linux that features transactional\npage migration and page shadowing. Nomad helps remove page migration off the\ncritical path of program execution and makes migration completely asynchronous.\nEvaluations with carefully crafted micro-benchmarks and real-world applications\nshow that Nomad is able to achieve up to 6x performance improvement over the\nstate-of-the-art transparent page placement (TPP) approach in Linux when under\nmemory pressure. We also compare Nomad with a recently proposed\nhardware-assisted, access sampling-based page migration approach and\ndemonstrate Nomad's strengths and potential weaknesses in various scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.13154v2"
    },
    {
        "title": "numaPTE: Managing Page-Tables and TLBs on NUMA Systems",
        "authors": [
            "Bin Gao",
            "Qingxuan Kang",
            "Hao-Wei Tee",
            "Kyle Timothy Ng Chu",
            "Alireza Sanaee",
            "Djordje Jevdjic"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Memory management operations that modify page-tables, typically performed\nduring memory allocation/deallocation, are infamous for their poor performance\nin highly threaded applications, largely due to process-wide TLB shootdowns\nthat the OS must issue due to the lack of hardware support for TLB coherence.\nWe study these operations in NUMA settings, where we observe up to 40x overhead\nfor basic operations such as munmap or mprotect. The overhead further increases\nif page-table replication is used, where complete coherent copies of the\npage-tables are maintained across all NUMA nodes. While eager system-wide\nreplication is extremely effective at localizing page-table reads during\naddress translation, we find that it creates additional penalties upon any\npage-table changes due to the need to maintain all replicas coherent.\n  In this paper, we propose a novel page-table management mechanism, called\nnumaPTE, to enable transparent, on-demand, and partial page-table replication\nacross NUMA nodes in order to perform address translation locally, while\navoiding the overheads and scalability issues of system-wide full page-table\nreplication. We then show that numaPTE's precise knowledge of page-table\nsharers can be leveraged to significantly reduce the number of TLB shootdowns\nissued upon any memory-management operation. As a result, numaPTE not only\navoids replication-related slowdowns, but also provides significant speedup\nover the baseline on memory allocation/deallocation and access control\noperations. We implement numaPTEin Linux on x86_64, evaluate it on 4- and\n8-socket systems, and show that numaPTE achieves the full benefits of eager\npage-table replication on a wide range of applications, while also achieving a\n12% and 36% runtime improvement on Webserver and Memcached respectively due to\na significant reduction in TLB shootdowns.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15558v1"
    },
    {
        "title": "Next4: Snapshots in Ext4 File System",
        "authors": [
            "Aditya Dani",
            "Shardul Mangade",
            "Piyush Nimbalkar",
            "Harshad Shirwadkar"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  The growing value of data as a strategic asset has given rise to the\nnecessity of implementing reliable backup and recovery solutions in the most\nefficient and cost-effective manner. The data backup methods available today on\nlinux are not effective enough, because while running, most of them block I/Os\nto guarantee data integrity. We propose and implement Next4 - file system based\nsnapshot feature in Ext4 which creates an instant image of the file system, to\nprovide incremental versions of data, enabling reliable backup and data\nrecovery. In our design, the snapshot feature is implemented by efficiently\ninfusing the copy-on-write strategy in the write-in-place, extent based Ext4\nfile system, without affecting its basic structure. Each snapshot is an\nincremental backup of the data within the system. What distinguishes Next4 is\nthe way that the data is backed up, improving both space utilization as well as\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.06790v1"
    },
    {
        "title": "LLM as a System Service on Mobile Devices",
        "authors": [
            "Wangsong Yin",
            "Mengwei Xu",
            "Yuanchun Li",
            "Xuanzhe Liu"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Being more powerful and intrusive into user-device interactions, LLMs are\neager for on-device execution to better preserve user privacy. In this work, we\npropose a new paradigm of mobile AI: LLM as a system service on mobile devices\n(LLMaaS). Unlike traditional DNNs that execute in a stateless manner, such a\nsystem service is stateful: LLMs execution often needs to maintain persistent\nstates (mainly KV cache) across multiple invocations. To minimize the LLM\ncontext switching overhead under tight device memory budget, this work presents\nLLMS, which decouples the memory management of app and LLM contexts with a key\nidea of fine-grained, chunk-wise, globally-optimized KV cache compression and\nswapping. By fully leveraging KV cache's unique characteristics, it proposes\nthree novel techniques: (1) Tolerance-Aware Compression: it compresses chunks\nbased on their measured accuracy tolerance to compression. (2) IO-Recompute\nPipelined Loading: it introduces recompute to swapping-in for acceleration. (3)\nChunk Lifecycle Management: it optimizes the memory activities of chunks with\nan ahead-of-time swapping-out and an LCTRU (Least Compression-Tolerable and\nRecently-Used) queue based eviction. In evaluations conducted on\nwell-established traces and various edge devices, \\sys reduces context\nswitching latency by up to 2 orders of magnitude when compared to competitive\nbaseline solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.11805v1"
    },
    {
        "title": "SVFF: An Automated Framework for SR-IOV Virtual Function Management in\n  FPGA Accelerated Virtualized Environments",
        "authors": [
            "Stefano Cirici",
            "Michele Paolino",
            "Daniel Raho"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  FPGA accelerator devices have emerged as a powerful platform for implementing\nhigh-performance and scalable solutions in a wide range of industries,\nleveraging their reconfigurability and virtualization capabilities.\nVirtualization, in particular, offers several benefits including improved\nsecurity by resource isolation and sharing, and SR-IOV is the main solution for\nenabling it on FPGAs.\n  This paper introduces the SR-IOV Virtual Function Framework (SVFF), a\nsolution that aims to simplify and enhance the management of Virtual Functions\n(VFs) on PCIe-attached FPGA devices in Linux and QEMU/KVM environments, solving\nthe lack of SR-IOV re-configuration support on guests. The framework leverages\nthe SR-IOV support in the Xilinx Queue-based Direct Memory Access (QDMA) to\nautomate the creation, attachment, detachment, and reconfiguration of VFs to\ndifferent Virtual Machines (VMs). A novel pause functionality for the VFIO\ndevice has been implemented in QEMU to enable the detachment of VFs from the\nhost without detaching them from the guest, making reconfiguration of VFs\ntransparent for guests that already have a VF attached to them without any\nperformance loss. The proposed solution offers the ability to automatically and\nseamlessly assign a set of VFs to different VMs and adjust the configuration on\nthe fly. Thanks to the pause functionality, it also offers the ability to\nattach additional VFs to new VMs without affecting devices already attached to\nother VMs.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.01225v1"
    },
    {
        "title": "SquirrelFS: using the Rust compiler to check file-system crash\n  consistency",
        "authors": [
            "Hayley LeBlanc",
            "Nathan Taylor",
            "James Bornholt",
            "Vijay Chidambaram"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  This work introduces a new approach to building crash-safe file systems for\npersistent memory. We exploit the fact that Rust's typestate pattern allows\ncompile-time enforcement of a specific order of operations. We introduce a\nnovel crash-consistency mechanism, Synchronous Soft Updates, that boils down\ncrash safety to enforcing ordering among updates to file-system metadata. We\nemploy this approach to build SquirrelFS, a new file system with\ncrash-consistency guarantees that are checked at compile time. SquirrelFS\navoids the need for separate proofs, instead incorporating correctness\nguarantees into the typestate itself. Compiling SquirrelFS only takes tens of\nseconds; successful compilation indicates crash consistency, while an error\nprovides a starting point for fixing the bug. We evaluate SquirrelFS against\nstate of the art file systems such as NOVA and WineFS, and find that SquirrelFS\nachieves similar or better performance on a wide range of benchmarks and\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.09649v1"
    },
    {
        "title": "Simulation of high-performance memory allocators",
        "authors": [
            "José L. Risco-Martín",
            "J. Manuel Colmenar",
            "David Atienza",
            "J. Ignacio Hidalgo"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  For the last thirty years, a large variety of memory allocators have been\nproposed. Since performance, memory usage and energy consumption of each memory\nallocator differs, software engineers often face difficult choices in selecting\nthe most suitable approach for their applications. To this end, custom\nallocators are developed from scratch, which is a difficult and error-prone\nprocess. This issue has special impact in the field of portable consumer\nembedded systems, that must execute a limited amount of multimedia\napplications, demanding high performance and extensive memory usage at a low\nenergy consumption. This paper presents a flexible and efficient simulator to\nstudy Dynamic Memory Managers (DMMs), a composition of one or more memory\nallocators. This novel approach allows programmers to simulate custom and\ngeneral DMMs, which can be composed without incurring any additional runtime\noverhead or additional programming cost. We show that this infrastructure\nsimplifies DMM construction, mainly because the target application does not\nneed to be compiled every time a new DMM must be evaluated and because we\npropose a structured method to search and build DMMs in an object-oriented\nfashion. Within a search procedure, the system designer can choose the \"best\"\nallocator by simulation for a particular target application and embedded\nsystem. In our evaluation, we show that our scheme delivers better performance,\nless memory usage and less energy consumption than single memory allocators.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.15776v1"
    },
    {
        "title": "E-Mapper: Energy-Efficient Resource Allocation for Traditional Operating\n  Systems on Heterogeneous Processors",
        "authors": [
            "Till Smejkal",
            "Robert Khasanov",
            "Jeronimo Castrillon",
            "Hermann Härtig"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Energy efficiency has become a key concern in modern computing. Major\nprocessor vendors now offer heterogeneous architectures that combine powerful\ncores with energy-efficient ones, such as Intel P/E systems, Apple M1 chips,\nand Samsungs Exyno's CPUs. However, apart from simple cost-based thread\nallocation strategies, today's OS schedulers do not fully exploit these\nsystems' potential for adaptive energy-efficient computing. This is, in part,\ndue to missing application-level interfaces to pass information about\ntask-level energy consumption and application-level elasticity. This paper\npresents E-Mapper, a novel resource management approach integrated into Linux\nfor improved execution on heterogeneous processors. In E-Mapper, we base\nresource allocation decisions on high-level application descriptions that user\ncan attach to programs or that the system can learn automatically at runtime.\nOur approach supports various programming models including OpenMP, Intel TBB,\nand TensorFlow. Crucially, E-Mapper leverages this information to extend beyond\nexisting thread-to-core allocation strategies by actively managing application\nconfigurations through a novel uniform application-resource manager interface.\nBy doing so, E-Mapper achieves substantial enhancements in both performance and\nenergy efficiency, particularly in multi-application scenarios. On an Intel\nRaptor Lake and an Arm big.LITTLE system, E-Mapper reduces the application\nexecution on average by 20 % with an average reduction in energy consumption of\n34 %. We argue that our solution marks a crucial step toward creating a generic\napproach for sustainable and efficient computing across different processor\narchitectures.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.18980v1"
    },
    {
        "title": "Data-driven Software-based Power Estimation for Embedded Devices",
        "authors": [
            "Haoyu Wang",
            "Xinyi Li",
            "Ti Zhou",
            "Man Lin"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Energy measurement of computer devices, which are widely used in the Internet\nof Things (IoT), is an important yet challenging task. Most of these IoT\ndevices lack ready-to-use hardware or software for power measurement. A\ncost-effective solution is to use low-end consumer-grade power meters. However,\nthese low-end power meters cannot provide accurate instantaneous power\nmeasurements. In this paper, we propose an easy-to-use approach to derive an\ninstantaneous software-based energy estimation model with only low-end power\nmeters based on data-driven analysis through machine learning. Our solution is\ndemonstrated with a Jetson Nano board and Ruideng UM25C USB power meter.\nVarious machine learning methods combined with our smart data collection method\nand physical measurement are explored. Benchmarks were used to evaluate the\nderived software-power model for the Jetson Nano board and Raspberry Pi. The\nresults show that 92% accuracy can be achieved compared to the long-duration\nmeasurement. A kernel module that can collect running traces of utilization and\nfrequencies needed is developed, together with the power model derived, for\npower prediction for programs running in real environment.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.02764v1"
    },
    {
        "title": "NVPC: A Transparent NVM Page Cache",
        "authors": [
            "Guoyu Wang",
            "Xilong Che",
            "Haoyang Wei",
            "Shuo Chen",
            "Puyi He",
            "Juncheng Hu"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.02911v1"
    },
    {
        "title": "Crash Consistency in DRAM-NVM-Disk Hybrid Storage System",
        "authors": [
            "Guoyu Wang",
            "Xilong Che",
            "Haoyang Wei",
            "Chenju Pei",
            "Juncheng Hu"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  NVM is used as a new hierarchy in the storage system, due to its intermediate\nspeed and capacity between DRAM, and its byte granularity. However, consistency\nproblems emerge when we attempt to put DRAM, NVM, and disk together as an\nefficient whole. In this paper, we discuss the challenging consistency problems\nfaced by heterogeneous storage systems, and propose our solution to the\nproblems. The discussion is based on NVPC as a case study, but can be inspiring\nand adaptive to all similar heterogeneous storage systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04238v1"
    },
    {
        "title": "Wasm-bpf: Streamlining eBPF Deployment in Cloud Environments with\n  WebAssembly",
        "authors": [
            "Yusheng Zheng",
            "Tong Yu",
            "Yiwei Yang",
            "Andrew Quinn"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  The extended Berkeley Packet Filter (eBPF) is extensively utilized for\nobservability and performance analysis in cloud-native environments. However,\ndeploying eBPF programs across a heterogeneous cloud environment presents\nchallenges, including compatibility issues across different kernel versions,\noperating systems, runtimes, and architectures. Traditional deployment methods,\nsuch as standalone containers or tightly integrated core applications, are\ncumbersome and inefficient, particularly when dynamic plugin management is\nrequired. To address these challenges, we introduce Wasm-bpf, a lightweight\nruntime on WebAssembly and the WebAssembly System Interface (WASI). Leveraging\nWasm platform independence and WASI standardized system interface, with\nenhanced relocation for different architectures, Wasm-bpf ensures\ncross-platform compatibility for eBPF programs. It simplifies deployment by\nintegrating with container toolchains, allowing eBPF programs to be packaged as\nWasm modules that can be easily managed within cloud environments.\nAdditionally, Wasm-bpf supports dynamic plugin management in WebAssembly. Our\nimplementation and evaluation demonstrate that Wasm-bpf introduces minimal\noverhead compared to native eBPF implementations while simplifying the\ndeployment process.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04856v1"
    },
    {
        "title": "FRAP: A Flexible Resource Accessing Protocol for Multiprocessor\n  Real-Time Systems",
        "authors": [
            "Shuai Zhao",
            "Hanzhi Xu",
            "Nan Chen",
            "Ruoxian Su",
            "Wanli Chang"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Fully-partitioned fixed-priority scheduling (FP-FPS) multiprocessor systems\nare widely found in real-time applications, where spin-based protocols are\noften deployed to manage the mutually exclusive access of shared resources.\nUnfortunately, existing approaches either enforce rigid spin priority rules for\nresource accessing or carry significant pessimism in the schedulability\nanalysis, imposing substantial blocking time regardless of task execution\nurgency or resource over-provisioning. This paper proposes FRAP, a spin-based\nflexible resource accessing protocol for FP-FPS systems. A task under FRAP can\nspin at any priority within a range for accessing a resource, allowing flexible\nand fine-grained resource control with predictable worst-case behaviour. Under\nflexible spinning, we demonstrate that the existing analysis techniques can\nlead to incorrect timing bounds and present a novel MCMF (minimum cost maximum\nflow)-based blocking analysis, providing predictability guarantee for FRAP. A\nspin priority assignment is reported that fully exploits flexible spinning to\nreduce the blocking time of tasks with high urgency, enhancing the performance\nof FRAP. Experimental results show that FRAP outperforms the existing\nspin-based protocols in schedulability by 15.20%-32.73% on average, up to\n65.85%.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.13772v2"
    },
    {
        "title": "Tide: A Split OS Architecture for Control Plane Offloading",
        "authors": [
            "Jack Tigar Humphries",
            "Neel Natu",
            "Kostis Kaffes",
            "Stanko Novaković",
            "Paul Turner",
            "Hank Levy",
            "David Culler",
            "Christos Kozyrakis"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  The end of Moore's Law is driving cloud providers to offload virtualization\nand the network data plane to SmartNICs to improve compute efficiency. Even\nthough individual OS control plane tasks consume up to 5% of cycles across the\nfleet, they remain on the host CPU because they are tightly intertwined with OS\nmechanisms. Moreover, offloading puts the slow PCIe interconnect in the\ncritical path of OS decisions.\n  We propose Tide, a new split OS architecture that separates OS control plane\npolicies from mechanisms and offloads the control plane policies onto a\nSmartNIC. Tide has a new host-SmartNIC communication API, state synchronization\nmechanism, and communication mechanisms that overcome the PCIe bottleneck, even\nfor $\\mu$s-scale workloads. Tide frees up host compute for applications and\nunlocks new optimization opportunities, including machine learning-driven\npolicies, scheduling on the network I/O path, and reducing on-host\ninterference. We demonstrate that Tide enables OS control planes that are\ncompetitive with on-host performance for the most difficult $\\mu$s-scale\nworkloads. Tide outperforms on-host control planes for memory management\n(saving 16 host cores), Stubby network RPCs (saving 8 cores), and GCE virtual\nmachine management (11.2% performance improvement).\n",
        "pdf_link": "http://arxiv.org/pdf/2408.17351v2"
    },
    {
        "title": "Foreactor: Exploiting Storage I/O Parallelism with Explicit Speculation",
        "authors": [
            "Guanzhou Hu",
            "Andrea Arpaci-Dusseau",
            "Remzi Arpaci-Dusseau"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  We introduce explicit speculation, a variant of I/O speculation technique\nwhere I/O system calls can be parallelized under the guidance of explicit\napplication code knowledge. We propose a formal abstraction -- the foreaction\ngraph -- which describes the exact pattern of I/O system calls in an\napplication function as well as any necessary computation associated to produce\ntheir argument values. I/O system calls can be issued ahead of time if the\ngraph says it is safe and beneficial to do so. With explicit speculation,\nserial applications can exploit storage I/O parallelism without involving\nexpensive prediction or checkpointing mechanisms.\n  Based on explicit speculation, we implement Foreactor, a library framework\nthat allows application developers to concretize foreaction graphs and enable\nconcurrent I/O with little or no modification to application source code.\nExperimental results show that Foreactor is able to improve the performance of\nboth synthetic benchmarks and real applications by significant amounts\n(29%-50%).\n",
        "pdf_link": "http://arxiv.org/pdf/2409.01580v1"
    },
    {
        "title": "Head-First Memory Allocation on Best-Fit with Space-Fitting",
        "authors": [
            "Adam Noto Hakarsa"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Although best-fit is known to be slow, it excels at optimizing memory space\nutilization. Interestingly, by keeping the free memory region at the top of the\nmemory, the process of memory allocation and deallocation becomes approximately\n34.86% faster while also maintaining external fragmentation at minimum.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.03488v1"
    },
    {
        "title": "Analysis of Synchronization Mechanisms in Operating Systems",
        "authors": [
            "Oluwatoyin Kode",
            "Temitope Oyemade"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  This research analyzed the performance and consistency of four\nsynchronization mechanisms-reentrant locks, semaphores, synchronized methods,\nand synchronized blocks-across three operating systems: macOS, Windows, and\nLinux. Synchronization ensures that concurrent processes or threads access\nshared resources safely, and efficient synchronization is vital for maintaining\nsystem performance and reliability. The study aimed to identify the\nsynchronization mechanism that balances efficiency, measured by execution time,\nand consistency, assessed by variance and standard deviation, across platforms.\nThe initial hypothesis proposed that mutex-based mechanisms, specifically\nsynchronized methods and blocks, would be the most efficient due to their\nsimplicity. However, empirical results showed that reentrant locks had the\nlowest average execution time (14.67ms), making them the most efficient\nmechanism, but with the highest variability (standard deviation of 1.15). In\ncontrast, synchronized methods, blocks, and semaphores exhibited higher average\nexecution times (16.33ms for methods and 16.67ms for blocks) but with greater\nconsistency (variance of 0.33). The findings indicated that while reentrant\nlocks were faster, they were more platform-dependent, whereas mutex-based\nmechanisms provided more predictable performance across all operating systems.\nThe use of virtual machines for Windows and Linux was a limitation, potentially\naffecting the results. Future research should include native testing and\nexplore additional synchronization mechanisms and higher concurrency levels.\nThese insights help developers and system designers optimize synchronization\nstrategies for either performance or stability, depending on the application's\nrequirements.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.11271v1"
    },
    {
        "title": "Dissecting CXL Memory Performance at Scale: Analysis, Modeling, and\n  Optimization",
        "authors": [
            "Jinshu Liu",
            "Hamid Hadian",
            "Hanchen Xu",
            "Daniel S. Berger",
            "Huaicheng Li"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  We present SupMario, a characterization framework designed to thoroughly\nanalyze, model, and optimize CXL memory performance. SupMario is based on\nextensive evaluation of 265 workloads spanning 4 real CXL devices within 7\nmemory latency configurations across 4 processor platforms. SupMario uncovers\nmany key insights, including detailed workload performance at sub-us memory\nlatencies (140-410 ns), CXL tail latencies, CPU tolerance to CXL latencies, CXL\nperformance root-cause analysis and precise performance prediction models. In\nparticular, SupMario performance models rely solely on 12 CPU performance\ncounters and accurately fit over 99% and 91%-94% workloads with a 10%\nmisprediction target for NUMA and CXL memory, respectively. We demonstrate the\npractical utility of SupMario characterization findings, models, and insights\nby applying them to popular CXL memory management schemes, such as page\ninterleaving and tiering policies, to identify system inefficiencies during\nruntime. We introduce a novel ``bestshot'' page interleaving policy and a\nregulated page tiering policy (Alto) tailored for memory bandwidth- and\nlatency-sensitive workloads. In bandwidth bound scenarios, our ``best-shot''\ninterleaving, guided by our novel performance prediction model, achieves\nclose-to optimal scenarios by exploiting the aggregate system and CXL/NUMA\nmemory bandwidth. For latency sensitive workloads, Alto, driven by our key\ninsight of utilizing ``amortized'' memory latency to regulate unnecessary page\nmigrations, achieves up to 177% improvement over state-of-the-art memory\ntiering systems like TPP, as demonstrated through extensive evaluation with 8\nreal-world applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.14317v1"
    },
    {
        "title": "Assessing FIFO and Round Robin Scheduling:Effects on Data Pipeline\n  Performance and Energy Usage",
        "authors": [
            "Malobika Roy Choudhury",
            "Akshat Mehrotra"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  In the case of compute-intensive machine learning, efficient operating system\nscheduling is crucial for performance and energy efficiency. This paper\nconducts a comparative study over FIFO(First-In-First-Out) and RR(Round-Robin)\nscheduling policies with the application of real-time machine learning training\nprocesses and data pipelines on Ubuntu-based systems. Knowing a few patterns of\nCPU usage and energy consumption, we identify which policy (the exclusive or\nthe shared) provides higher performance and/or lower energy consumption for\ntypical modern workloads. Results of this study would help in providing better\noperating system schedulers for modern systems like Ubuntu, working to improve\nperformance and reducing energy consumption in compute intensive workloads.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.15704v1"
    },
    {
        "title": "Exploring Time-Space trade-offs for synchronized in Lilliput",
        "authors": [
            "Dave Dice",
            "Alex Kogan"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  In the context of Project Lilliput, which attempts to reduce the size of\nobject header in the HotSpot Java Virtual Machine (JVM), we explore a curated\nset of synchronization algorithms. Each of the algorithms could serve as a\npotential replacement implementation for the \"synchronized\" construct in\nHotSpot. Collectively, the algorithms illuminate trade-offs in space-time\nproperties. The key design decisions are where to locate synchronization\nmetadata (monitor fields), how to map from an object to those fields, and the\nlifecycle of the monitor information. The reader is assumed to be familiar with\ncurrent HotSpot implementation of \"synchronized\" as well as the Compact Java\nMonitors (CJM) design and Project Lilliput.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.18342v1"
    },
    {
        "title": "Global Scheduling of Weakly-Hard Real-Time Tasks using Job-Level\n  Priority Classes",
        "authors": [
            "V. Gabriel Moyano",
            "Zain A. H. Hammadeh",
            "Selma Saidi",
            "Daniel Lüdtke"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Real-time systems are intrinsic components of many pivotal applications, such\nas self-driving vehicles, aerospace and defense systems. The trend in these\napplications is to incorporate multiple tasks onto fewer, more powerful\nhardware platforms, e.g., multi-core systems, mainly for reducing cost and\npower consumption. Many real-time tasks, like control tasks, can tolerate\noccasional deadline misses due to robust algorithms. These tasks can be modeled\nusing the weakly-hard model. Literature shows that leveraging the weakly-hard\nmodel can relax the over-provisioning associated with designed real-time\nsystems. However, a wide-range of the research focuses on single-core\nplatforms. Therefore, we strive to extend the state-of-the-art of scheduling\nweakly-hard real-time tasks to multi-core platforms. We present a global\njob-level fixed priority scheduling algorithm together with its schedulability\nanalysis. The scheduling algorithm leverages the tolerable continuous deadline\nmisses to assigning priorities to jobs. The proposed analysis extends the\nResponse Time Analysis (RTA) for global scheduling to test the schedulability\nof tasks. Hence, our analysis scales with the number of tasks and number of\ncores because, unlike literature, it depends neither on Integer Linear\nProgramming nor reachability trees. Schedulability analyses show that the\nschedulability ratio is improved by 40% comparing to the global Rate Monotonic\n(RM) scheduling and up to 60% more than the global EDF scheduling, which are\nthe state-of-the-art schedulers on the RTEMS real-time operating system. Our\nevaluation on industrial embedded multi-core platform running RTEMS shows that\nthe scheduling overhead of our proposal does not exceed 60 Nanosecond.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.01528v1"
    },
    {
        "title": "Optimizing over FP/EDF Execution Times: Known Results and Open Problems",
        "authors": [
            "Enrico Bini"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  In many use cases the execution time of tasks is unknown and can be chosen by\nthe designer to increase or decrease the application features depending on the\navailability of processing capacity. If the application has real-time\nconstraints, such as deadlines, then the necessary and sufficient\nschedulability test must allow the execution times to be left unspecified. By\ndoing so, the designer can then perform optimization of the execution times by\npicking the schedulable values that minimize any given cost.\n  In this paper, we review existing results on the formulation of both the\nFixed Priority and Earliest Deadline First exact schedulability constraints.\nThe reviewed formulations are expressed by a combination of linear constraints,\nwhich enables then optimization routines.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.14381v2"
    },
    {
        "title": "Transparent and Efficient Live Migration across Heterogeneous Hosts with\n  Wharf",
        "authors": [
            "Yiwei Yang",
            "Aibo Hu",
            "Yusheng Zheng",
            "Brian Zhao",
            "Xinqi Zhang",
            "Andrew Quinn"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Live migration allows a user to move a running application from one machine\n(a source) to another (a destination) without restarting it. The technique has\nproven useful for diverse tasks including load balancing, managing system\nupdates, improving data locality, and improving system resilience.\nUnfortunately, current live migration solutions fail to meet today's computing\nneeds. First, most techniques do not support heterogeneous source and\ndestination hosts, as they require the two machines to have the same\ninstruction set architecture (ISA) or use the same operating system (OS), which\nhampers numerous live migration usecases. Second, many techniques are not\ntransparent, as they require that applications be written in a specific\nhigh-level language or call specific library functions, which imposes barriers\nto entry for many users. We present a new lightweight abstraction, called a\nvessel, that supports transparent heterogeneous live migration. A vessel\nmaintains a machine-independent encoding of a process's state, using\nWebAssembly abstractions, allowing it to be executed on nearly-arbitrary ISAs.\nA vessel virtualizes all of its OS state, using the WebAssembly System\nInterface (WASI), allowing it to execute on nearly arbitrary OS. We introduce\ndocks and software systems that execute and migrate vessels. Docks face two key\nchallenges: First, maintaining a machine-independent encoding at all points in\na process is extremely expensive. So, docks instead ensure that a vessel is\nguaranteed to eventually reach a machine-independent point and delay the\ninitiation of vessel migration until the vessel reaches such a point. Second, a\ndock may receive a vessel migration that originates from a dock executing on a\ndifferent OS.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.15894v1"
    },
    {
        "title": "Exact schedulability test for sporadic mixed-criticality real-time\n  systems using antichains and oracles",
        "authors": [
            "Simon Picard",
            "Antonio Paolillo",
            "Gilles Geeraerts",
            "Joël Goossens"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  This work addresses the problem of exact schedulability assessment in\nuniprocessor mixed-criticality real-time systems with sporadic task sets. We\nmodel the problem by means of a finite automaton that has to be explored in\norder to check for schedulability. To mitigate the state explosion problem, we\nprovide a generic algorithm which is parameterised by several techniques called\noracles and simulation relations. These techniques leverage results from the\nscheduling literature as \"plug-ins\" that make the algorithm more efficient in\npractice. Our approach achieves up to a 99.998% reduction in the search space\nrequired for exact schedulability testing, making it practical for a range of\ntask sets, up to 8 tasks or maximum periods of 350. This method enables to\nchallenge the pessimism of an existing schedulability test and to derive a new\ndynamic-priority scheduler, demonstrating its good performance. This is the\nfull version of an RTNS 2024 paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.18308v1"
    },
    {
        "title": "Fast and Efficient Memory Reclamation For Serverless MicroVMs",
        "authors": [
            "Orestis Lagkas Nikolos",
            "Chloe Alverti",
            "Stratos Psomadakis",
            "Georgios Goumas",
            "Nectarios Koziris"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Resource elasticity is one of the key defining characteristics of the\nFunction-as-a-Service (FaaS) serverless computing paradigm. In order to provide\nstrong multi-tenant isolation, FaaS providers commonly sandbox functions inside\nvirtual machines (VMs or microVMs). While compute resources assigned to\nVM-sandboxed functions can be seamlessly adjusted on the fly, memory elasticity\nremains challenging, especially when scaling down. State-of-the-art mechanisms\nfor VM memory elasticity suffer from increased reclaim latency when memory\nneeds to be released, compounded by CPU and memory bandwidth overheads. We\nidentify the obliviousness of the Linux memory manager to the virtually\nhotplugged memory as the key issue hindering hot-unplug performance, and design\nHotMem, a novel approach for fast and efficient VM memory hot(un)plug,\ntargeting VM-sandboxed serverless functions. Our key insight is that by\nsegregating virtually hotplugged memory regions from regular VM memory, we are\nable to bound the lifetimes of allocations within these regions thus enabling\ntheir fast and efficient reclamation. We implement HotMem in Linux v6.6 and our\nevaluation shows that it is an order of magnitude faster than state-of-practice\nto reclaim VM memory, while achieving the same P99 function latency with a\nmodel that statically over-provisions VMs.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.12893v1"
    },
    {
        "title": "Interference-free Operating System: A 6 Years' Experience in Mitigating\n  Cross-Core Interference in Linux",
        "authors": [
            "Zhaomeng Deng",
            "Ziqi Zhang",
            "Ding Li",
            "Yao Guo",
            "Yunfeng Ye",
            "Yuxin Ren",
            "Ning Jia",
            "Xinwei Hu"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Real-time operating systems employ spatial and temporal isolation to\nguarantee predictability and schedulability of real-time systems on multi-core\nprocessors. Any unbounded and uncontrolled cross-core performance interference\nposes a significant threat to system time safety. However, the current Linux\nkernel has a number of interference issues and represents a primary source of\ninterference. Unfortunately, existing research does not systematically and\ndeeply explore the cross-core performance interference issue within the OS\nitself.\n  This paper presents our industry practice for mitigating cross-core\nperformance interference in Linux over the past 6 years. We have fixed dozens\nof interference issues in different Linux subsystems. Compared to the version\nwithout our improvements, our enhancements reduce the worst-case jitter by a\nfactor of 8.7, resulting in a maximum 11.5x improvement over system\nschedulability. For the worst-case latency in the Core Flight System and the\nRobot Operating System 2, we achieve a 1.6x and 1.64x reduction over RT-Linux.\nBased on our development experience, we summarize the lessons we learned and\noffer our suggestions to system developers for systematically eliminating\ncross-core interference from the following aspects: task management, resource\nmanagement, and concurrency management. Most of our modifications have been\nmerged into Linux upstream and released in commercial distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.18104v1"
    },
    {
        "title": "Combining Type Checking and Formal Verification for Lightweight OS\n  Correctness",
        "authors": [
            "Ramla Ijaz",
            "Kevin Boos",
            "Lin Zhong"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  This paper reports our experience of providing lightweight correctness\nguarantees to an open-source Rust OS, Theseus. First, we report new\ndevelopments in intralingual design that leverage Rust's type system to enforce\nadditional invariants at compile time, trusting the Rust compiler. Second, we\ndevelop a hybrid approach that combines formal verification, type checking, and\ninformal reasoning, showing how the type system can assist in increasing the\nscope of formally verified invariants. By slightly lessening the strength of\ncorrectness guarantees, this hybrid approach substantially reduces the proof\neffort. We share our experience in applying this approach to the memory\nsubsystem and the 10 Gb Ethernet driver of Theseus, demonstrate its utility,\nand quantify its reduced proof effort.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.00248v1"
    },
    {
        "title": "Exploiting Application-to-Architecture Dependencies for Designing\n  Scalable OS",
        "authors": [
            "Yao Xiao",
            "Nikos Kanakaris",
            "Anzhe Cheng",
            "Chenzhong Yin",
            "Nesreen K. Ahmed",
            "Shahin Nazarian",
            "Andrei Irimia",
            "Paul Bogdan"
        ],
        "category": "cs.OS",
        "published_year": "2025",
        "summary": "  With the advent of hundreds of cores on a chip to accelerate applications,\nthe operating system (OS) needs to exploit the existing parallelism provided by\nthe underlying hardware resources to determine the right amount of processes to\nbe mapped on the multi-core systems. However, the existing OS is not scalable\nand is oblivious to applications. We address these issues by adopting a\nmulti-layer network representation of the dynamic application-to\nOS-to-architecture dependencies, namely the NetworkedOS. We adopt a\ncompile-time analysis and construct a network representing the dependencies\nbetween dynamic instructions translated from the applications and the kernel\nand services. We propose an overlapping partitioning scheme to detect the\nclusters or processes that can potentially run in parallel to be mapped onto\ncores while reducing the number of messages transferred. At run time, processes\nare mapped onto the multi-core systems, taking into consideration the process\naffinity. Our experimental results indicate that NetworkedOS achieves\nperformance improvement as high as 7.11x compared to Linux running on a\n128-core system and 2.01x to Barrelfish running on a 64-core system.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.00994v2"
    },
    {
        "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
        "authors": [
            "Shaobo Li",
            "Yirui Eric Zhou",
            "Hao Ren",
            "Jian Huang"
        ],
        "category": "cs.OS",
        "published_year": "2025",
        "summary": "  Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.04993v1"
    },
    {
        "title": "The Design and Architecture of the Microsoft Cluster Service -- A\n  Practical Approach to High-Availability and Scalability",
        "authors": [
            "Werner Vogels",
            "Dan Dumitriu",
            "Ken Birman",
            "Rod Gamache",
            "Mike Massa",
            "Rob Short",
            "John Vert",
            "Joe Barrera"
        ],
        "category": "cs.OS",
        "published_year": "1998",
        "summary": "  Microsoft Cluster Service (MSCS) extends the Win-dows NT operating system to\nsupport high-availability services. The goal is to offer an execution\nenvironment where off-the-shelf server applications can continue to operate,\neven in the presence of node failures. Later ver-sions of MSCS will provide\nscalability via a node and application management system that allows\napplications to scale to hundreds of nodes. This paper provides a de-tailed\ndescription of the MSCS architecture and the de-sign decisions that have driven\nthe implementation of the service. The paper also describes how some major\nappli-cations use the MSCS features, and describes features added to make it\neasier to implement and manage fault-tolerant applications on MSCS.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/9809006v1"
    },
    {
        "title": "A nested transaction mechanism for LOCUS",
        "authors": [
            "Erik T. Mueller",
            "Johanna D. Moore",
            "Gerald J. Popek"
        ],
        "category": "cs.OS",
        "published_year": "1998",
        "summary": "  A working implementation of nested transactions has been produced for LOCUS,\nan integrated distributed operating system which provides a high degree of\nnetwork transparency. Several aspects of our mechanism are novel. First, the\nmechanism allows a transaction to access objects directly without regard to the\nlocation of the object. Second, processes running on behalf of a single\ntransaction may be located at many sites. Thus there is no need to invoke a new\ntransaction to perform processing or access objects at a remote site. Third,\nunlike other environments, LOCUS allows replication of data objects at more\nthan one site in the network, and this capability is incorporated into the\ntransaction mechanism. If the copy of an object that is currently being\naccessed becomes unavailable, it is possible to continue work by using another\none of the replicated copies. Finally, an efficient orphan removal algorithm is\npresented, and the problem of providing continued operation during network\npartitions is addressed in detail.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/9812011v1"
    },
    {
        "title": "Perpetual Adaptation of Software to Hardware: An Extensible Architecture\n  for Providing Code Optimization as a Central System Service",
        "authors": [
            "Thomas Kistler",
            "Michael Franz"
        ],
        "category": "cs.OS",
        "published_year": "1999",
        "summary": "  We present an open architecture for just-in-time code generation and dynamic\ncode optimization that is flexible, customizable, and extensible. While\nprevious research has primarily investigated functional aspects of such a\nsystem, architectural aspects have so far remained unexplored. In this paper,\nwe argue that these properties are important to generate optimal code for a\nvariety of hardware architectures and different processor generations within\nprocessor families. These properties are also important to make system-level\ncode generation useful in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/9903014v1"
    },
    {
        "title": "The combinatorics of resource sharing",
        "authors": [
            "V. C. Barbosa"
        ],
        "category": "cs.OS",
        "published_year": "2003",
        "summary": "  We discuss general models of resource-sharing computations, with emphasis on\nthe combinatorial structures and concepts that underlie the various deadlock\nmodels that have been proposed, the design of algorithms and deadlock-handling\npolicies, and concurrency issues. These structures are mostly graph-theoretic\nin nature, or partially ordered sets for the establishment of priorities among\nprocesses and acquisition orders on resources. We also discuss graph-coloring\nconcepts as they relate to resource sharing.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0309044v1"
    },
    {
        "title": "Predictable Software -- A Shortcut to Dependable Computing ?",
        "authors": [
            "George Candea"
        ],
        "category": "cs.OS",
        "published_year": "2004",
        "summary": "  Many dependability techniques expect certain behaviors from the underlying\nsubsystems and fail in chaotic ways if these expectations are not met. Under\nexpected circumstances, however, software tends to work quite well. This paper\nsuggests that, instead of fixing elusive bugs or rewriting software, we improve\nthe predictability of conditions faced by our programs. This approach might be\na cheaper and faster way to improve dependability of software. After\nidentifying some of the common triggers of unpredictability, the paper\ndescribes three engineering principles that hold promise in combating\nunpredictability, suggests a way to benchmark predictability, and outlines a\nbrief research agenda.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0403013v1"
    },
    {
        "title": "Microreboot -- A Technique for Cheap Recovery",
        "authors": [
            "George Candea",
            "Shinichi Kawamoto",
            "Yuichi Fujiki",
            "Greg Friedman",
            "Armando Fox"
        ],
        "category": "cs.OS",
        "published_year": "2004",
        "summary": "  A significant fraction of software failures in large-scale Internet systems\nare cured by rebooting, even when the exact failure causes are unknown.\nHowever, rebooting can be expensive, causing nontrivial service disruption or\ndowntime even when clusters and failover are employed. In this work we separate\nprocess recovery from data recovery to enable microrebooting -- a fine-grain\ntechnique for surgically recovering faulty application components, without\ndisturbing the rest of the application.\n  We evaluate microrebooting in an Internet auction system running on an\napplication server. Microreboots recover most of the same failures as full\nreboots, but do so an order of magnitude faster and result in an order of\nmagnitude savings in lost work. This cheap form of recovery engenders a new\napproach to high availability: microreboots can be employed at the slightest\nhint of failure, prior to node failover in multi-node clusters, even when\nmistakes in failure detection are likely; failure and recovery can be masked\nfrom end users through transparent call-level retries; and systems can be\nrejuvenated by parts, without ever being shut down.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0406005v3"
    },
    {
        "title": "Securing Data in Storage: A Review of Current Research",
        "authors": [
            "Paul Stanton"
        ],
        "category": "cs.OS",
        "published_year": "2004",
        "summary": "  Protecting data from malicious computer users continues to grow in\nimportance. Whether preventing unauthorized access to personal photographs,\nensuring compliance with federal regulations, or ensuring the integrity of\ncorporate secrets, all applications require increased security to protect data\nfrom talented intruders. Specifically, as more and more files are preserved on\ndisk the requirement to provide secure storage has increased in importance.\nThis paper presents a survey of techniques for securely storing data, including\ntheoretical approaches, prototype systems, and existing systems currently\navailable. Due to the wide variety of potential solutions available and the\nvariety of techniques to arrive at a particular solution, it is important to\nreview the entire field prior to selecting an implementation that satisfies\nparticular requirements. This paper provides an overview of the prominent\ncharacteristics of several systems to provide a foundation for making an\ninformed decision. Initially, the paper establishes a set of criteria for\nevaluating a storage solution based on confidentiality, integrity,\navailability, and performance. Then, using these criteria, the paper explains\nthe relevant characteristics of select storage systems and provides a\ncomparison of the major differences.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0409034v1"
    },
    {
        "title": "A Shared Write-protected Root Filesystem for a Group of Networked\n  Clients",
        "authors": [
            "Ignatios Souvatzis"
        ],
        "category": "cs.OS",
        "published_year": "2004",
        "summary": "  A method to boot a cluster of diskless network clients from a single\nwrite-protected NFS root file system is shown. The problems encountered when\nfirst implementing the setup and their solution are discussed. Finally, the\nsetup is briefly compared to using a kernel-embedded root file system.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0410007v2"
    },
    {
        "title": "Checkbochs: Use Hardware to Check Software",
        "authors": [
            "Sorav Bansal"
        ],
        "category": "cs.OS",
        "published_year": "2006",
        "summary": "  In this paper, we present a system called Checkbochs, a machine simulator\nthat checks rules about its guest operating system and applications at the\nhardware level. The properties to be checked can be implemented as `plugins' in\nthe Checkbochs simulator. Some of the properties that were checked using\nCheckbochs include null-pointer checks, format-string vulnerabilities,\nuser/kernel pointer checks, and race-conditions. On implementing these checks,\nwe were able to uncover previously-unknown bugs in widely used Linux\ndistributions. We also tested our tools on undergraduate coursework, and found\nnumerous bugs.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0601068v1"
    },
    {
        "title": "Virtualization: A double-edged sword",
        "authors": [
            "Joachim J. Wlodarz"
        ],
        "category": "cs.OS",
        "published_year": "2007",
        "summary": "  Virtualization became recently a hot topic once again, after being dormant\nfor more than twenty years. In the meantime, it has been almost forgotten, that\nvirtual machines are not so perfect isolating environments as it seems, when\nlooking at the principles. These lessons were already learnt earlier when the\nfirst virtualized systems have been exposed to real life usage.\n  Contemporary virtualization software enables instant creation and destruction\nof virtual machines on a host, live migration from one host to another,\nexecution history manipulation, etc. These features are very useful in\npractice, but also causing headaches among security specialists, especially in\ncurrent hostile network environments.\n  In the present contribution we discuss the principles, potential benefits and\nrisks of virtualization in a deja vu perspective, related to previous\nexperiences with virtualization in the mainframe era.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.2786v1"
    },
    {
        "title": "Practical Multiwriter Lock-Free Queues for \"Hard Real-Time\" Systems\n  without CAS",
        "authors": [
            "Jeremy Lee"
        ],
        "category": "cs.OS",
        "published_year": "2007",
        "summary": "  FIFO queues with a single reader and writer can be insufficient for \"hard\nreal-time\" systems where interrupt handlers require wait-free guarantees when\nwriting to message queues. We present an algorithm which elegantly and\npractically solves this problem on small processors that are often found in\nembedded systems. The algorithm does not require special CPU instructions (such\nas atomic CAS), and therefore is more robust than many existing methods that\nsuffer the ABA problem associated with swing pointers. The algorithm gives\n\"first-in, almost first-out\" guarantees under pathological interrupt\nconditions, which manifests as arbitrary \"shoving\" among nearly-simultaneous\narrivals at the end of the queue.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.4558v1"
    },
    {
        "title": "Telex: Principled System Support for Write-Sharing in Collaborative\n  Applications",
        "authors": [
            "Lamia Benmouffok",
            "Jean-Michel Busca",
            "Joan Manuel Marquès",
            "Marc Shapiro",
            "Pierre Sutra",
            "Georgios Tsoukalas"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  The Telex system is designed for sharing mutable data in a distributed\nenvironment, particularly for collaborative applications. Users operate on\ntheir local, persistent replica of shared documents; they can work disconnected\nand suffer no network latency. The Telex approach to detect and correct\nconflicts is application independent, based on an action-constraint graph (ACG)\nthat summarises the concurrency semantics of applications. The ACG is stored\nefficiently in a multilog structure that eliminates contention and is optimised\nfor locality. Telex supports multiple applications and multi-document updates.\nThe Telex system clearly separates system logic (which includes replication,\nviews, undo, security, consistency, conflicts, and commitment) from application\nlogic. An example application is a shared calendar for managing multi-user\nmeetings; the system detects meeting conflicts and resolves them consistently.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.4680v3"
    },
    {
        "title": "Local Read-Write Operations in Sensor Networks",
        "authors": [
            "Ted Herman",
            "Morten Mjelde"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  Designing protocols and formulating convenient programming units of\nabstraction for sensor networks is challenging due to communication errors and\nplatform constraints. This paper investigates properties and implementation\nreliability for a \\emph{local read-write} abstraction. Local read-write is\ninspired by the class of read-modify-write operations defined for shared-memory\nmultiprocessor architectures. The class of read-modify-write operations is\nimportant in solving consensus and related synchronization problems for\nconcurrency control. Local read-write is shown to be an atomic abstraction for\nsynchronizing neighborhood states in sensor networks. The paper compares local\nread-write to similar lightweight operations in wireless sensor networks, such\nas read-all, write-all, and a transaction-based abstraction: for some\noptimistic scenarios, local read-write is a more efficient neighborhood\noperation. A partial implementation is described, which shows that three\noutcomes characterize operation response: success, failure, and cancel. A\nfailure response indicates possible inconsistency for the operation result,\nwhich is the result of a timeout event at the operation's initiator. The paper\npresents experimental results on operation performance with different timeout\nvalues and situations of no contention, with some tests also on various\nneighborhood sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.1768v1"
    },
    {
        "title": "Interface Matching and Combining Techniques for Services Integration",
        "authors": [
            "Frédéric Le Mouël",
            "Noha Ibrahim",
            "Stéphane Frénot"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  The development of many highly dynamic environments, like pervasive\nenvironments, introduces the possibility to use geographically close-related\nservices. Dynamically integrating and unintegrating these services in running\napplications is a key challenge for this use. In this article, we classify\nservice integration issues according to interfaces exported by services and\ninternal combining techniques. We also propose a contextual integration\nservice, IntegServ, and an interface, Integrable, for developing services.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.3933v1"
    },
    {
        "title": "A Distributed and Deterministic TDMA Algorithm for\n  Write-All-With-Collision Model",
        "authors": [
            "Mahesh Arumugam"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  Several self-stabilizing time division multiple access (TDMA) algorithms are\nproposed for sensor networks. In addition to providing a collision-free\ncommunication service, such algorithms enable the transformation of programs\nwritten in abstract models considered in distributed computing literature into\na model consistent with sensor networks, i.e., write all with collision (WAC)\nmodel. Existing TDMA slot assignment algorithms have one or more of the\nfollowing properties: (i) compute slots using a randomized algorithm, (ii)\nassume that the topology is known upfront, and/or (iii) assign slots\nsequentially. If these algorithms are used to transform abstract programs into\nprograms in WAC model then the transformed programs are probabilistically\ncorrect, do not allow the addition of new nodes, and/or converge in a\nsequential fashion. In this paper, we propose a self-stabilizing deterministic\nTDMA algorithm where a sensor is aware of only its neighbors. We show that the\nslots are assigned to the sensors in a concurrent fashion and starting from\narbitrary initial states, the algorithm converges to states where\ncollision-free communication among the sensors is restored. Moreover, this\nalgorithm facilitates the transformation of abstract programs into programs in\nWAC model that are deterministically correct.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.0920v1"
    },
    {
        "title": "A Conceivable Origin of Machine Consciousness in the IDLE process",
        "authors": [
            "Norbert Bátfai"
        ],
        "category": "cs.OS",
        "published_year": "2009",
        "summary": "  In this short paper, we would like to call professional community's attention\nto a daring idea that is surely unhelpful, but is exciting for programmers and\nanyway conflicts with the trend of energy consumption in computer systems.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.5064v1"
    },
    {
        "title": "Temporal Debugging using URDB",
        "authors": [
            "Ana Maria Visan",
            "Artem Polyakov",
            "Praveen S. Solanki",
            "Kapil Arya",
            "Tyler Denniston",
            "Gene Cooperman"
        ],
        "category": "cs.OS",
        "published_year": "2009",
        "summary": "  A new style of temporal debugging is proposed. The new URDB debugger can\nemploy such techniques as temporal search for finding an underlying fault that\nis causing a bug. This improves on the standard iterative debugging style,\nwhich iteratively re-executes a program under debugger control in the search\nfor the underlying fault. URDB acts as a meta-debugger, with current support\nfor four widely used debuggers: gdb, MATLAB, python, and perl. Support for a\nnew debugger can be added in a few hours. Among its points of novelty are: (i)\nthe first reversible debuggers for MATLAB, python, and perl; (ii) support for\ntoday's multi-core architectures; (iii) reversible debugging of multi-process\nand distributed computations; and (iv) temporal search on changes in program\nexpressions. URDB gains its reversibility and temporal abilities through the\nfast checkpoint-restart capability of DMTCP (Distributed MultiThreaded\nCheckPointing). The recently enhanced DMTCP also adds ptrace support, enabling\none to freeze, migrate, and replicate debugging sessions.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.5046v1"
    },
    {
        "title": "On the stability of two-chunk file-sharing systems",
        "authors": [
            "Ilkka Norros",
            "Hannu Reittu",
            "Timo Eirola"
        ],
        "category": "cs.OS",
        "published_year": "2009",
        "summary": "  We consider five different peer-to-peer file sharing systems with two chunks,\nwith the aim of finding chunk selection algorithms that have provably stable\nperformance with any input rate and assuming non-altruistic peers who leave the\nsystem immediately after downloading the second chunk. We show that many\nalgorithms that first looked promising lead to unstable or oscillating\nbehavior. However, we end up with a system with desirable properties. Most of\nour rigorous results concern the corresponding deterministic large system\nlimits, but in two simplest cases we provide proofs for the stochastic systems\nalso.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.5577v1"
    },
    {
        "title": "Process Description of COM Object Life Cycle",
        "authors": [
            "Emil Vassev"
        ],
        "category": "cs.OS",
        "published_year": "2009",
        "summary": "  The objective of this article is to provide for the reader a basic\ndescription of all the steps involved in the COM object life-cycle process. COM\nis a software technology and process performer. The first section briefly\nintroduces the Component Object Model (COM), considering the process of the COM\nobject life cycle as the baseline of all COM issues. The second part describes\nin detail the basic steps of the process - client request, server location,\nobject creation, interaction, and disconnection. A brief description is given\nfor the components involved in each step. Finally, the third section provides a\nbrief conclusion summarizing all the process steps.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.4062v1"
    },
    {
        "title": "On the Design of an Optimal Multiprocessor Real-Time Scheduling\n  Algorithm under Practical Considerations (Extended Version)",
        "authors": [
            "Shelby Funk",
            "Vincent Nelis",
            "Joel Goossens",
            "Dragomir Milojevic",
            "Geoffrey Nelissen"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  This research addresses the multiprocessor scheduling problem of hard\nreal-time systems, and it especially focuses on optimal and global schedulers\nwhen practical constraints are taken into account. First, we propose an\nimprovement of the optimal algorithm BF. We formally prove that our adaptation\nis (i) optimal, i.e., it always generates a feasible schedule as long as such a\nschedule exists, and (ii) valid, i.e., it complies with the all the\nrequirements. We also show that it outperforms BF by providing a computing\ncomplexity of O(n), where n is the number of tasks to be scheduled. Next, we\npropose a schedulability analysis which indicates a priori whether the\nreal-time application can be scheduled by our improvement of BF without missing\nany deadline. This analysis is, to the best of our knowledge, the first such\ntest for multiprocessors that takes into account all the main overheads\ngenerated by the Operating System.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.4115v3"
    },
    {
        "title": "Determinating Timing Channels in Compute Clouds",
        "authors": [
            "Amittai Aviram",
            "Sen Hu",
            "Bryan Ford",
            "Ramakrishna Gummadi"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  Timing side-channels represent an insidious security challenge for cloud\ncomputing, because: (a) massive parallelism in the cloud makes timing channels\npervasive and hard to control; (b) timing channels enable one customer to steal\ninformation from another without leaving a trail or raising alarms; (c) only\nthe cloud provider can feasibly detect and report such attacks, but the\nprovider's incentives are not to; and (d) resource partitioning schemes for\ntiming channel control undermine statistical sharing efficiency, and, with it,\nthe cloud computing business model. We propose a new approach to timing channel\ncontrol, using provider-enforced deterministic execution instead of resource\npartitioning to eliminate timing channels within a shared cloud domain.\nProvider-enforced determinism prevents execution timing from affecting the\nresults of a compute task, however large or parallel, ensuring that a task's\noutputs leak no timing information apart from explicit timing inputs and total\ncompute duration. Experiments with a prototype OS for deterministic cloud\ncomputing suggest that such an approach may be practical and efficient. The OS\nsupports deterministic versions of familiar APIs such as processes, threads,\nshared memory, and file systems, and runs coarse-grained parallel tasks as\nefficiently and scalably as current timing channel-ridden systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.5303v2"
    },
    {
        "title": "Efficient System-Enforced Deterministic Parallelism",
        "authors": [
            "Amittai Aviram",
            "Shu-Chun Weng",
            "Sen Hu",
            "Bryan Ford"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  Deterministic execution offers many benefits for debugging, fault tolerance,\nand security. Running parallel programs deterministically is usually difficult\nand costly, however - especially if we desire system-enforced determinism,\nensuring precise repeatability of arbitrarily buggy or malicious software.\nDeterminator is a novel operating system that enforces determinism on both\nmultithreaded and multi-process computations. Determinator's kernel provides\nonly single-threaded, \"shared-nothing\" address spaces interacting via\ndeterministic synchronization. An untrusted user-level runtime uses distributed\ncomputing techniques to emulate familiar abstractions such as Unix processes,\nfile systems, and shared memory multithreading. The system runs parallel\napplications deterministically both on multicore PCs and across nodes in a\ncluster. Coarse-grained parallel benchmarks perform and scale comparably to -\nsometimes better than - conventional systems, though determinism is costly for\nfine-grained parallel applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.3450v1"
    },
    {
        "title": "Dynamic and Transparent Analysis of Commodity Production Systems",
        "authors": [
            "Aristide Fattori",
            "Roberto Paleari",
            "Lorenzo Martignoni",
            "Mattia Monga"
        ],
        "category": "cs.OS",
        "published_year": "2010",
        "summary": "  We propose a framework that provides a programming interface to perform\ncomplex dynamic system-level analyses of deployed production systems. By\nleveraging hardware support for virtualization available nowadays on all\ncommodity machines, our framework is completely transparent to the system under\nanalysis and it guarantees isolation of the analysis tools running on its top.\nThus, the internals of the kernel of the running system needs not to be\nmodified and the whole platform runs unaware of the framework. Moreover, errors\nin the analysis tools do not affect the running system and the framework. This\nis accomplished by installing a minimalistic virtual machine monitor and\nmigrating the system, as it runs, into a virtual machine. In order to\ndemonstrate the potentials of our framework we developed an interactive kernel\ndebugger, nicknamed HyperDbg. HyperDbg can be used to debug any critical kernel\ncomponent, and even to single step the execution of exception and interrupt\nhandlers.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.5845v1"
    },
    {
        "title": "Transparent Programming of Heterogeneous Smartphones for Sensing",
        "authors": [
            "Felix Xiaozhu Lin",
            "Zhen Wang",
            "Robert LiKamWa",
            "Lin Zhong"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  Sensing on smartphones is known to be power-hungry. It has been shown that\nthis problem can be solved by adding an ultra low-power processor to execute\nsimple, frequent sensor data processing. While very effective in saving energy,\nthis resulting heterogeneous, distributed architecture poses a significant\nchallenge to application development.\n  We present Reflex, a suite of runtime and compilation techniques to conceal\nthe heterogeneous, distributed nature from developers. The Reflex automatically\ntransforms the developer's code for distributed execution with the help of the\nReflex runtime. To create a unified system illusion, Reflex features a novel\nsoftware distributed shared memory (DSM) design that leverages the extreme\narchitectural asymmetry between the low-power processor and the powerful\ncentral processor to achieve both energy efficiency and performance.\n  We report a complete realization of Reflex for heterogeneous smartphones with\nMaemo/Linux as the central kernel. Using a tri-processor hardware prototype and\nsensing applications reported in recent literature, we evaluate the Reflex\nrealization for programming transparency, energy efficiency, and performance.\nWe show that Reflex supports a programming style that is very close to\ncontemporary smartphone programming. It allows existing sensing applications to\nbe ported with minor source code changes. Reflex reduces the system power in\nsensing by up to 83%, and its runtime system only consumes 10% local memory on\na typical ultra-low power processor.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.2348v1"
    },
    {
        "title": "Deterministic Real-time Thread Scheduling",
        "authors": [
            "Heechul Yun",
            "Cheolgi Kim",
            "Lui Sha"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  Race condition is a timing sensitive problem. A significant source of timing\nvariation comes from nondeterministic hardware interactions such as cache\nmisses. While data race detectors and model checkers can check races, the\nenormous state space of complex software makes it difficult to identify all of\nthe races and those residual implementation errors still remain a big\nchallenge. In this paper, we propose deterministic real-time scheduling methods\nto address scheduling nondeterminism in uniprocessor systems. The main idea is\nto use timing insensitive deterministic events, e.g, an instruction counter, in\nconjunction with a real-time clock to schedule threads. By introducing the\nconcept of Worst Case Executable Instructions (WCEI), we guarantee both\ndeterminism and real-time performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.2110v1"
    },
    {
        "title": "User Mode Memory Page Allocation: A Silver Bullet For Memory Allocation?",
        "authors": [
            "Niall Douglas"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  This paper proposes a novel solution: the elimination of paged virtual memory\nand partial outsourcing of memory page allocation and manipulation from the\noperating system kernel into the individual process' user space - a user mode\npage allocator - which allows an application to have direct, bare metal access\nto the page mappings used by the hardware Memory Management Unit (MMU) for its\npart of the overall address space. A user mode page allocator based emulation\nof the mmap() abstraction layer of dlmalloc is then benchmarked against the\ntraditional kernel mode implemented mmap() in a series of synthetic Monte-Carlo\nand real world application settings. Given the superb synthetic and positive\nreal world results from the profiling conducted, this paper proposes that with\nproper operating system and API support one could gain a further order higher\nperformance again while keeping allocator performance invariant to the amount\nof memory being allocated or freed i.e. a 100x performance improvement or more\nin some common use cases. It is rare that through a simple and easy to\nimplement API and operating system structure change one can gain a Silver\nBullet with the potential for a second one.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.1811v1"
    },
    {
        "title": "User Mode Memory Page Management: An old idea applied anew to the memory\n  wall problem",
        "authors": [
            "Niall Douglas"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  It is often said that one of the biggest limitations on computer performance\nis memory bandwidth (i.e.\"the memory wall problem\"). In this position paper, I\nargue that if historical trends in computing evolution (where growth in\navailable capacity is exponential and reduction in its access latencies is\nlinear) continue as they have, then this view is wrong - in fact we ought to be\nconcentrating on reducing whole system memory access latencies wherever\npossible, and by \"whole system\" I mean that we ought to look at how software\ncan be unnecessarily wasteful with memory bandwidth due to legacy design\ndecisions. To this end I conduct a feasibility study to determine whether we\nought to virtualise the MMU for each application process such that it has\ndirect access to its own MMU page tables and the memory allocated to a process\nis managed exclusively by the process and not the kernel. I find under typical\nconditions that nearly scale invariant performance to memory allocation size is\npossible such that hundreds of megabytes of memory can be allocated, relocated,\nswapped and deallocated in almost the same time as kilobytes (e.g. allocating\n8Mb is 10x quicker under this experimental allocator than a conventional\nallocator, and resizing a 128Kb block to 256Kb block is 4.5x faster). I find\nthat first time page access latencies are improved tenfold; moreover, because\nthe kernel page fault handler is never called, the lack of cache pollution\nimproves whole application memory access latencies increasing performance by up\nto 2x. Finally, I try binary patching existing applications to use the\nexperimental allocation technique, finding almost universal performance\nimprovements without having to recompile these applications to make better use\nof the new facilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.1815v1"
    },
    {
        "title": "Evolution of a Modular Software Network",
        "authors": [
            "Miguel A. Fortuna",
            "Juan A. Bonachela",
            "Simon A. Levin"
        ],
        "category": "cs.OS",
        "published_year": "2011",
        "summary": "  \"Evolution behaves like a tinkerer\" (Francois Jacob, Science, 1977). Software\nsystems provide a unique opportunity to understand biological processes using\nconcepts from network theory. The Debian GNU/Linux operating system allows us\nto explore the evolution of a complex network in a novel way. The modular\ndesign detected during its growth is based on the reuse of existing code in\norder to minimize costs during programming. The increase of modularity\nexperienced by the system over time has not counterbalanced the increase in\nincompatibilities between software packages within modules. This negative\neffect is far from being a failure of design. A random process of package\ninstallation shows that the higher the modularity the larger the fraction of\npackages working properly in a local computer. The decrease in the relative\nnumber of conflicts between packages from different modules avoids a failure in\nthe functionality of one package spreading throughout the entire system. Some\npotential analogies with the evolutionary and ecological processes determining\nthe structure of ecological networks of interacting species are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.5251v1"
    },
    {
        "title": "On Benchmarking Embedded Linux Flash File Systems",
        "authors": [
            "Pierre Olivier",
            "Jalil Boukhobza",
            "Eric Senn"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Due to its attractive characteristics in terms of performance, weight and\npower consumption, NAND flash memory became the main non volatile memory (NVM)\nin embedded systems. Those NVMs also present some specific\ncharacteristics/constraints: good but asymmetric I/O performance, limited\nlifetime, write/erase granularity asymmetry, etc. Those peculiarities are\neither managed in hardware for flash disks (SSDs, SD cards, USB sticks, etc.)\nor in software for raw embedded flash chips. When managed in software, flash\nalgorithms and structures are implemented in a specific flash file system\n(FFS). In this paper, we present a performance study of the most widely used\nFFSs in embedded Linux: JFFS2, UBIFS,and YAFFS. We show some very particular\nbehaviors and large performance disparities for tested FFS operations such as\nmounting, copying, and searching file trees, compression, etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.6391v1"
    },
    {
        "title": "Building Resilient Cloud Over Unreliable Commodity Infrastructure",
        "authors": [
            "Piyus Kedia",
            "Sorav Bansal",
            "Deepak Deshpande",
            "Sreekanth Iyer"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Cloud Computing has emerged as a successful computing paradigm for\nefficiently utilizing managed compute infrastructure such as high speed\nrack-mounted servers, connected with high speed networking, and reliable\nstorage. Usually such infrastructure is dedicated, physically secured and has\nreliable power and networking infrastructure. However, much of our idle compute\ncapacity is present in unmanaged infrastructure like idle desktops, lab\nmachines, physically distant server machines, and laptops. We present a scheme\nto utilize this idle compute capacity on a best-effort basis and provide high\navailability even in face of failure of individual components or facilities.\n  We run virtual machines on the commodity infrastructure and present a cloud\ninterface to our end users. The primary challenge is to maintain availability\nin the presence of node failures, network failures, and power failures. We run\nmultiple copies of a Virtual Machine (VM) redundantly on geographically\ndispersed physical machines to achieve availability. If one of the running\ncopies of a VM fails, we seamlessly switchover to another running copy. We use\nVirtual Machine Record/Replay capability to implement this redundancy and\nswitchover. In current progress, we have implemented VM Record/Replay for\nuniprocessor machines over Linux/KVM and are currently working on VM\nRecord/Replay on shared-memory multiprocessor machines. We report initial\nexperimental results based on our implementation.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.6406v1"
    },
    {
        "title": "JooFlux: Hijacking Java 7 InvokeDynamic To Support Live Code\n  Modifications",
        "authors": [
            "Julien Ponge",
            "Frédéric Le Mouël"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Changing functional and non-functional software implementation at runtime is\nuseful and even sometimes critical both in development and production\nenvironments. JooFlux is a JVM agent that allows both the dynamic replacement\nof method implementations and the application of aspect advices. It works by\ndoing bytecode transformation to take advantage of the new invokedynamic\ninstruction added in Java SE 7 to help implementing dynamic languages for the\nJVM. JooFlux can be managed using a JMX agent so as to operate dynamic\nmodifications at runtime, without resorting to a dedicated domain-specific\nlanguage. We compared JooFlux with existing AOP platforms and dynamic\nlanguages. Results demonstrate that JooFlux performances are close to the Java\nones --- with most of the time a marginal overhead, and sometimes a gain ---\nwhere AOP platforms and dynamic languages present significant overheads. This\npaves the way for interesting future evolutions and applications of JooFlux.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.1039v1"
    },
    {
        "title": "An Insight View of Kernel Visual Debugger in System Boot up",
        "authors": [
            "Mohamed Farag"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  For many years, developers could not figure out the mystery of OS kernels.\nThe main source of this mystery is the interaction between operating systems\nand hardware while system's boot up and kernel initialization. In addition,\nmany operating system kernels differ in their behavior toward many situations.\nFor instance, kernels act differently in racing conditions, kernel\ninitialization and process scheduling. For such operations, kernel debuggers\nwere designed to help in tracing kernel behavior and solving many kernel bugs.\nThe importance of kernel debuggers is not limited to kernel code tracing but\nalso, they can be used in verification and performance comparisons. However,\ndevelopers had to be aware of debugger commands thus introducing some\ndifficulties to non-expert programmers. Later, several visual kernel debuggers\nwere presented to make it easier for programmers to trace their kernel code and\nanalyze kernel behavior. Nowadays, several kernel debuggers exist for solving\nthis mystery but only very few support line-by-line debugging at run-time. In\nthis paper, a generic approach for operating system source code debugging in\ngraphical mode with line-by-line tracing support is proposed. In the context of\nthis approach, system boot up and evaluation of two operating system schedulers\nfrom several points of views will be discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.4839v1"
    },
    {
        "title": "Automatic Verification of Message-Based Device Drivers",
        "authors": [
            "Sidney Amani",
            "Peter Chubb",
            "Alastair F. Donaldson",
            "Alexander Legg",
            "Leonid Ryzhyk",
            "Yanjin Zhu"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  We develop a practical solution to the problem of automatic verification of\nthe interface between device drivers and the OS. Our solution relies on a\ncombination of improved driver architecture and verification tools. It supports\ndrivers written in C and can be implemented in any existing OS, which sets it\napart from previous proposals for verification-friendly drivers. Our\nLinux-based evaluation shows that this methodology amplifies the power of\nexisting verification tools in detecting driver bugs, making it possible to\nverify properties beyond the reach of traditional techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.6185v1"
    },
    {
        "title": "Schedulability Analysis of Distributed Real-Time Applications under\n  Dependence and Several Latency Constraints",
        "authors": [
            "Omar Kermia"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  This paper focuses on the analysis of real-time non preemptive multiprocessor\nscheduling with precedence and several latency constraints. It aims to specify\na schedulability condition which enables a designer to check a priori -without\nexecuting or simulating- if its scheduling of tasks will hold the precedences\nbetween tasks as well as several latency constraints imposed on determined\npairs of tasks. It is shown that the required analysis is closely linked to the\ntopological structure of the application graph. More precisely, it depends on\nthe configuration of tasks paths subject to latency constraints. As a result of\nthe study, a sufficient schedulability condition is introduced for precedences\nand latency constraints in the hardest configuration in term of complexity with\nan optimal number of processors in term of applications parallelism. In\naddition, the proposed conditions provides a practical lower bounds for general\ncases. Performances results and comparisons with an optimal approach\ndemonstrate the effectiveness of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.4800v1"
    },
    {
        "title": "Flashmon V2: Monitoring Raw NAND Flash Memory I/O Requests on Embedded\n  Linux",
        "authors": [
            "Pierre Olivier",
            "Jalil Boukhobza",
            "Eric Senn"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  This paper presents Flashmon version 2, a tool for monitoring embedded Linux\nNAND flash memory I/O requests. It is designed for embedded boards based\ndevices containing raw flash chips. Flashmon is a kernel module and stands for\n\"flash monitor\". It traces flash I/O by placing kernel probes at the NAND\ndriver level. It allows tracing at runtime the 3 main flash operations: page\nreads / writes and block erasures. Flashmon is (1) generic as it was\nsuccessfully tested on the three most widely used flash file systems that are\nJFFS2, UBIFS and YAFFS, and several NAND chip models. Moreover, it is (2) non\nintrusive, (3) has a controllable memory footprint, and (4) exhibits a low\noverhead (<6%) on the traced system. Finally, it is (5) simple to integrate and\nused as a standalone module or as a built-in function / module in existing\nkernel sources. Monitoring flash memory operations allows a better\nunderstanding of existing flash management systems by studying and analyzing\ntheir behavior. Moreover it is useful in development phase for prototyping and\nvalidating new solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.1714v1"
    },
    {
        "title": "Impacting the bioscience progress by backporting software for Bio-Linux",
        "authors": [
            "Sasa Paporovic"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  In year 2006 Bio-Linux with the work of Tim Booth and team gives its rising\nand provide an operating system that was and still specialized in providing a\nbioinformatic specific software environment for the working needs in this\ncorner of bioscience. It is shown that Bio-Linux is affected by a 2 year\nrelease cycle and with this the final releases of Bio-Linux will not have the\nlatest bioinformatic software on board. The paper shows how to get around this\nhuge time gap and bring new software for Bio-Linux on board through a process\nthat is called backporting. A summary of within the work to this paper just\nbackported bioinformatic tools is given. A describtion of a workflow for\ncontinuously integration of the newest bioinformatic tools gives an outlook to\nfurther concrete planned developments and the influence of speeding up\nscientific progress.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.1588v1"
    },
    {
        "title": "Impact of Limpware on HDFS: A Probabilistic Estimation",
        "authors": [
            "Thanh Do",
            "Haryadi S. Gunawi"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  With the advent of cloud computing, thousands of machines are connected and\nmanaged collectively. This era is confronted with a new challenge: performance\nvariability, primarily caused by large-scale management issues such as hardware\nfailures, software bugs, and configuration mistakes. In our previous work we\nhighlighted one overlooked cause: limpware - hardware whose performance\ndegrades significantly compared to its specification. We showed that limpware\ncan cause severe impact in current scale-out systems. In this report, we\nquantify how often these scenarios happen in Hadoop Distributed File System.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.3322v1"
    },
    {
        "title": "Managing NymBoxes for Identity and Tracking Protection",
        "authors": [
            "David Isaac Wolinsky",
            "Bryan Ford"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Despite the attempts of well-designed anonymous communication tools to\nprotect users from tracking or identification, flaws in surrounding software\n(such as web browsers) and mistakes in configuration may leak the user's\nidentity. We introduce Nymix, an anonymity-centric operating system\narchitecture designed \"top-to-bottom\" to strengthen identity- and\ntracking-protection. Nymix's core contribution is OS support for nym-browsing:\nindependent, parallel, and ephemeral web sessions. Each web session, or\npseudonym, runs in a unique virtual machine (VM) instance evolving from a\ncommon base state with support for long-lived sessions which can be anonymously\nstored to the cloud, avoiding de-anonymization despite potential confiscation\nor theft. Nymix allows a user to safely browse the Web using various different\ntransports simultaneously through a pluggable communication model that supports\nTor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600\nMB per nymbox and loads within 15 to 25 seconds.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.3665v2"
    },
    {
        "title": "Transparent Checkpoint-Restart over InfiniBand",
        "authors": [
            "Jiajun Cao",
            "Gregory Kerr",
            "Kapil Arya",
            "Gene Cooperman"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  InfiniBand is widely used for low-latency, high-throughput cluster computing.\nSaving the state of the InfiniBand network as part of distributed checkpointing\nhas been a long-standing challenge for researchers. Because of a lack of a\nsolution, typical MPI implementations have included custom checkpoint-restart\nservices that \"tear down\" the network, checkpoint each node as if the node were\na standalone computer, and then re-connect the network again. We present the\nfirst example of transparent, system-initiated checkpoint-restart that directly\nsupports InfiniBand. The new approach is independent of any particular Linux\nkernel, thus simplifying the current practice of using a kernel-based module,\nsuch as BLCR. This direct approach results in checkpoints that are found to be\nfaster than with the use of a checkpoint-restart service. The generality of\nthis approach is shown not only by checkpointing an MPI computation, but also a\nnative UPC computation (Berkeley Unified Parallel C), which does not use MPI.\nScalability is shown by checkpointing 2,048 MPI processes across 128 nodes\n(with 16 cores per node). In addition, a cost-effective debugging approach is\nalso enabled, in which a checkpoint image from an InfiniBand-based production\ncluster is copied to a local Ethernet-based cluster, where it can be restarted\nand an interactive debugger can be attached to it. This work is based on a\nplugin that extends the DMTCP (Distributed MultiThreaded CheckPointing)\ncheckpoint-restart package.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.3938v3"
    },
    {
        "title": "Evaluating Dynamic File Striping For Lustre",
        "authors": [
            "Joel Reed",
            "Jeremy Archuleta",
            "Michael J. Brim",
            "Joshua Lothian"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  We define dynamic striping as the ability to assign different Lustre striping\ncharacteristics to contiguous segments of a file as it grows. In this paper, we\nevaluate the effects of dynamic striping using a watermark-based strategy where\nthe stripe count or width is increased once a file's size exceeds one of the\nchosen watermarks. To measure the performance of this strategy we used a\nmodified version of the IOR benchmark, a netflow analysis workload, and the\nblastn algorithm from NCBI BLAST. The results indicate that dynamic striping is\nbeneficial to tasks with unpredictable data file size and large sequential\nreads, but are less conclusive for workloads with significant random read\nphases.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.06833v1"
    },
    {
        "title": "Improving Block-level Efficiency with scsi-mq",
        "authors": [
            "Blake Caldwell"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Current generation solid-state storage devices are exposing a new bottlenecks\nin the SCSI and block layers of the Linux kernel, where IO throughput is\nlimited by lock contention, inefficient interrupt handling, and poor memory\nlocality. To address these limitations, the Linux kernel block layer underwent\na major rewrite with the blk-mq project to move from a single request queue to\na multi-queue model. The Linux SCSI subsystem rework to make use of this new\nmodel, known as scsi-mq, has been merged into the Linux kernel and work is\nunderway for dm-multipath support in the upcoming Linux 4.0 kernel. These\npieces were necessary to make use of the multi-queue block layer in a Lustre\nparallel filesystem with high availability requirements. We undertook adding\nsupport of the 3.18 kernel to Lustre with scsi-mq and dm-multipath patches to\nevaluate the potential of these efficiency improvements. In this paper we\nevaluate the block-level performance of scsi-mq with backing storage hardware\nrepresentative of a HPC-targerted Lustre filesystem. Our findings show that\nSCSI write request latency is reduced by as much as 13.6%. Additionally, when\nprofiling the CPU usage of our prototype Lustre filesystem, we found that CPU\nidle time increased by a factor of 7 with Linux 3.18 and blk-mq as compared to\na standard 2.6.32 Linux kernel. Our findings demonstrate increased efficiency\nof the multi-queue block layer even with disk-based caching storage arrays used\nin existing parallel filesystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.07481v1"
    },
    {
        "title": "Adapting the DMTCP Plugin Model for Checkpointing of Hardware Emulation",
        "authors": [
            "Rohan Garg",
            "Kapil Arya",
            "Jiajun Cao",
            "Gene Cooperman",
            "Jeff Evans",
            "Ankit Garg",
            "Neil A. Rosenberg",
            "K. Suresh"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  Checkpoint-restart is now a mature technology. It allows a user to save and\nlater restore the state of a running process. The new plugin model for the\nupcoming version 3.0 of DMTCP (Distributed MultiThreaded Checkpointing) is\ndescribed here. This plugin model allows a target application to disconnect\nfrom the hardware emulator at checkpoint time and then re-connect to a possibly\ndifferent hardware emulator at the time of restart. The DMTCP plugin model is\nimportant in allowing three distinct parties to seamlessly inter-operate. The\nthree parties are: the EDA designer, who is concerned with formal verification\nof a circuit design; the DMTCP developers, who are concerned with providing\ntransparent checkpointing during the circuit emulation; and the hardware\nemulator vendor, who provides a plugin library that responds to checkpoint,\nrestart, and other events.\n  The new plugin model is an example of process-level virtualization:\nvirtualization of external abstractions from within a process. This capability\nis motivated by scenarios for testing circuit models with the help of a\nhardware emulator. The plugin model enables a three-way collaboration: allowing\na circuit designer and emulator vendor to each contribute separate proprietary\nplugins while sharing an open source software framework from the DMTCP\ndevelopers. This provides a more flexible platform, where different fault\ninjection models based on plugins can be designed within the DMTCP\ncheckpointing framework. After initialization, one restarts from a checkpointed\nstate under the control of the desired plugin. This restart saves the time\nspent in simulating the initialization phase, while enabling fault injection\nexactly at the region of interest. Upon restart, one can inject faults or\notherwise modify the remainder of the simulation. The work concludes with a\nbrief survey of checkpointing and process-level virtualization.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00897v1"
    },
    {
        "title": "Formalizing Memory Accesses and Interrupts",
        "authors": [
            "Reto Achermann",
            "Lukas Humbel",
            "David Cock",
            "Timothy Roscoe"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  The hardware/software boundary in modern heterogeneous multicore computers is\nincreasingly complex, and diverse across different platforms. A single memory\naccess by a core or DMA engine traverses multiple hardware translation and\ncaching steps, and the destination memory cell or register often appears at\ndifferent physical addresses for different cores. Interrupts pass through a\ncomplex topology of interrupt controllers and remappers before delivery to one\nor more cores, each with specific constraints on their configurations. System\nsoftware must not only correctly understand the specific hardware at hand, but\nalso configure it appropriately at runtime. We propose a formal model of\naddress spaces and resources in a system that allows us to express and verify\ninvariants of the system's runtime configuration, and illustrate (and motivate)\nit with several real platforms we have encountered in the process of OS\nimplementation.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.06571v1"
    },
    {
        "title": "Memos: Revisiting Hybrid Memory Management in Modern Operating System",
        "authors": [
            "Lei Liu",
            "Mengyao Xie",
            "Hao Yang"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  The emerging hybrid DRAM-NVM architecture is challenging the existing memory\nmanagement mechanism in operating system. In this paper, we introduce memos,\nwhich can schedule memory resources over the entire memory hierarchy including\ncache, channels, main memory comprising DRAM and NVM simultaneously. Powered by\nour newly designed kernel-level monitoring module and page migration engine,\nmemos can dynamically optimize the data placement at the memory hierarchy in\nterms of the on-line memory patterns, current resource utilization and feature\nof memory medium. Our experimental results show that memos can achieve high\nmemory utilization, contributing to system throughput by 19.1% and QoS by 23.6%\non average. Moreover, memos can reduce the NVM side memory latency by 3~83.3%,\nenergy consumption by 25.1~99%, and benefit the NVM lifetime significantly (40X\nimprovement on average).\n",
        "pdf_link": "http://arxiv.org/pdf/1703.07725v1"
    },
    {
        "title": "FreeGuard: A Faster Secure Heap Allocator",
        "authors": [
            "Sam Silvestro",
            "Hongyu Liu",
            "Corey Crosser",
            "Zhiqiang Lin",
            "Tongping Liu"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  In spite of years of improvements to software security, heap-related attacks\nstill remain a severe threat. One reason is that many existing memory\nallocators fall short in a variety of aspects. For instance,\nperformance-oriented allocators are designed with very limited countermeasures\nagainst attacks, but secure allocators generally suffer from significant\nperformance overhead, e.g., running up to 10x slower. This paper, therefore,\nintroduces FreeGuard, a secure memory allocator that prevents or reduces a wide\nrange of heap-related attacks, such as heap overflows, heap over-reads,\nuse-after-frees, as well as double and invalid frees. FreeGuard has similar\nperformance to the default Linux allocator, with less than 2% overhead on\naverage, but provides significant improvement to security guarantees. FreeGuard\nalso addresses multiple implementation issues of existing secure allocators,\nsuch as the issue of scalability. Experimental results demonstrate that\nFreeGuard is very effective in defending against a variety of heap-related\nattacks.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.02746v2"
    },
    {
        "title": "Reservation-Based Federated Scheduling for Parallel Real-Time Tasks",
        "authors": [
            "Niklas Ueter",
            "Georg von der Brüggen",
            "Jian-Jia Chen",
            "Jing Li",
            "Kunal Agrawal"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  This paper considers the scheduling of parallel real-time tasks with\narbitrary-deadlines. Each job of a parallel task is described as a directed\nacyclic graph (DAG). In contrast to prior work in this area, where\ndecomposition-based scheduling algorithms are proposed based on the\nDAG-structure and inter-task interference is analyzed as self-suspending\nbehavior, this paper generalizes the federated scheduling approach. We propose\na reservation-based algorithm, called reservation-based federated scheduling,\nthat dominates federated scheduling. We provide general constraints for the\ndesign of such systems and prove that reservation-based federated scheduling\nhas a constant speedup factor with respect to any optimal DAG task scheduler.\nFurthermore, the presented algorithm can be used in conjunction with any\nscheduler and scheduling analysis suitable for ordinary arbitrary-deadline\nsporadic task sets, i.e., without parallelism.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.05040v1"
    },
    {
        "title": "Virtual Breakpoints for x86/64",
        "authors": [
            "Gregory Michael Price"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Efficient, reliable trapping of execution in a program at the desired\nlocation is a linchpin technique for dynamic malware analysis. The progression\nof debuggers and malware is akin to a game of cat and mouse - each are\nconstantly in a state of trying to thwart one another. At the core of most\nefficient debuggers today is a combination of virtual machines and traditional\nbinary modification breakpoints (int3). In this paper, we present a design for\nVirtual Breakpoints. a modification to the x86 MMU which brings breakpoint\nmanagement into hardware alongside page tables. In this paper we demonstrate\nthe fundamental abstraction failures of current trapping methods, and design a\nnew mechanism from the hardware up. This design incorporates lessons learned\nfrom 50 years of virtualization and debugger design to deliver fast, reliable\ntrapping without the pitfalls of traditional binary modification.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.09250v3"
    },
    {
        "title": "Push Forward: Global Fixed-Priority Scheduling of Arbitrary-Deadline\n  Sporadic Task Systems",
        "authors": [
            "Jian-Jia Chen",
            "Georg von der Brüggen",
            "Niklas Ueter"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  The sporadic task model is often used to analyze recurrent execution of\nidentical tasks in real-time systems. A sporadic task defines an infinite\nsequence of task instances, also called jobs, that arrive under the minimum\ninter-arrival time constraint. To ensure the system safety, timeliness has to\nbe guaranteed in addition to functional correctness, i.e., all jobs of all\ntasks have to be finished before the job deadlines. We focus on analyzing\narbitrary-deadline task sets on a homogeneous (identical) multiprocessor system\nunder any given global fixed-priority scheduling approach and provide a series\nof schedulability tests with different tradeoffs between their time complexity\nand their accuracy. Under the arbitrary-deadline setting, the relative deadline\nof a task can be longer than the minimum inter-arrival time of the jobs of the\ntask. We show that global deadline-monotonic (DM) scheduling has a speedup\nbound of $3-1/M$ against any optimal scheduling algorithms, where $M$ is the\nnumber of identical processors, and prove that this bound is asymptotically\ntight.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.10376v1"
    },
    {
        "title": "Mosaic: An Application-Transparent Hardware-Software Cooperative Memory\n  Manager for GPUs",
        "authors": [
            "Rachata Ausavarungnirun",
            "Joshua Landgraf",
            "Vance Miller",
            "Saugata Ghose",
            "Jayneel Gandhi",
            "Christopher J. Rossbach",
            "Onur Mutlu"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Modern GPUs face a trade-off on how the page size used for memory management\naffects address translation and demand paging. Support for multiple page sizes\ncan help relax the page size trade-off so that address translation and demand\npaging optimizations work together synergistically. However, existing page\ncoalescing and splintering policies require costly base page migrations that\nundermine the benefits multiple page sizes provide. In this paper, we observe\nthat GPGPU applications present an opportunity to support multiple page sizes\nwithout costly data migration, as the applications perform most of their memory\nallocation en masse (i.e., they allocate a large number of base pages at once).\nWe show that this en masse allocation allows us to create intelligent memory\nallocation policies which ensure that base pages that are contiguous in virtual\nmemory are allocated to contiguous physical memory pages. As a result,\ncoalescing and splintering operations no longer need to migrate base pages.\n  We introduce Mosaic, a GPU memory manager that provides\napplication-transparent support for multiple page sizes. Mosaic uses base pages\nto transfer data over the system I/O bus, and allocates physical memory in a\nway that (1) preserves base page contiguity and (2) ensures that a large page\nframe contains pages from only a single memory protection domain. This\nmechanism allows the TLB to use large pages, reducing address translation\noverhead. During data transfer, this mechanism enables the GPU to transfer only\nthe base pages that are needed by the application over the system I/O bus,\nkeeping demand paging overhead low.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.11265v1"
    },
    {
        "title": "Modeling Processor Idle Times in MPSoC Platforms to Enable Integrated\n  DPM, DVFS, and Task Scheduling Subject to a Hard Deadline",
        "authors": [
            "Amirhossein Esmaili",
            "Mahdi Nazemi",
            "Massoud Pedram"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Energy efficiency is one of the most critical design criteria for modern\nembedded systems such as multiprocessor system-on-chips (MPSoCs). Dynamic\nvoltage and frequency scaling (DVFS) and dynamic power management (DPM) are two\nmajor techniques for reducing energy consumption in such embedded systems.\nFurthermore, MPSoCs are becoming more popular for many real-time applications.\nOne of the challenges of integrating DPM with DVFS and task scheduling of\nreal-time applications on MPSoCs is the modeling of idle intervals on these\nplatforms. In this paper, we present a novel approach for modeling idle\nintervals in MPSoC platforms which leads to a mixed integer linear programming\n(MILP) formulation integrating DPM, DVFS, and task scheduling of periodic task\ngraphs subject to a hard deadline. We also present a heuristic approach for\nsolving the MILP and compare its results with those obtained from solving the\nMILP.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07723v1"
    },
    {
        "title": "A WCET-aware cache coloring technique for reducing interference in\n  real-time systems",
        "authors": [
            "Fabien Bouquillon",
            "Clément Ballabriga",
            "Giuseppe Lipari",
            "Smail Niar"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  The predictability of a system is the condition to give saferbound on worst\ncase execution timeof real-time tasks which are running on it. Commercial\noff-the-shelf(COTS) processors are in-creasingly used in embedded systems and\ncontain shared cache memory. This component hasa hard predictable behavior\nbecause its state depends of theexecution history of the systems.To increase\npredictability of COTS component we use cache coloring, a technique widely\nusedto partition cache memory. Our main contribution is a WCET aware heuristic\nwhich parti-tion task according to the needs of each task. Our experiments are\nmade with CPLEX an ILPsolver with random tasks set generated running on\npreemptive system scheduled with earliestdeadline first(EDF).\n",
        "pdf_link": "http://arxiv.org/pdf/1903.09310v4"
    },
    {
        "title": "Understanding and taming SSD read performance variability: HDFS case\n  study",
        "authors": [
            "María F. Borge",
            "Florin Dinu",
            "Willy Zwaenepoel"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  In this paper we analyze the influence that lower layers (file system, OS,\nSSD) have on HDFS' ability to extract maximum performance from SSDs on the read\npath. We uncover and analyze three surprising performance slowdowns induced by\nlower layers that result in HDFS read throughput loss. First, intrinsic\nslowdown affects reads from every new file system extent for a variable amount\nof time. Second, temporal slowdown appears temporarily and periodically and is\nworkload-agnostic. Third, in permanent slowdown, some files can individually\nand permanently become slower after a period of time. We analyze the impact of\nthese slowdowns on HDFS and show significant throughput loss. Individually,\neach of the slowdowns can cause a read throughput loss of 10-15%. However,\ntheir effect is cumulative. When all slowdowns happen concurrently, read\nthroughput drops by as much as 30%. We further analyze mitigation techniques\nand show that two of the three slowdowns could be addressed via increased IO\nrequest parallelism in the lower layers. Unfortunately, HDFS cannot\nautomatically adapt to use such additional parallelism. Our results point to a\nneed for adaptability in storage stacks. The reason is that an access pattern\nthat maximizes performance in the common case is not necessarily the same one\nthat can mask performance fluctuations.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.09347v1"
    },
    {
        "title": "Study and Analysis of MAC/IPAD Lab Configuration",
        "authors": [
            "Ayman Noor"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  This paper is about three virtualization modes: VMware, Parallels, and Boot\nCamping. The trade off of their testing is the hardware requirements. The main\nquestion is, among the three, which is the most suitable? The answer actually\nvaries from user to user. It depends on the user needs. Moreover, it is\nnecessary to consider its performance, graphics, efficiency and reliability,\nand interoperability, and that is our major scope. In order to take the final\ndecision in choosing one of the modes it is important to run some tests, which\ncosts a lot in terms of money, complexity, and time consumption. Therefore, in\norder to overcome this trade off, most of the research has been done through\nonline benchmarking and my own anticipation. The final solution was extracted\nafter comparing all previously mentioned above and after rigorous testing made\nwhich will be introduced later in this document.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05405v1"
    },
    {
        "title": "Defending against malicious peripherals with Cinch",
        "authors": [
            "Sebastian Angel",
            "Riad S. Wahby",
            "Max Howald",
            "Joshua B. Leners",
            "Michael Spilo",
            "Zhen Sun",
            "Andrew J. Blumberg",
            "Michael Walfish"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  Malicious peripherals designed to attack their host computers are a growing\nproblem. Inexpensive and powerful peripherals that attach to plug-and-play\nbuses have made such attacks easy to mount. Making matters worse, commodity\noperating systems lack coherent defenses, and users are often unaware of the\nscope of the problem. We present Cinch, a pragmatic response to this threat.\nCinch uses virtualization to attach peripheral devices to a logically separate,\nuntrusted machine, and includes an interposition layer between the untrusted\nmachine and the protected one. This layer regulates interaction with devices\naccording to user-configured policies. Cinch integrates with existing OSes,\nenforces policies that thwart real-world attacks, and has low overhead.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.01449v4"
    },
    {
        "title": "Cache Contention on Multicore Systems: An Ontology-based Approach",
        "authors": [
            "Maruthi Rohit Ayyagari"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Multicore processors have proved to be the right choice for both desktop and\nserver systems because it can support high performance with an acceptable\nbudget expenditure. In this work, we have compared several works in cache\ncontention and found that such works have identified several techniques for\ncache contention other than cache size including FSB, Memory Controller and\nprefetching hardware. We found that Distributed Intensity Online (DIO) is a\nvery promising cache contention algorithm since it can achieve up to 2% from\nthe optimal technique. Moreover, we propose a new framework for cache\ncontention based on resource ontologies. In which ontologies instances will be\nused for communication between diverse processes instead of grasping schedules\nbased on hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.00834v1"
    },
    {
        "title": "On The Performance of ARM TrustZone",
        "authors": [
            "Julien Amacher",
            "Valerio Schiavoni"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  The TrustZone technology, available in the vast majority of recent ARM\nprocessors, allows the execution of code inside a so-called secure world. It\neffectively provides hardware-isolated areas of the processor for sensitive\ndata and code, i.e., a trusted execution environment (TEE). The OP-TEE\nframework provides a collection of toolchain, open-source libraries and secure\nkernel specifically geared to develop applications for TrustZone. This paper\npresents an in-depth performance- and energy-wise study of TrustZone using the\nOP-TEE framework, including secure storage and the cost of switching between\nsecure and unsecure worlds, using emulated and hardware measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.09799v2"
    },
    {
        "title": "Practical Fine-grained Privilege Separation in Multithreaded\n  Applications",
        "authors": [
            "Jun Wang",
            "Xi Xiong",
            "Peng Liu"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  An inherent security limitation with the classic multithreaded programming\nmodel is that all the threads share the same address space and, therefore, are\nimplicitly assumed to be mutually trusted. This assumption, however, does not\ntake into consideration of many modern multithreaded applications that involve\nmultiple principals which do not fully trust each other. It remains challenging\nto retrofit the classic multithreaded programming model so that the security\nand privilege separation in multi-principal applications can be resolved.\n  This paper proposes ARBITER, a run-time system and a set of security\nprimitives, aimed at fine-grained and data-centric privilege separation in\nmultithreaded applications. While enforcing effective isolation among\nprincipals, ARBITER still allows flexible sharing and communication between\nthreads so that the multithreaded programming paradigm can be preserved. To\nrealize controlled sharing in a fine-grained manner, we created a novel\nabstraction named ARBITER Secure Memory Segment (ASMS) and corresponding OS\nsupport. Programmers express security policies by labeling data and principals\nvia ARBITER's API following a unified model. We ported a widely-used, in-memory\ndatabase application (memcached) to ARBITER system, changing only around 100\nLOC. Experiments indicate that only an average runtime overhead of 5.6% is\ninduced to this security enhanced version of application.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.2553v1"
    },
    {
        "title": "Browsix: Bridging the Gap Between Unix and the Browser",
        "authors": [
            "Bobby Powers",
            "John Vilk",
            "Emery D. Berger"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  Applications written to run on conventional operating systems typically\ndepend on OS abstractions like processes, pipes, signals, sockets, and a shared\nfile system. Porting these applications to the web currently requires extensive\nrewriting or hosting significant portions of code server-side because browsers\npresent a nontraditional runtime environment that lacks OS functionality.\n  This paper presents Browsix, a framework that bridges the considerable gap\nbetween conventional operating systems and the browser, enabling unmodified\nprograms expecting a Unix-like environment to run directly in the browser.\nBrowsix comprises two core parts: (1) a JavaScript-only system that makes core\nUnix features (including pipes, concurrent processes, signals, sockets, and a\nshared file system) available to web applications; and (2) extended JavaScript\nruntimes for C, C++, Go, and Node.js that support running programs written in\nthese languages as processes in the browser. Browsix supports running a POSIX\nshell, making it straightforward to connect applications together via pipes.\n  We illustrate Browsix's capabilities via case studies that demonstrate how it\neases porting legacy applications to the browser and enables new functionality.\nWe demonstrate a Browsix-enabled LaTeX editor that operates by executing\nunmodified versions of pdfLaTeX and BibTeX. This browser-only LaTeX editor can\nrender documents in seconds, making it fast enough to be practical. We further\ndemonstrate how Browsix lets us port a client-server application to run\nentirely in the browser for disconnected operation. Creating these applications\nrequired less than 50 lines of glue code and no code modifications,\ndemonstrating how easily Browsix can be used to build sophisticated web\napplications from existing parts without modification.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.07862v2"
    },
    {
        "title": "Fine-Grain Checkpointing with In-Cache-Line Logging",
        "authors": [
            "Nachshon Cohen",
            "David T. Aksun",
            "Hillel Avni",
            "James R. Larus"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Non-Volatile Memory offers the possibility of implementing high-performance,\ndurable data structures. However, achieving performance comparable to\nwell-designed data structures in non-persistent (transient) memory is\ndifficult, primarily because of the cost of ensuring the order in which memory\nwrites reach NVM. Often, this requires flushing data to NVM and waiting a full\nmemory round-trip time.\n  In this paper, we introduce two new techniques: Fine-Grained Checkpointing,\nwhich ensures a consistent, quickly recoverable data structure in NVM after a\nsystem failure, and In-Cache-Line Logging, an undo-logging technique that\nenables recovery of earlier state without requiring cache-line flushes in the\nnormal case. We implemented these techniques in the Masstree data structure,\nmaking it persistent and demonstrating the ease of applying them to a highly\noptimized system and their low (5.9-15.4\\%) runtime overhead cost.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.00660v1"
    },
    {
        "title": "Virtual Gang based Scheduling of Real-Time Tasks on Multicore Platforms",
        "authors": [
            "Waqar Ali",
            "Rodolfo Pellizzoni",
            "Heechul Yun"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  We propose a virtual-gang based parallel real-time task scheduling approach\nfor multicore platforms. Our approach is based on the notion of a virtual-gang,\nwhich is a group of parallel real-time tasks that are statically linked and\nscheduled together by a gang scheduler. We present a light-weight intra-gang\nsynchronization framework, called RTG-Sync, and virtual gang formation\nalgorithms that provide strong temporal isolation and high real-time\nschedulability in scheduling real-time tasks on multicore. We evaluate our\napproach both analytically, with generated tasksets against state-of-the-art\napproaches, and empirically with a case-study involving real-world workloads on\na real embedded multicore platform. The results show that our approach provides\nsimple but powerful compositional analysis framework, achieves better analytic\nschedulability, especially when the effect of interference is considered, and\nis a practical solution for COTS multicore platforms.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10959v2"
    },
    {
        "title": "Study of Firecracker MicroVM",
        "authors": [
            "Madhur Jain"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Firecracker is a virtualization technology that makes use of Kernel Virtual\nMachine (KVM). Firecracker belongs to a new virtualization class named the\nmicro-virtual machines (MicroVMs). Using Firecracker, we can launch lightweight\nMicroVMs in non-virtualized environments in a fraction of a second, at the same\ntime offering the security and workload isolation provided by traditional VMs\nand also the resource efficiency that comes along with containers \\cite{b1}.\nFirecracker aims to provide a slimmed-down MicroVM, comprised of approximately\n50K lines of code in Rust and with a reduced attack surface for guest VMs. This\nreport will examine the internals of Firecracker and understand why Firecracker\nis the next big thing going forward in virtualization and cloud computing.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.12821v1"
    },
    {
        "title": "SGX-LKL: Securing the Host OS Interface for Trusted Execution",
        "authors": [
            "Christian Priebe",
            "Divya Muthukumaran",
            "Joshua Lind",
            "Huanzhou Zhu",
            "Shujie Cui",
            "Vasily A. Sartakov",
            "Peter Pietzuch"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Hardware support for trusted execution in modern CPUs enables tenants to\nshield their data processing workloads in otherwise untrusted cloud\nenvironments. Runtime systems for the trusted execution must rely on an\ninterface to the untrusted host OS to use external resources such as storage,\nnetwork, and other functions. Attackers may exploit this interface to leak data\nor corrupt the computation.\n  We describe SGX-LKL, a system for running Linux binaries inside of Intel SGX\nenclaves that only exposes a minimal, protected and oblivious host interface:\nthe interface is (i) minimal because SGX-LKL uses a complete library OS inside\nthe enclave, including file system and network stacks, which requires a host\ninterface with only 7 calls; (ii) protected because SGX-LKL transparently\nencrypts and integrity-protects all data passed via low-level I/O operations;\nand (iii) oblivious because SGX-LKL performs host operations independently of\nthe application workload. For oblivious disk I/O, SGX-LKL uses an encrypted\next4 file system with shuffled disk blocks. We show that SGX-LKL protects\nTensorFlow training with a 21% overhead.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.11143v3"
    },
    {
        "title": "SplitFS: Reducing Software Overhead in File Systems for Persistent\n  Memory",
        "authors": [
            "Rohan Kadekodi",
            "Se Kwon Lee",
            "Sanidhya Kashyap",
            "Taesoo Kim",
            "Aasheesh Kolli",
            "Vijay Chidambaram"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  We present SplitFS, a file system for persistent memory (PM) that reduces\nsoftware overhead significantly compared to state-of-the-art PM file systems.\nSplitFS presents a novel split of responsibilities between a user-space library\nfile system and an existing kernel PM file system. The user-space library file\nsystem handles data operations by intercepting POSIX calls, memory-mapping the\nunderlying file, and serving the read and overwrites using processor loads and\nstores. Metadata operations are handled by the kernel PM file system (ext4\nDAX). SplitFS introduces a new primitive termed relink to efficiently support\nfile appends and atomic data operations. SplitFS provides three consistency\nmodes, which different applications can choose from, without interfering with\neach other. SplitFS reduces software overhead by up-to 4x compared to the NOVA\nPM file system, and 17x compared to ext4-DAX. On a number of micro-benchmarks\nand applications such as the LevelDB key-value store running the YCSB\nbenchmark, SplitFS increases application performance by up to 2x compared to\next4 DAX and NOVA while providing similar consistency guarantees.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.10123v1"
    },
    {
        "title": "Period Adaptation for Continuous Security Monitoring in Multicore\n  Real-Time Systems",
        "authors": [
            "Monowar Hasan",
            "Sibin Mohan",
            "Rodolfo Pellizzoni",
            "Rakesh B. Bobba"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  We propose a design-time framework (named HYDRA-C) for integrating security\ntasks into partitioned real-time systems (RTS) running on multicore platforms.\nOur goal is to opportunistically execute security monitoring mechanisms in a\n'continuous' manner -- i.e., as often as possible, across cores, to ensure that\nsecurity tasks run with as few interruptions as possible. Our framework will\nallow designers to integrate security mechanisms without perturbing existing\nreal-time (RT) task properties or execution order. We demonstrate the framework\nusing a proof-of-concept implementation with intrusion detection mechanisms as\nsecurity tasks. We develop and use both, (a) a custom intrusion detection\nsystem (IDS), as well as (b) Tripwire -- an open source data integrity checking\ntool. These are implemented on a realistic rover platform designed using an ARM\nmulticore chip. We compare the performance of HYDRA-C with a state-of-the-art\nRT security integration approach for multicore-based RTS and find that our\nmethod can, on average, detect intrusions 19.05% faster without impacting the\nperformance of RT tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.11937v2"
    },
    {
        "title": "Scalable Range Locks for Scalable Address Spaces and Beyond",
        "authors": [
            "Alex Kogan",
            "Dave Dice",
            "Shady Issa"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Range locks are a synchronization construct designed to provide concurrent\naccess to multiple threads (or processes) to disjoint parts of a shared\nresource. Originally conceived in the file system context, range locks are\ngaining increasing interest in the Linux kernel community seeking to alleviate\nbottlenecks in the virtual memory management subsystem. The existing\nimplementation of range locks in the kernel, however, uses an internal spin\nlock to protect the underlying tree structure that keeps track of acquired and\nrequested ranges. This spin lock becomes a point of contention on its own when\nthe range lock is frequently acquired. Furthermore, where and exactly how\nspecific (refined) ranges can be locked remains an open question.\n  In this paper, we make two independent, but related contributions. First, we\npropose an alternative approach for building range locks based on linked lists.\nThe lists are easy to maintain in a lock-less fashion, and in fact, our range\nlocks do not use any internal locks in the common case. Second, we show how the\nrange of the lock can be refined in the mprotect operation through a\nspeculative mechanism. This refinement, in turn, allows concurrent execution of\nmprotect operations on non-overlapping memory regions. We implement our new\nalgorithms and demonstrate their effectiveness in user-space and kernel-space,\nachieving up to 9$\\times$ speedup compared to the stock version of the Linux\nkernel. Beyond the virtual memory management subsystem, we discuss other\napplications of range locks in parallel software. As a concrete example, we\nshow how range locks can be used to facilitate the design of scalable\nconcurrent data structures, such as skip lists.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.12144v1"
    },
    {
        "title": "BPF for storage: an exokernel-inspired approach",
        "authors": [
            "Yu Jian Wu",
            "Hongyi Wang",
            "Yuhong Zhong",
            "Asaf Cidon",
            "Ryan Stutsman",
            "Amy Tai",
            "Junfeng Yang"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  The overhead of the kernel storage path accounts for half of the access\nlatency for new NVMe storage devices. We explore using BPF to reduce this\noverhead, by injecting user-defined functions deep in the kernel's I/O\nprocessing stack. When issuing a series of dependent I/O requests, this\napproach can increase IOPS by over 2.5$\\times$ and cut latency by half, by\nbypassing kernel layers and avoiding user-kernel boundary crossings. However,\nwe must avoid losing important properties when bypassing the file system and\nblock layer such as the safety guarantees of the file system and translation\nbetween physical blocks addresses and file offsets. We sketch potential\nsolutions to these problems, inspired by exokernel file systems from the late\n90s, whose time, we believe, has finally come!\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12922v1"
    },
    {
        "title": "A Learned Cache Eviction Framework with Minimal Overhead",
        "authors": [
            "Dongsheng Yang",
            "Daniel S. Berger",
            "Kai Li",
            "Wyatt Lloyd"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Recent work shows the effectiveness of Machine Learning (ML) to reduce cache\nmiss ratios by making better eviction decisions than heuristics. However,\nstate-of-the-art ML caches require many predictions to make an eviction\ndecision, making them impractical for high-throughput caching systems. This\npaper introduces Machine learning At the Tail (MAT), a framework to build\nefficient ML-based caching systems by integrating an ML module with a\ntraditional cache system based on a heuristic algorithm. MAT treats the\nheuristic algorithm as a filter to receive high-quality samples to train an ML\nmodel and likely candidate objects for evictions. We evaluate MAT on 8\nproduction workloads, spanning storage, in-memory caching, and CDNs. The\nsimulation experiments show MAT reduces the number of costly ML\npredictions-per-eviction from 63 to 2, while achieving comparable miss ratios\nto the state-of-the-art ML cache system. We compare a MAT prototype system with\nan LRU-based caching system in the same setting and show that they achieve\nsimilar request rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.11886v1"
    },
    {
        "title": "Hello rootKitty: A lightweight invariance-enforcing framework",
        "authors": [
            "Francesco Gadaleta",
            "Nick Nikiforakis",
            "Yves Younan",
            "Wouter Joosen"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  In monolithic operating systems, the kernel is the piece of code that\nexecutes with the highest privileges and has control over all the software\nrunning on a host. A successful attack against an operating system's kernel\nmeans a total and complete compromise of the running system. These attacks\nusually end with the installation of a rootkit, a stealthy piece of software\nrunning with kernel privileges. When a rootkit is present, no guarantees can be\nmade about the correctness, privacy or isolation of the operating system.\n  In this paper we present \\emph{Hello rootKitty}, an invariance-enforcing\nframework which takes advantage of current virtualization technology to protect\na guest operating system against rootkits. \\emph{Hello rootKitty} uses the idea\nof invariance to detect maliciously modified kernel data structures and restore\nthem to their original legitimate values. Our prototype has negligible\nperformance and memory overhead while effectively protecting commodity\noperating systems from modern rootkits.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.5651v1"
    },
    {
        "title": "Effects of Hard Real-Time Constraints in Implementing the Myopic\n  Scheduling Algorithm",
        "authors": [
            "Kazi Sakib",
            "M. S. Hasan",
            "M. A. Hossain"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  Myopic is a hard real-time process scheduling algorithm that selects a\nsuitable process based on a heuristic function from a subset (Window)of all\nready processes instead of choosing from all available processes, like original\nheuristic scheduling algorithm. Performance of the algorithm significantly\ndepends on the chosen heuristic function that assigns weight to different\nparameters like deadline, earliest starting time, processing time etc. and the\nsizeof the Window since it considers only k processes from n processes (where,\nk<= n). This research evaluates the performance of the Myopic algorithm for\ndifferent parameters to demonstrate the merits and constraints of the\nalgorithm. A comparative performance of the impact of window size in\nimplementing the Myopic algorithm is presented and discussed through a set of\nexperiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.0122v1"
    },
    {
        "title": "HyBIS: Windows Guest Protection through Advanced Memory Introspection",
        "authors": [
            "Roberto di Pietro",
            "Federico Franzoni",
            "Flavio Lombardi"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  Effectively protecting the Windows OS is a challenging task, since most\nimplementation details are not publicly known. Windows has always been the main\ntarget of malwares that have exploited numerous bugs and vulnerabilities.\nRecent trusted boot and additional integrity checks have rendered the Windows\nOS less vulnerable to kernel-level rootkits. Nevertheless, guest Windows\nVirtual Machines are becoming an increasingly interesting attack target. In\nthis work we introduce and analyze a novel Hypervisor-Based Introspection\nSystem (HyBIS) we developed for protecting Windows OSes from malware and\nrootkits. The HyBIS architecture is motivated and detailed, while targeted\nexperimental results show its effectiveness. Comparison with related work\nhighlights main HyBIS advantages such as: effective semantic introspection,\nsupport for 64-bit architectures and for latest Windows (8.x and 10), advanced\nmalware disabling capabilities. We believe the research effort reported here\nwill pave the way to further advances in the security of Windows OSes.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.05851v1"
    },
    {
        "title": "Design and Implementation of Modified Fuzzy based CPU Scheduling\n  Algorithm",
        "authors": [
            "Rajani Kumari",
            "Vivek Kumar Sharma",
            "Sandeep Kumar"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  CPU Scheduling is the base of multiprogramming. Scheduling is a process which\ndecides order of task from a set of multiple tasks that are ready to execute.\nThere are number of CPU scheduling algorithms available, but it is very\ndifficult task to decide which one is better. This paper discusses the design\nand implementation of modified fuzzy based CPU scheduling algorithm. This paper\npresent a new set of fuzzy rules. It demonstrates that scheduling done with new\npriority improves average waiting time and average turnaround time.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.02621v1"
    },
    {
        "title": "PINPOINT: Efficient and Effective Resource Isolation for Mobile Security\n  and Privacy",
        "authors": [
            "Paul Ratazzi",
            "Ashok Bommisetti",
            "Nian Ji",
            "Wenliang Du"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Virtualization is frequently used to isolate untrusted processes and control\ntheir access to sensitive resources. However, isolation usually carries a price\nin terms of less resource sharing and reduced inter-process communication. In\nan open architecture such as Android, this price and its impact on performance,\nusability, and transparency must be carefully considered. Although previous\nefforts in developing general-purpose isolation solutions have shown that some\nof these negative side effects can be mitigated, doing so involves overcoming\nsignificant design challenges by incorporating numerous additional platform\ncomplexities not directly related to improved security. Thus, the general\npurpose solutions become inefficient and burdensome if the end-user has only\nspecific security goals. In this paper, we present PINPOINT, a resource\nisolation strategy that forgoes general-purpose solutions in favor of a\n\"building block\" approach that addresses specific end-user security goals.\nPINPOINT embodies the concept of Linux Namespace lightweight isolation, but\ndoes so in the Android Framework by guiding the security designer towards\nisolation points that are contextually close to the resource(s) that need to be\nisolated. This strategy allows the rest of the Framework to function fully as\nintended, transparently. We demonstrate our strategy with a case study on\nAndroid System Services, and show four applications of PINPOINTed system\nservices functioning with unmodified market apps. Our evaluation results show\nthat practical security and privacy advantages can be gained using our\napproach, without inducing the problematic side-effects that other\ngeneral-purpose designs must address.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.07732v1"
    },
    {
        "title": "HTS: A Hardware Task Scheduler for Heterogeneous Systems",
        "authors": [
            "Kartik Hegde",
            "Abhishek Srivastava",
            "Rohit Agrawal"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  As the Moore's scaling era comes to an end, application specific hardware\naccelerators appear as an attractive way to improve the performance and power\nefficiency of our computing systems. A massively heterogeneous system with a\nlarge number of hardware accelerators along with multiple general purpose CPUs\nis a promising direction, but pose several challenges in terms of the run-time\nscheduling of tasks on the accelerators and design granularity of accelerators.\nThis paper addresses these challenges by developing an example heterogeneous\nsystem to enable multiple applications to share the available accelerators. We\npropose to design accelerators at a lower abstraction to enable applications to\nbe broken down into tasks that can be mapped on several accelerators. We\nobserve that several real-life workloads can be broken down into common\nprimitives that are shared across many workloads. Finally, we propose and\ndesign a hardware task scheduler inspired by the hardware schedulers in\nout-of-order superscalar processors to efficiently utilize the accelerators in\nthe system by scheduling tasks in out-of-order and even speculatively. We\nevaluate the proposed system on both real-life and synthetic benchmarks based\non Digital Signal Processing~(DSP) applications. Compared to executing the\nbenchmark on a system with sequential scheduling, proposed scheduler achieves\nup to 12x improvement in performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.00271v1"
    },
    {
        "title": "Disaggregation and the Application",
        "authors": [
            "Sebastian Angel",
            "Mihir Nanavati",
            "Siddhartha Sen"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  This paper examines disaggregated data center architectures from the\nperspective of the applications that would run on these data centers, and\nchallenges the abstractions that have been proposed to date. In particular, we\nargue that operating systems for disaggregated data centers should not abstract\ndisaggregated hardware resources, such as memory, compute, and storage away\nfrom applications, but should instead give them information about, and control\nover, these resources. To this end, we propose additional OS abstractions and\ninterfaces for disaggregation and show how they can improve data transfer in\ndata parallel frameworks and speed up failure recovery in replicated,\nfault-tolerant applications. This paper studies the technical challenges in\nproviding applications with this additional functionality and advances several\npreliminary proposals to overcome these challenges.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.13056v1"
    },
    {
        "title": "Optimal Virtual Cluster-based Multiprocessor Scheduling",
        "authors": [
            "Arvind Easwaran",
            "Insik Shin",
            "Insup Lee"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Scheduling of constrained deadline sporadic task systems on multiprocessor\nplatforms is an area which has received much attention in the recent past. It\nis widely believed that finding an optimal scheduler is hard, and therefore\nmost studies have focused on developing algorithms with good processor\nutilization bounds. These algorithms can be broadly classified into two\ncategories: partitioned scheduling in which tasks are statically assigned to\nindividual processors, and global scheduling in which each task is allowed to\nexecute on any processor in the platform. In this paper we consider a third,\nmore general, approach called cluster-based scheduling. In this approach each\ntask is statically assigned to a processor cluster, tasks in each cluster are\nglobally scheduled among themselves, and clusters in turn are scheduled on the\nmultiprocessor platform. We develop techniques to support such cluster-based\nscheduling algorithms, and also consider properties that minimize total\nprocessor utilization of individual clusters. In the last part of this paper,\nwe develop new virtual cluster-based scheduling algorithms. For implicit\ndeadline sporadic task systems, we develop an optimal scheduling algorithm that\nis neither Pfair nor ERfair. We also show that the processor utilization bound\nof US-EDF{m/(2m-1)} can be improved by using virtual clustering. Since neither\npartitioned nor global strategies dominate over the other, cluster-based\nscheduling is a natural direction for research towards achieving improved\nprocessor utilization bounds.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.02439v1"
    },
    {
        "title": "SoftWear: Software-Only In-Memory Wear-Leveling for Non-Volatile Main\n  Memory",
        "authors": [
            "Christian Hakert",
            "Kuan-Hsun Chen",
            "Paul R. Genssler",
            "Georg von der Brüggen",
            "Lars Bauer",
            "Hussam Amrouch",
            "Jian-Jia Chen",
            "Jörg Henkel"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Several emerging technologies for byte-addressable non-volatile memory (NVM)\nhave been considered to replace DRAM as the main memory in computer systems\nduring the last years. The disadvantage of a lower write endurance, compared to\nDRAM, of NVM technologies like Phase-Change Memory (PCM) or Ferroelectric RAM\n(FeRAM) has been addressed in the literature. As a solution, in-memory\nwear-leveling techniques have been proposed, which aim to balance the\nwear-level over all memory cells to achieve an increased memory lifetime.\nGenerally, to apply such advanced aging-aware wear-leveling techniques proposed\nin the literature, additional special hardware is introduced into the memory\nsystem to provide the necessary information about the cell age and thus enable\naging-aware wear-leveling decisions.\n  This paper proposes software-only aging-aware wear-leveling based on common\nCPU features and does not rely on any additional hardware support from the\nmemory subsystem. Specifically, we exploit the memory management unit (MMU),\nperformance counters, and interrupts to approximate the memory write counts as\nan aging indicator. Although the software-only approach may lead to slightly\nworse wear-leveling, it is applicable on commonly available hardware. We\nachieve page-level coarse-grained wear-leveling by approximating the current\ncell age through statistical sampling and performing physical memory remapping\nthrough the MMU. This method results in non-uniform memory usage patterns\nwithin a memory page. Hence, we further propose a fine-grained wear-leveling in\nthe stack region of C / C++ compiled software.\n  By applying both wear-leveling techniques, we achieve up to $78.43\\%$ of the\nideal memory lifetime, which is a lifetime improvement of more than a factor of\n$900$ compared to the lifetime without any wear-leveling.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.03244v2"
    },
    {
        "title": "A Linux Kernel Scheduler Extension for Multi-core Systems",
        "authors": [
            "Aleix Roca",
            "Samuel Rodríguez",
            "Albert Segura",
            "Kevin Marquet",
            "Vicenç Beltran"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  The Linux kernel is mostly designed for multi-programed environments, but\nhigh-performance applications have other requirements. Such applications are\nrun standalone, and usually rely on runtime systems to distribute the\napplication's workload on worker threads, one per core. However, due to current\nOSes limitations, it is not feasible to track whether workers are actually\nrunning or blocked due to, for instance, a requested resource. For I/O\nintensive applications, this leads to a significant performance degradation\ngiven that the core of a blocked thread becomes idle until it is able to run\nagain. In this paper, we present the proof-of-concept of a Linux kernel\nextension denoted User-Monitored Threads (UMT) which tackles this problem. Our\nextension allows a user-space process to be notified of when the selected\nthreads become blocked or unblocked, making it possible for a runtime to\nschedule additional work on the idle core. We implemented the extension on the\nLinux Kernel 5.1 and adapted the Nanos6 runtime of the OmpSs-2 programming\nmodel to take advantage of it. The whole prototype was tested on two\napplications which, on the tested hardware and the appropriate conditions,\nreported speedups of almost 2x.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.06354v1"
    },
    {
        "title": "PIMOD: A Tool for Configuring Single-Board Computer Operating System\n  Images",
        "authors": [
            "Jonas Höchst",
            "Alvar Penning",
            "Patrick Lampe",
            "Bernd Freisleben"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Computer systems used in the field of humanitarian technology are often based\non general-purpose single-board computers, such as Raspberry Pis. While these\nsystems offer great flexibility for developers and users, configuration and\ndeployment either introduces overhead by executing scripts on multiple devices\nor requires deeper technical understanding when building operating system\nimages for such small computers from scratch. In this paper, we present PIMOD,\na software tool for configuring operating system images for single-board\ncomputer systems. We propose a simple yet comprehensive configuration language.\nIn a configuration profile, called Pifile, a small set of commands is used to\ndescribe the configuration of an operating system image. Virtualization\ntechniques are used during the execution of the profile in order to be\ndistribution and platform independent. Commands can be issued in the guest\noperating system, providing access to the distribution specific tools, e.g., to\nconfigure hardware parameters. The implementation of PIMOD is made public under\na free and open source license. PIMOD is evaluated in terms of user benefits,\nperformance compared to on-system configuration, and applicability across\ndifferent hardware platforms and operating systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.07833v1"
    },
    {
        "title": "Enhancing Application Performance by Memory Partitioning in Android\n  Platforms",
        "authors": [
            "Geunsik Lim",
            "Changwoo Min",
            "Young Ik Eom"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  This paper suggests a new memory partitioning scheme that can enhance process\nlifecycle, while avoiding Low Memory Killer and Out-of-Memory Killer operations\non mobile devices. Our proposed scheme offers the complete concept of virtual\nmemory nodes in operating systems of Android devices.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.10707v1"
    },
    {
        "title": "Android OS CASE STUDY",
        "authors": [
            "Mayank Goel",
            "Gourav Singal"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Android is a mobile operating system based on a modified version of the Linux\nkernel and other open source software, designed primarily for touchscreen\nmobile devices such as smartphones and tablets. It is an operating system for\nlow powered devices that run on battery and are full of hardware like Global\nPositioning System (GPS) receivers, cameras, light and orientation sensors,\nWi-Fi and LTE (4G telephony) connectivity and a touch screen. Like all\noperating systems, Android enables applications to make use of the hardware\nfeatures through abstraction and provide a defined environment for\napplications. The study includes following topic: Background And History\nAndroid Architecture Kernel And StartUp Process Process Management Deadlock CPU\nScheduling Memory Management Storage Management I/O Battery Optimization\n",
        "pdf_link": "http://arxiv.org/pdf/2104.09487v1"
    },
    {
        "title": "SoCRATES: System-on-Chip Resource Adaptive Scheduling using Deep\n  Reinforcement Learning",
        "authors": [
            "Tegg Taekyong Sung",
            "Bo Ryu"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Deep Reinforcement Learning (DRL) is being increasingly applied to the\nproblem of resource allocation for emerging System-on-Chip (SoC) applications,\nand has shown remarkable promises. In this paper, we introduce SoCRATES (SoC\nResource AdapTivE Scheduler), an extremely efficient DRL-based SoC scheduler\nwhich maps a wide range of hierarchical jobs to heterogeneous resources within\nSoC using the Eclectic Interaction Matching (EIM) technique. It is noted that\nthe majority of SoC resource management approaches have been targeting makespan\nminimization with fixed number of jobs in the system. In contrast, SoCRATES\naims at minimizing average latency in a steady-state condition while assigning\ntasks in the ready queue to heterogeneous resources (processing elements). We\nfirst show that the latency-minimization-driven SoC applications operate\nhigh-frequency job workload and distributed/parallel job execution. We then\ndemonstrate SoCRATES successfully addresses the challenge of concurrent\nobservations caused by the task dependency inherent in the latency minimization\nobjective. Extensive tests show that SoCRATES outperforms other existing neural\nand non-neural schedulers with as high as 38% gain in latency reduction under a\nvariety of job types and incoming rates. The resulting model is also compact in\nsize and has very favorable energy consumption behaviors, making it highly\npractical for deployment in future SoC systems with built-in neural\naccelerator.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14354v3"
    },
    {
        "title": "Lightweight Robust Size Aware Cache Management",
        "authors": [
            "Gil Einziger",
            "Ohad Eytan",
            "Roy Friedman",
            "Benjamin Manes"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Modern key-value stores, object stores, Internet proxy caches, as well as\nContent Delivery Networks (CDN) often manage objects of diverse sizes, e.g.,\nblobs, video files of different lengths, images with varying resolution, and\nsmall documents. In such workloads, size-aware cache policies outperform\nsize-oblivious algorithms. Unfortunately, existing size-aware algorithms tend\nto be overly complicated and computationally~expensive.\n  Our work follows a more approachable pattern; we extend the prevalent\n(size-oblivious) TinyLFU cache admission policy to handle variable sized items.\nImplementing our approach inside two popular caching libraries only requires\nminor changes. We show that our algorithms yield competitive or better\nhit-ratios and byte hit-ratios compared to the state of the art size-aware\nalgorithms such as AdaptSize, LHD, LRB, and GDSF. Further, a runtime comparison\nindicates that our implementation is faster by up to x3 compared to the best\nalternative, i.e., it imposes much lower CPU overhead.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.08770v2"
    },
    {
        "title": "MAGE: Nearly Zero-Cost Virtual Memory for Secure Computation",
        "authors": [
            "Sam Kumar",
            "David E. Culler",
            "Raluca Ada Popa"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Secure Computation (SC) is a family of cryptographic primitives for computing\non encrypted data in single-party and multi-party settings. SC is being\nincreasingly adopted by industry for a variety of applications. A significant\nobstacle to using SC for practical applications is the memory overhead of the\nunderlying cryptography. We develop MAGE, an execution engine for SC that\nefficiently runs SC computations that do not fit in memory. We observe that,\ndue to their intended security guarantees, SC schemes are inherently oblivious\n-- their memory access patterns are independent of the input data. Using this\nproperty, MAGE calculates the memory access pattern ahead of time and uses it\nto produce a memory management plan. This formulation of memory management,\nwhich we call memory programming, is a generalization of paging that allows\nMAGE to provide a highly efficient virtual memory abstraction for SC. MAGE\noutperforms the OS virtual memory system by up to an order of magnitude, and in\nmany cases, runs SC computations that do not fit in memory at nearly the same\nspeed as if the underlying machines had unbounded physical memory to fit the\nentire computation.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.14651v2"
    },
    {
        "title": "Report on the \"The Future of the Shell\" Panel at HotOS 2021",
        "authors": [
            "Michael Greenberg",
            "Konstantinos Kallas",
            "Nikos Vasilakis",
            "Stephen Kell"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  This document summarizes the challenges and possible research directions\naround the shell and its ecosystem, collected during and after the HotOS21\nPanel on the future of the shell. The goal is to create a snapshot of what a\nnumber of researchers from various disciplines -- connected to the shell to\nvarying degrees -- think about its future. We hope that this document will\nserve as a reference for future research on the shell and its ecosystem.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.11016v1"
    },
    {
        "title": "SLO beyond the Hardware Isolation Limits",
        "authors": [
            "Haoran Qiu",
            "Yongzhou Chen",
            "Tianyin Xu",
            "Zbigniew T. Kalbarczyk",
            "Ravishankar K. Iyer"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Performance isolation is a keystone for SLO guarantees with shared resources\nin cloud and datacenter environments. To meet SLO requirements, the state of\nthe art relies on hardware QoS support (e.g., Intel RDT) to allocate shared\nresources such as last-level caches and memory bandwidth for co-located\nlatency-critical applications. As a result, the number of latency-critical\napplications that can be deployed on a physical machine is bounded by the\nhardware allocation capability. Unfortunately, such hardware capability is very\nlimited. For example, Intel Xeon E5 v3 processors support at most four\npartitions for last-level caches, i.e., at most four applications can have\ndedicated resource allocation. This paper discusses the feasibility and\nunexplored challenges of providing SLO guarantees beyond the limits of hardware\ncapability. We present CoCo to show the feasibility and the benefits. CoCo\nschedules applications to time-share interference-free partitions as a\ntransparent software layer. Our evaluation shows that CoCo outperforms\nnon-partitioned and round-robin approaches by up to 9x and 1.2x.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.11666v1"
    },
    {
        "title": "Minimum Viable Device Drivers for ARM TrustZone",
        "authors": [
            "Liwei Guo",
            "Felix Xiaozhu Lin"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  While TrustZone can isolate IO hardware, it lacks drivers for modern IO\ndevices. Rather than porting drivers, we propose a novel approach to deriving\nminimum viable drivers: developers exercise a full driver and record the\ndriver/device interactions; the processed recordings, dubbed driverlets, are\nreplayed in the TEE at run time to access IO devices.\n  Driverlets address two key challenges: correctness and expressiveness, for\nwhich they build on a key construct called interaction template. The\ninteraction template ensures faithful reproduction of recorded IO jobs (albeit\non new IO data); it accepts dynamic input values; it tolerates nondeterministic\ndevice behaviors.\n  We demonstrate driverlets on a series of sophisticated devices, making them\naccessible to TrustZone for the first time to our knowledge. Our experiments\nshow that driverlets are secure, easy to build, and incur acceptable overhead\n(1.4x -2.7x compared to native drivers). Driverlets fill a critical gap in the\nTrustZone TEE, realizing its long-promised vision of secure IO.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.08303v2"
    },
    {
        "title": "KML: Using Machine Learning to Improve Storage Systems",
        "authors": [
            "Ibrahim Umit Akgun",
            "Ali Selman Aydin",
            "Andrew Burford",
            "Michael McNeill",
            "Michael Arkhangelskiy",
            "Aadil Shaikh",
            "Lukas Velikov",
            "Erez Zadok"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Operating systems include many heuristic algorithms designed to improve\noverall storage performance and throughput. Because such heuristics cannot work\nwell for all conditions and workloads, system designers resorted to exposing\nnumerous tunable parameters to users -- thus burdening users with continually\noptimizing their own storage systems and applications. Storage systems are\nusually responsible for most latency in I/O-heavy applications, so even a small\nlatency improvement can be significant. Machine learning (ML) techniques\npromise to learn patterns, generalize from them, and enable optimal solutions\nthat adapt to changing workloads. We propose that ML solutions become a\nfirst-class component in OSs and replace manual heuristics to optimize storage\nsystems dynamically. In this paper, we describe our proposed ML architecture,\ncalled KML. We developed a prototype KML architecture and applied it to two\ncase studies: optimizing readahead and NFS read-size values. Our experiments\nshow that KML consumes less than 4KB of dynamic kernel memory, has a CPU\noverhead smaller than 0.2%, and yet can learn patterns and improve I/O\nthroughput by as much as 2.3x and 15x for two case studies -- even for complex,\nnever-seen-before, concurrently running mixed workloads on different storage\ndevices.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.11554v2"
    },
    {
        "title": "Bento and the Art of Repeated Research",
        "authors": [
            "Peter-Jan Gootzen",
            "Animesh Trivedi"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Bento provides a new approach to developing file systems, with safety and\nhigh-velocity development in mind. This is achieved by using Rust, a modern and\nmemory-safe systems programming language, and by providing a framework to run a\nsingle file system implementation in kernel space with the VFS or in user space\nwith FUSE. In this paper, the benchmarking experiments from the Bento paper are\nrepeated. We fail to exactly reproduce the results of the Bento paper, but more\nor less find the same patterns albeit with more outlying results. Additionally\nwe unsuccessfully run a standardized test suite, and expand the set of\nexperiments with latency benchmarks and throughput benchmarks using a RAM block\ndevice. The latency benchmarks show that ext4 with journaling consistently\noutperforms Bento-fs and the RAM throughput benchmarks show no additional\nconsistent performance pattern. During this experimentation, a set of 12 bugs\nwas encountered and analyzed. We find that the ratio of memory related bugs is\nlower than other systems programming projects that use C as opposed to Rust,\nthus supporting the claims of the Bento framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.06810v1"
    },
    {
        "title": "How ISO C became unusable for operating systems development",
        "authors": [
            "Victor Yodaiken"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  The C programming language was developed in the 1970s as a fairly\nunconventional systems and operating systems development tool, but has, through\nthe course of the ISO Standards process, added many attributes of more\nconventional programming languages and become less suitable for operating\nsystems development. Operating system programming continues to be done in\nnon-ISO dialects of C. The differences provide a glimpse of operating system\nrequirements for programming languages.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.07845v1"
    },
    {
        "title": "Adelie: Continuous Address Space Layout Re-randomization for Linux\n  Drivers",
        "authors": [
            "Ruslan Nikolaev",
            "Hassan Nadeem",
            "Cathlyn Stone",
            "Binoy Ravindran"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  While address space layout randomization (ASLR) has been extensively studied\nfor user-space programs, the corresponding OS kernel's KASLR support remains\nvery limited, making the kernel vulnerable to just-in-time (JIT)\nreturn-oriented programming (ROP) attacks. Furthermore, commodity OSs such as\nLinux restrict their KASLR range to 32 bits due to architectural constraints\n(e.g., x86-64 only supports 32-bit immediate operands for most instructions),\nwhich makes them vulnerable to even unsophisticated brute-force ROP attacks due\nto low entropy. Most in-kernel pointers remain static, exacerbating the problem\nwhen pointers are leaked.\n  Adelie, our kernel defense mechanism, overcomes KASLR limitations, increases\nKASLR entropy, and makes successful ROP attacks on the Linux kernel much harder\nto achieve. First, Adelie enables the position-independent code (PIC) model so\nthat the kernel and its modules can be placed anywhere in the 64-bit virtual\naddress space, at any distance apart from each other. Second, Adelie implements\nstack re-randomization and address encryption on modules. Finally, Adelie\nenables efficient continuous KASLR for modules by using the PIC model to make\nit (almost) impossible to inject ROP gadgets through these modules regardless\nof gadget's origin.\n  Since device drivers (typically compiled as modules) are often developed by\nthird parties and are typically less tested than core OS parts, they are also\noften more vulnerable. By fully re-randomizing device drivers, the last two\ncontributions together prevent most JIT ROP attacks since vulnerable modules\nare very likely to be a starting point of an attack. Furthermore, some OS\ninstances in virtualized environments are specifically designated to run device\ndrivers, where drivers are the primary target of JIT ROP attacks. Our\nevaluation shows high efficiency of Adelie's approach.\n  [full abstract is in the paper]\n",
        "pdf_link": "http://arxiv.org/pdf/2201.08378v1"
    },
    {
        "title": "Pond: CXL-Based Memory Pooling Systems for Cloud Platforms",
        "authors": [
            "Huaicheng Li",
            "Daniel S. Berger",
            "Stanko Novakovic",
            "Lisa Hsu",
            "Dan Ernst",
            "Pantea Zardoshti",
            "Monish Shah",
            "Samir Rajadnya",
            "Scott Lee",
            "Ishwar Agarwal",
            "Mark D. Hill",
            "Marcus Fontoura",
            "Ricardo Bianchini"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Public cloud providers seek to meet stringent performance requirements and\nlow hardware cost. A key driver of performance and cost is main memory. Memory\npooling promises to improve DRAM utilization and thereby reduce costs. However,\npooling is challenging under cloud performance requirements. This paper\nproposes Pond, the first memory pooling system that both meets cloud\nperformance goals and significantly reduces DRAM cost. Pond builds on the\nCompute Express Link (CXL) standard for load/store access to pool memory and\ntwo key insights. First, our analysis of cloud production traces shows that\npooling across 8-16 sockets is enough to achieve most of the benefits. This\nenables a small-pool design with low access latency. Second, it is possible to\ncreate machine learning models that can accurately predict how much local and\npool memory to allocate to a virtual machine (VM) to resemble same-NUMA-node\nmemory performance. Our evaluation with 158 workloads shows that Pond reduces\nDRAM costs by 7% with performance within 1-5% of same-NUMA-node VM allocations.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.00241v4"
    },
    {
        "title": "No Provisioned Concurrency: Fast RDMA-codesigned Remote Fork for\n  Serverless Computing",
        "authors": [
            "Xingda Wei",
            "Fangming Lu",
            "Tianxia Wang",
            "Jinyu Gu",
            "Yuhan Yang",
            "Rong Chen",
            "Haibo Chen"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Serverless platforms essentially face a tradeoff between container startup\ntime and provisioned concurrency (i.e., cached instances), which is further\nexaggerated by the frequent need for remote container initialization. This\npaper presents MITOSIS, an operating system primitive that provides fast remote\nfork, which exploits a deep codesign of the OS kernel with RDMA. By leveraging\nthe fast remote read capability of RDMA and partial state transfer across\nserverless containers, MITOSIS bridges the performance gap between local and\nremote container initialization. MITOSIS is the first to fork over 10,000 new\ncontainers from one instance across multiple machines within a second, while\nallowing the new containers to efficiently transfer the pre-materialized states\nof the forked one. We have implemented MITOSIS on Linux and integrated it with\nFN, a popular serverless platform. Under load spikes in real-world serverless\nworkloads, MITOSIS reduces the function tail latency by 89% with orders of\nmagnitude lower memory usage. For serverless workflow that requires state\ntransfer, MITOSIS improves its execution time by 86%.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.10225v3"
    },
    {
        "title": "Finding and Analyzing Crash-Consistency Bugs in Persistent-Memory File\n  Systems",
        "authors": [
            "Hayley LeBlanc",
            "Shankara Pailoor",
            "Isil Dillig",
            "James Bornholt",
            "Vijay Chidambaram"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  We present a study of crash-consistency bugs in persistent-memory (PM) file\nsystems and analyze their implications for file-system design and testing crash\nconsistency. We develop FlyTrap, a framework to test PM file systems for\ncrash-consistency bugs. FlyTrap discovered 18 new bugs across four PM file\nsystems; the bugs have been confirmed by developers and many have been already\nfixed. The discovered bugs have serious consequences such as breaking the\natomicity of rename or making the file system unmountable. We present a\ndetailed study of the bugs we found and discuss some important lessons from\nthese observations. For instance, one of our findings is that many of the bugs\nare due to logic errors, rather than errors in using flushes or fences; this\nhas important applications for future work on testing PM file systems. Another\nkey finding is that many bugs arise from attempts to improve efficiency by\nperforming metadata updates in-place and that recovery code that deals with\nrebuilding in-DRAM state is a significant source of bugs. These observations\nhave important implications for designing and testing PM file systems. Our code\nis available at https://github.com/utsaslab/flytrap .\n",
        "pdf_link": "http://arxiv.org/pdf/2204.06066v1"
    },
    {
        "title": "rgpdOS: GDPR Enforcement By The Operating System",
        "authors": [
            "Alain Tchana",
            "Raphael Colin",
            "Adrien Le Berre",
            "Vincent Berger",
            "Benoit Combemale",
            "Natacha Crooks",
            "Ludovic Pailler"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  The General Data Protection Regulation (GDPR) forces IT companies to comply\nwith a number of principles when dealing with European citizens' personal data.\nNon-compliant companies are exposed to penalties which may represent up to 4%\nof their turnover. Currently, it is very hard for companies driven by personal\ndata to make their applications GDPR-compliant, especially if those\napplications were developed before the GDPR was established. We present rgpdOS,\na GDPR-aware operating system that aims to bring GDPR-compliance to every\napplication, while requiring minimal changes to application code.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.10929v2"
    },
    {
        "title": "The Next-Generation OS Process Abstraction",
        "authors": [
            "Rodrigo Siqueira",
            "Nelson Lago",
            "Fabio Kon",
            "Dejan Milojičić"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Operating Systems are built upon a set of abstractions to provide resource\nmanagement and programming APIs for common functionality, such as\nsynchronization, communication, protection, and I/O. The process abstraction is\nthe bridge across these two aspects; unsurprisingly, research efforts pay\nparticular attention to the process abstraction, aiming at enhancing security,\nimproving performance, and supporting hardware innovations. However, given the\nintrinsic difficulties to implement modifications at the OS level, recent\nendeavors have not yet been widely adopted in production-oriented OSes. Still,\nwe believe the current hardware evolution and new application requirements\nprovide favorable conditions to change this trend. This paper evaluates recent\nresearch on OS process features identifying potential evolution paths. We\nderive a set of relevant process characteristics, and propose how to extend\nthem as to benefit OSes and applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.12270v1"
    },
    {
        "title": "Understanding NVMe Zoned Namespace (ZNS) Flash SSD Storage Devices",
        "authors": [
            "Nick Tehrany",
            "Animesh Trivedi"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  The standardization of NVMe Zoned Namespaces (ZNS) in the NVMe 2.0\nspecification presents a unique new addition to storage devices. Unlike\ntraditional SSDs, where the flash media management idiosyncrasies are hidden\nbehind a flash translation layer (FTL) inside the device, ZNS devices push\ncertain operations regarding data placement and garbage collection out from the\ndevice to the host. This allows the host to achieve more optimal data placement\nand predictable garbage collection overheads, along with lower device write\namplification. Thus, additionally increasing flash media lifetime. As a result,\nZNS devices are gaining significant attention in the research community.\n  However, with the current software stack there are numerous ways of\nintegrating ZNS devices into a host system. In this work, we begin to\nsystematically analyze the integration options, report on the current software\nsupport for ZNS devices in the Linux Kernel, and provide an initial set of\nperformance measurements. Our main findings show that larger I/O sizes are\nrequired to saturate the ZNS device bandwidth, and configuration of the I/O\nscheduler can provide workload dependent performance gains, requiring careful\nconsideration of ZNS integration and configuration depending on the application\nworkload and its access patterns. Our dataset and code are available at https:\n//github.com/nicktehrany/ZNS-Study.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.01547v1"
    },
    {
        "title": "Rapid Recovery of Program Execution Under Power Failures for Embedded\n  Systems with NVM",
        "authors": [
            "Min Jia",
            "Edwin Hsing. -M. Sha",
            "Qingfeng Zhuge",
            "Rui Xu",
            "Shouzhen Gu"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  After power is switched on, recovering the interrupted program from the\ninitial state can cause negative impact. Some programs are even unrecoverable.\nTo rapid recovery of program execution under power failures, the execution\nstates of checkpoints are backed up by NVM under power failures for embedded\nsystems with NVM. However, frequent checkpoints will shorten the lifetime of\nthe NVM and incur significant write overhead. In this paper, the technique of\ncheckpoint setting triggered by function calls is proposed to reduce the write\non NVM. The evaluation results show an average of 99.8% and 80.5$% reduction on\nNVM backup size for stack backup, compared to the log-based method and\nstep-based method. In order to better achieve this, we also propose\npseudo-function calls to increase backup points to reduce recovery costs, and\nexponential incremental call-based backup methods to reduce backup costs in the\nloop. To further avoid the content on NVM is cluttered and out of NVM, a method\nto clean the contents on the NVM that are useless for restoration is proposed.\nBased on aforementioned problems and techniques, the recovery technology is\nproposed, and the case is used to analyze how to recover rapidly under\ndifferent power failures.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.08826v1"
    },
    {
        "title": "MUSTACHE: Multi-Step-Ahead Predictions for Cache Eviction",
        "authors": [
            "Gabriele Tolomei",
            "Lorenzo Takanen",
            "Fabio Pinelli"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  In this work, we propose MUSTACHE, a new page cache replacement algorithm\nwhose logic is learned from observed memory access requests rather than fixed\nlike existing policies. We formulate the page request prediction problem as a\ncategorical time series forecasting task. Then, our method queries the learned\npage request forecaster to obtain the next $k$ predicted page memory references\nto better approximate the optimal B\\'el\\'ady's replacement algorithm. We\nimplement several forecasting techniques using advanced deep learning\narchitectures and integrate the best-performing one into an existing\nopen-source cache simulator. Experiments run on benchmark datasets show that\nMUSTACHE outperforms the best page replacement heuristic (i.e., exact LRU),\nimproving the cache hit ratio by 1.9% and reducing the number of reads/writes\nrequired to handle cache misses by 18.4% and 10.3%.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.02177v1"
    },
    {
        "title": "MeSHwA: The case for a Memory-Safe Software and Hardware Architecture\n  for Serverless Computing",
        "authors": [
            "Anjo Vahldiek-Oberwagner",
            "Mona Vij"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Motivated by developer productivity, serverless computing, and microservices\nhave become the de facto development model in the cloud. Microservices\ndecompose monolithic applications into separate functional units deployed\nindividually. This deployment model, however, costs CSPs a large infrastructure\ntax of more than 25%. To overcome these limitations, CSPs shift workloads to\nInfrastructure Processing Units (IPUs) like Amazon's Nitro or, complementary,\ninnovate by building on memory-safe languages and novel software abstractions.\n  Based on these trends, we hypothesize a \\arch providing a general-purpose\nruntime environment to specialize functionality when needed and strongly\nisolate components. To achieve this goal, we investigate building a single\naddress space OS or a multi-application library OS, possible hardware\nimplications, and demonstrate their capabilities, drawbacks and requirements.\nThe goal is to bring the advantages to all application workloads including\nlegacy and memory-unsafe applications, and analyze how hardware may improve the\nefficiency and security.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.08056v1"
    },
    {
        "title": "Automated Cache for Container Executables",
        "authors": [
            "Vanessa Sochat",
            "Matthieu Muffato",
            "Audrey Stott",
            "Marco De La Pierre",
            "Georgia Stuart"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Linux container technologies such as Docker and Singularity offer\nencapsulated environments for easy execution of software. In high performance\ncomputing, this is especially important for evolving and complex software\nstacks with conflicting dependencies that must co-exist. Singularity Registry\nHPC (\"shpc\") was created as an effort to install containers in this environment\nas modules, seamlessly allowing for typically hidden executables inside\ncontainers to be presented to the user as commands, and as such significantly\nsimplifying the user experience. A remaining challenge, however, is deriving\nthe list of important executables in the container. In this work, we present\nnew automation and methods that allow for not only discovering new containers\nin large community sets, but also deriving container entries with important\nexecutables. With this work we have added over 8,000 containers from the\nBioContainers community that can be maintained and updated by the software\nautomation over time. All software is publicly available on the GitHub\nplatform, and can be beneficial to container registries and infrastructure\nproviders for automatically generating container modules to lower the usage\nentry barrier and improve user experience.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.07376v1"
    },
    {
        "title": "MProtect: Operating System Memory Management without Access",
        "authors": [
            "Caihua Li",
            "Seung-seob Lee",
            "Min Hong Yun",
            "Lin Zhong"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Modern operating systems (OSes) have unfettered access to application data,\nassuming that applications trust them. This assumption, however, is problematic\nunder many scenarios where either the OS provider is not trustworthy or the OS\ncan be compromised due to its large attack surface. Our investigation began\nwith the hypothesis that unfettered access to memory is not fundamentally\nnecessary for the OS to perform its own job, including managing the memory. The\nresult is a system called MProtect that leverages a small piece of software\nrunning at a higher privilege level than the OS. MProtect protects the entire\nuser space of a process, requires only a small modification to the OS, and\nsupports major architectures such as ARM, x86 and RISC-V. Unlike prior works\nthat resorted to nested virtualization, which is often undesirable in mobile\nand embedded systems, MProtect mediates how the OS accesses the memory and\nhandles exceptions. We report an implementation of MProtect called MGuard with\nARMv8/Linux and evaluate its performance with both macro and microbenchmarks.\nWe show MGuard has a runtime TCB 2~3 times smaller than related systems and\nenjoys competitive performance while supporting legitimate OS access to the\nuser space.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.12671v1"
    },
    {
        "title": "Programmable System Call Security with eBPF",
        "authors": [
            "Jinghao Jia",
            "YiFei Zhu",
            "Dan Williams",
            "Andrea Arcangeli",
            "Claudio Canella",
            "Hubertus Franke",
            "Tobin Feldman-Fitzthum",
            "Dimitrios Skarlatos",
            "Daniel Gruss",
            "Tianyin Xu"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  System call filtering is a widely used security mechanism for protecting a\nshared OS kernel against untrusted user applications. However, existing system\ncall filtering techniques either are too expensive due to the context switch\noverhead imposed by userspace agents, or lack sufficient programmability to\nexpress advanced policies. Seccomp, Linux's system call filtering module, is\nwidely used by modern container technologies, mobile apps, and system\nmanagement services. Despite the adoption of the classic BPF language (cBPF),\nsecurity policies in Seccomp are mostly limited to static allow lists,\nprimarily because cBPF does not support stateful policies. Consequently, many\nessential security features cannot be expressed precisely and/or require kernel\nmodifications.\n  In this paper, we present a programmable system call filtering mechanism,\nwhich enables more advanced security policies to be expressed by leveraging the\nextended BPF language (eBPF). More specifically, we create a new Seccomp eBPF\nprogram type, exposing, modifying or creating new eBPF helper functions to\nsafely manage filter state, access kernel and user state, and utilize\nsynchronization primitives. Importantly, our system integrates with existing\nkernel privilege and capability mechanisms, enabling unprivileged users to\ninstall advanced filters safely. Our evaluation shows that our eBPF-based\nfiltering can enhance existing policies (e.g., reducing the attack surface of\nearly execution phase by up to 55.4% for temporal specialization), mitigate\nreal-world vulnerabilities, and accelerate filters.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.10366v1"
    },
    {
        "title": "Policy/mechanism separation in the Warehouse-Scale OS",
        "authors": [
            "Mark Mansi",
            "Michael M. Swift"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  \"As many of us know from bitter experience, the policies provided in extant\noperating systems, which are claimed to work well and behave fairly 'on the\naverage', often fail to do so in the special cases important to us\" [Wulf et\nal. 1974]. Written in 1974, these words motivated moving policy decisions into\nuser-space. Today, as warehouse-scale computers (WSCs) have become ubiquitous,\nit is time to move policy decisions away from individual servers altogether.\nBuilt-in policies are complex and often exhibit bad performance at scale.\nMeanwhile, the highly-controlled WSC setting presents opportunities to improve\nperformance and predictability.\n  We propose moving all policy decisions from the OS kernel to the cluster\nmanager (CM), in a new paradigm we call Grape CM. In this design, the role of\nthe kernel is reduced to monitoring, sending metrics to the CM, and executing\npolicy decisions made by the CM. The CM uses metrics from all kernels across\nthe WSC to make informed policy choices, sending commands back to each kernel\nin the cluster. We claim that Grape CM will improve performance, transparency,\nand simplicity. Our initial experiments show how the CM can identify the\noptimal set of huge pages for any workload or improve memcached latency by 15%.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.09725v1"
    },
    {
        "title": "NVMM cache design: Logging vs. Paging",
        "authors": [
            "Rémi Dulong",
            "Quentin Acher",
            "Baptiste Lepers",
            "Valerio Schiavoni",
            "Pascal Felber",
            "Gaël Thomas"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Modern NVMM is closing the gap between DRAM and persistent storage, both in\nterms of performance and features. Having both byte addressability and\npersistence on the same device gives NVMM an unprecedented set of features,\nleading to the following question: How should we design an NVMM-based caching\nsystem to fully exploit its potential? We build two caching mechanisms, NVPages\nand NVLog, based on two radically different design approaches. NVPages stores\nmemory pages in NVMM, similar to the Linux page cache (LPC). NVLog uses NVMM to\nstore a log of pending write operations to be submitted to the LPC, while it\nensures reads with a small DRAM cache. Our study shows and quantifies\nadvantages and flaws for both designs.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.02244v1"
    },
    {
        "title": "SWAM: Revisiting Swap and OOMK for Improving Application Responsiveness\n  on Mobile Devices",
        "authors": [
            "Geunsik Lim",
            "Donghyun Kang",
            "MyungJoo Ham",
            "Young Ik Eom"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Existing memory reclamation policies on mobile devices may be no longer valid\nbecause they have negative effects on the response time of running\napplications. In this paper, we propose SWAM, a new integrated memory\nmanagement technique that complements the shortcomings of both the swapping and\nkilling mechanism in mobile devices and improves the application\nresponsiveness. SWAM consists of (1) Adaptive Swap that performs swapping\nadaptively into memory or storage device while managing the swap space\ndynamically, (2) OOM Cleaner that reclaims shared object pages in the swap\nspace to secure available memory and storage space, and (3) EOOM Killer that\nterminates processes in the worst case while prioritizing the lowest\ninitialization cost applications as victim processes first. Experimental\nresults demonstrate that SWAM significantly reduces the number of applications\nkilled by OOMK (6.5x lower), and improves application launch time (36% faster)\nand response time (41% faster), compared to the conventional schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.08345v1"
    },
    {
        "title": "Unleashing Unprivileged eBPF Potential with Dynamic Sandboxing",
        "authors": [
            "Soo Yee Lim",
            "Xueyuan Han",
            "Thomas Pasquier"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  For safety reasons, unprivileged users today have only limited ways to\ncustomize the kernel through the extended Berkeley Packet Filter (eBPF). This\nis unfortunate, especially since the eBPF framework itself has seen an increase\nin scope over the years. We propose SandBPF, a software-based kernel isolation\ntechnique that dynamically sandboxes eBPF programs to allow unprivileged users\nto safely extend the kernel, unleashing eBPF's full potential. Our early\nproof-of-concept shows that SandBPF can effectively prevent exploits missed by\neBPF's native safety mechanism (i.e., static verification) while incurring\n0%-10% overhead on web server benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01983v2"
    },
    {
        "title": "Case Study: Securing MMU-less Linux Using CHERI",
        "authors": [
            "Hesham Almatary",
            "Alfredo Mazzinghi",
            "Robert N. M. Watson"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  MMU-less Linux variant lacks security because it does not have protection or\nisolation mechanisms. It also does not use MPUs as they do not fit with its\nsoftware model because of the design drawbacks of MPUs (\\ie coarse-grained\nprotection with fixed number of protected regions). We secure the existing\nMMU-less Linux version of the RISC-V port using CHERI. CHERI is a\nhardware-software capability-based system that extends the ISA, toolchain,\nprogramming languages, operating systems, and applications in order to provide\ncomplete pointer and memory safety. We believe that CHERI could provide\nsignificant security guarantees for high-end dynamic MMU-less embedded systems\nat lower costs, compared to MMUs and MPUs, by: 1) building the entire software\nstack in pure-capability CHERI C mode which provides complete spatial memory\nsafety at the kernel and user-level, 2) isolating user programs as separate\nELFs, each with its own CHERI-based capability table; this provides spatial\nmemory safety similar to what the MMU offers (\\ie user programs cannot access\neach other's memory), 3) isolating user programs from the kernel as the kernel\nhas its own capability table from the users and vice versa, and 4)\ncompartmentalising kernel modules using CompartOS' linkage-based\ncompartmentalisation. This offers a new security front that is not possible\nusing the current MMU-based Linux, where vulnerable/malicious kernel modules\n(\\eg device drivers) executing in the kernel space would not compromise or take\ndown the entire system. These are the four main contributions of this paper,\npresenting novel CHERI-based mechanisms to secure MMU-less embedded Linux.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.00933v2"
    },
    {
        "title": "A Survey of the Security Challenges and Requirements for IoT Operating\n  Systems",
        "authors": [
            "Alvi Jawad"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  The Internet of Things (IoT) is becoming an integral part of our modern lives\nas we converge towards a world surrounded by ubiquitous connectivity. The\ninherent complexity presented by the vast IoT ecosystem ends up in an\ninsufficient understanding of individual system components and their\ninteractions, leading to numerous security challenges. In order to create a\nsecure IoT platform from the ground up, there is a need for a unifying\noperating system (OS) that can act as a cornerstone regulating the development\nof stable and secure solutions. In this paper, we present a classification of\nthe security challenges stemming from the manifold aspects of IoT development.\nWe also specify security requirements to direct the secure development of an\nunifying IoT OS to resolve many of those ensuing challenges. Survey of several\nmodern IoT OSs confirm that while the developers of the OSs have taken many\nalternative approaches to implement security, we are far from engineering an\nadequately secure and unified architecture. More broadly, the study presented\nin this paper can help address the growing need for a secure and unified\nplatform to base IoT development on and assure the safe, secure, and reliable\noperation of IoT in critical domains.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.19825v1"
    },
    {
        "title": "Cascade: A Platform for Delay-Sensitive Edge Intelligence",
        "authors": [
            "Weijia Song",
            "Thiago Garrett",
            "Yuting Yang",
            "Mingzhao Liu",
            "Edward Tremel",
            "Lorenzo Rosa",
            "Andrea Merlina",
            "Roman Vitenberg",
            "Ken Birman"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Interactive intelligent computing applications are increasingly prevalent,\ncreating a need for AI/ML platforms optimized to reduce per-event latency while\nmaintaining high throughput and efficient resource management. Yet many\nintelligent applications run on AI/ML platforms that optimize for high\nthroughput even at the cost of high tail-latency. Cascade is a new AI/ML\nhosting platform intended to untangle this puzzle. Innovations include a\nlegacy-friendly storage layer that moves data with minimal copying and a \"fast\npath\" that collocates data and computation to maximize responsiveness. Our\nevaluation shows that Cascade reduces latency by orders of magnitude with no\nloss of throughput.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.17329v1"
    },
    {
        "title": "Stop Hiding The Sharp Knives: The WebAssembly Linux Interface",
        "authors": [
            "Arjun Ramesh",
            "Tianshu Huang",
            "Ben L. Titzer",
            "Anthony Rowe"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  WebAssembly is gaining popularity as a portable binary format targetable from\nmany programming languages. With a well-specified low-level virtual instruction\nset, minimal memory footprint and many high-performance implementations, it has\nbeen successfully adopted for lightweight in-process memory sandboxing in many\ncontexts. Despite these advantages, WebAssembly lacks many standard system\ninterfaces, making it difficult to reuse existing applications.\n  This paper proposes WALI: The WebAssembly Linux Interface, a thin layer over\nLinux's userspace system calls, creating a new class of virtualization where\nWebAssembly seamlessly interacts with native processes and the underlying\noperating system. By virtualizing the lowest level of userspace, WALI offers\napplication portability with little effort and reuses existing compiler\nbackends. With WebAssembly's control flow integrity guarantees, these modules\ngain an additional level of protection against remote code injection attacks.\nFurthermore, capability-based APIs can themselves be virtualized and\nimplemented in terms of WALI, improving reuse and robustness through better\nlayering. We present an implementation of WALI in a modern WebAssembly engine\nand evaluate its performance on a number of applications which we can now\ncompile with mostly trivial effort.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.03858v1"
    },
    {
        "title": "On a Foundation Model for Operating Systems",
        "authors": [
            "Divyanshu Saxena",
            "Nihal Sharma",
            "Donghyun Kim",
            "Rohit Dwivedula",
            "Jiayi Chen",
            "Chenxi Yang",
            "Sriram Ravula",
            "Zichao Hu",
            "Aditya Akella",
            "Sebastian Angel",
            "Joydeep Biswas",
            "Swarat Chaudhuri",
            "Isil Dillig",
            "Alex Dimakis",
            "P. Brighten Godfrey",
            "Daehyeok Kim",
            "Chris Rossbach",
            "Gang Wang"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  This paper lays down the research agenda for a domain-specific foundation\nmodel for operating systems (OSes). Our case for a foundation model revolves\naround the observations that several OS components such as CPU, memory, and\nnetwork subsystems are interrelated and that OS traces offer the ideal dataset\nfor a foundation model to grasp the intricacies of diverse OS components and\ntheir behavior in varying environments and workloads. We discuss a wide range\nof possibilities that then arise, from employing foundation models as policy\nagents to utilizing them as generators and predictors to assist traditional OS\ncontrol algorithms. Our hope is that this paper spurs further research into OS\nfoundation models and creating the next generation of operating systems for the\nevolving computing landscape.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.07813v1"
    },
    {
        "title": "Characterizing Physical Memory Fragmentation",
        "authors": [
            "Mark Mansi",
            "Michael M. Swift"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  External fragmentation of physical memory occurs when adjacent differently\nsized regions of allocated physical memory are freed at different times,\ncausing free memory to be physically discontiguous. It can significantly\ndegrade system performance and efficiency, such as reducing the ability to use\nhuge pages, a critical optimization on modern large-memory system. For decades\nsystem developers have sought to avoid and mitigate fragmentation, but few\nprior studies quantify and characterize it in production settings.\n  Moreover, prior work often artificially fragments physical memory to create\nmore realistic performance evaluations, but their fragmentation methodologies\nare ad hoc and unvalidated. Out of 13 papers, we found 11 different\nmethodologies, some of which were subsequently found inadequate. The importance\nof addressing fragmentation necessitates a validated and principled\nmethodology.\n  Our work fills these gaps in knowledge and methodology. We conduct a study of\nmemory fragmentation in production by observing 248 machines in the Computer\nSciences Department at University of Wisconsin - Madison for a week. We\nidentify six key memory usage patterns, and find that Linux's file cache and\npage reclamation systems are major contributors to fragmentation because they\noften obliviously break up contiguous memory. Finally, we create and\\'uril, a\ntool to artificially fragment memory during experimental research evaluations.\nWhile and\\'uril ultimately fails as a scientific tool, we discuss its design\nideas, merits, and failings in hope that they may inspire future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.03523v1"
    },
    {
        "title": "Herding LLaMaS: Using LLMs as an OS Module",
        "authors": [
            "Aditya K Kamath",
            "Sujay Yadalam"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Computer systems are becoming increasingly heterogeneous with the emergence\nof new memory technologies and compute devices. GPUs alongside CPUs have become\ncommonplace and CXL is poised to be a mainstay of cloud systems. The operating\nsystem is responsible for managing these hardware resources, requiring\nmodification every time a new device is released. Years of research and\ndevelopment are sunk into tuning the OS for high performance with each new\nheterogeneous device. With the recent explosion in memory technologies and\ndomain-specific accelerators, it would be beneficial to have an OS that could\nprovide high performance for new devices without significant effort.\n  We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large\nLanguage Models (LLMs) to extract the useful features of new devices from their\ntextual description and uses these features to make operating system decisions\nat runtime. Adding support to LLaMaS for a new device is as simple as\ndescribing the system and new device properties in plaintext.\n  LLaMaS reduces the burden on system administrators to enable easy integration\nof new devices into production systems.\n  Preliminary evaluation using ChatGPT shows that LLMs are capable of\nextracting device features from text and make correct OS decisions based on\nthose features.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.08908v1"
    },
    {
        "title": "A System-Level Dynamic Binary Translator using Automatically-Learned\n  Translation Rules",
        "authors": [
            "Jinhu Jiang",
            "Chaoyi Liang",
            "Rongchao Dong",
            "Zhaohui Yang",
            "Zhongjun Zhou",
            "Wenwen Wang",
            "Pen-Chung Yew",
            "Weihua Zhang"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  System-level emulators have been used extensively for system design,\ndebugging and evaluation. They work by providing a system-level virtual machine\nto support a guest operating system (OS) running on a platform with the same or\ndifferent native OS that uses the same or different instruction-set\narchitecture. For such system-level emulation, dynamic binary translation (DBT)\nis one of the core technologies. A recently proposed learning-based DBT\napproach has shown a significantly improved performance with a higher quality\nof translated code using automatically learned translation rules. However, it\nhas only been applied to user-level emulation, and not yet to system-level\nemulation. In this paper, we explore the feasibility of applying this approach\nto improve system-level emulation, and use QEMU to build a prototype. ... To\nachieve better performance, we leverage several optimizations that include\ncoordination overhead reduction to reduce the overhead of each coordination,\nand coordination elimination and code scheduling to reduce the coordination\nfrequency. Experimental results show that it can achieve an average of 1.36X\nspeedup over QEMU 6.1 with negligible coordination overhead in the system\nemulation mode using SPEC CINT2006 as application benchmarks and 1.15X on\nreal-world applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09688v1"
    },
    {
        "title": "THEMIS: Time, Heterogeneity, and Energy Minded Scheduling for Fair\n  Multi-Tenant Use in FPGAs",
        "authors": [
            "Emre Karabulut",
            "Arsalan Ali Malik",
            "Amro Awad",
            "Aydin Aysu"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Using correct design metrics and understanding the limitations of the\nunderlying technology is critical to developing effective scheduling\nalgorithms. Unfortunately, existing scheduling techniques used \\emph{incorrect}\nmetrics and had \\emph{unrealistic} assumptions for fair scheduling of\nmulti-tenant FPGAs where each tenant is aimed to share approximately the same\nnumber of resources both spatially and temporally.\n  This paper introduces an enhanced fair scheduling algorithm for multi-tenant\nFPGA use, addressing previous metric and assumption issues, with three specific\nimprovements claimed First, our method ensures spatiotemporal fairness by\nconsidering both spatial and temporal aspects, addressing the limitation of\nprior work that assumed uniform task latency. Second, we incorporate energy\nconsiderations into fairness by adjusting scheduling intervals and accounting\nfor energy overhead, thereby balancing energy efficiency with fairness. Third,\nwe acknowledge overlooked aspects of FPGA multi-tenancy, including\nheterogeneous regions and the constraints on dynamically merging/splitting\npartially reconfigurable regions. We develop and evaluate our improved fair\nscheduling algorithm with these three enhancements. Inspired by the Greek\ngoddess of law and personification of justice, we name our fair scheduling\nsolution THEMIS: \\underline{T}ime, \\underline{H}eterogeneity, and\n\\underline{E}nergy \\underline{Mi}nded \\underline{S}cheduling.\n  We used the Xilinx Zedboard XC7Z020 to quantify our approach's savings.\nCompared to previous algorithms, our improved scheduling algorithm enhances\nfairness between 24.2--98.4\\% and allows a trade-off between 55.3$\\times$ in\nenergy vs. 69.3$\\times$ in fairness. The paper thus informs cloud providers\nabout future scheduling optimizations for fairness with related challenges and\nopportunities.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.00507v1"
    },
    {
        "title": "Potential of WebAssembly for Embedded Systems",
        "authors": [
            "Stefan Wallentowitz",
            "Bastian Kersting",
            "Dan Mihai Dumitriu"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Application virtual machines provide strong isolation properties and are\nestablished in the context of software portability. Those opportunities make\nthem interesting for scalable and secure IoT deployments. WebAssembly is an\napplication virtual machine with origins in web browsers, that is getting\nrapidly adopted in other domains. The strong and steadily growing ecosystem\nmakes WebAssembly an interesting candidate for Embedded Systems. This position\npaper discusses the usage of WebAssembly in Embedded Systems. After introducing\nthe basic concepts of WebAssembly and existing runtime environments, we give an\noverview of the challenges for the efficient usage of WebAssembly in Embedded\nSystems. The paper concludes with a real world case study that demonstrates the\nviability, before giving an outlook on open issues and upcoming work.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.09213v1"
    },
    {
        "title": "Integrating Artificial Intelligence into Operating Systems: A\n  Comprehensive Survey on Techniques, Applications, and Future Directions",
        "authors": [
            "Yifan Zhang",
            "Xinkui Zhao",
            "Ziying Li",
            "Jianwei Yin",
            "Lufei Zhang",
            "Zuoning Chen"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  In the era of the Internet of Everything, operating systems (OSs) face\nunprecedented challenges posed by an evolving application landscape and\nincreasingly heterogeneous hardware ecosystems. This shift toward increasingly\ndynamic and unpredictable operational contexts presents significant challenges\nfor both OS developers and users. Against this backdrop, the fusion of\nArtificial Intelligence (AI) with Operating Systems emerges as a critical\nfrontier for innovation. This survey delves into the intricate interplay\nbetween AI and OSs, illustrating how existing OS mechanisms combined with AI\nsignificantly elevate the performance, security, and efficiency of modern\noperating systems. We investigate a range of AI methodologies applied to\noptimize core OS functionalities and clarify the correlation to related\nstudies. Our analysis touches on the existing hurdles and prospective avenues\nin this interdisciplinary domain, underscoring the imperative for robust and\nseamless integration of AI capabilities into OS architectures.\n  Through an examination of illustrative case studies and cutting-edge\ndevelopments, we offer a thorough review of the current status of AI-OS\nintegration, accentuating its pivotal role in steering the evolution of\nadvanced computing paradigms. We also envision the promising prospects of\nIntelligent Operating Systems, debating how groundbreaking OS designs will\nusher in novel possibilities and highlight the central role that AI will assume\nin propelling these next-generation systems forward. This forward-thinking\noutlook illuminates the profound influence of AI on the foundational elements\nof computing, heralding the advent of a new epoch characterized by intelligent,\nself-adapting, and highly adaptive software ecosystems.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.14567v2"
    },
    {
        "title": "Skip TLB flushes for reused pages within mmap's",
        "authors": [
            "Frederic Schimmelpfennig",
            "André Brinkmann",
            "Hossein Asadi",
            "Reza Salkhordeh"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.10946v1"
    },
    {
        "title": "eBPF-mm: Userspace-guided memory management in Linux with eBPF",
        "authors": [
            "Konstantinos Mores",
            "Stratos Psomadakis",
            "Georgios Goumas"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  We leverage eBPF in order to implement custom policies in the Linux memory\nsubsystem. Inspired by CBMM, we create a mechanism that provides the kernel\nwith hints regarding the benefit of promoting a page to a specific size. We\nintroduce a new hook point in Linux page fault handling path for eBPF programs,\nproviding them the necessary context to determine the page size to be used. We\nthen develop a framework that allows users to define profiles for their\napplications and load them into the kernel. A profile consists of memory\nregions of interest and their expected benefit from being backed by 4KB, 64KB\nand 2MB pages. In our evaluation, we profiled our workloads to identify hot\nmemory regions using DAMON.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.11220v1"
    },
    {
        "title": "Energy-Efficient Computation with DVFS using Deep Reinforcement Learning\n  for Multi-Task Systems in Edge Computing",
        "authors": [
            "Xinyi Li",
            "Ti Zhou",
            "Haoyu Wang",
            "Man Lin"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Periodic soft real-time systems have broad applications in many areas, such\nas IoT. Finding an optimal energy-efficient policy that is adaptable to\nunderlying edge devices while meeting deadlines for tasks has always been\nchallenging. This research studies generalized systems with multi-task,\nmulti-deadline scenarios with reinforcement learning-based DVFS for energy\nsaving. This work addresses the limitation of previous work that models a\nperiodic system as a single task and single-deadline scenario, which is too\nsimplified to cope with complex situations. The method encodes time series\ninformation in the Linux kernel into information that is easy to use for\nreinforcement learning, allowing the system to generate DVFS policies to adapt\nsystem patterns based on the general workload. For encoding, we present two\ndifferent methods for comparison. Both methods use only one performance\ncounter: system utilization and the kernel only needs minimal information from\nthe userspace. Our method is implemented on Jetson Nano Board (2GB) and is\ntested with three fixed multitask workloads, which are three, five, and eight\ntasks in the workload, respectively. For randomness and generalization, we also\ndesigned a random workload generator to build different multitask workloads to\ntest. Based on the test results, our method could save 3%-10% power compared to\nLinux built-in governors.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.19434v2"
    },
    {
        "title": "The eBPF Runtime in the Linux Kernel",
        "authors": [
            "Bolaji Gbadamosi",
            "Luigi Leonardi",
            "Tobias Pulls",
            "Toke Høiland-Jørgensen",
            "Simone Ferlin-Reiter",
            "Simo Sorce",
            "Anna Brunström"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Extended Berkeley Packet Filter (eBPF) is a runtime that enables users to\nload programs into the operating system (OS) kernel, like Linux or Windows, and\nexecute them safely and efficiently at designated kernel hooks. Each program\npasses through a verifier that reasons about the safety guarantees for\nexecution. Hosting a safe virtual machine runtime within the kernel makes it\ndynamically programmable. Unlike the popular approach of bypassing or\ncompletely replacing the kernel, eBPF gives users the flexibility to modify the\nkernel on the fly, rapidly experiment and iterate, and deploy solutions to\nachieve their workload-specific needs, while working in concert with the\nkernel.\n  In this paper, we present the first comprehensive description of the design\nand implementation of the eBPF runtime in the Linux kernel. We argue that eBPF\ntoday provides a mature and safe programming environment for the kernel. It has\nseen wide adoption since its inception and is increasingly being used not just\nto extend, but program entire components of the kernel, while preserving its\nruntime integrity. We outline the compelling advantages it offers for\nreal-world production usage, and illustrate current use cases. Finally, we\nidentify its key challenges, and discuss possible future directions.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.00026v2"
    },
    {
        "title": "SJMalloc: the security-conscious, fast, thread-safe and memory-efficient\n  heap allocator",
        "authors": [
            "Stephan Bauroth"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Heap-based exploits that leverage memory management errors continue to pose a\nsignificant threat to application security. The root cause of these\nvulnerabilities are the memory management errors within the applications,\nhowever various hardened allocator designs have been proposed as mitigation. A\ncommon feature of these designs is the strategic decision to store heap\nmetadata separately from the application data in use, thereby reducing the risk\nof metadata corruption leading to security breaches. Despite their potential\nbenefits, hardened allocators have not been widely adopted in real-world\napplications. The primary barrier to their adoption is the performance\noverheads they introduce. These overheads can negatively impact the efficiency\nand speed of applications, which is a critical consideration for developers and\nsystem administrators. Having learned from previous implementations, we\ndeveloped SJMalloc, a general-purpose, high-performance allocator that\naddresses these concerns. SJMalloc stores its metadata out-of-band, away from\nthe application's data on the heap. This design choice not only enhances\nsecurity but also improves performance. Across a variety of real-world\nworkloads, SJMalloc demonstrates a ~6% performance improvement compared to\nGLibcs allocator, while using only ~5% more memory. Furthermore, SJMalloc\nsuccessfully passes the generic elements of the GLibc malloc testsuite and can\nthus be used as a drop-in replacement for the standard allocator, offering an\neasy upgrade path for enhanced security and performance without requiring\nchanges to existing applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.17928v1"
    },
    {
        "title": "Microsecond-scale Dynamic Validation of Idempotency for GPU Kernels",
        "authors": [
            "Mingcong Han",
            "Weihang Shen",
            "Guanwen Peng",
            "Rong Chen",
            "Haibo Chen"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  We discovered that a GPU kernel can have both idempotent and non-idempotent\ninstances depending on the input. These kernels, called\nconditionally-idempotent, are prevalent in real-world GPU applications (490 out\nof 547 from six applications). Consequently, prior work that classifies GPU\nkernels as either idempotent or non-idempotent can severely compromise the\ncorrectness or efficiency of idempotence-based systems. This paper presents\nPICKER, the first system for instance-level idempotency validation. PICKER\ndynamically validates the idempotency of GPU kernel instances before their\nexecution, by utilizing their launch arguments. Several optimizations are\nproposed to significantly reduce validation latency to microsecond-scale.\nEvaluations using representative GPU applications (547 kernels and 18,217\ninstances in total) show that PICKER can identify idempotent instances with no\nfalse positives and a false-negative rate of 18.54%, and can complete the\nvalidation within 5 us for all instances. Furthermore, by integrating PICKER, a\nfault-tolerant system can reduce the checkpoint cost to less than 4% and a\nscheduling system can reduce the preemption latency by 84.2%.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.23661v1"
    },
    {
        "title": "Enhancing Adaptive Mixed-Criticality Scheduling with Deep Reinforcement\n  Learning",
        "authors": [
            "Bruno Mendes",
            "Pedro F. Souto",
            "Pedro C. Diniz"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Adaptive Mixed-Criticality (AMC) is a fixed-priority preemptive scheduling\nalgorithm for mixed-criticality hard real-time systems. It dominates many other\nscheduling algorithms for mixed-criticality systems, but does so at the cost of\noccasionally dropping jobs of less important/critical tasks, when low-priority\njobs overrun their time budgets. In this paper we enhance AMC with a deep\nreinforcement learning (DRL) approach based on a Deep-Q Network. The DRL agent\nis trained off-line, and at run-time adjusts the low-criticality budgets of\ntasks to avoid budget overruns, while ensuring that no job misses its deadline\nif it does not overrun its budget. We have implemented and evaluated this\napproach by simulating realistic workloads from the automotive domain. The\nresults show that the agent is able to reduce budget overruns by at least up to\n50%, even when the budget of each task is chosen based on sampling the\ndistribution of its execution time. To the best of our knowledge, this is the\nfirst use of DRL in AMC reported in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.00572v1"
    },
    {
        "title": "Mercury: QoS-Aware Tiered Memory System",
        "authors": [
            "Jiaheng Lu",
            "Yiwen Zhang",
            "Hasan Al Maruf",
            "Minseo Park",
            "Yunxuan Tang",
            "Fan Lai",
            "Mosharaf Chowdhury"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Memory tiering has received wide adoption in recent years as an effective\nsolution to address the increasing memory demands of memory-intensive\nworkloads. However, existing tiered memory systems often fail to meet\nservice-level objectives (SLOs) when multiple applications share the system\nbecause they lack Quality-of-Service (QoS) support. Consequently, applications\nsuffer severe performance drops due to local memory contention and memory\nbandwidth interference.\n  In this paper, we present Mercury, a QoS-aware tiered memory system that\nensures predictable performance for coexisting memory-intensive applications\nwith different SLOs. Mercury enables per-tier page reclamation for\napplication-level resource management and uses a proactive admission control\nalgorithm to satisfy SLOs via per-tier memory capacity allocation and intra-\nand inter-tier bandwidth interference mitigation. It reacts to dynamic\nrequirement changes via real-time adaptation. Extensive evaluations show that\nMercury improves application performance by up to 53.4% and 20.3% compared to\nTPP and Colloid, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.08938v1"
    },
    {
        "title": "Optimizing System Memory Bandwidth with Micron CXL Memory Expansion\n  Modules on Intel Xeon 6 Processors",
        "authors": [
            "Rohit Sehgal",
            "Vishal Tanna",
            "Vinicius Petrucci",
            "Anil Godbole"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  High-Performance Computing (HPC) and Artificial Intelligence (AI) workloads\ntypically demand substantial memory bandwidth and, to a degree, memory\ncapacity. CXL memory expansion modules, also known as CXL \"type-3\" devices,\nenable enhancements in both memory capacity and bandwidth for server systems by\nutilizing the CXL protocol which runs over the PCIe interfaces of the\nprocessor. This paper discusses experimental findings on achieving increased\nmemory bandwidth for HPC and AI workloads using Micron's CXL modules. This is\nthe first study that presents real data experiments utilizing eight CXL E3.S\n(x8) Micron CZ122 devices on the Intel Xeon 6 processor 6900P (previously\ncodenamed Granite Rapids AP) featuring 128 cores, alongside Micron DDR-5 memory\noperating at 6400 MT/s on each of the CPU's 12 DRAM channels. The eight CXL\nmemories were set up as a unified NUMA configuration, employing software-based\npage level interleaving mechanism, available in Linux kernel v6.9+, between\nDDR5 and CXL memory nodes to improve overall system bandwidth. Memory expansion\nvia CXL boosts read-only bandwidth by 24% and mixed read/write bandwidth by up\nto 39%. Across HPC and AI workloads, the geometric mean of performance speedups\nis 24%.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.12491v1"
    },
    {
        "title": "Fast, Secure, Adaptable: LionsOS Design, Implementation and Performance",
        "authors": [
            "Gernot Heiser",
            "Ivan Velickovic",
            "Peter Chubb",
            "Alwin Joshy",
            "Anuraag Ganesh",
            "Bill Nguyen",
            "Cheng Li",
            "Courtney Darville",
            "Guangtao Zhu",
            "James Archer",
            "Jingyao Zhou",
            "Krishnan Winter",
            "Lucy Parker",
            "Szymon Duchniewicz",
            "Tianyi Bai"
        ],
        "category": "cs.OS",
        "published_year": "2025",
        "summary": "  We present LionsOS, an operating system for security- and safety-critical\nembedded systems. LionsOS is based on the formally verified seL4 microkernel\nand designed with verification in mind. It uses a static architecture and\nfeatures a highly modular design driven by strict separation of concerns and a\nfocus on simplicity. We demonstrate that LionsOS outperforms Linux.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.06234v1"
    },
    {
        "title": "Symbol Resolution MatRs: Make it Fast and Observable with Stable Linking",
        "authors": [
            "Farid Zakaria",
            "Andrew Quinn",
            "Thomas R. W. Scogland"
        ],
        "category": "cs.OS",
        "published_year": "2025",
        "summary": "  Dynamic linking is the standard mechanism for using external dependencies\nsince it enables code reuse, streamlines software updates, and reduces\ndisk/network use. Dynamic linking waits until runtime to calculate an\napplication's relocation mapping, i.e., the mapping between each externally\nreferenced symbol in the application to the dependency that provides the\nsymbol. Unfortunately, it comes with two downsides. First, dynamic linking\nlimits the performance of current systems since it can take seconds to\ncalculate a relocation mapping for a large program. Second, dynamic linking\nlimits the dependency management of applications since it prevents a developer\nfrom accurately observing a relocation mapping except at runtime.\n  This paper makes the key insight that the benefits conventionally attributed\nto dynamic linking: code reuse, streamlined software updates, and reduced\ndisk/network use are actually benefits of shared libraries. Thus, we present\nstable linking, a new mechanism for using dependencies that uses shared\nlibraries to retain their benefits but eliminates the downsides of dynamic\nlinking. Stable linking separates a system's state into management times; when\nthe system can be modified, and epochs when it cannot. Stable linking\ncalculates each application's relocation mapping at the beginning of each\nepoch, allows developers to inspect the relocation mapping during the epoch,\nand reuses the mapping for subsequent executions in the epoch. We design and\nbuild MatR, the first stable linker. We use MatR in three workloads and show\nthat it improves upon dynamic linking performance by a factor of 2.19 on\naverage. Additionally, we use the system in three vignettes, or case-studies,\nthat illustrate the system's improvements to dependency management.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.06716v1"
    },
    {
        "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
        "authors": [
            "Robert Gifford",
            "Abby Eisenklam",
            "Georgiy A. Bondar",
            "Yifan Cai",
            "Tushar Sial",
            "Linh Thi Xuan Phan",
            "Abhishek Halder"
        ],
        "category": "cs.OS",
        "published_year": "2025",
        "summary": "  As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.08484v1"
    },
    {
        "title": "A Hardware Time Manager Implementation for the Xenomai Real-Time Kernel\n  of Embedded Linux",
        "authors": [
            "Pierre Olivier",
            "Jalil Boukhobza"
        ],
        "category": "cs.OS",
        "published_year": "2012",
        "summary": "  Nowadays, the use of embedded operating systems in different embedded\nprojects is subject to a tremendous growth. Embedded Linux is becoming one of\nthose most popular EOSs due to its modularity, efficiency, reliability, and\ncost. One way to make it hard real-time is to include a real-time kernel like\nXenomai. One of the key characteristics of a Real-Time Operating System (RTOS)\nis its ability to meet execution time deadlines deterministically. So, the more\nprecise and flexible the time management can be, the better it can handle\nefficiently the determinism for different embedded applications. RTOS time\nprecision is characterized by a specific periodic interrupt service controlled\nby a software time manager. The smaller the period of the interrupt, the better\nthe precision of the RTOS, the more it overloads the CPU, and though reduces\nthe overall efficiency of the RTOS. In this paper, we propose to drastically\nreduce these overheads by migrating the time management service of Xenomai into\na configurable hardware component to relieve the CPU. The hardware component is\nimplemented in a Field Programmable Gate Array coupled to the CPU. This work\nwas achieved in a Master degree project where students could apprehend many\nfields of embedded systems: RTOS programming, hardware design, performance\nevaluation, etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.6428v1"
    },
    {
        "title": "Performance Evaluation of Java File Security System (JFSS)",
        "authors": [
            "Brijender Kahanwal",
            "Dr. Tejinder Pal Singh",
            "Dr. R. K. Tuteja"
        ],
        "category": "cs.OS",
        "published_year": "2013",
        "summary": "  Security is a critical issue of the modern file and storage systems, it is\nimperative to protect the stored data from unauthorized access. We have\ndeveloped a file security system named as Java File Security System (JFSS) [1]\nthat guarantee the security to files on the demand of all users. It has been\ndeveloped on Java platform. Java has been used as programming language in order\nto provide portability, but it enforces some performance limitations. It is\ndeveloped in FUSE (File System in User space) [3]. Many efforts have been done\nover the years for developing file systems in user space (FUSE). All have their\nown merits and demerits. In this paper we have evaluated the performance of\nJava File Security System (JFSS). Over and over again, the increased security\ncomes at the expense of user convenience, performance or compatibility with\nother systems. JFSS system performance evaluations show that encryption\noverheads are modest as compared to security.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.3686v1"
    },
    {
        "title": "Energy-Efficient Scheduling for Homogeneous Multiprocessor Systems",
        "authors": [
            "Mason Thammawichai",
            "Eric C. Kerrigan"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  We present a number of novel algorithms, based on mathematical optimization\nformulations, in order to solve a homogeneous multiprocessor scheduling\nproblem, while minimizing the total energy consumption. In particular, for a\nsystem with a discrete speed set, we propose solving a tractable linear\nprogram. Our formulations are based on a fluid model and a global scheduling\nscheme, i.e. tasks are allowed to migrate between processors. The new methods\nare compared with three global energy/feasibility optimal workload allocation\nformulations. Simulation results illustrate that our methods achieve both\nfeasibility and energy optimality and outperform existing methods for\nconstrained deadline tasksets. Specifically, the results provided by our\nalgorithm can achieve up to an 80% saving compared to an algorithm without a\nfrequency scaling scheme and up to 70% saving compared to a constant frequency\nscaling scheme for some simulated tasksets. Another benefit is that our\nalgorithms can solve the scheduling problem in one step instead of using a\nrecursive scheme. Moreover, our formulations can solve a more general class of\nscheduling problems, i.e. any periodic real-time taskset with arbitrary\ndeadline. Lastly, our algorithms can be applied to both online and offline\nscheduling schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.05567v2"
    },
    {
        "title": "Feedback Scheduling for Energy-Efficient Real-Time Homogeneous\n  Multiprocessor Systems",
        "authors": [
            "Mason Thammawichai",
            "Eric C. Kerrigan"
        ],
        "category": "cs.OS",
        "published_year": "2016",
        "summary": "  Real-time scheduling algorithms proposed in the literature are often based on\nworst-case estimates of task parameters. The performance of an open-loop scheme\ncan be degraded significantly if there are uncertainties in task parameters,\nsuch as the execution times of the tasks. Therefore, to cope with such a\nsituation, a closed-loop scheme, where feedback is exploited to adjust the\nsystem parameters, can be applied. We propose an optimal control framework that\ntakes advantage of feeding back information of finished tasks to solve a\nreal-time multiprocessor scheduling problem with uncertainty in task execution\ntimes, with the objective of minimizing the total energy consumption.\nSpecifically, we propose a linear programming based algorithm to solve a\nworkload partitioning problem and adopt McNaughton's wrap around algorithm to\nfind the task execution order. The simulation results illustrate that our\nfeedback scheduling algorithm can save energy by as much as 40% compared to an\nopen-loop method for two processor models, i.e. a PowerPC 405LP and an XScale\nprocessor.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.02635v1"
    },
    {
        "title": "Performance Evaluation of Container-based Virtualization for High\n  Performance Computing Environments",
        "authors": [
            "Carlos Arango",
            "Rémy Dernat",
            "John Sanabria"
        ],
        "category": "cs.OS",
        "published_year": "2017",
        "summary": "  Virtualization technologies have evolved along with the development of\ncomputational environments since virtualization offered needed features at that\ntime such as isolation, accountability, resource allocation, resource fair\nsharing and so on. Novel processor technologies bring to commodity computers\nthe possibility to emulate diverse environments where a wide range of\ncomputational scenarios can be run. Along with processors evolution, system\ndevelopers have created different virtualization mechanisms where each new\ndevelopment enhanced the performance of previous virtualized environments.\nRecently, operating system-based virtualization technologies captured the\nattention of communities abroad (from industry to academy and research) because\ntheir important improvements on performance area.\n  In this paper, the features of three container-based operating systems\nvirtualization tools (LXC, Docker and Singularity) are presented. LXC, Docker,\nSingularity and bare metal are put under test through a customized single node\nHPL-Benchmark and a MPI-based application for the multi node testbed. Also the\ndisk I/O performance, Memory (RAM) performance, Network bandwidth and GPU\nperformance are tested for the COS technologies vs bare metal. Preliminary\nresults and conclusions around them are presented and discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10140v1"
    },
    {
        "title": "DEAP Cache: Deep Eviction Admission and Prefetching for Cache",
        "authors": [
            "Ayush Mangal",
            "Jitesh Jain",
            "Keerat Kaur Guliani",
            "Omkar Bhalerao"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Recent approaches for learning policies to improve caching, target just one\nout of the prefetching, admission and eviction processes. In contrast, we\npropose an end to end pipeline to learn all three policies using machine\nlearning. We also take inspiration from the success of pretraining on large\ncorpora to learn specialized embeddings for the task. We model prefetching as a\nsequence prediction task based on past misses. Following previous works\nsuggesting that frequency and recency are the two orthogonal fundamental\nattributes for caching, we use an online reinforcement learning technique to\nlearn the optimal policy distribution between two orthogonal eviction\nstrategies based on them. While previous approaches used the past as an\nindicator of the future, we instead explicitly model the future frequency and\nrecency in a multi-task fashion with prefetching, leveraging the abilities of\ndeep networks to capture futuristic trends and use them for learning eviction\nand admission. We also model the distribution of the data in an online fashion\nusing Kernel Density Estimation in our approach, to deal with the problem of\ncaching non-stationary data. We present our approach as a \"proof of concept\" of\nlearning all three components of cache strategies using machine learning and\nleave improving practical deployment for future work.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09206v1"
    },
    {
        "title": "LazyFP: Leaking FPU Register State using Microarchitectural\n  Side-Channels",
        "authors": [
            "Julian Stecklina",
            "Thomas Prescher"
        ],
        "category": "cs.OS",
        "published_year": "2018",
        "summary": "  Modern processors utilize an increasingly large register set to facilitate\nefficient floating point and SIMD computation. This large register set is a\nburden for operating systems, as its content needs to be saved and restored\nwhen the operating system context switches between tasks. As an optimization,\nthe operating system can defer the context switch of the FPU and SIMD register\nset until the first instruction is executed that needs access to these\nregisters. Meanwhile, the old content is left in place with the hope that the\ncurrent task might not use these registers at all. This optimization is\ncommonly called lazy FPU context switching. To make it possible, a processor\noffers the ability to toggle the availability of instructions utilizing\nfloating point and SIMD registers. If the instructions are turned off, any\nattempt of executing them will generate a fault.\n  In this paper, we present an attack that exploits lazy FPU context switching\nand allows an adversary to recover the FPU and SIMD register set of arbitrary\nprocesses or VMs. The attack works on processors that transiently execute FPU\nor SIMD instructions that follow an instruction generating the fault indicating\nthe first use of FPU or SIMD instructions. On operating systems using lazy FPU\ncontext switching, the FPU and SIMD register content of other processes or\nvirtual machines can then be reconstructed via cache side effects.\n  With SIMD registers not only being used for cryptographic computation, but\nalso increasingly for simple operations, such as copying memory, we argue that\nlazy FPU context switching is a dangerous optimization that needs to be turned\noff in all operating systems, if there is a chance that they run on affected\nprocessors.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07480v1"
    },
    {
        "title": "Occlum: Secure and Efficient Multitasking Inside a Single Enclave of\n  Intel SGX",
        "authors": [
            "Youren Shen",
            "Hongliang Tian",
            "Yu Chen",
            "Kang Chen",
            "Runji Wang",
            "Yi Xu",
            "Yubin Xia"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Intel Software Guard Extensions (SGX) enables user-level code to create\nprivate memory regions called enclaves, whose code and data are protected by\nthe CPU from software and hardware attacks outside the enclaves. Recent work\nintroduces library operating systems (LibOSes) to SGX so that legacy\napplications can run inside enclaves with few or even no modifications. As\nvirtually any non-trivial application demands multiple processes, it is\nessential for LibOSes to support multitasking. However, none of the existing\nSGX LibOSes support multitasking both securely and efficiently.\n  This paper presents Occlum, a system that enables secure and efficient\nmultitasking on SGX. We implement the LibOS processes as SFI-Isolated Processes\n(SIPs). SFI is a software instrumentation technique for sandboxing untrusted\nmodules (called domains). We design a novel SFI scheme named MPX-based,\nMulti-Domain SFI (MMDSFI) and leverage MMDSFI to enforce the isolation of SIPs.\nWe also design an independent verifier to ensure the security guarantees of\nMMDSFI. With SIPs safely sharing the single address space of an enclave, the\nLibOS can implement multitasking efficiently. The Occlum LibOS outperforms the\nstate-of-the-art SGX LibOS on multitasking-heavy workloads by up to 6,600X on\nmicro-benchmarks and up to 500X on application benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.07450v1"
    },
    {
        "title": "AppStreamer: Reducing Storage Requirements of Mobile Games through\n  Predictive Streaming",
        "authors": [
            "Nawanol Theera-Ampornpunt",
            "Shikhar Suryavansh",
            "Sameer Manchanda",
            "Rajesh Panta",
            "Kaustubh Joshi",
            "Mostafa Ammar",
            "Mung Chiang",
            "Saurabh Bagchi"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Storage has become a constrained resource on smartphones. Gaming is a popular\nactivity on mobile devices and the explosive growth in the number of games\ncoupled with their growing size contributes to the storage crunch. Even where\nstorage is plentiful, it takes a long time to download and install a heavy app\nbefore it can be launched. This paper presents AppStreamer, a novel technique\nfor reducing the storage requirements or startup delay of mobile games, and\nheavy mobile apps in general. AppStreamer is based on the intuition that most\napps do not need the entirety of its files (images, audio and video clips,\netc.) at any one time. AppStreamer can, therefore, keep only a small part of\nthe files on the device, akin to a \"cache\", and download the remainder from a\ncloud storage server or a nearby edge server when it predicts that the app will\nneed them in the near future. AppStreamer continuously predicts file blocks for\nthe near future as the user uses the app, and fetches them from the storage\nserver before the user sees a stall due to missing resources. We implement\nAppStreamer at the Android file system layer. This ensures that the apps\nrequire no source code or modification, and the approach generalizes across\napps. We evaluate AppStreamer using two popular games: Dead Effect 2, a 3D\nfirst-person shooter, and Fire Emblem Heroes, a 2D turn-based strategy\nrole-playing game. Through a user study, 75% and 87% of the users respectively\nfind that AppStreamer provides the same quality of user experience as the\nbaseline where all files are stored on the device. AppStreamer cuts down the\nstorage requirement by 87% for Dead Effect 2 and 86% for Fire Emblem Heroes.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.08169v1"
    },
    {
        "title": "Data Centers Job Scheduling with Deep Reinforcement Learning",
        "authors": [
            "Sisheng Liang",
            "Zhou Yang",
            "Fang Jin",
            "Yong Chen"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Efficient job scheduling on data centers under heterogeneous complexity is\ncrucial but challenging since it involves the allocation of multi-dimensional\nresources over time and space. To adapt the complex computing environment in\ndata centers, we proposed an innovative Advantage Actor-Critic (A2C) deep\nreinforcement learning based approach called A2cScheduler for job scheduling.\nA2cScheduler consists of two agents, one of which, dubbed the actor, is\nresponsible for learning the scheduling policy automatically and the other one,\nthe critic, reduces the estimation error. Unlike previous policy gradient\napproaches, A2cScheduler is designed to reduce the gradient estimation variance\nand to update parameters efficiently. We show that the A2cScheduler can achieve\ncompetitive scheduling performance using both simulated workloads and real data\ncollected from an academic data center.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.07820v2"
    },
    {
        "title": "Mining Block I/O Traces for Cache Preloading with Sparse Temporal\n  Non-parametric Mixture of Multivariate Poisson",
        "authors": [
            "Lavanya Sita Tekumalla",
            "Chiranjib Bhattacharyya"
        ],
        "category": "cs.OS",
        "published_year": "2014",
        "summary": "  Existing caching strategies, in the storage domain, though well suited to\nexploit short range spatio-temporal patterns, are unable to leverage long-range\nmotifs for improving hitrates. Motivated by this, we investigate novel Bayesian\nnon-parametric modeling(BNP) techniques for count vectors, to capture long\nrange correlations for cache preloading, by mining Block I/O traces. Such\ntraces comprise of a sequence of memory accesses that can be aggregated into\nhigh-dimensional sparse correlated count vector sequences.\n  While there are several state of the art BNP algorithms for clustering and\ntheir temporal extensions for prediction, there has been no work on exploring\nthese for correlated count vectors. Our first contribution addresses this gap\nby proposing a DP based mixture model of Multivariate Poisson (DP-MMVP) and its\ntemporal extension(HMM-DP-MMVP) that captures the full covariance structure of\nmultivariate count data. However, modeling full covariance structure for count\nvectors is computationally expensive, particularly for high dimensional data.\nHence, we exploit sparsity in our count vectors, and as our main contribution,\nintroduce the Sparse DP mixture of multivariate Poisson(Sparse-DP-MMVP),\ngeneralizing our DP-MMVP mixture model, also leading to more efficient\ninference. We then discuss a temporal extension to our model for cache\npreloading.\n  We take the first step towards mining historical data, to capture long range\npatterns in storage traces for cache preloading. Experimentally, we show a\ndramatic improvement in hitrates on benchmark traces and lay the groundwork for\nfurther research in storage domain to reduce latencies using data mining\ntechniques to capture long range motifs.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.3463v1"
    },
    {
        "title": "A Case Study: Task Scheduling Methodologies for High Speed Computing\n  Systems",
        "authors": [
            "Mahendra Vucha",
            "Arvind Rajawat"
        ],
        "category": "cs.OS",
        "published_year": "2015",
        "summary": "  High Speed computing meets ever increasing real-time computational demands\nthrough the leveraging of flexibility and parallelism. The flexibility is\nachieved when computing platform designed with heterogeneous resources to\nsupport multifarious tasks of an application where as task scheduling brings\nparallel processing. The efficient task scheduling is critical to obtain\noptimized performance in heterogeneous computing Systems (HCS). In this paper,\nwe brought a review of various application scheduling models which provide\nparallelism for homogeneous and heterogeneous computing systems. In this paper,\nwe made a review of various scheduling methodologies targeted to high speed\ncomputing systems and also prepared summary chart. The comparative study of\nscheduling methodologies for high speed computing systems has been carried out\nbased on the attributes of platform & application as well. The attributes are\nexecution time, nature of task, task handling capability, type of host &\ncomputing platform. Finally a summary chart has been prepared and it\ndemonstrates that the need of developing scheduling methodologies for\nHeterogeneous Reconfigurable Computing Systems (HRCS) which is an emerging high\nspeed computing platform for real time applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.01370v1"
    },
    {
        "title": "The Preliminary Evaluation of a Hypervisor-based Virtualization\n  Mechanism for Intel Optane DC Persistent Memory Module",
        "authors": [
            "Takahiro Hirofuchi",
            "Ryousei Takano"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Non-volatile memory (NVM) technologies, being accessible in the same manner\nas DRAM, are considered indispensable for expanding main memory capacities.\nIntel Optane DCPMM is a long-awaited product that drastically increases main\nmemory capacities. However, a substantial performance gap exists between DRAM\nand DCPMM. In our experiments, the read/write latencies of DCPMM were 400% and\n407% higher than those of DRAM, respectively. The read/write bandwidths were\n37% and 8% of those of DRAM. This performance gap in main memory presents a new\nchallenge to researchers; we need a new system software technology supporting\nemerging hybrid memory architecture. In this paper, we present RAMinate, a\nhypervisor-based virtualization mechanism for hybrid memory systems, and a key\ntechnology to address the performance gap in main memory systems. It provides\ngreat flexibility in memory management and maximizes the performance of virtual\nmachines (VMs) by dynamically optimizing memory mappings. Through experiments,\nwe confirmed that even though a VM has only 1% of DRAM in its RAM, the\nperformance degradation of the VM was drastically alleviated by memory mapping\noptimization. The elapsed time to finish the build of Linux Kernel in the VM\nwas 557 seconds, which was only 13% increase from the 100% DRAM case (i.e., 495\nseconds). When the optimization mechanism was disabled, the elapsed time\nincreased to 624 seconds (i.e. 26% increase from the 100% DRAM case).\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12014v1"
    },
    {
        "title": "Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory\n  Machines",
        "authors": [
            "Reto Achermann",
            "Ashish Panwar",
            "Abhishek Bhattacharjee",
            "Timothy Roscoe",
            "Jayneel Gandhi"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  Multi-socket machines with 1-100 TBs of physical memory are becoming\nprevalent. Applications running on multi-socket machines suffer non-uniform\nbandwidth and latency when accessing physical memory. Decades of research have\nfocused on data allocation and placement policies in NUMA settings, but there\nhave been no studies on the question of how to place page-tables amongst\nsockets. We make the case for explicit page-table allocation policies and show\nthat page-table placement is becoming crucial to overall performance. We\npropose Mitosis to mitigate NUMA effects on page-table walks by transparently\nreplicating and migrating page-tables across sockets without application\nchanges. This reduces the frequency of accesses to remote NUMA nodes when\nperforming page-table walks. Mitosis uses two components: (i) a mechanism to\nenable efficient page-table replication and migration; and (ii) policies for\nprocesses to efficiently manage and control page-table replication and\nmigration. We implement Mitosis in Linux and evaluate its benefits on real\nhardware. Mitosis improves performance for large-scale multi-socket workloads\nby up to 1.34x by replicating page-tables across sockets. Moreover, it improves\nperformance by up to 3.24x in cases when the OS migrates a process across\nsockets by enabling cross-socket page-table migration.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.05398v2"
    },
    {
        "title": "PiBooster: A Light-Weight Approach to Performance Improvements in Page\n  Table Management for Paravirtual Virtual-Machines",
        "authors": [
            "Zhi Zhang",
            "Yueqiang Cheng"
        ],
        "category": "cs.OS",
        "published_year": "2019",
        "summary": "  In paravirtualization, the page table management components of the guest\noperating systems are properly patched for the security guarantees of the\nhypervisor. However, none of them pay enough attentions to the performance\nimprovements, which results in two noticeable performance issues. First, such\nsecurity patches exacerbate the problem that the execution paths of the guest\npage table (de)allocations become extremely long, which would consequently\nincrease the latencies of process creations and exits. Second, the patches\nintroduce many additional IOTLB flushes, leading to extra IOTLB misses, and the\nmisses would have negative impacts on I/O performance of all peripheral\ndevices. In this paper, we propose PiBooster, a novel lightweight approach for\nimproving the performance in page table management. First, PiBooster shortens\nthe execution paths of the page table (de)allocations by the PiBooster cache,\nwhich maintains dedicated buffers for serving page table (de)allocations.\nSecond, PiBooster eliminates the additional IOTLB misses with a fine-grained\nvalidation scheme, which performs page table and DMA validations separately,\ninstead of doing both together. We implement a prototype on Xen with Linux as\nthe guest kernel. We do small modifications on Xen (166 SLoC) and Linux kernel\n(350 SLoC). We evaluate the I/O performance in both micro and macro ways. The\nmicro experiment results indicate that PiBooster is able to completely\neliminate the additional IOTLB flushes in the workload-stable environments, and\neffectively reduces (de)allocation time of the page table by 47% on average.\nThe macro benchmarks show that the latencies of the process creations and exits\nare expectedly reduced by 16% on average. Moreover, the SPECINT,lmbench and\nnetperf results indicate that PiBooster has no negative performance impacts on\nCPU computation, network I/O, and disk I/O.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.09277v1"
    },
    {
        "title": "Efficient Schedulability Test for Dynamic-Priority Scheduling of\n  Mixed-Criticality Real-Time Systems",
        "authors": [
            "Xiaozhe Gu",
            "Arvind Easwaran"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Systems in many safety-critical application domains are subject to\ncertification requirements. In such a system, there are typically different\napplications providing functionalities that have varying degrees of\ncriticality. Consequently, the certification requirements for functionalities\nat these different criticality levels are also varying, with very high levels\nof assurance required for a highly critical functionality, whereas relatively\nlow levels of assurance required for a less critical functionality. Considering\nthe timing assurance given to various applications in the form of guaranteed\nbudgets within deadlines, a theory of real-time scheduling for such\nmulti-criticality systems has been under development in the recent past. In\nparticular, an algorithm called Earliest Deadline First with Virtual Deadlines\n(EDF-VD) has shown a lot of promise for systems with two criticality levels,\nespecially in terms of practical performance demonstrated through experiment\nresults. In this paper we design a new schedulability test for EDF-VD that\nextend these performance benefits to multi-criticality systems. We propose a\nnew test based on demand bound functions and also present a novel virtual\ndeadline assignment strategy. Through extensive experiments we show that the\nproposed technique significantly outperforms existing strategies for a variety\nof generic real-time systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05160v1"
    },
    {
        "title": "Multi-Rate Fluid Scheduling of Mixed-Criticality Systems on\n  Multiprocessors",
        "authors": [
            "Saravanan Ramanathan",
            "Arvind Easwaran",
            "Hyeonjoong Cho"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  In this paper we consider the problem of mixed-criticality (MC) scheduling of\nimplicit-deadline sporadic task systems on a homogenous multiprocessor\nplatform. Focusing on dual-criticality systems, algorithms based on the fluid\nscheduling model have been proposed in the past. These algorithms use a\ndual-rate execution model for each high-criticality task depending on the\nsystem mode. Once the system switches to the high-criticality mode, the\nexecution rates of such tasks are increased to meet their increased demand.\nAlthough these algorithms are speed-up optimal, they are unable to schedule\nseveral feasible dual-criticality task systems. This is because a single fixed\nexecution rate for each high-criticality task after the mode switch is not\nefficient to handle the high variability in demand during the transition period\nimmediately following the mode switch. This demand variability exists as long\nas the carry-over jobs of high-criticality tasks, that is jobs released before\nthe mode switch, have not completed. Addressing this shortcoming, we propose a\nmulti-rate fluid execution model for dual-criticality task systems in this\npaper. Under this model, high-criticality tasks are allocated varying execution\nrates in the transition period after the mode switch to efficiently handle the\ndemand variability. We derive a sufficient schedulability test for the proposed\nmodel and show its dominance over the dual-rate fluid execution model. Further,\nwe also present a speed-up optimal rate assignment strategy for the multi-rate\nmodel, and experimentally show that the proposed model outperforms all the\nexisting MC scheduling algorithms with known speed-up bounds.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05168v1"
    },
    {
        "title": "Leveraging Architectural Support of Three Page Sizes with Trident",
        "authors": [
            "Venkat Sri Sai Ram",
            "Ashish Panwar",
            "Arkaprava Basu"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Large pages are commonly deployed to reduce address translation overheads for\nbig-memory workloads. Modern x86-64 processors from Intel and AMD support two\nlarge page sizes -- 1GB and 2MB. However, previous works on large pages have\nprimarily focused on 2MB pages, partly due to lack of substantial evidence on\nthe profitability of 1GB pages to real-world applications. We argue that in\nfact, inadequate system software support is responsible for a decade of\nunderutilized hardware support for 1GB pages.\n  Through extensive experimentation on a real system, we demonstrate that 1GB\npages can improve performance over 2MB pages, and when used in tandem with 2MB\npages for an important set of applications; the support for the latter is\ncrucial but missing in current systems. Our design and implementation of\n\\trident{} in Linux fully exploit hardware supported large pages by dynamically\nand transparently allocating 1GB, 2MB, and 4KB pages as deemed suitable.\n\\trident{} speeds up eight memory-intensive applications by {$18\\%$}, on\naverage, over Linux's use of 2MB pages. We also propose \\tridentpv{}, an\nextension to \\trident{} that effectively virtualizes 1GB pages via copy-less\npromotion and compaction in the guest OS. Overall, this paper shows that even\nGB-sized pages have considerable practical significance with adequate software\nenablement, in turn motivating architects to continue investing/innovating in\nlarge pages.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.12092v1"
    },
    {
        "title": "Thread Evolution Kit for Optimizing Thread Operations on CE/IoT Devices",
        "authors": [
            "Geunsik Lim",
            "Donghyun Kang",
            "Young Ik Eom"
        ],
        "category": "cs.OS",
        "published_year": "2021",
        "summary": "  Most modern operating systems have adopted the one-to-one thread model to\nsupport fast execution of threads in both multi-core and single-core systems.\nThis thread model, which maps the kernel-space and user-space threads in a\none-to-one manner, supports quick thread creation and termination in\nhigh-performance server environments. However, the performance of time-critical\nthreads is degraded when multiple threads are being run in low-end CE devices\nwith limited system resources. When a CE device runs many threads to support\ndiverse application functionalities, low-level hardware specifications often\nlead to significant resource contention among the threads trying to obtain\nsystem resources. As a result, the operating system encounters challenges, such\nas excessive thread context switching overhead, execution delay of\ntime-critical threads, and a lack of virtual memory for thread stacks. This\npaper proposes a state-of-the-art Thread Evolution Kit (TEK) that consists of\nthree primary components: a CPU Mediator, Stack Tuner, and Enhanced Thread\nIdentifier. From the experiment, we can see that the proposed scheme\nsignificantly improves user responsiveness (7x faster) under high CPU\ncontention compared to the traditional thread model. Also, TEK solves the\nsegmentation fault problem that frequently occurs when a CE application\nincreases the number of threads during its execution.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.08062v1"
    },
    {
        "title": "DuVisor: a User-level Hypervisor Through Delegated Virtualization",
        "authors": [
            "Jiahao Chen",
            "Dingji Li",
            "Zeyu Mi",
            "Yuxuan Liu",
            "Binyu Zang",
            "Haibing Guan",
            "Haibo Chen"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Today's mainstream virtualization systems comprise of two cooperative\ncomponents: a kernel-resident driver that accesses virtualization hardware and\na user-level helper process that provides VM management and I/O virtualization.\nHowever, this virtualization architecture has intrinsic issues in both security\n(a large attack surface) and performance. While there is a long thread of work\ntrying to minimize the kernel-resident driver by offloading functions to user\nmode, they face a fundamental tradeoff between security and performance: more\noffloading may reduce the kernel attack surface, yet increase the runtime ring\ncrossings between the helper process and the driver, and thus more performance\ncost.\n  This paper explores a new design called delegated virtualization, which\ncompletely separates the control plane (the kernel driver) from the data plane\n(the helper process) and thus eliminates the kernel driver from runtime\nintervention. The resulting user-level hypervisor, called DuVisor, can handle\nall VM operations without trapping into the kernel once the kernel driver has\ndone the initialization. DuVisor retrofits existing hardware virtualization\nsupport with a new delegated virtualization extension to directly handle VM\nexits, configure virtualization registers, manage the stage-2 page table and\nvirtual devices in user mode. We have implemented the hardware extension on an\nopen-source RISC-V CPU and built a Rust-based hypervisor atop the hardware.\nEvaluation on FireSim shows that DuVisor outperforms KVM by up to 47.96\\% in a\nvariety of real-world applications and significantly reduces the attack\nsurface.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.09652v1"
    },
    {
        "title": "Ecovisor: A Virtual Energy System for Carbon-Efficient Applications",
        "authors": [
            "Abel Souza",
            "Noman Bashir",
            "Jorge Murillo",
            "Walid Hanafy",
            "Qianlin Liang",
            "David Irwin",
            "Prashant Shenoy"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Cloud platforms' rapid growth is raising significant concerns about their\ncarbon emissions. To reduce emissions, future cloud platforms will need to\nincrease their reliance on renewable energy sources, such as solar and wind,\nwhich have zero emissions but are highly unreliable. Unfortunately, today's\nenergy systems effectively mask this unreliability in hardware, which prevents\napplications from optimizing their carbon-efficiency, or work done per kilogram\nof carbon emitted. To address this problem, we design an \"ecovisor\", which\nvirtualizes the energy system and exposes software-defined control of it to\napplications. An ecovisor enables each application to handle clean energy's\nunreliability in software based on its own specific requirements. We implement\na small-scale ecovisor prototype that virtualizes a physical energy system to\nenable software-based application-level i) visibility into variable grid\ncarbon-intensity and renewable generation and ii) control of server power usage\nand battery charging/discharging. We evaluate the ecovisor approach by showing\nhow multiple applications can concurrently exercise their virtual energy system\nin different ways to better optimize carbon-efficiency based on their specific\nrequirements compared to a general system-wide policy.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.04951v1"
    },
    {
        "title": "Cutting-plane algorithms for preemptive uniprocessor real-time\n  scheduling problems",
        "authors": [
            "Abhishek Singh"
        ],
        "category": "cs.OS",
        "published_year": "2022",
        "summary": "  Fixed-point iteration algorithms like RTA (response time analysis) and QPA\n(quick processor-demand analysis) are arguably the most popular ways of solving\nschedulability problems for preemptive uniprocessor FP (fixed-priority) and EDF\n(earliest-deadline-first) systems. Several IP (integer program) formulations\nhave also been proposed for these problems, but it is unclear whether the\nalgorithms for solving these formulations are related to RTA and QPA. By\ndiscovering connections between the problems and the algorithms, we show that\nRTA and QPA are, in fact, suboptimal cutting-plane algorithms for specific IP\nformulations of FP and EDF schedulability, where optimality is defined with\nrespect to convergence rate. We propose optimal cutting-plane algorithms for\nthese IP formulations. We compare the new algorithms with RTA and QPA on large\ncollections of synthetic systems to gauge the improvement in convergence rates\nand running times.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.11185v5"
    },
    {
        "title": "Virtualization of Tiny Embedded Systems with a robust real-time capable\n  and extensible Stack Virtual Machine REXAVM supporting Material-integrated\n  Intelligent Systems and Tiny Machine Learning",
        "authors": [
            "Stefan Bosse",
            "Sarah Bornemann",
            "Björn Lüssem"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  In the past decades, there has been a significant increase in sensor density\nand sensor deployment, driven by a significant miniaturization and decrease in\nsize down to the chip level, addressing ubiquitous computing, edge computing,\nas well as distributed sensor networks. Material-integrated and intelligent\nsystems (MIIS) provide the next integration and application level, but they\ncreate new challenges and introduce hard constraints (resources, energy supply,\ncommunication, resilience, and security). Commonly, low-resource systems are\nstatically programmed processors with application-specific software or\napplication-specific hardware (FPGA). This work demonstrates the need for and\nsolution to virtualization in such low-resource and constrained systems towards\nresilient distributed sensor and cyber-physical networks using a unified\nlow-resource, customizable, and real-time capable embedded and extensible stack\nvirtual machine (REXAVM) that can be implemented and cooperate in both software\nand hardware. In a holistic architecture approach, the VM specifically\naddresses digital signal processing and tiny machine learning. The REXAVM is\nhighly customizable through the use of VM program code generators at compile\ntime and incremental code processing at run time. The VM uses an integrated,\nhighly efficient just-in-time compiler to create Bytecode from text code. This\npaper shows and evaluates the suitability of the proposed VM architecture for\noperationally equivalent software and hardware (FPGA) implementations. Specific\ncomponents supporting tiny ML and DSP using fixed-point arithmetic with respect\nto efficiency and accuracy are discussed. An extended use-case section\ndemonstrates the usability of the introduced VM architecture for a broad range\nof applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.09002v1"
    },
    {
        "title": "SGDP: A Stream-Graph Neural Network Based Data Prefetcher",
        "authors": [
            "Yiyuan Yang",
            "Rongshang Li",
            "Qiquan Shi",
            "Xijun Li",
            "Gang Hu",
            "Xing Li",
            "Mingxuan Yuan"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Data prefetching is important for storage system optimization and access\nperformance improvement. Traditional prefetchers work well for mining access\npatterns of sequential logical block address (LBA) but cannot handle complex\nnon-sequential patterns that commonly exist in real-world applications. The\nstate-of-the-art (SOTA) learning-based prefetchers cover more LBA accesses.\nHowever, they do not adequately consider the spatial interdependencies between\nLBA deltas, which leads to limited performance and robustness. This paper\nproposes a novel Stream-Graph neural network-based Data Prefetcher (SGDP).\nSpecifically, SGDP models LBA delta streams using a weighted directed graph\nstructure to represent interactive relations among LBA deltas and further\nextracts hybrid features by graph neural networks for data prefetching. We\nconduct extensive experiments on eight real-world datasets. Empirical results\nverify that SGDP outperforms the SOTA methods in terms of the hit ratio by\n6.21%, the effective prefetching ratio by 7.00%, and speeds up inference time\nby 3.13X on average. Besides, we generalize SGDP to different variants by\ndifferent stream constructions, further expanding its application scenarios and\ndemonstrating its robustness. SGDP offers a novel data prefetching solution and\nhas been verified in commercial hybrid storage systems in the experimental\nphase. Our codes and appendix are available at\nhttps://github.com/yyysjz1997/SGDP/.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.03864v2"
    },
    {
        "title": "CARTOS: A Charging-Aware Real-Time Operating System for Intermittent\n  Batteryless Devices",
        "authors": [
            "Mohsen Karimi",
            "Yidi Wang",
            "Youngbin Kim",
            "Yoojin Lim",
            "Hyoseung Kim"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  This paper presents CARTOS, a charging-aware real-time operating system\ndesigned to enhance the functionality of intermittently-powered batteryless\ndevices (IPDs) for various Internet of Things (IoT) applications. While IPDs\noffer significant advantages such as extended lifespan and operability in\nextreme environments, they pose unique challenges, including the need to ensure\nforward progress of program execution amidst variable energy availability and\nmaintaining reliable real-time time behavior during power disruptions. To\naddress these challenges, CARTOS introduces a mixed-preemption scheduling model\nthat classifies tasks into computational and peripheral tasks, and ensures\ntheir efficient and timely execution by adopting just-in-time checkpointing for\ndivisible computation tasks and uninterrupted execution for indivisible\nperipheral tasks. CARTOS also supports processing chains of tasks with\nprecedence constraints and adapts its scheduling in response to environmental\nchanges to offer continuous execution under diverse conditions. CARTOS is\nimplemented with new APIs and components added to FreeRTOS but is designed for\nportability to other embedded RTOSs. Through real hardware experiments and\nsimulations, CARTOS exhibits superior performance over state-of-the-art\nmethods, demonstrating that it can serve as a practical platform for developing\nresilient, real-time sensing applications on IPDs.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.07227v2"
    },
    {
        "title": "Telescope: Telemetry at Terabyte Scale",
        "authors": [
            "Alan Nair",
            "Sandeep Kumar",
            "Aravinda Prasad",
            "Andy Rudoff",
            "Sreenivas Subramoney"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Data-hungry applications that require terabytes of memory have become\nwidespread in recent years. To meet the memory needs of these applications,\ndata centers are embracing tiered memory architectures with near and far memory\ntiers. Precise, efficient, and timely identification of hot and cold data and\ntheir placement in appropriate tiers is critical for performance in such\nsystems. Unfortunately, the existing state-of-the-art telemetry techniques for\nhot and cold data detection are ineffective at the terabyte scale.\n  We propose Telescope, a novel technique that profiles different levels of the\napplication's page table tree for fast and efficient identification of hot and\ncold data. Telescope is based on the observation that, for a memory- and\nTLB-intensive workload, higher levels of a page table tree are also frequently\naccessed during a hardware page table walk. Hence, the hotness of the higher\nlevels of the page table tree essentially captures the hotness of its subtrees\nor address space sub-regions at a coarser granularity. We exploit this insight\nto quickly converge on even a few megabytes of hot data and efficiently\nidentify several gigabytes of cold data in terabyte-scale applications.\nImportantly, such a technique can seamlessly scale to petabyte-scale\napplications.\n  Telescope's telemetry achieves 90%+ precision and recall at just 0.009%\nsingle CPU utilization for microbenchmarks with a 5 TB memory footprint. Memory\ntiering based on Telescope results in 5.6% to 34% throughput improvement for\nreal-world benchmarks with a 1-2 TB memory footprint compared to other\nstate-of-the-art telemetry techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.10275v2"
    },
    {
        "title": "LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent\n  Ecosystem",
        "authors": [
            "Yingqiang Ge",
            "Yujie Ren",
            "Wenyue Hua",
            "Shuyuan Xu",
            "Juntao Tan",
            "Yongfeng Zhang"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  This paper envisions a revolutionary AIOS-Agent ecosystem, where Large\nLanguage Model (LLM) serves as the (Artificial) Intelligent Operating System\n(IOS, or AIOS)--an operating system \"with soul\". Upon this foundation, a\ndiverse range of LLM-based AI Agent Applications (Agents, or AAPs) are\ndeveloped, enriching the AIOS-Agent ecosystem and signaling a paradigm shift\nfrom the traditional OS-APP ecosystem. We envision that LLM's impact will not\nbe limited to the AI application level, instead, it will in turn revolutionize\nthe design and implementation of computer system, architecture, software, and\nprogramming language, featured by several main concepts: LLM as OS\n(system-level), Agents as Applications (application-level), Natural Language as\nProgramming Interface (user-level), and Tools as Devices/Libraries\n(hardware/middleware-level). We begin by introducing the architecture of\ntraditional OS. Then we formalize a conceptual framework for AIOS through \"LLM\nas OS (LLMOS)\", drawing analogies between AIOS and traditional OS: LLM is\nlikened to OS kernel, context window to memory, external storage to file\nsystem, hardware tools to peripheral devices, software tools to programming\nlibraries, and user prompts to user commands. Subsequently, we introduce the\nnew AIOS-Agent Ecosystem, where users can easily program Agent Applications\n(AAPs) using natural language, democratizing the development of software, which\nis different from the traditional OS-APP ecosystem. Following this, we explore\nthe diverse scope of Agent Applications. We delve into both single-agent and\nmulti-agent systems, as well as human-agent interaction. Lastly, drawing on the\ninsights from traditional OS-APP ecosystem, we propose a roadmap for the\nevolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the\nfuture research and development, suggesting systematic progresses of AIOS and\nits Agent applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.03815v2"
    },
    {
        "title": "Design and Implementation Considerations for a Virtual File System Using\n  an Inode Data Structure",
        "authors": [
            "Qin Sun",
            "Grace McKenzie",
            "Guanqun Song",
            "Ting Zhu"
        ],
        "category": "cs.OS",
        "published_year": "2023",
        "summary": "  Virtual file systems are a tool to centralize and mobilize a file system that\ncould otherwise be complex and consist of multiple hierarchies, hard disks, and\nmore. In this paper, we discuss the design of Unix-based file systems and how\nthis type of file system layout using inode data structures and a disk emulator\ncan be implemented as a single-file virtual file system in Linux. We explore\nthe ways that virtual file systems are vulnerable to security attacks and\nintroduce straightforward solutions that can be implemented to help prevent or\nmitigate the consequences of such attacks.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.15153v1"
    },
    {
        "title": "When eBPF Meets Machine Learning: On-the-fly OS Kernel\n  Compartmentalization",
        "authors": [
            "Zicheng Wang",
            "Tiejin Chen",
            "Qinrun Dai",
            "Yueqi Chen",
            "Hua Wei",
            "Qingkai Zeng"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Compartmentalization effectively prevents initial corruption from turning\ninto a successful attack. This paper presents O2C, a pioneering system designed\nto enforce OS kernel compartmentalization on the fly. It not only provides\nimmediate remediation for sudden threats but also maintains consistent system\navailability through the enforcement process.\n  O2C is empowered by the newest advancements of the eBPF ecosystem which\nallows to instrument eBPF programs that perform enforcement actions into the\nkernel at runtime. O2C takes the lead in embedding a machine learning model\ninto eBPF programs, addressing unique challenges in on-the-fly\ncompartmentalization. Our comprehensive evaluation shows that O2C effectively\nconfines damage within the compartment. Further, we validate that decision tree\nis optimally suited for O2C owing to its advantages in processing tabular data,\nits explainable nature, and its compliance with the eBPF ecosystem. Last but\nnot least, O2C is lightweight, showing negligible overhead and excellent\nsacalability system-wide.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05641v1"
    },
    {
        "title": "AIOS: LLM Agent Operating System",
        "authors": [
            "Kai Mei",
            "Xi Zhu",
            "Wujiang Xu",
            "Wenyue Hua",
            "Mingyu Jin",
            "Zelong Li",
            "Shuyuan Xu",
            "Ruosong Ye",
            "Yingqiang Ge",
            "Yongfeng Zhang"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  LLM-based intelligent agents face significant deployment challenges,\nparticularly related to resource management. Allowing unrestricted access to\nLLM or tool resources can lead to inefficient or even potentially harmful\nresource allocation and utilization for agents. Furthermore, the absence of\nproper scheduling and resource management mechanisms in current agent designs\nhinders concurrent processing and limits overall system efficiency. As the\ndiversity and complexity of agents continue to grow, addressing these resource\nmanagement issues becomes increasingly critical to LLM-based agent systems. To\naddress these challenges, this paper proposes the architecture of AIOS\n(LLM-based AI Agent Operating System) under the context of managing LLM-based\nagents. It introduces a novel architecture for serving LLM-based agents by\nisolating resources and LLM-specific services from agent applications into an\nAIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling,\ncontext management, memory management, storage management, access control) and\nefficient management of resources (e.g., LLM and external tools) for runtime\nagents. To enhance usability, AIOS also includes an AIOS-Agent SDK, a\ncomprehensive suite of APIs designed for utilizing functionalities provided by\nthe AIOS kernel. Experimental results demonstrate that using AIOS can achieve\nup to 2.1x faster execution for serving agents built by various agent\nframeworks. The source code is available at\nhttps://github.com/agiresearch/AIOS.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.16971v3"
    },
    {
        "title": "AIOps Solutions for Incident Management: Technical Guidelines and A\n  Comprehensive Literature Review",
        "authors": [
            "Youcef Remil",
            "Anes Bendimerad",
            "Romain Mathonat",
            "Mehdi Kaytoue"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  The management of modern IT systems poses unique challenges, necessitating\nscalability, reliability, and efficiency in handling extensive data streams.\nTraditional methods, reliant on manual tasks and rule-based approaches, prove\ninefficient for the substantial data volumes and alerts generated by IT\nsystems. Artificial Intelligence for Operating Systems (AIOps) has emerged as a\nsolution, leveraging advanced analytics like machine learning and big data to\nenhance incident management. AIOps detects and predicts incidents, identifies\nroot causes, and automates healing actions, improving quality and reducing\noperational costs. However, despite its potential, the AIOps domain is still in\nits early stages, decentralized across multiple sectors, and lacking\nstandardized conventions. Research and industrial contributions are distributed\nwithout consistent frameworks for data management, target problems,\nimplementation details, requirements, and capabilities. This study proposes an\nAIOps terminology and taxonomy, establishing a structured incident management\nprocedure and providing guidelines for constructing an AIOps framework. The\nresearch also categorizes contributions based on criteria such as incident\nmanagement tasks, application areas, data sources, and technical approaches.\nThe goal is to provide a comprehensive review of technical and research aspects\nin AIOps for incident management, aiming to structure knowledge, identify gaps,\nand establish a foundation for future developments in the field.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.01363v1"
    },
    {
        "title": "xNVMe: Unleashing Storage Hardware-Software Co-design",
        "authors": [
            "Simon A. F. Lund",
            "Vivek Shah"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  NVMe SSD hardware has witnessed widespread deployment as commodity and\nenterprise hardware due to its high performance and rich feature set. Despite\nthe open specifications of various NVMe protocols by the NVMe Express group and\nNVMe being of software abstractions to program the underlying hardware. The\nmyriad storage I/O paths such as POSIX storage API, ad-hoc OS mechanisms, and\nuserspace I/O libraries have different syntax and semantics that complicate\nsoftware development and stand in the way of mass adoption and evolution of the\nNVMe ecosystem. To unify the diverse I/O storage paths, we built xNVMe that\nexposes a single message-passing API to support both asynchronous and\nsynchronous communication with NVMe devices. xNVMe provides various command\nsets to support diverse storage I/O paths in different OS (e.g., Linux,\nFreeBSD, Windows, and MacOS) and userspace libraries (e.g., SPDK) with minimal\noverhead. xNVMe is an Open Source project and has gained traction amongst\nvarious industry stakeholders. In this paper, we elaborate on the lessons that\nwe have learned in the project during its evolution. We also provide some\nongoing and future work planned for the project. We hope the database and\nstorage systems community can join in the effort to both extend xNVMe and\nleverage it as a building block for innovative co-design of storage systems on\nmodern NVMe hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.06980v1"
    },
    {
        "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
        "authors": [
            "Ziming Mao",
            "Rishabh Iyer",
            "Scott Shenker",
            "Ion Stoica"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.20221v1"
    },
    {
        "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
        "authors": [
            "Chiyu Cheng",
            "Chang Zhou",
            "Yang Zhao",
            "Jin Cao"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.00068v1"
    },
    {
        "title": "SAFIUS - A secure and accountable filesystem over untrusted storage",
        "authors": [
            "V Sriram",
            "Ganesh Narayan",
            "K Gopinath"
        ],
        "category": "cs.OS",
        "published_year": "2008",
        "summary": "  We describe SAFIUS, a secure accountable file system that resides over an\nuntrusted storage. SAFIUS provides strong security guarantees like\nconfidentiality, integrity, prevention from rollback attacks, and\naccountability. SAFIUS also enables read/write sharing of data and provides the\nstandard UNIX-like interface for applications. To achieve accountability with\ngood performance, it uses asynchronous signatures; to reduce the space required\nfor storing these signatures, a novel signature pruning mechanism is used.\nSAFIUS has been implemented on a GNU/Linux based system modifying OpenGFS.\nPreliminary performance studies show that SAFIUS has a tolerable overhead for\nproviding secure storage: while it has an overhead of about 50% of OpenGFS in\ndata intensive workloads (due to the overhead of performing\nencryption/decryption in software), it is comparable (or better in some cases)\nto OpenGFS in metadata intensive workloads.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.2365v1"
    },
    {
        "title": "DBOS: A Proposal for a Data-Centric Operating System",
        "authors": [
            "Michael Cafarella",
            "David DeWitt",
            "Vijay Gadepally",
            "Jeremy Kepner",
            "Christos Kozyrakis",
            "Tim Kraska",
            "Michael Stonebraker",
            "Matei Zaharia"
        ],
        "category": "cs.OS",
        "published_year": "2020",
        "summary": "  Current operating systems are complex systems that were designed before\ntoday's computing environments. This makes it difficult for them to meet the\nscalability, heterogeneity, availability, and security challenges in current\ncloud and parallel computing environments. To address these problems, we\npropose a radically new OS design based on data-centric architecture: all\noperating system state should be represented uniformly as database tables, and\noperations on this state should be made via queries from otherwise stateless\ntasks. This design makes it easy to scale and evolve the OS without\nwhole-system refactoring, inspect and debug system state, upgrade components\nwithout downtime, manage decisions using machine learning, and implement\nsophisticated security features. We discuss how a database OS (DBOS) can\nimprove the programmability and performance of many of today's most important\napplications and propose a plan for the development of a DBOS proof of concept.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11112v1"
    },
    {
        "title": "Accelerator-as-a-Service in Public Clouds: An Intra-Host Traffic\n  Management View for Performance Isolation in the Wild",
        "authors": [
            "Jiechen Zhao",
            "Ran Shu",
            "Katie Lim",
            "Zewen Fan",
            "Thomas Anderson",
            "Mingyu Gao",
            "Natalie Enright Jerger"
        ],
        "category": "cs.OS",
        "published_year": "2024",
        "summary": "  I/O devices in public clouds have integrated increasing numbers of hardware\naccelerators, e.g., AWS Nitro, Azure FPGA and Nvidia BlueField. However, such\nspecialized compute (1) is not explicitly accessible to cloud users with\nperformance guarantee, (2) cannot be leveraged simultaneously by both providers\nand users, unlike general-purpose compute (e.g., CPUs). Through ten\nobservations, we present that the fundamental difficulty of democratizing\naccelerators is insufficient performance isolation support. The key obstacles\nto enforcing accelerator isolation are (1) too many unknown traffic patterns in\npublic clouds and (2) too many possible contention sources in the datapath. In\nthis work, instead of scheduling such complex traffic on-the-fly and augmenting\nisolation support on each system component, we propose to model traffic as\nnetwork flows and proactively re-shape the traffic to avoid unpredictable\ncontention. We discuss the implications of our findings on the design of future\nI/O management stacks and device interfaces.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.10098v1"
    }
]