[
    {
        "title": "Strategy-proofness and single-peackedness in bounded distributive\n  lattices",
        "authors": [
            "Ernesto Savaglio",
            "Stefano Vannucci"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  Two distinct specifications of single peakedness as currently met in the\nrelevant literature are singled out and discussed. Then, it is shown that,\nunder both of those specifications, a voting rule as defined on a bounded\ndistributive lattice is strategy-proof on the set of all profiles of single\npeaked total preorders if and only if it can be represented as an iterated\nmedian of projections and constants, or equivalently as the behaviour of a\ncertain median tree-automaton. The equivalence of individual and coalitional\nstrategy-proofness that is known to hold for single peaked domains in bounded\nlinear orders fails in such a general setting. A related impossibility result\non anonymous coalitionally strategy-proof voting rules is also obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.5120v1"
    },
    {
        "title": "Structural social capital and health in Italy",
        "authors": [
            "Damiano Fiorillo",
            "Fabio Sabatini"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  This paper presents the first empirical assessment of the causal relationship\nbetween social capital and health in Italy. The analysis draws on the 2000 wave\nof the Multipurpose Survey on Household conducted by the Italian Institute of\nStatistics on a representative sample of the population (n = 46,868). Our\nmeasure of social capital is the frequency of meetings with friends. Based on\nIV and bivariate probit estimates, we find that individuals who meet friends\nevery day or at least two times a week are approximately 11% to 16% more likely\nto report good health.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.1671v1"
    },
    {
        "title": "Comeback kids: an evolutionary approach of the long-run innovation\n  process",
        "authors": [
            "Shidong Wang",
            "Renaud Foucart",
            "Cheng Wan"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We provide a theoretical framework to understand when firms may benefit from\nexploiting previously abandoned technologies and brands. We model for the long\nrun process of innovation, allowing for sustainable diversity and comebacks of\nold brands and technologies. We present two extensions to the logistic and\nLotka-Volterra equations, which describe the diffusion of an innovation. First,\nwe extend the short-term competition to a long-term process characterized by a\nsequence of innovations and substitutions. Second, by allowing the\nsubstitutions to be incomplete, we extend the one-dimensional process to a\ntree-form multidimensional one featuring diversification throughout the\nlong-term development.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.2167v2"
    },
    {
        "title": "Long Term Risk: A Martingale Approach",
        "authors": [
            "Likuan Qin",
            "Vadim Linetsky"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  This paper extends the long-term factorization of the stochastic discount\nfactor introduced and studied by Alvarez and Jermann (2005) in discretetime\nergodic environments and by Hansen and Scheinkman (2009) and Hansen (2012) in\nMarkovian environments to general semimartingale environments. The transitory\ncomponent discounts at the stochastic rate of return on the long bond and is\nfactorized into discounting at the long-term yield and a positive\nsemimartingale that extends the principal eigenfunction of Hansen and\nScheinkman (2009) to the semimartingale setting. The permanent component is a\nmartingale that accomplishes a change of probabilities to the long forward\nmeasure, the limit of T-forward measures. The change of probabilities from the\ndata generating to the long forward measure absorbs the long-term risk-return\ntrade-off and interprets the latter as the long-term risk-neutral measure.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.3078v6"
    },
    {
        "title": "An Optimal Multiple Stopping Approach to Infrastructure Investment\n  Decisions",
        "authors": [
            "Eric Dahlgren",
            "Tim Leung"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  The energy and material processing industries are traditionally characterized\nby very large-scale physical capital that is custom-built with long lead times\nand long lifetimes. However, recent technological advancement in low-cost\nautomation has made possible the parallel operation of large numbers of\nsmall-scale and modular production units. Amenable to mass-production, these\nunits can be more rapidly deployed but they are also likely to have a much\nquicker turnover. Such a paradigm shift motivates the analysis of the combined\neffect of lead time and lifetime on infrastructure investment decisions. In\norder to value the underlying real option, we introduce an optimal multiple\nstopping approach that accounts for operational flexibility, delay induced by\nlead time, and multiple (finite/infinite) future investment opportunities. We\nprovide an analytical characterization of the firm's value function and optimal\nstopping rule. This leads us to develop an iterative numerical scheme, and\nexamine how the investment decisions depend on lead time and lifetime, as well\nas other parameters. Furthermore, our model can be used to analyze the critical\ninvestment cost that makes small-scale (short lead time, short lifetime)\nalternatives competitive with traditional large-scale infrastructure.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00861v1"
    },
    {
        "title": "Market Dynamics and Indirect Network Effects in Electric Vehicle\n  Diffusion",
        "authors": [
            "Zhe Yu",
            "Shanjun Li",
            "Lang Tong"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  The diffusion of electric vehicles (EVs) is studied in a two-sided market\nframework consisting of EVs on the one side and EV charging stations (EVCSs) on\nthe other. A sequential game is introduced as a model for the interactions\nbetween an EVCS investor and EV consumers. A consumer chooses to purchase an EV\nor a conventional gasoline alternative based on the upfront costs of purchase,\nthe future operating costs and the availability of charging stations. The\ninvestor, on the other hand, maximizes his profit by deciding whether to build\ncharging facilities at a set of potential EVCS sites or to defer his\ninvestments. The solution of the sequential game characterizes the EV-EVCS\nmarket equilibrium. The market solution is compared with that of a social\nplanner who invests in EVCSs with the goal of maximizing the social welfare. It\nis shown that the market solution underinvests EVCSs, leading to slower EV\ndiffusion. The effects of subsidies for EV purchase and EVCSs are also\nconsidered.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.03840v2"
    },
    {
        "title": "Pareto Efficient Nash Implementation Via Approval Voting",
        "authors": [
            "Yakov Babichenko",
            "Leonard J. Schulman"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We study implementation of a social choice correspondence in the case of two\nplayers who have von Neumann - Morgenstern utilities over a finite set of\nsocial alternatives, and the mechanism is allowed to output lotteries. Our main\npositive result shows that a close variant of the popular approval voting\nmechanism succeeds in selecting only Pareto efficient alternatives as pure Nash\nequilibria outcomes. Moreover, we provide an exact characterization of pure\nNash equilibria profiles and outcomes of the mechanism. The characterization\ndemonstrates a close connection between the approval voting mechanism and the\nnotion of average fixed point, which is a point that is equal to the average of\nall points that it does not Pareto dominate.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.05238v2"
    },
    {
        "title": "International R&D Spillovers and other Unobserved Common Spillovers and\n  Shocks",
        "authors": [
            "Diego-Ivan Ruge-Leiva"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Studies which are based on Coe and Helpman (1995) and use weighted foreign\nR&D variables to estimate channel-specific R&D spillovers disregard the\ninteraction between international R&D spillovers and other unobserved common\nspillovers and shocks. Using a panel of 50 economies from 1970-2011, we find\nthat disregarding this interaction leads to inconsistent estimates whenever\nknowledge spillovers and other unobserved effects are correlated with foreign\nand domestic R&D. When this interaction is modeled, estimates are consistent;\nhowever, they confound foreign and domestic R&D effects with unobserved\neffects. Thus, the coefficient of a weighted foreign R&D variable cannot\ncapture genuine channel-specific R&D spillovers.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.06805v1"
    },
    {
        "title": "Re-visiting the Distance Coefficient in Gravity Model",
        "authors": [
            "Haonan Wu"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper revisits the classic gravity model in international trade and\nreexamines the distance coefficient. As pointed out by Frankel (1997), this\ncoefficient measures the relative unit transportation cost between short\ndistance and long distance rather than the absolute level of average\ntransportation cost. Our results confirm this point in the sense that the\ncoefficient has been very stable between 1991-2006, despite the obvious\ntechnological progress taken place during this period. Moreover, by comparing\nthe sensitivity of these coefficients to change in oil prices at short periods\nof time, in which technology remained unchanged, we conclude that the average\ntechnology has indeed reduced the average trading cost. The results are robust\nwhen we divide the aggregate international trades into different industries.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.05283v2"
    },
    {
        "title": "Dynamic Games with Almost Perfect Information",
        "authors": [
            "Wei He",
            "Yeneng Sun"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper aims to solve two fundamental problems on finite or infinite\nhorizon dynamic games with perfect or almost perfect information. Under some\nmild conditions, we prove (1) the existence of subgame-perfect equilibria in\ngeneral dynamic games with almost perfect information, and (2) the existence of\npure-strategy subgame-perfect equilibria in perfect-information dynamic games\nwith uncertainty. Our results go beyond previous works on continuous dynamic\ngames in the sense that public randomization and the continuity requirement on\nthe state variables are not needed. As an illustrative application, a dynamic\nstochastic oligopoly market with intertemporally dependent payoffs is\nconsidered.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08900v1"
    },
    {
        "title": "Empirical Relevance of Ambiguity in First Price Auction Models",
        "authors": [
            "Gaurab Aryal",
            "Dong-Hyuk Kim"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We study the identification and estimation of first-price auction models\nwhere bidders have ambiguity about the valuation distribution and their\npreferences are represented by maxmin expected utility. When entry is\nexogenous, the distribution and ambiguity structure are nonparametrically\nidentified, separately from risk aversion (CRRA). We propose a flexible\nBayesian method based on Bernstein polynomials. Monte Carlo experiments show\nthat our method estimates parameters precisely, and chooses reserve prices with\n(nearly) optimal revenues, whether there is ambiguity or not. Furthermore, if\nthe model is misspecified -- incorrectly assuming no ambiguity among bidders --\nit may induce estimation bias with a substantial revenue loss.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.02516v1"
    },
    {
        "title": "The Social Cost of Carbon with Economic and Climate Risks",
        "authors": [
            "Yongyang Cai",
            "Kenneth L. Judd",
            "Thomas S. Lontzek"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  There is great uncertainty about future climate conditions and the\nappropriate policies for managing interactions between the climate and the\neconomy. We develop a multidimensional computational model to examine how\nuncertainties and risks in the economic and climate systems affect the social\ncost of carbon (SCC)---that is, the present value of the marginal damage to\neconomic output caused by carbon emissions. The SCC is substantially increased\nby economic and climate risks at both current and future times. Furthermore,\nthe SCC is itself a stochastic process with significant variation; for example,\nthe basic elements of risk incorporated into our model cause the SCC in 2100 to\nbe, with significant probability, ten times what it would be without those\nrisks. We have only imprecise information about what parameter values are best\nfor approximating reality. To deal with this parametric uncertainty we perform\nextensive uncertainty quantification and show that these findings are robust\nfor a wide range of alternative specifications. More generally, this work shows\nthat large-scale computing can enable economists to examine substantially more\ncomplex and realistic models for the purposes of policy analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.06909v2"
    },
    {
        "title": "Homogenization and Clustering as a Non-Statistical Methodology to Assess\n  Multi-Parametrical Chain Problems",
        "authors": [
            "Johannes Freiesleben",
            "Nicolas Guérin"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We present a new theoretical and numerical assessment methodology for a\none-dimensional process chain with general applicability to management problems\nsuch as the optimization of decision chains or production chains. The process\nis thereby seen as a chain of subsequently arranged units with random\nparameters influencing the objective function. For solving such complex chain\nproblems, analytical methods usually fail and statistical methods only provide\napproximate solutions while requiring massive computing power. We took insights\nfrom physics to develop a new methodology based on homogenization and\nclustering. The core idea is to replace the complex real chain with a virtual\nchain that homogenizes the involved parameters and clusters the working units\ninto global units to facilitate computation. This methodology drastically\nreduces computing time, allows for the derivation of analytical formulas, and\nprovides fast and objective insights about the optimization problem under\ninvestigation. We illustrate the analytical potency of this methodology by\napplying it to the production problem of selecting the economically superior\nquality maintenance strategy. It can further be applied to all sequential\nmulti-parametrical chain problems commonly found in business.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.03874v2"
    },
    {
        "title": "From 0D to 1D spatial models using OCMat",
        "authors": [
            "Dieter Grass"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We show that the standard class of optimal control models in OCMat can be\nused to analyze 1D spatial distributed systems. This approach is an\nintermediate step on the way to the FEM discretization approach presented in\nGrass and Uecker (2015). Therefore, the spatial distributed model is\ntransformed into a standard model by a finite difference discretization. This\n(high dimensional) standard model is then analyzed using OCMAT. As an example\nwe apply this method to the spatial distributed shallow lake model formulated\nin Brock and Xepapadeas (2008). The results are then compared with those of the\nFEM discretization in GRass and Uecker (2015)\n",
        "pdf_link": "http://arxiv.org/pdf/1505.03956v1"
    },
    {
        "title": "On the Failures of Bonus Plans",
        "authors": [
            "David Lagziel",
            "Ehud Lehrer"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  A decision maker (DM) has some funds invested through two investment firms.\nShe wishes to allocate additional funds according to the firms' earnings. The\nDM, on the one hand, tries to maximize the total expected earnings, while the\nfirms, on the other hand, try to maximize the overall expected funds they\nmanage. In this paper we prove that, for every market, the DM has an optimal\nbonus policy such that the firms are motivated to act according to the\ninterests of the DM. On the other hand, we also prove that the only policy that\nis optimal in every market, is independent of the actions and earnings of the\nfirms.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04587v1"
    },
    {
        "title": "Optimal forest rotation age under efficient climate change mitigation",
        "authors": [
            "Tommi Ekholm"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper considers the optimal rotation of forests when the carbon flows of\nforest growth and harvest are priced with an increasing price. Such an\nevolution of carbon price is generally associated with economically efficient\nclimate change mitigation, and would provide incentives for the land-owner for\nenhanced carbon sequestration. With an infinitely long sequence of even-aged\nforest rotations, the optimal harvest age changes with subsequent rotations due\nto the changing carbon price. The first-order optimality conditions therefore\nalso involve an infinite chain of lengths for consecutive forest rotations, and\nallow the approximation of the infinite-time problem with a truncated series of\nforest rotations.\n  Illustrative numerical calculations show that when starting from bare land,\nthe initial carbon price and its growth rate both primarily increase the length\nof the first rotation. With some combinations of the carbon pricing parameters,\nthe optimal harvest age can be several hundred years if the forest carbon is\nreleased to the atmosphere upon harvest. This effect is not, however, entirely\nmonotonous. Consequently, the currently optimal harvest ages are generally\nlower with higher rates of carbon price increase. This creates an interesting\ntemporal aspect, suggesting that the supply of wood and carbon sequestration by\nforests can change considerably during subsequent rotations under an increasing\nprice on carbon.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.05669v2"
    },
    {
        "title": "Analysis of Professional Trajectories using Disconnected Self-Organizing\n  Maps",
        "authors": [
            "Etienne Côme",
            "Marie Cottrell",
            "Patrice Gaubert"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In this paper we address an important economic question. Is there, as\nmainstream economic theory asserts it, an homogeneous labor market with\nmechanisms which govern supply and demand for work, producing an equilibrium\nwith its remarkable properties? Using the Panel Study of Income Dynamics (PSID)\ncollected on the period 1984-2003, we study the situations of American workers\nwith respect to employment. The data include all heads of household (men or\nwomen) as well as the partners who are on the labor market, working or not.\nThey are extracted from the complete survey and we compute a few relevant\nfeatures which characterize the worker's situations. To perform this analysis,\nwe suggest using a Self-Organizing Map (SOM, Kohonen algorithm) with specific\nstructure based on planar graphs, with disconnected components (called D-SOM),\nespecially interesting for clustering. We compare the results to those obtained\nwith a classical SOM grid and a star-shaped map (called SOS). Each component of\nD-SOM takes the form of a string and corresponds to an organized cluster. From\nthis clustering, we study the trajectories of the individuals among the classes\nby using the transition probability matrices for each period and the\ncorresponding stationary distributions. As a matter of fact, we find clear\nevidence of heterogeneous parts, each one with high homo-geneity, representing\nsituations well identified in terms of activity and wage levels and in degree\nof stability in the workplace. These results and their interpretation in\neconomic terms contribute to the debate about flexibility which is commonly\nseen as a way to obtain a better level of equilibrium on the labor market.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.00578v1"
    },
    {
        "title": "Inequality and risk aversion in economies open to altruistic attitudes",
        "authors": [
            "Eleonora Perversi",
            "Eugenio Regazzini"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper attempts to find a relationship between agents' risk aversion and\ninequality of incomes. Specifically, a model is proposed for the evolution in\ntime of surplus/deficit distribution, and the long-time distributions are\ncharacterized almost completely. They turn out to be weak Pareto laws with\nexponent linked to the relative risk aversion index which, in turn, is supposed\nto be the same for every agent. On the one hand, the aforesaid link is\nexpressed by an affine transformation. On the other hand, the level of the\nrelative risk aversion index results from a frequency distribution of\nobservable quantities stemming from how agents interact in an economic sense.\nCombination of these facts is conducive to the specification of qualitative and\nquantitative characteristics of actions fit for the control of income\nconcentration.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.00894v3"
    },
    {
        "title": "Axiomatization of the Choquet integral for 2-dimensional heterogeneous\n  product sets",
        "authors": [
            "Mikhail Timonin"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We prove a representation theorem for the Choquet integral model. The\npreference relation is defined on a two-dimensional heterogeneous product set\n$X = X_1 \\times X_2$ where elements of $X_1$ and $X_2$ are not necessarily\ncomparable with each other. However, making such comparisons in a meaningful\nway is necessary for the construction of the Choquet integral (and any\nrank-dependent model). We construct the representation, study its uniqueness\nproperties, and look at applications in multicriteria decision analysis,\nstate-dependent utility theory, and social choice. Previous axiomatizations of\nthis model, developed for decision making under uncertainty, relied heavily on\nthe notion of comonotocity and that of a \"constant act\". However, that requires\n$X$ to have a special structure, namely, all factors of this set must be\nidentical. Our characterization does not assume commensurateness of criteria a\npriori, so defining comonotonicity becomes impossible.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04167v2"
    },
    {
        "title": "Modélisation spatiale de la formation des agglomérations dans la\n  zone algéroise",
        "authors": [
            "Smicha Ait Amokthar",
            "Nadjia El Saadi",
            "Yacine Belarbi"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  The goal of this study is to analyze the dynamics underlying Algiers urban\narea formation with reference to The New Economic Geography (NEG) theories and\nmore precisely to the paper of Paul Krugman (1991), \"Increasing returns and\neconomic geography\" which explains the mechanisms of economic activities\nconcentration through two types of forces: centripetal forces enhancing the\neconomic activities concentration and centrifugal forces hindering the\nagglomeration process. In fact, these mechanisms are translated into a system\nof nonlinear equations which is very hard to solve analytically. As a\nconsequence, the use of numerical methods is highly advocated. We present some\nnumerical simulations using real Algerian data.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.00511v1"
    },
    {
        "title": "Existence of continuous euclidean embeddings for a weak class of orders",
        "authors": [
            "Lawrence Carr"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We prove that if $X$ is a topological space that admits Debreu's classical\nutility theorem (eg.\\ $X$ is separable and connected, second countable, etc.),\nthen order relations on $X$ satisfying milder completeness conditions can be\ncontinuously embedded in $\\mathbb R^I$ for $I$ some index set. In the\nparticular case where $X$ is a compact metric space, this closes a conjecture\nof Nishimura \\& Ok (2015). We also show that when $\\mathbb R^I$ is given a\nnon-standard partial order coinciding with Pareto improvement, the analogous\nembedding theorem fails to hold in the continuous case.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.00607v3"
    },
    {
        "title": "Role of non-timber forest products in sustaining forest-based\n  livelihoods and rural households' resilience capacity in and around protected\n  area- a Bangladesh study",
        "authors": [
            "S. A. Mukul",
            "A. Z. M. M. Rashid",
            "M. B. Uddin",
            "N. A. Khan"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  People in developing world derive a significant part of their livelihoods\nfrom various forest products, particularly non-timber forest products. This\narticle attempts to explore the contribution of NTFPs in sustaining\nforest-based rural livelihood in and around a protected area of Bangladesh, and\ntheir potential role in enhancing households resilience capacity. Based on\nempirical investigation our study revealed that, local communities gather a\nsubstantial amount of NTFPs from national park despite the official\nrestrictions. 27 percent households of the area received at least some cash\nbenefit from the collection, processing and selling of NTFPs, and NTFPs\ncontribute as HHs primary, supplementary and emergency sources of income. NTFPs\nalso constituted an estimated 19 percent of HHs net annual income, and were the\nprimary occupation for about 18 percent of the HHs. HHs dependency on nearby\nforests for various NTFPs varied vis-a-vis their socio-economic condition as\nwell as with their location from the park. Based on our case study the article\nalso offers some clues for improving the situation in PA.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.02056v1"
    },
    {
        "title": "A Model for Tax Evasion with Some Realistic Properties",
        "authors": [
            "Richard Vale"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We present a discrete-time dynamic model of income tax evasion. The model is\nsolved exactly in the case of a single taxpayer and shown to have some\nrealistic properties, including avoiding the Yitzhaki paradox. The extension to\nan agent-based model with a network of taxpayers is also investigated.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.02476v1"
    },
    {
        "title": "Transfer pricing manipulation, tax penalty cost and the impact of\n  foreign profit taxation",
        "authors": [
            "Alex Augusto Timm Rathke"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper analizes the optimal level of transfer pricing manipulation when\nthe expected tax penalty is a function of the tax enforcement and the market\nprice parameter. The arm's length principle implies the existence of a range of\nacceptable prices shaped by market, and firms can manipulate transfer prices\nmore freely if market price range is wide, or if its delimitations are\ndifficult to determine. Home taxation of foreign profits can reduce income\nshifting incentive, depending on the portion of repatriation for tax purposes.\nWe find that the limited tax credit rule tends to be a less efficient measure,\nnonetheless it is the most widely adopted rule by countries, so to spark the\nperspective of more powerful approaches for taxation of foreign profits.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.03853v1"
    },
    {
        "title": "Optimal Taxation with Endogenous Default under Incomplete Markets",
        "authors": [
            "Demian Pouzo",
            "Ignacio Presno"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In a dynamic economy, we characterize the fiscal policy of the government\nwhen it levies distortionary taxes and issues defaultable bonds to finance its\nstochastic expenditure. Default may occur in equilibrium as it prevents the\ngovernment from incurring in future tax distortions that would come along with\nthe service of the debt. Households anticipate the possibility of default\ngenerating endogenous credit limits. These limits hinder the government's\nability to smooth taxes using debt, implying more volatile and less serially\ncorrelated fiscal policies, higher borrowing costs and lower levels of\nindebtedness. In order to exit temporary financial autarky following a default\nevent, the government has to repay a random fraction of the defaulted debt. We\nshow that the optimal fiscal and renegotiation policies have implications\naligned with the data.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.03924v2"
    },
    {
        "title": "The nonlinear Bernstein-Schrödinger equation in Economics",
        "authors": [
            "Alfred Galichon",
            "Scott Kominers",
            "Simon Weber"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In this paper we relate the Equilibrium Assignment Problem (EAP), which is\nunderlying in several economics models, to a system of nonlinear equations that\nwe call the \"nonlinear Bernstein-Schr\\\"odinger system\", which is well-known in\nthe linear case, but whose nonlinear extension does not seem to have been\nstudied. We apply this connection to derive an existence result for the EAP,\nand an efficient computational method.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05114v1"
    },
    {
        "title": "Is Collusion-Proof Procurement Expensive?",
        "authors": [
            "Gaurab Aryal",
            "Maria F. Gabrielli"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Collusion among bidders adversely affects procurement cost and in some cases\nefficiency, and it seems collusion is more prevalent that we would like.\nStatistical methods of detecting collusion just using bid data, in a hope to\ndeter future collusion, is perilous, and access to additional data is rare and\noften always after the fact. In this paper, we estimate the extra cost of\nimplementing a new procurement rule proposed by Chen and Micali [2012] that is\nrobust to collusion and always guarantees the efficient outcome. The rule\nrequires bidders to report their coalition and to ensure\nincentive-compatibility, the mechanism allows them to attain rents. We estimate\nthis rent using data from California highway construction and find it to be\nanywhere between 1.6% to 5%. Even after we factor in the marginal excess burden\nof taxes needed to finance these rents, the cost ranges between 2.08% and 6.5%,\nsuggesting that there is a room to think about running this new auction,\nsuggesting we should consider this auction.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05353v3"
    },
    {
        "title": "Autonomics: an autonomous and intelligent economic platform and next\n  generation money tool",
        "authors": [
            "Benjamin Munro",
            "Julia McLachlan"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We propose a high level network architecture for an economic system that\nintegrates money, governance and reputation. We introduce a method for issuing,\nand redeeming a digital coin using a mechanism to create a sustainable global\neconomy and a free market. To maintain a currency's value over time, and\ntherefore be money proper, we claim it must be issued by the buyer and backed\nfor value by the seller, exchanging the products of labour, in a free market.\nWe also claim that a free market and sustainable economy cannot be maintained\nusing economically arbitrary creation and allocation of money. Nakamoto, with\nBitcoin, introduced a new technology called the cryptographic blockchain to\noperate a decentralised and distributed accounts ledger without the need for an\nuntrusted third party. This blockchain technology creates and allocates new\ndigital currency as a reward for \"proof-of-work\", to secure the network.\nHowever, no currency, digital or otherwise, has solved how to create and\nallocate money in an economically non-arbitrary way, or how to govern and trust\na world-scale free enterprise money system. We propose an \"Ontologically\nNetworked Exchange\" (ONE), with purpose as its highest order domain. Each\npurpose is defined in a contract, and the entire economy of contracts is\nstructured in a unified ontology. We claim to secure the ONE network using\neconomically non-arbitrary methodologies and economically incented human\nbehaviour. Decisions influenced by reputation help to secure the network\nwithout an untrusted third party. The stack of contracts, organised in a\nunified ontology, functions as a super recursive algorithm, with individual use\nprogramming the algorithm, acting as the \"oracle\". The state of the algorithm\nbecomes the \"memory\" of a scalable and trustable artificial intelligence (AI).\nThis AI offers a new platform for what we call the \"Autonomy-of-Things\" (AoT).\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05355v1"
    },
    {
        "title": "The Poker-Litigation Game",
        "authors": [
            "Enrique Guerra-Pujol"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Is litigation a serious search for truth or simply a game of skill or luck?\nAlthough the process of litigation has been modeled as a Prisoner's Dilemma, as\na War of Attrition, as a Game of Chicken and even as a simple coin toss, no one\nhas formally modeled litigation as a game of poker. This paper is the first to\ndo so. We present a simple \"poker-litigation game\" and find the optimal\nstrategy for playing this game.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01214v1"
    },
    {
        "title": "Dynamic Model of the Price Dispersion of Homogeneous Goods",
        "authors": [
            "Joachim Kaldasch"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Presented is an analytic microeconomic model of the temporal price dispersion\nof homogeneous goods in polypoly markets. This new approach is based on the\nidea that the price dispersion has its origin in the dynamics of the purchase\nprocess. The price dispersion is determined by the chance that demanded and\nsupplied product units meet in a given price interval. It can be characterized\nby a fat-tailed Laplace distribution for short and by a lognormal distribution\nfor long time horizons. Taking random temporal variations of demanded and\nsupplied units into account both the mean price and also the standard deviation\nof the price dispersion are governed by a lognormal distribution. A comparison\nwith empirical investigations confirms the model statements.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01216v1"
    },
    {
        "title": "Measuring economic complexity of countries and products: which metric to\n  use?",
        "authors": [
            "Manuel Sebastian Mariani",
            "Alexandre Vidmer",
            "Matus Medo",
            "Yi-Cheng Zhang"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Evaluating the economies of countries and their relations with products in\nthe global market is a central problem in economics, with far-reaching\nimplications to our theoretical understanding of the international trade as\nwell as to practical applications, such as policy making and financial\ninvestment planning. The recent Economic Complexity approach aims to quantify\nthe competitiveness of countries and the quality of the exported products based\non the empirical observation that the most competitive countries have\ndiversified exports, whereas developing countries only export few low quality\nproducts -- typically those exported by many other countries. Two different\nmetrics, Fitness-Complexity and the Method of Reflections, have been proposed\nto measure country and product score in the Economic Complexity framework. We\nuse international trade data and a recent ranking evaluation measure to\nquantitatively compare the ability of the two metrics to rank countries and\nproducts according to their importance in the network. The results show that\nthe Fitness-Complexity metric outperforms the Method of Reflections in both the\nranking of products and the ranking of countries. We also investigate a\nGeneralization of the Fitness-Complexity metric and show that it can produce\nimproved rankings provided that the input data are reliable.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01482v1"
    },
    {
        "title": "Interdisciplinary Business Games on Sustainable Development: Theoretical\n  Foundations and Prospects of Implementation",
        "authors": [
            "Boris Bolshakov",
            "Ekaterina Shamaeva",
            "Eugene Popov"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  The article defines the place of business games among all games in general\nbased on the classification by F.G. Junger; it provides critical analysis of\nexisting business games types; it also formulates requirements and lays\ntheoretical foundations and elements of the methodology and organization of\ninterdisciplinary business games (IBG) on sustainable development as a special\ntype of business games. In addition, it examines the prospects of IBG\nimplementation in higher education for sustainable development, using\ninformation technology and computer resources.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01484v1"
    },
    {
        "title": "The Principle of the Malevolent Hiding Hand; or, the Planning Fallacy\n  Writ Large",
        "authors": [
            "Bent Flyvbjerg",
            "Cass R. Sunstein"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We identify and document a new principle of economic behavior: the principle\nof the Malevolent Hiding Hand. In a famous discussion, Albert Hirschman\ncelebrated the Hiding Hand, which he saw as a benevolent mechanism by which\nunrealistically optimistic planners embark on unexpectedly challenging plans,\nonly to be rescued by human ingenuity, which they could not anticipate, but\nwhich ultimately led to success, principally in the form of unexpectedly high\nnet benefits. Studying eleven projects, Hirschman suggested that the Hiding\nHand is a general phenomenon. But the Benevolent Hiding Hand has an evil twin,\nthe Malevolent Hiding Hand, which blinds excessively optimistic planners not\nonly to unexpectedly high costs but also to unexpectedly low net benefits.\nStudying a much larger sample than Hirschman did, we find that the Malevolent\nHiding Hand is common and that the phenomenon that Hirschman identified is\nrare. This sobering finding suggests that Hirschman's phenomenon is a special\ncase; it attests to the pervasiveness of the planning fallacy, writ very large.\nOne implication involves the continuing need for unbiased cost-benefit analyses\nand other economic decision support tools; another is that such tools might\nsometimes prove unreliable.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01526v1"
    },
    {
        "title": "IMF Lending and Economic Growth: An Empirical Analysis of Ukraine",
        "authors": [
            "Roman Kononenko"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This study uses Vector Autoregression (VAR) Methodology as well as Vector\nError Correction (VEC) Methodology to examine the existence and direction of\ncausality between economic growth and IMF lending for Ukraine. The paper\nexamines the IMF lending data for the period of 1991-2010. Robust empirical\nanalysis indicates that IMF lending has a negative effect of on Ukraine's\neconomic growth in the short term. Policy implications of this finding are\nthat, despite short-run decline in economic growth, IMF lending can result in a\nlong-run sustainable growth for Ukraine. For this, policymakers need to ensure\nthat fund's money are used not only to cover budget's deficit, but also to\nfinance institutional reforms.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01741v1"
    },
    {
        "title": "Universalized Prisoner's Dilemma With Risk",
        "authors": [
            "Paul Studtmann"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In this paper I present a mathematically novel approach to the Prisoner's\nDilemma. I do so by first defining recursively a distinct action type, what I\ncall 'universalizing', that I add to the original prisoner's dilemma. Such a\nmodified version of the Prisoner's Dilemma provides a very food productive\nmodel of the choices that would be made in a prisoner's dilemma by agents who\ntrust each other. As I show, players playing a universalized prisoner's dilemma\nget as far out of the dilemma as is mathematically possible. I then add the\nconcept of risk to the universalized version of prisoner's dilemma. Doing so\nprovide a model that is sensitive to the trustworthiness of the agents in any\nprisoner's dilemma. As I show, with no risk, agents get out of the prisoners\ndilemma; and with maximal risk, the succumb to it. succumb to it.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.00665v1"
    },
    {
        "title": "Information equilibrium as an economic principle",
        "authors": [
            "Jason Smith"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  A general information equilibrium model in the case of ideal information\ntransfer is defined and then used to derive the relationship between supply\n(information destination) and demand (information source) with the price as the\ndetector of information exchange between demand and supply. We recover the\nproperties of the traditional economic supply-demand diagram. Information\nequilibrium is then applied to macroeconomic problems, recovering some common\nmacroeconomic models in particular limits like the AD-AS model, IS-LM model (in\na low inflation limit), the quantity theory of money (in a high inflation\nlimit) and the Solow-Swan growth model. Information equilibrium results in\nempirically accurate models of inflation and interest rates, and can be used to\nmotivate a 'statistical economics', analogous to statistical mechanics for\nthermodynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.02435v1"
    },
    {
        "title": "The Corporate Social Responsibility is just a twist in a Möbius Strip",
        "authors": [
            "Nazaria Solferino",
            "Viviana Solferino"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In recent years economics agents and systems have became more and more\ninteracting and juxtaposed, therefore the social sciences need to rely on the\nstudies of physical sciences to analyze this complexity in the relationships.\nAccording to this point of view we rely on the geometrical model of the\nM\\\"obius strip used in the electromagnetism which analyzes the moves of the\nelectrons that produce energy. We use a similar model in a Corporate Social\nResponsibility context to devise a new cost function in order to take into\naccount of three positive crossed effects on the efficiency: i)cooperation\namong stakeholders in the same sector, ii)cooperation among similar\nstakeholders in different sectors and iii)the stakeholders' loyalty towards the\ncompany. By applying this new cost function to a firm's decisional problem we\nfind that investing in Corporate Social Responsibility activities is ever\nconvenient depending on the number of sectors, the stakeholders' sensitivity to\nthese investments and the decay rate to alienation. Our work suggests a new\nmethod of analysis which should be developed not only at a theoretical but also\nat an empirical level.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.03398v1"
    },
    {
        "title": "Dynamics and Stability in Retail Competition",
        "authors": [
            "Marcelo J. Villena",
            "Axel A. Araneda"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Retail competition today can be described by three main features: i)\noligopolistic competition, ii) multi-store settings, and iii) the presence of\nlarge economies of scale. In these markets, firms usually apply a centralized\ndecisions making process in order to take full advantage of economies of\nscales, e.g. retail distribution centers. In this paper, we model and analyze\nthe stability and chaos of retail competition considering all these issues. In\nparticular, a dynamic multi-market Cournot-Nash equilibrium with global\neconomies and diseconomies of scale model is developed. We confirm the\nnon-intuitive hypothesis that retail multi-store competition is more unstable\nthat traditional small business that cover the same demand. The main sources of\nstability are the scale parameter and the number of markets\n",
        "pdf_link": "http://arxiv.org/pdf/1510.04550v2"
    },
    {
        "title": "Basic industrial funds of cargo motor transport enterprises: problems of\n  effective use",
        "authors": [
            "Oleksandr Vashkiv"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This work investigates the structure of basic industrial funds of cargo motor\ntransport enterprises and peculiarities of the processes of their reproduction\nin the conditions of social and economic relations transformation. On the basis\nof statistic data of cargo-motor transport enterprises of Ivano-Frankivsk, Lviv\nand Ternopil regions the author investigates the effect of production factors\non the level of capital productivity of the basic funds, he determines reserves\nof its increase. The author motivates the necessity of adaptive qualitative\nchanges in the management and realization of industrial potential and\ninnovations activization in the sphere of cargo motor transportations,\nscientiffically grounded recommendations for efficiency increase of the usage\nof basic industrial funds of cargo motor transportation enterprises in modern\neconomic conditions are provided in this work.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.05698v1"
    },
    {
        "title": "Estimating the Impact of Wind Generation in the UK",
        "authors": [
            "Lisa MH Hall",
            "Alastair Buckley",
            "Jose Mawyin"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper studies the impact of wind generation on market prices and system\ncosts in the UK between 2013 and 2014. The wider effects and implications of\nwind generation is of direct relevance and importance to policy makers, as well\nas the grid operator and market traders. We compare electricity generation from\nCoal, Gas and wind, on both the wholesale and imbalance market. We calculate\nthe system cost of wind generation (government subsidies and curtailment costs)\nand the total energy costs. For the first time in the UK, we calculate the\nMerit Order Effect on spot price due to the wind component and show a $1.67\\%$\nprice decrease for every percentage point of wind generation (compared to the\n``zero-wind'' price). The net result of total costs and price savings is\nroughly zero (slight positive gain). We also consider the effect of not having\neither an onshore or an offshore wind component. We show that the Merit-Order\nEffect savings are heavily reduced, leading to an outgoing cost of wind\ngeneration in both cases. It is therefore important to have a significant total\npercentage of wind generation, from both onshore and offshore farms.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.05854v2"
    },
    {
        "title": "Mathematics of Predicting Growth",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Mathematical methods of analysis of data and of predicting growth are\ndiscussed. The starting point is the analysis of the growth rates, which can be\nexpressed as a function of time or as a function of the size of the growing\nentity. Application of these methods is illustrated using the world economic\ngrowth but they can be applied to any other type of growth.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.06337v1"
    },
    {
        "title": "A Link between Sequential Semi-anonymous Nonatomic Games and their Large\n  Finite Counterparts",
        "authors": [
            "Jian Yang"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We show that equilibria of a sequential semi-anonymous nonatomic game (SSNG)\ncan be adopted by players in corresponding large but finite dynamic games to\nachieve near-equilibrium payoffs. Such equilibria in the form of random\nstate-to-action rules are parsimonious in form and easy to execute, as they are\nboth oblivious of past history and blind to other players' present states. Our\ntransient results can be extended to a stationary case, where the finite\ncounterparts are special discounted stochastic games. The kind of equilibria we\nadopt for SSNG are similar to distributional equilibria that are well\nunderstood in literature, and they themselves are shown to exist.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.06809v2"
    },
    {
        "title": "Analysis of Markovian Competitive Situations using Nonatomic Games",
        "authors": [
            "Jian Yang"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  For dynamic situations where the evolution of a player's state is influenced\nby his own action as well as other players' states and actions, we show that\nequilibria derived for nonatomic games (NGs) can be used by their large finite\ncounterparts to achieve near-equilibrium performances. We focus on the case\nwith quite general spaces but also with independently generated shocks driving\nrandom actions and state transitions. The NG equilibria we consider are random\nstate-to-action maps that pay no attention to players' external environments.\nThey are adoptable by a variety of real situations where awareness of other\nplayers' states can be anywhere between full and non-existent. Transient\nresults here also form the basis of a link between an NG's stationary\nequilibrium (SE) and good stationary profiles for large finite games.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.06813v3"
    },
    {
        "title": "Empirical Methods for Dynamic Power Law Distributions in the Social\n  Sciences",
        "authors": [
            "Ricardo T. Fernholz"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  This paper introduces nonparametric econometric methods that characterize\ngeneral power law distributions under basic stability conditions. These methods\nextend the literature on power laws in the social sciences in several\ndirections. First, we show that any stationary distribution in a random growth\nsetting is shaped entirely by two factors - the idiosyncratic volatilities and\nreversion rates (a measure of cross-sectional mean reversion) for different\nranks in the distribution. This result is valid regardless of how growth rates\nand volatilities vary across different economic agents, and hence applies to\nGibrat's law and its extensions. Second, we present techniques to estimate\nthese two factors using panel data. Third, we show how our results offer a\nstructural explanation for a generalized size effect in which higher-ranked\nprocesses grow more slowly than lower-ranked processes on average. Finally, we\nemploy our empirical methods using panel data on commodity prices and show that\nour techniques accurately describe the empirical distribution of relative\ncommodity prices. We also show the existence of a generalized \"size\" effect for\ncommodities, as predicted by our econometric theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.00159v3"
    },
    {
        "title": "On the parameter identifiability problem in Agent Based economical\n  models",
        "authors": [
            "Di Molfetta Giuseppe"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Identifiability of parameters is a fundamental prerequisite for model\nidentification. It concerns uniqueness of the model parameters determined from\nexperimental or simulated observations. This dissertation specifically deals\nwith structural or a priori identifiability: whether or not parameters can be\nidentified from a given model structure and experimental measurements. We\nbriefly present the identifiability problem in linear and non linear dynamical\nmodel. We compare DSGE and Agent Based model (ABM) in terms of identifiability\nof the structural parameters and we finally discuss limits and perspective of\nnumerical protocols to test global identifiability in case of ergodic and\nmarkovian economical systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.01271v1"
    },
    {
        "title": "The Postulate of the Three Regimes of Economic Growth Contradicted by\n  Data",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Economic growth in Western Europe, Eastern Europe, Asia, countries of the\nformer USSR, Africa and Latin America were analysed. It is demonstrated that\nthe fundamental postulate of the Unified Growth Theory about the existence of\nthe three regimes of growth (Malthusian regime, post-Malthusian regime and\nsustained-growth regime) is contradicted by data. These regimes did not exist.\nIn particular, there was no escape from the Malthusian trap because there was\nno trap. Economic growth in all these regions was not stagnant but hyperbolic.\nUnified Growth Theory is fundamentally incorrect. However, this theory is also\ndangerously misleading because it claims a transition from the endless epoch of\nstagnation to the new era of sustained economic growth, the interpretation\ncreating the sense of security and a promise of prosperity. The data show that\nthe opposite is true. Economic growth in the past was sustained and secure.\nNow, it is supported by the increasing ecological deficit. The long-term\nsustained and secure economic growth has yet to be created. It did not happen\nautomatically, as suggested incorrectly by the Unified Growth Theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05718v2"
    },
    {
        "title": "The Invisible Hand of Laplace: the Role of Market Structure in Price\n  Convergence and Oscillation",
        "authors": [
            "Yuval Rabani",
            "Leonard J. Schulman"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  A fundamental question about a market is under what conditions, and then how\nrapidly, does price signaling cause price equilibration. Qualitatively, this\nought to depend on how well-connected the market is. We address this question\nquantitatively for a certain class of Arrow-Debreu markets with continuous-time\nproportional t\\^{a}tonnement dynamics. We show that the algebraic connectivity\nof the market determines the effectiveness of price signaling equilibration.\nThis also lets us study the rate of external noise that a market can tolerate\nand still maintain near-equilibrium prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.07628v1"
    },
    {
        "title": "Fairs for e-commerce: the benefits of aggregating buyers and sellers",
        "authors": [
            "Pierluigi Gallo",
            "Francesco Randazzo",
            "Ignazio Gallo"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In recent years, many new and interesting models of successful online\nbusiness have been developed. Many of these are based on the competition\nbetween users, such as online auctions, where the product price is not fixed\nand tends to rise. Other models, including group-buying, are based on\ncooperation between users, characterized by a dynamic price of the product that\ntends to go down. There is not yet a business model in which both sellers and\nbuyers are grouped in order to negotiate on a specific product or service. The\npresent study investigates a new extension of the group-buying model, called\nfair, which allows aggregation of demand and supply for price optimization, in\na cooperative manner. Additionally, our system also aggregates products and\ndestinations for shipping optimization. We introduced the following new\nrelevant input parameters in order to implement a double-side aggregation: (a)\nprice-quantity curves provided by the seller; (b) waiting time, that is, the\nlonger buyers wait, the greater discount they get; (c) payment time, which\ndetermines if the buyer pays before, during or after receiving the product; (d)\nthe distance between the place where products are available and the place of\nshipment, provided in advance by the buyer or dynamically suggested by the\nsystem. To analyze the proposed model we implemented a system prototype and a\nsimulator that allow to study effects of changing some input parameters. We\nanalyzed the dynamic price model in fairs having one single seller and a\ncombination of selected sellers. The results are very encouraging and motivate\nfurther investigation on this topic.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.09071v1"
    },
    {
        "title": "Commodity Dynamics: A Sparse Multi-class Approach",
        "authors": [
            "Luca Barbaglia",
            "Ines Wilms",
            "Christophe Croux"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The correct understanding of commodity price dynamics can bring relevant\nimprovements in terms of policy formulation both for developing and developed\ncountries. Agricultural, metal and energy commodity prices might depend on each\nother: although we expect few important effects among the total number of\npossible ones, some price effects among different commodities might still be\nsubstantial. Moreover, the increasing integration of the world economy suggests\nthat these effects should be comparable for different markets. This paper\nintroduces a sparse estimator of the Multi-class Vector AutoRegressive model to\ndetect common price effects between a large number of commodities, for\ndifferent markets or investment portfolios. In a first application, we consider\nagricultural, metal and energy commodities for three different markets. We show\na large prevalence of effects involving metal commodities in the Chinese and\nIndian markets, and the existence of asymmetric price effects. In a second\napplication, we analyze commodity prices for five different investment\nportfolios, and highlight the existence of important effects from energy to\nagricultural commodities. The relevance of biofuels is hereby confirmed.\nOverall, we find stronger similarities in commodity price effects among\nportfolios than among markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01224v3"
    },
    {
        "title": "Aggregating time preferences with decreasing impatience",
        "authors": [
            "Nina Anchugina",
            "Matthew Ryan",
            "Arkadii Slinko"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  It is well-known that for a group of time-consistent decision makers their\ncollective time preferences may become time-inconsistent. Jackson and Yariv\n(2014) demonstrated that the result of aggregation of exponential discount\nfunctions always exhibits present bias. We show that when preferences satisfy\nthe axioms of Fishburn and Rubinstein (1982), present bias is equivalent to\ndecreasing impatience (DI). Applying the notion of comparative DI introduced by\nPrelec (2004), we generalize the result of Jackson and Yariv (2014). We prove\nthat the aggregation of distinct discount functions from comparable DI classes\nresults in the collective discount function which is strictly more DI than the\nleast DI of the functions being aggregated. We also prove an analogue of\nWeitzman's (1998) result, for hyperbolic rather than exponential discount\nfunctions. We show that if a decision maker is uncertain about her hyperbolic\ndiscount rate, then long-term costs and benefits will be discounted at a rate\nwhich is the probability-weighted harmonic mean of the possible hyperbolic\ndiscount rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01819v1"
    },
    {
        "title": "On the survival of poor peasants",
        "authors": [
            "Andrea C. Levi",
            "Ubaldo Garibaldi"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Previously, in underdeveloped countries, people tried to keep the prices of\nfood products artificially low, in order to help the poor to buy their food.\nBut it became soon clear that such system, although helpful for the city poor,\nwas disastrous for the peasants (who usually are even poorer), so that hunger\nincreased, instead of decreasing. More recently, thus, higher prices have been\nimposed. But a high-price system does not solve the problems. It helps, indeed,\na peasant to buy in the city non-edible products, but not to buy (more\nexpensive) food products from other peasants. The question is discussed here in\nmore detail starting from the simplest conceivable case of two peasants\nproducing each a different food product (bread and cheese, say), then\ngeneralizing to several food items and to any number of peasants producing a\ngiven food item j. Like in every economic system which wants to be sustainable,\nor able to reproduce itself in a stationary state at least, prices are\ndetermined by the necessity of exchanging \"means of production\" among\n\"industries\", except that here industriesare replaced by working peasants and\nmeans of production are replaced by food. It is found that prices must obey\ncertain inequalities related to the minimal amount of each food item necessary\nfor survival. Inequalities may be rewritten as equations and, in an important\nspecial case, such equations give rise to a simple version of the matrix\nequation used by famous authors to describe the economy.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.04223v1"
    },
    {
        "title": "A Contextual Model Of The Secessionist Rebellion in Eastern Ukraine",
        "authors": [
            "Olga Nicoara",
            "David White"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  This paper explores the possible contextual factors that drove some\nindividuals to lead, and others to join the pro-secessionist rebellion in the\n2013-2014 conflict in Eastern Ukraine. We expand on the existing rational\nchoice literature on revolutionary participation and rebellious movements by\nbuilding a contextual choice model accounting for both cost-benefit and\nbehavioral considerations taken by Pro-Russian militants and rebels in the\nregion of Donbass. Our model generates predictions about the characteristics of\nthe socio-political-cultural context that are most likely to ignite and sustain\nhierarchical rebel movements similar to those in Ukraine.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.02748v1"
    },
    {
        "title": "Incentivizing Resilience in Financial Networks",
        "authors": [
            "Matt V. Leduc",
            "Stefan Thurner"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  When banks extend loans to each other, they generate a negative externality\nin the form of systemic risk. They create a network of interbank exposures by\nwhich they expose other banks to potential insolvency cascades. In this paper,\nwe show how a regulator can use information about the financial network to\ndevise a transaction-specific tax based on a network centrality measure that\ncaptures systemic importance. Since different transactions have different\nimpact on creating systemic risk, they are taxed differently. We call this tax\na Systemic Risk Tax (SRT). We use an equilibrium concept inspired by the\nmatching markets literature to show analytically that this SRT induces a unique\nequilibrium matching of lenders and borrowers that is systemic-risk efficient,\ni.e. it minimizes systemic risk given a certain transaction volume. On the\nother hand, we show that without this SRT multiple equilibrium matchings exist,\nwhich are generally inefficient. This allows the regulator to effectively\nstimulate a `rewiring' of the equilibrium interbank network so as to make it\nmore resilient to insolvency cascades, without sacrificing transaction volume.\nMoreover, we show that a standard financial transaction tax (e.g. a Tobin-like\ntax) has no impact on reshaping the equilibrium financial network because it\ntaxes all transactions indiscriminately. A Tobin-like tax is indeed shown to\nhave a limited effect on reducing systemic risk while it decreases transaction\nvolume.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.03595v5"
    },
    {
        "title": "Note on level r consensus",
        "authors": [
            "Nikolay L. Poliakov"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We show that the hierarchy of level $r$ consensus partially collapses. In\nparticular, any profile $\\pi\\in \\mathcal{P}$ that exhibits consensus of level\n$(K-1)!$ around $\\succ_0$ in fact exhibits consensus of level $1$ around\n$\\succ_0$.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.04816v1"
    },
    {
        "title": "Inferring the contiguity matrix for spatial autoregressive analysis with\n  applications to house price prediction",
        "authors": [
            "Somwrita Sarkar",
            "Sanjay Chawla"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Inference methods in traditional statistics, machine learning and data mining\nassume that data is generated from an independent and identically distributed\n(iid) process. Spatial data exhibits behavior for which the iid assumption must\nbe relaxed. For example, the standard approach in spatial regression is to\nassume the existence of a contiguity matrix which captures the spatial\nautoregressive properties of the data. However all spatial methods, till now,\nhave assumed that the contiguity matrix is given apriori or can be estimated by\nusing a spatial similarity function. In this paper we propose a convex\noptimization formulation to solve the spatial autoregressive regression (SAR)\nmodel in which both the contiguity matrix and the non-spatial regression\nparameters are unknown and inferred from the data. We solve the problem using\nthe alternating direction method of multipliers (ADMM) which provides a\nsolution which is both robust and efficient. While our approach is general we\nuse data from housing markets of Boston and Sydney to both guide the analysis\nand validate our results. A novel side effect of our approach is the automatic\ndiscovery of spatial clusters which translate to submarkets in the housing data\nsets.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.01999v1"
    },
    {
        "title": "Matrix-vector representation of various solution concepts",
        "authors": [
            "Fuad Aleskerov",
            "Andrey Subochev"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  A unified matrix-vector representation is developed of such solution concepts\nas the core, the uncovered, the uncaptured, the minimal weakly stable, the\nminimal undominated, the minimal dominant and the untrapped sets. We also\npropose several new versions of solution sets.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02378v1"
    },
    {
        "title": "Alternative versions of the global competitive industrial performance\n  ranking constructed by methods from social choice theory",
        "authors": [
            "Andrey Subochev",
            "Igor Zakhlebin"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The Competitive Industrial Performance index (developed by experts of the\nUNIDO) is designed as a measure of national competitiveness. Index is an\naggregate of eight observable variables, representing different dimensions of\ncompetitive industrial performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02421v1"
    },
    {
        "title": "Fair division with divisible and indivisible items",
        "authors": [
            "Alexander Rubchinsky"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In the work the fair division problem for two participants in presence of\nboth divisible and indivisible items is considered. The set of all divisions is\nformally described; it is demonstrated that fair (in terms of Brams and Taylor)\ndivisions, unlikely the case where all the items are divisible, not always\nexist. The necessary and sufficient conditions of existence of proportional and\nequitable division were found.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02423v1"
    },
    {
        "title": "Fashion, fads and the popularity of choices: micro-foundations for\n  diffusion consumer theory",
        "authors": [
            "Jean-Francois Mercure"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Knowledge acquisition by consumers is a key process in the diffusion of\ninnovations. However, in standard theories of the representative agent, agents\ndo not learn and innovations are adopted instantaneously. Here, we show that in\na discrete choice model where utility-maximising agents with heterogenous\npreferences learn about products through peers, their stock of knowledge on\nproducts becomes heterogenous, fads and fashions arise, and transitivity in\naggregate preferences is lost. Non-equilibrium path-dependent dynamics emerge,\nthe representative agent exhibits behavioural rules different than individual\nagents, and aggregate utility cannot be optimised. Instead, an evolutionary\ntheory of product innovation and diffusion emerges.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.04155v3"
    },
    {
        "title": "The Oxford Olympics Study 2016: Cost and Cost Overrun at the Games",
        "authors": [
            "Bent Flyvbjerg",
            "Allison Stewart",
            "Alexander Budzier"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Given that Olympic Games held over the past decade each have cost USD 8.9\nbillion on average, the size and financial risks of the Games warrant study.\nThe objectives of the Oxford Olympics study are to (1) establish the actual\noutturn costs of previous Olympic Games in a manner where cost can consistently\nbe compared across Games; (2) establish cost overruns for previous Games, i.e.,\nthe degree to which final outturn costs reflect projected budgets at the bid\nstage, again in a way that allows comparison across Games; (3) test whether the\nOlympic Games Knowledge Management Program has reduced cost risk for the Games,\nand, finally, (4) benchmark cost and cost overrun for the Rio 2016 Olympics\nagainst previous Games. The main contribution of the Oxford study is to\nestablish a phenomenology of cost and cost overrun at the Olympics, which\nallows consistent and systematic comparison across Games. This has not been\ndone before. The study concludes that for a city and nation to decide to stage\nthe Olympic Games is to decide to take on one of the most costly and\nfinancially most risky type of megaproject that exists, something that many\ncities and nations have learned to their peril.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.04484v1"
    },
    {
        "title": "Self-organization in a distributed coordination game through heuristic\n  rules",
        "authors": [
            "S. Agarwal",
            "D. Ghosh",
            "A. S. Chakrabarti"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In this paper we consider a distributed coordination game played by a large\nnumber of agents with finite information sets, which characterizes emergence of\na single dominant attribute out of a large number of competitors. Formally, $N$\nagents play a coordination game repeatedly which has exactly $N$ Nash\nequilibria and all of the equilibria are equally preferred by the agents. The\nproblem is to select one equilibrium out of $N$ possible equilibria in the\nleast number of attempts. We propose a number of heuristic rules based on\nreinforcement learning to solve the coordination problem. We see that the\nagents self-organize into clusters with varying intensities depending on the\nheuristic rule applied although all clusters but one are transitory in most\ncases. Finally, we characterize a trade-off in terms of the time requirement to\nachieve a degree of stability in strategies and the efficiency of such a\nsolution.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.00213v1"
    },
    {
        "title": "Multidimensional Polarization Index and its Application to an Analysis\n  of the Russian State Duma",
        "authors": [
            "Fuad Aleskerov",
            "Victoria Oleynik"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The multidimensional extension of the Aleskerov-Golubenko polarization index\nis developed. Several versions of the polarization index are proposed based on\ndifferent distance functions. Basic properties of the index are examined.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.01351v1"
    },
    {
        "title": "Optimal Population in a Finite Horizon",
        "authors": [
            "Satoshi Nakano",
            "Kazuhiko Nishimura"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  A favorable population schedule for the entire potential human family is\nsought, under the overlapping generations framework, by treating population (or\nfertility) as a planning variable in a dynamical social welfare maximization\ncontext. The utilitarian and maximin social welfare functions are examined,\nwith zero future discounting, while infinity in the maximand is circumvented by\nintroducing the depletion of energy resources and its postponement through\ntechnological innovations. The model is formulated as a free-horizon dynamical\nplanning problem, solved via a non-linear optimizer. Under exploratory\nscenarios, we visualize the potential trade-offs between the two welfare\ncriteria.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.01535v3"
    },
    {
        "title": "Toward Development of a New Health Economic Evaluation Definition",
        "authors": [
            "Alexei Botchkarev"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Economic evaluation is a dynamically advancing knowledge area of health\neconomics. It has been conceived to provide evidence for allocating scarce\nresources to gain the best value for money. The problem of efficiency of\ninvestments becomes even more crucial with advances in modern medicine and\npublic health which bring about both improved patient outcomes and higher\ncosts. Despite the abundance of literature on the economic evaluation concepts,\nsome key notions including the definition of the health economic evaluation\nremain open for discussion. Academic literature offers a large number and\ngrowing variety of economic evaluation definitions. It testifies to the fact\nthat existing definitions do not meet requirements of economists. The aim of\nthis study was to examine existing definitions and reveal their common\nfeatures.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.01891v1"
    },
    {
        "title": "The Opium for the Poor Is Opium. Medicare Providers in States with Low\n  Income Prescribe High Levels of Opiates",
        "authors": [
            "Eugen Tarnow"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The majority of Medicare opioid prescriptions originate with family practice\nand internal medicine providers.\n  I show that the average number of Medicare opium prescriptions by these\nproviders vary strongly by state and that 54% of the variance is accounted for\nby the state median household income. I also show that there is a very similar\nrelationship in opioid claims per capita and per Medicare recipient.\n  In all cases Alabama is the state with the most claims and Hawaii is the\nstate with the least claims.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.02428v3"
    },
    {
        "title": "Chinese Medical Device Market and The Investment Vector",
        "authors": [
            "Weifan Zhang",
            "Rebecca Liu",
            "Chris Chatwin"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  China has attracted increasing amounts of foreign investment since it opened\nits doors to the world and whilst many analysts have focused on foreign\ninvestment in popular areas, little has been written about medical device\ninvestment. The purpose of this article is to analyze the status of the Chinese\nmedical device market from the perspective of the healthcare industry and its\nimportant market drivers; the study reveals that the medical device market has\nsignificant growth potential. This article aims to identify and assess the\nprofitable sectors of medical device technologies as a guide for international\ncompanies and investors.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.05200v1"
    },
    {
        "title": "The complex dynamics of products and its asymptotic properties",
        "authors": [
            "Orazio Angelini",
            "Matthieu Cristelli",
            "Andrea Zaccaria",
            "Luciano Pietronero"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We analyse global export data within the Economic Complexity framework. We\ncouple the new economic dimension Complexity, which captures how sophisticated\nproducts are, with an index called logPRODY, a measure of the income of the\nrespective exporters. Products' aggregate motion is treated as a 2-dimensional\ndynamical system in the Complexity-logPRODY plane. We find that this motion can\nbe explained by a quantitative model involving the competition on the markets,\nthat can be mapped as a scalar field on the Complexity-logPRODY plane and acts\nin a way akin to a potential. This explains the movement of products towards\nareas of the plane in which the competition is higher. We analyse market\ncomposition in more detail, finding that for most products it tends, over time,\nto a characteristic configuration, which depends on the Complexity of the\nproducts. This market configuration, which we called asymptotic, is\ncharacterized by higher levels of competition.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.00274v2"
    },
    {
        "title": "Urban-rural gap and poverty traps in China: A prefecture level analysis",
        "authors": [
            "Jian-Xin Wu",
            "Ling-Yun He"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Urban-rural gap and regional inequality are long standing problems in China\nand result in considerable number of studies. This paper examines the dynamic\nbehaviors of incomes for both urban and rural areas with a prefectural data\nset. The analysis is conducted by using a distribution dynamics approach, which\nhave advantages in examination on persistence, polarization and convergence\nclubs. The results show that persistence and immobility are the dominant\ncharacteristics in the income distribution dynamics. The prefectural urban and\nrural areas converge into their own steady states differentiated in income\nlevels. This pattern of urban-rural gap also exists in three regional groups,\nnamely the eastern, central and western regions. Examination on the dynamics of\nthe poorest areas shows that geographical poverty traps exist in both urban and\nrural prefectural areas. Our results indicate that more policy interventions\nare required to narrow down the urban-rural gap and to eliminate the poverty\ntraps in China.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.05171v1"
    },
    {
        "title": "Price Dynamics Via Expectations, and the Role of Money Therein",
        "authors": [
            "Gesine A. Steudle",
            "Saini Yang",
            "Carlo C. Jaeger"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Beyond its obvious macro-economic relevance, fiat money has important\nmicro-economic implications. They matter for addressing No. 8 in Smale's\n\"Mathematical Problems for the Next Century\": extend the mathematical model of\ngeneral equilibrium theory to include price adjustments. In the canonical\nArrow-Debreu framework, equilibrium prices are set by a fictitious auctioneer.\nRemoving that fiction raises the question of how prices are set and adjusted by\ndecentralised actors with incomplete information. We investigate this question\nthrough a very basic model where a unique factor of production, labour,\nproduces a single consumption good, called jelly for brevity. The point of the\nmodel is to study a price dynamics based on the firm's expectations about jelly\ndemand and labour supply. The system tends towards economic equilibrium,\nhowever, depending on the initial conditions it might not get there. In\ndifferent model versions, different kinds of money are introduced. Compared to\nthe case of no money, the introduction of money as a store of value facilitates\nthe system reaching economic equilibrium. If money is introduced as a third\ncommodity, i.e. there is also a demand for money, the system dynamics in\ngeneral becomes more complex.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.05583v3"
    },
    {
        "title": "Two approaches to modeling the interaction of small and medium\n  price-taking traders with a stock exchange by mathematical programming\n  techniques",
        "authors": [
            "A. Belenky",
            "L. Egorova"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The paper presents two new approaches to modeling the interaction of small\nand medium pricetaking traders with a stock exchange. In the framework of these\napproaches, the traders can form and manage their portfolios of financial\ninstruments traded on a stock exchange with the use of linear, integer, and\nmixed programming techniques. Unlike previous authors publications on the\nsubject, besides standard securities, the present publication considers\nderivative financial instruments such as futures and options contracts.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.05703v1"
    },
    {
        "title": "Long-run dynamics of the U.S. patent classification system",
        "authors": [
            "Francois Lafond",
            "Daniel Kim"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Almost by definition, radical innovations create a need to revise existing\nclassification systems. In this paper, we argue that classification system\nchanges and patent reclassification are common and reveal interesting\ninformation about technological evolution. To support our argument, we present\nthree sets of findings regarding classification volatility in the U.S. patent\nclassification system. First, we study the evolution of the number of distinct\nclasses. Reconstructed time series based on the current classification scheme\nare very different from historical data. This suggests that using the current\nclassification to analyze the past produces a distorted view of the evolution\nof the system. Second, we study the relative sizes of classes. The size\ndistribution is exponential so classes are of quite different sizes, but the\nlargest classes are not necessarily the oldest. To explain this pattern with a\nsimple stochastic growth model, we introduce the assumption that classes have a\nregular chance to be split. Third, we study reclassification. The share of\npatents that are in a different class now than they were at birth can be quite\nhigh. Reclassification mostly occurs across classes belonging to the same\n1-digit NBER category, but not always. We also document that reclassified\npatents tend to be more cited than non-reclassified ones, even after\ncontrolling for grant year and class of origin.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.02104v2"
    },
    {
        "title": "How well do experience curves predict technological progress? A method\n  for making distributional forecasts",
        "authors": [
            "François Lafond",
            "Aimee Gotway Bailey",
            "Jan David Bakker",
            "Dylan Rebois",
            "Rubina Zadourian",
            "Patrick McSharry",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Experience curves are widely used to predict the cost benefits of increasing\nthe deployment of a technology. But how good are such forecasts? Can one\npredict their accuracy a priori? In this paper we answer these questions by\ndeveloping a method to make distributional forecasts for experience curves. We\ntest our method using a dataset with proxies for cost and experience for 51\nproducts and technologies and show that it works reasonably well. The framework\nthat we develop helps clarify why the experience curve method often gives\nsimilar results to simply assuming that costs decrease exponentially. To\nillustrate our method we make a distributional forecast for prices of solar\nphotovoltaic modules.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.05979v2"
    },
    {
        "title": "Ex-post core, fine core and rational expectations equilibrium\n  allocations",
        "authors": [
            "Anuj Bhowmik",
            "Jiling Cao"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This paper investigates the ex-post core and its relationships to the fine\ncore and the set of rational expectations equilibrium allocations in an\noligopolistic economy with asymmetric information, in which the set of agents\nconsists of some large agents and a continuum of small agents and the space of\nstates of nature is a general probability space. We show that under appropriate\nassumptions, the ex-post core is not empty and contains the set of rational\nexpectations equilibrium allocations. We provide an example of a pure exchange\ncontinuum economy with asymmetric information and infinitely many states of\nnature, in which the ex-post core does not coincide with the set of rational\nexpectations equilibrium allocations. We also show that when our economic model\ncontains either no large agents or at least two large agents with the same\ncharacteristics, the fine core is contained in the ex-post core.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.08807v1"
    },
    {
        "title": "Agent-Based Model Calibration using Machine Learning Surrogates",
        "authors": [
            "Francesco Lamperti",
            "Andrea Roventini",
            "Amir Sani"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Taking agent-based models (ABM) closer to the data is an open challenge. This\npaper explicitly tackles parameter space exploration and calibration of ABMs\ncombining supervised machine-learning and intelligent sampling to build a\nsurrogate meta-model. The proposed approach provides a fast and accurate\napproximation of model behaviour, dramatically reducing computation time. In\nthat, our machine-learning surrogate facilitates large scale explorations of\nthe parameter-space, while providing a powerful filter to gain insights into\nthe complex functioning of agent-based models. The algorithm introduced in this\npaper merges model simulation and output analysis into a surrogate meta-model,\nwhich substantially ease ABM calibration. We successfully apply our approach to\nthe Brock and Hommes (1998) asset pricing model and to the \"Island\" endogenous\ngrowth model (Fagiolo and Dosi, 2003). Performance is evaluated against a\nrelatively large out-of-sample set of parameter combinations, while employing\ndifferent user-defined statistical tests for output analysis. The results\ndemonstrate the capacity of machine learning surrogates to facilitate fast and\nprecise exploration of agent-based models' behaviour over their often rugged\nparameter spaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.10639v2"
    },
    {
        "title": "Simple wealth distribution model causing inequality-induced crisis\n  without external shocks",
        "authors": [
            "Henri Benisty"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We address the issue of the dynamics of wealth accumulation and economic\ncrisis triggered by extreme inequality, attempting to stick to most possibly\nintrinsic assumptions. Our general framework is that of pure or modified\nmultiplicative processes, basically geometric Brownian motions. In contrast\nwith the usual approach of injecting into such stochastic agent models either\nspecific, idiosyncratic internal nonlinear interaction patterns, or macroscopic\ndisruptive features, we propose a dynamic inequality model where the attainment\nof a sizable fraction of the total wealth by very few agents induces a crisis\nregime with strong intermittency, the explicit coupling between the richest and\nthe rest being a mere normalization mechanism, hence with minimal extrinsic\nassumptions. The model thus harnesses the recognized lack of ergodicity of\ngeometric Brownian motions. It also provides a statistical intuition to the\nconsequences of Thomas Piketty's recent \"$r>g$\" (return rate $>$ growth rate)\nparadigmatic analysis of very-long-term wealth trends. We suggest that the\n\"water-divide\" of wealth flow may define effective classes, making an objective\nentry point to calibrate the model. Consistently, we check that a tax mechanism\nassociated to a few percent relative bias on elementary daily transactions is\nable to slow or stop the build-up of large wealth. When extreme fluctuations\nare tamed down to a stationary regime with sizable but steadier inequalities,\nit should still offer opportunities to study the dynamics of crisis and the\ninner effective classes induced through external or internal factors.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.06429v1"
    },
    {
        "title": "Stability of zero-growth economics analysed with a Minskyan model",
        "authors": [
            "Adam B. Barrett"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  As humanity is becoming increasingly confronted by Earth's finite biophysical\nlimits, there is increasing interest in questions about the stability and\nequitability of a zero-growth capitalist economy, most notably: if one\nmaintains a positive interest rate for loans, can a zero-growth economy be\nstable? This question has been explored on a few different macroeconomic\nmodels, and both `yes' and `no' answers have been obtained. However, economies\ncan become unstable whether or not there is ongoing underlying growth in\nproductivity with which to sustain growth in output. Here we attempt, for the\nfirst time, to assess via a model the relative stability of growth versus\nno-growth scenarios. The model employed draws from Keen's model of the Minsky\nfinancial instability hypothesis. The analysis focuses on dynamics as opposed\nto equilibrium, and scenarios of growth and no-growth of output (GDP) are\nobtained by tweaking a productivity growth input parameter. We confirm that,\nwith or without growth, there can be both stable and unstable scenarios. To\nmaintain stability, firms must not change their debt levels or target debt\nlevels too quickly. Further, according to the model, the wages share is higher\nfor zero-growth scenarios, although there are more frequent substantial drops\nin employment.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.08161v3"
    },
    {
        "title": "Wright meets Markowitz: How standard portfolio theory changes when\n  assets are technologies following experience curves",
        "authors": [
            "Rupert Way",
            "François Lafond",
            "Fabrizio Lillo",
            "Valentyn Panchenko",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We consider how to optimally allocate investments in a portfolio of competing\ntechnologies using the standard mean-variance framework of portfolio theory. We\nassume that technologies follow the empirically observed relationship known as\nWright's law, also called a \"learning curve\" or \"experience curve\", which\npostulates that costs drop as cumulative production increases. This introduces\na positive feedback between cost and investment that complicates the portfolio\nproblem, leading to multiple local optima, and causing a trade-off between\nconcentrating investments in one project to spur rapid progress vs.\ndiversifying over many projects to hedge against failure. We study the\ntwo-technology case and characterize the optimal diversification in terms of\nprogress rates, variability, initial costs, initial experience, risk aversion,\ndiscount rate and total demand. The efficient frontier framework is used to\nvisualize technology portfolios and show how feedback results in nonlinear\ndistortions of the feasible set. For the two-period case, in which learning and\nuncertainty interact with discounting, we compare different scenarios and find\nthat the discount rate plays a critical role.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.03423v3"
    },
    {
        "title": "Classifications of Innovations Survey and Future Directions",
        "authors": [
            "Mario Coccia"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  The purpose of this paper is to focus on similarity and/or heterogeneity of\ntaxonomies of innovation present in the economic fields to show as the economic\nliterature uses different names to indicate the same type of technical change\nand innovation, and the same name for different types of innovation. This\nambiguity of classification makes it impossible to compare the various studies;\nmoreover the numerous typologies existing in the economics of innovation,\ntechnometrics, economics of technical change, management of technology, etc.,\nhave hindered the development of knowledge in these fields. The research\npresents also new directions on the classification of innovation that try to\novercome these problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.08955v1"
    },
    {
        "title": "Nonparametric Regression with Multiple Thresholds: Estimation and\n  Inference",
        "authors": [
            "Yan-Yu Chiou",
            "Mei-Yuan Chen",
            "Jau-er Chen"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This paper examines nonparametric regression with an exogenous threshold\nvariable, allowing for an unknown number of thresholds. Given the number of\nthresholds and corresponding threshold values, we first establish the\nasymptotic properties of the local constant estimator for a nonparametric\nregression with multiple thresholds. However, the number of thresholds and\ncorresponding threshold values are typically unknown in practice. We then use\nour testing procedure to determine the unknown number of thresholds and derive\nthe limiting distribution of the proposed test. The Monte Carlo simulation\nresults indicate the adequacy of the modified test and accuracy of the\nsequential estimation of the threshold values. We apply our testing procedure\nto an empirical study of the 401(k) retirement savings plan with income\nthresholds.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.09418v2"
    },
    {
        "title": "Why we like the ECI+ algorithm",
        "authors": [
            "Andrea Gabrielli",
            "Matthieu Cristelli",
            "Dario Mazzilli",
            "Andrea Tacchella",
            "Andrea Zaccaria",
            "Luciano Pietronero"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Recently a measure for Economic Complexity named ECI+ has been proposed by\nAlbeaik et al. We like the ECI+ algorithm because it is mathematically\nidentical to the Fitness algorithm, the measure for Economic Complexity we\nintroduced in 2012. We demonstrate that the mathematical structure of ECI+ is\nstrictly equivalent to that of Fitness (up to normalization and rescaling). We\nthen show how the claims of Albeaik et al. about the ability of Fitness to\ndescribe the Economic Complexity of a country are incorrect. Finally, we\nhypothesize how the wrong results reported by these authors could have been\nobtained by not iterating the algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.01161v1"
    },
    {
        "title": "Optimal Learning under Robustness and Time-Consistency",
        "authors": [
            "Larry G. Epstein",
            "Shaolin Ji"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We model learning in a continuous-time Brownian setting where there is prior\nambiguity. The associated model of preference values robustness and is\ntime-consistent. It is applied to study optimal learning when the choice\nbetween actions can be postponed, at a per-unit-time cost, in order to observe\na signal that provides information about an unknown parameter. The\ncorresponding optimal stopping problem is solved in closed-form, with a focus\non two specific settings: Ellsberg's two-urn thought experiment expanded to\nallow learning before the choice of bets, and a robust version of the classical\nproblem of sequential testing of two simple hypotheses about the unknown drift\nof a Wiener process. In both cases, the link between robustness and the demand\nfor learning is studied.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.01890v2"
    },
    {
        "title": "729 new measures of economic complexity (Addendum to Improving the\n  Economic Complexity Index)",
        "authors": [
            "Saleh Albeaik",
            "Mary Kaltenberg",
            "Mansour Alsaleh",
            "César A. Hidalgo"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Recently we uploaded to the arxiv a paper entitled: Improving the Economic\nComplexity Index. There, we compared three metrics of the knowledge intensity\nof an economy, the original metric we published in 2009 (the Economic\nComplexity Index or ECI), a variation of the metric proposed in 2012, and a\nvariation we called ECI+. It was brought to our attention that the definition\nof ECI+ was equivalent to the variation of the metric proposed in 2012. We have\nverified this claim, and found that while the equations are not exactly the\nsame, they are similar enough to be our own oversight. More importantly, we now\nask: how many variations of the original ECI work? In this paper we provide a\nsimple unifying framework to explore multiple variations of ECI, including both\nthe original 2009 ECI and the 2012 variation. We found that a large fraction of\nvariations have a similar predictive power, indicating that the chance of\nfinding a variation of ECI that works, after the seminal 2009 measure, are\nsurprisingly high. In fact, more than 28 percent of these variations have a\npredictive power that is within 90 percent of the maximum for any variation.\nThese findings show that, once the idea of measuring economic complexity was\nout, creating a variation with a similar predictive power (like the ones\nproposed in 2012) was trivial (a 1 in 3 shot). More importantly, the result\nshow that using exports data to measure the knowledge intensity of an economy\nis a robust phenomenon that works for multiple functional forms. Moreover, the\nfact that multiple variations of the 2009 ECI perform close to the maximum,\ntells us that no variation of ECI will have a performance that is substantially\nbetter. This suggests that research efforts should focus on uncovering the\nmechanisms that contribute to the diffusion and accumulation of productive\nknowledge instead of on exploring small variations to existing measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04107v1"
    },
    {
        "title": "Dynamics of Investor Spanning Trees Around Dot-Com Bubble",
        "authors": [
            "Sindhuja Ranganathan",
            "Mikko Kivelä",
            "Juho Kanniainen"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We identify temporal investor networks for Nokia stock by constructing\nnetworks from correlations between investor-specific net-volumes and analyze\nchanges in the networks around dot-com bubble. We conduct the analysis\nseparately for households, non-financial institutions, and financial\ninstitutions. Our results indicate that spanning tree measures for households\nreflected the boom and crisis: the maximum spanning tree measures had clear\nupward tendency in the bull markets when the bubble was building up, and, even\nmore importantly, the minimum spanning tree measures pre-reacted the burst of\nbubble. At the same time, we find less clear reactions in minimal and maximal\nspanning trees of non-financial and financial institutions around the bubble,\nwhich suggest that household investors can have a greater herding tendency\naround bubbles.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04430v1"
    },
    {
        "title": "Generalizations of Szpilrajn's Theorem in economic and game theories",
        "authors": [
            "Athanasios Andrikopoulos"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Szpilrajn's Lemma entails that each partial order extends to a linear order.\nDushnik and Miller use Szpilrajn's Lemma to show that each partial order has a\nrelizer. Since then, many authors utilize Szpilrajn's Theorem and the\nWell-ordering principle to prove more general existence type theorems on\nextending binary relations. Nevertheless, we are often interested not only in\nthe existence of extensions of a binary relation $R$ satisfying certain axioms\nof orderability, but in something more: (A) The conditions of the sets of\nalternatives and the properties which $R$ satisfies to be inherited when one\npasses to any member of a subfamily of the family of extensions of $R$ and: (B)\nThe size of a family of ordering extensions of $R$, whose intersection is $R$,\nto be the smallest one. The key to addressing these kinds of problems is the\nszpilrajn inherited method. In this paper, we define the notion of\n$\\Lambda(m)$-consistency, where $m$ can reach the first infinite ordinal\n$\\omega$, and we give two general inherited type theorems on extending binary\nrelations, a Szpilrajn type and a Dushnik-Miller type theorem, which generalize\nall the well known existence and inherited type extension theorems in the\nliterature. \\keywords{Consistent binary relations, Extension theorems,\nIntersection of binary relations.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04711v1"
    },
    {
        "title": "New Market Creation via Innovation: A Study on Tata Nano",
        "authors": [
            "Swati Singh",
            "Manoj Joshi"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This research paper focuses on how innovations support new market creation\nemerging from latent opportunities for low-income group. It also emphasizes on\nnovel strategies that can be implemented for sustaining. The paper concludes\nwith a discussion on the implications of the study and directions to stimulate\nfuture research on the subject.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04952v1"
    },
    {
        "title": "Unemployment: Study of Causes and Possible Solutions",
        "authors": [
            "Thomas Pedro Eggarter"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  The following measures against unemployment are proposed: In the short term,\nto promote greater income for the poorest sectors. It is shown that this can be\npaid with the resulting increased production, without losing income to the\nother economic agents. In the mid term, the creation of ad-hoc companies for\ninvestment in projects profitable but long lasting. And in the long run, the\nabandonment of the competitive models. As these proposals go against current\nideas (liberalisation, labour market flexibility, free market, etc.), the\nstatements are rigorously demonstrated, even at the risk of making the lecture\nharder.\n  Part 1 explores the problem and uses a simple model and others heuristic\narguments to create familiarity with macroeconomic models. Part 2 is a\nsimplified summary of Macroeconomic Theory textbook. It serves as a review to\nthe reader whose knowledge in economy are out of date, or as a first\napproximation to the topic if he or she does not have them. In the light of the\ntheory, economic policies are evaluated for the Argentine case in the 90's. The\nwork accepts the Keynesian explanation of unemployment (insufficient demand),\nbut we disagree on its solution (public expenditure). Finally, in Part 3 we\nelaborate and justify the proposals.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.06704v1"
    },
    {
        "title": "Relationship between Remittances and Macroeconomic Variables in Times of\n  Political and Social Upheaval: Evidence from Tunisia's Arab Spring",
        "authors": [
            "Jamal Bouoiyour",
            "Refk Selmi",
            "Amal Miftah"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  If Tunisia was hailed as a success story with its high rankings on economic,\neducational, and other indicators compared to other Arab countries, the 2011\npopular uprisings demonstrate the need for political reforms but also major\neconomic reforms. The Arab spring highlights the fragility of its main economic\npillars including the tourism and the foreign direct investment. In such\nturbulent times, the paper examines the economic impact of migrant'\nremittances, expected to have a countercyclical behavior. Our results reveal\nthat prior to the Arab Spring, the impacts of remittances on growth and\nconsumption seem negative and positive respectively, while they varyingly\ninfluence local investment. These three relationships held in the short-run. By\nconsidering the period surrounding the 2011 uprisings, the investment effect of\nremittances becomes negative and weak in the short-and medium-run, whereas\npositive and strong remittances' impacts on growth and consumption are found in\nthe long term.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07037v1"
    },
    {
        "title": "The Keynesian Model in the General Theory: A Tutorial",
        "authors": [
            "Raul Rojas"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This small overview of the General Theory is the kind of summary I would have\nliked to have read, before embarking in a comprehensive study of the General\nTheory at the time I was a student. As shown here, the main ideas are quite\nsimple and easy to visualize. Unfortunately, numerous introductions to\nKeynesian theory are not actually based on Keynes opus magnum, but in obscure\nneoclassical reinterpretations. This is completely pointless since Keynes' book\nis so readable.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07509v1"
    },
    {
        "title": "Promotion through Connections: Favors or Information?",
        "authors": [
            "Yann Bramoullé",
            "Kenan Huremović"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Connections appear to be helpful in many contexts, such as obtaining a job, a\npromotion, a grant, a loan, or publishing a paper. This may be due either to\nfavoritism or to information conveyed by connections. Attempts at identifying\nboth effects have relied on measures of true quality, generally built from data\ncollected long after promotion. Building on earlier work on discrimination, we\npropose a new method to identify favors and information from data collected at\nthe time of promotion. Under weak assumptions, we show that promotion decisions\nfor connected candidates look more random to the econometrician due to the\ninformation channel. We derive new identification results and estimate the\nstrength of the two effects. We adapt the control function approach to address\nthe issue of the selection into connections. Applying our methodology to\nacademic promotions in Spain and Italy, as well as political advancements in\nChina, we find evidence that connections may both convey information and\nattract favors.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07723v3"
    },
    {
        "title": "A Simple Algorithm for Solving Ramsey Optimal Policy with Exogenous\n  Forcing Variables",
        "authors": [
            "Jean-Bernard Chatelain",
            "Kirsten Ralf"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This algorithm extends Ljungqvist and Sargent (2012) algorithm of Stackelberg\ndynamic game to the case of dynamic stochastic general equilibrium models\nincluding exogenous forcing variables. It is based Anderson, Hansen, McGrattan,\nSargent (1996) discounted augmented linear quadratic regulator. It adds an\nintermediate step in solving a Sylvester equation. Forward-looking variables\nare also optimally anchored on forcing variables. This simple algorithm calls\nfor already programmed routines for Ricatti, Sylvester and Inverse matrix in\nMatlab and Scilab. A final step using a change of basis vector computes a\nvector auto regressive representation including Ramsey optimal policy rule\nfunction of lagged observable variables, when the exogenous forcing variables\nare not observable.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07996v1"
    },
    {
        "title": "Spontaneous Segregation of Agents Across Double Auction Markets",
        "authors": [
            "Aleksandra Alorić",
            "Peter Sollich",
            "Peter McBurney"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  In this paper we investigate the possibility of spontaneous segregation into\ngroups of traders that have to choose among several markets. Even in the\nsimplest case of two markets and Zero Intelligence traders, we are able to\nobserve segregation effects below a critical value Tc of the temperature T; the\nlatter regulates how strongly traders bias their decisions towards choices with\nlarge accumulated scores. It is notable that segregation occurs even though the\ntraders are statistically homogeneous. Traders can in principle change their\nloyalty to a market, but the relevant persistence times become long below Tc.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.09327v1"
    },
    {
        "title": "Econophysics of Business Cycles: Aggregate Economic Fluctuations, Mean\n  Risks and Mean Square Risks",
        "authors": [
            "Victor Olkhov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This paper presents hydrodynamic-like model of business cycles aggregate\nfluctuations of economic and financial variables. We model macroeconomics as\nensemble of economic agents on economic space and agent's risk ratings play\nrole of their coordinates. Sum of economic variables of agents with coordinate\nx define macroeconomic variables as functions of time and coordinates x. We\ndescribe evolution and interactions between macro variables on economic space\nby hydrodynamic-like equations. Integral of macro variables over economic space\ndefines aggregate economic or financial variables as functions of time t only.\nHydrodynamic-like equations define fluctuations of aggregate variables. Motion\nof agents from low risk to high risk area and back define the origin for\nrepeated fluctuations of aggregate variables. Economic or financial variables\non economic space may define statistical moments like mean risk, mean square\nrisk and higher. Fluctuations of statistical moments describe phases of\nfinancial and economic cycles. As example we present a simple model relations\nbetween Assets and Revenue-on-Assets and derive hydrodynamic-like equations\nthat describe evolution and interaction between these variables.\nHydrodynamic-like equations permit derive systems of ordinary differential\nequations that describe fluctuations of aggregate Assets, Assets mean risks and\nAssets mean square risks. Our approach allows describe business cycle aggregate\nfluctuations induced by interactions between any number of economic or\nfinancial variables.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.00282v1"
    },
    {
        "title": "Relatedness, Knowledge Diffusion, and the Evolution of Bilateral Trade",
        "authors": [
            "Bogang Jun",
            "Aamena Alshamsi",
            "Jian Gao",
            "Cesar A Hidalgo"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  During the last decades two important contributions have reshaped our\nunderstanding of international trade. First, countries trade more with those\nwith whom they share history, language, and culture, suggesting that trade is\nlimited by information frictions. Second, countries are more likely to start\nexporting products that are similar to their current exports, suggesting that\nknowledge diffusion among related industries is a key constrain shaping the\ndiversification of exports. But does knowledge about how to export to a\ndestination also diffuses among related products and geographic neighbors? Do\ncountries need to learn how to trade each product to each destination? Here, we\nuse bilateral trade data from 2000 to 2015 to show that countries are more\nlikely to increase their exports of a product to a destination when: (i) they\nexport related products to it, (ii) they export the same product to the\nneighbor of a destination, (iii) they have neighbors who export the same\nproduct to that destination. Then, we explore the magnitude of these effects\nfor new, nascent, and experienced exporters, (exporters with and without\ncomparative advantage in a product) and also for groups of products with\ndifferent level of technological sophistication. We find that the effects of\nproduct and geographic relatedness are stronger for new exporters, and also,\nthat the effect of product relatedness is stronger for more technologically\nsophisticated products. These findings support the idea that international\ntrade is shaped by information frictions that are reduced in the presence of\nrelated products and experienced geographic neighbors.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.05392v1"
    },
    {
        "title": "Modeling of the Labour Force Redistribution in Investment Projects with\n  Account of their Delay",
        "authors": [
            "I. D. Kolesin",
            "O. A. Malafeyev",
            "I. V. Zaitseva",
            "A. N. Ermakova",
            "D. V. Shlaev"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  The mathematical model of the labour force redistribution in investment\nprojects is presented in the article. The redistribution mode of funds, labour\nforce in particular, according to the equal risk approach applied to the loss\nof some assets due to delay in all the investment projects is provided in the\nmodel. The sample of the developed model for three investment projects with the\nspecified labour force volumes and their defined unit costs at the particular\nmoment is given.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.06380v1"
    },
    {
        "title": "Dis-embedded Openness: Inequalities in European Economic Integration at\n  the Sectoral Level",
        "authors": [
            "Balazs Vedres",
            "Carl Nordlund"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  The process of European integration resulted in a marked increase in\ntransnational economic flows, yet regional inequalities along many\ndevelopmental indicators remain. We analyze the unevenness of European\neconomies with respect to the embedding of export sectors in upstream domestic\nflows, and their dependency on dominant export partners. We use the WIOD data\nset of sectoral flows for the period of 1995-2011 for 24 European countries. We\nfound that East European economies were significantly more likely to experience\nincreasing unevenness and dependency with increasing openness, while core\ncountries of Europe managed to decrease their unevenness while increasing their\nopenness. Nevertheless, by analyzing the trajectories of changes for each\ncountry, we see that East European countries are also experiencing a turning\npoint, either switching to a path similar to the core, or to a retrograde path\nwith decreasing openness. We analyze our data using pooled time series models\nand case studies of country trajectories.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02626v1"
    },
    {
        "title": "The effects of energy and commodity prices on commodity output in a\n  three-factor, two-good general equilibrium trade model",
        "authors": [
            "Yoshiaki Nakada"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We analyze the effects of energy and commodity prices on commodity output\nusing a three-factor, two-good general equilibrium trade model with three\nfactors: capital, labor, and imported energy. We derive a sufficient condition\nfor each sign pattern of each relationship to hold, which no other studies have\nderived. We assume factor-intensity ranking is constant and use the EWS\n(economy-wide substitution)-ratio vector and the Hadamard product in the\nanalysis. The results reveal that the position of the EWS-ratio vector\ndetermines the relationships. Specifically, the strengthening (resp. reduction)\nof import restrictions can increase (resp. decrease) the commodity output of\nexportables, if capital and labor, domestic factors, are economy-wide\ncomplements. This seems paradoxical.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.10096v2"
    },
    {
        "title": "Comment on Suzuki's rebuttal of Batra and Casas",
        "authors": [
            "Yoshiaki Nakada"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Batra and Casas (1976) claimed that 'a strong Rybczynski result' arises in\nthe three-factor two-good general equilibrium trade model. In subsequent\ncomments, Suzuki (1983) contended that this could not be the case. Among his\ncomments, Suzuki found that the set of three equations holds for the\nAllen-partial elasticity of substitution under the assumption of perfect\ncomplementarity, and he applied these to his analysis. In the following, I\ndemonstrate that these are impossible, hence his dissenting proof is not\nplausible.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.10138v1"
    },
    {
        "title": "Factor endowment -- commodity output relationships in a three-factor,\n  two-good general equilibrium trade model",
        "authors": [
            "Yoshiaki Nakada"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We analyze the Rybczynski sign pattern, which expresses the factor endowment\n- commodity output relationships in a three-factor, two-good general\nequilibrium trade model. The relationship determines whether a strong\nRybczynski result holds. We search for a sufficient condition for each\nRybczynski sign pattern to hold in a systematic manner, which no other studies\nhave derived. We assume factor-intensity ranking is constant. We use the EWS\n(economy-wide substitution)-ratio vector and the Hadamard product in our\nanalysis. We show that the position of the EWS-ratio vector determines the\nRybczynski sign pattern. This article provides a basis for further\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.11429v1"
    },
    {
        "title": "Revisiting the determinacy on New Keynesian Models: A survey",
        "authors": [
            "Alberto F. Boix",
            "Adrián Segura Moreiras"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  The goal of this paper is to review some analytic techniques that are\npotentially useful to shed light on the determinacy question that arises in New\nKeynesian models as result of a combination of several monetary policy rules;\nin these models, we provide conditions to guarantee existence and uniqueness of\nequilibrium by means of results that are obtained from theoretical analysis. In\nparticular, these methods confirm the well known fact that Taylor--like rules\nin interest rate setting are not the only way to reach determinacy of the\nrational expectations equilibrium in the New Keynesian setting. The key\ntechnical tool we use for that purposes is the so--called Budan--Fourier\nTheorem, that we review along the paper. All the ideas and techniques presented\nhave been already used, our contribution that might be original here are the\norganization and emphasis.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.03681v3"
    },
    {
        "title": "Dynamic intersectoral models with power-law memory",
        "authors": [
            "Valentina V. Tarasova",
            "Vasily E. Tarasov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Intersectoral dynamic models with power-law memory are proposed. The\nequations of open and closed intersectoral models, in which the memory effects\nare described by the Caputo derivatives of non-integer orders, are derived. We\nsuggest solutions of these equations, which have the form of linear\ncombinations of the Mittag-Leffler functions and which are characterized by\ndifferent effective growth rates. Examples of intersectoral dynamics with\npower-law memory are suggested for two sectoral cases. We formulate two\nprinciples of intersectoral dynamics with memory: the principle of changing of\ntechnological growth rates and the principle of domination change. It has been\nshown that in the input-output economic dynamics the effects of fading memory\ncan change the economic growth rate and dominant behavior of economic sectors.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09087v1"
    },
    {
        "title": "Concept of dynamic memory in economics",
        "authors": [
            "Valentina V. Tarasova",
            "Vasily E. Tarasov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  In this paper we discuss a concept of dynamic memory and an application of\nfractional calculus to describe the dynamic memory. The concept of memory is\nconsidered from the standpoint of economic models in the framework of\ncontinuous time approach based on fractional calculus. We also describe some\ngeneral restrictions that can be imposed on the structure and properties of\ndynamic memory. These restrictions include the following three principles: (a)\nthe principle of fading memory; (b) the principle of memory homogeneity on time\n(the principle of non-aging memory); (c) the principle of memory reversibility\n(the principle of memory recovery). Examples of different memory functions are\nsuggested by using the fractional calculus. To illustrate an application of the\nconcept of dynamic memory in economics we consider a generalization of the\nHarrod-Domar model, where the power-law memory is taken into account.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09088v1"
    },
    {
        "title": "Logistic map with memory from economic model",
        "authors": [
            "Valentina V. Tarasova",
            "Vasily E. Tarasov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  A generalization of the economic model of logistic growth, which takes into\naccount the effects of memory and crises, is suggested. Memory effect means\nthat the economic factors and parameters at any given time depend not only on\ntheir values at that time, but also on their values at previous times. For the\nmathematical description of the memory effects, we use the theory of\nderivatives of non-integer order. Crises are considered as sharp splashes\n(bursts) of the price, which are mathematically described by the\ndelta-functions. Using the equivalence of fractional differential equations and\nthe Volterra integral equations, we obtain discrete maps with memory that are\nexact discrete analogs of fractional differential equations of economic\nprocesses. We derive logistic map with memory, its generalizations, and\n\"economic\" discrete maps with memory from the fractional differential\nequations, which describe the economic natural growth with competition,\npower-law memory and crises.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09092v1"
    },
    {
        "title": "Accelerators in macroeconomics: Comparison of discrete and continuous\n  approaches",
        "authors": [
            "Valentina V. Tarasova",
            "Vasily E. Tarasov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We prove that the standard discrete-time accelerator equation cannot be\nconsidered as an exact discrete analog of the continuous-time accelerator\nequation. This leads to fact that the standard discrete-time macroeconomic\nmodels cannot be considered as exact discretization of the corresponding\ncontinuous-time models. As a result, the equations of the continuous and\nstandard discrete models have different solutions and can predict the different\nbehavior of the economy. In this paper, we propose a self-consistent\ndiscrete-time description of the economic accelerators that is based on the\nexact finite differences. For discrete-time approach, the model equations with\nexact differences have the same solutions as the corresponding continuous-time\nmodels and these discrete and continuous models describe the same behavior of\nthe economy. Using the Harrod-Domar growth model as an example, we show that\nequations of the continuous-time model and the suggested exact discrete model\nhave the same solutions and these models predict the same behavior of the\neconomy.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09605v1"
    },
    {
        "title": "A quantitative approach to choose among multiple mutually exclusive\n  decisions: comparative expected utility theory",
        "authors": [
            "Pengyu Zhu"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  Mutually exclusive decisions have been studied for decades. Many well-known\ndecision theories have been defined to help people either to make rational\ndecisions or to interpret people's behaviors, such as expected utility theory,\nregret theory, prospect theory, and so on. The paper argues that none of these\ndecision theories are designed to provide practical, normative and quantitative\napproaches for multiple mutually exclusive decisions. Different decision-makers\nshould naturally make different choices for the same decision question, as they\nhave different understandings and feelings on the same possible outcomes.The\nauthor tries to capture the different understandings and feelings from\ndifferent decision-makers, and model them into a quantitative decision\nevaluation process, which everyone could benefit from. The basic elements in\nclassic expected utility theory are kept in the new decision theory, but the\ninfluences from mutually exclusive decisions will also be modeled into the\nevaluation process. This may sound like regret theory, but the new approach is\ndesigned to fit multiple mutually exclusive decision scenarios, and it does not\nrequire a definition of probability weighting function. The new theory is\ndesigned to be simple and straightforward to use, and the results are expected\nto be rational for each decision-maker.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02422v1"
    },
    {
        "title": "Games of Incomplete Information and Myopic Equilibria",
        "authors": [
            "R. Simon",
            "S. Spiez",
            "H. Torunczyk"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  A new concept of an equilibrium in games is introduced that solves an open\nquestion posed by A. Neyman.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02444v2"
    },
    {
        "title": "Diversification, economies of scope, and exports growth of Chinese firms",
        "authors": [
            "Mercedes Campi",
            "Marco Dueñas",
            "Le Li",
            "Huabin Wu"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  In the 1990s, China started a process of structural reforms and of trade\nliberalization, which was followed by the accession to the World Trade\nOrganization (WTO) in 2001. In this paper, we analyze trade patterns of Chinese\nfirms for the period 2000-2006, characterized by a notable increase in exports\nvolumes. Theoretically, in a more open economy, firms are expected to move from\nthe production of a set of less-competitive products towards more\ninternationally competitive ones, which implies specialization. We study\nseveral stylized facts on the distribution of Chinese firms trade and growth\nrates, and we analyze whether firms have diversified or specialized their trade\npatterns between 2000 and 2006. We show that Chinese export patterns are very\nheterogeneous, that the volatility of growth rates depends on the level of\nexports, and that volatility is stronger after trade liberalization. Both,\ndiversification in products and destinations have a positive impact on trade\ngrowth, but diversification of destinations has a stronger effect. We conclude\nthat the success of Chinese exports is not only due to an increase in the\nintensive margin, related to the existence of economies of scale, but also due\nto an increase in the extensive margin, related to the existence of economies\nof scope.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02681v2"
    },
    {
        "title": "The time interpretation of expected utility theory",
        "authors": [
            "Ole Peters",
            "Alexander Adamou"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  Ergodicity economics is a new branch of economic theory that notes the\nconceptual difference between time averages and expectation values, which\ncoincide only for ergodic observables. It postulates that individual agents\nmaximise the time average growth rate of wealth, known widely as growth\noptimality. This contrasts with the dominant behavioural model in economics,\nexpected utility theory, in which agents maximise expectation values of changes\nin psychologically transformed wealth. Historically, growth optimality was\nexplored for additive and multiplicative gambles. Here we apply it to a general\nclass of wealth dynamics, extending the range of economic situations where it\nmay be used. Moreover, we show a correspondence between growth optimality and\nexpected utility theory, in which the ergodicity transformation in the former\nis identified as the utility function in the latter. This correspondence offers\na theoretical basis for choosing utility functions and predicts that wealth\ndynamics are strong determinants of risk preferences.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.03680v2"
    },
    {
        "title": "When does a disaster become a systemic event? Estimating indirect\n  economic losses from natural disasters",
        "authors": [
            "Sebastian Poledna",
            "Stefan Hochrainer-Stigler",
            "Michael Gregor Miess",
            "Peter Klimek",
            "Stefan Schmelzer",
            "Johannes Sorger",
            "Elena Shchekinova",
            "Elena Rovenskaya",
            "JoAnne Linnerooth-Bayer",
            "Ulf Dieckmann",
            "Stefan Thurner"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  Reliable estimates of indirect economic losses arising from natural disasters\nare currently out of scientific reach. To address this problem, we propose a\nnovel approach that combines a probabilistic physical damage catastrophe model\nwith a new generation of macroeconomic agent-based models (ABMs). The ABM moves\nbeyond the state of the art by exploiting large data sets from detailed\nnational accounts, census data, and business information, etc., to simulate\ninteractions of millions of agents representing \\emph{each} natural person or\nlegal entity in a national economy. The catastrophe model introduces a copula\napproach to assess flood losses, considering spatial dependencies of the flood\nhazard. These loss estimates are used in a damage scenario generator that\nprovides input for the ABM, which then estimates indirect economic losses due\nto the event. For the first time, we are able to link environmental and\neconomic processes in a computer simulation at this level of detail. We show\nthat moderate disasters induce comparably small but positive short- to\nmedium-term, and negative long-term economic impacts. Large-scale events,\nhowever, trigger a pronounced negative economic response immediately after the\nevent and in the long term, while exhibiting a temporary short- to medium-term\neconomic boost. We identify winners and losers in different economic sectors,\nincluding the fiscal consequences for the government. We quantify the critical\ndisaster size beyond which the resilience of an economy to rebuild reaches its\nlimits. Our results might be relevant for the management of the consequences of\nsystemic events due to climate change and other disasters.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.09740v1"
    },
    {
        "title": "New Proposals of a Stress Measure in a Capital and its Robust Estimator",
        "authors": [
            "Tadeusz Klecha",
            "Daniel Kosiorowski",
            "Dominik Mielczarek",
            "Jerzy P. Rydlewski"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  In this paper a novel approach for a measurement of stresses in a capital,\nwhich induce the capital flows between economic systems, is proposed. The\nproposals appeal to an apparatus offered by the statistical theory of shape. We\npropose a stress functional basing on a concept of mean shape determined by\nrepresentative particles of a capital carrier. We also propose methods of\ndescribing changes in an amount and a structure of stresses in a capital\nappealing, among others, to a Bookstein's pair of thin plain spline\ndeformation, and a measure of a shape variability. We apply our approach to an\nindirect verification of the hypothesis according to which a capital flow\nbetween economic systems is related to an activity of an inner force related to\nstresses in a capital. We indicate, that the stresses create a phenomenon\nanalogous to the heat, which may be interpreted in terms of a positive economic\nexternal effect, which attracts a capital from environment of a system to the\nsystem. For empirical studies we propose robust approach to estimate the stress\nfunctional basing on the data depth concept. In the empirical research we use\ndata on five branch stock indexes from Warsaw Stock Exchange. The studied\nperiod involves the financial crisis of 2007.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.03756v1"
    },
    {
        "title": "Price Impact Under Heterogeneous Beliefs and Restricted Participation",
        "authors": [
            "Michail Anthropelos",
            "Constantinos Kardaras"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  We consider a financial market in which traders potentially face restrictions\nin trading some of the available securities. Traders are heterogeneous with\nrespect to their beliefs and risk profiles, and the market is assumed thin:\ntraders strategically trade against their price impacts. We prove existence and\nuniqueness of a corresponding equilibrium, and provide an efficient algorithm\nto numerically obtain the equilibrium prices and allocations given market's\ninputs. We find that restrictions may increase the market's welfare if traders\nhave different views regarding the covariance matrix of securities returns. The\nlatter heterogeneity regarding covariance matrix disagreement is essential in\nmodelling; for instance, when traders agree on the covariance matrix,\nrestricting participation in some securities for some traders leaves\nequilibrium prices unaltered in the unrestricted securities, a certainly\nundesirable model effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.09954v4"
    },
    {
        "title": "Planning Fallacy or Hiding Hand: Which Is the Better Explanation?",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  This paper asks and answers the question of whether Kahneman's planning\nfallacy or Hirschman's Hiding Hand best explain performance in capital\ninvestment projects. I agree with my critics that the Hiding Hand exists, i.e.,\nsometimes benefit overruns outweigh cost overruns in project planning and\ndelivery. Specifically, I show this happens in one fifth of projects, based on\nthe best and largest dataset that exists. But that was not the main question I\nset out to answer. My main question was whether the Hiding Hand is \"typical,\"\nas claimed by Hirschman. I show this is not the case, with 80 percent of\nprojects not displaying Hiding Hand behavior. Finally, I agree it would be\nimportant to better understand the circumstances where the Hiding Hand actually\nworks. However, if you want to understand how projects \"typically\" work, as\nHirschman said he did, then the theories of the planning fallacy, optimism\nbias, and strategic misrepresentation - according to which cost overruns and\nbenefit shortfalls are the norm - will serve you significantly better than the\nprinciple of the Hiding Hand. The latter will lead you astray, because it is a\nspecial case instead of a typical one.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.09999v1"
    },
    {
        "title": "Econophysics Beyond General Equilibrium: the Business Cycle Model",
        "authors": [
            "Victor Olkhov"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  Current business cycle theory is an application of the general equilibrium\ntheory. This paper presents the business cycle model without using general\nequilibrium framework. We treat agents risk assessments as their coordinates x\non economic space and establish distribution of all economic agents by their\nrisk coordinates. We suggest aggregation of agents and their variables by\nscales large to compare with risk scales of single agents and small to compare\nwith economic domain on economic space. Such model is alike to transition from\nkinetic description of multi-particle system to hydrodynamic approximation.\nAggregates of agents extensive variables with risk coordinate x determine macro\nvariables as functions of x alike to hydrodynamic variables. Economic and\nfinancial transactions between agents define evolution of their variables.\nAggregation of transactions between agents with risk coordinates x and y\ndetermine macro transactions as functions of x and y and define evolution of\nmacro variables at points x and y. We describe evolution and interactions\nbetween macro transactions by hydrodynamic-like system of economic equations.\nWe show that business cycles are described as consequence of the system of\neconomic equations on macro transactions. As example we describe Credit\ntransactions CL(tax,y) that provide Loans from Creditors at point x to\nBorrowers at point y and Loan-Repayment transactions LR(t,x,y) that describe\nrepayments from Borrowers at point y to Creditors at point x. We use\nhydrodynamic-like economic equations and derive from them the system of\nordinary differential equations that describe business cycle fluctuations of\nmacro Credits C(t) and macro Loan-Repayments LR(t) of the entire economics. The\nnature of business cycle fluctuations is explained as oscillations of \"mean\nrisk\" of economic variables on bounded economic domain of economic space.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.04721v1"
    },
    {
        "title": "Chocs technologiques, chocs des prix et fluctuations du chômage en\n  République Démocratique du Congo",
        "authors": [
            "Antoine Kamiantako Miyamueni",
            "Henry Ngongo Muganza"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  The main purpose of this research is to analyze the effects of macroeconomic\nshocks on unemployment fluctuations in the Democratic Republic of Congo (DRC).\nUsing the SVECM model on DRC data for the period 1960 to 2014, the conclusion\nis that the high and persistent level of unemployment is mainly explained\npermanently by technological and price shocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.09532v1"
    },
    {
        "title": "Data-based Automatic Discretization of Nonparametric Distributions",
        "authors": [
            "Alexis Akira Toda"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  Although using non-Gaussian distributions in economic models has become\nincreasingly popular, currently there is no systematic way for calibrating a\ndiscrete distribution from the data without imposing parametric assumptions.\nThis paper proposes a simple nonparametric calibration method based on the\nGolub-Welsch algorithm for Gaussian quadrature. Application to an optimal\nportfolio problem suggests that assuming Gaussian instead of nonparametric\nshocks leads to up to 17% overweighting in the stock portfolio because the\ninvestor underestimates the probability of crashes.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.00896v2"
    },
    {
        "title": "Indicators of availability of non-market relations in the sphere of\n  labor market in Ukraine",
        "authors": [
            "Valery Tabakov"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  There are identified indicators of availability a non-market relations in the\nsphere of labor market in Ukraine. It is concluded that illegal tax money paid\nby legally working in Ukraine, as insurance premiums in the event of\nunemployment. It is concluded that increased pressure from the government on\nlabor market regulators Ukraine established on a parity basis. There are\nformulated recommendations for the implementation of the principle of a free\nmarket economy in the regulation of the labor market of Ukraine.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.4387v1"
    },
    {
        "title": "Visualising stock flow consistent models as directed acyclic graphs",
        "authors": [
            "Peter G. Fennell",
            "David O'Sullivan",
            "Antoine Godin",
            "Stephen Kinsella"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We show how every stock-flow consistent model of the macroeconomy can be\nrepresented as a directed acyclic graph. The advantages of representing the\nmodel in this way include graphical clarity, causal inference, and model\nspecification. We provide many examples implemented with a new software\npackage.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.4541v1"
    },
    {
        "title": "Turnpike Property and Convergence Rate for an Investment Model with\n  General Utility Functions",
        "authors": [
            "Baojun Bian",
            "Harry Zheng"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  In this paper we aim to address two questions faced by a long-term investor\nwith a power-type utility at high levels of wealth: one is whether the turnpike\nproperty still holds for a general utility that is not necessarily\ndifferentiable or strictly concave, the other is whether the error and the\nconvergence rate of the turnpike property can be estimated. We give positive\nanswers to both questions. To achieve these results, we first show that there\nis a classical solution to the HJB equation and give a representation of the\nsolution in terms of the dual function of the solution to the dual HJB\nequation. We demonstrate the usefulness of that representation with some\nnontrivial examples that would be difficult to solve with the trial and error\nmethod. We then combine the dual method and the partial differential equation\nmethod to give a direct proof to the turnpike property and to estimate the\nerror and the convergence rate of the optimal policy when the utility function\nis continuously differentiable and strictly concave. We finally relax the\nconditions of the utility function and provide some sufficient conditions that\nguarantee the turnpike property and the convergence rate in terms of both\nprimal and dual utility functions.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.7802v1"
    },
    {
        "title": "The Role of Money in the Business Cycle",
        "authors": [
            "Zhao Jianglin"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  The aim of this paper is to reemphasize the money theory of exchange which is\ncentered on the function of exchange medium of money, and make a contribution\ntowards linearization of the quantity equation of exchange. A dynamical\nquantity equation is presented and an important balanced path of economic\nevolution is derived. To understand the business cycle we propose a hypothesis\nof natural cycle and driving cycle concerning the evolution of the balanced\npath and plentiful conclusions can be made.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.00947v2"
    },
    {
        "title": "Viability and Arbitrage under Knightian Uncertainty",
        "authors": [
            "Matteo Burzoni",
            "Frank Riedel",
            "H. Mete Soner"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We reconsider the microeconomic foundations of financial economics. Motivated\nby the importance of Knightian Uncertainty in markets, we present a model that\ndoes not carry any probabilistic structure ex ante, yet is based on a common\norder. We derive the fundamental equivalence of economic viability of asset\nprices and absence of arbitrage. We also obtain a modified version of the\nFundamental Theorem of Asset Pricing using the notion of sublinear pricing\nmeasures. Different versions of the Efficient Market Hypothesis are related to\nthe assumptions one is willing to impose on the common order.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.03335v6"
    },
    {
        "title": "How do fishery policies affect Hawaii's longline fishing industry?\n  Calibrating a positive mathematical programming model",
        "authors": [
            "Jonathan R. Sweeney",
            "Richard E. Howitt",
            "Hing Ling Chan",
            "Minling Pan",
            "PingSun Leung"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We present a vessel and target-specific positive mathematical programming\nmodel (PMP) for Hawaii's longline fishing fleet. Although common in\nagricultural economics, PMP modeling is rarely attempted in fisheries. To\ndemonstrate the flexibility of the PMP framework, we separate tuna and\nswordfish production technologies into three policy relevant fishing targets.\nWe find the model most accurately predicts vessel-specific annual bigeye catch\nin the WCPO, with an accuracy of 12% to 35%, and a correlation between 0.30 and\n0.53. To demonstrate the model's usefulness to policy makers, we simulate the\neconomic impact to individual vessels from increasing and decreasing the bigeye\ncatch limit in the WCPO by 10%. Our results suggest that such policy changes\nwill have moderate impacts on most vessels, but large impacts on a few\ngenerating a fat tailed distribution. These results offer insights into the\nrange of winners and losers resulting from changes in fishery policies, and\ntherefore, which policies are more likely to gain widespread industry support.\nAs a tool for fishery management, the calibrated PMP model offers a flexible\nand easy-to-use framework, capable of capturing the heterogeneous response of\nfishing vessels to evaluate policy changes.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.03960v1"
    },
    {
        "title": "Geopolitical Model of Investment Project Implementation",
        "authors": [
            "Oleg Malafeyev",
            "Konstantin Farvazov",
            "Olga Zenovich"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Two geopolitical actors implement a geopolitical project that involves\ntransportaion and storage of some commodities. They interact with each other\nthrough a transport network. The network consists of several interconnected\nvertices. Some of the vetrices are trading hubs, storage spaces, production\nhubs and goods buyers. Actors wish to satify the demand of buyers and recieve\nthe highest possible profit subject to compromise solution principle. A\nnumerical example is given.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.06829v1"
    },
    {
        "title": "A Dynamical Model of the Industrial Economy of the Humber Region",
        "authors": [
            "Christopher J. K. Knight",
            "Alexandra S. Penn",
            "Rebecca B. Hoyle"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  The Humber region in the UK is a large and diverse industrial area centred\naround oil refining, chemical industries and energy production. However there\nis currently a desire to see the region transition towards a more bio-based\neconomy. New bio-related industries are being situated in the region as a\nconsequence of policy and economic incentives. Many of these industries are\nconnected through their supply chains, either directly, or by sharing common\nsuppliers or customers and the growth or decline of one industry can hence have\nimpacts on many others. Therefore an important question to consider is what\neffect this movement towards bio-based industry will actually have on the\nregional economy as a whole. In this paper we develop a general abstract\ndynamical model for the metabolic interactions of firms or industries. This\ndynamical model has been applied to the Humber region in order to gain a deeper\nunderstanding of how the region may develop. The model suggests that the\ntransition to a bio-based economy will occur with oil refining losing its\ndominance to bioethanol production and biological chemical production, whilst\nanaerobic digestion grows as a major source of electricity, in turn driving up\nthe value of regional waste aggregators and arable farming in the overall\neconomy.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.3167v1"
    },
    {
        "title": "Stability and Identification with Optimal Macroprudential Policy Rules",
        "authors": [
            "Jean-Bernard Chatelain",
            "Kirsten Ralf"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  This paper investigates the identification, the determinacy and the stability\nof ad hoc, \"quasi-optimal\" and optimal policy rules augmented with financial\nstability indicators (such as asset prices deviations from their fundamental\nvalues) and minimizing the volatility of the policy interest rates, when the\ncentral bank precommits to financial stability. Firstly, ad hoc and\nquasi-optimal rules parameters of financial stability indicators cannot be\nidentified. For those rules, non zero policy rule parameters of financial\nstability indicators are observationally equivalent to rule parameters set to\nzero in another rule, so that they are unable to inform monetary policy.\nSecondly, under controllability conditions, optimal policy rules parameters of\nfinancial stability indicators can all be identified, along with a bounded\nsolution stabilizing an unstable economy as in Woodford (2003), with\ndeterminacy of the initial conditions of non- predetermined variables.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.3347v1"
    },
    {
        "title": "The Italian Crisis and Producer Households Debt: a Source of Stability?\n  A Reproducible Research",
        "authors": [
            "Stefano Olgiati",
            "Gilberto Bronzini",
            "Alessandro Danovi"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  The European Credit Research Institute Research Report 2013 identifies\nHouseholds debt \"rapid increase and abrupt retrenchment\" among the causes of\nmacroeconomic instability in the European Union after 2008. In our research: i)\nwe accessed the Bank of Italy Online Statistical Database on Customers and Risk\nfor Producer Households and Non-Financial Corporations with R Sweave open\naccess statistical software, which makes our analysis freely reproducible by\nother researchers; ii) we subset the European System of Accounts sector\nHouseholds into the Bank of Italy sub-sectors Households and Producer\nHouseholds, which are market producing entities limited to informal\npartnerships, de facto companies and sole proprietorships with up to five\nemployees and iii) we tested the hypothesis of \"rapid increase and abrupt\nretrenchment\" of debt for this subset in Italy for the period 1996-2013. We\nfound that PH debt (bad debt) has been more stable with a lower Variation\nCoefficient of 10.3% (14.2%) versus 13.2% (20.1%) in NFC. We also found that\nthe time series of the ratio of debt granted to NFC (numerator) versus PH\n(denominator) is best described (Multiple Squared 0.95) by the concavity of the\n5th degree coefficient (slope -1.22; 95% CI -1.52 - -0.91) of a 5th order\npolynomial linear regression and by the convexity of the 2nd degree coefficient\n(slope 4.26; 95% CI 2.53 - 5.99) for bad debt (Multiple R Squared 0.47), with\nthis concavity of debt and convexity of bad debt beginning with the Italian\ncrisis in the second trimester of 2008. We reject the hypothesis (p < 0.01) of\n\"rapid increase and abrupt retrenchment\" of debt for the subset Producer\nHouseholds during the Italian Crisis. We generate the hypothesis that this\nsubset could represent a prospective source of stability relative to\nNon-Financial Corporation.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.7377v1"
    },
    {
        "title": "The Theory of a Heliospheric Economy",
        "authors": [
            "Thomas Tarler"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Despite more than 50 years of human space exploration, no paper in the field\nof economics has been published regarding the theory of a space-based economy.\nThe aim of this paper is to develop quantitative techniques to estimate\nconditions of the human heliospheric expansion. An empirical analysis of\ncurrent space commercialization and reasoning from first economic principles\nyields an evolutionary prisoner's dilemma game on a dynamically scaled\nheterogeneous Newman-Watts Small World Network to generate a new space. The\nanalysis allows for scalar measurements of behavior, market structures, wealth,\nand technological prowess, with time measured relative to the system. Four\nmajor phases of heliospheric expansion become evident, in which the dynamic of\nthe economic environment drives further exploration. Further research could\ncombine empirical estimations of parameters with computer simulations to prove\nresults to inform long-term business plans or public policy to further\nincentivize human heliospheric domination.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.00146v2"
    },
    {
        "title": "Business cycle synchronization within the European Union: A wavelet\n  cohesion approach",
        "authors": [
            "Lubos Hanus",
            "Lukas Vacha"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In this paper, we map the process of business cycle synchronization across\nthe European Union. We study this synchronization by applying wavelet\ntechniques, particularly the cohesion measure with time-varying weights. This\nnovel approach allows us to study the dynamic relationship among selected\ncountries from a different perspective than the usual time-domain models.\nAnalyzing monthly data from 1990 to 2014, we show an increasing co-movement of\nthe Visegrad countries with the European Union after the countries began\npreparing for the accession to the European Union. With particular focus on the\nVisegrad countries we show that participation in a currency union possibly\nincreases the co-movement. Furthermore, we find a high degree of\nsynchronization in long-term horizons by analyzing the Visegrad Four and\nSouthern European countries' synchronization with the core countries of the\nEuropean Union.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.03106v3"
    },
    {
        "title": "Time-scale analysis of co-movement in EU sovereign bond markets",
        "authors": [
            "Filip Smolik",
            "Lukas Vacha"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We study the co-movement of the 10-year sovereign bond yields of 11 EU\ncountries. Our analysis is focused mainly on changes in co-movement during the\nfinancial crisis period, especially around two significant dates - the fall of\nLehman Brothers, September 15, 2008, and the announcement of the increase of\nGreece's public deficit on October 20, 2009. We study co-movement dynamics\nusing wavelet analysis, which allows us to observe how co-movement changes\nacross frequencies and over time. We divide the countries into three groups:\nthe core of the Eurozone, the periphery of the Eurozone and the states outside\nthe Eurozone. The results indicate that co-movement decreased considerably\nduring the crisis period for all country pairs but that there are significant\ndifferences among the groups. Furthermore, we demonstrate that the co-movement\nof bond yields is frequency (scale) dependent.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.03347v3"
    },
    {
        "title": "How log-normal is your country? An analysis of the statistical\n  distribution of the exported volumes of products",
        "authors": [
            "Mario Alberto Annunziata",
            "Alberto Petri",
            "Giorgio Pontuale",
            "Andrea Zaccaria"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We have considered the statistical distributions of the volumes of the\ndifferent products exported by 148 countries. We have found that the form of\nthese distributions is not unique but heavily depends on the level of\ndevelopment of the nation, as expressed by macroeconomic indicators like GDP,\nGDP per capita, total export and a recently introduced measure for countries'\neconomic complexity called fitness. We have identified three major classes: a)\nan incomplete log-normal shape, truncated on the left side, for the less\ndeveloped countries, b) a complete log-normal, with a wider range of volumes,\nfor nations characterized by intermediate economy, and c) a strongly asymmetric\nshape for countries with a high degree of development. The ranking curves of\nthe exported volumes from each country seldom cross each other, showing a clear\nhierarchy of export volumes. Finally, the log-normality hypothesis has been\nchecked for the distributions of all the 148 countries through different tests,\nKolmogorov-Smirnov and Cramer-Von Mises, confirming that it cannot be rejected\nonly for the countries of intermediate economy.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.03597v1"
    },
    {
        "title": "Resolute refinements of social choice correspondences",
        "authors": [
            "Daniela Bubboloni",
            "Michele Gori"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Many classical social choice correspondences are resolute only in the case of\ntwo alternatives and an odd number of individuals. Thus, in most cases, they\nadmit several resolute refinements, each of them naturally interpreted as a\ntie-breaking rule, satisfying different properties. In this paper we look for\nclasses of social choice correspondences which admit resolute refinements\nfulfilling suitable versions of anonymity and neutrality. In particular,\nsupposing that individuals and alternatives have been exogenously partitioned\ninto subcommittees and subclasses, we find out arithmetical conditions on the\nsizes of subcommittees and subclasses that are necessary and sufficient for\nmaking any social choice correspondence which is efficient, anonymous with\nrespect to subcommittees, neutral with respect to subclasses and possibly\nimmune to the reversal bias admit a resolute refinement sharing the same\nproperties.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.06069v2"
    },
    {
        "title": "A Bayesian Model of the Litigation Game",
        "authors": [
            "Enrique Guerra-Pujol"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Over a century ago, Oliver Wendell Holmes invited scholars to look at the law\nthrough the lens of probability theory: \"The prophecies of what the courts will\ndo in fact, and nothing more pretentious, are what I mean by the law.\" Yet few\nlegal scholars have taken up this intriguing invitation. As such, in place of\nprevious approaches to the study of law, this paper presents a non-normative,\nmathematical approach to law and the legal process. Specifically, we present a\nformal Bayesian model of civil and criminal litigation, or what we refer to as\nthe litigation game; that is, instead of focusing on the rules of civil or\ncriminal procedure or substantive legal doctrine, we ask and attempt to answer\na mathematical question: what is the posterior probability that a defendant in\na civil or criminal trial will be found liable, given that the defendant has,\nin fact, committed a wrongful act?\n",
        "pdf_link": "http://arxiv.org/pdf/1506.07854v1"
    },
    {
        "title": "Note on tax enforcement and transfer pricing manipulation",
        "authors": [
            "Alex Augusto Timm Rathke"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This note proposes the segregation of independent endogenous and exogenous\ncomponents of tax penalty probability to introduce a formal demonstration that\nenforcement and tax penalties are negatively related with income shifting. JEL\nF23; H26.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.08743v1"
    },
    {
        "title": "Key drivers of EU budget allocation: Does power matter?",
        "authors": [
            "Vera Zaporozhets",
            "María García-Valiñas",
            "Sascha Kurz"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We examine the determinants of the EU budget expenditures allocation among\ndifferent countries. In line with earlier literature, we consider two\nalternative explanations for the EU budget distribution: political power vs.\nneeds view. Extending the original data set from Kauppi and Widgr\\'en (2004),\nwe analyze the robustness of their predictions when applying a different\nmeasure of power and more sophisticated econometric techniques. We conclude\nthat the nucleolus is a good alternative to the Shapley-Shubik index in\ndistributive situations such as the case of EU budget allocation. Our results\nalso show that when explaining budget shares, the relative weight of political\npower based on the nucleolus is lower than the predictions of previous studies\nbased on the Shapley-Shubik index.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.01267v1"
    },
    {
        "title": "Optimal environmental tax swaps and double dividend hypothesis",
        "authors": [
            "Su-Mei Chen",
            "Ling-Yun He"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Taking environmental tax rate as given, is there an optimal allocation of tax\nrevenues to benefit economic variables? This paper analyzes this issue in an\noverlapping-generations model with the pollution-related health damage. It\nfinds the optimal allocations towards pollution abatement and labor income to\nmaximize the steady-state lifetime welfare and per-worker output, respectively.\nMoreover, a greater shift towards labor income might enhance steady-state\nwelfare while reducing per-worker output.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.01626v1"
    },
    {
        "title": "Sovereign Default Risk and Uncertainty Premia",
        "authors": [
            "Demian Pouzo",
            "Ignacio Presno"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper studies how international investors' concerns about model\nmisspecification affect sovereign bond spreads. We develop a general\nequilibrium model of sovereign debt with endogenous default wherein investors\nfear that the probability model of the underlying state of the borrowing\neconomy is misspecified. Consequently, investors demand higher returns on their\nbond holdings to compensate for the default risk in the context of uncertainty.\nIn contrast with the existing literature on sovereign default, we match the\nbond spreads dynamics observed in the data together with other business cycle\nfeatures for Argentina, while preserving the default frequency at historical\nlow levels.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.06960v1"
    },
    {
        "title": "Unified Growth Theory Contradicted by the Economic Growth in Europe",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Historical economic growth in Western and Eastern Europe is analysed. These\nregions should have produced the best and the most convincing confirmation of\nthe Unified Growth Theory because they, and in particular Western Europe, were\nthe centre of the Industrial Revolution, which according to Galor was the prime\nengine of economic growth. However, the data for Western and Eastern Europe\nshow a remarkable disagreement with the Unified Growth Theory. There is no\nconnection, whatever, between the data and the Unified Growth Theory. The data\nshow that there was never a transition from stagnation to growth because there\nwas no stagnation. Industrial Revolution, which should have the strongest\ninfluence in these regions, had absolutely no impact on changing the economic\ngrowth trajectories. The alleged remarkable or stunning escape from Malthusian\ntrap did not happen because there was no trap. Unified Growth Theory does not\nexplain the mechanism of the economic growth because its explanations are based\non mythical features, which did not exist, the features contradicted by data.\nThis theory needs to be either thoroughly revised or most likely replaced by a\ntheory supported by a professional analysis of economic growth data.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.08067v2"
    },
    {
        "title": "Structural propagation in a production network with restoring\n  substitution elasticities",
        "authors": [
            "Satoshi Nakano",
            "Kazuhiko Nishimura"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We model an economy-wide production network by cascading binary compounding\nfunctions, based on the sequential processing nature of the production\nactivities. As we observe a hierarchy among the intermediate processes spanning\nthe empirical input--output transactions, we utilize a stylized sequence of\nprocesses for modeling the intra-sectoral production activities. Under the\nproductivity growth that we measure jointly with the state-restoring elasticity\nparameters for each sectoral activity, the network of production completely\nreplicates the records of multi-sectoral general equilibrium prices and shares\nfor all factor inputs observed in two temporally distant states. Thereupon, we\nstudy propagation of a small exogenous productivity shock onto the structure of\nproduction networks by way of hierarchical clustering.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.02216v7"
    },
    {
        "title": "Economic information from Smart Meter: Nexus Between Demand Profile and\n  Electricity Retail Price Between Demand Profile and Electricity Retail Price",
        "authors": [
            "Yang Yu",
            "Guangyi Liu",
            "Wendong Zhu",
            "Fei Wang",
            "Bin Shu",
            "Kai Zhang",
            "Ram Rajagopal",
            "Nicolas Astier"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In this paper, we demonstrate that a consumer's marginal system impact is\nonly determined by their demand profile rather than their demand level. Demand\nprofile clustering is identical to cluster consumers according to their\nmarginal impacts on system costs. A profile-based uniform-rate price is\neconomically efficient as real-time pricing. We develop a criteria system to\nevaluate the economic efficiency of an implemented retail price scheme in a\ndistribution system by comparing profile clustering and daily-average\nclustering. Our criteria system can examine the extent of a retail price\nscheme's inefficiency even without information about the distribution system's\ndaily cost structure. We analyze data from a real distribution system in China.\nIn this system, targeting each consumer's high-impact days is more efficient\nthan target high-impact consumers.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.02646v1"
    },
    {
        "title": "Sur la décomposabilité empirique des indicateurs de pauvreté",
        "authors": [
            "Gane Samb Lo",
            "Cheikh Mohamed Haidara"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We study the empirical decomposition of poverty indicators. This property is\nvery important and convenient in the context of the fight against poverty.\nIndeed, it makes it possible to put in place sectoral poverty reduction\npolicies on the basis of a relevant stratification laid down at the outset. The\nsimultaneous impacts of these policies, measured as reduction gains over the\npopulation as a whole, is then obtained by aggregating those obtained at each\nstratum by a relatively simple formula. It turns out that indicators as\nimportant as those of Sen and Shorrocks do not verify this property contrary to\nthe elements of the class of Foster - Greer and Thorbecke. Given the data from\nthe 1996 Senegalese Survey of Households (ESAM), we show that the lack of\ndecomposability of these indicators on the income variable for several types of\npopulation stratification is practically zero , of the order of one to two per\nthousand. This makes it possible to use the decomposition of the Sen and\nShorrocks indicators without any untoward consequences. An explanatory model of\nthese results is presented for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.02649v1"
    },
    {
        "title": "A geometric approach to the transfer problem for a finite number of\n  traders",
        "authors": [
            "Tomohiro Uchiyama"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We present a complete characterization of the classical transfer problem for\nan exchange economy with an arbitrary finite number of traders. Our method is\ngeometric, using an equilibrium manifold developed by Debreu, Mas-Colell, and\nBalasko. We show that for a regular equilibrium the transfer problem arises if\nand only if the index at the equilibrium is $-1$. This implies that the\ntransfer problem does not happen if the equilibrium is Walras tatonnement\nstable. Our result generalizes Balasko's analogous result for an exchange\neconomy with two traders.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.04491v1"
    },
    {
        "title": "Economic Growth Model with Constant Pace and Dynamic Memory",
        "authors": [
            "Valentina V. Tarasova",
            "Vasily E. Tarasov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  The article discusses a generalization of model of economic growth with\nconstant pace, which takes into account the effects of dynamic memory. Memory\nmeans that endogenous or exogenous variable at a given time depends not only on\ntheir value at that time, but also on their values at previous times. To\ndescribe the dynamic memory we use derivatives of non-integer orders. We obtain\nthe solutions of fractional differential equations with derivatives of\nnon-integral order, which describe the dynamics of the output caused by the\nchanges of the net investments and effects of power-law fading memory.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06299v2"
    },
    {
        "title": "Economics cannot isolate itself from political theory: a mathematical\n  demonstration",
        "authors": [
            "Brendan Markey-Towler"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The purpose of this paper is to provide a confession of sorts from an\neconomist to political science and philosophy. A confession of the weaknesses\nof the political position of the economist. It is intended as a guide for\npolitical scientists and philosophers to the ostensible policy criteria of\neconomics, and an illustration of an argument that demonstrates\nlogico-mathematically, therefore incontrovertibly, that any policy statement by\nan economist contains, or is, a political statement. It develops an inescapable\ncompulsion that the absolute primacy and priority of political theory and\nphilosophy in the development of policy criteria must be recognised. Economic\npolicy cannot be divorced from politics as a matter of mathematical fact, and\nrather, as Amartya Sen has done, it ought embrace political theory and\nphilosophy.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06410v1"
    },
    {
        "title": "Econophysics Macroeconomic Model",
        "authors": [
            "Victor Olkhov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This paper presents macroeconomic model that is based on parallels between\nmacroeconomic multi-agent systems and multi-particle systems. We use risk\nratings of economic agents as their coordinates on economic space. Aggregates\nof economic or financial variables like Investment, Assets, Demand, Credits and\netc. of economic agents near point x define corresponding macroeconomic\nvariables as functions of time t and coordinates x on economic space. Parallels\nbetween multi-agent and multi-particle systems on economic space allow describe\ntransition from economic kinetic-like to economic hydrodynamic-like\napproximation and derive macroeconomic hydrodynamic-like equations on economic\nspace. Economic or financial transactions between economic agents determine\nevolution of macroeconomic variables This paper describes local macroeconomic\napproximation that takes into account transactions between economic agents with\ncoordinates near same point x on economic space only and describes interaction\nbetween macroeconomic variables by linear differential operators. For simple\nmodel of interaction between macroeconomic variables as Demand on Investment\nand Interest Rate we derive hydrodynamic-like equations in a closed form. For\nperturbations of these macroeconomic variables we derive macroeconomic wave\nequations. Macroeconomic waves on economic space can propagate with exponential\ngrowth of amplitude and cause irregular time fluctuations of macroeconomic\nvariables or induce economic crises.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06625v1"
    },
    {
        "title": "Research and Teaching Efficiencies of Turkish Universities with\n  Heterogeneity Considerations: Application of Multi-Activity DEA and DEA by\n  Sequential Exclusion of Alternatives Methods",
        "authors": [
            "Y. Çinar"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The research and teaching efficiencies of 45 Turkish state universities are\nevaluated by using Multi-Activity Data Envelopment Analysis (MA-DEA) model\ndeveloped by Beasley (1995). Universities are multi-purpose institutions,\ntherefore they face multiple production functions simultaneously associated\nwith research and teaching activities. MA-DEA allows assigning priorities and\nallocating shared resources to these activities.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.07318v1"
    },
    {
        "title": "An analysis of potential conflict zones in the arctic region",
        "authors": [
            "F. Aleskerov",
            "E. Victorova"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  As a result of the climate change the situation in Arctic area leads to\nseveral important consequences. On the one hand, oil and gas resources can be\nexploited much easier than before. Thus, one can already observe discussions on\ndisputed shelf zones where the deposits are located. On the other hand, oil and\ngas excavation leads to serious potential threats to fishing by changing\nnatural habitats which in turn can create serious damage to the economies of\nsome countries in the region. Another set of problems arises due to the\nextension of navigable season for Arctic Shipping Routes.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.07321v1"
    },
    {
        "title": "Heterogeneity of the educational system: an introduction to the problem",
        "authors": [
            "F. Aleskerov",
            "I. Frumin",
            "E. Kardanova"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We analyze a heterogeneity of the educational system on the basis of one\nparameter: input grades of university students. We propose a mathematical model\nbased on the construction of universities interval order. We use the Hamming\ndistance to evaluate the heterogeneity of the educational system, and the\nUnified State Examination (USE) scores of Russian students to illustrate the\napplication of the model. We show that institutions taking weak students turn\nthe whole system of universities into a poorly structured nonhomogeneous\nsystem. In contrast, after deleting the weakest part, the remaining set of\nuniversities becomes a well-structured system\n",
        "pdf_link": "http://arxiv.org/pdf/1701.07322v1"
    },
    {
        "title": "Hyperbolic Discounting of the Far-Distant Future",
        "authors": [
            "Nina Anchugina",
            "Matthew Ryan",
            "Arkadii Slinko"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We prove an analogue of Weitzman's (1998) famous result that an exponential\ndiscounter who is uncertain of the appropriate exponential discount rate should\ndiscount the far-distant future using the lowest (i.e., most patient) of the\npossible discount rates. Our analogous result applies to a hyperbolic\ndiscounter who is uncertain about the appropriate hyperbolic discount rate. In\nthis case, the far-distant future should be discounted using the\nprobability-weighted harmonic mean of the possible hyperbolic discount rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.01362v1"
    },
    {
        "title": "Demonetization and Its Impact on Employment in India",
        "authors": [
            "Pawan Kumar"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  On November 08, the sudden announcement to demonetization the high\ndenomination currency notes sent tremors all across the country. Given the\ntiming, and socioeconomic and political repercussions of the decision, many\ntermed it a financial emergency. Given high proportion of these notes in\ncirculation, over 86 percent, it led to most economic activities, particularly\nemployment, affected in a big way. Political parties, however, seemed divided\non the issue, i.e. those in favor of the decision feel it will help to curb the\ngalloping size of black money, fake currency, cross boarder terrorism, etc. In\nsharp contrast, the others believe it is a purely misleading, decision, based\non no or poor understanding of black economy, and hence is only politically\nmotivated in wake of the assembly elections due in a couple of states.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.01686v1"
    },
    {
        "title": "The Installation Costs of a Satellite and Space Shuttle Launch Complex\n  as a Public Expenditure Project",
        "authors": [
            "Dogus Ozuyar",
            "Sevilay Gumus Ozuyar",
            "Oguzhan Karadeniz",
            "Ozge Varol"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  From the 1940's to the present, space explorations, which is a highly\nimportant topic for the world and human beings, penetrate into many areas from\nthe communication to the national security as well as from the discovery of\nexoplanets and new life forms to space mining. On the top of the countries\nwhich do researches on these fields are the developed countries and the\ndeveloping countries are only used as launch areas, in an irrelevant manner of\nthe research and development. However, developing countries can significantly\nreduce foreign dependency and security flaws as well as providing important\nreputation gain in international platforms by conducting space research and\ndevelopment activities as already done by developed countries. All the large\nscale space probes conducted by developed countries oblige Turkey to develop\nspace researches in terms of economy, security and scientific aspects. Due to\nthese reasons, the approximate costs of a launch base, which will be installed\nto conduct space researches in Turkey, and of a satellite or a spacecraft,\nwhich will be able to launch from this base and serve a variety of purposes,\nare calculated in this study. In an effort to make the mentioned calculations,\nexamples of various countries that have already established a launch base and\nalready launched from these bases are analyzed and some projections are built\nfor Turkey by calculating the estimated costs. Since these projections must be\ncarried out by the Republic of Turkey since the private sector in Turkey will\nnot be willing to invest in such activities, the possible public advantages can\nbe gained through these activities are also mentioned and evaluated.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.02007v1"
    },
    {
        "title": "One-Switch Discount Functions",
        "authors": [
            "Nina Anchugina"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Bell (1988) introduced the one-switch property for preferences over sequences\nof dated outcomes. This property concerns the effect of adding a common delay\nto two such sequences: it says that the preference ranking of the delayed\nsequences is either independent of the delay, or else there is a unique delay\nsuch that one strict ranking prevails for shorter delays and the opposite\nstrict ranking for longer delays. For preferences that have a discounted\nutility (DU) representation, Bell (1988) argues that the only discount\nfunctions consistent with the one-switch property are sums of exponentials.\nThis paper proves that discount functions of the linear times exponential form\nalso satisfy the one-switch property. We further demonstrate that preferences\nwhich have a DU representation with a linear times exponential discount\nfunction exhibit increasing impatience (Takeuchi (2011)). We also clarify an\nambiguity in the original Bell (1988) definition of the one-switch property by\ndistinguishing a weak one-switch property from the (strong) one-switch\nproperty. We show that the one-switch property and the weak one-switch property\ndefinitions are equivalent in a continuous-time version of the Anscombe and\nAumann (1963) setting.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.02254v1"
    },
    {
        "title": "Econophysics of Macroeconomics: \"Action-at-a-Distance\" and Waves",
        "authors": [
            "Victor Olkhov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We present macroeconomic model that describes evolution of macroeconomic\nvariables and macroeconomic waves on economic space. Risk ratings of economic\nagents play role of their coordinates on economic space. Aggregation of\neconomic variables like Assets and Investment, Credits and Loans of economic\nagents at point x define corresponding macroeconomic variables as functions of\ntime t and coordinates x on economic space. Evolution of macroeconomic\nvariables is determined by economic and financial transactions between economic\nagents. Such transactions can occur between economic agents with any\ncoordinates x and y and that reflect non-local \"action-at-a-distance\" character\nof internal macroeconomic interactions. For instance, Buy-Sell transactions\nbetween points x and y on economic space define dynamics of Assets at point x\nand Investment at point y. Aggregates of transactions between economic agents\nat point x and y on economic space define economic fields as functions of two\ncoordinates. To describe dynamics of economic fields on economic space we\nderive hydrodynamic-like equations. For simple models of interactions between\neconomic fields we derive hydrodynamic-like equations in a closed form and\nobtain wave equations for their perturbations. Economic field waves propagate\non economic space and their amplitudes can grow up as exponent in time and may\ndisturb economic stability. Diversities of macroeconomic and financial waves on\neconomic space in simple models uncover importance of wave processes for\nmacroeconomic modeling and forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.02763v1"
    },
    {
        "title": "A Theory of Market Efficiency",
        "authors": [
            "Anup Rao"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We introduce a mathematical theory called market connectivity that gives\nconcrete ways to both measure the efficiency of markets and find inefficiencies\nin large markets. The theory leads to new methods for testing the famous\nefficient markets hypothesis that do not suffer from the joint-hypothesis\nproblem that has plagued past work. Our theory suggests metrics that can be\nused to compare the efficiency of one market with another, to find\ninefficiencies that may be profitable to exploit, and to evaluate the impact of\npolicy and regulations on market efficiency.\n  A market's efficiency is tied to its ability to communicate information\nrelevant to market participants. Market connectivity calculates the speed and\nreliability with which this communication is carried out via trade in the\nmarket. We model the market by a network called the trade network, which can be\ncomputed by recording transactions in the market over a fixed interval of time.\nThe nodes of the network correspond to participants in the market. Every pair\nof nodes that trades in the market is connected by an edge that is weighted by\nthe rate of trade, and associated with a vector that represents the type of\nitem that is bought or sold.\n  We evaluate the ability of the market to communicate by considering how it\ndeals with shocks. A shock is a change in the beliefs of market participants\nabout the value of the products that they trade. We compute the effect of every\npotential significant shock on trade in the market. We give mathematical\ndefinitions for a few concepts that measure the ability of the market to\neffectively dissipate shocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.03290v1"
    },
    {
        "title": "Labor Contract Law -An Economic View",
        "authors": [
            "Yaofeng Fu",
            "Ruokun Huang",
            "Yiran Sheng"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  China's new labor law -- Labor Contract Law has been put into practice for\nover one year. Since its inception, debates have been whirling around the\nnation, if not the world. In this article, we take an economic perspective to\nanalyze the possible impact of the core item -- open-ended employment contract,\nand we find that it deals poorly with adverse selection, with moral hazard\nproblems arise, which fails to meet the expectations of law-makers and other\nparties.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.03977v1"
    },
    {
        "title": "Multi-Dimensional Pass-Through and Welfare Measures under Imperfect\n  Competition",
        "authors": [
            "Takanori Adachi",
            "Michal Fabinger"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This paper provides a comprehensive analysis of welfare measures when\noligopolistic firms face multiple policy interventions and external changes\nunder general forms of market demands, production costs, and imperfect\ncompetition. We present our results in terms of two welfare measures, namely,\nmarginal cost of public funds and incidence, in relation to multi-dimensional\npass-through. Our arguments are best understood with two-dimensional taxation\nwhere homogeneous firms face unit and ad valorem taxes. The first part of the\npaper studies this leading case. We show, e.g., that there exists a simple and\nempirically relevant set of sufficient statistics for the marginal cost of\npublic funds, namely unit tax and ad valorem pass-through and industry demand\nelasticity. We then specialize our general setting to the case of price or\nquantity competition and show how the marginal cost of public funds and the\npass-through are expressed using elasticities and curvatures of regular and\ninverse demands. Based on the results of the leading case, the second part of\nthe paper presents a generalization with the tax revenue function specified as\na general function parameterized by a vector of multi-dimensional tax\nparameters. We then argue that our results are carried over to the case of\nheterogeneous firms and other extensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.04967v3"
    },
    {
        "title": "Explaining the Mechanism of Growth in the Past Two Million Years Vol. I",
        "authors": [
            "Ron W. Nielsen"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Economic growth and the growth of human population in the past 2,000,000\nyears are extensively examined. Data are found to be in a clear contradiction\nof the currently accepted explanations of the mechanism of growth, which\nrevolve around two fundamental but incorrect doctrines: (1) the doctrine of\nstagnation (inappropriately labelled also as Malthusian stagnation, because\nMalthus never claimed that his positive checks would cause a long-lasting and\nwide-spread stagnation) and (2) the doctrine of explosion described also as a\ntakeoff, sprint, spike or by other similar attributes. These doctrines and\nother related postulates are contradicted even by precisely the same data,\nwhich are used in the economic research and by the research results published\nin a prestigious scientific journal as early as in 1960. The generally accepted\nexplanations are not based on a rigorous analysis of data but on impressions\ncreated by the easily misleading features of hyperbolic distributions. Two\nleading theories: the Demographic Transitions Theory (or Model) and the Unified\nGrowth Theory are fundamentally incorrect. Descriptions of the past\nsocio-economic conditions are not questioned. They might have been harsh,\ndifficult and primitive but they are not reflected in the growth trajectories.\nThey did not create stagnation in the economic growth and in the growth of\npopulation. Likewise, impacts of the Industrial Revolution on many aspects of\nlife are not questioned. It is only demonstrated that this event had absolutely\nno impact on shaping growth trajectories. A general law of growth is formulated\nand used to explain the mechanism of growth of human population and of economic\ngrowth. The growth was predominantly hyperbolic. Such a growth is described by\nexceptionally simple mathematical function and the explanation of the mechanism\nof growth turns out to be also simple.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.01768v1"
    },
    {
        "title": "Robust Forecast Aggregation",
        "authors": [
            "Itai Areili",
            "Yakov Babichenko",
            "Rann Smorodinsky"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Bayesian experts who are exposed to different evidence often make\ncontradictory probabilistic forecasts. An aggregator, ignorant of the\nunderlying model, uses this to calculate her own forecast. We use the notions\nof scoring rules and regret to propose a natural way to evaluate an aggregation\nscheme. We focus on a binary state space and construct low regret aggregation\nschemes whenever there are only two experts which are either Blackwell-ordered\nor receive conditionally i.i.d. signals. In contrast, if there are many experts\nwith conditionally i.i.d. signals, then no scheme performs (asymptotically)\nbetter than a $(0.5,0.5)$ forecast.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.02838v3"
    },
    {
        "title": "Mean Field Game Approach to Production and Exploration of Exhaustible\n  Commodities",
        "authors": [
            "Michael Ludkovski",
            "Xuwei Yang"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  In a game theoretic framework, we study energy markets with a continuum of\nhomogenous producers who produce energy from an exhaustible resource such as\noil. Each producer simultaneously optimizes production rate that drives her\nrevenues, as well as exploration effort to replenish her reserves. This\nexploration activity is modeled through a controlled point process that leads\nto stochastic increments to reserves level. The producers interact with each\nother through the market price that depends on the aggregate production. We\nemploy a mean field game approach to solve for a Markov Nash equilibrium and\ndevelop numerical schemes to solve the resulting system of non-local HJB and\ntransport equations with non-local coupling. A time-stationary formulation is\nalso explored, as well as the fluid limit where exploration becomes\ndeterministic.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.05131v1"
    },
    {
        "title": "Disruptive firms",
        "authors": [
            "Mario Coccia"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This study proposes the concept of disruptive firms: they are firms with\nmarket leadership that deliberate introduce new and improved generations of\ndurable goods that destroy, directly or indirectly, similar products present in\nmarkets in order to support their competitive advantage and/or market\nleadership. These disruptive firms support technological and industrial change\nand induce consumers to buy new products to adapt to new socioeconomic\nenvironment. In particular, disruptive firms generate and spread path-breaking\ninnovations in order to achieve and sustain the goal of a (temporary) profit\nmonopoly. This organizational behaviour and strategy of disruptive firms\nsupport technological change. This study can be useful for bringing a new\nperspective to explain and generalize one of the determinants that generates\ntechnological and industrial change. Overall, then this study suggests that one\nof the general sources of technological change is due to disruptive firms\n(subjects), rather than disruptive technologies (objects), that generate market\nshifts in a Schumpeterian world of innovation-based competition.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.06132v1"
    },
    {
        "title": "The tipping point: a mathematical model for the profit-driven\n  abandonment of restaurant tipping",
        "authors": [
            "Sara M. Clifton",
            "Eileen Herbers",
            "Jack Chen",
            "Daniel M. Abrams"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  The custom of voluntarily tipping for services rendered has gone in and out\nof fashion in America since its introduction in the 19th century. Restaurant\nowners that ban tipping in their establishments often claim that social justice\ndrives their decisions, but we show that rational profit-maximization may also\njustify the decisions. Here, we propose a conceptual model of restaurant\ncompetition for staff and customers, and we show that there exists a critical\nconventional tip rate at which restaurant owners should eliminate tipping to\nmaximize profit. Because the conventional tip rate has been increasing steadily\nfor the last several decades, our model suggests that restaurant owners may\nabandon tipping en masse when that critical tip rate is reached.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.06893v2"
    },
    {
        "title": "Functional Forms for Tractable Economic Models and the Cost Structure of\n  International Trade",
        "authors": [
            "Michal Fabinger",
            "E. Glen Weyl"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We present functional forms allowing a broader range of analytic solutions to\ncommon economic equilibrium problems. These can increase the realism of\npen-and-paper solutions or speed large-scale numerical solutions as\ncomputational subroutines. We use the latter approach to build a tractable\nheterogeneous firm model of international trade accommodating economies of\nscale in export and diseconomies of scale in production, providing a natural,\nunified solution to several puzzles concerning trade costs. We briefly\nhighlight applications in a range of other fields. Our method of generating\nanalytic solutions is a discrete approximation to a logarithmically modified\nLaplace transform of equilibrium conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.02270v2"
    },
    {
        "title": "The missing assets and the size of Shadow Banking: an update",
        "authors": [
            "Davide Fiaschi",
            "Imre Kondor",
            "Matteo Marsili",
            "Valerio Volpati"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In a recent paper, using data from Forbes Global 2000, we have observed that\nthe upper tail of the firm size distribution (by assets) falls off much faster\nthan a Pareto distribution. The missing mass was suggested as an indicator of\nthe size of the Shadow Banking (SB) sector. This short note provides the latest\nfigures of the missing assets for 2013, 2014 and 2015. In 2013 and 2014 the\ndynamics of the missing assets continued being strongly correlated with\nestimates of the size of the SB sector of the Financial Stability Board. In\n2015 we find a sharp decrease in the size of missing assets, suggesting that\nthe SB sector is deflating.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.02760v1"
    },
    {
        "title": "Toward Economics as a New Complex System",
        "authors": [
            "Taisei Kaizoji"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The 2015 Nobel Prize in Economic Sciences was awarded to Eugene Fama, Lars\nPeter Hansen and Robert Shiller for their contributions to the empirical\nanalysis of asset prices. Eugene Fama [1] is an advocate of the efficient\nmarket hypothesis. The efficient market hypothesis assumes that asset price is\ndetermined by using all available information and only reacts to new\ninformation not incorporated into the fundamentals. Thus, the movement of stock\nprices is unpredictable. Robert Shiller [2] has been studying the existence of\nirrational bubbles, which are defined as the long term deviations of asset\nprice from the fundamentals. This drives us to the unsettled question of how\nthe market actually works.\n  In this paper, I look back at the development of economics and consider the\ndirection in which we should move in order to truly understand the workings of\nan economic society.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.05280v1"
    },
    {
        "title": "Exponential Structure of Income Inequality: Evidence from 67 Countries",
        "authors": [
            "Yong Tao",
            "Xiangjun Wu",
            "Tao Zhou",
            "Weibo Yan",
            "Yanyuxiang Huang",
            "Han Yu",
            "Benedict Mondal",
            "Victor M. Yakovenko"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Economic competition between humans leads to income inequality, but, so far,\nthere has been little understanding of underlying quantitative mechanisms\ngoverning such a collective behavior. We analyze datasets of household income\nfrom 67 countries, ranging from Europe to Latin America, North America and\nAsia. For all of the countries, we find a surprisingly uniform rule: Income\ndistribution for the great majority of populations (low and middle income\nclasses) follows an exponential law. To explain this empirical observation, we\npropose a theoretical model within the standard framework of modern economics\nand show that free competition and Rawls' fairness are the underlying\nmechanisms producing the exponential pattern. The free parameters of the\nexponential distribution in our model have an explicit economic interpretation\nand direct relevance to policy measures intended to alleviate income\ninequality.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.01624v2"
    },
    {
        "title": "Are Chinese transport policies effective? A new perspective from direct\n  pollution rebound effect, and empirical evidence from road transport sector",
        "authors": [
            "Lu-Yi Qiu",
            "Ling-Yun He"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The air pollution has become a serious challenge in China. Emissions from\nmotor vehicles have been found as one main source of air pollution. Although\nthe Chinese government has taken numerous policies to mitigate the harmful\nemissions from road transport sector, it is still uncertain for both policy\nmakers and researchers to know to what extent the policies are effective in the\nshort and long terms. Inspired by the concept and empirical results from\ncurrent literature on energy rebound effect (ERE), we first propose a new\nconcept of pollution rebound effect (PRE). Then, we estimate direct air PRE as\na measure for the effectiveness of the policies of reducing air pollution from\ntransport sector based on time-series data from the period 1986-2014. We find\nthat the short-term direct air PRE is -1.4105, and the corresponding long-run\nPRE is -1.246. The negative results indicate that the direct air PRE does not\nexist in road passenger transport sector in China, either in the short term or\nin the long term during the period 1986-2014. This implies that the Chinese\ntransport policies are effective in terms of harmful emissions reduction in the\ntransport sector. This research, to the best of our knowledge, is the first\nattempt to quantify the effectiveness of the transport policies in the\ntransitional China.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.02653v1"
    },
    {
        "title": "China building energy consumption: definitions and measures from an\n  operational perspective",
        "authors": [
            "Ling-Yun He",
            "Wei Wei"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  There is an increasing awareness of the significance of Chinese building\nenergy consumption(BEC). However, something worth discussing is that estimate\nthe building energy consumption adopting the definition of life cycle or\noperation. In the existing studies with various evaluation methods, the issue\nabout the amount of energy consumed by China buildings has not been understood.\nIn order to settle the disputes over the calculation of BEC, this paper\nestablish an appropriate accounting method of building energy to present BEC\nsituation in China and lay the foundation for building energy efficiency.\nAdopting the conception of building operational energy consumption, we find\nthat the energy consumption of buildings just accounts for 15% - 16% of the\nfinal total energy consumption in China; by contrast, the previous calculations\nusually have double accounting through top-down approach if central heat-supply\nof buildings was given into additional consideration.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.02654v1"
    },
    {
        "title": "The demand for road transport in China: imposing theoretical regularity\n  and flexible functional forms selection",
        "authors": [
            "Ling-yun He",
            "Li Liu"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Road transport sector is found to be one of the major emitters, and\nresponsible for serious air pollution and huge pubic health losses. One\nimportant parameter for determining the consequences of transport demand shocks\nfor the macroeconomy, air pollution and public health is the elasticity of the\ndemand for transport. Most published studies that use flexible functional forms\nhave ignored the theoretical regularity conditions implied by microeconomic\ntheories. Moreover, even a few studies have checked and/or imposed regularity\nconditions, most of them equate curvature alone with regularity, thus ignoring\nor minimizing the importance of other regularities. And then, the results\nappear biased and may in fact be biased. Therefore, we select three of the most\nwidely used flexible functional forms, the Rotterdam model, the Almost Ideal\nDemand System (AIDS), and the quadratic AIDS (QUAIDS) to investigate the demand\nfor road transport in China using recent annual expenditure data, over a 13\nyear period from 2002 to 2014, on three expenditure categories in the\ntransportation sector: private transportation, local transportation and\nintercity transportation. Estimation shows that the AIDS model is the only\nmodel that is able to provide theoretically consistent estimates of the\nresidents demand for road transport in China. Our estimates show that the\nprivate transportation is a luxury among the transportation goods, and is\nelastic in price changes relatively. The empirical results imply that the\nprivate and the local transportation, the local and intercity transportation\nare gross complements. And, the private transportation is a substitute for the\ninter-city transportation, while the intercity transportation is a complement\nof the private transportation.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.02656v1"
    },
    {
        "title": "How do Chinese cities grow? A distribution dynamics approach",
        "authors": [
            "Jian-Xin Wu",
            "Ling-Yun He"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  This paper examines the dynamic behavior of city size using a distribution\ndynamics approach with Chinese city data for the period 1984-2010. Instead of\nconvergence, divergence or paralleled growth, multimodality and persistence are\nthe dominant characteristics in the distribution dynamics of Chinese\nprefectural cities. Moreover, initial city size matters, initially small and\nmedium-sized cities exhibit strong tendency of convergence, while large cities\nshow significant persistence and multimodality in the sample period.\nExamination on the regional city groups shows that locational fundamentals have\nimportant impact on the distribution dynamics of city size.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.02657v1"
    },
    {
        "title": "The distribution dynamics of Carbon Dioxide Emission intensity across\n  Chinese provinces: A weighted Approach",
        "authors": [
            "Jian-Xin Wu",
            "Ling-Yun He"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  This paper examines the distribution dynamics of carbon dioxide (CO2)\nemission intensity across 30 Chinese provinces using a weighted distribution\ndynamics approach. The results show that CO2 emission intensity tends to\ndiverge during the sample period of 1995-2014. However, convergence clubs are\nfound in the ergodic distributions of the full sample and two sub-sample\nperiods. Divergence, polarization and stratification are the dominant\ncharacteristics in the distribution dynamics. Weightings with economic and\npopulation sizes have important impacts on current distributions and hence long\nrun steady distributions. Neglecting economic size may under-estimate the\ndeterioration in the long run steady state. The result also shows that\nconditioning on space and income cannot eliminate the multimodality in the long\nrun distribution. However, capital intensity has important impact on the\nformation of convergence clubs. Our findings have contributions in the\nunderstanding of the spatial dynamic behaviours of CO2 emissions across Chinese\nprovinces, and have important policy implications for CO2 emissions reduction\nin China.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.02658v1"
    },
    {
        "title": "The Topology of Inter-industry Relations from the Portuguese National\n  Accounts",
        "authors": [
            "Tanya Araújo",
            "Rui Faustino"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In last years, the Portuguese economy has gone through a severe adjustment\nprocess, affecting almost all industrial sectors, the building blocks of\neconomic structures. Research on economic structural changes has made use of\ninput/output tables to define networks of industrial relations. Here, these\nnetworks are induced from output tables of the Portuguese national accounting\nsystem, being each inter-industry relation defined by the output made by any\ntwo industries for the products that they both produce. The topological\nanalysis of these networks allows to uncover a particular structure that comes\nout during the Portuguese adjustment program. The evolution of the industrial\nnetworks shows an important structural change in 2011-2014, confirming the\nusefulness of inducting similarity networks from output tables and the\nconsequent promising power of the graph formulation for the analysis of\ninter-industry relations.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.06291v1"
    },
    {
        "title": "Rating evaluation of sports development efficiency using statistical\n  analysis: evidence from Russian football",
        "authors": [
            "Ilya Solntsev",
            "Anatoly Vorobyev",
            "Elnura Irmatova",
            "Nikita Osokin"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Increasing investments into various dimensions of sports draw a significant\namount of attention to the way these resources are being managed and which\norganizations achieve development goals with higher efficiency. This paper\nreviews the methodology of designing an efficiency rating model for assessing\nsports entities, focusing on the experience of Russian football. The Russian\nRegional Efficiency of Football Development model aims to evaluate the regional\nfederations of the Football Union of Russian via 5 dimensions. The scoring\nmethod of the model is based on the three-sigma rule of distribution. Support\nfactors in the form of population density and climate were also included, since\nRussian regions significantly differentiate in these aspects. The findings of\nthis paper showcased that not a single region was able to achieve a maximum 5-\nstar rating, while regions set to host the 2018 FIFA World Cup did not score\nbetter compared to others. In conclusion the authors provide various\nsuggestions on further developing and implementing rating models within global\nsports organizations.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07543v1"
    },
    {
        "title": "A Proposal to Extend Expected Utility in a Quantum Probabilistic\n  Framework",
        "authors": [
            "Diederik Aerts",
            "Emmanuel Haven",
            "Sandro Sozzo"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Expected utility theory (EUT) is widely used in economic theory. However, its\nsubjective probability formulation, first elaborated by Savage, is linked to\nEllsberg-like paradoxes and ambiguity aversion. This has led various scholars\nto work out non-Bayesian extensions of EUT which cope with its paradoxes and\nincorporate attitudes toward ambiguity. A variant of the Ellsberg paradox,\nrecently proposed by Mark Machina and confirmed experimentally, challenges\nexisting non-Bayesian models of decision-making under uncertainty. Relying on a\ndecade of research which has successfully applied the formalism of quantum\ntheory to model cognitive entities and fallacies of human reasoning, we put\nforward a non-Bayesian extension of EUT in which subjective probabilities are\nrepresented by quantum probabilities, while the preference relation between\nacts depends on the state of the situation that is the object of the decision.\nWe show that the benefits of using the quantum theoretical framework enables\nthe modeling of the Ellsberg and Machina paradoxes, as the representation of\nambiguity and behavioral attitudes toward it. The theoretical framework\npresented here is a first step toward the development of a `state-dependent\nnon-Bayesian extension of EUT' and it has potential applications in economic\nmodeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08583v1"
    },
    {
        "title": "Fractional Dynamics of Natural Growth and Memory Effect in Economics",
        "authors": [
            "Valentina V. Tarasova",
            "Vasily E. Tarasov"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  A generalization of the economic model of natural growth, which takes into\naccount the power-law memory effect, is suggested. The memory effect means the\ndependence of the process not only on the current state of the process, but\nalso on the history of changes of this process in the past. For the\nmathematical description of the economic process with power-law memory we used\nthe theory of derivatives of non-integer order and fractional-order\ndifferential equation. We propose equations take into account the effects of\nmemory with one-parameter power-law damping. Solutions of these fractional\ndifferential equations are suggested. We proved that the growth and downturn of\noutput depend on the memory effects. We demonstrate that the memory effect can\nlead to decrease of output instead of its growth, which is described by model\nwithout memory effect. Memory effect can lead to increase of output, rather\nthan decrease, which is described by model without memory effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.09060v2"
    },
    {
        "title": "The Industry Supply Function and the Long-Run Competitive Equilibrium\n  with Heterogeneous Firms",
        "authors": [
            "Ignacio Esponda",
            "Demian Pouzo"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In developing the theory of long-run competitive equilibrium (LRCE), Marshall\n(1890) used the notion of a representative firm. The identity of this firm,\nhowever, remained unclear. Subsequent theory either focused on the case where\nall firms are identical or else incorporated heterogeneity but disregarded the\nnotion of a representative firm. Using Hopenhayn's (1992) model of competitive\nindustry dynamics, we extend the theory of LRCE to account for heterogeneous\nfirms and show that the long-run supply function can indeed be characterized as\nthe solution to the minimization of a representative average cost function.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.09549v3"
    },
    {
        "title": "Wage gap between men and women in Tunisia",
        "authors": [
            "Hela Jeddi",
            "Dhafer Malouche"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper focuses on estimating wage differences between males and females\nin Tunisia by using the Oaxaca-Blinder decomposition, a technical that isolates\nwage gap due to characteristics, from wage gap due to discrimination against\nwomen. The data used in the analysis is obtained from the Tunisian Population\nand Employment Survey 2005. It is estimated that, the gender wage gap is about\n19% and the results ascertain that the gender wage gap is mostly attributed to\ndiscrimination, especially to underestimation of females'caracteristics on the\nlabor market.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.02229v1"
    },
    {
        "title": "A simple framework for the axiomatization of exponential and\n  quasi-hyperbolic discounting",
        "authors": [
            "Nina Anchugina"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  The main goal of this paper is to investigate which normative requirements,\nor axioms, lead to exponential and quasi-hyperbolic forms of discounting.\nExponential discounting has a well-established axiomatic foundation originally\ndeveloped by Koopmans (1960, 1972) and Koopmans et al. (1964) with subsequent\ncontributions by several other authors, including Bleichrodt et al. (2008). The\npapers by Hayashi (2003) and Olea and Strzalecki (2014) axiomatize\nquasi-hyperbolic discounting. The main contribution of this paper is to provide\nan alternative foundation for exponential and quasi-hyperbolic discounting,\nwith simple, transparent axioms and relatively straightforward proofs. Using\ntechniques by Fishburn (1982) and Harvey (1986), we show that Anscombe and\nAumann's (1963) version of Subjective Expected Utility theory can be readily\nadapted to axiomatize the aforementioned types of discounting, in both finite\nand infinite horizon settings.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.06454v1"
    },
    {
        "title": "Some Dynamic Market Models",
        "authors": [
            "Jan A. Audestad"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In this text, we study the temporal behavior of markets using models\nexpressible as ordinary differential equations. The markets studied are those\nwhere each customer buys only one copy of the good, for example, subscription\nof smartphone service, journals and newspapers, and goods such as books, music\nand games. The underlying model is the diffusion model of Frank Bass. Evolution\nof markets with no competitors and markets with several competitors are\nanalyzed where, in particulat, the effects of churning upon the market\nevolution is investigated. Analytic solutions are given for the temporal\nevolution of several types of interactive games.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.07203v1"
    },
    {
        "title": "Water Stress on U.S. Power Production at Decadal Time Horizons",
        "authors": [
            "Poulomi Ganguli",
            "Devashish Kumar",
            "Auroop R. Ganguly"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Thermoelectric power production at risk, owing to current and projected water\nscarcity and rising stream temperatures, is assessed for the contiguous United\nStates at decadal scales. Regional water scarcity is driven by climate\nvariability and change, as well as by multi-sector water demand. While a\nplanning horizon of zero to about thirty years is occasionally prescribed by\nstakeholders, the challenges to risk assessment at these scales include the\ndifficulty in delineating decadal climate trends from intrinsic natural or\nmultiple model variability. Current generation global climate or earth system\nmodels are not credible at the spatial resolutions of power plants, especially\nfor surface water quantity and stream temperatures, which further exacerbates\nthe assessment challenge. Population changes, which are difficult to project,\ncannot serve as adequate proxies for changes in the water demand across\nsectors. The hypothesis that robust assessments of power production at risk are\npossible, despite the uncertainties, has been examined as a proof of concept.\nAn approach is presented for delineating water scarcity and temperature from\nclimate models, observations and population storylines, as well as for\nassessing power production at risk by examining geospatial correlations of\npower plant locations within regions where the usable water supply for energy\nproduction happens to be scarcer and warmer. Our analyses showed that in the\nnear term, more than 200 counties are likely to be exposed to water scarcity in\nthe next three decades. Further, we noticed that stream gauges in more than\nfive counties in the 2030s and ten counties in the 2040s showed a significant\nincrease in water temperature, which exceeded the power plant effluent\ntemperature threshold set by the EPA. Power plants in South Carolina,\nLouisiana, and Texas are likely to be vulnerable owing to climate-driven water\nstresses.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.08449v1"
    },
    {
        "title": "Complex economies have a lateral escape from the poverty trap",
        "authors": [
            "Emanuele Pugliese",
            "Guido L. Chiarotti",
            "Andrea Zaccaria",
            "Luciano Pietronero"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We analyze the decisive role played by the complexity of economic systems at\nthe onset of the industrialization process of countries over the past 50 years.\nOur analysis of the input growth dynamics, based on a recently introduced\nmeasure of economic complexity, reveals that more differentiated and more\ncomplex economies face a lower barrier (in terms of GDP per capita) when\nstarting the transition towards industrialization. Moreover, adding the\ncomplexity dimension to the industrialization process description helps to\nreconcile current theories with empirical findings.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.08622v1"
    },
    {
        "title": "Measurement of the evolution of technology: A new perspective",
        "authors": [
            "Mario Coccia"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  A fundamental problem in technological studies is how to measure the\nevolution of technology. The literature has suggested several approaches to\nmeasuring the level of technology (or state-of-the-art) and changes in\ntechnology. However, the measurement of technological advances and\ntechnological evolution is often a complex and elusive topic in science. The\nstudy here starts by establishing a conceptual framework of technological\nevolution based on the theory of technological parasitism, in broad analogy\nwith biology. Then, the measurement of the evolution of technology is modelled\nin terms of morphological changes within complex systems considering the\ninteraction between a host technology and its subsystems of technology. The\ncoefficient of evolutionary growth of the model here indicates the grade and\ntype of the evolutionary route of a technology. This coefficient is quantified\nin real instances using historical data of farm tractor, freight locomotive and\nelectricity generation technology in steam-powered plants and\ninternal-combustion plants. Overall, then, it seems that the approach here is\nappropriate in grasping the typology of evolution of complex systems of\ntechnology and in predicting which technologies are likeliest to evolve\nrapidly.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08698v1"
    },
    {
        "title": "Can Analysts Predict Rallies Better Than Crashes?",
        "authors": [
            "Ivan Medovikov"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We use the copula approach to study the structure of dependence between\nsell-side analysts' consensus recommendations and subsequent security returns,\nwith a focus on asymmetric tail dependence. We match monthly vintages of\nI/B/E/S recommendations for the period January to December 2011 with excess\nsecurity returns during six months following recommendation issue. Using a\nsymmetrized Joe-Clayton Copula (SJC) model we find evidence to suggest that\nanalysts can identify stocks that will substantially outperform, but not\nunderperform relative to the market, and that their predictive ability is\nconditional on recommendation changes.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.3225v1"
    },
    {
        "title": "Superstars in politics: the role of the media in the rise and success of\n  Junichiro Koizumi",
        "authors": [
            "Eiji Yamamura",
            "Fabio Sabatini"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  This paper explores the role of mass media in people perceptions of\ncharismatic leaders, focusing on the case of Junichiro Koizumi, Prime Minister\nof Japan from 2001 to 2006. Using survey data collected immediately after his\n2005 landslide electoral victory, this study empirically assesses the influence\nof television and newspapers on support for Koizumi and for the most\ndistinctive policy action he announced during his campaign, the privatization\nof the postal service.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.1726v1"
    },
    {
        "title": "Microscopic Models for Welfare Measures Addressing a Reduction of\n  Economic Inequality",
        "authors": [
            "Maria Letizia Bertotti",
            "Giovanni Modanese"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We formulate a flexible micro-to-macro kinetic model which is able to explain\nthe emergence of income profiles out of a whole of individual economic\ninteractions. The model is expressed by a system of several nonlinear\ndifferential equations which involve parameters defined by probabilities.\nSociety is described as an ensemble of individuals divided into income classes;\nthe individuals exchange money through binary and ternary interactions, leaving\nthe total wealth unchanged. The ternary interactions represent taxation and\nredistribution effects. Dynamics is investigated through computational\nsimulations, the focus being on the effects that different fiscal policies and\ndifferently weighted welfare policies have on the long-run income\ndistributions. The model provides a tool which may contribute to the\nidentification of the most effective actions towards a reduction of economic\ninequality. We find for instance that, under certain hypotheses, the Gini index\nis more affected by a policy of reduction of the welfare and subsidies for the\nrich classes than by an increase of the upper tax rate. Such a policy also has\nthe effect of slightly increasing the total tax revenue.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.3749v2"
    },
    {
        "title": "A finite set of equilibria for the indeterminacy of linear rational\n  expectations models",
        "authors": [
            "Jean-Bernard Chatelain",
            "Kirsten Ralf"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  This paper demonstrates the existence of a finite set of equilibria in the\ncase of the indeterminacy of linear rational expectations models. The number of\nequilibria corresponds to the number of ways to select n eigenvectors among a\nlarger set of eigenvectors related to stable eigenvalues. A finite set of\nequilibria is a substitute to continuous (uncountable) sets of sunspots\nequilibria, when the number of independent eigenvectors for each stable\neigenvalue is equal to one.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.6222v1"
    },
    {
        "title": "Semiparametric Estimation of First-Price Auction Models",
        "authors": [
            "Gaurab Aryal",
            "Maria Florencia Gabrielli",
            "Quang Vuong"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We propose a semiparametric method to estimate the density of private values\nin first-price auctions. Specifically, we model private values through a set of\nconditional moment restrictions and use a two-step procedure. In the first step\nwe recover a sample of pseudo private values using Local Polynomial Estimator.\nIn the second step we use a GMM procedure to estimate the parameter(s) of\ninterest. We show that the proposed semiparametric estimator is consistent, has\nan asymptotic normal distribution, and attains the parametric (\"root-n\") rate\nof convergence.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.7140v4"
    },
    {
        "title": "On the convergence of the Fitness-Complexity Algorithm",
        "authors": [
            "Emanuele Pugliese",
            "Andrea Zaccaria",
            "Luciano Pietronero"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We investigate the convergence properties of an algorithm which has been\nrecently proposed to measure the competitiveness of countries and the quality\nof their exported products. These quantities are called respectively Fitness F\nand Complexity Q. The algorithm was originally based on the adjacency matrix M\nof the bipartite network connecting countries with the products they export,\nbut can be applied to any bipartite network. The structure of the adjacency\nmatrix turns to be essential to determine which countries and products converge\nto non zero values of F and Q. Also the speed of convergence to zero depends on\nthe matrix structure. A major role is played by the shape of the ordered matrix\nand, in particular, only those matrices whose diagonal does not cross the empty\npart are guaranteed to have non zero values as outputs when the algorithm\nreaches the fixed point. We prove this result analytically for simplified\nstructures of the matrix, and numerically for real cases. Finally, we propose\nsome practical indications to take into account our results when the algorithm\nis applied.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.0249v2"
    },
    {
        "title": "RHOMOLO: A Dynamic Spatial General Equilibrium Model for Assessing the\n  Impact of Cohesion Policy",
        "authors": [
            "Andries Brandsma",
            "d'Artis Kancs",
            "Philippe Monfort",
            "Alexandra Rillaers"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  The paper presents the newly developed dynamic spatial general equilibrium\nmodel of European Commission, RHOMOLO. The model incorporates several elements\nfrom economic geography in a novel and theoretically consistent way. It\ndescribes the location choice of different types of agents and captures the\ninterplay between agglomeration and dispersion forces in determining the\nspatial equilibrium. The model is also dynamic as it allows for the\naccumulation of factors of production, human capital and technology. This makes\nRHOMOLO well suited for simulating policy scenario related to the EU cohesion\npolicy and for the analysis of its impact on the regions and the Member States\nof the union.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.5068v2"
    },
    {
        "title": "Conditional Preference Orders and their Numerical Representations",
        "authors": [
            "Samuel Drapeau",
            "Asgar Jamneshan"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We provide an axiomatic system modeling conditional preference orders which\nis based on conditional set theory. Conditional numerical representations are\nintroduced, and a conditional version of the theorems of Debreu on the\nexistence of numerical representations is proved. The conditionally continuous\nrepresentations follow from a conditional version of Debreu's Gap Lemma the\nproof of which relies on a conditional version of the axiom of choice, free of\nany measurable selection argument. We give a conditional version of the von\nNeumann and Morgenstern representation as well as automatic conditional\ncontinuity results, and illustrate them by examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.5466v4"
    },
    {
        "title": "Essay on the State of Research and Innovation in France and the European\n  Union",
        "authors": [
            "Antoine Kornprobst"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Innovation in the economy is an important engine of growth and no economy,\nwhatever its complexity and degree of advancement, whether it is based on\nindustry, agriculture, high tech or the providing of services, can be truly\nhealthy without innovating actors within it. The aim of this work, done by an\napplied mathematician working in finance, not by an economist or a lawyer,\nisn't to provide an exhaustive view of the all the mechanisms in France and in\nEurope that aim at fostering innovation in the economy and to offer solutions\nfor removing all the roadblocks that still hinder innovation; indeed such a\nstudy would go far beyond the scope of this study. What I modestly attempted to\nachieve in this study was firstly to draw a panorama of what is working and\nwhat needs to perfected as far as innovation is concerned in France and Europe,\nthen secondly to offer some solutions and personal thoughts to boost\ninnovation.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.00679v2"
    },
    {
        "title": "Teaching Economics and Providing Visual \"Big Pictures\"",
        "authors": [
            "Seyyed Ali Zeytoon Nejad Moosavian"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The goal of this paper is to investigate the importance of providing visual\n\"big pictures\" in the teaching of economics. The plurality and variety of\nconcepts, variables, diagrams, and models involved in economics can be a source\nof confusion for many economics students. However, reviewing the existing\nliterature on the importance of providing visual \"big pictures\" in the process\nof learning suggests that furnishing students with a visual \"big picture\" that\nillustrates the ways through which those numerous, diverse concepts are\nconnected to each other could be an effective solution to clear up the\nmentioned mental chaos. As a practical example, this paper introduces a \"big\npicture\" that can be used as a good resource in intermediate macroeconomics\nclasses. This figure presents twenty-seven commonly-discussed macroeconomic\ndiagrams in the intermediate macroeconomics course, and gives little detail on\nsome of these diagrams, aiming at helping students to get the whole picture at\nonce on a single piece of paper. This macroeconomics big picture mostly focuses\non the routes through which common diagrams in macroeconomics are connected to\neach other, and finally introduces the general macroeconomic equilibrium that\nis graphically derived through those connections.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.01771v1"
    },
    {
        "title": "Do Mature Economies Grow Exponentially?",
        "authors": [
            "Steffen Lange",
            "Peter Pütz",
            "Thomas Kopp"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Most models that try to explain economic growth indicate exponential growth\npaths. In recent years, however, a lively discussion has emerged considering\nthe validity of this notion. In the empirical literature dealing with drivers\nof economic growth, the majority of articles is based upon an implicit\nassumption of exponential growth. Few scholarly articles have addressed this\nissue so far. In order to shed light on this issue, we estimate autoregressive\nintegrated moving average time series models based on Gross Domestic Product\nPer Capita data for 18 mature economies from 1960 to 2013. We compare the\nadequacy of linear and exponential growth models and conduct several robustness\nchecks. Our fndings cast doubts on the widespread belief of exponential growth\nand suggest a deeper discussion on alternative economic grow theories.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04028v1"
    },
    {
        "title": "A Statistical Model of Inequality",
        "authors": [
            "Ricardo T. Fernholz"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  This paper develops a nonparametric statistical model of wealth distribution\nthat imposes little structure on the fluctuations of household wealth. In this\nsetting, we use new techniques to obtain a closed-form household-by-household\ncharacterization of the stable distribution of wealth and show that this\ndistribution is shaped entirely by two factors - the reversion rates (a measure\nof cross-sectional mean reversion) and idiosyncratic volatilities of wealth\nacross different ranked households. By estimating these factors, our model can\nexactly match the U.S. wealth distribution. This provides information about the\ncurrent trajectory of inequality as well as estimates of the distributional\neffects of progressive capital taxes. We find evidence that the U.S. wealth\ndistribution might be on a temporarily unstable trajectory, thus suggesting\nthat further increases in top wealth shares are likely in the near future. For\ncapital taxes, we find that a small tax levied on just 1% of households\nsubstantially reshapes the distribution of wealth and reduces inequality.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04093v1"
    },
    {
        "title": "Conjoint axiomatization of the Choquet integral for heterogeneous\n  product sets",
        "authors": [
            "Mikhail Timonin"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We propose an axiomatization of the Choquet integral model for the general\ncase of a heterogeneous product set $X = X_1 \\times \\ldots \\times X_n$. In MCDA\nelements of $X$ are interpreted as alternatives, characterized by criteria\ntaking values from the sets $X_i$. Previous axiomatizations of the Choquet\nintegral have been given for particular cases $X = Y^n$ and $X = \\mathbb{R}^n$.\nHowever, within multicriteria context such identicalness, hence\ncommensurateness, of criteria cannot be assumed a priori. This constitutes the\nmajor difference of this paper from the earlier axiomatizations. In particular,\nthe notion of \"comonotonicity\" cannot be used in a heterogeneous structure, as\nthere does not exist a \"built-in\" order between elements of sets $X_i$ and\n$X_j$. However, such an order is implied by the representation model. Our\napproach does not assume commensurateness of criteria. We construct the\nrepresentation and study its uniqueness properties.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.08142v1"
    },
    {
        "title": "Knight--Walras Equilibria",
        "authors": [
            "Patrick Beissner",
            "Frank Riedel"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Knightian uncertainty leads naturally to nonlinear expectations. We introduce\na corresponding equilibrium concept with sublinear prices and establish their\nexistence. In general, such equilibria lead to Pareto inefficiency and coincide\nwith Arrow--Debreu equilibria only if the values of net trades are\nambiguity--free in the mean. Without aggregate uncertainty, inefficiencies\narise generically.\n  We introduce a constrained efficiency concept, uncertainty--neutral\nefficiency and show that Knight--Walras equilibrium allocations are efficient\nin this constrained sense. Arrow--Debreu equilibria turn out to be non--robust\nwith respect to the introduction of Knightian uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.04385v1"
    },
    {
        "title": "A constraint-based framework to study rationality, competition and\n  cooperation in fisheries",
        "authors": [
            "Christian Mullon",
            "Charles Mullon"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In this paper, we present a simplified framework to represent competition,\ncoordination and bargaining in fisheries when they operate under financial and\ntechnological constraints. Competition within constraints leads to a particular\ntype of mathematical game in which the strategy choice by one player changes\nstrategy set of the other. By studying the equilibria and bargaining space of\nthis game when players maximize either profit or fishing capacity, we highlight\nthat differences in financial constraints among players leads to a tougher\nplay, with a reduced bargaining space as the least constrained player can\nreadily exclude another from the competition. The exacerbating effects of\nconstraints on competition are even stronger when players maximize capacity. We\ndiscuss the significance of our results for global ocean governance in a\ncurrent context characterized by financialization and technological\ndevelopment. We suggest that in order to maximize the chances of fruitful\nnegociations and aim towards a fair sharing of sea resources, it would be\nhelpful to focus on leveling current differences in the constraints faced\nbetween competing fishing systems by supporting local financial systems and\ntechnological control, before implementing sophisticated economic tools.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.08166v2"
    },
    {
        "title": "Exploring the determinants of Bitcoin's price: an application of\n  Bayesian Structural Time Series",
        "authors": [
            "Obryan Poyser"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Currently, there is no consensus on the real properties of Bitcoin. The\ndiscussion comprises its use as a speculative or safe haven assets, while other\nauthors argue that the augmented attractiveness could end accomplishing money's\nfunctions that economic theory demands. This paper explores the association\nbetween Bitcoin's market price and a set of internal and external factors using\nBayesian Structural Time Series Approach. I aim to contribute to the discussion\nby differentiating among several attractiveness sources and employing a method\nthat provides a more flexible analytic framework that decompose each of the\ncomponents of the time series, apply variable selection, include information on\nprevious studies, and dynamically examine the behavior of the explanatory\nvariables, all in a transparent and tractable setting. The results show that\nthe Bitcoin price is negatively associated with a neutral investor's sentiment,\ngold's price and Yuan to USD exchange rate, while positively related to stock\nmarket index, USD to Euro exchange rate and variated signs among the different\ncountries' search trends. Hence, I find that Bitcoin has mixed properties since\nstill seems to act as a speculative, safe haven and a potential a capital\nflights instrument.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01437v1"
    },
    {
        "title": "Econophysics of Macro-Finance: Local Multi-fluid Models and Surface-like\n  Waves of Financial Variables",
        "authors": [
            "Victor Olkhov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This paper models macro financial variables alike to financial fluids with\nlocal interactions and describes surface-like waves of Investment and Profits.\nWe regard macro-finance as ensemble of economic agents and use their risk\nratings as coordinates on economic space. Aggregations of agent's financial\nvariables with risk coordinates x on economic space define macro financial\nvariables as function of x. We describe evolution and interactions between\nmacro financial variables alike to financial fluids by hydrodynamic-like\nequations. Minimum and maximum risk grades define most secure and most risky\nagents respectively. That determines borders of macro-finance domain that is\nfilled by economic agents. Perturbations of agent's risk coordinates near risk\nborders of macro domain cause disturbances of macro financial variables like\nInvestment and Profits. Such disturbances can generate waves that propagate\nalong risk borders. These waves may exponentially amplify perturbations inside\nof macro domain and impact financial sustainability. We study simple model\nInvestment and Profits and describe linear approximation of steady state\ndistributions of Investment and Profits on macro-finance domain that fulfill\ndreams of Investors: \"more risks-more Profits\". We describe Investment and\nProfits waves on risk border of economic space alike to surface waves in\nfluids. We present simple examples that specify waves as possible origin of\ntime fluctuations of macro financial variables. Description of possible steady\nstate distributions of macro financial variables and financial risk waves on\neconomic space could help for better policy-making and managing sustainable\nmacro-finance.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01748v1"
    },
    {
        "title": "Informing Additive Manufacturing technology adoption: total cost and the\n  impact of capacity utilisation",
        "authors": [
            "Martin Baumers",
            "Luca Beltrametti",
            "Angelo Gasparre",
            "Richard Hague"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Informing Additive Manufacturing (AM) technology adoption decisions, this\npaper investigates the relationship between build volume capacity utilisation\nand efficient technology operation in an inter-process comparison of the costs\nof manufacturing a complex component used in the packaging industry.\nConfronting the reported costs of a conventional machining and welding pathway\nwith an estimator of the costs incurred through an AM route utilising Direct\nMetal Laser Sintering (DMLS), we weave together four aspects: optimised\ncapacity utilisation, ancillary process steps, the effect of build failure, and\ndesign adaptation. Recognising that AM users can fill unused machine capacity\nwith other, potentially unrelated, geometries, we posit a characteristic of\n'fungible' build capacity. This aspect is integrated in the cost estimation\nframework through computational build volume packing, drawing on a basket of\nsample geometries. We show that the unit cost in mixed builds at full capacity\nis lower than in builds limited to a single type of geometry; in our study this\nresults in a mean unit cost overstatement of 157%. The estimated manufacturing\ncosts savings from AM adoption range from 36% to 46%. Additionally, we indicate\nthat operating cost savings resulting from design adaptation are likely to far\noutweigh the manufacturing cost advantage.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.02090v1"
    },
    {
        "title": "Economics of limiting cumulative CO2 emissions",
        "authors": [
            "Ashwin K Seshadri"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Global warming from carbon dioxide (CO2) is known to depend on cumulative CO2\nemissions. We introduce a model of global expenditures on limiting cumulative\nCO2 emissions, taking into account effects of decarbonization and rising global\nincome and making an approximation to the marginal abatement costs (MAC) of\nCO2. Discounted mitigation expenditures are shown to be a convex function of\ncumulative CO2 emissions. We also consider minimum-expenditure solutions for\nmeeting cumulative emissions goals, using a regularized variational method\nyielding an initial value problem in the integrated decarbonization rate. A\nquasi-stationary solution to this problem can be obtained for a special case,\nyielding decarbonization rate that is proportional to annual CO2 emissions.\nMinimum-expenditure trajectories in scenarios where CO2 emissions decrease must\nbegin with rapid decarbonization at rate decreasing with time. Due to the shape\nof global MAC the fraction of global income spent on CO2 mitigation (\"burden\")\ngenerally increases with time, as cheaper avenues for mitigation are exhausted.\nTherefore failure to rapidly decarbonize early on reduces expenditures by a\nsmall fraction (on the order of 0.01 %) of income in the present, but leads to\nmuch higher burden to future generations (on the order of 1 % of income).\n",
        "pdf_link": "http://arxiv.org/pdf/1706.03502v1"
    },
    {
        "title": "Food Productivity Trends from Hybrid Corn: Statistical Analysis of\n  Patents and Field-test data",
        "authors": [
            "Mariam Barry",
            "Giorgio Triulzi",
            "Christopher L. Magee"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  In this research we study productivity trends of hybrid corn - an important\nsubdomain of food production. We estimate the yearly rate of yield improvement\nof hybrid corn (measured as bushel per acre) by using both information on\nyields contained in US patent documents for patented hybrid corn varieties and\non field-test data of several hybrid corn varieties performed at US State\nlevel. We have used a generalization of Moore's law to fit productivity trends\nand obtain the performance improvement rate by analyzing time series of hybrid\ncorn performance for a period covering the last thirty years. The linear\nregressions results obtained from different data sources indicate that the\nestimated improvement rates per year are between 1.2 and 2.4 percent. In\nparticular, using yields reported in a sample of patents filed between 1985 and\n2010, we estimated an improvement rate of 0.015 (R2 = 0.74, Pvalue = 1.37 x\n10^-8). Moreover, we apply two predicting models developed by Benson and Magee\n(2015) and Triulzi and Magee (2016) that only use patent metadata to estimate\nthe rate of improvement. We compare these predicted values to the rate\nestimated using US States field-test data. We find that, due to a turning point\nin patenting practices which begun in 2008, only the predicted rate (rate =\n0.015) using patents filed before 2008 is consistent with the empirical rate.\nFinally, we also investigate at the micro level - on the basis of 70 patents\n(granted between 1986 and 2015) - whether the number of citations received by a\npatent is correlated with performance achieved by the patented variety. We find\nthat the relative performance (yield ratio) of the patented seed is positively\ncorrelated with the total number of citations received by the patent (until\nDecember 2015) but not the citations received within 3 years after the granted\nyear, with the patent application year used as control variable.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.05911v1"
    },
    {
        "title": "Deep Learning in (and of) Agent-Based Models: A Prospectus",
        "authors": [
            "Sander van der Hoog"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  A very timely issue for economic agent-based models (ABMs) is their empirical\nestimation. This paper describes a line of research that could resolve the\nissue by using machine learning techniques, using multi-layer artificial neural\nnetworks (ANNs), or so called Deep Nets. The seminal contribution by Hinton et\nal. (2006) introduced a fast and efficient training algorithm called Deep\nLearning, and there have been major breakthroughs in machine learning ever\nsince. Economics has not yet benefited from these developments, and therefore\nwe believe that now is the right time to apply Deep Learning and multi-layered\nneural networks to agent-based models in economics.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.06302v1"
    },
    {
        "title": "Virtual Relationships: Short- and Long-run Evidence from BitCoin and\n  Altcoin Markets",
        "authors": [
            "Pavel Ciaian",
            "Miroslava Rajcaniova",
            "d'Artis Kancs"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This study empirically examines interdependencies between BitCoin and altcoin\nmarkets in the short- and long-run. We apply time-series analytical mechanisms\nto daily data of 17 virtual currencies (BitCoin + 16 alternative virtual\ncurrencies) and two Altcoin price indices for the period 2013-2016. Our\nempirical findings confirm that indeed BitCoin and Altcoin markets are\ninterdependent. The BitCoin-Altcoin price relationship is significantly\nstronger in the short-run than in the long-run. We cannot fully confirm the\nhypothesis that the BitCoin price relationship is stronger with those Altcoins\nthat are more similar in their price formation mechanism to BitCoin. In the\nlong-run, macro-financial indicators determine the altcoin price formation to a\ngreater degree than BitCoin does. The virtual currency supply is exogenous and\ntherefore plays only a limited role in the price formation.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.07216v1"
    },
    {
        "title": "Non-Local Macroeconomic Transactions and Credits-Loans Surface-Like\n  Waves",
        "authors": [
            "Victor Olkhov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This paper describes surface-like waves of macroeconomic Credits-Loans\ntransactions on economic space. We use agent's risk ratings as their\ncoordinates and describe evolution of macro variables by transactions between\nagents. Aggregations of agent's variables with risk coordinates x on economic\nspace define macro variables as function of x. Aggregations of transactions\nbetween agents at point x and y determine functions of two variables (x,y) on\neconomic space. As example we study Credits transactions provided from agents\nat point x to agents at point y and thus amount of Loans received by agents at\npoint y from agents at point x at moment t during time term dt. We model\nevolution of macro transactions by hydrodynamic-like equations. Agents fill\nmacro domain on economic space that is bounded by minimum risk ratings of most\nsecure and maximum risk ratings of most risky agents. Economic and financial\nshocks can disturb steady borders of macro domain and cause perturbations of\ntransactions. Such disturbances can generate waves that can propagate along\nrisk borders alike to surface waves in fluids. As example, we describe simple\nmodel interactions between two transactions by hydrodynamic like equations in a\nclosed form. We introduce notions of \"macro accelerations\" and their potentials\nthat establish steady state distributions of transactions on economic space.\nFor this model in linear approximation we describe surface-like waves and show\nthat perturbations induced by surface-like waves can exponentially grow up\ninside macro domain and induce macro instabilities in a low risk area.\nDescription of possible steady state distributions of transactions and\nsurface-like waves on economic space might be important for macro modeling and\npolicy-making.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.07758v1"
    },
    {
        "title": "The effect of the behavior of an average consumer on the public debt\n  dynamics",
        "authors": [
            "Roberto De Luca",
            "Marco Di Mauro",
            "Angelo Falzarano",
            "Adele Naddeo"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  An important issue within the present economic crisis is understanding the\ndynamics of the public debt of a given country, and how the behavior of average\nconsumers and tax payers in that country affects it. Starting from a model of\nthe average consumer behavior introduced earlier by the authors, we propose a\nsimple model to quantitatively address this issue. The model is then studied\nand analytically solved under some reasonable simplifying assumptions. In this\nway we obtain a condition under which the public debt steadily decreases.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.07759v1"
    },
    {
        "title": "Intergenerational mobility measures in a bivariate normal model",
        "authors": [
            "Yonatan Berman"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We model the joint log-income distribution of parents and children and derive\nanalytic expressions for canonical relative and absolute intergenerational\nmobility measures. We find that both types of mobility measures can be\nexpressed as a function of the other.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.07783v1"
    },
    {
        "title": "Bilateral multifactor CES general equilibrium with state-replicating\n  Armington elasticities",
        "authors": [
            "Jiyoung Kim",
            "Satoshi Nakano",
            "Kazuhiko Nishimura"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We measure elasticity of substitution between foreign and domestic\ncommodities by two-point calibration such that the Armington aggregator can\nreplicate the two temporally distant observations of market shares and prices.\nAlong with the sectoral multifactor CES elasticities which we estimate by\nregression using a set of disaggregated linked input--output observations, we\nintegrate domestic production of two countries, namely, Japan and the Republic\nof Korea, with bilateral trade models and construct a bilateral general\nequilibrium model. Finally, we make an assessment of a tariff elimination\nscheme between the two countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.09365v2"
    },
    {
        "title": "Parameter estimation for stable distributions with application to\n  commodity futures log returns",
        "authors": [
            "Michael Kateregga",
            "Sure Mataramvura",
            "David Taylor"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This paper explores the theory behind the rich and robust family of\n{\\alpha}-stable distributions to estimate parameters from financial asset\nlog-returns data. We discuss four-parameter estimation methods including the\nquantiles, logarithmic moments method, maximum likelihood (ML), and the\nempirical characteristics function (ECF) method. The contribution of the paper\nis two-fold: first, we discuss the above parametric approaches and investigate\ntheir performance through error analysis. Moreover, we argue that the ECF\nperforms better than the ML over a wide range of shape parameter values,\n{\\alpha}{\\alpha} including values closest to 0 and 2 and that the ECF has a\nbetter convergence rate than the ML. Secondly, we compare the t location-scale\ndistribution to the general stable distribution and show that the former fails\nto capture skewness which might exist in the data. This is observed through\napplying the ECF to commodity futures log-returns data to obtain the skewness\nparameter.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.09756v1"
    },
    {
        "title": "Instabilities in large economies: aggregate volatility without\n  idiosyncratic shocks",
        "authors": [
            "Julius Bonart",
            "Jean-Philippe Bouchaud",
            "Augustin Landier",
            "David Thesmar"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We study a dynamical model of interconnected firms which allows for certain\nmarket imperfections and frictions, restricted here to be myopic price\nforecasts and slow adjustment of production. Whereas the standard rational\nequilibrium is still formally a stationary solution of the dynamics, we show\nthat this equilibrium becomes linearly unstable in a whole region of parameter\nspace. When agents attempt to reach the optimal production target too quickly,\ncoordination breaks down and the dynamics becomes chaotic. In the unstable,\n\"turbulent\" phase, the aggregate volatility of the total output remains\nsubstantial even when the amplitude of idiosyncratic shocks goes to zero or\nwhen the size of the economy becomes large. In other words, crises become\nendogenous. This suggests an interesting resolution of the \"small shocks, large\nbusiness cycles\" puzzle.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.5022v1"
    },
    {
        "title": "Game Theory, Statistical Mechanics and Income Inequality",
        "authors": [
            "Venkat Venkatasubramanian",
            "Yu Luo",
            "Jay Sethuraman"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  The widening inequality in income distribution in recent years, and the\nassociated excessive pay packages of CEOs in the U.S. and elsewhere, is of\ngrowing concern among policy makers as well as the common person. However,\nthere seems to be no satisfactory answer, in conventional economic theories and\nmodels, to the fundamental question of what kind of pay distribution we ought\nto see, at least under ideal conditions, in a free market environment and\nwhether this distribution is fair. We propose a game theoretic framework that\naddresses these questions and show that the lognormal distribution is the\nfairest inequality of pay in an organization comprising of homogenous agents,\nachieved at equilibrium, under ideal free market conditions. We also show that\nfor a population of two different classes of agents, the final distribution is\na combination of two different lognormal distributions where one of them,\ncorresponding to the top 3-5% of the population, can be misidentified as a\nPareto distribution. Our theory also shows the deep and direct connection\nbetween potential game theory and statistical mechanics through entropy, which\nis a measure of fairness in a distribution. This leads us to propose the fair\nmarket hypothesis, that the self-organizing dynamics of the ideal free market,\ni.e., Adam Smith's \"invisible hand\", not only promotes efficiency but also\nmaximizes fairness under the given constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.6620v2"
    },
    {
        "title": "A Note on Kuhn's Theorem with Ambiguity Averse Players",
        "authors": [
            "Gaurab Aryal",
            "Ronald Stauber"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  Kuhn's Theorem shows that extensive games with perfect recall can\nequivalently be analyzed using mixed or behavioral strategies, as long as\nplayers are expected utility maximizers. This note constructs an example that\nillustrate the limits of Kuhn's Theorem in an environment with ambiguity averse\nplayers who use maxmin decision rule and full Bayesian updating.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.1022v2"
    },
    {
        "title": "How the Taxonomy of Products Drives the Economic Development of\n  Countries",
        "authors": [
            "Andrea Zaccaria",
            "Matthieu Cristelli",
            "Andrea Tacchella",
            "Luciano Pietronero"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We introduce an algorithm able to reconstruct the relevant network structure\non which the time evolution of country-product bipartite networks takes place.\nThe significant links are obtained by selecting the largest values of the\nprojected matrix. We first perform a number of tests of this filtering\nprocedure on synthetic cases and a toy model. Then we analyze the bipartite\nnetwork constituted by countries and exported products, using two databases for\na total of almost 50 years. It is then possible to build a hierarchically\ndirected network, in which the taxonomy of products emerges in a natural way.\nWe study the influence of the structure of this taxonomy network on countries'\ndevelopment; in particular, guided by an example taken from the\nindustrialization of South Korea, we link the structure of the taxonomy network\nto the empirical temporal connections between product activations, finding that\nthe most relevant edges for countries' development are the ones suggested by\nour network. These results suggest paths in the product space which are easier\nto achieve, and so can drive countries' policies in the industrialization\nprocess.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.2138v1"
    },
    {
        "title": "Can Google searches help nowcast and forecast unemployment rates in the\n  Visegrad Group countries?",
        "authors": [
            "Jaroslav Pavlicek",
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  Online activity of the Internet users has been repeatedly shown to provide a\nrich information set for various research fields. We focus on the job-related\nsearches on Google and their possible usefulness in the region of the Visegrad\nGroup -- the Czech Republic, Hungary, Poland and Slovakia. Even for rather\nsmall economies, the online searches of their inhabitants can be successfully\nutilized for macroeconomic predictions. Specifically, we study the unemployment\nrates and their interconnection to the job-related searches. We show that the\nGoogle searches strongly enhance both nowcasting and forecasting models of the\nunemployment rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.6639v1"
    },
    {
        "title": "Berk-Nash Equilibrium: A Framework for Modeling Agents with Misspecified\n  Models",
        "authors": [
            "Ignacio Esponda",
            "Demian Pouzo"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We develop an equilibrium framework that relaxes the standard assumption that\npeople have a correctly-specified view of their environment. Each player is\ncharacterized by a (possibly misspecified) subjective model, which describes\nthe set of feasible beliefs over payoff-relevant consequences as a function of\nactions. We introduce the notion of a Berk-Nash equilibrium: Each player\nfollows a strategy that is optimal given her belief, and her belief is\nrestricted to be the best fit among the set of beliefs she considers possible.\nThe notion of best fit is formalized in terms of minimizing the\nKullback-Leibler divergence, which is endogenous and depends on the equilibrium\nstrategy profile. Standard solution concepts such as Nash equilibrium and\nself-confirming equilibrium constitute special cases where players have\ncorrectly-specified models. We provide a learning foundation for Berk-Nash\nequilibrium by extending and combining results from the statistics literature\non misspecified learning and the economics literature on learning in games.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1152v4"
    },
    {
        "title": "Cooperation under Incomplete Information on the Discount Factors",
        "authors": [
            "Cy Maor",
            "Eilon Solan"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  In repeated games, cooperation is possible in equilibrium only if players are\nsufficiently patient, and long-term gains from cooperation outweigh short-term\ngains from deviation. What happens if the players have incomplete information\nregarding each other's discount factors? In this paper we look at repeated\ngames in which each player has incomplete information regarding the other\nplayer's discount factor, and ask when full cooperation can arise in\nequilibrium. We provide necessary and sufficient conditions that allow full\ncooperation in equilibrium that is composed of grim trigger strategies, and\ncharacterize the states of the world in which full cooperation occurs. We then\nask whether these \"cooperation events\" are close to those in the complete\ninformation case, when the information on the other player's discount factor is\n\"almost\" complete.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1368v1"
    },
    {
        "title": "It's not the economy, stupid! How social capital and GDP relate to\n  happiness over time",
        "authors": [
            "Stefano Bartolini",
            "Francesco Sarracino"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  What predicts the evolution over time of subjective well-being? We correlate\nthe trends of subjective well-being with the trends of social capital and/or\nGDP. We find that in the long and medium run social capital largely predicts\nthe trends of subjective wellbeing in our sample of countries. In the\nshort-term this relationship weakens. Indeed, in the short run, changes in\nsocial capital predict a much smaller portion of the changes in subjective\nwell-being than over longer periods. GDP follows a reverse path, thus\nconfirming the Easterlin paradox: in the short run GDP is more positively\ncorrelated to well-being than in the medium-term, while in the long run this\ncorrelation vanishes.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.2138v1"
    },
    {
        "title": "Firm size distribution in Italy and employment protection",
        "authors": [
            "Luca Amendola"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  The number of Italian firms in function of the number of workers is well\napproximated by an inverse power law up to 15 workers but shows a clear\ndownward deflection beyond this point, both when using old pre-1999 data and\nwhen using recent (2014) data. This phenomenon could be associated with\nemployent protection legislation which applies to companies with more than 15\nworkers (the Statuto dei Lavoratori). The deflection disappears for agriculture\nfirms, for which the protection legislation applies already above 5 workers. In\nthis note it is estimated that a correction of this deflection could bring an\nincrease from 3.9 to 5.8% in new jobs in firms with a workforce between 5 to 25\nworkers.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.0950v1"
    },
    {
        "title": "Taxation as an instrument of stimulation of innovation-active business\n  entities",
        "authors": [
            "Andrey Nechaev"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  The analysis of the theoretical material revealed the lack of consensus on\ndefini-tion of the tax stimulation of innovation-active business entities\nwithin the re-gional taxation. The definition tax stimulation of\ninnovation-active business en-tities is specified.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.2746v1"
    },
    {
        "title": "A New Methodology for Estimating Internal Credit Risk and Bankruptcy\n  Prediction under Basel II Regime",
        "authors": [
            "M. Naresh Kumar",
            "V. Sree Hari Rao"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Credit estimation and bankruptcy prediction methods have been utilizing\nAltman's $z$ score method for the last several years. It is reported in many\nstudies that $z$ score is sensitive to changes in accounting figures.\nResearches have proposed different variations to conventional $z$ score that\ncan improve the prediction accuracy. In this paper we develop a new\nmultivariate non-linear model for computing the $z$ score. In addition we\ndevelop a new credit risk index by fitting a Pearson type-III distribution to\nthe transformed financial ratios. The results from our study have shown that\nthe new $z$ score can predict the bankruptcy with an accuracy of $98.6\\%$ as\ncompared to $93.5\\%$ by the Altman's $z$ score. Also, the discriminate analysis\nrevealed that the new transformed financial ratios could predict the bankruptcy\nprobability with an accuracy of $93.0\\%$ as compared to $87.4\\%$ using the\nweights of Altman's $z$ score.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00882v1"
    },
    {
        "title": "How predictable is technological progress?",
        "authors": [
            "J. Doyne Farmer",
            "Francois Lafond"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Recently it has become clear that many technologies follow a generalized\nversion of Moore's law, i.e. costs tend to drop exponentially, at different\nrates that depend on the technology. Here we formulate Moore's law as a\ncorrelated geometric random walk with drift, and apply it to historical data on\n53 technologies. We derive a closed form expression approximating the\ndistribution of forecast errors as a function of time. Based on hind-casting\nexperiments we show that this works well, making it possible to collapse the\nforecast errors for many different technologies at different time horizons onto\nthe same universal distribution. This is valuable because it allows us to make\nforecasts for any given technology with a clear understanding of the quality of\nthe forecasts. As a practical demonstration we make distributional forecasts at\ndifferent time horizons for solar photovoltaic modules, and show how our method\ncan be used to estimate the probability that a given technology will outperform\nanother technology at a given point in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.05274v4"
    },
    {
        "title": "Equilibrium in Misspecified Markov Decision Processes",
        "authors": [
            "Ignacio Esponda",
            "Demian Pouzo"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We study Markov decision problems where the agent does not know the\ntransition probability function mapping current states and actions to future\nstates. The agent has a prior belief over a set of possible transition\nfunctions and updates beliefs using Bayes' rule. We allow her to be\nmisspecified in the sense that the true transition probability function is not\nin the support of her prior. This problem is relevant in many economic settings\nbut is usually not amenable to analysis by the researcher. We make the problem\ntractable by studying asymptotic behavior. We propose an equilibrium notion and\nprovide conditions under which it characterizes steady state behavior. In the\nspecial case where the problem is static, equilibrium coincides with the\nsingle-agent version of Berk-Nash equilibrium (Esponda and Pouzo (2016)). We\nalso discuss subtle issues that arise exclusively in dynamic settings due to\nthe possibility of a negative value of experimentation.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.06901v2"
    },
    {
        "title": "Sorting in Networks: Adversity and Structure",
        "authors": [
            "Andreas Bjerre-Nielsen"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  People choose friendships with people similar to themselves, i.e. they sort\nby resemblence. Economic studies have shown when sorting is optimal and\nconstitute an equilibrium, however, this presumes lack of beneficial\nspillovers. We investigate formation of economic and social networks where\nagents may form or cut ties. We combine a setup with link formation where\nagents have types that determine the value of a connection. We provide\nconditions for sorting in friendships, i.e. that agents tend to partner only\nwith those with those sufficiently similar to themselves. Conditions are\nprovided with and without beneficial spillovers from indirect connections. We\nshow that sorting may be suboptimal, yet a socially stable outcome, despite\notherwise obeying the conditions for sorting in Becker (1973). We analyze\npolicy tools to mitigate suboptimal sorting. Another feature is that agents\nwith higher value are more central in networks under certain conditions; a side\neffect is sorting by degree centrality under certain conditions. Finally we\nillustrate the limits to patterns of sorting and centrality.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.07389v5"
    },
    {
        "title": "Economic inequality and mobility in kinetic models for social sciences",
        "authors": [
            "Maria Letizia Bertotti",
            "Giovanni Modanese"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Statistical evaluations of the economic mobility of a society are more\ndifficult than measurements of the income distribution, because they require to\nfollow the evolution of the individuals' income for at least one or two\ngenerations. In micro-to-macro theoretical models of economic exchanges based\non kinetic equations, the income distribution depends only on the asymptotic\nequilibrium solutions, while mobility estimates also involve the detailed\nstructure of the transition probabilities of the model, and are thus an\nimportant tool for assessing its validity. Empirical data show a remarkably\ngeneral negative correlation between economic inequality and mobility, whose\nexplanation is still unclear. It is therefore particularly interesting to study\nthis correlation in analytical models. In previous work we investigated the\nbehavior of the Gini inequality index in kinetic models in dependence on\nseveral parameters which define the binary interactions and the taxation and\nredistribution processes: saving propensity, taxation rates gap, tax evasion\nrate, welfare means-testing etc. Here, we check the correlation of mobility\nwith inequality by analyzing the mobility dependence from the same parameters.\nAccording to several numerical solutions, the correlation is confirmed to be\nnegative.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.03232v1"
    },
    {
        "title": "Graph representation of balance sheets: from exogenous to endogenous\n  money",
        "authors": [
            "Cyril Pitrou"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  The nature of monetary arrangements is often discussed without any reference\nto its detailed construction. We present a graph representation that allows for\na clear understanding of modern monetary systems. First, we show that systems\nbased on commodity money are incompatible with credit. We then study the\ncurrent chartalist systems based on pure fiat money, and we discuss the\nconsolidation of the central bank with the Treasury. We obtain a visual\nexplanation about how commercial banks are responsible for endogenous money\ncreation whereas the Treasury and the central bank are in charge of the total\namount of net money. Finally we draw an analogy between systems based on gold\nconvertibility and currency pegs to show that fixed exchange rates can never be\nmaintained.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.03895v3"
    },
    {
        "title": "Linking Economic Complexity, Institutions and Income Inequality",
        "authors": [
            "D. Hartmann",
            "M. R. Guevara",
            "C. Jara-Figueroa",
            "M. Aristaran",
            "C. A. Hidalgo"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  A country's mix of products predicts its subsequent pattern of\ndiversification and economic growth. But does this product mix also predict\nincome inequality? Here we combine methods from econometrics, network science,\nand economic complexity to show that countries exporting complex products (as\nmeasured by the Economic Complexity Index) have lower levels of income\ninequality than countries exporting simpler products. Using multivariate\nregression analysis, we show that economic complexity is a significant and\nnegative predictor of income inequality and that this relationship is robust to\ncontrolling for aggregate measures of income, institutions, export\nconcentration, and human capital. Moreover, we introduce a measure that\nassociates a product to a level of income inequality equal to the average GINI\nof the countries exporting that product (weighted by the share the product\nrepresents in that country's export basket). We use this measure together with\nthe network of related products (or product space) to illustrate how the\ndevelopment of new products is associated with changes in income inequality.\nThese findings show that economic complexity captures information about an\neconomy's level of development that is relevant to the ways an economy\ngenerates and distributes its income. Moreover, these findings suggest that a\ncountry's productive structure may limit its range of income inequality.\nFinally, we make our results available through an online resource that allows\nfor its users to visualize the structural transformation of over 150 countries\nand their associated changes in income inequality between 1963 and 2008.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.07907v4"
    },
    {
        "title": "Return spillovers around the globe: A network approach",
        "authors": [
            "Stefan Lyocsa",
            "Tomas Vyrost",
            "Eduard Baumohl"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Using a rolling windows analysis of filtered and aligned stock index returns\nfrom 40 countries during the period 2006-2014, we construct Granger causality\nnetworks and investigate the ensuing structure of the relationships by studying\nnetwork properties and fitting spatial probit models. We provide evidence that\nstock market volatility and market size increases, while foreign exchange\nvolatility decreases the probability of return spillover from a given market.\nWe also show that market development and returns on the foreign exchange market\nand stock market also matter, but they exhibit significant time-varying\nbehaviour with alternating effects. These results suggest that higher market\nintegration periods are alternated with periods where investors appear to be\nchasing returns. Despite the significance of market characteristics and market\nconditions, what in reality matters for information propagation is the temporal\ndistance between closing hours, i.e. the temporal proximity effect. This\nimplies that choosing markets which trade in similar hours bears additional\ncosts to investors, as the probability of return spillovers increases. The same\neffect was observed with regard to the temporal distance to the US market.\nFinally, we confirm the existence of the preferential attachment effect, i.e.\nthe probability of a given market to propagate return spillovers to a new\nmarket depends endogenously and positively on the existing number of return\nspillovers from that market.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.06242v2"
    },
    {
        "title": "On growth-optimal tax rates and the issue of wealth inequalities",
        "authors": [
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We introduce a highly stylized, yet non trivial model of the economy, with a\npublic and private sector coupled through a wealth tax and a redistribution\npolicy. The model can be fully solved analytically, and allows one to address\nthe question of optimal taxation and of wealth inequalities. We find that\naccording to the assumption made on the relative performance of public and\nprivate sectors, three situations are possible. Not surprisingly, the optimal\nwealth tax rate is either 0% for a deeply dysfunctional government and/or\nhighly productive private sector, or 100 % for a highly efficient public sector\nand/or debilitated/risk averse private investors. If the gap between the\npublic/private performance is moderate, there is an optimal positive wealth tax\nrate maximizing economic growth, even -- counter-intuitively -- when the\nprivate sector generates more growth. The compromise between profitable private\ninvestments and taxation however leads to a residual level of inequalities. The\nmechanism leading to an optimal growth rate is related the well-known\nexplore/exploit trade-off.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.00275v2"
    },
    {
        "title": "Symmetry restoration by pricing in a duopoly of perishable goods",
        "authors": [
            "Su Do Yi",
            "Seung Ki Baek",
            "Guillaume Chevereau",
            "Eric Bertin"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Competition is a main tenet of economics, and the reason is that a perfectly\ncompetitive equilibrium is Pareto-efficient in the absence of externalities and\npublic goods. Whether a product is selected in a market crucially relates to\nits competitiveness, but the selection in turn affects the landscape of\ncompetition. Such a feedback mechanism has been illustrated in a duopoly model\nby Lambert et al., in which a buyer's satisfaction is updated depending on the\n{\\em freshness} of a purchased product. The probability for buyer $n$ to select\nseller $i$ is assumed to be $p_{n,i} \\propto e^{ S_{n,i}/T}$, where $S_{n,i}$\nis the buyer's satisfaction and $T$ is an effective temperature to introduce\nstochasticity. If $T$ decreases below a critical point $T_c$, the system\nundergoes a transition from a symmetric phase to an asymmetric one, in which\nonly one of the two sellers is selected. In this work, we extend the model by\nincorporating a simple price system. By considering a greed factor $g$ to\ncontrol how the satisfaction depends on the price, we argue the existence of an\noscillatory phase in addition to the symmetric and asymmetric ones in the\n$(T,g)$ plane, and estimate the phase boundaries through mean-field\napproximations. The analytic results show that the market preserves the\ninherent symmetry between the sellers for lower $T$ in the presence of the\nprice system, which is confirmed by our numerical simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.00975v2"
    },
    {
        "title": "A conjecture about the efficiency of first price mechanisms",
        "authors": [
            "Endre Csóka"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We present different versions of a conjecture which would express that first\nprice mechanisms never work very badly in a very general class of problems. The\ndefinitions include most of the problems where there is a principal (seller)\nwho has the right to exclude others from the game. The exact definitions are\nmotivated by the \"first price mechanism\" in E Cs: \"Efficient Teamwork\", but the\nconjecture is relevant for most auction problems, e.g. for combinatorial\nauctions.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.03651v3"
    },
    {
        "title": "Stochastic Frontier I & D of fractal dimensions for technological\n  innovation",
        "authors": [
            "Maria Ramos-Escamilla"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper presents an analysis of the study variables such as gdp,\nemployment levels, the level of R & D and technology that will serve as the\nbasis for stochastic modeling of production possibilities frontier in the\ngoodness of fractal dimensions Ex Ante and Ex Post a priori to determine the\nlevels of causality immediately and check its accuracy and power of indexing,\nusing high frequency data and thus address the response this assumption of\nstochastic frontiers with level N of partitions in time.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01212v1"
    },
    {
        "title": "Stochastic Optimal Growth Model with Risk Sensitive Preferences",
        "authors": [
            "Nicole Bäuerle",
            "Anna Jaśkiewicz"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper studies a one-sector optimal growth model with i.i.d. productivity\nshocks that are allowed to be unbounded. The utility function is assumed to be\nnon-negative and unbounded from above. The novel feature in our framework is\nthat the agent has risk sensitive preferences in the sense of Hansen and\nSargent (1995). Under mild assumptions imposed on the productivity and utility\nfunctions we prove that the maximal discounted non-expected utility in the\ninfinite time horizon satisfies the optimality equation and the agent possesses\na stationary optimal policy. A new point used in our analysis is an inequality\nfor the so-called associated random variables. We also establish the Euler\nequation that incorporates the solution to the optimality equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.05638v1"
    },
    {
        "title": "Mathematical Analysis of the Historical Economic Growth",
        "authors": [
            "Ron W. Nielsen"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Data describing historical economic growth are analysed. Included in the\nanalysis is the world and regional economic growth. The analysis demonstrates\nthat historical economic growth had a natural tendency to follow hyperbolic\ndistributions. Parameters describing hyperbolic distributions have been\ndetermined. A search for takeoffs from stagnation to growth produced negative\nresults. This analysis throws a new light on the interpretation of the\nmechanism of the historical economic growth and suggests new lines of research.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.06612v4"
    },
    {
        "title": "Game-theoretic Modeling of Players' Ambiguities on External Factors",
        "authors": [
            "Jian Yang"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We propose a game-theoretic framework that incorporates both incomplete\ninformation and general ambiguity attitudes on factors external to all players.\nOur starting point is players' preferences on payoff-distribution vectors,\nessentially mappings from states of the world to distributions of payoffs to be\nreceived by players. There are two ways in which equilibria for this preference\ngame can be defined. When the preferences possess ever more features, we can\ngradually add ever more structures to the game. These include real-valued\nutility-like functions over payoff-distribution vectors, sets of probabilistic\npriors over states of the world, and eventually the traditional\nexpected-utility framework involving one single prior. We establish equilibrium\nexistence results, show the upper hemi-continuity of equilibrium sets over\nchanging ambiguity attitudes, and uncover relations between the two versions of\nequilibria. Some attention is paid to the enterprising game, in which players\nexhibit ambiguity seeking attitudes while betting optimistically on the\nfavorable resolution of ambiguities. The two solution concepts are unified at\nthis game's pure equilibria, whose existence is guaranteed when strategic\ncomplementarities are present. The current framework can be applied to settings\nlike auctions involving ambiguity on competitors' assessments of item worths.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.06812v4"
    },
    {
        "title": "Exchanging Goods Using Valuable Money",
        "authors": [
            "J. V. Howard"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  A group of people wishes to use money to exchange goods efficiently over\nseveral time periods. However, there are disadvantages to using any of the\ngoods as money, and in addition fiat money issued in the form of notes or coins\nwill be valueless in the final time period, and hence in all earlier periods.\nAlso, Walrasian market prices are determined only up to an arbitrary rescaling.\nNevertheless we show that it is possible to devise a system which uses money to\nexchange goods and in which money has a determinate positive value. In this\nsystem, tokens are initially supplied to all traders by a central authority and\nrecovered by a purchase tax. All trades must be made using tokens or promissory\nnotes for tokens. This mechanism controls the flow rather than the stock of\nmoney: it introduces some trading frictions, some redistribution of wealth, and\nsome distortion of prices, but these effects can all be made small.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07888v3"
    },
    {
        "title": "A Stochastic Electricity Market Clearing Formulation with Consistent\n  Pricing Properties",
        "authors": [
            "Victor M. Zavala",
            "Kibaek Kim",
            "Mihai Anitescu",
            "John Birge"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We argue that deterministic market clearing formulations introduce arbitrary\ndistortions between day-ahead and expected real-time prices that bias economic\nincentives and block diversification. We extend and analyze the stochastic\nclearing formulation proposed by Pritchard et al. (2010) in which the social\nsurplus function induces penalties between day-ahead and real-time quantities.\nWe prove that the formulation yields price distortions that are bounded by the\nbid prices, and we show that adding a similar penalty term to transmission\nflows and phase angles ensures boundedness throughout the network. We prove\nthat when the price distortions are zero, day-ahead quantities converge to the\nquantile of real-time counterparts. The undesired effects of price distortions\nsuggest that stochastic settings provide significant benefits over\ndeterministic ones that go beyond social surplus improvements. We propose\nadditional metrics to evaluate these benefits.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.08335v1"
    },
    {
        "title": "A Simple extension of Dematerialization Theory: Incorporation of\n  Technical Progress and the Rebound Effect",
        "authors": [
            "Christopher L. Magee",
            "Tessaleno C. Devezas"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Dematerialization is the reduction in the quantity of materials needed to\nproduce something useful over time. Dematerialization fundamentally derives\nfrom ongoing increases in technical performance but it can be counteracted by\ndemand rebound - increases in usage because of increased value (or decreased\ncost) that also results from increasing technical performance. A major question\nthen is to what extent technological performance improvement can offset and is\noffsetting continuously increasing economic consumption. This paper contributes\nto answering this question by offering some simple quantitative extensions to\nthe theory of dematerialization. The paper then empirically examines the\nmaterials consumption trends as well as cost trends for a large set of\nmaterials and a few modern artifacts over the past decades. In each of 57 cases\nexamined, the particular combinations of demand elasticity and technical\nperformance rate improvement are not consistent with dematerialization.\nOverall, the theory extension and empirical examination indicate that there is\nno dematerialization occurring even for cases of information technology with\nrapid technical progress. Thus, a fully passive policy stance that relies on\nunfettered technological change is not supported by our results.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.00090v1"
    },
    {
        "title": "Economic and Technological Complexity: A Model Study of Indicators of\n  Knowledge-based Innovation Systems",
        "authors": [
            "Inga Ivanova",
            "Oivind Strand",
            "Duncan Kushnir",
            "Loet Leydesdorff"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The Economic Complexity Index (ECI; Hidalgo & Hausmann, 2009) measures the\ncomplexity of national economies in terms of product groups. Analogously to\nECI, a Patent Complexity Index (PatCI) can be developed on the basis of a\nmatrix of nations versus patent classes. Using linear algebra, the three\ndimensions: countries, product groups, and patent classes can be combined into\na measure of \"Triple Helix\" complexity (THCI) including the trilateral\ninteraction terms between knowledge production, wealth generation, and\n(national) control. THCI can be expected to capture the extent of systems\nintegration between the global dynamics of markets (ECI) and technologies\n(PatCI) in each national system of innovation. We measure ECI, PatCI, and THCI\nduring the period 2000-2014 for the 34 OECD member states, the BRICS countries,\nand a group of emerging and affiliated economies (Argentina, Hong Kong,\nIndonesia, Malaysia, Romania, and Singapore). The three complexity indicators\nare correlated between themselves; but the correlations with GDP per capita are\nvirtually absent. Of the world's major economies, Japan scores highest on all\nthree indicators, while China has been increasingly successful in combining\neconomic and technological complexity. We could not reproduce the correlation\nbetween ECI and average income that has been central to the argument about the\nfruitfulness of the economic complexity approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.02348v4"
    },
    {
        "title": "Mediation with near insolvent defaulting suppliers: a linear\n  optimisation model to find an optimal outcome",
        "authors": [
            "Eric Lavallee"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  This paper presents a model to describe contractual dispute resolution by\nmediation in situations where a defaulting supplier is near insolvent. While\neach party has internal constraints, and if alternate performances are\navailable, such as more costly alternative goods, the proposed approach allows\nthe mediator to find an optimal solution. The notion of optimality is presented\nas adherence to the initial contract, therefore optimising a value function for\nthe non defaulting party. The proposed model includes describing the evolution\nover time of each party's perceived constraints using a phasor like approach\nwith a modulation to the core constraints phasing out of the real part and\nphasing in the imaginary part of complex numbers. The offers related to\nalternative performances by the defaulting party are modelled by a Gompertz\nfunction, being an exponential learning curve of the supplier in regards to the\nreaction to its offers, limited by another exponential function when\napproaching its internal constraints. Furthermore, the model takes into account\nthe discount associated to the delay in the delivery time of the alternative\nperformances.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.04466v1"
    },
    {
        "title": "Blunt Honesty, Incentives, and Knowledge Exchange",
        "authors": [
            "Bruce Knuteson"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We propose a simple mechanism to facilitate the buying and selling of useful,\nbluntly honest information. The for-profit, arm's length knowledge exchange\nthis mechanism enables may dramatically increase the pace of scientific\nprogress.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.06188v1"
    },
    {
        "title": "When does inequality freeze an economy?",
        "authors": [
            "João Pedro Jerico",
            "François P. Landes",
            "Matteo Marsili",
            "Isaac Pérez Castillo",
            "Valerio Volpati"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Inequality and its consequences are the subject of intense recent debate.\nUsing a simplified model of the economy, we address the relation between\ninequality and liquidity, the latter understood as the frequency of economic\nexchanges. Assuming a Pareto distribution of wealth for the agents, that is\nconsistent with empirical findings, we find an inverse relation between wealth\ninequality and overall liquidity. We show that an increase in the inequality of\nwealth results in an even sharper concentration of the liquid financial\nresources. This leads to a congestion of the flow of goods and the arrest of\nthe economy when the Pareto exponent reaches one.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.07300v3"
    },
    {
        "title": "A Rank-Based Approach to Zipf's Law",
        "authors": [
            "Ricardo T. Fernholz",
            "Robert Fernholz"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  An Atlas model is a rank-based system of continuous semimartingales for which\nthe steady-state values of the processes follow a power law, or Pareto\ndistribution. For a power law, the log-log plot of these steady-state values\nversus rank is a straight line. Zipf's law is a power law for which the slope\nof this line is -1. In this note, rank-based conditions are found under which\nan Atlas model will follow Zipf's law. An advantage of this rank-based approach\nis that it provides information about the dynamics of systems that result in\nZipf's law.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.08533v1"
    },
    {
        "title": "The Impact of Services on Economic Complexity: Service Sophistication as\n  Route for Economic Growth",
        "authors": [
            "Viktor Stojkoski",
            "Zoran Utkovski",
            "Ljupco Kocarev"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Economic complexity reflects the amount of knowledge that is embedded in the\nproductive structure of an economy. By combining tools from network science and\neconometrics, a robust and stable relationship between a country's productive\nstructure and its economic growth has been established. Here we report that not\nonly goods but also services are important for predicting the rate at which\ncountries will grow. By adopting a terminology which classifies manufactured\ngoods and delivered services as products, we investigate the influence of\nservices on the country's productive structure. In particular, we provide\nevidence that complexity indices for services are in general higher than those\nfor goods, which is reflected in a general tendency to rank countries with\ndeveloped service sector higher than countries with economy centred on\nmanufacturing of goods. By focusing on country dynamics based on experimental\ndata, we investigate the impact of services on the economic complexity of\ncountries measured in the product space (consisting of both goods and\nservices). Importantly, we show that diversification of service exports and its\nsophistication can provide an additional route for economic growth in both\ndeveloping and developed countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.06284v2"
    },
    {
        "title": "A new structural stochastic volatility model of asset pricing and its\n  stylized facts",
        "authors": [
            "Radu T. Pruna",
            "Maria Polukarov",
            "Nicholas R. Jennings"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Building on a prominent agent-based model, we present a new structural\nstochastic volatility asset pricing model of fundamentalists vs. chartists\nwhere the prices are determined based on excess demand. Specifically, this\nallows for modelling stochastic interactions between agents, based on a herding\nprocess corrected by a price misalignment, and incorporating strong noise\ncomponents in the agents' demand. The model's parameters are estimated using\nthe method of simulated moments, where the moments reflect the basic properties\nof the daily returns of a stock market index. In addition, for the first time\nwe apply a (parametric) bootstrap method in a setting where the switching\nbetween strategies is modelled using a discrete choice approach. As we\ndemonstrate, the resulting dynamics replicate a rich set of the stylized facts\nof the daily financial data including: heavy tails, volatility clustering, long\nmemory in absolute returns, as well as the absence of autocorrelation in raw\nreturns, volatility-volume correlations, aggregate Gaussianity, concave price\nimpact and extreme price events.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.08824v1"
    },
    {
        "title": "The puzzle that just isn't",
        "authors": [
            "Christian Mueller-Kademann"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In his stimulating article on the reasons for two puzzling observations about\nthe behaviour of interest rates, exchange rates and the rate of inflation,\nCharles Engel (2016) puts forward an explanation that rests on the concept of a\nnon-pecuniary liquidity return on assets. Albeit intriguing the analysis\nstruggles to account for a number of facts which are familiar to participants\nof the foreign exchange and bond markets. Reconciling these facts in\nconjunction with a careful dissection of the \"puzzle\" to begin with, shows that\nthe forward premium puzzle just does not exist, at least not in its canonical\nform.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.08895v1"
    },
    {
        "title": "Residential income segregation: A behavioral model of the housing market",
        "authors": [
            "Marco Pangallo",
            "Jean Pierre Nadal",
            "Annick Vignes"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We represent the functioning of the housing market and study the relation\nbetween income segregation, income inequality and house prices by introducing a\nspatial Agent-Based Model (ABM). Differently from traditional models in urban\neconomics, we explicitly specify the behavior of buyers and sellers and the\nprice formation mechanism. Buyers who differ by income select among\nheterogeneous neighborhoods using a probabilistic model of residential choice;\nsellers employ an aspiration level heuristic to set their reservation offer\nprice; prices are determined through a continuous double auction. We first\nprovide an approximate analytical solution of the ABM, shedding light on the\nstructure of the model and on the effect of the parameters. We then simulate\nthe ABM and find that: (i) a more unequal income distribution lowers the prices\nglobally, but implies stronger segregation; (ii) a spike of the demand in one\npart of the city increases the prices all over the city; (iii) subsidies are\nmore efficient than taxes in fostering social mixing.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00424v3"
    },
    {
        "title": "A non-equilibrium formulation of food security resilience",
        "authors": [
            "Matteo Smerlak",
            "Bapu Vaitla"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Resilience, the ability to recover from adverse events (\"shocks\"), is of\nfundamental importance to food security. This is especially true in poor\ncountries, where basic needs are frequently threatened by economic,\nenvironmental, and health shocks. An empirically sound formalization of the\nconcept of food security resilience, however, is lacking. Here we introduce a\ngeneral framework for quantifying resilience based on a simple definition: a\nunit is resilient if $(a)$ its long-term food security trend is not\ndeteriorating and $(b)$ the effects of shocks on this trend do not persist over\ntime. Our approach can be applied to any food security variable for which\nhigh-frequency time-series data is available, can accommodate any unit of\nanalysis (e.g., individuals, households, countries), and is especially useful\nin rapidly changing contexts wherein standard equilibrium-based economic models\nare ineffective. We illustrate our method with an analysis of per capita\nkilocalorie availability for 161 countries between 1961 and 2011. We find that\nresilient countries are not necessarily those that are characterized by high\nlevels or less volatile fluctuations of kilocalorie intake. Accordingly, food\nsecurity policies and programs will need to be tailored not only to welfare\nlevels at any one time, but also to long-run welfare dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.02783v1"
    },
    {
        "title": "Optimal Consumption, Investment and Housing with Means-tested Public\n  Pension in Retirement",
        "authors": [
            "Johan G. Andreasson",
            "Pavel V. Shevchenko",
            "Alex Novikov"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In this paper, we develop an expected utility model for the retirement\nbehavior in the decumulation phase of Australian retirees with sequential\nfamily status subject to consumption, housing, investment, bequest and\ngovernment provided means-tested Age Pension. We account for mortality risk and\nrisky investment assets, and introduce a health proxy to capture the decreasing\nlevel of consumption for older retirees. Then we find optimal housing at\nretirement, and optimal consumption and optimal risky asset allocation\ndepending on age and wealth. The model is solved numerically as a stochastic\ncontrol problem, and is calibrated using the maximum likelihood method on\nempirical data of consumption and housing from the Australian Bureau of\nStatistics 2009-2010 Survey. The model fits the characteristics of the data\nwell to explain the behavior of Australian retirees. The key findings are the\nfollowing: First, the optimal policy is highly sensitive to means-tested Age\nPension early in retirement but this sensitivity fades with age. Secondly, the\nallocation to risky assets shows a complex relationship with the means-tested\nAge Pension that disappears once minimum withdrawal rules are enforced. As a\ngeneral rule, when wealth decreases the proportion allocated to risky assets\nincreases, due to the Age Pension working as a buffer against investment\nlosses. Finally, couples can be more aggressive with risky allocations due to\ntheir longer life expectancy compared with singles.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.08984v1"
    },
    {
        "title": "Divisive-agglomerative algorithm and complexity of automatic\n  classification problems",
        "authors": [
            "Alexander Rubchinsky"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  An algorithm of solution of the Automatic Classification (AC for brevity)\nproblem is set forth in the paper. In the AC problem, it is required to find\none or several artitions, starting with the given pattern matrix or\ndissimilarity, similarity matrix.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02419v1"
    },
    {
        "title": "Rating models: emerging market distinctions",
        "authors": [
            "Alexander Karminsky"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The Basel II Accords have sparked increased interest in the development of\napproaches based on internal ratings systems and have initiated the elaboration\nof models for remote ratings forecasts based on external ones as part of Risk\nManagement and Early Warning Systems. This article evaluates the peculiarities\nof current ratings systems and addresses specific issues of development of\neconometrical rating models for emerging market companies.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02422v1"
    },
    {
        "title": "The fallacy of evidence based policy",
        "authors": [
            "Andrea Saltelli",
            "Mario Giampietro"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  The use of science for policy is at the core of a perfect storm generated by\nthe insurgence of several concurrent crises: of science, of trust, of\nsustainability. The modern positivistic model of science for policy, known as\nevidence based policy, is based on dramatic simplifications and compressions of\navailable perceptions of the state of affairs and possible explanations\n(hypocognition). This model can result in flawed prescriptions. The flaws\nbecome more evident when dealing with complex issues characterized by\nconcomitant uncertainties in the normative, descriptive and ethical domains. In\nthis situation evidence-based policy may concur to the fragility of the social\nsystem. Science plays an important role in reducing the feeling of\nvulnerability of humans by projecting a promise of protection against\nuncertainties. In many applications quantitative science is used to remove\nuncertainty by transforming it into probability, so that mathematical modelling\ncan play the ritual role of haruspices. This epistemic governance arrangement\nis today in crisis. The primacy of science to adjudicate political issues must\npass through an assessment of the level of maturity and effectiveness of the\nvarious disciplines deployed. The solution implies abandoning dreams of\nprediction, control and optimization obtained by relying on a limited set of\nsimplified narratives to define the problem and moving instead to an open\nexploration of a broader set of plausible and relevant stories. Evidence based\npolicy has to be replaced by robust policy, where robustness is tested with\nrespect to feasibility (compatibility with processes outside human control);\nviability (compatibility with processes under human control, in relation to\nboth the economic and technical dimensions), and desirability domain\n(compatibility with a plurality of normative considerations relevant to a\nplurality of actors).\n",
        "pdf_link": "http://arxiv.org/pdf/1607.07398v1"
    },
    {
        "title": "Bayesian nonparametric sparse VAR models",
        "authors": [
            "Monica Billio",
            "Roberto Casarin",
            "Luca Rossini"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  High dimensional vector autoregressive (VAR) models require a large number of\nparameters to be estimated and may suffer of inferential problems. We propose a\nnew Bayesian nonparametric (BNP) Lasso prior (BNP-Lasso) for high-dimensional\nVAR models that can improve estimation efficiency and prediction accuracy. Our\nhierarchical prior overcomes overparametrization and overfitting issues by\nclustering the VAR coefficients into groups and by shrinking the coefficients\nof each group toward a common location. Clustering and shrinking effects\ninduced by the BNP-Lasso prior are well suited for the extraction of causal\nnetworks from time series, since they account for some stylized facts in\nreal-world networks, which are sparsity, communities structures and\nheterogeneity in the edges intensity. In order to fully capture the richness of\nthe data and to achieve a better understanding of financial and macroeconomic\nrisk, it is therefore crucial that the model used to extract network accounts\nfor these stylized facts.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.02740v6"
    },
    {
        "title": "Monetary economics from econophysics perspective",
        "authors": [
            "Victor M. Yakovenko"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  This is an invited article for the Discussion and Debate special issue of The\nEuropean Physical Journal Special Topics on the subject \"Can Economics Be a\nPhysical Science?\" The first part of the paper traces the personal path of the\nauthor from theoretical physics to economics. It briefly summarizes\napplications of statistical physics to monetary transactions in an ensemble of\neconomic agents. It shows how a highly unequal probability distribution of\nmoney emerges due to irreversible increase of entropy in the system. The second\npart examines deep conceptual and controversial issues and fallacies in\nmonetary economics from econophysics perspective. These issues include the\nnature of money, conservation (or not) of money, distinctions between money vs.\nwealth and money vs. debt, creation of money by the state and debt by the\nbanks, the origins of monetary crises and capitalist profit. Presentation uses\nplain language understandable to laypeople and may be of interest to both\nspecialists and general public.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04832v1"
    },
    {
        "title": "Electoral Stability and Rigidity",
        "authors": [
            "Michael Y. Levy"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Some argue that political stability is best served through a two-party\nsystem. This study refutes this. The author mathematically defines the\nstability and rigidity of electoral systems comprised of any quantity of\nelectors and parties. In fact, stability is a function of the quantity of\nelectors - i.e., the number of occupied seats at the table. As the number of\nelectors increases, the properties of an electorate are increasingly well\nresolved, and well described by those of an electorate that is least excessive\n-- that is to say an electorate that is closest to equilibrium. Further,\nelectoral rigidity is a function of the quantity of parties and their\nprobabilities of representation. An absolutely rigid system admits no\nfluctuations -- whatever happens to one elector will happen to all electors. As\nthe quantity of parties increases so does the number of party lines, and with\nit the quantity of alternatives with which to respond to an external stimulus.\nRigidity is significant in a social system that places high value on party\nloyalty. In conclusion, (i) electoral stability is best served by increasing\nthe quantity of electors; (ii) electoral rigidity is best served by decreasing\nthe quantity of parties, and by increasing the representation of some parties\nat the expense of others; and (iii) the less stable a branch of government, the\nmore concern is placed on those who would hold those offices for the people.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05038v1"
    },
    {
        "title": "The interaction between trade and FDI: the CEE countries experience",
        "authors": [
            "Claudiu Tiberiu Albulescu",
            "Daniel Goyeau"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Inside the EU, the commercial integration of the CEE countries has gained\nremarkable momentum before the crisis appearance, but it has slightly slowed\ndown afterwards. Consequently, the interest in identifying the factors\nsupporting the commercial integration process is high. Recent findings in the\nnew trade theory suggest that FDI influence the trade intensity but the studies\napproaching this relationship for the CEE countries present mixed evidence, and\ninvestigate the commercial integration of CEE countries with the old EU\nmembers. Against this background, the purpose of this paper is to assess the\nCEE countries' intra-integration, focusing on the Czech Republic, Hungary,\nPoland and the Slovak Republic. For each country we employ a panel\ngravitational model for the bilateral trade and FDI, considering its\ninteractions with the other three countries in the sample on the one hand, and\nwith the three EU main commercial partners on the other hand. We investigate\ndifferent facets of the trade -- FDI nexus, resorting to a fixed effects model,\na random effects model, as well as to an instrumental variable estimator, over\nthe period 2000-2013. Our results suggest that outward FDI sustains the CEE\ncountries' commercial integration, while inward FDI has no significant effect.\nIn all the cases a complementarity effect between trade and FDI is documented,\nwhich is stronger for the CEE countries' historical trade partners.\nConsequently, these findings show that CEE countries' policymakers are\ninterested in encouraging the outward FDI toward their neighbour countries in\norder to increase the commercial integration.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.02334v1"
    },
    {
        "title": "The Solution to Science's Replication Crisis",
        "authors": [
            "Bruce Knuteson"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The solution to science's replication crisis is a new ecosystem in which\nscientists sell what they learn from their research. In each pairwise\ntransaction, the information seller makes (loses) money if he turns out to be\ncorrect (incorrect). Responsibility for the determination of correctness is\ndelegated, with appropriate incentives, to the information purchaser. Each\ntransaction is brokered by a central exchange, which holds money from the\nanonymous information buyer and anonymous information seller in escrow, and\nwhich enforces a set of incentives facilitating the transfer of useful, bluntly\nhonest information from the seller to the buyer. This new ecosystem, capitalist\nscience, directly addresses socialist science's replication crisis by\nexplicitly rewarding accuracy and penalizing inaccuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.03223v1"
    },
    {
        "title": "The Informational Content of the Limit Order Book: An Empirical Study of\n  Prediction Markets",
        "authors": [
            "Joachim R. Groeger"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In this paper I empirically investigate prediction markets for binary\noptions. Advocates of prediction markets have suggested that asset prices are\nconsistent estimators of the \"true\" probability of a state of the world being\nrealized. I test whether the market reaches a \"consensus.\" I find little\nevidence for convergence in beliefs. I then determine whether an econometrician\nusing data beyond execution prices can leverage this data to estimate the\nconsensus belief. I use an incomplete specification of equilibrium outcomes to\nderive bounds on beliefs from order submission decisions. Interval estimates of\nmean beliefs cannot exclude aggregate beliefs equal to 0.5.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.03471v1"
    },
    {
        "title": "Quantifying China's Regional Economic Complexity",
        "authors": [
            "Jian Gao",
            "Tao Zhou"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  China has experienced an outstanding economic expansion during the past\ndecades, however, literature on non-monetary metrics that reveal the status of\nChina's regional economic development are still lacking. In this paper, we fill\nthis gap by quantifying the economic complexity of China's provinces through\nanalyzing 25 years' firm data. First, we estimate the regional economic\ncomplexity index (ECI), and show that the overall time evolution of provinces'\nECI is relatively stable and slow. Then, after linking ECI to the economic\ndevelopment and the income inequality, we find that the explanatory power of\nECI is positive for the former but negative for the latter. Next, we compare\ndifferent measures of economic diversity and explore their relationships with\nmonetary macroeconomic indicators. Results show that the ECI index and the\nnon-linear iteration based Fitness index are comparative, and they both have\nstronger explanatory power than other benchmark measures. Further multivariate\nregressions suggest the robustness of our results after controlling other\nsocioeconomic factors. Our work moves forward a step towards better\nunderstanding China's regional economic development and non-monetary\nmacroeconomic indicators.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.01292v2"
    },
    {
        "title": "Collective Learning in China's Regional Economic Development",
        "authors": [
            "Jian Gao",
            "Bogang Jun",
            "Alex \"Sandy\" Pentland",
            "Tao Zhou",
            "Cesar A. Hidalgo"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Industrial development is the process by which economies learn how to produce\nnew products and services. But how do economies learn? And who do they learn\nfrom? The literature on economic geography and economic development has\nemphasized two learning channels: inter-industry learning, which involves\nlearning from related industries; and inter-regional learning, which involves\nlearning from neighboring regions. Here we use 25 years of data describing the\nevolution of China's economy between 1990 and 2015--a period when China\nmultiplied its GDP per capita by a factor of ten--to explore how Chinese\nprovinces diversified their economies. First, we show that the probability that\na province will develop a new industry increases with the number of related\nindustries that are already present in that province, a fact that is suggestive\nof inter-industry learning. Also, we show that the probability that a province\nwill develop an industry increases with the number of neighboring provinces\nthat are developed in that industry, a fact suggestive of inter-regional\nlearning. Moreover, we find that the combination of these two channels exhibit\ndiminishing returns, meaning that the contribution of either of these learning\nchannels is redundant when the other one is present. Finally, we address\nendogeneity concerns by using the introduction of high-speed rail as an\ninstrument to isolate the effects of inter-regional learning. Our\ndifferences-in-differences (DID) analysis reveals that the introduction of high\nspeed-rail increased the industrial similarity of pairs of provinces connected\nby high-speed rail. Also, industries in provinces that were connected by rail\nincreased their productivity when they were connected by rail to other\nprovinces where that industry was already present. These findings suggest that\ninter-regional and inter-industry learning played a role in China's great\neconomic expansion.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.01369v1"
    },
    {
        "title": "Multi-unit Assignment under Dichotomous Preferences",
        "authors": [
            "Josue Ortega"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  I study the problem of allocating objects among agents without using money.\nAgents can receive several objects and have dichotomous preferences, meaning\nthat they either consider objects to be acceptable or not. In this setup, the\negalitarian solution is more appealing than the competitive equilibrium with\nequal incomes because it is Lorenz dominant, unique in utilities, and group\nstrategy-proof. Moreover, it can be adapted to satisfy a new fairness axiom\nthat arises naturally in this context. Both solutions are disjoint.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.10897v6"
    },
    {
        "title": "ICT and Employment in India: A Sectoral Level Analysis",
        "authors": [
            "Dr. Pawan Kumar"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  How technology affects growth or employment has long been debated. With a\nhiatus, the debate revived once again in the form of how Information and\nCommunications Technology, as a form of new technology, exerts on productivity\nand employment. Information and Communications Technology perceived as General\nPurpose Technology like steam engine or electricity in the past, ushered the\nworld into a new techno-economic paradigm, given its deep social, economic and\ncultural implications. For instance, within economic implication, it is hard to\nimagine an economic activity that does not it, directly or indirectly.\nEventually, Information and Communications Technology intensity, measure as the\nratio of Information and Communications Technology investment to total\ninvestment, increased phenomenally in industries across sectors.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01316v1"
    },
    {
        "title": "An equation for a time-dependent profit rate",
        "authors": [
            "Rafael D. Sorkin"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Taking as a hypothesis a form of the labour theory of value, and $without$\n$assuming$ $equilibrium$, we derive an equation that yields the profit-rate\n$\\pi$ as a function of time. For a mature economy, $\\pi(t)$ reduces to the\nproduct of two factors: ($i$) a certain $retarded$ $average$ of the sum of the\ngrowth-rates of productivity and of the size of the labour-force measured by\nhours worked, and ($ii$) the ratio of the current rate of surplus value to its\nown retarded average. We also suggest an empirical test of the equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.02559v1"
    },
    {
        "title": "Social Integration in Two-Sided Matching Markets",
        "authors": [
            "Josue Ortega"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  When several two-sided matching markets merge into one, it is inevitable that\nsome agents will become worse off if the matching mechanism used is stable. I\nformalize this observation by defining the property of integration\nmonotonicity, which requires that every agent becomes better off after any\nnumber of matching markets merge. Integration monotonicity is also incompatible\nwith the weaker efficiency property of Pareto optimality.\n  Nevertheless, I obtain two possibility results. First, stable matching\nmechanisms never hurt more than one-half of the society after the integration\nof several matching markets occurs. Second, in random matching markets there\nare positive expected gains from integration for both sides of the market,\nwhich I quantify.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.08033v5"
    },
    {
        "title": "A Quantum-like Model of Selection Behavior",
        "authors": [
            "Masanari Asano",
            "Irina Basieva",
            "Andrei Khrennikov",
            "Masanori Ohya",
            "Yoshiharu Tanaka"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  In this paper, we introduce a new model of selection behavior under risk that\ndescribes an essential cognitive process for comparing values of objects and\nmaking a selection decision. This model is constructed by the quantum-like\napproach that employs the state representation specific to quantum theory,\nwhich has the mathematical framework beyond the classical probability theory.\nWe show that our quantum approach can clearly explain the famous examples of\nanomalies for the expected utility theory, the Ellsberg paradox, the Machina\nparadox and the disparity between WTA and WTP. Further, we point out that our\nmodel mathematically specifies the characteristics of the probability weighting\nfunction and the value function, which are basic concepts in the prospect\ntheory.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.08536v1"
    },
    {
        "title": "Lorenz curves interpretations of the Bruss-Duerinckx theorem for\n  resource dependent branching processes",
        "authors": [
            "Alexandre Jacquemain"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  The Bruss and Duerinckx theorem for resource dependent branching processes\nstates that the survival of any society form is nested in an envelope formed by\ntwo extreme policies. The objective of this paper is to give a novel\ninterpretation of this theorem through the use of Lorenz curves. This\nrepresentation helps us visualize how the parameters interplay. Besides, as we\nwill show, it clarifies the impact of inequality in consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.01085v1"
    },
    {
        "title": "Systematic Noise: Micro-movements in Equity Options Markets",
        "authors": [
            "Adam Wu"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Equity options are known to be notoriously difficult to price accurately, and\neven with the development of established mathematical models there are many\nassumptions that must be made about the underlying processes driving market\nmovements. As such, the theoretical prices outputted by these models are often\nslightly different from the realized or actual market price. The choice of\nmodel traders use can create many different valuations on the same asset, which\nmay lead to a form of systematic micro-movement or noise. The analysis in this\npaper demonstrates that approximately 1.7%-4.5% of market volume for options\nwritten on the SPY ETF within the last two years could potentially be due to\nsystematic noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.06855v1"
    },
    {
        "title": "An equilibrium-conserving taxation scheme for income from capital",
        "authors": [
            "Jacques Tempere"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Under conditions of market equilibrium, the distribution of capital income\nfollows a Pareto power law, with an exponent that characterizes the given\nequilibrium. Here, a simple taxation scheme is proposed such that the post-tax\ncapital income distribution remains an equilibrium distribution, albeit with a\ndifferent exponent. This taxation scheme is shown to be progressive, and its\nparameters can be simply derived from (i) the total amount of tax that will be\nlevied, (ii) the threshold selected above which capital income will be taxed\nand (iii) the total amount of capital income. The latter can be obtained either\nby using Piketty's estimates of the capital/labor income ratio or by fitting\nthe initial Pareto exponent. Both ways moreover provide a check on the amount\nof declared income from capital.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.08275v1"
    },
    {
        "title": "Optimal Inflation Target: Insights from an Agent-Based Model",
        "authors": [
            "Jean-Philippe Bouchaud",
            "Stanislao Gualdi",
            "Marco Tarzia",
            "Francesco Zamponi"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Which level of inflation should Central Banks be targeting? We investigate\nthis issue in the context of a simplified Agent Based Model of the economy.\nDepending on the value of the parameters that describe the behaviour of agents\n(in particular inflation anticipations), we find a rich variety of behaviour at\nthe macro-level. Without any active monetary policy, our ABM economy can be in\na high inflation/high output state, or in a low inflation/low output state.\nHyper-inflation, deflation and \"business cycles\" between coexisting states are\nalso found. We then introduce a Central Bank with a Taylor rule-based inflation\ntarget, and study the resulting aggregate variables. Our main result is that\ntoo-low inflation targets are in general detrimental to a CB-monitored economy.\nOne symptom is a persistent under-realisation of inflation, perhaps similar to\nthe current macroeconomic situation. Higher inflation targets are found to\nimprove both unemployment and negative interest rate episodes. Our results are\ncompared with the predictions of the standard DSGE model.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.05117v2"
    },
    {
        "title": "Ownership Cost Calculations for Distributed Energy Resources Using\n  Uncertainty and Risk Analyses",
        "authors": [
            "S. Ali Pourmousavi",
            "Mahdi Behrangrad",
            "Ali Jahanbani Ardakani",
            "M. Hashem Nehrir"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Ownership cost calculation plays an important role in optimal operation of\ndistributed energy resources (DERs) and microgrids (MGs) in the future power\nsystem, known as smart grid. In this paper, a general framework for ownership\ncost calculation is proposed using uncertainty and risk analyses. Four\nownership cost calculation approaches are introduced and compared based on\ntheir associated risk values. Finally, the best method is chosen based on a\nseries of simulation results, performed for a typical diesel generator (DiG).\nAlthough simulation results are given for a DiG (as commonly used in MGs), the\nproposed approaches can be applied to other MG components, such as batteries,\nwith slight modifications, as presented in this paper. The analyses and\nproposed approaches can be useful in MG optimal design, optimal power flow, and\nmarket-based operation of the smart grid for accurate operational cost\ncalculations.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08023v1"
    },
    {
        "title": "Interpreting Economic Complexity",
        "authors": [
            "Penny Mealy",
            "J. Doyne Farmer",
            "Alexander Teytelboym"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Two network measures known as the Economic Complexity Index (ECI) and Product\nComplexity Index (PCI) have provided important insights into patterns of\neconomic development. We show that the ECI and PCI are equivalent to a spectral\nclustering algorithm that partitions a similarity graph into two parts. The\nmeasures are also related to various dimensionality reduction methods and can\nbe interpreted as vectors that determine distances between nodes based on their\nsimilarity. Our results shed a new light on the ECI's empirical success in\nexplaining cross-country differences in GDP/capita and economic growth, which\nis often linked to the diversity of country export baskets. In fact, countries\nwith high (low) ECI tend to specialize in high (low) PCI products. We also find\nthat the ECI and PCI uncover economically informative specialization patterns\nacross US states and UK regions.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08245v3"
    },
    {
        "title": "An Inverse Problem Study: Credit Risk Ratings as a Determinant of\n  Corporate Governance and Capital Structure in Emerging Markets: Evidence from\n  Chinese Listed Companies",
        "authors": [
            "ManYing Kang",
            "Marcel Ausloos"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Credit risk rating is shown to be a relevant determinant in order to estimate\ngood corporate governance and to self-optimize capital structure. The\nconclusion is argued from a study on a selected (and justified) sample of (182)\ncompanies listed on the Shanghai Stock Exchange and the Shenzhen Stock Exchange\nand which use the same Shanghai Brilliance Credit Rating & Investors Service\nCompany assessment criteria, for their credit ratings, from 2010 to 2015.\nPractically, 3 debt ratios are examined in terms of 11 characteristic\nvariables. Moreover, any relationship between credit rating and corporate\ngovernance can be thought to be an interesting finding. The relationship\nbetween credit rating and leverage is not as evident as that found by other\nresearchers from different countries; it is significantly positively related to\nthe outside director, firm size, tangible assets and firm age, and CEO and\nchairman office plurality. However, leverage is found to be negatively\ncorrelated with board size, profitability, growth opportunity, and non-debt tax\nshield. Credit rating is positively associated with leverage, but in a less\nsignificant way. CEO-Board chairship duality is insignificantly related to\nleverage. The non-debt tax shield is significantly correlated with leverage.\nThe correlation coefficient between CEO duality and auditor is positive but\nweakly significant, but seems not consistent with expectations. Finally,\nprofitability cause could be regarded as an interesting finding. Indeed, there\nis an inverse correlation between profitability and total debt (Notice that the\nresult supports the pecking order theory). In conclusion, it appears that\ncredit rating has less effect on the so listed large Chinese companies than in\nother countries. Nevertheless, the perspective of assessing credit risk rating\nby relevant agencies is indubitably a recommended time dependent leverage\ndeterminant.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.00602v1"
    },
    {
        "title": "Closed-form Solutions for the Lucas-Uzawa model: Unique or Multiple",
        "authors": [
            "Rehana Naz"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Naz and Chaudhry [3] established multiple closed-form solutions for the basic\nLucas-Uzawa model. According to Boucekkine and Ruiz-Tamarit [1] and Chilarescu\n[2] unique closed-form solutions exist for the basic Lucas-Uzawa model. We\nequate expressions for variables h(t) and u(t). We provide here condition for\nthe unique closed-form solution and proposed an open question for evaluation of\nintegral in closed-form. A similar analysis is carried out for the Lucas-Uzawa\nmodel with logarithmic utility preferences.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.08654v1"
    },
    {
        "title": "Economic interpretation of fractional derivatives",
        "authors": [
            "Valentina V. Tarasova",
            "Vasily E. Tarasov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  An economic interpretation of the Caputo derivatives of non-integer orders is\nproposed. The suggested economic interpretation of the fractional derivatives\nis based on a generalization of average and marginal values of economic\nindicators. We formulate an economic interpretation by using the concept of the\nT-indicator that allows us to describe economic processes with memory. The\nstandard average and marginal values of indicator are special cases of the\nproposed T-indicator, when the order is equal to zero and one, respectively.\nThe fractional derivatives are interpreted as economic characteristics\n(indicators) that are intermediate between the standard average and marginal\nvalues of indicators.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09575v1"
    },
    {
        "title": "Corruption-free scheme of entering into contract: mathematical model",
        "authors": [
            "Oleg Malafeyev",
            "Olga Koroleva",
            "Dmitriy Prusskiy",
            "Olga Zenovich"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  The main purpose of this paper is to formalize the modelling process,\nanalysis and mathematical definition of corruption when entering into a\ncontract between principal agent and producers. The formulation of the problem\nand the definition of concepts for the general case are considered. For\ndefiniteness, all calculations and formulas are given for the case of three\nproducers, one principal agent and one intermediary. Economic analysis of\ncorruption allowed building a mathematical model of interaction between agents.\nFinancial resources distribution problem in a contract with a corrupted\nintermediary is considered.Then proposed conditions for corruption emergence\nand its possible consequences. Optimal non-corruption schemes of financial\nresources distribution in a contract are formed, when principal agent's choice\nis limited first only by asymmetrical information and then also by external\ninfluences.Numerical examples suggesting optimal corruption-free agents'\nbehaviour are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.02333v2"
    },
    {
        "title": "Coherent diversification in corporate technological portfolios",
        "authors": [
            "Emanuele Pugliese",
            "Lorenzo Napolitano",
            "Andrea Zaccaria",
            "Luciano Pietronero"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We study the relationship between firms' performance and their technological\nportfolios using tools borrowed from the complexity science. In particular, we\nask whether the accumulation of knowledge and capabilities related to a\ncoherent set of technologies leads firms to experience advantages in terms of\nproductive efficiency. To this end, we analyzed both the balance sheets and the\npatenting activity of about 70 thousand firms that have filed at least one\npatent over the period 2004-2013. From this database it is possible to define a\nmeasure of the firms' coherent diversification, based on the network of\ntechnological fields, and relate it to the firms' perfomance in terms of labor\nproductivity. Such a measure favors companies with a diversification structure\ncomprising blocks of closely related fields over firms with the same breadth of\nscope, but a more scattered diversification structure. We find that the\ncoherent diversification of firms is quantitatively related to their economic\nperformance and captures relevant information about their productive structure.\nIn particular, we prove on a statistical basis that a naive definition of\ntechnological diversification can explain labor productivity only as a proxy of\nsize and coherent diversification. This approach can be used to investigate\npossible synergies within firms and to recommend viable partners for merging\nand acquisitions.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02188v1"
    },
    {
        "title": "Zipf's Law for Atlas Models",
        "authors": [
            "Ricardo T. Fernholz",
            "Robert Fernholz"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  A set of data with positive values follows a Pareto distribution if the\nlog-log plot of value versus rank is approximately a straight line. A Pareto\ndistribution satisfies Zipf's law if the log-log plot has a slope of -1. Since\nmany types of ranked data follow Zipf's law, it is considered a form of\nuniversality. We propose a mathematical explanation for this phenomenon based\non Atlas models and first-order models, systems of positive continuous\nsemimartingales with parameters that depend only on rank. We show that the\nstable distribution of an Atlas model will follow Zipf's law if and only if two\nnatural conditions, conservation and completeness, are satisfied. Since Atlas\nmodels and first-order models can be constructed to approximate systems of\ntime-dependent rank-based data, our results can explain the universality of\nZipf's law for such systems. However, ranked data generated by other means may\nfollow non-Zipfian Pareto distributions. Hence, our results explain why Zipf's\nlaw holds for word frequency, firm size, household wealth, and city size, while\nit does not hold for earthquake magnitude, cumulative book sales, the intensity\nof solar flares, and the intensity of wars, all of which follow non-Zipfian\nPareto distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.04285v7"
    },
    {
        "title": "Good signals gone bad: dynamic signalling with switching efforts",
        "authors": [
            "Sander Heinsalu"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This paper examines signalling when the sender exerts effort and receives\nbenefits over time. Receivers only observe a noisy public signal about the\neffort, which has no intrinsic value.\n  The modelling of signalling in a dynamic context gives rise to novel\nequilibrium outcomes. In some equilibria, a sender with a higher cost of effort\nexerts strictly more effort than his low-cost counterpart. The low-cost type\ncan compensate later for initial low effort, but this is not worthwhile for a\nhigh-cost type. The interpretation of a given signal switches endogenously over\ntime, depending on which type the receivers expect to send it.\n  JEL classification: D82, D83, C73.\n  Keywords: Dynamic games, signalling , incomplete information\n",
        "pdf_link": "http://arxiv.org/pdf/1707.04699v1"
    },
    {
        "title": "Dynamic Semiparametric Models for Expected Shortfall (and Value-at-Risk)",
        "authors": [
            "Andrew J. Patton",
            "Johanna F. Ziegel",
            "Rui Chen"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Expected Shortfall (ES) is the average return on a risky asset conditional on\nthe return being below some quantile of its distribution, namely its\nValue-at-Risk (VaR). The Basel III Accord, which will be implemented in the\nyears leading up to 2019, places new attention on ES, but unlike VaR, there is\nlittle existing work on modeling ES. We use recent results from statistical\ndecision theory to overcome the problem of \"elicitability\" for ES by jointly\nmodelling ES and VaR, and propose new dynamic models for these risk measures.\nWe provide estimation and inference methods for the proposed models, and\nconfirm via simulation studies that the methods have good finite-sample\nproperties. We apply these models to daily returns on four international equity\nindices, and find the proposed new ES-VaR models outperform forecasts based on\nGARCH or rolling window models.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05108v1"
    },
    {
        "title": "Improving the Economic Complexity Index",
        "authors": [
            "Saleh Albeaik",
            "Mary Kaltenberg",
            "Mansour Alsaleh",
            "Cesar A. Hidalgo"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  How much knowledge is there in an economy? In recent years, data on the mix\nof products that countries export has been used to construct measures of\neconomic complexity that estimate the knowledge available in an economy and\npredict future economic growth. Here we introduce a new and simpler metric of\neconomic complexity (ECI+) that measures the total exports of an economy\ncorrected by how difficult it is to export each product. We use data from 1973\nto 2013 to compare the ability of ECI+, the Economic Complexity Index (ECI),\nand Fitness complexity, to predict future economic growth using 5, 10, and\n20-year panels in a pooled OLS, a random effects model, and a fixed effects\nmodel. We find that ECI+ outperforms ECI and Fitness in its ability to predict\neconomic growth and in the consistency of its estimators across most\neconometric specifications. On average, one standard deviation increase in ECI+\nis associated with an increase in annualized growth of about 4% to 5%. We then\ncombine ECI+ with measures of physical capital, human capital, and\ninstitutions, to find a robust model of economic growth. The ability of ECI+ to\npredict growth, and the value of its coefficient, is robust to these controls.\nAlso, we find that human capital, political stability, and control of\ncorruption; are positively associated with future economic growth, and that\nincome is negatively associated with growth, in agreement with the traditional\ngrowth literature. Finally, we use ECI+ to generate economic growth predictions\nfor the next 20 years and compare these predictions with the ones obtained\nusing ECI and Fitness. These findings improve the methods available to estimate\nthe knowledge intensity of economies and predict future economic growth.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05826v3"
    },
    {
        "title": "Cross Ranking of Cities and Regions: Population vs. Income",
        "authors": [
            "Roy Cerqueti",
            "Marcel Ausloos"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper explores the relationship between the inner economical structure\nof communities and their population distribution through a rank-rank analysis\nof official data, along statistical physics ideas within two techniques. The\ndata is taken on Italian cities. The analysis is performed both at a global\n(national) and at a more local (regional) level in order to distinguish \"macro\"\nand \"micro\" aspects. First, the rank-size rule is found not to be a standard\npower law, as in many other studies, but a doubly decreasing power law. Next,\nthe Kendall and the Spearman rank correlation coefficients which measure pair\nconcordance and the correlation between fluctuations in two rankings,\nrespectively, - as a correlation function does in thermodynamics, are\ncalculated for finding rank correlation (if any) between demography and wealth.\nResults show non only global disparities for the whole (country) set, but also\n(regional) disparities, when comparing the number of cities in regions, the\nnumber of inhabitants in cities and that in regions, as well as when comparing\nthe aggregated tax income of the cities and that of regions. Different outliers\nare pointed out and justified. Interestingly, two classes of cities in the\ncountry and two classes of regions in the country are found. \"Common sense\"\nsocial, political, and economic considerations sustain the findings. More\nimportantly, the methods show that they allow to distinguish communities, very\nclearly, when specific criteria are numerically sound. A specific modeling for\nthe findings is presented, i.e. for the doubly decreasing power law and the two\nphase system, based on statistics theory, e.g., urn filling. The model ideas\ncan be expected to hold when similar rank relationship features are observed in\nfields. It is emphasized that the analysis makes more sense than one through a\nPearson value-value correlation analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.02414v1"
    },
    {
        "title": "On the Characteristics of the Free Market in a Cooperative Society",
        "authors": [
            "Norbert Agbeko"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  The key characteristic of a true free market economy is that exchanges are\nentirely voluntary. When there is a monopoly in the creation of currency as we\nhave in today's markets, you no longer have a true free market. Features of the\ncurrent economic system such as central banking and taxation would be\nnonexistent in a free market. This paper examines how currency monopoly leads\nto the instabilities and imbalances that we see in today's economy. It also\nproposes that currencies should emerge from the voluntary exchange of goods and\nservices, and studies economic interaction across all scales, by considering\neconomic action in cases where the self-interests of individuals are\ncoincident. By examining the voluntary exchange of goods and services at the\nscale of an entire society, it is shown that a new currency system, which\nresolves a lot of the problems caused by the current fiat currency system,\nemerges naturally from the free market. The new currency system is robust and\nefficient, and provides a way for public goods and services to be provided, and\nits providers compensated, without the need for direct taxation.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.03917v1"
    },
    {
        "title": "An Exchange Rate Target Zone Model with a Terminal Condition and\n  Mean-Reverting Fundamentals",
        "authors": [
            "Viktors Ajevskis"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper proposes a target zones exchange rate model with a terminal\ncondition of entering a currency zone. It is assumed that the exchange rate is\na function of the fundamental and time. Another essential assumptions of the\nmodel is that the fundamental process is bounded inside a band and that\nterminal condition for the exchange rate holds. The fundamental is specified in\ntwo ways: as a regulated Brownian motion and Ornstein-Uhlenbeck processes. For\nthe case of the Brownian motion process the closed form solution of the problem\nis obtained, whereas for the Ornstein-Uhlenbeck process the closed form\nsolution does not exist, therefore we had to use numerical method for solving\nof the problem. Both specifications are compared numerically.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.04880v1"
    },
    {
        "title": "Understanding the Impact of Microcredit Expansions: A Bayesian\n  Hierarchical Analysis of 7 Randomised Experiments",
        "authors": [
            "Rachael Meager"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Bayesian hierarchical models are a methodology for aggregation and synthesis\nof data from heterogeneous settings, used widely in statistics and other\ndisciplines. I apply this framework to the evidence from 7 randomized\nexperiments of expanding access to microcredit to assess the general impact of\nthe intervention on household outcomes and the heterogeneity in this impact\nacross sites. The results suggest that the effect of microcredit is likely to\nbe positive but small relative to control group average levels, and the\npossibility of a negative impact cannot be ruled out. By contrast, common\nmeta-analytic methods that pool all the data without assessing the\nheterogeneity misleadingly produce \"statistically significant\" results in 2 of\nthe 6 household outcomes. Standard pooling metrics for the studies indicate on\naverage 60% pooling on the treatment effects, suggesting that the site-specific\neffects are reasonably externally valid, and thus informative for each other\nand for the general case. The cross-study heterogeneity is almost entirely\ngenerated by heterogeneous effects for the 27% households who previously\noperated businesses before microcredit expansion, although this group is likely\nto see much larger impacts overall. A Ridge regression procedure to assess the\ncorrelations between site-specific covariates and treatment effects indicates\nthat the remaining heterogeneity is strongly correlated with differences in\neconomic variables, but not with differences in study design protocols. The\naverage interest rate and the average loan size have the strongest correlation\nwith the treatment effects, and both are negative.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.06669v3"
    },
    {
        "title": "Forecasting crude oil market volatility: can the Regime Switching GARCH\n  model beat the single-regime GARCH models?",
        "authors": [
            "Yue-Jun Zhang",
            "Ting Yao",
            "Ling-Yun He"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In order to obtain a reasonable and reliable forecast method for crude oil\nprice volatility, this paper evaluates the forecast performance of\nsingle-regime GARCH models (including the standard linear GARCH model and the\nnonlinear GJR-GARCH and EGARCH models) and the two-regime Markov Regime\nSwitching GARCH (MRS-GARCH) model for crude oil price volatility at different\ndata frequencies and time horizons. The results indicate that, first, the\ntwo-regime MRS-GARCH model beats other three single-regime GARCH type models in\nin-sample data estimation under most evaluation criteria, although it appears\ninferior under a few of other evaluation criteria. Second, the two-regime\nMRS-GARCH model overall provides more accurate volatility forecast for daily\ndata but this superiority dies way for weekly and monthly data. Third, among\nthe three single-regime GARCH type models, the volatility forecast of the\nnonlinear GARCH models exhibit greater accuracy than the linear GARCH model for\ndaily data at longer time horizons. Finally, the linear single-regime GARCH\nmodel overall performs better than other three nonlinear GARCH type models in\nValue-at-Risk (VaR) forecast.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.01676v1"
    },
    {
        "title": "The role of money and the financial sector in energy-economy models used\n  for assessing climate policy",
        "authors": [
            "H. Pollitt",
            "J. -F. Mercure"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper outlines a critical gap in the assessment methodology used to\nestimate the macroeconomic costs and benefits of climate policy. It shows that\nthe vast majority of models used for assessing climate policy use assumptions\nabout the financial system that sit at odds with the observed reality. In\nparticular, the models' assumptions lead to `crowding out' of capital, which\ncause them to show negative impacts from climate policy in virtually all cases.\nWe compare this approach with that of the E3ME model, which follows\nnon-equilibrium economic theory and adopts a more empirical approach. While the\nnon-equilibrium model also has limitations, its treatment of the financial\nsystem is more consistent with reality and it shows that green investment need\nnot crowd out investment in other parts of the economy -- and may therefore\noffer an economic stimulus.\n  The implication of this finding is that standard CGE models consistently\nover-estimate the costs of climate policy in terms of GDP and welfare,\npotentially by a substantial amount. These findings overly restrict the range\nof possible emission pathways accessible using climate policy from the\nviewpoint of the decision-maker, and may also lead to misleading information\nused for policy making. Improvements in both modelling approaches should be\nsought with some urgency -- both to provide a better assessment of potential\nclimate policy and to improve understanding of the dynamics of the global\nfinancial system more generally.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.02912v1"
    },
    {
        "title": "Unified Growth Theory Contradicted by the Economic Growth in Africa",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  One of the fundamental postulates of the Unified Growth Theory is the claimed\nexistence of three distinctly different regimes of economic growth governed by\nthree distinctly different mechanisms of growth. However, Galor also proposed\nthat the timing of these regimes is different for developed countries and for\nless-developed countries. Africa is the perfect example of economic growth in\nless-developed countries. The data used by Galor, but never properly\ninvestigated, are now analysed. They turn out to be in dramatic contradiction\nof this theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.03164v2"
    },
    {
        "title": "Macroeconomic Dynamics of Assets, Leverage and Trust",
        "authors": [
            "Jeroen Rozendaal",
            "Yannick Malevergne",
            "Didier Sornette"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  A macroeconomic model based on the economic variables (i) assets, (ii)\nleverage (defined as debt over asset) and (iii) trust (defined as the maximum\nsustainable leverage) is proposed to investigate the role of credit in the\ndynamics of economic growth, and how credit may be associated with both\neconomic performance and confidence. Our first notable finding is the mechanism\nof reward/penalty associated with patience, as quantified by the return on\nassets. In regular economies where the EBITA/Assets ratio is larger than the\ncost of debt, starting with a trust higher than leverage results in the highest\nlong-term return on assets (which can be seen as a proxy for economic growth).\nOur second main finding concerns a recommendation for the reaction of a central\nbank to an external shock that affects negatively the economic growth. We find\nthat late policy intervention in the model economy results in the highest\nlong-term return on assets and largest asset value. But this comes at the cost\nof suffering longer from the crisis until the intervention occurs. The\nphenomenon can be ascribed to the fact that postponing intervention allows\ntrust to increase first, and it is most effective to intervene when trust is\nhigh. These results derive from two fundamental assumptions underlying our\nmodel: (a) trust tends to increase when it is above leverage; (b) economic\nagents learn optimally to adjust debt for a given level of trust and amount of\nassets. Using a Markov Switching Model for the EBITA/Assets ratio, we have\nsuccessfully calibrated our model to the empirical data of the return on equity\nof the EURO STOXX 50 for the time period 2000-2013. We find that dynamics of\nleverage and trust can be highly non-monotonous with curved trajectories, as a\nresult of the nonlinear coupling between the variables.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.03618v1"
    },
    {
        "title": "European Union gas market development",
        "authors": [
            "Tobias Baltensperger",
            "Rudolf M. Füchslin",
            "Pius Krütli",
            "John Lygeros"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  The recently announced Energy Union by the European Commission is the most\nrecent step in a series of developments aiming at integrating the EU's gas\nmarkets to increase social welfare (SW) and security of gas supply. Based on a\nspatial partial equilibrium model, we analyze the changes in consumption,\nprices, and SW up to 2022 induced by the infrastructure expansions planned for\nthis period. We find that wholesale prices decrease slightly and converge at\nWestern European levels, the potential of suppliers to exert market power\ndecreases significantly, and consumer surplus increases by 15.9% in the EU. Our\nresults allow us to distinguish three categories of projects: (i) New gas\nsources developed and brought to the EU markets. These projects decrease prices\nand increase SW in a large number of countries. The only project in this\ncategory is the Trans-Anatolian Gas Pipeline; (ii) Existing gas sources made\navailable to additional countries. This leads to an increase of SW in the newly\nconnected countries, and a decrease everywhere else. These projects mainly\ninvolve pipeline and regasification terminal capacity enhancements; (iii)\nProjects with a marginal effect on the (fully functioning) market. Most storage\nexpansion projects fall into this category, plus the recently announced Turkish\nStream. Our results indicate that if all proposed infrastructure projects are\nrealized, the EU's single market will become a reality in 2019. However, we\nalso find that SW can only be increased significantly for the EU as a whole if\nnew gas sources become accessible. Consequently, we suggest that the EU should\nemphasize on measures to increase the available volumes, in particular once the\nintegration of the market is completed. At the same time, efficiency gains,\nalbeit decreasing SW, help to improve the situation of consumers and decrease\nthe dependency of the EU as a whole on external suppliers.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.05343v1"
    },
    {
        "title": "Asymptotic efficiency of the proportional compensation scheme for a\n  large number of producers",
        "authors": [
            "Dmitry B. Rokhlin",
            "Anatoly Usov"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We consider a manager, who allocates some fixed total payment amount between\n$N$ rational agents in order to maximize the aggregate production. The profit\nof $i$-th agent is the difference between the compensation (reward) obtained\nfrom the manager and the production cost. We compare (i) the \\emph{normative}\ncompensation scheme, where the manager enforces the agents to follow an optimal\ncooperative strategy; (ii) the \\emph{linear piece rates} compensation scheme,\nwhere the manager announces an optimal reward per unit good; (iii) the\n\\emph{proportional} compensation scheme, where agent's reward is proportional\nto his contribution to the total output. Denoting the correspondent total\nproduction levels by $s^*$, $\\hat s$ and $\\overline s$ respectively, where the\nlast one is related to the unique Nash equilibrium, we examine the limits of\nthe prices of anarchy $\\mathscr A_N=s^*/\\overline s$, $\\mathscr A_N'=\\hat\ns/\\overline s$ as $N\\to\\infty$. These limits are calculated for the cases of\nidentical convex costs with power asymptotics at the origin, and for power\ncosts, corresponding to the Coob-Douglas and generalized CES production\nfunctions with decreasing returns to scale. Our results show that\nasymptotically no performance is lost in terms of $\\mathscr A'_N$, and in terms\nof $\\mathscr A_N$ the loss does not exceed $31\\%$.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06038v1"
    },
    {
        "title": "Les produits Halal dans les {é}conomies occidentales",
        "authors": [
            "Abdelatif Kerzabi"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In last years, we hear about halal products in non- Muslim societies\nincluding European and American ones. In France, for example, sales of halal\nproducts sold in stores during the year 2010, increased 23 % and represented\n5.5 billion euros, including 1.1 billion for the fast food, and it has not\nstopped growing since. A new market is not only about Muslims. Halal is a\nreligious interpretation but its rapid development requires that we question\nhis true motives are not just religious\n",
        "pdf_link": "http://arxiv.org/pdf/1701.07323v1"
    },
    {
        "title": "Decision structure of risky choice",
        "authors": [
            "Lamb Wubin",
            "Naixin Ren"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  As we know, there is a controversy about the decision making under risk\nbetween economists and psychologists. We discuss to build a unified theory of\nrisky choice, which would explain both of compensatory and non-compensatory\ntheories. For risky choice, according to cognition ability, we argue that\npeople could not build a continuous and accurate subjective probability world,\nbut several order concepts, such as small, middle and large probability. People\nmake decisions based on information, experience, imagination and other things.\nAll of these things are so huge that people have to prepare some strategies.\nThat is, people have different strategies when facing to different situations.\nThe distributions of these things have different decision structures. More\nprecisely, decision making is a process of simplifying the decision structure.\nHowever, the process of decision structure simplifying is not stuck in a rut,\nbut through different path when facing problems repeatedly. It is why\npreference reversal always happens when making decisions. The most efficient\nway to simplify the decision structure is calculating expected value or making\ndecisions based on one or two dimensions. We also argue that the deliberation\ntime at least has four parts, which are consist of substitution time, first\norder time, second order time and calculation time. Decision structure also can\nsimply explain the phenomenon of paradoxes and anomalies. JEL Codes: C10, D03,\nD81\n",
        "pdf_link": "http://arxiv.org/pdf/1701.08567v2"
    },
    {
        "title": "Towards a taxonomy of learning dynamics in 2 x 2 games",
        "authors": [
            "Marco Pangallo",
            "James Sanders",
            "Tobias Galla",
            "Doyne Farmer"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Do boundedly rational players learn to choose equilibrium strategies as they\nplay a game repeatedly? A large literature in behavioral game theory has\nproposed and experimentally tested various learning algorithms, but a\ncomparative analysis of their equilibrium convergence properties is lacking. In\nthis paper we analyze Experience-Weighted Attraction (EWA), which generalizes\nfictitious play, best-response dynamics, reinforcement learning and also\nreplicator dynamics. Studying $2\\times 2$ games for tractability, we recover\nsome well-known results in the limiting cases in which EWA reduces to the\nlearning rules that it generalizes, but also obtain new results for other\nparameterizations. For example, we show that in coordination games EWA may only\nconverge to the Pareto-efficient equilibrium, never reaching the\nPareto-inefficient one; that in Prisoner Dilemma games it may converge to fixed\npoints of mutual cooperation; and that limit cycles or chaotic dynamics may be\nmore likely with longer or shorter memory of previous play.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.09043v4"
    },
    {
        "title": "Zipf's law for share price and company fundamentals",
        "authors": [
            "Taisei Kaizoji",
            "Michiko Miyano"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We statistically investigate the distribution of share price and the\ndistributions of three common financial indicators using data from\napproximately 8,000 companies publicly listed worldwide for the period\n2004-2013. We find that the distribution of share price follows Zipf's law;\nthat is, it can be approximated by a power law distribution with exponent equal\nto 1. An examination of the distributions of dividends per share, cash flow per\nshare, and book value per share - three financial indicators that can be\nassumed to influence corporate value (i.e. share price) - shows that these\ndistributions can also be approximated by a power law distribution with\npower-law exponent equal to 1. We estimate a panel regression model in which\nshare price is the dependent variable and the three financial indicators are\nexplanatory variables. The two-way fixed effects model that was selected as the\nbest model has quite high power for explaining the actual data. From these\nresults, we can surmise that the reason why share price follows Zipf's law is\nthat corporate value, i.e. company fundamentals, follows Zipf's law.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.00144v1"
    },
    {
        "title": "Learning and Type Compatibility in Signaling Games",
        "authors": [
            "Drew Fudenberg",
            "Kevin He"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Which equilibria will arise in signaling games depends on how the receiver\ninterprets deviations from the path of play. We develop a micro-foundation for\nthese off-path beliefs, and an associated equilibrium refinement, in a model\nwhere equilibrium arises through non-equilibrium learning by populations of\npatient and long-lived senders and receivers. In our model, young senders are\nuncertain about the prevailing distribution of play, so they rationally send\nout-of-equilibrium signals as experiments to learn about the behavior of the\npopulation of receivers. Differences in the payoff functions of the types of\nsenders generate different incentives for these experiments. Using the Gittins\nindex (Gittins, 1979), we characterize which sender types use each signal more\noften, leading to a constraint on the receiver's off-path beliefs based on\n\"type compatibility\" and hence a learning-based equilibrium selection.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.01819v3"
    },
    {
        "title": "An Optimized Microeconomic Modeling System for Analyzing Industrial\n  Externalities in Non-OECD Countries",
        "authors": [
            "Agnibho Roy",
            "Abhishek Mohan"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  In this paper, we provide an integrated systems modeling approach to\nanalyzing global externalities from a microeconomic perspective. Various forms\nof policy (fiscal, monetary, etc.) have addressed flaws and market failures in\nmodels, but few have been able to successfully eliminate modern externalities\nthat remain an environmental and human threat. We assess three primary global\nindustries (pollution, agriculture, and energy) with respect to non-OECD\nentities through both qualitative and quantitative studies. By combining key\nmutual points of specific externalities present within each respective\nindustry, we are able to propose an alternative and optimized solution to\ninternalizing them via incentives and cooperative behavior rather than by\ntraditional Pigouvian taxes and subsidies.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.02755v1"
    },
    {
        "title": "Necessary and Sufficient Conditions for Existence and Uniqueness of\n  Recursive Utilities",
        "authors": [
            "Jaroslav Borovicka",
            "John Stachurski"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We study existence, uniqueness and computability of solutions for a class of\ndiscrete time recursive utilities models. By combining two streams of the\nrecent literature on recursive preferences---one that analyzes principal\neigenvalues of valuation operators and another that exploits the theory of\nmonotone concave operators---we obtain conditions that are both necessary and\nsufficient for existence and uniqueness of solutions. We also show that the\nnatural iterative algorithm is convergent if and only if a solution exists.\nConsumption processes are allowed to be nonstationary.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.06526v4"
    },
    {
        "title": "Simulating the deep decarbonisation of residential heating for limiting\n  global warming to 1.5C",
        "authors": [
            "Florian Knobloch",
            "Hector Pollitt",
            "Unnada Chewpreecha",
            "Vassilis Daioglou",
            "Jean-Francois Mercure"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Whole-economy scenarios for limiting global warming to 1.5C suggest that\ndirect carbon emissions in the buildings sector should decrease to almost zero\nby 2050, but leave unanswered the question how this could be achieved by\nreal-world policies. We take a modelling-based approach for simulating which\npolicy measures could induce an almost-complete decarbonisation of residential\nheating, the by far largest source of direct emissions in residential\nbuildings. Under which assumptions is it possible, and how long would it take?\nPolicy effectiveness highly depends on behavioural decision- making by\nhouseholds, especially in a context of deep decarbonisation and rapid\ntransformation. We therefore use the non-equilibrium bottom-up model FTT:Heat\nto simulate policies for a transition towards low-carbon heating in a context\nof inertia and bounded rationality, focusing on the uptake of heating\ntechnologies. Results indicate that the near-zero decarbonisation is achievable\nby 2050, but requires substantial policy efforts. Policy mixes are projected to\nbe more effective and robust for driving the market of efficient low-carbon\ntechnologies, compared to the reliance on a carbon tax as the only policy\ninstrument. In combination with subsidies for renewables, near-complete\ndecarbonisation could be achieved with a residential carbon tax of\n50-200Euro/tCO2. The policy-induced technology transition would increase\naverage heating costs faced by households initially, but could also lead to\ncost reductions in most world regions in the medium term. Model projections\nillustrate the uncertainty that is attached to household behaviour for\nprematurely replacing heating systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.11019v3"
    },
    {
        "title": "An Equilibrium Model with Computationally Constrained Agents",
        "authors": [
            "Wolfgang Kuhle"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We study a large economy in which firms cannot compute exact solutions to the\nnon-linear equations that characterize the equilibrium price at which they can\nsell future output. Instead, firms use polynomial expansions to approximate\nprices. The precision with which they can compute prices is endogenous and\ndepends on the overall level of supply. At the same time, firms' individual\nsupplies, and thus aggregate supply, depend on the precision with which they\napproximate prices. This interrelation between supply and price forecast\ninduces multiple equilibria, with inefficiently low output, in economies that\notherwise have a unique, efficient equilibrium. Moreover, exogenous parameter\nchanges, which would increase output were there no computational frictions, can\ndiminish agents' ability to approximate future prices, and reduce output. Our\nmodel therefore accommodates the intuition that interventions, such as\nunprecedented quantitative easing, can put agents into \"uncharted territory\".\n",
        "pdf_link": "http://arxiv.org/pdf/1611.01771v1"
    },
    {
        "title": "Optimal Extraction and Taxation of Strategic Natural Resources: A\n  Differential Game Approach",
        "authors": [
            "Moustapha Pemy"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  This paper studies the optimal extraction and taxation of nonrenewable\nnatural resources. It is well known that the market values of the main\nstrategic resources such as oil, natural gas, uranium, copper,..., etc,\nfluctuate randomly following global and seasonal macroeconomic parameters,\nthese values are modeled using Markov switching L\\'evy processes. We formulate\nthis problem as a differential game. The two players of this differential game\nare the mining company whose aim is to maximize the revenues generated from its\nextracting activities and the government agency in charge of regulating and\ntaxing natural resources. We prove the existence of a Nash equilibrium. The\ncorresponding Hamilton Jacobi Isaacs equations are completely solved and the\nvalue functions as well as the optimal extraction and taxation rates are\nderived in closed-form. A Numerical example is presented to illustrate our\nfindings.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.02547v2"
    },
    {
        "title": "Choquet integral in decision analysis - lessons from the axiomatization",
        "authors": [
            "Mikhail Timonin"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The Choquet integral is a powerful aggregation operator which lists many\nwell-known models as its special cases. We look at these special cases and\nprovide their axiomatic analysis. In cases where an axiomatization has been\npreviously given in the literature, we connect the existing results with the\nframework that we have developed. Next we turn to the question of learning,\nwhich is especially important for the practical applications of the model. So\nfar, learning of the Choquet integral has been mostly confined to the learning\nof the capacity. Such an approach requires making a powerful assumption that\nall dimensions (e.g. criteria) are evaluated on the same scale, which is rarely\njustified in practice. Too often categorical data is given arbitrary numerical\nlabels (e.g. AHP), and numerical data is considered cardinally and ordinally\ncommensurate, sometimes after a simple normalization. Such approaches clearly\nlack scientific rigour, and yet they are commonly seen in all kinds of\napplications. We discuss the pros and cons of making such an assumption and\nlook at the consequences which axiomatization uniqueness results have for the\nlearning problems. Finally, we review some of the applications of the Choquet\nintegral in decision analysis. Apart from MCDA, which is the main area of\ninterest for our results, we also discuss how the model can be interpreted in\nthe social choice context. We look in detail at the state-dependent utility,\nand show how comonotonicity, central to the previous axiomatizations, actually\nimplies state-independency in the Choquet integral model. We also discuss the\nconditions required to have a meaningful state-dependent utility representation\nand show the novelty of our results compared to the previous methods of\nbuilding state-dependent models.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.09926v1"
    },
    {
        "title": "Information, Impact, Ignorance, Illegality, Investing, and Inequality",
        "authors": [
            "Bruce Knuteson"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We note a simple mechanism that may at least partially resolve several\noutstanding economic puzzles, including why the cyclically adjusted price to\nearnings ratio of the S&P 500 index has been oddly high for the past two\ndecades, why gains to capital have outpaced gains to wages, and the persistence\nof the equity premium.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.06855v3"
    },
    {
        "title": "Economic Accelerator with Memory: Discrete Time Approach",
        "authors": [
            "Valentina V. Tarasova",
            "Vasily E. Tarasov"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Accelerators with power-law memory are proposed in the framework of the\ndiscrete time approach. To describe discrete accelerators we use the capital\nstock adjustment principle, which has been suggested by Matthews.The suggested\ndiscrete accelerators with memory describe the economic processes with the\npower-law memory and the periodic sharp splashes (kicks). In continuous time\napproach the memory is described by fractional-order differential equations. In\ndiscrete time approach the accelerators with memory are described by discrete\nmaps with memory, which are derived from the fractional-order differential\nequation without approximations. In order to derive these maps we use the\nequivalence of fractional-order differential equations and the Volterra\nintegral equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07913v2"
    },
    {
        "title": "A New Class of Problems in the Calculus of Variations",
        "authors": [
            "Ivar Ekeland",
            "Yiming Long",
            "Qinglong Zhou"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper investigates an infinite-horizon problems in the one-dimensional\ncalculus of variations, arising from the Ramsey model of endogeneous economic\ngrowth. Following Chichilnisky, we introduce an additional term, which models\nconcern for the well-being of future generations. We show that there are no\noptimal solutions, but that there are equilibrium strateges, i.e. Nash\nequilibria of the leader-follower game between successive generations. To solve\nthe problem, we approximate the Chichilnisky criterion by a biexponential\ncriterion, we characterize its equilibria by a pair of coupled differential\nequations of HJB type, and we go to the limit. We find all the equilibrium\nstrategies for the Chichilnisky criterion. The mathematical analysis is\ndifficult because one has to solve an implicit differential equation in the\nsense of Thom. Our analysis extends earlier work by Ekeland and Lazrak. It is\nshown that optimal solutions a class of problems raising from time\ninconsistency problems in the framework of the neoclassical one-sector model of\neconomic growth, and contains new results in environment economics. Without\nexogenous commitment mechanism, a notion of the equilibrium strategies instead\nof the optimal strategies is introduced. We characterized the equilibrium\nstrategies by an integro-differential equation system. For two special\ncriteria, the bi-exponential criteria and the Chichilnisky criteria, we\nestablished the existence of the equilibrium strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.00065v1"
    },
    {
        "title": "Sustainability in the Stochastic Ramsey Model",
        "authors": [
            "Rabi Bhattacharya",
            "Hyeonju Kim",
            "Mukul Majumdar"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In this paper we provide a self-contained exposition of the problem of\nsustaining a constant consumption level in a Ramsey model. Our focus is on the\ncase in which the output capital-ratio is random. After a brief review of the\nknown results on the probabilities of sustaining a target consumption from an\ninitial stock, we present some new results on estimating the probabilities by\nusing Chebyshev inequalities. Some numerical calculations for these estimates\nare also provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.07419v1"
    },
    {
        "title": "Statistical mechanics of complex economies",
        "authors": [
            "Marco Bardoscia",
            "Giacomo Livan",
            "Matteo Marsili"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In the pursuit of ever increasing efficiency and growth, our economies have\nevolved to remarkable degrees of complexity, with nested production processes\nfeeding each other in order to create products of greater sophistication from\nless sophisticated ones, down to raw materials. The engine of such an expansion\nhave been competitive markets that, according to General Equilibrium Theory\n(GET), achieve efficient allocations under specific conditions. We study large\nrandom economies within the GET framework, as templates of complex economies,\nand we find that a non-trivial phase transition occurs: the economy freezes in\na state where all production processes collapse when either the number of\nprimary goods or the number of available technologies fall below a critical\nthreshold. As in other examples of phase transitions in large random systems,\nthis is an unintended consequence of the growth in complexity. Our findings\nsuggest that the Industrial Revolution can be regarded as a sharp transition\nbetween different phases, but also imply that well developed economies can\ncollapse if too many intermediate goods are introduced.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.09203v4"
    },
    {
        "title": "Minimising the expectation value of the procurement cost in electricity\n  markets based on the prediction error of energy consumption",
        "authors": [
            "Naoya Yamaguchi",
            "Maiya Hori",
            "Yoshinari Ideguchi"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  In this paper, we formulate a method for minimising the expectation value of\nthe procurement cost of electricity in two popular spot markets: {\\it\nday-ahead} and {\\it intra-day}, under the assumption that expectation value of\nunit prices and the distributions of prediction errors for the electricity\ndemand traded in two markets are known. The expectation value of the total\nelectricity cost is minimised over two parameters that change the amounts of\nelectricity. Two parameters depend only on the expected unit prices of\nelectricity and the distributions of prediction errors for the electricity\ndemand traded in two markets. That is, even if we do not know the predictions\nfor the electricity demand, we can determine the values of two parameters that\nminimise the expectation value of the procurement cost of electricity in two\npopular spot markets. We demonstrate numerically that the estimate of two\nparameters often results in a small variance of the total electricity cost, and\nillustrate the usefulness of the proposed procurement method through the\nanalysis of actual data.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04532v2"
    },
    {
        "title": "The Economics of BitCoin Price Formation",
        "authors": [
            "Pavel Ciaian",
            "Miroslava Rajcaniova",
            "d'Artis Kancs"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  This paper analyses the relationship between BitCoin price and supply-demand\nfundamentals of BitCoin, global macro-financial indicators and BitCoin\nattractiveness for investors. Using daily data for the period 2009-2014 and\napplying time-series analytical mechanisms, we find that BitCoin market\nfundamentals and BitCoin attractiveness for investors have a significant impact\non BitCoin price. Our estimates do not support previous findings that the\nmacro-financial developments are driving BitCoin price.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.4498v1"
    },
    {
        "title": "Gambling in contests with random initial law",
        "authors": [
            "Han Feng",
            "David Hobson"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  This paper studies a variant of the contest model introduced in Seel and\nStrack [J. Econom. Theory 148 (2013) 2033-2048]. In the Seel-Strack contest,\neach agent or contestant privately observes a Brownian motion, absorbed at\nzero, and chooses when to stop it. The winner of the contest is the agent who\nstops at the highest value. The model assumes that all the processes start from\na common value $x_0>0$ and the symmetric Nash equilibrium is for each agent to\nutilise a stopping rule which yields a randomised value for the stopped\nprocess. In the two-player contest, this randomised value has a uniform\ndistribution on $[0,2x_0]$. In this paper, we consider a variant of the problem\nwhereby the starting values of the Brownian motions are independent,\nnonnegative random variables that have a common law $\\mu$. We consider a\ntwo-player contest and prove the existence and uniqueness of a symmetric Nash\nequilibrium for the problem. The solution is that each agent should aim for the\ntarget law $\\nu$, where $\\nu$ is greater than or equal to $\\mu$ in convex\norder; $\\nu$ has an atom at zero of the same size as any atom of $\\mu$ at zero,\nand otherwise is atom free; on $(0,\\infty)$ $\\nu$ has a decreasing density; and\nthe density of $\\nu$ only decreases at points where the convex order constraint\nis binding.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.7801v2"
    },
    {
        "title": "When does the stock market listen to economic news? New evidence from\n  copulas and news wires",
        "authors": [
            "Ivan Medovikov"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We study association between macroeconomic news and stock market returns\nusing the statistical theory of copulas, and a new comprehensive measure of\nnews based on the indexing of news wires. We find the impact of economic news\non equity returns to be nonlinear and asymmetric. In particular, controlling\nfor economic conditions and surprises associated with releases of economic\ndata, we find that the market reacts strongly and negatively to the most\nunfavourable macroeconomic news, but appears to largely discount the good news.\nThis relationship persists throughout the different stages of the business\ncycle.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.8427v1"
    },
    {
        "title": "Observing Each Other's Observations in the Electronic Mail Game",
        "authors": [
            "Dominik Grafenhofer",
            "Wolgang Kuhle"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We study a Bayesian coordination game where agents receive private\ninformation on the game's payoff structure. In addition, agents receive private\nsignals on each other's private information. We show that once agents possess\nthese different types of information, there exists a coordination game in the\nevaluation of this information. And even though the precisions of both signal\ntypes is exogenous, the precision with which agents predict each other's\nactions at equilibrium turns out to be endogenous. As a consequence, we find\nthat there exist multiple equilibria if the private signals' precision is high.\nThese equilibria differ with regard to the way that agents weight their private\ninformation to reason about each other's actions.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.00882v1"
    },
    {
        "title": "Dynamic Multi-Factor Bid-Offer Adjustment Model: A Feedback Mechanism\n  for Dealers (Market Makers) to Deal (Grapple) with the Uncertainty Principle\n  of the Social Sciences",
        "authors": [
            "Ravi Kashyap"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The author seeks to develop a model to alter the bid-offer spread, currently\nquoted by market makers, that varies with the market and trading conditions.\nThe dynamic nature of financial markets and trading, as with the rest of social\nsciences, where changes can be observed and decisions can be made by\nparticipants to influence the system, means that this model has to be adaptive\nand include a feedback loop that alters the bid-offer adjustment based on the\nmodifications observed in the market and trading conditions, without a\nsignificant time delay.\n  The factors used to adjust the spread are price volatility, which is publicly\nobservable, and trade count and volume, which are generally known only to the\nmarket maker, in various instruments over different historical durations in\ntime. The contributions of each factor to the bid-offer adjustment are computed\nseparately and then consolidated to produce a very adaptive bid-offer\nquotation. The author uses the currency markets to build the sample model\nbecause they are extremely liquid and trading in them is not as transparent as\nother financial instruments, such as equities. Simulating the number of trades\nand the average size of trades from a lognormal distribution, the parameters of\nthe lognormal distributions are chosen such that the total volume in a certain\ninterval matches the volume publicly mentioned by currency trading firms. This\nmethodology can easily be extended to other financial instruments and possibly\nto any product with the ability to make electronic price quotations, or can\neven be used to periodically perform manual price updates on products that are\ntraded non-electronically.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.00085v2"
    },
    {
        "title": "Dependence of technological improvement on artifact interactions",
        "authors": [
            "Subarna Basnet",
            "Christopher L. Magee"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Empirical research has shown performance improvement of many different\ntechnological domains occurs exponentially but with widely varying improvement\nrates. What causes some technologies to improve faster than others do? Previous\nquantitative modeling research has identified artifact interactions, where a\ndesign change in one component influences others, as an important determinant\nof improvement rates. The models predict that improvement rate for a domain is\nproportional to the inverse of the domain interaction parameter. However, no\nempirical research has previously studied and tested the dependence of\nimprovement rates on artifact interactions. A challenge to testing the\ndependence is that any method for measuring interactions has to be applicable\nto a wide variety of technologies. Here we propose a patent-based method that\nis both technology domain-agnostic and less costly than alternative methods. We\nuse textual content from patent sets in 27 domains to find the influence of\ninteractions on improvement rates. Qualitative analysis identified six specific\nkeywords that signal artifact interactions. Patent sets from each domain were\nthen examined to determine the total count of these 6 keywords in each domain,\ngiving an estimate of artifact interactions in each domain. It is found that\nimprovement rates are positively correlated with the inverse of the total count\nof keywords with correlation coefficient of +0.56 with a p-value of 0.002. The\nempirical results agree with model predictions and support the suggestion that\ndomains with higher number of artifacts interactions (higher complexity) will\nimprove at a slower pace.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.02677v2"
    },
    {
        "title": "Unified Growth Theory Contradicted by the Absence of Takeoffs in the\n  Gross Domestic Product",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Data describing historical economic growth are analysed. They demonstrate\nconvincingly that the takeoffs from stagnation to growth, claimed in the\nUnified Growth Theory, never happened. This theory is again contradicted by\ndata, which were used, but never properly analysed, during its formulation. The\nabsence of the claimed takeoffs demonstrates also that the postulate of the\ndifferential takeoffs is contradicted by data.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04686v2"
    },
    {
        "title": "General Equilibrium and Recession Phenomenon",
        "authors": [
            "Nicholas S. Gonchar",
            "Wolodymyr H. Kozyrski",
            "Anatol S. Zhokhin"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The theorems we proved describe the structure of economic equilibrium in the\nexchange economy model. We have studied the structure of property vectors under\ngiven structure of demand vectors at which given price vector is equilibrium\none. On this ground, we describe the general structure of the equilibrium state\nand give characteristic of equilibrium state describing economic recession. The\ntheory developed is applied to explain the state of the economy in some\nEuropean countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04949v1"
    },
    {
        "title": "Tipping elements and climate-economic shocks: Pathways toward integrated\n  assessment",
        "authors": [
            "Robert E. Kopp",
            "Rachael Shwom",
            "Gernot Wagner",
            "Jiacan Yuan"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The literature on the costs of climate change often draws a link between\nclimatic 'tipping points' and large economic shocks, frequently called\n'catastrophes'. The use of the phrase 'tipping points' in this context can be\nmisleading. In popular and social scientific discourse, 'tipping points'\ninvolve abrupt state changes. For some climatic 'tipping points,' the\ncommitment to a state change may occur abruptly, but the change itself may be\nrate-limited and take centuries or longer to realize. Additionally, the\nconnection between climatic 'tipping points' and economic losses is tenuous,\nthough emerging empirical and process-model-based tools provide pathways for\ninvestigating it. We propose terminology to clarify the distinction between\n'tipping points' in the popular sense, the critical thresholds exhibited by\nclimatic and social 'tipping elements,' and 'economic shocks'. The last may be\nassociated with tipping elements, gradual climate change, or non-climatic\ntriggers. We illustrate our proposed distinctions by surveying the literature\non climatic tipping elements, climatically sensitive social tipping elements,\nand climate-economic shocks, and we propose a research agenda to advance the\nintegrated assessment of all three.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.00850v3"
    },
    {
        "title": "Financial Services, Economic Growth and Well-Being: A Four-Pronged Study",
        "authors": [
            "Ravi Kashyap"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  A four-pronged approach to dealing with Social Science Phenomenon is\noutlined. This methodology is applied to Financial Services, Economic Growth\nand Well-Being. The four prongs are like the four directions for an army\ngeneral looking for victory. Just like the four directions, we need to be aware\nthat there is a degree of interconnectedness in the below four prongs.\n-Uncertainty Principle of the Social Sciences -Responsibilities of Fiscal\nJanitors -Need for Smaller Organizations -Redirecting Growth that Generates\nGarbage The importance of gaining a more profound comprehension of welfare and\ndelineating its components into those that result from an increase in goods and\nservices, and hence can be attributed to economic growth, and into those that\nare not related to economic growth but lead to a better quality of life, is\nhighlighted. The reasoning being that economic growth alone is an inadequate\nindicator of well-being. Hand in hand with a better understanding of the\ncharacteristics of welfare, comes the need to consider the metrics we currently\nhave that gauge economic growth and supplement those with measures that capture\nwell-being more holistically.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.00991v2"
    },
    {
        "title": "Big is Fragile: An Attempt at Theorizing Scale",
        "authors": [
            "Atif Ansar",
            "Bent Flyvbjerg",
            "Alexander Budzier",
            "Daniel Lunn"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In this paper we characterise the propensity of big capital investments to\nsystematically deliver poor outcomes as \"fragility,\" a notion suggested by\nNassim Taleb. A thing or system that is easily harmed by randomness is fragile.\nWe argue that, contrary to their appearance, big capital investments break\neasily - i.e. deliver negative net present value - due to various sources of\nuncertainty that impact them during their long gestation, implementation, and\noperation periods. We do not refute the existence of economies of scale and\nscope. Instead we argue that big capital investments have a disproportionate\n(non-linear) exposure to uncertainties that deliver poor or negative returns\nabove and beyond their economies of scale and scope. We further argue that to\nsucceed, leaders of capital projects need to carefully consider where scaling\npays off and where it does not. To automatically assume that \"bigger is\nbetter,\" which is common in megaproject management, is a recipe for failure.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01416v2"
    },
    {
        "title": "Mathematical analysis of historical income per capita distributions",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Data describing historical growth of income per capita [Gross Domestic\nProduct per capita (GDP/cap)] for the world economic growth and for the growth\nin Western Europe, Eastern Europe, Asia, former USSR, Africa and Latin America\nare analysed. They follow closely the linearly-modulated hyperbolic\ndistributions represented by the ratios of hyperbolic distributions obtained by\nfitting the GDP and population data. Results of this analysis demonstrate that\nincome per capita was increasing monotonically. There was no stagnation and\nthere were no transitions from stagnation to growth. The usually postulated\ndramatic escapes from the Malthusian trap never happened because there was no\ntrap. Unified Growth Theory is fundamentally incorrect because its central\npostulates are contradicted repeatedly by data, which were used but never\nanalysed during the formulation of this theory. The large body of\nreadily-available data opens new avenues for the economic and demographic\nresearch. They show that certain fundamental postulates revolving around the\nconcept of Malthusian stagnation need to be replaced by the evidence-based\ninterpretations. Within the range of analysable data, which for the growth of\npopulation extends down to 10,000 BC, growth of human population and economic\ngrowth were hyperbolic. There was no Malthusian stagnation and there were no\ntransitions to distinctly faster trajectories. Industrial Revolution had no\nimpact on changing growth trajectories.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01685v3"
    },
    {
        "title": "Interest Rates and Inflation",
        "authors": [
            "Michael Coopersmith",
            "Pascal J. Gambardella"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  This article is an extension of the work of one of us (Coopersmith, 2011) in\nderiving the relationship between certain interest rates and the inflation rate\nof a two component economic system. We use the well-known Fisher relation\nbetween the difference of the nominal interest rate and its inflation adjusted\nvalue to eliminate the inflation rate and obtain a delay differential equation.\nWe provide computer simulated solutions for this equation over regimes of\ninterest. This paper could be of interest to three audiences: those in\nEconomics who are interested in interest and inflation; those in Mathematics\nwho are interested in examining a detailed analysis of a delay differential\nequation, which includes a summary of existing results, simulations, and an\nexact solution; and those in Physics who are interested in non-traditional\napplications of traditional methods of modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.08311v1"
    },
    {
        "title": "The unresolved mystery of the great divergence is solved",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The so-called great divergence in the income per capita is described in the\nUnified Growth Theory as the mind-boggling and unresolved mystery about the\ngrowth process. This mystery has now been solved: the great divergence never\nhappened. It was created by the manipulation of data. Economic growth in\nvarious regions is at different levels of development but it follows similar,\nnon-divergent trajectories. Unified Growth Theory is shown yet again to be\nincorrect and scientifically unacceptable. It promotes incorrect and even\npotentially dangerous concepts. The distorted presentation of data supporting\nthe concept of the great divergence shows that economic growth is now\ndeveloping along moderately-increasing trajectories but mathematical analysis\nof the same data and even their undistorted presentation shows that these\ntrajectories are now increasing approximately vertically with time. So, while\nthe distorted presentation of data used in the Unified Growth Theory suggests\ngenerally sustainable and secure economic growth, the undistorted presentation\nof data demonstrates that the growth is unsustainable and insecure. The concept\nof takeoffs from stagnation to the sustained-growth regime promoted in the\nUnified Growth Theory is also dangerously misleading because it suggests a\nsustainable and prosperous future while the mathematical analysis of data shows\nthat the current economic growth is insecure and unsustainable.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.08344v2"
    },
    {
        "title": "Electoral Systems Used around the World",
        "authors": [
            "Siamak F. Shahandashti"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We give an overview of the diverse electoral systems used in local, national,\nor super-national elections around the world. We discuss existing methods for\nselecting single and multiple winners and give real-world examples for some\nmore elaborate systems. Eventually, we elaborate on some of the better known\nstrengths and weaknesses of various methods from both the theoretical and\npractical points of view.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.01343v2"
    },
    {
        "title": "Elections in Russia, 1991-2008",
        "authors": [
            "Daniel Treisman"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In this paper, I review the main trends in voting in national elections in\nRussia since 1991, discuss the evidence of manipulation or falsification by the\nauthorities, and use statistical techniques to examine the determinants of\nvoting trends.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05545v1"
    },
    {
        "title": "Far from equilibrium: Wealth reallocation in the United States",
        "authors": [
            "Yonatan Berman",
            "Ole Peters",
            "Alexander Adamou"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Studies of wealth inequality often assume that an observed wealth\ndistribution reflects a system in equilibrium. This constraint is rarely tested\nempirically. We introduce a simple model that allows equilibrium but does not\nassume it. To geometric Brownian motion (GBM) we add reallocation: all\nindividuals contribute in proportion to their wealth and receive equal shares\nof the amount collected. We fit the reallocation rate parameter required for\nthe model to reproduce observed wealth inequality in the United States from\n1917 to 2012. We find that this rate was positive until the 1980s, after which\nit became negative and of increasing magnitude. With negative reallocation, the\nsystem cannot equilibrate. Even with the positive reallocation rates observed,\nequilibration is too slow to be practically relevant. Therefore, studies which\nassume equilibrium must be treated skeptically. By design they are unable to\ndetect the dramatic conditions found here when data are analysed without this\nconstraint.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05631v1"
    },
    {
        "title": "Foreign exchange risk premia: from traditional to state-space analyses",
        "authors": [
            "Siwat Nakmai"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  This paper examines foreign exchange risk premia from simple univariate\nregressions to the state-space method. The adjusted traditional regressions\nproperly figure out the existence and time-evolving property of the risk\npremia. Successively, the state-space estimations overall are quite rationally\ncompetent in examining the essence of time variability of the unobservable risk\npremia. To be more precise, the coefficients on the lagged estimated\ntime-series are significant and the disturbance combined from the observation\nand transition equations in the state-space system, rational and premium\nerrors, respectively, is statistically white noise. Such the two residuals are\ndiscovered to move oppositely with their covariance approaching zero suggested\nby the empirics. Besides, foreign exchange risk premia are projected and found\nsignificantly stationary at level and relatively volatile throughout time with\nsome clustering. This volatility is however not quite dominant in the\ndeviations of forward prediction errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.08025v1"
    },
    {
        "title": "Q-system Cluster Algebras, Paths and Total Positivity",
        "authors": [
            "P. Di Francesco",
            "R. Kedem"
        ],
        "category": "q-fin.EC",
        "published_year": "2009",
        "summary": "  We review the solution of the $A_r$ Q-systems in terms of the partition\nfunction of paths on a weighted graph, and show that it is possible to modify\nthe graphs and transfer matrices so as to provide an explicit connection to the\ntheory of planar networks introduced in the context of totally positive\nmatrices by Fomin and Zelevinsky.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.3421v2"
    },
    {
        "title": "Arbitrage-free exchange rate ensembles over a general trade network",
        "authors": [
            "Stan Palasek"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  It is assumed that under suitable economic and information-theoretic\nconditions, market exchange rates are free from arbitrage. Commodity markets in\nwhich trades occur over a complete graph are shown to be trivial. We therefore\nexamine the vector space of no-arbitrage exchange rate ensembles over an\narbitrary connected undirected graph. Consideration is given for the minimal\ninformation for determination of an exchange rate ensemble. We conclude with a\ntopical discussion of exchanges in which our analyses may be relevant,\nincluding the emergent but highly-regulated (and therefore not a complete\ngraph) market for digital currencies.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.1547v1"
    },
    {
        "title": "Income Inequality in the 21st Century -- A biased summary of Piketty's\n  Capital in the Twenty-First Century",
        "authors": [
            "Dietrich Stauffer"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  Capital usually leads to income, and income is more accurately and easily\nmeasured. Thus we summarize income distributions in USA, Germany, etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.7115v2"
    },
    {
        "title": "Worldwide clustering of the corruption perception",
        "authors": [
            "Michal Paulus",
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We inspect a possible clustering structure of the corruption perception among\n134 countries. Using the average linkage clustering, we uncover a well-defined\nhierarchy in the relationships among countries. Four main clusters are\nidentified and they suggest that countries worldwide can be quite well\nseparated according to their perception of corruption. Moreover, we find a\nstrong connection between corruption levels and a stage of development inside\nthe clusters. The ranking of countries according to their corruption perfectly\ncopies the ranking according to the economic performance measured by the gross\ndomestic product per capita of the member states. To the best of our knowledge,\nthis study is the first one to present an application of hierarchical and\nclustering methods to the specific case of corruption.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00104v1"
    },
    {
        "title": "An equilibrium model for spot and forward prices of commodities",
        "authors": [
            "Michail Anthropelos",
            "Michael Kupper",
            "Antonis Papapantoleon"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We consider a market model that consists of financial investors and producers\nof a commodity. Producers optionally store some production for future sale and\ngo short on forward contracts to hedge the uncertainty of the future commodity\nprice. Financial investors take positions in these contracts in order to\ndiversify their portfolios. The spot and forward equilibrium commodity prices\nare endogenously derived as the outcome of the interaction between producers\nand investors. Assuming that both are utility maximizers, we first prove the\nexistence of an equilibrium in an abstract setting. Then, in a framework where\nthe consumers' demand and the exogenously priced financial market are\ncorrelated, we provide semi-explicit expressions for the equilibrium prices and\nanalyze their dependence on the model parameters. The model can explain why\nincreased investors' participation in forward commodity markets and higher\ncorrelation between the commodity and the stock market could result in higher\nspot prices and lower forward premia.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00674v3"
    },
    {
        "title": "Systemic trade-risk of critical resources",
        "authors": [
            "Peter Klimek",
            "Michael Obersteiner",
            "Stefan Thurner"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In the wake of the 2008 financial crisis the role of strongly interconnected\nmarkets in fostering systemic instability has been increasingly acknowledged.\nTrade networks of commodities are susceptible to deleterious cascades of supply\nshocks that increase systemic trade-risks and pose a threat to geopolitical\nstability. On a global and a regional level we show that supply risk, scarcity,\nand price volatility of non-fuel mineral resources are intricately connected\nwith the structure of the world-trade network of or spanned by these resources.\nOn the global level we demonstrate that the scarcity of a resource, as measured\nby its trade volume compared to extractable reserves, is closely related to the\nsusceptibility of the trade network with respect to cascading shocks. On the\nregional level we find that to some extent the region-specific price volatility\nand supply risk can be understood by centrality measures that capture systemic\ntrade-risk. The resources associated with the highest systemic trade-risk\nindicators are often those that are produced as byproducts of major metals. We\nidentify significant shortcomings in the management of systemic trade-risk, in\nparticular in the EU.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.03508v1"
    },
    {
        "title": "Efficient Network Structures with Separable Heterogeneous Connection\n  Costs",
        "authors": [
            "Babak Heydari",
            "Mohsen Mosleh",
            "Kia Dalili"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We introduce a heterogeneous connection model for network formation to\ncapture the effect of cost heterogeneity on the structure of efficient\nnetworks. In the proposed model, connection costs are assumed to be separable,\nwhich means the total connection cost for each agent is uniquely proportional\nto its degree. For these sets of networks, we provide the analytical solution\nfor the efficient network and discuss stability impli- cations. We show that\nthe efficient network exhibits a core-periphery structure, and for a given\ndensity, we find a lower bound for clustering coefficient of the efficient\nnetwork.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.06634v3"
    },
    {
        "title": "Diversification Preferences in the Theory of Choice",
        "authors": [
            "Enrico G. De Giorgi",
            "Ola Mahmoud"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Diversification represents the idea of choosing variety over uniformity.\nWithin the theory of choice, desirability of diversification is axiomatized as\npreference for a convex combination of choices that are equivalently ranked.\nThis corresponds to the notion of risk aversion when one assumes the\nvon-Neumann-Morgenstern expected utility model, but the equivalence fails to\nhold in other models. This paper studies axiomatizations of the concept of\ndiversification and their relationship to the related notions of risk aversion\nand convex preferences within different choice theoretic models. Implications\nof these notions on portfolio choice are discussed. We cover model-independent\ndiversification preferences, preferences within models of choice under risk,\nincluding expected utility theory and the more general rank-dependent expected\nutility theory, as well as models of choice under uncertainty axiomatized via\nChoquet expected utility theory. Remarks on interpretations of diversification\npreferences within models of behavioral choice are given in the conclusion.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.02025v3"
    },
    {
        "title": "Intransitivity in Theory and in the Real World",
        "authors": [
            "A. Y. Klimenko"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This work considers reasons for and implications of discarding the assumption\nof transitivity, which (transitivity) is the fundamental postulate in the\nutility theory of Von Neumann and Morgenstern, the adiabatic accessibility\nprinciple of Caratheodory and most other theories related to preferences or\ncompetition. The examples of intransitivity are drawn from different fields,\nsuch as law, biology, game theory, economics and competitive evolutionary\ndynamic. This work is intended as a common platform that allows us to discuss\nintransitivity in the context of different disciplines. The basic concepts and\nterms that are needed for consistent treatment of intransitivity in various\napplications are presented and analysed in a unified manner. The analysis\npoints out conditions that necessitate appearance of intransitivity, such as\nmultiplicity of preference criteria and imperfect (i.e. approximate)\ndiscrimination of different cases. The present work observes that with\nincreasing presence and strength of intransitivity, thermodynamics gradually\nfades away leaving space for more general kinetic considerations.\nIntransitivity in competitive systems is linked to complex phenomena that would\nbe difficult or impossible to explain on the basis of transitive assumptions.\nHuman preferences that seem irrational from the perspective of the conventional\nutility theory, become perfectly logical in the intransitive and relativistic\nframework suggested here. The example of competitive simulations for the\nrisk/benefit dilemma demonstrates the significance of intransitivity in cyclic\nbehaviour and abrupt changes in the system. The evolutionary intransitivity\nparameter, which is introduced in the Appendix, is a general measure of\nintransitivity, which is particularly useful in evolving competitive systems.\nQuantum preferences are also considered in the Appendix.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.03169v1"
    },
    {
        "title": "Reputational Learning and Network Dynamics",
        "authors": [
            "Simpson Zhang",
            "Mihaela van der Schaar"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In many real world networks agents are initially unsure of each other's\nqualities and must learn about each other over time via repeated interactions.\nThis paper is the first to provide a methodology for studying the dynamics of\nsuch networks, taking into account that agents differ from each other, that\nthey begin with incomplete information, and that they must learn through past\nexperiences which connections/links to form and which to break. The network\ndynamics in our model vary drastically from the dynamics in models of complete\ninformation. With incomplete information and learning, agents who provide high\nbenefits will develop high reputations and remain in the network, while agents\nwho provide low benefits will drop in reputation and become ostracized. We\nshow, among many other things, that the information to which agents have access\nand the speed at which they learn and act can have a tremendous impact on the\nresulting network dynamics. Using our model, we can also compute the ex ante\nsocial welfare given an arbitrary initial network, which allows us to\ncharacterize the socially optimal network structures for different sets of\nagents. Importantly, we show through examples that the optimal network\nstructure depends sharply on both the initial beliefs of the agents, as well as\nthe rate of learning by the agents. Due to the potential negative consequences\nof ostracism, it may be necessary to place agents with lower initial\nreputations at less central positions within the network.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04065v3"
    },
    {
        "title": "Taming the Basel Leverage Cycle",
        "authors": [
            "Christoph Aymanns",
            "Fabio Caccioli",
            "J. Doyne Farmer",
            "Vincent W. C. Tan"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Effective risk control must make a tradeoff between the microprudential risk\nof exogenous shocks to individual institutions and the macroprudential risks\ncaused by their systemic interactions. We investigate a simple dynamical model\nfor understanding this tradeoff, consisting of a bank with a leverage target\nand an unleveraged fundamental investor subject to exogenous noise with\nclustered volatility. The parameter space has three regions: (i) a stable\nregion, where the system always reaches a fixed point equilibrium; (ii) a\nlocally unstable region, characterized by cycles and chaotic behavior; and\n(iii) a globally unstable region. A crude calibration of parameters to data\nputs the model in region (ii). In this region there is a slowly building price\nbubble, resembling a \"Great Moderation\", followed by a crash, with a period of\napproximately 10-15 years, which we dub the \"Basel leverage cycle\". We propose\na criterion for rating macroprudential policies based on their ability to\nminimize risk for a given average leverage. We construct a one parameter family\nof leverage policies that allows us to vary from the procyclical policies of\nBasel II or III, in which leverage decreases when volatility increases, to\ncountercyclical policies in which leverage increases when volatility increases.\nWe find the best policy depends critically on three parameters: The average\nleverage used by the bank; the relative size of the bank and the\nfundamentalist, and the amplitude of the exogenous noise. Basel II is optimal\nwhen the exogenous noise is high, the bank is small and leverage is low; in the\nopposite limit where the bank is large or leverage is high the optimal policy\nis closer to constant leverage. We also find that systemic risk can be\ndramatically decreased by lowering the leverage target adjustment speed of the\nbanks.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04136v1"
    },
    {
        "title": "Game Design and Analysis for Price based Demand Response: An Aggregate\n  Game Approach",
        "authors": [
            "Maojiao Ye",
            "Guoqiang Hu"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In this paper, an aggregate game approach is proposed for the modeling and\nanalysis of energy consumption control in smart grid. Since the electricity\nuser's cost function depends on the aggregate load, which is unknown to the end\nusers, an aggregate load estimator is employed to estimate it. Based on the\ncommunication among the users about their estimations on the aggregate load,\nNash equilibrium seeking strategies are proposed for the electricity users. By\nusing singular perturbation analysis and Lyapunov stability analysis, a local\nconvergence result to the Nash equilibrium is presented for the energy\nconsumption game that may have multiple Nash equilibria. For the energy\nconsumption game with a unique Nash equilibrium, it is shown that the players'\nstrategies converge to the Nash equilibrium non-locally. More specially, if the\nunique Nash equilibrium is an inner Nash equilibrium, then the convergence rate\ncan be quantified. Energy consumption game with stubborn players is also\ninvestigated. Convergence to the best response strategies for the rational\nplayers is ensured. Numerical examples are provided to verify the effectiveness\nof the proposed methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.02636v3"
    },
    {
        "title": "Agent based simulations visualize Adam Smith's invisible hand by solving\n  Friedrich Hayek's Economic Calculus",
        "authors": [
            "Klaus Jaffe"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Inspired by Adam Smith and Friedrich Hayek, many economists have postulated\nthe existence of invisible forces that drive economic markets. These market\nforces interact in complex ways making it difficult to visualize or understand\nthe interactions in every detail. Here I show how these forces can transcend a\nzero-sum game and become a win-win business interaction, thanks to emergent\nsocial synergies triggered by division of labor. Computer simulations with the\nmodel Sociodynamica show here the detailed dynamics underlying this phenomenon\nin a simple virtual economy. In these simulations, independent agents act in an\neconomy exploiting and trading two different goods in a heterogeneous\nenvironment. All and each of the various forces and individuals were tracked\ncontinuously, allowing to unveil a synergistic effect on economic output\nproduced by the division of labor between agents. Running simulations in a\nhomogeneous environment, for example, eliminated all benefits of division of\nlabor. The simulations showed that the synergies unleashed by division of labor\narise if: Economies work in a heterogeneous environment; agents engage in\ncomplementary activities whose optimization processes diverge; agents have\nmeans to synchronize their activities. This insight, although trivial if viewed\na posteriori, improve our understanding of the source and nature of synergies\nin real economic markets and might render economic and natural sciences more\nconsilient.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.04264v2"
    },
    {
        "title": "Pricing and Referrals in Diffusion on Networks",
        "authors": [
            "Matt V. Leduc",
            "Matthew O. Jackson",
            "Ramesh Johari"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  When a new product or technology is introduced, potential consumers can learn\nits quality by trying the product, at a risk, or by letting others try it and\nfree-riding on the information that they generate. We propose a dynamic game to\nstudy the adoption of technologies of uncertain value, when agents are\nconnected by a network and a monopolist seller chooses a policy to maximize\nprofits. Consumers with low degree (few friends) have incentives to adopt\nearly, while consumers with high degree have incentives to free ride. The\nseller can induce high-degree consumers to adopt early by offering referral\nincentives - rewards to early adopters whose friends buy in the second period.\nReferral incentives thus lead to a `double-threshold strategy' by which low and\nhigh-degree agents adopt the product early while middle-degree agents wait. We\nshow that referral incentives are optimal on certain networks while\ninter-temporal price discrimination (i.e., a first-period price discount) is\noptimal on others, and discuss welfare implications.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.06544v5"
    },
    {
        "title": "Emergence of Cooperative Long-term Market Loyalty in Double Auction\n  Markets",
        "authors": [
            "Aleksandra Aloric",
            "Peter Sollich",
            "Peter McBurney",
            "Tobias Galla"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  Loyal buyer-seller relationships can arise by design, e.g. when a seller\ntailors a product to a specific market niche to accomplish the best possible\nreturns, and buyers respond to the dedicated efforts the seller makes to meet\ntheir needs. We ask whether it is possible, instead, for loyalty to arise\nspontaneously, and in particular as a consequence of repeated interaction and\nco-adaptation among the agents in a market. We devise a stylized model of\ndouble auction markets and adaptive traders that incorporates these features.\nTraders choose where to trade (which market) and how to trade (to buy or to\nsell) based on their previous experience. We find that when the typical scale\nof market returns (or, at fixed scale of returns, the intensity of choice)\nbecome higher than some threshold, the preferred state of the system is\nsegregated: both buyers and sellers are segmented into subgroups that are\npersistently loyal to one market over another. We characterize the segregated\nstate analytically in the limit of large markets: it is stabilized by some\nagents acting cooperatively to enable trade, and provides higher rewards than\nits unsegregated counterpart both for individual traders and the population as\na whole.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07927v2"
    },
    {
        "title": "Tsallis statistics in the income distribution of Brazil",
        "authors": [
            "Abner D. Soares",
            "Newton J. Moura Jr.",
            "Marcelo B. Ribeiro"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  This paper discusses the empirical evidence of Tsallis statistical functions\nin the personal income distribution of Brazil. Yearly samples from 1978 to 2014\nwere linearized by the q-logarithm and straight lines were fitted to the entire\nrange of the income data in all samples, producing a two-parameters-only single\nfunction representation of the whole distribution in every year. The results\nshowed that the time evolution of the parameters is periodic and plotting one\nin terms of the other reveals a cycle mostly clockwise. It was also found that\nthe empirical data oscillate periodically around the fitted straight lines with\nthe amplitude growing as the income values increase. Since the entire income\ndata range can be fitted by a single function, this raises questions on\nprevious results claiming that the income distribution is constituted by a well\ndefined two-classes-base income structure, since such a division in two very\ndistinct income classes might not be an intrinsic property of societies, but a\nconsequence of an a priori fitting-choice procedure that may leave aside\npossibly important income dynamics at the intermediate levels.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.06855v2"
    },
    {
        "title": "Multidimensional matching",
        "authors": [
            "Pierre-André Chiappori",
            "Robert McCann",
            "Brendan Pass"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We present a general analysis of multidimensional matching problems with\ntransferable utility, paying particular attention to the case in which the\ndimensions of heterogeneity on the two sides of the market are unequal. A\nparticular emphasis is put on problems where agents on one side of the market\nare multidimensional and agents on the other side are uni-dimensional, we\ndescribe a general approach to solve such problems. Lastly, we analyze several\nexamples, including an hedonic model with differentiated products, a marriage\nmarket model where wives are differentiated in income and fertility, and a\ncompetitive variation of the Rochet-Chon\\'e problem. In the latter example, we\nshow that the bunching phenomena, observed by Rochet and Chon\\'e in the\nmonopoly context, do not occur in the competitive context\n",
        "pdf_link": "http://arxiv.org/pdf/1604.05771v1"
    },
    {
        "title": "Effects of Sea Level Rise on Economy of the United States",
        "authors": [
            "Monika Novackova",
            "Richard S. J. Tol"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We report the first ex post study of the economic impact of sea level rise.\nWe apply two econometric approaches to estimate the past effects of sea level\nrise on the economy of the USA, viz. Barro type growth regressions adjusted for\nspatial patterns and a matching estimator. Unit of analysis is 3063 counties of\nthe USA. We fit growth regressions for 13 time periods and we estimated\nnumerous varieties and robustness tests for both growth regressions and\nmatching estimator. Although there is some evidence that sea level rise has a\npositive effect on economic growth, in most specifications the estimated\neffects are insignificant. We therefore conclude that there is no stable,\nsignificant effect of sea level rise on economic growth. This finding\ncontradicts previous ex ante studies.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.06247v1"
    },
    {
        "title": "The structure of the climate debate",
        "authors": [
            "Richard S. J. Tol"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  First-best climate policy is a uniform carbon tax which gradually rises over\ntime. Civil servants have complicated climate policy to expand bureaucracies,\npoliticians to create rents. Environmentalists have exaggerated climate change\nto gain influence, other activists have joined the climate bandwagon. Opponents\nto climate policy have attacked the weaknesses in climate research. The climate\ndebate is convoluted and polarized as a result, and climate policy complex.\nClimate policy should become easier and more rational as the Paris Agreement\nhas shifted climate policy back towards national governments. Changing\npolitical priorities, austerity, and a maturing bureaucracy should lead to a\nmore constructive climate debate.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05597v1"
    },
    {
        "title": "Export dynamics as an optimal growth problem in the network of global\n  economy",
        "authors": [
            "Michele Caraglio",
            "Fulvio Baldovin",
            "Attilio L. Stella"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We analyze export data aggregated at world global level of 219 classes of\nproducts over a period of 39 years. Our main goal is to set up a dynamical\nmodel to identify and quantify plausible mechanisms by which the evolutions of\nthe various exports affect each other. This is pursued through a stochastic\ndifferential description, partly inspired by approaches used in population\ndynamics or directed polymers in random media. We outline a complex network of\ntransfer rates which describes how resources are shifted between different\nproduct classes, and determines how casual favorable conditions for one export\ncan spread to the other ones. A calibration procedure allows to fit four free\nmodel-parameters such that the dynamical evolution becomes consistent with the\naverage growth, the fluctuations, and the ranking of the export values observed\nin real data. Growth crucially depends on the balance between maintaining and\nshifting resources to different exports, like in an explore-exploit problem.\nRemarkably, the calibrated parameters warrant a close-to-maximum growth rate\nunder the transient conditions realized in the period covered by data, implying\nan optimal self organization of the global export. According to the model,\nmajor structural changes in the global economy take tens of years.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.04956v1"
    },
    {
        "title": "Network Structure and Naive Sequential Learning",
        "authors": [
            "Krishna Dasaratha",
            "Kevin He"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We study a sequential-learning model featuring a network of naive agents with\nGaussian information structures. Agents apply a heuristic rule to aggregate\npredecessors' actions. They weigh these actions according the strengths of\ntheir social connections to different predecessors. We show this rule arises\nendogenously when agents wrongly believe others act solely on private\ninformation and thus neglect redundancies among observations. We provide a\nsimple linear formula expressing agents' actions in terms of network paths and\nuse this formula to characterize the set of networks where naive agents\neventually learn correctly. This characterization implies that, on all networks\nwhere later agents observe more than one neighbor, there exist\ndisproportionately influential early agents who can cause herding on incorrect\nactions. Going beyond existing social-learning results, we compute the\nprobability of such mislearning exactly. This allows us to compare likelihoods\nof incorrect herding, and hence expected welfare losses, across network\nstructures. The probability of mislearning increases when link densities are\nhigher and when networks are more integrated. In partially segregated networks,\ndivergent early signals can lead to persistent disagreement between groups.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.02105v7"
    },
    {
        "title": "Exploring the relationship between technological improvement and\n  innovation diffusion: An empirical test",
        "authors": [
            "JongRoul Woo",
            "Christopher L. Magee"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Different technological domains have significantly different rates of\nperformance improvement. Prior theory indicates that such differing rates\nshould influence the relative speed of diffusion of the products embodying the\ndifferent technologies since improvement in performance during the diffusion\nprocess increases the desirability of the product diffusing. However, there has\nnot been a broad empirical attempt to examine this effect and to clarify the\nunderlying cause. Therefore, this paper reviews the theoretical basis and\nfocuses upon empirical tests of this effect across multiple products and their\nunderlying technologies. The results for 18 different diffusing products show\nthe expected relationship-faster diffusion for products based on more rapidly\nimproving technological domains- between technological improvement and\ndiffusion with strong statistical significance. The empirical examination also\ndemonstrates that technological improvement does not slow down in the latter\nparts of diffusion when penetration does slow down. This finding indicates that\ndiffusion slow down in the latter stages is due to market saturation effects\nand is not due to slowdown of performance improvement.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.03597v3"
    },
    {
        "title": "The phase space structure of the oligopoly dynamical system by means of\n  Darboux integrability",
        "authors": [
            "Adam Krawiec",
            "Tomasz Stachowiak",
            "Marek Szydlowski"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We investigate the dynamical complexity of Cournot oligopoly dynamics of\nthree firms by using the qualitative methods of dynamical systems to study the\nphase structure of this model. The phase space is organized with\none-dimensional and two-dimensional invariant submanifolds (for the monopoly\nand duopoly) and unique stable node (global attractor) in the positive quadrant\nof the phase space (Cournot equilibrium). We also study the integrability of\nthe system. We demonstrate the effectiveness of the method of the Darboux\npolynomials in searching for first integrals of the oligopoly. The general\nmethod as well as examples of adopting this method are presented. We study\nDarboux non-integrability of the oligopoly for linear demand functions and find\nfirst integrals of this system for special classes of the system, in\nparticular, rational integrals can be found for a quite general set of model\nparameters. We show how first integral can be useful in lowering the dimension\nof the system using the example of $n$ almost identical firms. This first\nintegral also gives information about the structure of the phase space and the\nbehaviour of trajectories in the neighbourhood of a Nash equilibrium\n",
        "pdf_link": "http://arxiv.org/pdf/1708.02193v1"
    },
    {
        "title": "Indirect Inference with a Non-Smooth Criterion Function",
        "authors": [
            "David T. Frazier",
            "Tatsushi Oka",
            "Dan Zhu"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Indirect inference requires simulating realisations of endogenous variables\nfrom the model under study. When the endogenous variables are discontinuous\nfunctions of the model parameters, the resulting indirect inference criterion\nfunction is discontinuous and does not permit the use of derivative-based\noptimisation routines. Using a change of variables technique, we propose a\nnovel simulation algorithm that alleviates the discontinuities inherent in such\nindirect inference criterion functions, and permits the application of\nderivative-based optimisation routines to estimate the unknown model\nparameters. Unlike competing approaches, this approach does not rely on kernel\nsmoothing or bandwidth parameters. Several Monte Carlo examples that have\nfeatured in the literature on indirect inference with discontinuous outcomes\nillustrate the approach, and demonstrate the superior performance of this\napproach over existing alternatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.02365v3"
    },
    {
        "title": "Quantum Barro--Gordon Game in Monetary Economics",
        "authors": [
            "Ali Hussein Samadi",
            "Afshin Montakhab",
            "Hussein Marzban",
            "Sakine Owjimehr"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  Classical game theory addresses decision problems in multi-agent environment\nwhere one rational agent's decision affects other agents' payoffs. Game theory\nhas widespread application in economic, social and biological sciences. In\nrecent years quantum versions of classical games have been proposed and\nstudied. In this paper, we consider a quantum version of the classical\nBarro-Gordon game which captures the problem of time inconsistency in monetary\neconomics. Such time inconsistency refers to the temptation of weak policy\nmaker to implement high inflation when the public expects low inflation. The\ninconsistency arises when the public punishes the weak policy maker in the next\ncycle. We first present a quantum version of the Barro-Gordon game. Next, we\nshow that in a particular case of the quantum game, time-consistent Nash\nequilibrium could be achieved when public expects low inflation, thus resolving\nthe game.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.05689v1"
    },
    {
        "title": "On Game-Theoretic Risk Management (Part Three) - Modeling and\n  Applications",
        "authors": [
            "Stefan Rass"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  The game-theoretic risk management framework put forth in the precursor\nreports \"Towards a Theory of Games with Payoffs that are\nProbability-Distributions\" (arXiv:1506.07368 [q-fin.EC]) and \"Algorithms to\nCompute Nash-Equilibria in Games with Distributions as Payoffs\"\n(arXiv:1511.08591v1 [q-fin.EC]) is herein concluded by discussing how to\nintegrate the previously developed theory into risk management processes. To\nthis end, we discuss how loss models (primarily but not exclusively\nnon-parametric) can be constructed from data. Furthermore, hints are given on\nhow a meaningful game theoretic model can be set up, and how it can be used in\nvarious stages of the ISO 27000 risk management process. Examples related to\nadvanced persistent threats and social engineering are given. We conclude by a\ndiscussion on the meaning and practical use of (mixed) Nash equilibria\nequilibria for risk management.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.00708v1"
    },
    {
        "title": "Capital Structure in U.S., a Quantile Regression Approach with\n  Macroeconomic Impacts",
        "authors": [
            "Andreas Kaloudis",
            "Dimitrios Tsolis"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  The major perspective of this paper is to provide more evidence into the\nempirical determinants of capital structure adjustment in different\nmacroeconomics states by focusing and discussing the relative importance of\nfirm-specific and macroeconomic characteristics from an alternative scope in\nU.S. This study extends the empirical research on the topic of capital\nstructure by focusing on a quantile regression method to investigate the\nbehavior of firm-specific characteristics and macroeconomic variables across\nall quantiles of distribution of leverage (total debt, long-terms debt and\nshort-terms debt). Thus, based on a partial adjustment model, we find that\nlong-term and short-term debt ratios varying regarding their partial adjustment\nspeeds; the short-term debt raises up while the long-term debt ratio slows down\nfor same periods.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06651v1"
    },
    {
        "title": "Stock management (Gestão de estoques)",
        "authors": [
            "Cainan K. de Oliveira",
            "Henrique G. Menck",
            "Pedro Y. Takito",
            "Eliandro Rodrigues Cirilo",
            "Neyva Maria Lopes Romeiro",
            "Érica R. Takano Natti",
            "Paulo Laerte Natti"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  There is a great need to stock materials for production, but storing\nmaterials comes at a cost. Lack of organization in the inventory can result in\na very high cost for the final product, in addition to generating other\nproblems in the production chain. In this work we present mathematical and\nstatistical methods applicable to stock management. The stock analysis using\nABC curves serves to identify which are the priority items, the most expensive\nand with the highest turnover (demand), and thus determine, through stock\ncontrol models, the purchase lot size and the periodicity that minimize the\ntotal costs of storing these materials. Using the Economic Order Quantity (EOQ)\nmodel and the (Q,R) model, the inventory costs of a company were minimized. The\ncomparison of the results provided by the models was performed.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.10003v2"
    },
    {
        "title": "A simple dynamical model leading to Pareto wealth distribution and\n  stability",
        "authors": [
            "Ricardo Pérez-Marco"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We propose a simple dynamical model of wealth evolution. The invariant\ndistributions are of Pareto type and are dynamically stable as conjectured by\nPareto.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.4857v2"
    },
    {
        "title": "Quick or Persistent? Strategic Investment Demanding Versatility",
        "authors": [
            "Jan-Henrik Steg",
            "Jacco Thijssen"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In this paper we analyse a dynamic model of investment under uncertainty in a\nduopoly, in which each firm has an option to switch from the present market to\na new market. We construct a subgame perfect equilibrium in mixed strategies\nand show that both preemption and attrition can occur along typical equilibrium\npaths. In order to determine the attrition region a two-dimensional constrained\noptimal stopping problem needs to be solved, for which we characterize the\nnon-trivial stopping boundary in the state space. We explicitly determine\nMarkovian equilibrium stopping rates in the attrition region and show that\nthere is always a positive probability of eventual preemption, contrasting the\ndeterministic version of the model. A simulation-based numerical example\nillustrates the model and shows the relative likelihoods of investment taking\nplace in attrition and preemption regions.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.04698v1"
    },
    {
        "title": "Modeling and Computation of Mean Field Equilibria in Producers' Game\n  with Emission Permits Trading",
        "authors": [
            "Shuhua Chang",
            "Xinyu Wang",
            "Alexander Shananin"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  In this paper, we present a mean field game to model the production behaviors\nof a very large number of producers, whose carbon emissions are regulated by\ngovernment. Especially, an emission permits trading scheme is considered in our\nmodel, in which each enterprise can trade its own permits flexibly. By means of\nthe mean field equilibrium, we obtain a Hamilton-Jacobi-Bellman (HJB) equation\ncoupled with a Kolmogorov equation, which are satisfied by the adjoint state\nand the density of producers (agents), respectively. Then, we propose a\nso-called fitted finite volume method to solve the HJB equation and the\nKolmogorov equation. The efficiency and the usefulness of this method are\nillustrated by the numerical experiments. Under different conditions, the\nequilibrium states as well as the effects of the emission permits price are\nexamined, which demonstrates that the emission permits trading scheme\ninfluences the producers' behaviors, that is, more populations would like to\nchoose a lower rather than a higher emission level when the emission permits\nare expensive.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.04869v1"
    },
    {
        "title": "Interpolating between matching and hedonic pricing models",
        "authors": [
            "Brendan Pass"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  We consider the theoretical properties of a model which encompasses\nbi-partite matching under transferable utility on the one hand, and hedonic\npricing on the other. This framework is intimately connected to tripartite\nmatching problems (known as multi-marginal optimal transport problems in the\nmathematical literature). We exploit this relationship in two ways; first, we\nshow that a known structural result from multi-marginal optimal transport can\nbe used to establish an upper bound on the dimension of the support of stable\nmatchings. Next, assuming the distribution of agents on one side of the market\nis continuous, we identify a condition on their preferences that ensures purity\nand uniqueness of the stable matching; this condition is a variant of a known\ncondition in the mathematical literature, which guarantees analogous properties\nin the multi-marginal optimal transport problem. We exhibit several examples of\nsurplus functions for which our condition is satisfied, as well as some for\nwhich it fails.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.04431v1"
    },
    {
        "title": "Bank monitoring incentives under moral hazard and adverse selection",
        "authors": [
            "Nicolás Hernández Santibáñez",
            "Dylan Possamaï",
            "Chao Zhou"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  In this paper, we extend the optimal securitisation model of Pag\\`es [50] and\nPossama\\\"i and Pag\\`es [51] between an investor and a bank to a setting\nallowing both moral hazard and adverse selection. Following the recent approach\nto these problems of Cvitani\\'c, Wan and Yang [14], we characterise explicitly\nand rigorously the so-called credible set of the continuation and temptation\nvalues of the bank, and obtain the value function of the investor as well as\nthe optimal contracts through a recursive system of first-order variational\ninequalities with gradient constraints. We provide a detailed discussion of the\nproperties of the optimal menu of contracts.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.05864v3"
    },
    {
        "title": "Economic inequality and mobility for stochastic models with\n  multiplicative noise",
        "authors": [
            "Maria Letizia Bertotti",
            "Amit K Chattopadhyay",
            "Giovanni Modanese"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  In this article, we discuss a dynamical stochastic model that represents the\ntime evolution of income distribution of a population, where the dynamics\ndevelop from an interplay of multiple economic exchanges in the presence of\nmultiplicative noise. The model remit stretches beyond the conventional\nframework of a Langevin-type kinetic equation in that our model dynamics is\nself-consistently constrained by dynamical conservation laws emerging from\npopulation and wealth conservation. This model is numerically solved and\nanalyzed to interpret the inequality of income as a function of relevant\ndynamical parameters like the {\\it mobility} $M$ and the {\\it total income}\n$\\mu$. In our model, inequality is quantified by the {\\it Gini index} $G$. In\nparticular, correlations between any two of the mobility index $M$ and/or the\ntotal income $\\mu$ with the Gini index $G$ are investigated and compared with\nthe analogous correlations resulting from an equivalent additive noise model.\nOur findings highlight the importance of a multiplicative noise based economic\nmodeling structure in the analysis of inequality. The model also depicts the\nnature of correlation between mobility and total income of a population from\nthe perspective of inequality measure.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.08391v1"
    },
    {
        "title": "Naive Diversification Preferences and their Representation",
        "authors": [
            "Enrico G. De Giorgi",
            "Ola Mahmoud"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  A widely applied diversification paradigm is the naive diversification choice\nheuristic. It stipulates that an economic agent allocates equal decision\nweights to given choice alternatives independent of their individual\ncharacteristics. This article provides mathematically and economically sound\nchoice theoretic foundations for the naive approach to diversification. We\naxiomatize naive diversification by defining it as a preference for equality\nover inequality and derive its relationship to the classical diversification\nparadigm. In particular, we show that (i) the notion of permutation invariance\nlies at the core of naive diversification and that an economic agent is a naive\ndiversifier if and only if his preferences are convex and permutation\ninvariant; (ii) Schur-concave utility functions capture the idea of being\ninequality averse on top of being risk averse; and (iii) the transformations,\nwhich rebalance unequal decision weights to equality, are characterized in\nterms of their implied turnover.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.01285v2"
    },
    {
        "title": "EM Algorithm and Stochastic Control in Economics",
        "authors": [
            "Steven Kou",
            "Xianhua Peng",
            "Xingbo Xu"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Generalising the idea of the classical EM algorithm that is widely used for\ncomputing maximum likelihood estimates, we propose an EM-Control (EM-C)\nalgorithm for solving multi-period finite time horizon stochastic control\nproblems. The new algorithm sequentially updates the control policies in each\ntime period using Monte Carlo simulation in a forward-backward manner; in other\nwords, the algorithm goes forward in simulation and backward in optimization in\neach iteration. Similar to the EM algorithm, the EM-C algorithm has the\nmonotonicity of performance improvement in each iteration, leading to good\nconvergence properties. We demonstrate the effectiveness of the algorithm by\nsolving stochastic control problems in the monopoly pricing of perishable\nassets and in the study of real business cycle.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.01767v1"
    },
    {
        "title": "The Coconut Model with Heterogeneous Strategies and Learning",
        "authors": [
            "Sven Banisch",
            "Eckehard Olbrich"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In this paper, we develop an agent-based version of the Diamond search\nequilibrium model - also called Coconut Model. In this model, agents are faced\nwith production decisions that have to be evaluated based on their expectations\nabout the future utility of the produced entity which in turn depends on the\nglobal production level via a trading mechanism. While the original dynamical\nsystems formulation assumes an infinite number of homogeneously adapting agents\nobeying strong rationality conditions, the agent-based setting allows to\ndiscuss the effects of heterogeneous and adaptive expectations and enables the\nanalysis of non-equilibrium trajectories. Starting from a baseline\nimplementation that matches the asymptotic behavior of the original model, we\nshow how agent heterogeneity can be accounted for in the aggregate dynamical\nequations. We then show that when agents adapt their strategies by a simple\ntemporal difference learning scheme, the system converges to one of the fixed\npoints of the original system. Systematic simulations reveal that this is the\nonly stable equilibrium solution.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.00221v1"
    },
    {
        "title": "A Markovian Model of the Evolving World Input-Output Network",
        "authors": [
            "Vahid Moosavi",
            "Giulio Isacchini"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The initial theoretical connections between Leontief input-output models and\nMarkov chains were established back in 1950s. However, considering the wide\nvariety of mathematical properties of Markov chains, there has not been a full\ninvestigation of evolving world economic networks with Markov chain formalism.\nUsing the recently available world input-output database, we modeled the\nevolution of the world economic network from 1995 to 2011 through analysis of a\nseries of finite Markov chains. We assessed different aspects of this evolving\nsystem via different properties of the Markov chains such as mixing time,\nKemeny constant, steady state probabilities and perturbation analysis of the\ntransition matrices. First, we showed how the time series of mixing times and\nKemeny constants could be used as an aggregate index of globalization. Next, we\nfocused on the steady state probabilities as a measure of structural power of\nthe economies that are comparable to GDP shares of economies as the traditional\nindex of economies. Further, we introduced two measures of systemic risk,\ncalled systemic influence and systemic fragility, where the former is the ratio\nof number of influenced nodes to the total number of nodes, caused by a shock\nin the activity of a node and the latter is based on the number of times a\nspecific economic node is affected by a shock in the activity of any of the\nother nodes. Finally, focusing on Kemeny constant as a global indicator of\nmonetary flow across the network, we showed that there is a paradoxical effect\nof a change in activity levels of economic nodes on the overall flow of the\nnetwork. While the economic slowdown of the majority of nodes with high\nstructural power results to a slower average monetary flow over the network,\nthere are some nodes, where their slowdowns improve the overall quality of the\nnetwork in terms of connectivity and the average monetary flow.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.06186v3"
    },
    {
        "title": "Long and Short Memory in Economics: Fractional-Order Difference and\n  Differentiation",
        "authors": [
            "Vasily E. Tarasov",
            "Valentina V. Tarasova"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Long and short memory in economic processes is usually described by the\nso-called discrete fractional differencing and fractional integration. We prove\nthat the discrete fractional differencing and integration are the\nGrunwald-Letnikov fractional differences of non-integer order d. Equations of\nARIMA(p,d,q) and ARFIMA(p,d,q) models are the fractional-order difference\nequations with the Grunwald-Letnikov differences of order d. We prove that the\nlong and short memory with power law should be described by the exact\nfractional-order differences, for which the Fourier transform demonstrates the\npower law exactly. The fractional differencing and the Grunwald-Letnikov\nfractional differences cannot give exact results for the long and short memory\nwith power law, since the Fourier transform of these discrete operators satisfy\nthe power law in the neighborhood of zero only. We prove that the economic\nprocesses with the continuous time long and short memory, which is\ncharacterized by the power law, should be described by the fractional\ndifferential equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07903v3"
    },
    {
        "title": "The prevalence of chaotic dynamics in games with many players",
        "authors": [
            "James B. T. Sanders",
            "J. Doyne Farmer",
            "Tobias Galla"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We study adaptive learning in a typical p-player game. The payoffs of the\ngames are randomly generated and then held fixed. The strategies of the players\nevolve through time as the players learn. The trajectories in the strategy\nspace display a range of qualitatively different behaviors, with attractors\nthat include unique fixed points, multiple fixed points, limit cycles and\nchaos. In the limit where the game is complicated, in the sense that the\nplayers can take many possible actions, we use a generating-functional approach\nto establish the parameter range in which learning dynamics converge to a\nstable fixed point. The size of this region goes to zero as the number of\nplayers goes to infinity, suggesting that complex non-equilibrium behavior,\nexemplified by chaos, may be the norm for complicated games with many players.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08111v1"
    },
    {
        "title": "Population and trends in the global mean temperature",
        "authors": [
            "Richard S. J. Tol"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The Fisher Ideal index, developed to measure price inflation, is applied to\ndefine a population-weighted temperature trend. This method has the advantages\nthat the trend is representative for the population distribution throughout the\nsample but without conflating the trend in the population distribution and the\ntrend in the temperature. I show that the trend in the global area-weighted\naverage surface air temperature is different in key details from the\npopulation-weighted trend. I extend the index to include urbanization and the\nurban heat island effect. This substantially changes the trend again. I further\nextend the index to include international migration, but this has a minor\nimpact on the trend.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.09123v1"
    },
    {
        "title": "Investor Experiences and Financial Market Dynamics",
        "authors": [
            "Ulrike Malmendier",
            "Demian Pouzo",
            "Victoria Vanasco"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  How do macro-financial shocks affect investor behavior and market dynamics?\nRecent evidence on experience effects suggests a long-lasting influence of\npersonally experienced outcomes on investor beliefs and investment, but also\nsignificant differences across older and younger generations. We formalize\nexperience-based learning in an OLG model, where different cross-cohort\nexperiences generate persistent heterogeneity in beliefs, portfolio choices,\nand trade. The model allows us to characterize a novel link between investor\ndemographics and the dependence of prices on past dividends, while also\ngenerating known features of asset prices, such as excess volatility and return\npredictability. The model produces new implications for the cross-section of\nasset holdings, trade volume, and investors' heterogenous responses to recent\nfinancial crises, which we show to be in line with the data.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.09553v3"
    },
    {
        "title": "The Origin and the Resolution of Nonuniqueness in Linear Rational\n  Expectations",
        "authors": [
            "John G. Thistle"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  The nonuniqueness of rational expectations is explained: in the stochastic,\ndiscrete-time, linear, constant-coefficients case, the associated free\nparameters are coefficients that determine the public's most immediate\nreactions to shocks. The requirement of model-consistency may leave these\nparameters completely free, yet when their values are appropriately specified,\na unique solution is determined. In a broad class of models, the requirement of\nleast-square forecast errors determines the parameter values, and therefore\ndefines a unique solution. This approach is independent of dynamical stability,\nand generally does not suppress model dynamics.\n  Application to a standard New Keynesian example shows that the traditional\nsolution suppresses precisely those dynamics that arise from rational\nexpectations. The uncovering of those dynamics reveals their incompatibility\nwith the new I-S equation and the expectational Phillips curve.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.06657v3"
    },
    {
        "title": "Preemptive Investment under Uncertainty",
        "authors": [
            "Jan-Henrik Steg"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  This paper provides a general characterization of subgame perfect equilibria\nfor strategic timing problems, where two firms have the (real) option to make\nan irreversible investment. Profit streams are uncertain and depend on the\nmarket structure. The analysis is based directly on the inherent economic\nstructure of the model. In particular, the determination of equilibria with\npreemptive investment is reduced to solving a single class of constrained\noptimal stopping problems. The general results are applied to typical\nstate-space models, completing commonly insufficient equilibrium arguments,\nshowing when uncertainty leads to qualitatively different behavior, and\nestablishing additional equilibria that are Pareto improvements.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.03863v3"
    },
    {
        "title": "Mislearning from Censored Data: The Gambler's Fallacy and Other\n  Correlational Mistakes in Optimal-Stopping Problems",
        "authors": [
            "Kevin He"
        ],
        "category": "q-fin.EC",
        "published_year": "2018",
        "summary": "  I study endogenous learning dynamics for people who misperceive intertemporal\ncorrelations in random sequences. Biased agents face an optimal-stopping\nproblem. They are uncertain about the underlying distribution and learn its\nparameters from predecessors. Agents stop when early draws are \"good enough,\"\nso predecessors' experiences contain negative streaks but not positive streaks.\nWhen agents wrongly expect systematic reversals (the \"gambler's fallacy\"), they\nunderstate the likelihood of consecutive below-average draws, converge to\nover-pessimistic beliefs about the distribution's mean, and stop too early.\nAgents uncertain about the distribution's variance overestimate it to an extent\nthat depends on predecessors' stopping thresholds. I also analyze how other\nmisperceptions of intertemporal correlation interact with endogenous data\ncensoring.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08170v6"
    },
    {
        "title": "Evaluating gambles using dynamics",
        "authors": [
            "Ole Peters",
            "Murray Gell-Mann"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  Gambles are random variables that model possible changes in monetary wealth.\nClassic decision theory transforms money into utility through a utility\nfunction and defines the value of a gamble as the expectation value of utility\nchanges. Utility functions aim to capture individual psychological\ncharacteristics, but their generality limits predictive power. Expectation\nvalue maximizers are defined as rational in economics, but expectation values\nare only meaningful in the presence of ensembles or in systems with ergodic\nproperties, whereas decision-makers have no access to ensembles and the\nvariables representing wealth in the usual growth models do not have the\nrelevant ergodic properties. Simultaneously addressing the shortcomings of\nutility and those of expectations, we propose to evaluate gambles by averaging\nwealth growth over time. No utility function is needed, but a dynamic must be\nspecified to compute time averages. Linear and logarithmic \"utility functions\"\nappear as transformations that generate ergodic observables for purely additive\nand purely multiplicative dynamics, respectively. We highlight inconsistencies\nthroughout the development of decision theory, whose correction clarifies that\nour perspective is legitimate. These invalidate a commonly cited argument for\nbounded utility functions.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.0585v2"
    },
    {
        "title": "Merchant Sharing Towards a Zero Marginal Cost Economy",
        "authors": [
            "Laurent Fournier"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  This paper is the first attempt to formalize a new field of economics;\nstudding the Intangibles Goods available on the Internet. We are taking\nadvantage of the digital world's specific rules, in particular the zero\nmarginal cost, to propose a theory of trading & sharing unified. A function\nbased money is created as a world-wide currency; \"cup\". We argue that our\nsystem discourage speculation activities while it makes easy captured taxes for\ngovernments. The implementation removes the today's paywall on the Internet and\nprovides a simple-to-use, open-source, free-of-charge, highly-secure,\nperson-to-person, privacy-respectful, digital payment tool for citizens, using\nstandard smart-phones with a strong authentication. Next step will be the\npropagation of the network application and we expect many shared benefits for\nthe whole economics development.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.2051v1"
    },
    {
        "title": "The dynamics of the leverage cycle",
        "authors": [
            "Christoph Aymanns",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  We present a simple agent-based model of a financial system composed of\nleveraged investors such as banks that invest in stocks and manage their risk\nusing a Value-at-Risk constraint, based on historical observations of asset\nprices. The Value-at-Risk constraint implies that when perceived risk is low,\nleverage is high and vice versa, a phenomenon that has been dubbed pro-cyclical\nleverage. We show that this leads to endogenous irregular oscillations, in\nwhich gradual increases in stock prices and leverage are followed by drastic\nmarket collapses, i.e. a leverage cycle. This phenomenon is studied using\nsimplified models that give a deeper understanding of the dynamics and the\nnature of the feedback loops and instabilities underlying the leverage cycle.\nWe introduce a flexible leverage regulation policy in which it is possible to\ncontinuously tune from pro-cyclical to countercyclical leverage. When the\npolicy is sufficiently countercyclical and bank risk is sufficiently low the\nendogenous oscillation disappears and prices go to a fixed point. While there\nis always a leverage ceiling above which the dynamics are unstable,\ncountercyclical leverage can be used to raise the ceiling. We also study the\nimpact on leverage cycles of direct, temporal control of the bank's riskiness\nvia the bank's required Value-at-Risk quantile. Under such a rule the regulator\nrelaxes the Value-at-Risk quantile following a negative stock price shock and\ntightens it following a positive shock. While such a policy rule can reduce the\namplitude of leverage cycles, its effectiveness is highly dependent on the\nchoice of parameters. Finally, we investigate fixed limits on leverage and show\nhow they can control the leverage cycle.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.5305v2"
    },
    {
        "title": "Assessing the Inequalities of Wealth in Regions: the Italian Case",
        "authors": [
            "Roy Cerqueti",
            "Marcel Ausloos"
        ],
        "category": "q-fin.EC",
        "published_year": "2014",
        "summary": "  This paper discusses region wealth size distributions, through their member\ncities aggregated tax income. As an illustration, the official data of the\nItalian Ministry of Economics and Finance has been considered, for all Italian\nmunicipalities, over the period 2007-2011. Yearly data of the aggregated tax\nincome is transformed into a few indicators: the Gini, Theil, and\nHerfindahl-Hirschman indices. On one hand, the relative interest of each index\nis discussed. On the other hand, numerical results confirm that Italy is\ndivided into very different regional realities, a few which are specifically\noutlined. This shows the interest of transforming data in an adequate manner\nand of comparing such indices.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.4922v1"
    },
    {
        "title": "Monetary Policy and Dark Corners in a stylized Agent-Based Model",
        "authors": [
            "Stanislao Gualdi",
            "Marco Tarzia",
            "Francesco Zamponi",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.EC",
        "published_year": "2015",
        "summary": "  We extend in a minimal way the stylized model introduced in in \"Tipping\nPoints in Macroeconomic Agent Based Models\" [JEDC 50, 29-61 (2015)], with the\naim of investigating the role and efficacy of monetary policy of a `Central\nBank' that sets the interest rate such as to steer the economy towards a\nprescribed inflation and employment level. Our major finding is that provided\nits policy is not too aggressive (in a sense detailed in the paper) the Central\nBank is successful in achieving its goals. However, the existence of different\nequilibrium states of the economy, separated by phase boundaries (or \"dark\ncorners\"), can cause the monetary policy itself to trigger instabilities and be\ncounter-productive. In other words, the Central Bank must navigate in a narrow\nwindow: too little is not enough, too much leads to instabilities and wildly\noscillating economies. This conclusion strongly contrasts with the prediction\nof DSGE models.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.00434v2"
    },
    {
        "title": "A Mathematical Model of Foreign Capital Inflow",
        "authors": [
            "Gopal K. Basak",
            "Pranab Kumar Das",
            "Allena Rohit"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  The paper models foreign capital inflow from the developed to the developing\ncountries in a stochastic dynamic programming (SDP) framework. Under some\nregularity conditions, the existence of the solutions to the SDP problem is\nproved and they are then obtained by numerical technique because of the\nnon-linearity of the related functions. A number of comparative dynamic\nanalyses explore the impact of parameters of the model on dynamic paths of\ncapital inflow, interest rate in the international loan market and the exchange\nrate.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.02438v2"
    },
    {
        "title": "Can banks default overnight? Modeling endogenous contagion on O/N\n  interbank market",
        "authors": [
            "Paweł Smaga",
            "Mateusz Wiliński",
            "Piotr Ochnicki",
            "Piotr Arendarski",
            "Tomasz Gubiec"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  We propose a new model of the liquidity driven banking system focusing on\novernight interbank loans. This significant branch of the interbank market is\ncommonly neglected in the banking system modeling and systemic risk analysis.\nWe construct a model where banks are allowed to use both the interbank and the\nsecurities markets to manage their liquidity demand and supply as driven by\nprudential requirements in a volatile environment. The network of interbank\nloans is dynamic and simulated every day. We show how only the intrasystem cash\nfluctuations, without any external shocks, may lead to systemic defaults, what\nmay be a symptom of the self-organized criticality of the system. We also\nanalyze the impact of different prudential regulations and market conditions on\nthe interbank market resilience. We confirm that central bank's asset purchase\nprograms, limiting the declines in government bond prices, can successfully\nstabilize bank's liquidity demand. The model can be used to analyze the\ninterbank market impact of macroprudential tools.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.05142v1"
    },
    {
        "title": "The mathematics of non-linear metrics for nested networks",
        "authors": [
            "Rui-Jie Wu",
            "Gui-Yuan Shi",
            "Yi-Cheng Zhang",
            "Manuel Sebastian Mariani"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Numerical analysis of data from international trade and ecological networks\nhas shown that the non-linear fitness-complexity metric is the best candidate\nto rank nodes by importance in bipartite networks that exhibit a nested\nstructure. Despite its relevance for real networks, the mathematical properties\nof the metric and its variants remain largely unexplored. Here, we perform an\nanalytic and numeric study of the fitness-complexity metric and a new variant,\ncalled minimal extremal metric. We rigorously derive exact expressions for node\nscores for perfectly nested networks and show that these expressions explain\nthe non-trivial convergence properties of the metrics. A comparison between the\nfitness-complexity metric and the minimal extremal metric on real data reveals\nthat the latter can produce improved rankings if the input data are reliable.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06407v1"
    },
    {
        "title": "The behavioural aspect of green technology investments: a general\n  positive model in the context of heterogeneous agents",
        "authors": [
            "F. Knobloch",
            "J. -F. Mercure"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  Studies report that firms do not invest in cost-effective green technologies.\nWhile economic barriers can explain parts of the gap, behavioural aspects cause\nfurther under-valuation. This could be partly due to systematic deviations of\ndecision-making agents' perceptions from normative benchmarks, and partly due\nto their diversity. This paper combines available behavioural knowledge into a\nsimple model of technology adoption. Firms are modelled as heterogeneous agents\nwith different behavioural responses. To quantify the gap, the model simulates\ntheir investment decisions from different theoretical perspectives. While\nrelevant parameters are uncertain at the micro-level, using distributed agent\nperspectives provides a realistic representation of the macro adoption rate.\nThe model is calibrated using audit data for proposed investments in energy\nefficient electric motors. The inclusion of behavioural factors reduces\nsignificantly expected adoption rates: from 81% using a normative optimisation\nperspective, down to 20% using a behavioural perspective. The effectiveness of\nvarious policies is tested.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06888v1"
    },
    {
        "title": "Contracting theory with competitive interacting agents",
        "authors": [
            "Romuald Elie",
            "Dylan Possamaï"
        ],
        "category": "q-fin.EC",
        "published_year": "2016",
        "summary": "  In a framework close to the one developed by Holmstr\\\"om and Milgrom [44], we\nstudy the optimal contracting scheme between a Principal and several Agents.\nEach hired Agent is in charge of one project, and can make efforts towards\nmanaging his own project, as well as impact (positively or negatively) the\nprojects of the other Agents. Considering economic Agents in competition with\nrelative performance concerns, we derive the optimal contracts in both first\nbest and moral hazard settings. The enhanced resolution methodology relies\nheavily on the connection between Nash equilibria and multidimensional\nquadratic BSDEs. The optimal contracts are linear and each agent is paid a\nfixed proportion of the terminal value of all the projects of the firm.\nBesides, each Agent receives his reservation utility, and those with high\ncompetitive appetence are assigned less volatile projects, and shall even\nreceive help from the other Agents. From the principal point of view, it is in\nthe firm interest in our model to strongly diversify the competitive appetence\nof the Agents.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.08099v1"
    },
    {
        "title": "Testing Ambiguity and Machina Preferences Within a Quantum-theoretic\n  Framework for Decision-making",
        "authors": [
            "Diederik Aerts",
            "Suzette Geriente",
            "Catarina Moreira",
            "Sandro Sozzo"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  The Machina thought experiments pose to major non-expected utility models\nchallenges that are similar to those posed by the Ellsberg thought experiments\nto subjective expected utility theory (SEUT). We test human choices in the\n`Ellsberg three-color example', confirming typical ambiguity aversion patterns,\nand the `Machina 50/51 and reflection examples', partially confirming the\npreferences hypothesized by Machina. Then, we show that a quantum-theoretic\nframework for decision-making under uncertainty recently elaborated by some of\nus allows faithful modeling of all data on the Ellsberg and Machina paradox\nsituations. In the quantum-theoretic framework subjective probabilities are\nrepresented by quantum probabilities, while quantum state transformations\nenable representations of ambiguity aversion and subjective attitudes toward\nit.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.02168v1"
    },
    {
        "title": "A Partial Solution to Continuous Blotto",
        "authors": [
            "Kostyantyn Mazur"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  This paper analyzes the structure of mixed-strategy equilibria for Colonel\nBlotto games, where the outcome on each battlefield is a polynomial function of\nthe difference between the two players' allocations. This paper severely\nreduces the set of strategies that needs to be searched to find a Nash\nequilibrium. It finds that there exists a Nash equilibrium where both players'\nmixed strategies are discrete distributions, and it places an upper bound on\nthe number of points in the supports of these discrete distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.08479v2"
    },
    {
        "title": "The Internet as Quantitative Social Science Platform: Insights from a\n  Trillion Observations",
        "authors": [
            "Klaus Ackermann",
            "Simon D Angus",
            "Paul A Raschky"
        ],
        "category": "q-fin.EC",
        "published_year": "2017",
        "summary": "  With the large-scale penetration of the internet, for the first time,\nhumanity has become linked by a single, open, communications platform.\nHarnessing this fact, we report insights arising from a unified internet\nactivity and location dataset of an unparalleled scope and accuracy drawn from\nover a trillion (1.5$\\times 10^{12}$) observations of end-user internet\nconnections, with temporal resolution of just 15min over 2006-2012. We first\napply this dataset to the expansion of the internet itself over 1,647 urban\nagglomerations globally. We find that unique IP per capita counts reach\nsaturation at approximately one IP per three people, and take, on average, 16.1\nyears to achieve; eclipsing the estimated 100- and 60- year saturation times\nfor steam-power and electrification respectively. Next, we use intra-diurnal\ninternet activity features to up-scale traditional over-night sleep\nobservations, producing the first global estimate of over-night sleep duration\nin 645 cities over 7 years. We find statistically significant variation between\ncontinental, national and regional sleep durations including some evidence of\nglobal sleep duration convergence. Finally, we estimate the relationship\nbetween internet concentration and economic outcomes in 411 OECD regions and\nfind that the internet's expansion is associated with negative or positive\nproductivity gains, depending strongly on sectoral considerations. To our\nknowledge, our study is the first of its kind to use online/offline activity of\nthe entire internet to infer social science insights, demonstrating the\nunparalleled potential of the internet as a social data-science platform.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.05632v1"
    }
]